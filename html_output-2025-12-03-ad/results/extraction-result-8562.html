<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8562 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8562</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8562</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270688598</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.14852v2.pdf" target="_blank">Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8562.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8562.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3 (TQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3-8B (evaluated as an LLM on Text-only SpatialEval tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source large language model evaluated in Text-only (TQA) format on SpatialEval tasks; used as a baseline for LLM performance on grid-based spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based LLM from the LLaMA family used in this paper as a text-only backbone; evaluated with standard decoding (top-p for non-deterministic settings) and prompted to supply a concise answer plus step-by-step explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based spatial reasoning (rigid grid with objects per cell; tasks include counting and identifying object at coordinates).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>TQA (Text-only): full textual description of the grid (rows described by item lists) plus multiple-choice questions; prompt appended: 'First, provide a concise answer in one sentence. Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation.' Accuracy used as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Pure language reasoning over the textual representation (no visual grounding); chain-of-thought style prompting (step-by-step explanation) was used to elicit reasoning; default decoding strategies per model (deterministic or top-p) were used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracy on Spatial-Grid: 71.9% (average accuracy across the Spatial-Grid questions as stated in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High TQA accuracy (71.9%) indicates the LLM can perform spatial reasoning when provided an explicit textual encoding of the spatial configuration; authors interpret this as LLMs leveraging structured textual representations to solve grid tasks effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms many vision-enabled counterparts when those rely on VQA (vision-only) inputs; specifically compared in the paper to VLM variants sharing the same backbone where VLMs often perform worse on VQA than the LLM on TQA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends on high-quality, explicit textual representations; when information must be extracted from images (VQA), the model (or its VLM counterparts) often performs much worse. The paper notes that LLM performance here is dependent on the text fully encoding the image, which is not always available in real-world settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8562.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8562.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-1.6-Mistral (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-1.6-Mistral-7B (vision-language model evaluated on Vision-only SpatialEval tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal language model using a Mistral backbone and visual encoder evaluated in Vision-only (VQA) format on SpatialEval; used to assess models that must extract spatial info from images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal language model (LLaVA family) with a Mistral-7B language backbone and visual encoder; used in the paper in VQA and VTQA settings with the same step-by-step prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid (and Spatial-Map / Maze-Nav in aggregate analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based and map/maze spatial puzzles requiring visual extraction of object positions, counting, and path/turn reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>VQA (Vision-only): input is the image only (no textual description), question given in text; models prompted for concise answer plus step-by-step explanation; accuracy averaged across questions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Model attempts to 'translate' visual input into language-space representations via its visual encoder prior to reasoning; uses the same chain-of-thought prompting style. Decoding uses default deterministic or top-p strategies depending on checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracy on Spatial-Grid (vision-only) for this VLM: 47.1% (the paper reports LLaVA-1.6-Mistral-7B yields 47.1% on Spatial-Grid; this is ≈15% lower than a comparable text-only backbone reported).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Low VQA accuracy relative to text-only LLM counterpart suggests limited successful visual extraction and spatial reasoning from raw images. The paper presents this as evidence that the model does not reliably convert visual spatial structure into the language reasoning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to Mistral-7B-Instruct (LLM text-only) which achieved 62.1% on Spatial-Grid; shows the VLM with the same backbone performs ~15% worse in VQA mode. Also compared to LLaVA variants with larger backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles when required to derive spatial relations from images alone, sometimes performing near or below random guessing on synthetic spatial tasks. Paper argues current VLM architectures that 'translate' vision into text-space are insufficient for robust spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8562.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8562.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-1.6-34B (No Image ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-1.6-34B (ablation comparing vision-text, vision-only, and no-image inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multimodal model evaluated in ablation settings where image was removed, replaced by random, or kept; used to probe reliance on visual input and evidence of spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal language model (34B variant) from the LLaVA family combining a large language backbone with a visual encoder; evaluated with the paper's canonical prompt and decoding defaults.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid (ablation reported), Maze-Nav and Spatial-Map (general ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid and maze/map navigation tasks requiring counting, coordinate lookup, and path/turn reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>VTQA ablations: (1) Original Image + text (VTQA), (2) No Image (text only), (3) Random Image (mismatched but task-related image), (4) Noise Image. Models given same textual questions with appended step-by-step prompt. Accuracy averaged across samples reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Uses multimodal encoder to integrate image and text; authors probe whether inclusion of image helps versus harms by replacing or removing it. The reasoning elicited via solicited step-by-step explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation result: on Spatial-Grid the performance of LLaVA-1.6-34B improved by 20.1% when the original image was removed (No Image) compared to when the original image was presented. (Reported as averaged accuracy change in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Counter-intuitive evidence: removing visual input forced the model to rely on textual descriptions and led to better performance, indicating the model often fails to correctly use spatial information from images and may even be distracted by visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared across the ablation conditions and against LLM text-only baselines; No Image often outperforms Vision-only, and Random Image sometimes does not hurt or even improves performance — suggesting weak reliance on actual visual spatial cues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Visual input can harm performance when textual descriptions are sufficient; model may not integrate vision and text effectively and can ignore or be misled by images (including mismatched images). This demonstrates failure modes in multimodal grounding for spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8562.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8562.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (proprietary, VTQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o / GPT-4V family (OpenAI proprietary multimodal models evaluated by the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary multimodal models (GPT-4, GPT-4V, GPT-4o) evaluated on SpatialEval tasks; represent state-of-the-art proprietary performance and are compared to open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (and GPT-4V/GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI proprietary models: GPT-4 (text-only), GPT-4V (vision-capable), and GPT-4o (multimodal variant evaluated in the paper). Evaluated with same step-by-step prompting; due to cost/availability only a single run was performed for proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not reported in paper (proprietary, size unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Grid (and Spatial-Map / Maze-Nav summarized across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid, map, and maze navigation tasks requiring spatial relation reasoning, counting, and path/turn reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated under TQA (text-only), VQA (vision-only), and VTQA (vision + textual description) modalities. Prompting requested concise answer + detailed step-by-step explanation; accuracy used as metric; only one run per proprietary model.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard chain-of-thought style prompt; models reason in their multimodal pipeline when images provided. Decoding and internal mechanisms proprietary and not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top proprietary performance example: GPT-4o achieves an accuracy of 0.989 (98.9%) on Spatial-Grid in the Vision-text format (reported from Table 7). Also reported: GPT-4V's performance increases by 25.6% on Spatial-Grid when switching from Vision-only to Vision-text input.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High VTQA accuracy demonstrates stronger spatial reasoning when both modalities or rich textual descriptions are provided, but the large gains from adding text indicate even these models rely heavily on textual descriptions rather than purely visual spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Proprietary models significantly outperform the open-source models overall, but trends mirror open-source findings: VQA under-performs vs TQA/VTQA and addition of text improves performance; no consistent advantage of VTQA over TQA in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even state-of-the-art proprietary models do not fully leverage visual inputs alone; their strong performance often depends on textual redundancy. Paper notes that despite high accuracy on some tasks, overall limitations remain in using visual input effectively and in reasoning jointly in a vision-language latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8562.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8562.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogVLM (exception)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogVLM (a VLM evaluated in the paper; an exception where the VLM did not outperform its LLM backbone on text-only input)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model evaluated on SpatialEval; unlike most VLMs in the study, CogVLM did not outperform its LLM backbone under text-only input, highlighting heterogeneity in multimodal fine-tuning effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model (CogVLM) combining visual expertise with a language backbone; included in the suite of VLMs evaluated across modalities with standard step-by-step prompting and default decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not explicitly stated in the paper for this entry (use of Hugging Face checkpoint noted).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Map, Maze-Nav, Spatial-Grid (evaluated across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Map relation, maze navigation, and grid counting/coordinate tasks requiring spatial relations and navigation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated in VQA (vision-only), VTQA (vision+text), and text-only (TQA) settings; for TQA the VLM is fed text-only input (no image) to compare the effect of multimodal fine-tuning on the language backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Model applies its multimodal encoder when images are present; when text-only is provided it uses the language backbone; authors probe whether multimodal fine-tuning benefits text-only capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate results show most VLMs outperform their LLM backbones on TQA (text-only) except for CogVLM, which did not show this improvement (exact accuracy numbers for CogVLM per task are in the detailed appendices/tables referenced by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The fact CogVLM fails to gain over its LLM counterpart on text-only indicates multimodal fine-tuning effects vary and do not uniformly produce improved spatial reasoning in the language backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to other VLMs that showed improved text-only performance vs LLM backbones, CogVLM is an explicit counterexample called out by the authors. Also compared across VQA/VTQA modes where CogVLM behaved differently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Illustrates that multimodal training does not guarantee consistent improvements in spatial reasoning across architectures; model-dependent variation exists and some VLMs may not transfer multimodal gains to text-only spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8562.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8562.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLMs (aggregate findings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language Models (aggregated results across many VLM checkpoints evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate summary of many VLMs (Bunny family, CogVLM, CogAgent, InstructBLIP family, LLaVA family, Qwen-VL, etc.) evaluated on the SpatialEval benchmark showing systematic trends about VQA, TQA, and VTQA modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various VLMs (Bunny, CogVLM, CogAgent, InstructBLIP, LLaVA, Qwen-VL, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collection of multimodal models evaluated on SpatialEval; these use combinations of visual encoders (EVA, SigLIP, etc.) and language backbones (Phi2, Mistral, Vicuna, LLaMA derivatives) and were tested in VQA, VTQA and text-only modes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Varied (3B -> 34B across family members; specific checkpoints listed in paper's appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial-Map, Maze-Nav, Spatial-Grid, Spatial-Real</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Map relation reasoning, maze navigation (path and turn counting), structured grid reasoning (counting and coordinates), and real-image spatial reasoning requiring caption comprehension and spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Each task presented in three modalities: TQA (text-only), VQA (vision-only), and VTQA (vision + long textual description). Multiple-choice (4 options) questions; prompt requests concise answer + detailed step-by-step explanation. Accuracy is primary metric; non-deterministic decodings averaged across three runs for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>VLMs rely on visual encoder to convert pixels into language-space features and then perform reasoning in the language model; authors probe the effect of textual redundancy (VTQA), removal or mismatching of images (No Image, Random Image, Noise Image), and decoding strategies. Chain-of-thought prompting used across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate observations: many VLMs perform at or barely above random guessing in VQA across Spatial-Map, Maze-Nav, Spatial-Grid; adding textual descriptions (VTQA) typically improves accuracy vs VQA; VLMs often outperform their LLM backbones in text-only mode (TQA applied to VLM), except for some exceptions (e.g., CogVLM). Specific numeric examples: many open-source VLMs often close to random line (paper's Figure 5); VTQA > VQA with modality gaps (e.g., spatial tasks show VTQA improves over VQA by average differences shown in Figure 11).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Multiple controlled ablations: (1) No Image often improves or does not hurt performance compared to Original Image (forcing reliance on text), (2) Random Image sometimes improves or does not hurt (suggesting weak reliance on true visual cues), (3) VTQA improves over VQA, showing text redundancy helps. These results constitute evidence that many VLMs do not robustly extract spatial structure from images and instead exploit textual cues when available.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Extensive comparisons done between matched language backbones (LLM vs VLM with same backbone) showing VLMs rarely improve via vision-only input; comparisons across VQA, VTQA, and TQA; proprietary models perform better overall but same modality trends hold.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Common failure modes across VLMs: poor utilization of visual spatial layout, visual inputs sometimes harm performance relative to text-only, sensitivity to prompt/decoding settings, and inconsistent multimodal fine-tuning benefits. Authors conclude existing architectures that translate vision into language-space are insufficient for human-like spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating spatial understanding of large language models <em>(Rating: 2)</em></li>
                <li>Geomverse: A systematic evaluation of large models for geometric reasoning <em>(Rating: 2)</em></li>
                <li>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? <em>(Rating: 2)</em></li>
                <li>Isobench: Benchmarking multimodal foundation models on isomorphic representations <em>(Rating: 1)</em></li>
                <li>Stepgame: A new benchmark for robust multihop spatial reasoning in texts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8562",
    "paper_id": "paper-270688598",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "LLaMA-3 (TQA)",
            "name_full": "LLaMA-3-8B (evaluated as an LLM on Text-only SpatialEval tasks)",
            "brief_description": "Open-source large language model evaluated in Text-only (TQA) format on SpatialEval tasks; used as a baseline for LLM performance on grid-based spatial puzzles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B",
            "model_description": "Open-source transformer-based LLM from the LLaMA family used in this paper as a text-only backbone; evaluated with standard decoding (top-p for non-deterministic settings) and prompted to supply a concise answer plus step-by-step explanation.",
            "model_size": "8B",
            "puzzle_name": "Spatial-Grid",
            "puzzle_type": "Grid-based spatial reasoning (rigid grid with objects per cell; tasks include counting and identifying object at coordinates).",
            "task_setup": "TQA (Text-only): full textual description of the grid (rows described by item lists) plus multiple-choice questions; prompt appended: 'First, provide a concise answer in one sentence. Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation.' Accuracy used as metric.",
            "mechanisms_or_strategies": "Pure language reasoning over the textual representation (no visual grounding); chain-of-thought style prompting (step-by-step explanation) was used to elicit reasoning; default decoding strategies per model (deterministic or top-p) were used.",
            "performance_metrics": "Reported accuracy on Spatial-Grid: 71.9% (average accuracy across the Spatial-Grid questions as stated in the paper).",
            "evidence_of_spatial_reasoning": "High TQA accuracy (71.9%) indicates the LLM can perform spatial reasoning when provided an explicit textual encoding of the spatial configuration; authors interpret this as LLMs leveraging structured textual representations to solve grid tasks effectively.",
            "comparisons": "Outperforms many vision-enabled counterparts when those rely on VQA (vision-only) inputs; specifically compared in the paper to VLM variants sharing the same backbone where VLMs often perform worse on VQA than the LLM on TQA.",
            "limitations_or_failure_cases": "Performance depends on high-quality, explicit textual representations; when information must be extracted from images (VQA), the model (or its VLM counterparts) often performs much worse. The paper notes that LLM performance here is dependent on the text fully encoding the image, which is not always available in real-world settings.",
            "uuid": "e8562.0",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaVA-1.6-Mistral (VQA)",
            "name_full": "LLaVA-1.6-Mistral-7B (vision-language model evaluated on Vision-only SpatialEval tasks)",
            "brief_description": "A multimodal language model using a Mistral backbone and visual encoder evaluated in Vision-only (VQA) format on SpatialEval; used to assess models that must extract spatial info from images.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-Mistral-7B",
            "model_description": "Multimodal language model (LLaVA family) with a Mistral-7B language backbone and visual encoder; used in the paper in VQA and VTQA settings with the same step-by-step prompting.",
            "model_size": "7B (Mistral backbone)",
            "puzzle_name": "Spatial-Grid (and Spatial-Map / Maze-Nav in aggregate analyses)",
            "puzzle_type": "Grid-based and map/maze spatial puzzles requiring visual extraction of object positions, counting, and path/turn reasoning.",
            "task_setup": "VQA (Vision-only): input is the image only (no textual description), question given in text; models prompted for concise answer plus step-by-step explanation; accuracy averaged across questions reported.",
            "mechanisms_or_strategies": "Model attempts to 'translate' visual input into language-space representations via its visual encoder prior to reasoning; uses the same chain-of-thought prompting style. Decoding uses default deterministic or top-p strategies depending on checkpoint.",
            "performance_metrics": "Reported accuracy on Spatial-Grid (vision-only) for this VLM: 47.1% (the paper reports LLaVA-1.6-Mistral-7B yields 47.1% on Spatial-Grid; this is ≈15% lower than a comparable text-only backbone reported).",
            "evidence_of_spatial_reasoning": "Low VQA accuracy relative to text-only LLM counterpart suggests limited successful visual extraction and spatial reasoning from raw images. The paper presents this as evidence that the model does not reliably convert visual spatial structure into the language reasoning pipeline.",
            "comparisons": "Directly compared to Mistral-7B-Instruct (LLM text-only) which achieved 62.1% on Spatial-Grid; shows the VLM with the same backbone performs ~15% worse in VQA mode. Also compared to LLaVA variants with larger backbones.",
            "limitations_or_failure_cases": "Struggles when required to derive spatial relations from images alone, sometimes performing near or below random guessing on synthetic spatial tasks. Paper argues current VLM architectures that 'translate' vision into text-space are insufficient for robust spatial reasoning.",
            "uuid": "e8562.1",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaVA-1.6-34B (No Image ablation)",
            "name_full": "LLaVA-1.6-34B (ablation comparing vision-text, vision-only, and no-image inputs)",
            "brief_description": "Large multimodal model evaluated in ablation settings where image was removed, replaced by random, or kept; used to probe reliance on visual input and evidence of spatial reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-34B",
            "model_description": "Large multimodal language model (34B variant) from the LLaVA family combining a large language backbone with a visual encoder; evaluated with the paper's canonical prompt and decoding defaults.",
            "model_size": "34B",
            "puzzle_name": "Spatial-Grid (ablation reported), Maze-Nav and Spatial-Map (general ablations)",
            "puzzle_type": "Grid and maze/map navigation tasks requiring counting, coordinate lookup, and path/turn reasoning.",
            "task_setup": "VTQA ablations: (1) Original Image + text (VTQA), (2) No Image (text only), (3) Random Image (mismatched but task-related image), (4) Noise Image. Models given same textual questions with appended step-by-step prompt. Accuracy averaged across samples reported.",
            "mechanisms_or_strategies": "Uses multimodal encoder to integrate image and text; authors probe whether inclusion of image helps versus harms by replacing or removing it. The reasoning elicited via solicited step-by-step explanations.",
            "performance_metrics": "Ablation result: on Spatial-Grid the performance of LLaVA-1.6-34B improved by 20.1% when the original image was removed (No Image) compared to when the original image was presented. (Reported as averaged accuracy change in the paper.)",
            "evidence_of_spatial_reasoning": "Counter-intuitive evidence: removing visual input forced the model to rely on textual descriptions and led to better performance, indicating the model often fails to correctly use spatial information from images and may even be distracted by visual input.",
            "comparisons": "Compared across the ablation conditions and against LLM text-only baselines; No Image often outperforms Vision-only, and Random Image sometimes does not hurt or even improves performance — suggesting weak reliance on actual visual spatial cues.",
            "limitations_or_failure_cases": "Visual input can harm performance when textual descriptions are sufficient; model may not integrate vision and text effectively and can ignore or be misled by images (including mismatched images). This demonstrates failure modes in multimodal grounding for spatial tasks.",
            "uuid": "e8562.2",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4o (proprietary, VTQA)",
            "name_full": "GPT-4o / GPT-4V family (OpenAI proprietary multimodal models evaluated by the paper)",
            "brief_description": "Proprietary multimodal models (GPT-4, GPT-4V, GPT-4o) evaluated on SpatialEval tasks; represent state-of-the-art proprietary performance and are compared to open-source models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (and GPT-4V/GPT-4)",
            "model_description": "OpenAI proprietary models: GPT-4 (text-only), GPT-4V (vision-capable), and GPT-4o (multimodal variant evaluated in the paper). Evaluated with same step-by-step prompting; due to cost/availability only a single run was performed for proprietary models.",
            "model_size": "Not reported in paper (proprietary, size unspecified)",
            "puzzle_name": "Spatial-Grid (and Spatial-Map / Maze-Nav summarized across tasks)",
            "puzzle_type": "Grid, map, and maze navigation tasks requiring spatial relation reasoning, counting, and path/turn reasoning.",
            "task_setup": "Evaluated under TQA (text-only), VQA (vision-only), and VTQA (vision + textual description) modalities. Prompting requested concise answer + detailed step-by-step explanation; accuracy used as metric; only one run per proprietary model.",
            "mechanisms_or_strategies": "Standard chain-of-thought style prompt; models reason in their multimodal pipeline when images provided. Decoding and internal mechanisms proprietary and not detailed in the paper.",
            "performance_metrics": "Top proprietary performance example: GPT-4o achieves an accuracy of 0.989 (98.9%) on Spatial-Grid in the Vision-text format (reported from Table 7). Also reported: GPT-4V's performance increases by 25.6% on Spatial-Grid when switching from Vision-only to Vision-text input.",
            "evidence_of_spatial_reasoning": "High VTQA accuracy demonstrates stronger spatial reasoning when both modalities or rich textual descriptions are provided, but the large gains from adding text indicate even these models rely heavily on textual descriptions rather than purely visual spatial understanding.",
            "comparisons": "Proprietary models significantly outperform the open-source models overall, but trends mirror open-source findings: VQA under-performs vs TQA/VTQA and addition of text improves performance; no consistent advantage of VTQA over TQA in all cases.",
            "limitations_or_failure_cases": "Even state-of-the-art proprietary models do not fully leverage visual inputs alone; their strong performance often depends on textual redundancy. Paper notes that despite high accuracy on some tasks, overall limitations remain in using visual input effectively and in reasoning jointly in a vision-language latent space.",
            "uuid": "e8562.3",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CogVLM (exception)",
            "name_full": "CogVLM (a VLM evaluated in the paper; an exception where the VLM did not outperform its LLM backbone on text-only input)",
            "brief_description": "A vision-language model evaluated on SpatialEval; unlike most VLMs in the study, CogVLM did not outperform its LLM backbone under text-only input, highlighting heterogeneity in multimodal fine-tuning effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CogVLM",
            "model_description": "Vision-language model (CogVLM) combining visual expertise with a language backbone; included in the suite of VLMs evaluated across modalities with standard step-by-step prompting and default decoding.",
            "model_size": "Not explicitly stated in the paper for this entry (use of Hugging Face checkpoint noted).",
            "puzzle_name": "Spatial-Map, Maze-Nav, Spatial-Grid (evaluated across tasks)",
            "puzzle_type": "Map relation, maze navigation, and grid counting/coordinate tasks requiring spatial relations and navigation reasoning.",
            "task_setup": "Evaluated in VQA (vision-only), VTQA (vision+text), and text-only (TQA) settings; for TQA the VLM is fed text-only input (no image) to compare the effect of multimodal fine-tuning on the language backbone.",
            "mechanisms_or_strategies": "Model applies its multimodal encoder when images are present; when text-only is provided it uses the language backbone; authors probe whether multimodal fine-tuning benefits text-only capability.",
            "performance_metrics": "Aggregate results show most VLMs outperform their LLM backbones on TQA (text-only) except for CogVLM, which did not show this improvement (exact accuracy numbers for CogVLM per task are in the detailed appendices/tables referenced by the paper).",
            "evidence_of_spatial_reasoning": "The fact CogVLM fails to gain over its LLM counterpart on text-only indicates multimodal fine-tuning effects vary and do not uniformly produce improved spatial reasoning in the language backbone.",
            "comparisons": "Compared to other VLMs that showed improved text-only performance vs LLM backbones, CogVLM is an explicit counterexample called out by the authors. Also compared across VQA/VTQA modes where CogVLM behaved differently.",
            "limitations_or_failure_cases": "Illustrates that multimodal training does not guarantee consistent improvements in spatial reasoning across architectures; model-dependent variation exists and some VLMs may not transfer multimodal gains to text-only spatial reasoning.",
            "uuid": "e8562.4",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "VLMs (aggregate findings)",
            "name_full": "Vision-Language Models (aggregated results across many VLM checkpoints evaluated in this paper)",
            "brief_description": "Aggregate summary of many VLMs (Bunny family, CogVLM, CogAgent, InstructBLIP family, LLaVA family, Qwen-VL, etc.) evaluated on the SpatialEval benchmark showing systematic trends about VQA, TQA, and VTQA modalities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various VLMs (Bunny, CogVLM, CogAgent, InstructBLIP, LLaVA, Qwen-VL, etc.)",
            "model_description": "Collection of multimodal models evaluated on SpatialEval; these use combinations of visual encoders (EVA, SigLIP, etc.) and language backbones (Phi2, Mistral, Vicuna, LLaMA derivatives) and were tested in VQA, VTQA and text-only modes.",
            "model_size": "Varied (3B -&gt; 34B across family members; specific checkpoints listed in paper's appendix).",
            "puzzle_name": "Spatial-Map, Maze-Nav, Spatial-Grid, Spatial-Real",
            "puzzle_type": "Map relation reasoning, maze navigation (path and turn counting), structured grid reasoning (counting and coordinates), and real-image spatial reasoning requiring caption comprehension and spatial inference.",
            "task_setup": "Each task presented in three modalities: TQA (text-only), VQA (vision-only), and VTQA (vision + long textual description). Multiple-choice (4 options) questions; prompt requests concise answer + detailed step-by-step explanation. Accuracy is primary metric; non-deterministic decodings averaged across three runs for open-source models.",
            "mechanisms_or_strategies": "VLMs rely on visual encoder to convert pixels into language-space features and then perform reasoning in the language model; authors probe the effect of textual redundancy (VTQA), removal or mismatching of images (No Image, Random Image, Noise Image), and decoding strategies. Chain-of-thought prompting used across experiments.",
            "performance_metrics": "Aggregate observations: many VLMs perform at or barely above random guessing in VQA across Spatial-Map, Maze-Nav, Spatial-Grid; adding textual descriptions (VTQA) typically improves accuracy vs VQA; VLMs often outperform their LLM backbones in text-only mode (TQA applied to VLM), except for some exceptions (e.g., CogVLM). Specific numeric examples: many open-source VLMs often close to random line (paper's Figure 5); VTQA &gt; VQA with modality gaps (e.g., spatial tasks show VTQA improves over VQA by average differences shown in Figure 11).",
            "evidence_of_spatial_reasoning": "Multiple controlled ablations: (1) No Image often improves or does not hurt performance compared to Original Image (forcing reliance on text), (2) Random Image sometimes improves or does not hurt (suggesting weak reliance on true visual cues), (3) VTQA improves over VQA, showing text redundancy helps. These results constitute evidence that many VLMs do not robustly extract spatial structure from images and instead exploit textual cues when available.",
            "comparisons": "Extensive comparisons done between matched language backbones (LLM vs VLM with same backbone) showing VLMs rarely improve via vision-only input; comparisons across VQA, VTQA, and TQA; proprietary models perform better overall but same modality trends hold.",
            "limitations_or_failure_cases": "Common failure modes across VLMs: poor utilization of visual spatial layout, visual inputs sometimes harm performance relative to text-only, sensitivity to prompt/decoding settings, and inconsistent multimodal fine-tuning benefits. Authors conclude existing architectures that translate vision into language-space are insufficient for human-like spatial reasoning.",
            "uuid": "e8562.5",
            "source_info": {
                "paper_title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating spatial understanding of large language models",
            "rating": 2
        },
        {
            "paper_title": "Geomverse: A systematic evaluation of large models for geometric reasoning",
            "rating": 2
        },
        {
            "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?",
            "rating": 2
        },
        {
            "paper_title": "Isobench: Benchmarking multimodal foundation models on isomorphic representations",
            "rating": 1
        },
        {
            "paper_title": "Stepgame: A new benchmark for robust multihop spatial reasoning in texts",
            "rating": 1
        }
    ],
    "cost": 0.016580249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models
4 Nov 2024</p>
<p>Jiayu Wang 
University of Wisconsin</p>
<p>Yifei Ming yifei.ming@salesforce.com 
Salesforce AI Research</p>
<p>Zhenmei Shi zhmeishi@cs.wisc.edu 
University of Wisconsin</p>
<p>Vibhav Vineet vibhav.vineet@microsoft.com 
Microsoft Research</p>
<p>Xin Wang milawang@cs.wisc.edu 
Microsoft Research</p>
<p>Yixuan Li 
University of Wisconsin</p>
<p>Neel Joshi 
Microsoft Research</p>
<ul>
<li>Madison 
Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models
4 Nov 20244CCDE80EEBDC99A51E1C398F22E6C0C7arXiv:2406.14852v2[cs.CV]
Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains.Despite this promise, spatial understanding and reasoning-a fundamental component of human cognition-remains under-explored.We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting.We conduct a comprehensive evaluation of competitive language and vision-language models.Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided.Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance.We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.Our code is available at https://github.com/jiayuww/SpatialEval.</li>
</ul>
<p>Introduction</p>
<p>The recent breakthroughs in foundation models have had a transformative effect on research and industry, and we have seen these models rapidly integrated into products and new businesses that are shaping people's lives for the better.This sea change was initially driven by large language models (LLMs), which have shown at times near unbelievable, human-level performance across a wide range of tasks.Over the past year, many of these models have been extended to handle images in addition to text, leading to a significant increase in vision-language models (VLMs), especially multimodal large language models (MLLMs), which demonstrate groundbreaking performance in image-related tasks as in the text domain.</p>
<p>However, the reality is not quite as rosy as advertised.While these models have been instrumental in advancing the state-of-the-art in complex reasoning tasks such as common sense reasoning, mathematical problem solving, and scientific question answering [17,30,41,72,75], they have not been as effective for a number of problem domains.In particular, as we will show in this paper, they have limited performance on tasks that require detailed visual understanding and reasoning of images.</p>
<p>Visual understanding and reasoning-an intrinsic part of human perception and cognitive ability-have been largely under-explored when it comes to LLMs and VLMs.In fact, it is often argued that the visual sense is the dominant sense in people, yet when it comes to current models, it seems quite secondary.Spatial reasoning, in particular, is fundamental to everyday human activities such as navigating environments, understanding maps, and manipulating objects.It encompasses skills that are crucial for both survival and higher-order cognition, including the ability to navigate through space, recognize patterns, and deduce relationships from spatial configurations.</p>
<p>In this paper, we propose SpatialEval, a novel benchmark containing four tasks (Spatial-Map, Maze-Nav, Spatial-Grid, and Spatial-Real, Section 2.1) to explore the performance of LLMs and VLMs on diverse aspects of spatial reasoning, including relationship, navigation, position understanding, and object counting.Humans excel at such tasks, making them essential capabilities for intelligent systems to emulate for safe and effective deployment in the real world.</p>
<p>Our dataset, however, is constructed with a key twist -each problem in our benchmark has an image and a text representation that is sufficient for answering each spatial understanding question.We denote the use of these sources as VQA, which is the standard task of visual-question answering that consists of a vision-only input and a question, TQA, text-only input and a question, and VTQA, a combination of the previous with vision and text input.</p>
<p>We conduct a systematic and comprehensive evaluation of a wide range of open-source and proprietary LLMs and VLMs.We perform in-depth analysis and unveil several novel and surprising results that challenge the current understanding of how these models process spatial information:</p>
<p>• Spatial reasoning remains challenging: VLMs frequently struggle with spatial reasoning tasks, with some competitive models performing worse than random guessing.</p>
<p>• Visual vs. textual inputs: Without detailed textual descriptions, multimodal models rarely surpass the performance of their LLM backbones when relying solely on visual inputs.This underscores the critical role of text in enhancing model performance on spatial reasoning.</p>
<p>• Reduced reliance on visual information: When both textual and visual inputs are provided, multimodal language models tend to rely less on the visual component if sufficient textual clues are available.</p>
<p>• Textual performance of VLMs: VLMs often outperform their LLM counterparts with text-only inputs, indicating that the language model backbones within VLMs benefit from multimodal training, despite a lack of similar benefits from the visual components.</p>
<p>We surmise the limitations in VLMs' spatial understanding stem from the overly simplistic handling of visual information in current architectures and training pipelines.We believe the contributions of this paper will drive changes in model design, accelerating improvements that could unlock more robust spatial reasoning capabilities, and help bridge the gap toward human-like intelligence.</p>
<p>2 Dataset and Task Construction</p>
<p>Dataset Setup</p>
<p>To evaluate the spatial reasoning abilities of LLMs and VLMs, we construct four diverse tasks including spatial relationships, navigation, position understanding, and counting.To systematically study the impact of modality, we design three types of input formats for each task: (1) TQA (Textonly): the input is purely textual and contains all necessary information for a person to answer the questions.( 2) VQA (Vision-only): the input consists solely of an image, which provides sufficient details for a person to easily answer, a format also referred to as Visual Question Answering (VQA) in the literature.( 3) VTQA (Vision-text): the input includes both an image and its textual representation with detailed descriptions, rendering the information in both modalities redundant.We evaluate LLMs using Text-only inputs and VLMs using Text-only, Vision-only, and Vision-text inputs on the same set of questions (    Vision-text.We evaluate language models (w.TQA input) and vision-language models (w.VQA and VTQA inputs) on the same set of questions.</p>
<p>Spatial-Map.Understanding the spatial relationships among objects on a map is a fundamental aspect of human cognitive abilities.To simulate this environment, we create a map-like dataset termed Spatial-Map with K objects, where K is configurable.Each object is associated with a unique location name, such as Unicorn Umbrellas and Gale Gifts.To study the impact of modality, the textual representation of each input consists of pairwise relations such as Brews Brothers Pub is to the Southeast of Whale's Watches.An example with K = 6 is shown in Figure 1, with Text-only, Vision-only, and Vision-text inputs.These questions include asking about the spatial relationships between two locations and the number of objects that meet specific spatial criteria.</p>
<p>Maze-Nav.Navigation through complex spaces is essential for intelligent systems.To evaluate such abilities, we have developed a maze-like dataset named Maze-Nav.Visually, each sample can be</p>
<p>Here is a Maze represented in ASCII code (LHS) where the symbols have the following meanings: # represents walls that are impassable barriers." " (space) represents the navigable path within the maze, ....A right turn is defined as a change in movement direction that is 90 degrees clockwise relative to the previous direction.A left turn is defined as a change in movement direction that is 90 degrees anticlockwise relative to the previous direction.represented as colored blocks where different colors signify distinct elements: a green block marks the starting point (S), a red block indicates the exit (E), black blocks represent impassable walls, white blocks denote navigable paths, and blue blocks trace the path from S to E. The objective is to navigate from S to E following the blue path, with movement permitted in the four cardinal directions (up, down, left, right).Alternatively, each input can be depicted in a textual format using ASCII code.An example is illustrated in Figure 2, featuring Text-only, Vision-only, and Vision-text inputs.We construct this task based on an open-sourced library [26].The questions asked include counting the number of turns from S to E and determining the spatial relationship between S and E. While such questions are easy for humans, we will show in Section 3 that they still pose significant challenges for modern multimodal language models.Spatial-Grid.To investigate spatial understanding within structured environments, we introduce a grid-like dataset named Spatial-Grid, contrasting with the Spatial-Map where objects are positioned arbitrarily.Visually, each input consists of a grid of cells, each containing an image (e.g., a rabbit).An example is illustrated in Figure 3.Alternatively, this grid can also be represented in a purely textual format; for instance, the first row might be described as: elephant | cat | giraffe | elephant | cat.The evaluations focus on tasks such as counting specific objects (e.g., rabbits) and identifying the object located at a specific coordinate in the grid (e.g., first row, second column).</p>
<p>Spatial-Real.To extend the evaluation of spatial reasoning beyond synthetic environments, we introduce Spatial-Real, a task built on the Densely Captioned Images (DCI) dataset [67], where each image has a detailed caption with more than 1,000 words on average.As DCI does not contain questions, we curate multiple-choice questions regarding spatial reasoning (object counting, relation, and position understanding) and annotate the answers.An example is shown in Figure 4. We provide detailed analysis on Spatial-Real in Appendix E.</p>
<p>Models</p>
<p>We consider a wide range of competitive open-source language models with different scales, including Phi2-2.7B[35], the LLaMA family (LLaMA-2-7B, LLaMA-2-13B, and LLaMA-3-8B) [65], Mistral-7B [27], the Vicuna family (Vicuna-7B-1.5and Vicuna-13B-1.5)[11], and Nous-Hermes-2-Yi-34B.</p>
<p>For multimodal language models, we consider the Bunny family (Bunny-Phi-2-SigLIP, Bunny-Phi-1.5-SigLIP,Bunny-Phi-2-EVA, and Bunny-Phi-1.5-EVA)[22], CogVLM [69], CogAgent [23], InstructBLIP family (InstructBLIP-Vicuna-7B and InstructBLIP-Vicuna-13B) [13], and LLaVA family (LLaVA-1.6-Mistral-7B,LLaVA-1.6-Vicuna-7B,LLaVA-1.6-Vicuna-13B, and LLaVA-1.6-34B) [38].We also evaluate the proprietary models: Open AI's GPT-4V, GPT-4o, GPT-4, Google Gemini Pro 1.0, and Anthropic Claude 3 Opus.</p>
<p>Evaluation.As each question contains four options, we use accuracy as the main evaluation metric.</p>
<p>The same user prompt is appended at the end of each question: First, provide a concise answer in one sentence.Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation.For each model, we adopt the default configurations and decoding strategies, e.g., argmax for deterministic decoding and top-p for non-deterministic decoding.For non-deterministic decoding, the results are averaged over three independent runs for open-source models.For proprietary models, due to their limited availability and increased compute time and cost, we only perform one run.We summarize the terminologies regarding input modalities for LLMs and VLMs in We describe the Text-only, Vision-only, and Vision-text input modalities based on how we feed the image information to the models.Vision-only input means the image is fed directly to the models without textual description, while all questions are presented in text.</p>
<p>Main Results and Analysis</p>
<p>Spatial reasoning remains surprisingly challenging.The evaluation results on open-source models on Spatial-Map, Maze-Nav, and Spatial-Grid are shown in Figure 5.For each task, the reported accuracy is averaged over all questions.For vision-language models, we choose the Vision-only input format, commonly used in Visual Question Answering (VQA).We use a dashed red line in each figure to indicate the expected accuracy if answering by random guessing.Our findings reveal several notable insights: (1) Vision-only inputs: Despite the simplicity of these tasks for humans, most competitive multimodal models perform at levels similar to or barely above random guessing.</p>
<p>Maze-Nav</p>
<p>Original Image No Image VLMs exhibit improved performance when visual input is absent.We conducted experiments by entirely removing the Original Image and relying solely on the textual description.The results are shown in Figure 8.For each task, we report the accuracy averaged across all questions.Remarkably, the absence of visual input leads to better performance across a range of VLM architectures.For instance, the performance of LLaVA-1.6-34B on the Spatial-Grid task improved by 20.1% when no image was presented compared to scenarios with the original image.This observation underscores that when textual information alone can address the questions, additional visual inputs do not necessarily enhance, and may even hinder the performance, a sharp contrast to human capabilities where visual cues significantly aid in understanding.The removal of visual input forces the models to utilize the textual information to solve spatial reasoning tasks.</p>
<p>Maze-Nav</p>
<p>Original Image Random Image Mismatched image-text does not necessarily hurt.To build on previous findings, we further investigate the effects of replacing the Original Image with a Random Image (illustrated in Figure 7).Unlike a noise image, a random image is task-related but may provide conflicting information compared to the textual description.Intuitively, one might expect that such random images would degrade VLM performance due to contradictory cues.However, as demonstrated in Figure 10, this expectation does not always hold true.For instance, Random Image in the Maze-Nav task leads to improved performance across various VLM architectures.This outcome implies that VLMs are not heavily reliant on visual information, particularly when adequate textual clues are provided.</p>
<p>Leveraging Redundancy in Multimodal Inputs</p>
<p>Multimodal language models offer considerable versatility in handling multimodal inputs.While the visual input alone often provides sufficient details for humans to address spatial reasoning tasks with ease, we propose that VLMs significantly benefit from the inclusion of textual descriptions alongside visual data, even if this introduces substantial redundancy.We verify this hypothesis by comparing VQA (Vision-only input) and VTQA (Vision-text input) across diverse VLM architectures.The results are shown in Figure 11, where each vertex on the spider plot represents the average accuracy of a (Vision-only, Vision-text) pair based on the same VLM.For Spatial-Map and Spatial-Grid, we can clearly see that having the additional textual input (VTQA) enhances the performance compared Text-only input with LLM vs. Text-only input with VLM.Given the demonstrated efficacy of textonly inputs, we conducted an ablation study to compare LLMs and VLMs using text-only inputs.We consider VLMs that are capable of processing text without accompanying visual data.The results are illustrated in Figure 12.Except for CogVLM, the majority of VLMs outperform their corresponding LLM backbones.This suggests that the language model backbones in VLMs demonstrate enhanced spatial reasoning abilities through multimodal learning.Conversely, the addition of visual information does not necessarily provide further benefits.</p>
<p>Proprietary vs. Open-Source Models</p>
<p>As many recent benchmarks have shown that proprietary models generally outperform open-source models, it is important to understand if our observed trends hold with proprietary models.The performance of several top proprietary models (GPT-4, GPT-4V, GPT-4o, Gemini Pro 1.0, and Claude 3 Opus) are shown in Figure 13.We have the following salient observations: (1) A significant performance gap exists between SoTA open-source models and proprietary models, as expected.Furthermore, with both Text-only and Vision-text formats, GPT-4V and GPT-4o significantly outperform random guessing across all tasks.For instance, in the Vision-text format, GPT-4o achieves</p>
<p>Comparison Results</p>
<p>Summary of Findings TQA (LLM) vs. VQA Figure 6 VQA rarely enhances the performance compared to TQA (LLM).</p>
<p>VTQA vs. TQA (VLM) Figure 8 VLMs exhibit improved performance in spatial reasoning tasks when the image input is absent.</p>
<p>VQA vs. VTQA Figure 11 Given the same image input, additional textual description enhances VLM's performance.TQA (VLM) vs. TQA (LLM) Figure 12 Multimodal fine-tuning enhances LLM's spatial reasoning ability.TQA (LLM) vs. VTQA Figure 16 No definitive winner.an accuracy of 0.989 on Spatial-Grid (Table 7).( 2) Yet, the trends we observed with open source models hold, VQA consistently under-performs compared to TQA and VTQA, for example, GPT-4V's performance improves by 25.6% on Spatial-Grid when switching from Vision-only to Vision-text input; and again no clear winner between TQA and VTQA (see Appendix D for further details), showing that proprietary models, even the new GPT-4o model, still do not appear to fully leverage visual inputs.We summarize the key findings of this work in Table 2.</p>
<p>Related Work</p>
<p>Large language models.Large language models (LLMs) have achieved outstanding performance across a diverse array of fields, including finance [34], bioinformatics [63], law [60], education [29], coding [24], and creative tasks [2,36].LLM architectures have gone through significant changes in recent years, with notable developments such as BERT [14], OPT [76], PaLM [12], Gemma family [62], Mistral family [27], GPT family [2,10], Claude family [5], and LLaMA family [3,64,66].These models have demonstrated emergent abilities and revolutionized numerous domains, supporting capabilities such as in-context learning [46,51,59], compositional reasoning [16,20,70], and taskspecific adaptation [57,58,71].However, visual understanding and reasoning-an intrinsic part of human cognitive ability-remains largely under-explored for LLMs.</p>
<p>Vision-language models and multi-modal language models.The success of LLMs has propelled the adoption of the Transformer architecture [68] within the computer vision community, such as ViT [15], Beit [9], CLIP [54], MAE [21], Swin [42,43], and DiT [53].Building on the capabilities of powerful LLMs, multi-modal language models (MLLMs) such as Flamingo [4], LLaMA-Adapter [19,74], LLava [37,39], stable-diffusion [55], BLIP [32,33], MiniGPT-4 [77], Qwen [7,8], Gemini [61], MM1 [45] have significantly expanded the range of problems that can be addressed with improved reliability [49].These models adeptly handle inputs from diverse modalities and have demonstrated remarkable performance on diverse tasks such as mathematical reasoning [44],</p>
<p>image-text retrieval [47,73], and visual reasoning [6,18,25,28,31,40,50].</p>
<p>Spatial understanding and reasoning.Spatial reasoning entails comprehending and manipulating spatial relationships, a task significantly more challenging than visual grounding [1,28,52].Although progress in natural language processing, evaluations, and benchmarks on LLMs such as GPT-4 [2] and Claude3 [5] have predominantly focused on textual or relational reasoning.This focus often overlooks the intricate nature of spatial reasoning tasks.Notably, recent studies [17,30,41,48,56,72,75] have conducted thorough evaluations across a diverse range of tasks.Yet, they demonstrate a scant exploration of spatial reasoning capabilities, highlighting a gap in assessing this complex cognitive skill in current benchmarks.In particular, Zhang et al. [75] suggest that MLLMs primarily leverage textual cues rather than visual diagrams to solve math problems while focusing only on math problems.Yamada et al. [72] design simple navigation tasks and finds LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains, while our benchmarks are more complicated than their navigation tasks based on simple geometry maps.Fu et al. [17] propose a benchmark on science and games with limited exploration related to spatial reasoning, while we are dedicated diverse spatial reasoning tasks with in-depth analysis.</p>
<p>Discussion and Conclusions</p>
<p>We explored the spatial understanding capabilities of VLMs and LLMs.Our experiments resulted in several surprising conclusions across SoTA open-source and proprietary models.(1) VLMs struggle with spatial reasoning tasks, (2) multimodal models rarely surpass LLMs when relying on visual inputs, (3) when both textual and visual inputs are provided, multimodal language models rely less on the visual inputs, and (4) VLMs often outperform their LLM counterparts with text-only inputs.This challenges the belief that current VLMs are highly performant at a wide range of vision-text tasks.However, with further thought, perhaps this is to be expected after all.The currently known architectures for VLMs attempt to "translate" the vision input into the language space and all reasoning is then performed in the language domain.It is logical that this automatic translation path is worse than a human provided translation to text, as in our text-only scenario.Thus our work shows the limits of the translation approach.Instead, to bridge the gap to human performance, future models require new architectures that treat vision input as a first-class source of information and reason in a joint vision-language space.It is our hope that our work informs development on this path.</p>
<p>C.2 Hyperparameters and Error Bars</p>
<p>The hyperparameters discussed in this paper pertain to the decoding strategies of each model.For deterministic decoding, we adhere to the default settings specified for each model.The models employing deterministic decoding (argmax) include Bunny-Phi-2-SigLIP, CogAgent, CogVLM, InstructBLIP-Vicuna-13B, and InstructBLIP-Vicuna-7B.For non-deterministic decoding, we utilize the default hyperparameters provided by Hugging Face (for instance, Top-P is set at 0.9 and the temperature at 0.2 for LLaVA-1.6).Error bars for the main results (Figure 5) are obtained with three independent runs.</p>
<p>C.3 Model Checkpoints</p>
<p>For most open-source models, we use the checkpoints provided by Hugging Face as shown in Table 3.</p>
<p>For Bunny variants (Bunny-Phi-1.5-EVA,Bunny-Phi-1.5-SigLIPand Bunny-Phi-2-EVA), we use merged weights following instructions in https://github.com/BAAI-DCAI/Bunny/.</p>
<p>Model Name</p>
<p>Table 3: Model checkpoints from Hugging Face.</p>
<p>C.4 Detailed Illustration of Datasets</p>
<p>In the main text, we abbreviated the textual descriptions in Figure 1, Figure 2 due to space constraints.In this section, we provide the full descriptions for each task to facilitate a better understanding of spatial reasoning tasks.The complete illustrations are displayed in Figure 14 for Spatial-Map and Figure 15 for Maze-Nav, respectively.Three questions (termed Q1 to Q3) are associated with each sample in tasks Spaitial-Map, Maze-Nav, and Spatial-Grid.</p>
<p>Here is a Maze represented in ASCII code (LHS), where the symbols have the following meanings:</p>
<p>-# represents walls that are impassable barriers.</p>
<p>-" " (space) represents the navigable path within the maze, but not necessarily the correct path to the exit.</p>
<p>-S denotes the starting point of the maze.</p>
<p>-E marks the endpoint or the exit the maze.</p>
<p>-X marks the specific route you should follow to navigate from the starting point 'S' to the 'E'.-The objective is to navigate from S to E by following the by Movement is allowed in any of the four cardinal (up, down, left, right) along spaces, but to solve the maze, follow designated by X.</p>
<p>-turn defined as a change in movement direction that is 90 degrees clockwise relative to the previous direction.</p>
<p>-A left turn is defined as a change in movement direction that is degrees anticlockwise relative to the previous direction.-" " (space) represents the navigable path within the maze, not necessarily the correct path to the exit.-X marks the specific route you should follow to navigate from the starting point 'S' to endpoint 'E'.-S denotes starting point of the -E marks the endpoint or the the maze.</p>
<p>-The objective is to navigate from S to E by following the path marked by X. Movement is allowed in any of the four cardinal (up, down, right) the spaces, but to solve the maze, follow the path designated by X.</p>
<p>Figure 1 :
1
Figure1: Illustration of the Spatial-Map task, which simulates a map with multiple locations.To investigate the impact of modality, we consider three input formats: Text-only, Vision-only, and Vision-text.We evaluate language models (w.TQA input) and vision-language models (w.VQA and VTQA inputs) on the same set of questions.</p>
<p><Img> 5 QFigure 2 :
52
Figure 2: Illustration of the Maze-Nav task, which evaluates the model's ability to navigate from the starting point (S) to the exit (E).</p>
<p>Figure 3 :
3
Figure 3: Illustration of the Spatial-Grid task, which evaluates the model's spatial reasoning ability in a rigid grid structure.</p>
<p>Figure 4 :
4
Figure 4: Illustration of the Spatial-Real task, which is built on real images with long captions, featuring detailed descriptions averaging over 1,000 words per image.</p>
<p>Figure 6 :
6
Figure 6: TQA (LLM) vs. VQA on spatial reasoning tasks.Each vertex on the spider plot represents the Avg Acc of a (VLM, LLM) pair with the same language backbone, i.e., LLM v.s.VLM further finetuned on that.VLMs are depicted in red, and LLMs in blue.We can see that VLMs rarely enhance the performance compared to their LLM counterparts.</p>
<p>4</p>
<p>Delving Into Spatial Reasoning for Vision-Language Models 4.1 Seeing Without Understanding: The Blindness of Multimodal Language Models Original Image Random Image Noise Image</p>
<p>Figure 7 :
7
Figure 7: Illustration of Random Image and Noise Image for the example in Figure 2. To better understand how VLMs process visual information, we conduct a series of controlled experiments in the VTQA (Vision-text input) setting.For each sample, we replace the original image input (that matches the textual description) with either: (1) No Image: only keep the textual input without the image input, (2) Noise Image: a Gaussian noise image irrelevant to the task, and (3) Random Image: a random image from the dataset that does not match the textual description, as shown in Figure 7.</p>
<p>Figure 8 :
8
Figure 8: VTQA vs. TQA (VLM) on spatial reasoning tasks.VLMs exhibit improved performance in spatial reasoning tasks when visual input is absent.</p>
<p>Figure 10 :
10
Figure 10: Original Image vs. Random Image in VTQA.On Maze-Nav, replacing the original image with a random image leads to performance improvement across diverse VLM architectures.</p>
<p>( c )Figure 11 :
c11
Figure 11: VQA vs. VTQA on spatial reasoning tasks.Each vertex on the spider plot represents the Avg Acc of a (Vision-only, Vision-text) pair with the same VLM model.We can see that having the additional textual input (VTQA) enhances the performance compared to only using images (VQA).</p>
<p>Figure 12 :
12
Figure 12: Comparison of Text-only input with LLM (TQA) vs. Text-only input with VLM (No Img).We consider VLMs that support text-only inputs.Each vertex on the spider plot represents the Avg Acc of a (LLM, VLM) pair with the same language model backbone.</p>
<p>Figure 13 :
13
Figure 13: Results with proprietary models.Similar trends are observed as with open-source models.</p>
<p><Img></p>
<p>The figure represents a Maze, where the colored blocks have the following meanings: -Black blocks represent walls that are impassable barriers.White blocks represent navigable paths within the maze, but not necessarily the path to the exit.-Green block denotes the starting point (S) of the maze.-Red block marks the endpoint or the exit (E) of the maze.-The objective is to navigate from S to E by following the path marked by Blue.Movement is allowed in any of the four cardinal directions (up, down, left, right) along the White blocks, but to solve the maze, follow the path designated by Blue blocks.-A right turn is defined as a change in movement direction is 90 degrees clockwise relative to the previous direction.-A turn is defined as a change in movement direction that 90 degrees anticlockwise relative to the previous direction.<Img> The represents a Maze, where the colored blocks have the meanings: -Black blocks represent walls that are impassable barriers.-White blocks represent navigable paths within the maze, but necessarily the correct path to the -Green block denotes the starting point (S) of the maze.-Red block the endpoint or exit (E) of the maze.-The objective is to navigate S to E by following the path marked by Blue.Movement is allowed in any of the four cardinal directions (up, down, left, right) along White blocks, but to solve the maze, follow the path designated by Blue blocks.The same Maze can be represented in ASCII code (LHS), where the symbols have the following meanings: -# represents walls that are impassable barriers.</p>
<p>QuestionsQ:Figure</p>
<p>Figure Illustration of the Maze-Nav task with complete textual descriptions in TQA, VQA, and VTQA.</p>
<p>Table</p>
<p>Table 1 :
1Model Input Modality TermDescriptionLLMText-onlyTQA (LLM)Text-only input that includes all necessary infor-mation to answer questions without visual context.VLMText-onlyTQA (VLM)Text-only input as in TQA (LLM) but applied to VLMs (e.g., the LLaVA family).VLMVision-onlyVQAInput only includes an image without correspond-ing textual description.VLMVision-textVTQAInput includes both an image and its textual de-scription.</p>
<p>Table 1 :
1
Terms regarding input modalities for LLMs and VLMs.</p>
<p>Table 2 :
2
Summary of main findings.</p>
<p>AcknowledgementThe authors would like to thank NeurIPS anonymous reviewers for their insightful feedback and helpful discussions.Yifei  Ming and Yixuan Li are funded in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 &amp; IIS-2331669, and Office of Naval Research under grant number N00014-23-1-2643.Link LLaMA-2-13B https://huggingface.co/meta-llama/Llama-2-13b-chat-hf LLaMA-2-7B https://huggingface.co/meta-llama/Llama-2-7b-chat-hf LLaMA-3-8B https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct Mistral-7B https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2Nous-Hermes-2-Yi-34B https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B Phi-2 https://huggingface.co/microsoft/phi-2 Vicuna-13B-1.5 https://huggingface.co/lmsys/vicuna-13b-v1.5 Vicuna-7B-1.5 https://huggingface.co/lmsys/vicuna-7b-v1.5 LLaVA-1.6-34Bhttps://huggingface.co/liuhaotian/llava-v1.6-34b LLaVA-1.6-Mistral-7Bhttps://huggingface.co/liuhaotian/llava-v1.6-mistral-7b LLaVA-1.6-Vicuna-13Bhttps://huggingface.co/liuhaotian/llava-v1.6-vicuna-13b LLaVA-1.6-Vicuna-7Bhttps://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b Bunny-Phi-2-SigLIP https://huggingface.co/BAAI/Bunny-v1_0-3B Qwen-VL https://huggingface.co/Qwen/Qwen-VL-Chat CogAgent https://huggingface.co/THUDM/cogagent-vqa-hf CogVLM https://huggingface.co/THUDM/cogvlm-chat-hf InstructBLIP-Vicuna-13B https://huggingface.co/Salesforce/instructblip-vicuna-13b InstructBLIP-Vicuna-7B https://huggingface.co/Salesforce/instructblip-vicuna-7b* The work was completed in part during Yifei Ming's internship at Microsoft Research, as well as PhD thesis research at UW-Madison.38th Conference on Neural Information Processing Systems (NeurIPS 2024).AppendixA How Does SpatialEval Expand Beyond Traditional VQA?We highlight several key rationales for SpatialEval in comparison to existing VQA benchmarks:• Task scope and focus: Existing benchmarks such as Visual Genome[31], GQA[25], and BLINK[18]focus only on VQA, where the image is required but the text description is often omitted or optional.In contrast, SpatialEval further explores spatial reasoning across different settings: TQA (LLM), TQA (VLM), and VTQA, where images or texts can be optional, thereby broadening the scope of tasks.• Evaluation scheme: we primarily focus on generative LLMs and VLMs which can "elaborate on the reasoning behind your answer in a detailed, step-by-step explanation" (Section 2.2).The task in prior VQA benchmarks is often treated as discriminative with no explicit reasoning.Therefore, it remains unknown if previous observations can be naturally transferred to foundation models pre-trained on web-scale data.• The Textual Representation of Images: The textual descriptions in prior VQA benchmarks are often brief or directly imply the answers.While these datasets are valuable, they do not consistently offer the level of complexity we require, such as numerous objects containing dense visual information along with detailed natural language captions that fully convey the image content.In SpatialEval, we provide long and dense captions for each image.As a result, answers cannot be easily inferred.We also aim to isolate object detection capability from spatial reasoning ability by simplifying objects to symbols (e.g., Spatial-Map).• IQ test for VLMs and LLMs: Tasks in SpatialEval are designed to serve as cognitive tests that evaluate basic capabilities of multi-modal foundation models.While three tasks feature synthetic visual content, humans can solve them with near-perfect accuracy.This indicates that the tasks are within the realm of human cognitive capabilities.B Limitations and Societal ImpactLimitations.In this work, we introduce four novel tasks and conduct a comprehensive evaluation of diverse large language models (LLMs) and vision-language models (VLMs), presenting a controlled and thorough evaluation of spatial reasoning capabilities.While our analysis is detailed and extensive, it remains primarily empirical.We believe that embarking on a formal theoretical study, despite its challenges, would significantly enrich our understanding of pre-trained multimodal language models.Moreover, our focus has been on in-depth analysis rather than on developing new training strategies or adaptation algorithms to enhance spatial reasoning.Recognizing these points, we identify them as valuable directions for future research.Societal impact.Regarding the positive societal impact, our evaluations and observations could catalyze the development of new algorithms that enhance the spatial reasoning abilities of LLMs and VLMs.Improved understanding of these models has the potential to significantly benefit sectors requiring robust spatial understanding and navigation.This could lead to more reliable and efficient systems that enhance safety and user experience.As for potential negative societal impacts, our work primarily involves empirical evaluations using synthetic datasets designed to probe spatial reasoning.Therefore, we do not anticipate any direct negative societal impacts arising from our current research.C Experimental DetailsC.   Impact of prompting techniques.In line with our to sampling strategies, our primary goal in prompting techniques is to report the best model performance given the same question.The prompt technique we use, which asks for step-by-step explanation is the most effective query among others in our initial studies.As a concrete example, we compare the original prompting strategy, "First, provide a concise answer in one sentence.Then, elaborate on the reasoning behind your answer in a detailed, step-by-step explanation" (step-by-step explanation), with a simpler prompt, Answer: (completion).Results in Table4show that the simpler completion prompt consistently underperforms compared to the step-by-step explanation prompt.Impact of temperature for decoding strategies.We conducted an ablation study to examine how different temperatures affect the model performance.A higher temperature allows for more diversity in model responses.Most models consistently underperform when the temperature is set to 1.0 compared to 0.2 (our default) as shown in Table5.Input Modality ModelAvg Acc (temperature=1) Avg Acc (temperature=0.E Results on Spatial-Real taskTable6presents the performance on the Spatial-Real task.The same trends observed in synthetic tasks persist with real images (see VQA vs. VTQA, TQA (LLM) vs. VTQA, TQA (LLM) vs. VQA in Table2).Notably, compared to synthetic tasks (Figure5and Figure11), the overall accuracy on Spatial-Real is increased across all three input types (TQA, VQA, and VTQA).However, the modality gap (accuracy difference between VTQA and VQA) widens significantly, from 7.0% on synthetic tasks to 30.0% on Spatial-Real.This that the performance disparity is more pronounced on natural images.F Detailed Experimental ResultsResults for proprietary models are summarized in Table7.In Section 3 and Section 4, to clearly illustrate the impact of modality, we present the results averaged over all questions in Figure6for VQA vs. TQA and Figure11for VQA vs. VTQA.This section provides a comprehensive breakdown of results for individual questions.Detailed comparative results for Spatial-Map, Maze-Nav, and Spatial-Grid are shown in Table8, Table9, and Table10, respectively.We compare LLM and VLM with the same language model backbone.For the VLM assessments, we consider inputs in both vision-only (VQA) and vision-text (VTQA) formats.
LLaMA-2-13B LLaMA-2-7B LLaMA-3-8B Mistral-7B. </p>
<p>. Nous-Hermes, -2-Yi-34B Phi-2 Vicuna-13B-1.5 Vicuna-7B-1.5 LLaVA-1.6-34B</p>
<p>InstructBLIP-Vicuna-13B InstructBLIP-Vicuna-7B Spatial-Map. -7b Llava-1.6-Mistral, -13b Llava-1.6-Vicuna, -7b Llava-1.6-Vicuna, -Vl Bunny-Phi-2-Siglip Qwen, Bunny-Phi-2-Eva Bunny-Phi-1.5-Eva Bunny-Phi-1.5-Siglip, Cogvlm Cogagent, </p>
<p>Model Vision-Language Model Figure 5: Performance overview on spatial reasoning tasks. We report the accuracy averaged over all questions. We consider the VQA (Vision-only) format for vision-language models. The dashed red line denotes the expected accuracy for random guessing. For Spatial-Map and Maze-Nav tasks, only a few models outperform random guessing by a notable margin. </p>
<p>Despite these successes, the performance of these models still lags significantly behind human levels. These results underscore the need for further development of techniques tailored to spatial understanding and reasoning. The impact of input modality. To investigate the impact of modality, we compare the performance of a large language model (LLM) and a vision-language model (VLM) with the same language backbone. We consider the VQA (Vision-only) format for vision-language models. The results are shown in Figure 6. Each vertex on the spider plot represents the average accuracy of a (VLM, LLM) pair. We observe that on Spatial-Map and Spatial-Grid, the majority of VLMs yield worse performance compared to their LLM counterpart, despite having an additional visual encoder. For example, on Spatial-Grid. where Llama-3 achieves an accuracy of 71.9%, followed by Mistral-7B-Instruct at 62.1%, both notably surpassing random guessing. Mixtral-7B achieves an average accuracy of 62.1%, while LLaVA-v1.6-Mistral-7B only yields an accuracy of 47.1% (15% ↓). Detailed results can be seen in Appendix F. (c) TQA vs. VQA on Spatial-Grid</p>
<p>VQA on Spatial-Map (b) TQA vs. VQA on Maze-Nav LLaVA-1.6-Mistral-7B. TQA vs. </p>
<p>LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-13B. </p>
<p>CogAgent CogVLM LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-13B. </p>
<p>CogAgent CogVLM InstructBLIP-Vicuna-13B InstructBLIP-Vicuna-7B InstructBLIP-Vicuna-7B. </p>
<p>LLaVA-1.6-Mistral-7B. </p>
<p>LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM InstructBLIP-Vicuna-13B LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM InstructBLIP-Vicuna-13B LLaVA-1.6-Mistral-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-Vicuna-7B. </p>
<p>LLaVA-1.6-34B InstructBLIP-Vicuna-7B CogAgent CogVLM InstructBLIP-Vicuna-13B. </p>
<p>No Img on Spatial-Grid (a) TQA vs. No Img on Spatial-Map (b) TQA vs. No Img on Maze-Nav LLaVA-1.6-34B LLaVA-1.6-Vicuna-7B. TQA vs. </p>
<p>CogVLM LLaVA-1.6-Vicuna-7B Bunny-Phi-2-SigLIP LLaVA-1.6-34B LLaVA-1.6-Vicuna-7B. </p>
<p>Can language models encode perceptual structure without grounding? a case study in color. Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Søgaard, arXiv:2109.061292021arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. A I Meta, 2024</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023arXiv preprint</p>
<p>Beit: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Songhao Piao, Furu Wei, arXiv:2106.082542021arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, Steven Hoi, Advances in Neural Information Processing Systems. 362024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Ronan Bhagavatula, Le Bras, Advances in Neural Information Processing Systems. 202436</p>
<p>Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger, arXiv:2404.01266Isobench: Benchmarking multimodal foundation models on isomorphic representations. 2024arXiv preprint</p>
<p>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, Ranjay Krishna, arXiv:2404.12390Blink: Multimodal large language models can see but not perceive. 2024arXiv preprint</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010Llama-adapter v2: Parameter-efficient visual instruction model. 2023arXiv preprint</p>
<p>Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou, arXiv:2402.09469Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. 2024arXiv preprint</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Efficient multimodal learning from data-centric perspective. Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao, arXiv:2402.115302024arXiv preprint</p>
<p>Cogagent: A visual language model for gui agents. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2024</p>
<p>Large language models for software engineering: A systematic literature review. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, Haoyu Wang, 2024</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Rusheb Michael Igorevich Ivanitskiy, Alex F Shah, Tilman Spies, Dan Räuker, Can Valentine, Lucia Rager, Chris Quirke, Guillaume Mathwin, Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. A configurable library for generating and manipulating maze datasets. 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Learning and individual differences. 1031022742023</p>
<p>Geomverse: A systematic evaluation of large models for geometric reasoning. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, Radu Soricut, arXiv:2312.122412023arXiv preprint</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Large language models in finance: A survey. Yinheng Li, Shaofei Wang, Han Ding, Hang Chen, Proceedings of the Fourth ACM International Conference on AI in Finance. the Fourth ACM International Conference on AI in Finance2023</p>
<p>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, arXiv:2403.071832024arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.03744Improved baselines with visual instruction tuning. 2023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. January 2024</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng, arXiv:2402.176442024arXiv preprint</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, arXiv:2307.06281Is your multi-modal model an all-around player?. 2023arXiv preprint</p>
<p>Swin transformer v2: Scaling up capacity and resolution. Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv:2310.02255Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>Brandon Mckinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, arXiv:2403.09611Methods, analysis &amp; insights from multimodal llm pre-training. 2024arXiv preprint</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Understanding retrieval-augmented task adaptation for visionlanguage models. Yifei Ming, Yixuan Li, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningJul 2024</p>
<p>Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjmashidi, Spartqa, arXiv:2104.05832A textual question answering benchmark for spatial reasoning. 2021arXiv preprint</p>
<p>Generalized out-of-distribution detection and beyond in vision language model era: A survey. Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa, arXiv:2407.217942024arXiv preprint</p>
<p>Unsolvable problem detection: Evaluating trustworthiness of vision language models. Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa, arXiv:2403.203312024arXiv preprint</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, arXiv:2209.11895-context learning and induction heads. 2022arXiv preprint</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International conference on learning representations. 2021</p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Highresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Stepgame: A new benchmark for robust multihop spatial reasoning in texts. Zhengxiang Shi, Qiang Zhang, Aldo Lipani, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2022</p>
<p>The trade-off between universality and label efficiency of representations from contrastive learning. Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, Somesh Jha, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Domain generalization via nuclear norm regularization. Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang, Conference on Parsimony and Learning (Proceedings Track. 2023</p>
<p>Why larger language models do in-context learning differently. Zhenmei Shi, Junyi Wei, Zhuoyan Xu, Yingyu Liang, R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models. 2023</p>
<p>A short survey of viewing large language models in legal aspect. Zhongxiang Sun, arXiv:2303.091362023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>A picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. 2023</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Zhuoyan Xu, Zhenmei Shi, Yingyu Liang, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024</p>
<p>Towards few-shot adaptation of foundation models via multitask finetuning. Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Evaluating spatial understanding of large language models. Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo Kasai, Ilker Yildirim, Transactions on Machine Learning Research. 2024</p>
<p>Context-aware attention network for image-text retrieval. Qi Zhang, Zhen Lei, Zhaoxiang Zhang, Stan Z Li, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023arXiv preprint</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, arXiv:2403.146242024arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
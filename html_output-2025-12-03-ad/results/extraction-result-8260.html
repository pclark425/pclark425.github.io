<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8260 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8260</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8260</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-269293342</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.13985v2.pdf" target="_blank">Information Re-Organization Improves Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Improving the reasoning capabilities of large language models (LLMs) has attracted considerable interest. Recent approaches primarily focus on improving the reasoning process to yield a more precise final answer. However, in scenarios involving contextually aware reasoning, these methods neglect the importance of first identifying logical relationships from the context before proceeding with the reasoning. This oversight could lead to a superficial understanding and interaction with the context, potentially undermining the quality and reliability of the reasoning outcomes. In this paper, we propose an information re-organization (InfoRE) method before proceeding with the reasoning to enhance the reasoning ability of LLMs. Our re-organization method involves initially extracting logical relationships from the contextual content, such as documents or paragraphs, and subsequently pruning redundant content to minimize noise. Then, we utilize the re-organized information in the reasoning process. This enables LLMs to deeply understand the contextual content by clearly perceiving these logical relationships, while also ensuring high-quality responses by eliminating potential noise. To demonstrate the effectiveness of our approach in improving the reasoning ability, we conduct experiments using Llama2-70B, GPT-3.5, and GPT-4 on various contextually aware multi-hop reasoning tasks. Using only a zero-shot setting, our method achieves an average absolute improvement of 4% across all tasks, highlighting its potential to improve the reasoning performance of LLMs. Our source code is available at https://github.com/hustcxx/InfoRE.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8260.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8260.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation large language model (70 billion parameters) used in this paper for extraction and reasoning experiments with top_p=1.0 and temperature=0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model with ~70B parameters (Llama2 family). Used both for information extraction (MindMap generation) and final reasoning in zero-shot configuration (top_p=1.0, temperature=0.0).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Standard (direct reasoning with original textual context)', 'Chain-of-Thought (CoT) prompting', 'InfoRE (Information Re-Organization: extraction -> MindMap + pruning)', 'InfoRE + CoT (combination of re-organized context with CoT prompt)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Standard: prompt to answer using original documents. CoT: append 'Let's think step by step.' to elicit intermediate chain-of-thought. InfoRE: first extract logical relations and multi-hop connections from context into a MindMap via an LLM, then prune irrelevant relations with a BERT-based policy trained with PPO (reward = F1), and finally reason using the pruned MindMap as context. InfoRE+CoT: use the re-organized context and also prompt 'Let's think step by step.'</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Zero-shot comparisons between Standard, CoT, InfoRE, and InfoRE+CoT; ablation removing extraction or pruning and replacing RL-based pruning with similarity-based pruning to compare pruning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Context-aware multi-hop reasoning tasks: claim verification (HOVER split into 2-hop/3-hop/4-hop, FEVEROUS, SCIFACT), question answering and reading comprehension (2WikiMultiHopQA, StrategyQA, MuSiQue, HotpotQA, WIKIHOP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Claim verification (HOVER 2-hop/3-hop/4-hop): Standard = 49.41 / 48.35 / 47.82; InfoRE = 52.83 / 51.42 / 50.04 (absolute increases +3.42 / +3.07 / +2.22); CoT = 50.02 / 48.76 / 48.01; InfoRE+CoT = 53.20 / 51.70 / 50.15. FEVEROUS: Standard 63.39 -> InfoRE 67.84 (+4.45). SCIFACT: Standard 60.70 -> InfoRE 63.81 (+3.11). Overall paper reports average absolute improvement across tasks ~4% (zero-shot) when using InfoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>InfoRE reduces contextual misunderstanding errors (primary error class for baseline) and corrected ~14% of baseline errors (most corrections were contextual misunderstandings). Re-organized MindMap improves depth and clarity of context relative to original text (quantitative rankings). Replacing RL pruning with similarity-based pruning reduces performance (RL pruning preferred). Combining InfoRE with CoT yields further improvements over either alone, indicating complementarity.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>For Llama2-70B, InfoRE (context re-organization via extraction + RL-based pruning) consistently outperforms Standard and CoT in zero-shot multi-hop context-aware tasks; extraction contributes the largest single benefit, pruning also helps, and RL-based pruning outperforms a similarity-based alternative. InfoRE and CoT are complementary (InfoRE+CoT best).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information Re-Organization Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8260.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8260.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used instruction-tuned large language model (text-davinci-003) used here for extraction and reasoning under zero-shot protocols (top_p=1.0, temperature=0.0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-family model (text-davinci-003) used for MindMap extraction and final reasoning; configured with top_p=1.0 and temperature=0.0 for deterministic zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Standard (direct reasoning with original context)', 'Chain-of-Thought (CoT) prompting', 'InfoRE (extraction -> MindMap + RL pruning)', 'InfoRE + CoT']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting setups as for other models: Standard answers from documents; CoT by appending 'Let's think step by step.'; InfoRE uses the model (or another LLM) to output a MindMap JSON of logical relations/evidence, then a BERT/PPO policy prunes irrelevant relations, finally the model reasons using the pruned MindMap; InfoRE+CoT combines re-organized context with CoT prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Zero-shot head-to-head experiments comparing Standard, CoT, InfoRE, and InfoRE+CoT across multiple datasets; ablation experiments on InfoRE with and without extraction/pruning and with similarity-based pruning vs RL-based pruning (on 2WikiMultiHopQA).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same set: HOVER (2/3/4-hop), FEVEROUS, SCIFACT, 2WikiMultiHopQA, StrategyQA, MuSiQue, HotpotQA, WIKIHOP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Claim verification (HOVER 2-hop/3-hop/4-hop): Standard = 64.74 / 63.04 / 61.54; InfoRE = 68.21 / 66.45 / 64.91 (improvements +3.47 / +3.41 / +3.37). CoT = 66.70 / 64.52 / 62.69; InfoRE+CoT = 69.02 / 67.53 / 65.66. FEVEROUS: Standard 87.67 -> InfoRE 91.31 (+3.64). SCIFACT: Standard 77.42 -> InfoRE 81.54 (+4.12). Ablation on 2WikiMultiHopQA: full InfoRE = 64.58 F1; w/o extraction = 61.64 (-2.94); w/o pruning = 63.05 (-1.53); similarity-based pruning = 63.32 (-1.26). Cross-validation: using GPT-4 to produce re-organized info and reasoning with GPT-3.5 (InfoRE*) further improved FEVEROUS to 92.50 and 2WikiMultiHopQA to 66.61 (vs InfoRE with same-model reorg 91.31 / 64.58).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Extraction is the more critical component than pruning; RL-based pruning yields better results than a similarity-based heuristic. Higher-quality re-organization (produced by GPT-4) can boost performance of a weaker reasoner (GPT-3.5). InfoRE mainly mitigates contextual misunderstanding errors; combining InfoRE with CoT yields additive gains.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>For GPT-3.5, InfoRE yields consistent absolute gains (≈3–4% F1) over Standard and modestly outperforms CoT; removal of extraction yields the largest drop, confirming that exposing logical relationships is pivotal. RL-based pruning outperforms similarity pruning. Combining InfoRE with CoT gives additional improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information Re-Organization Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8260.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8260.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model from OpenAI used in the paper for both extraction (MindMap generation) and reasoning; configured deterministically for experiments (top_p=1.0, temperature=0.0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced multimodal-capable LLM (GPT-4 family); in experiments, GPT-4 and GPT-4-32k (for qualitative ranking) are used for MindMap extraction, ranking re-organized information, and final reasoning in zero-shot mode.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Standard (direct reasoning)', 'Chain-of-Thought (CoT) prompting', 'InfoRE (extraction -> MindMap + RL pruning)', 'InfoRE + CoT']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Standard: answer from provided documents. CoT: append 'Let's think step by step.' InfoRE: GPT-4 (or other LLM) used to produce MindMap JSON (explicit logical relations and multi-hop links) given claim and evidence; BERT-based policy (PPO) prunes relations; GPT-4 then reasons from re-organized context. InfoRE+CoT combines the re-organized context with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons (zero-shot) between Standard, CoT, InfoRE, and InfoRE+CoT across multiple datasets; cross-validation experiments where re-organization is performed by GPT-3.5 or GPT-4 while reasoning is done by the other model to study quality effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same multi-hop benchmarks: HOVER (2/3/4-hop), FEVEROUS, SCIFACT, 2WikiMultiHopQA, StrategyQA, HotpotQA, WIKIHOP, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Claim verification (HOVER 2-hop/3-hop/4-hop): Standard = 72.40 / 71.02 / 70.06; InfoRE = 75.87 / 74.06 / 73.08 (+3.47 / +3.04 / +3.02); CoT = 73.82 / 72.07 / 70.68; InfoRE+CoT = 76.69 / 75.16 / 73.62. FEVEROUS: Standard 92.33 -> InfoRE 95.62 (+3.29). SCIFACT: Standard 91.40 -> InfoRE 93.67 (+2.27). Question answering / reading comprehension (InfoRE on GPT-4): 2WikiMultiHopQA = 76.52 F1, StrategyQA = 71.20 F1, HotpotQA = 83.22 F1 (paper highlights GPT-4 achieves highest F1 with InfoRE). Cross-validation: using GPT-3.5 re-org with GPT-4 reasoning (InfoRE†) yields FEVEROUS 94.67 and 2WikiMultiHopQA 75.07 (slightly lower than same-model reorg but still improved vs baseline). Overall the paper reports average absolute improvement of ~4% across tasks with InfoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 produces higher-quality MindMaps (ranked best in depth and clarity). Using higher-quality re-organization (GPT-4 produced) benefits weaker reasoners more (e.g., GPT-3.5 benefits when given GPT-4 re-org). InfoRE mainly reduces contextual misunderstanding errors; combining InfoRE with CoT leads to near-best or best performance on many datasets. CoT alone gives only marginal gains in some hard settings (e.g., CoT gave +0.62% on HOVER 4-hop vs Standard, whereas InfoRE gave +3.02%).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>GPT-4 benefits from InfoRE with consistent gains (≈3% absolute on many claim-verification hops and large gains on FEVEROUS/SCIFACT), and InfoRE complements CoT (InfoRE+CoT often best). The quality of re-organized information affects downstream reasoning; extraction of logical relations (MindMap) is the dominant contributor and RL-based pruning is superior to naive similarity pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information Re-Organization Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of Thoughts: Solving Elaborate Problems with Large Language Models <em>(Rating: 2)</em></li>
                <li>Take a step back: Evoking reasoning via abstraction in large language models <em>(Rating: 1)</em></li>
                <li>BASS: Boosting abstractive summarization with unified semantic graph <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8260",
    "paper_id": "paper-269293342",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Llama2-70B",
            "name_full": "Llama 2 (70B)",
            "brief_description": "An open foundation large language model (70 billion parameters) used in this paper for extraction and reasoning experiments with top_p=1.0 and temperature=0.0.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2-70B",
            "model_description": "Transformer-based large language model with ~70B parameters (Llama2 family). Used both for information extraction (MindMap generation) and final reasoning in zero-shot configuration (top_p=1.0, temperature=0.0).",
            "reasoning_methods": [
                "Standard (direct reasoning with original textual context)",
                "Chain-of-Thought (CoT) prompting",
                "InfoRE (Information Re-Organization: extraction -&gt; MindMap + pruning)",
                "InfoRE + CoT (combination of re-organized context with CoT prompt)"
            ],
            "reasoning_methods_description": "Standard: prompt to answer using original documents. CoT: append 'Let's think step by step.' to elicit intermediate chain-of-thought. InfoRE: first extract logical relations and multi-hop connections from context into a MindMap via an LLM, then prune irrelevant relations with a BERT-based policy trained with PPO (reward = F1), and finally reason using the pruned MindMap as context. InfoRE+CoT: use the re-organized context and also prompt 'Let's think step by step.'",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Zero-shot comparisons between Standard, CoT, InfoRE, and InfoRE+CoT; ablation removing extraction or pruning and replacing RL-based pruning with similarity-based pruning to compare pruning strategies.",
            "task_or_benchmark": "Context-aware multi-hop reasoning tasks: claim verification (HOVER split into 2-hop/3-hop/4-hop, FEVEROUS, SCIFACT), question answering and reading comprehension (2WikiMultiHopQA, StrategyQA, MuSiQue, HotpotQA, WIKIHOP).",
            "performance_results": "Claim verification (HOVER 2-hop/3-hop/4-hop): Standard = 49.41 / 48.35 / 47.82; InfoRE = 52.83 / 51.42 / 50.04 (absolute increases +3.42 / +3.07 / +2.22); CoT = 50.02 / 48.76 / 48.01; InfoRE+CoT = 53.20 / 51.70 / 50.15. FEVEROUS: Standard 63.39 -&gt; InfoRE 67.84 (+4.45). SCIFACT: Standard 60.70 -&gt; InfoRE 63.81 (+3.11). Overall paper reports average absolute improvement across tasks ~4% (zero-shot) when using InfoRE.",
            "qualitative_findings": "InfoRE reduces contextual misunderstanding errors (primary error class for baseline) and corrected ~14% of baseline errors (most corrections were contextual misunderstandings). Re-organized MindMap improves depth and clarity of context relative to original text (quantitative rankings). Replacing RL pruning with similarity-based pruning reduces performance (RL pruning preferred). Combining InfoRE with CoT yields further improvements over either alone, indicating complementarity.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "For Llama2-70B, InfoRE (context re-organization via extraction + RL-based pruning) consistently outperforms Standard and CoT in zero-shot multi-hop context-aware tasks; extraction contributes the largest single benefit, pruning also helps, and RL-based pruning outperforms a similarity-based alternative. InfoRE and CoT are complementary (InfoRE+CoT best).",
            "uuid": "e8260.0",
            "source_info": {
                "paper_title": "Information Re-Organization Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (text-davinci-003)",
            "brief_description": "A widely-used instruction-tuned large language model (text-davinci-003) used here for extraction and reasoning under zero-shot protocols (top_p=1.0, temperature=0.0).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "Instruction-tuned GPT-family model (text-davinci-003) used for MindMap extraction and final reasoning; configured with top_p=1.0 and temperature=0.0 for deterministic zero-shot evaluation.",
            "reasoning_methods": [
                "Standard (direct reasoning with original context)",
                "Chain-of-Thought (CoT) prompting",
                "InfoRE (extraction -&gt; MindMap + RL pruning)",
                "InfoRE + CoT"
            ],
            "reasoning_methods_description": "Same prompting setups as for other models: Standard answers from documents; CoT by appending 'Let's think step by step.'; InfoRE uses the model (or another LLM) to output a MindMap JSON of logical relations/evidence, then a BERT/PPO policy prunes irrelevant relations, finally the model reasons using the pruned MindMap; InfoRE+CoT combines re-organized context with CoT prompt.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Zero-shot head-to-head experiments comparing Standard, CoT, InfoRE, and InfoRE+CoT across multiple datasets; ablation experiments on InfoRE with and without extraction/pruning and with similarity-based pruning vs RL-based pruning (on 2WikiMultiHopQA).",
            "task_or_benchmark": "Same set: HOVER (2/3/4-hop), FEVEROUS, SCIFACT, 2WikiMultiHopQA, StrategyQA, MuSiQue, HotpotQA, WIKIHOP.",
            "performance_results": "Claim verification (HOVER 2-hop/3-hop/4-hop): Standard = 64.74 / 63.04 / 61.54; InfoRE = 68.21 / 66.45 / 64.91 (improvements +3.47 / +3.41 / +3.37). CoT = 66.70 / 64.52 / 62.69; InfoRE+CoT = 69.02 / 67.53 / 65.66. FEVEROUS: Standard 87.67 -&gt; InfoRE 91.31 (+3.64). SCIFACT: Standard 77.42 -&gt; InfoRE 81.54 (+4.12). Ablation on 2WikiMultiHopQA: full InfoRE = 64.58 F1; w/o extraction = 61.64 (-2.94); w/o pruning = 63.05 (-1.53); similarity-based pruning = 63.32 (-1.26). Cross-validation: using GPT-4 to produce re-organized info and reasoning with GPT-3.5 (InfoRE*) further improved FEVEROUS to 92.50 and 2WikiMultiHopQA to 66.61 (vs InfoRE with same-model reorg 91.31 / 64.58).",
            "qualitative_findings": "Extraction is the more critical component than pruning; RL-based pruning yields better results than a similarity-based heuristic. Higher-quality re-organization (produced by GPT-4) can boost performance of a weaker reasoner (GPT-3.5). InfoRE mainly mitigates contextual misunderstanding errors; combining InfoRE with CoT yields additive gains.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "For GPT-3.5, InfoRE yields consistent absolute gains (≈3–4% F1) over Standard and modestly outperforms CoT; removal of extraction yields the largest drop, confirming that exposing logical relationships is pivotal. RL-based pruning outperforms similarity pruning. Combining InfoRE with CoT gives additional improvement.",
            "uuid": "e8260.1",
            "source_info": {
                "paper_title": "Information Re-Organization Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A state-of-the-art large language model from OpenAI used in the paper for both extraction (MindMap generation) and reasoning; configured deterministically for experiments (top_p=1.0, temperature=0.0).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Advanced multimodal-capable LLM (GPT-4 family); in experiments, GPT-4 and GPT-4-32k (for qualitative ranking) are used for MindMap extraction, ranking re-organized information, and final reasoning in zero-shot mode.",
            "reasoning_methods": [
                "Standard (direct reasoning)",
                "Chain-of-Thought (CoT) prompting",
                "InfoRE (extraction -&gt; MindMap + RL pruning)",
                "InfoRE + CoT"
            ],
            "reasoning_methods_description": "Standard: answer from provided documents. CoT: append 'Let's think step by step.' InfoRE: GPT-4 (or other LLM) used to produce MindMap JSON (explicit logical relations and multi-hop links) given claim and evidence; BERT-based policy (PPO) prunes relations; GPT-4 then reasons from re-organized context. InfoRE+CoT combines the re-organized context with CoT prompting.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparisons (zero-shot) between Standard, CoT, InfoRE, and InfoRE+CoT across multiple datasets; cross-validation experiments where re-organization is performed by GPT-3.5 or GPT-4 while reasoning is done by the other model to study quality effects.",
            "task_or_benchmark": "Same multi-hop benchmarks: HOVER (2/3/4-hop), FEVEROUS, SCIFACT, 2WikiMultiHopQA, StrategyQA, HotpotQA, WIKIHOP, etc.",
            "performance_results": "Claim verification (HOVER 2-hop/3-hop/4-hop): Standard = 72.40 / 71.02 / 70.06; InfoRE = 75.87 / 74.06 / 73.08 (+3.47 / +3.04 / +3.02); CoT = 73.82 / 72.07 / 70.68; InfoRE+CoT = 76.69 / 75.16 / 73.62. FEVEROUS: Standard 92.33 -&gt; InfoRE 95.62 (+3.29). SCIFACT: Standard 91.40 -&gt; InfoRE 93.67 (+2.27). Question answering / reading comprehension (InfoRE on GPT-4): 2WikiMultiHopQA = 76.52 F1, StrategyQA = 71.20 F1, HotpotQA = 83.22 F1 (paper highlights GPT-4 achieves highest F1 with InfoRE). Cross-validation: using GPT-3.5 re-org with GPT-4 reasoning (InfoRE†) yields FEVEROUS 94.67 and 2WikiMultiHopQA 75.07 (slightly lower than same-model reorg but still improved vs baseline). Overall the paper reports average absolute improvement of ~4% across tasks with InfoRE.",
            "qualitative_findings": "GPT-4 produces higher-quality MindMaps (ranked best in depth and clarity). Using higher-quality re-organization (GPT-4 produced) benefits weaker reasoners more (e.g., GPT-3.5 benefits when given GPT-4 re-org). InfoRE mainly reduces contextual misunderstanding errors; combining InfoRE with CoT leads to near-best or best performance on many datasets. CoT alone gives only marginal gains in some hard settings (e.g., CoT gave +0.62% on HOVER 4-hop vs Standard, whereas InfoRE gave +3.02%).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "GPT-4 benefits from InfoRE with consistent gains (≈3% absolute on many claim-verification hops and large gains on FEVEROUS/SCIFACT), and InfoRE complements CoT (InfoRE+CoT often best). The quality of re-organized information affects downstream reasoning; extraction of logical relations (MindMap) is the dominant contributor and RL-based pruning is superior to naive similarity pruning.",
            "uuid": "e8260.2",
            "source_info": {
                "paper_title": "Information Re-Organization Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Take a step back: Evoking reasoning via abstraction in large language models",
            "rating": 1,
            "sanitized_title": "take_a_step_back_evoking_reasoning_via_abstraction_in_large_language_models"
        },
        {
            "paper_title": "BASS: Boosting abstractive summarization with unified semantic graph",
            "rating": 1,
            "sanitized_title": "bass_boosting_abstractive_summarization_with_unified_semantic_graph"
        }
    ],
    "cost": 0.01510675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Information Re-Organization Improves Reasoning in Large Language Models
24 May 2024</p>
<p>Xixoxia Cheng 
Department of Computer Science
Zhejiang University zjucxx
zqtan, lokilanka</p>
<p>Zeqi Tan 
Department of Computer Science
Zhejiang University zjucxx
zqtan, lokilanka</p>
<p>Wei Xue 
Department of Computer Science
Zhejiang University zjucxx
zqtan, lokilanka</p>
<p>Weiming Lu 
Department of Computer Science
Zhejiang University zjucxx
zqtan, lokilanka</p>
<p>Information Re-Organization Improves Reasoning in Large Language Models
24 May 20244E528B03B3133C09B3F70D15C8377CFFarXiv:2404.13985v2[cs.CL]
Improving the reasoning capabilities of large language models (LLMs) has attracted considerable interest.Recent approaches primarily focus on improving the reasoning process to yield a more precise final answer.However, in scenarios involving contextually aware reasoning, these methods neglect the importance of first identifying logical relationships from the context before proceeding with the reasoning.This oversight could lead to a superficial understanding and interaction with the context, potentially undermining the quality and reliability of the reasoning outcomes.In this paper, we propose an information re-organization (InfoRE) method before proceeding with the reasoning to enhance the reasoning ability of LLMs.Our re-organization method involves initially extracting logical relationships from the contextual content, such as documents or paragraphs, and subsequently pruning redundant content to minimize noise.Then, we utilize the re-organized information in the reasoning process.This enables LLMs to deeply understand the contextual content by clearly perceiving these logical relationships, while also ensuring high-quality responses by eliminating potential noise.To demonstrate the effectiveness of our approach in improving the reasoning ability, we conduct experiments using Llama2-70B, GPT-3.5, and GPT-4 on various contextually aware multi-hop reasoning tasks.Using only a zero-shot setting, our method achieves an average absolute improvement of 4% across all tasks, highlighting its potential to improve the reasoning performance of LLMs * .</p>
<p>Introduction</p>
<p>Large language models (LLMs) demonstrate powerful generative capabilities and achieve remarkable performance across a range of linguistic tasks [1][2][3].However, their capabilities in performing complex reasoning tasks -an essential aspect of advanced language understanding and intelligent decision-making -still present substantial challenges [4,5].This has spurred researchers to explore innovative strategies [6][7][8] to improve the reasoning capabilities of these models.</p>
<p>Recently, diverse methods have been developed to enhance the reasoning ability of LLMs.For example, a notable method Chain-of-Thought (CoT) [6], incorporates a series of intermediate reasoning steps into the reasoning.CoT [6] allows for a more transparent and understandable path to the final answer, making it easier to follow the logic behind the conclusion.Building upon this foundation, subsequent approaches such as Tree of Thoughts (ToT) [8] and Graph of Thoughts (GoT) [8] are proposed to further refine the reasoning steps and enhance the accuracy and reliability of LLMs.Different from the sequential intermediate steps of Chain-of-Thought [6], Tree of Thoughts [8] and Graph of Thoughts [8] model the problem solving process into structured tools of tree and graph, respectively.A critical observation is that these existing approaches primarily focus on improving the reasoning process of large language models, as shown in Figure 1 (left).However, in scenarios involving contextually aware reasoning, it is equally important to first identify logical relationships of context before proceeding with the reasoning, not just improve their reasoning process.This is because logical relationships, such as parallelism, causal connections, contrasts, etc., are essential elements of reasoning [9].Nevertheless, these existing methods often neglect this crucial step.Such an oversight can lead to a superficial understanding and interaction with the context, potentially undermining the quality and reliability of the reasoning results.</p>
<p>Context:</p>
<p>[Re-Organized TEXT] [TEXT]</p>
<p>Question:</p>
<p>[QUESTION] Answer: …… Final answer is [ANSWER].</p>
<p>InfoRE</p>
<p>Context:</p>
<p>[TEXT] Question:</p>
<p>[QUESTION] Answer:</p>
<p>First, …… Next, ……</p>
<p>Final answer is [ANSWER].</p>
<p>Existing Figure 1: InfoRE (Ours) vs existing methods.In contrast to the existing methods that primarily focus on the reasoning process, our InfoRE emphasizes the re-organization of context information.</p>
<p>Inspired by the fact that when faced with context-aware reasoning tasks humans often first re-organize existing contextual information to uncover the logical relationships, eliminate noises, and enhance their understanding of the context, we propose an information re-organization (InfoRE) method, to ground reasoning by the re-organized information.As shown in Figure 1 (right), different from previous methodologies that primarily focus on refining the reasoning steps to enhance the reasoning capabilities of LLMs, our approach takes a novel direction of context re-organization.We emphasize the utilization of re-organized contextual content to explicitly present the logical relationships that are often implicit within the plain text, promoting more effective reasoning.Specifically, our reorganization method comprises two operations: extraction and pruning.The extraction first uncovers the implicit logical relationships within the contextual content by transforming the content into a MindMap structure [10].We employ this structure because it is rich in logical relationships and encompasses multi-hop connections.Pruning is then used to further minimize noise that is irrelevant to the reasoning objective.The pruning operation uses a pre-trained BERT [11] based model trained with reinforcement learning (RL).Finally, we utilize the re-organized context to reason.This enables LLMs to deeply understand the context by clearly perceiving these logical relationships, facilitating the quality and reliability of reasoning.Besides, our information re-organization method can be integrated with existing prompt methods, like CoT [6], to further improve the reasoning ability of LLMs.To verify the efficacy of our proposed InfoRE method, we conduct experiments using Llama2-70B [2], GPT-3.5 [1], and GPT-4 [3] on various contextually aware multi-hop reasoning tasks, including claim verification [12], question answering [13], and reading comprehension [14].Using only a zero-shot setting, our method achieves an average improvement of 4% across all tasks, highlighting its potential to improve the reasoning performance of LLMs.</p>
<p>Our main contributions are as follows:</p>
<p>• In contrast to existing methods that primarily focus on refining the reasoning steps to enhance the reasoning capabilities of LLMs, we take a novel direction of context re-organization.• The re-organization method initially uncovers the logical relationships that encompass multi-hop connections in the contextual content by extraction, and subsequently minimizes noise by pruning.• Experiment improvements on contextually aware multi-hop reasoning tasks across claim verification, question answering, and reading comprehension show the efficacy of our proposed method.</p>
<p>Related Work</p>
<p>Reasoning with LLMs Large language models (LLMs) [2,1,3], have revolutionized the field of natural language processing (NLP) and demonstrate remarkable proficiency across a range of linguistic tasks.To further improve the reasoning ability of LLMs has attracted considerable interest.</p>
<p>In-context learning [1], as a promising approach to enhance the reasoning abilities of LLMs, has been verified in mathematical reasoning task [6].In addition, Chain of Thought (CoT) [6] incorporates a coherent series of intermediate reasoning steps to improve the reasoning ability of LLMs.Following this paradigm, Tree-of-Thoughts (ToT) [7] and Graph-of-Thoughts (GoT) [8] have also been proposed to focus on improving the structure of the intermediate chain.Then self-consistency [15] and plan-tosolve [15] focus on the reliability of the chain.Recently, the step-back prompting [16] is proposed, which obtains the high-level concept and first principles from instances by abstraction in the first step, then guides the reasoning of LLMs with the obtained concept and principles.For tasks that require multi-hop reasoning, current approaches [17,18] generally decompose multi-hop problems into simpler sub-tasks problems relying on the in-context learning ability of LLMs.In contrast to existing methods, we take a novel direction of context re-organization to enhance the reasoning capabilities of LLMs for contextually aware reasoning tasks.</p>
<p>Information Re-organization Information re-organization is a technique that leverages other structures to improve the clarity and comprehensibility of information.Moreover, information reorganization can also reveal conceptual relationships implicit in the original textual text.The graph, as the most common re-organization structure, is often used to enhance contextual representation in a variety of tasks.For example, BASS [19] re-organizes the original document to a semantic graph to improve the summary generation.DGM [20] re-organizes the context to an explicit discourse graph in the reading comprehension task.MindMap [10] is a powerful structure method for representing knowledge and concepts, it can be used to construct a hierarchical abstraction of natural language text [21].Previous method [22] uses it to organize a large amount of scientific material to enhance the clarity of the text material.In this paper, we use it as the structure of re-organized information to uncover the logical relationships and multi-hop connections implicit within the plain context.Then we use this re-organized information to further improve the reasoning ability of LLMs.</p>
<p>Methodology</p>
<p>In this section, we first give a formulation of a context-aware reasoning task in §3.1 and then describe our method in detail.As shown in Figure 2, the framework of our method consists of two components, an information re-organization §3.2 and a reasoning step using the re-organized context §3.3.</p>
<p>Task Formulation</p>
<p>Given a sample (c, q, a) from sample space (C, Q, A), where c denotes gold context, q denotes question, the task aims to obtain answer a to the question q, based on gold context c with a large language model.In particular, it requires multi-hop reasoning to get the answer a to question q.</p>
<p>Information Re-Organization</p>
<p>The purpose of the information re-organization is to obtain the logical relationships from the context and minimize the noise irrelevant to the question.This goal is achieved through two operations: extraction and pruning.</p>
<p>Extraction</p>
<p>For a question q, if the derivation of its answer a relies on context c, then a deeper understanding of c is crucial.Logical relationships, such as parallelism, causal connections, contrasts, etc., are essential elements of understanding and reasoning [9].However, the logical relationships in the plain context are often implicit.Therefore, we perform a extraction operation on the plain context to uncover the logical relations in it using a language model.The process can be defined as:
g = f θ (c, q, P in )(1)
where f θ (•) is a language model parameterized by θ, P in is input task prompt.The specific task prompt P in used in our paper is displayed in the Appendix A. We perform Equation 1 for each c in the sample space to obtain the extracted context space G.We use the MindMap [10] structure to display the reorganized content, because it serves as a powerful structure for representing knowledge, concepts, and perspectives, contains not only logical relationships but also multi-hop connections.</p>
<p>As shown in Figure 2(bottom left), the extracted context g ∈ G contains not only parallel logical relationships, e.g., Director&amp;Producer, but also causal relationships, e.g., Julius Caesar→Director.Furthermore, it also describes the three-hop connections, such as Julius Caesar→Director→Name, Julius Caesar→ Director→Occupation, and Julius Caesar→Director→Education. This information with logical relationships enables LLMs to deeply understand the contextual content by clearly perceiving these logical relationships, facilitating the quality and reliability of reasoning.Additionally, this multi-hop connection corresponds to the complex multi-hop problem and therefore helps to solve this multi-hop question q.</p>
<p>Pruning</p>
<p>As described in Section 3.2, the extracted context g ∈ G contains various logical relationships and rich attributes.However, not all logical relationships and attributes help answer the question q.On the contrary, some may even interfere with the response to the question.For example, consider the question in Figure 2, "Where did the producer of Julius Caesar study or work?".In this case, the content "Julius Caesar → Production Company" is a distracting element, and "Julius Caesar → Adaptation" is irrelevant to the question.
… BERT 𝒆 𝑪𝑳𝑺 𝒆 𝒓 𝟏 Linear 𝒆 𝒓 𝟐 𝒆 𝒓 𝒏 𝒆 𝒒 𝒆 𝑪𝑳𝑺 𝒆 𝑺𝑬𝑷
Action Probabilities</p>
<p>[CLS] relation attributes [SEP] [CLS] relation attributes … [SEP] Question [SEP]</p>
<p>Softmax To further reduce the interference of distracting or irrelevant logical relations and attributes on the retrieval of answers for question q, we use a pruning model trained through reinforcement learning (RL).The pruning model is based on the pre-trained BERT [11], as shown in Figure 3.Its input consists of concatenated logical relationships, their corresponding attribute values from g, and the question q.For example, to prune the relation Julius Caesar → Adaptation, the input is: [CLS] Julius Caesar Adaptation Play by Shakespeare [SEP] Question [SEP].We give a detailed demonstration of input format in Appendix B.</p>
<p>RL Formulation</p>
<p>We formulate the pruning policy model optimization as an RL problem and employ proximal policy optimization (PPO) [23].The action is keeping or deleting the logical relations.The policy decides the action probability given the question and g.We fine-tune the policy model π by optimizing the reward r:
E π [r] = E g∼G,q∼Q,z∼π(•|x,q) <a href="2">r(z, q)</a>
Reward Function Our goal is to maximize LLM's generation toward the desired target by an alignment measure R, and we use this as a reward.In this paper, the alignment metric we have chosen is the F1 score.To keep the policy network π from moving too far from the old result, we add a clipped term in the reward.Therefore, the final reward becomes:
r(z, q) = min(R(z, q), clip(π(z | x, q), 1 − ϵ, 1 + ϵ))(3)
where ϵ is a hyperparameter indicating the range of CLIP to be performed.After pruning, the extracted contextual content g becomes the context g ′ , which is closely related to the reasoning objective q.We perform a pruning operation for each g ∈ G to obtain the pruned context space G ′ .</p>
<p>Reasoning</p>
<p>After the re-organization process in Section 3.2, we get the re-organized context g ′ ∈ G</p>
<p>′ .Then the re-organized context g ′ can be used as a context alone or combined with the original context c to get the final answer of q.The reasoning process can be defined as:
o = f θ (g ′ , [c] , q, P r )(4
) where P r denotes prompt, and the content within [] denotes that it is optional.The specific prompt used in reasoning is displayed in the Appendix C.</p>
<p>Experimental</p>
<p>Tasks and Datasets</p>
<p>To verify the effectiveness of our information re-organization method, we conduct experiments across a range of contextually aware multi-hop reasoning tasks and datasets, including claim verification [12], question answering [13], and reading comprehension [14].The detailed dataset information, including data splits and statistics, is available in Appendix D.</p>
<p>Claim Verification</p>
<p>The task involves assessing a given claim against a set of evidence documents to determine whether they support or refute the claim [12].We consider HOVER [24] and FEVEROUS [25], which comprise complex claims that necessitate multi-hop reasoning for verification.Besides, we also take into account the SCIFACT [26] dataset, notable for its inclusion of scientific claims.</p>
<p>Question Answering For this task, we consider the following datasets: 2WikiMultiHopQA [27], StrategyQA [28], MuSiQue [29], and HotpotQA [30].To answer the questions in these datasets requires not only multi-hop reasoning but also cross-document analysis.</p>
<p>Reading Comprehension Machine reading comprehension task requires a model to process documents and select an answer from the provided candidates to a question about the content [14].We primarily consider WIKIHOP [31] in the task, which necessitates multi-hop reasoning to derive the final answer.Additionally, HotpotQA [30] is also frequently considered as part of the QA domain.</p>
<p>Baselines</p>
<p>In our paper, we compare our method InfoRE to two reasoning baselines: 1) Standard.2) CoT.The Standard approach is a method that directly reasons with the original textual context.The Chain of Thought (CoT) [6] method involves augmenting standard reasoning methods by adding a step-by-step thought process.In our paper, we conduct the CoT strategy by appending the sentence "Let's think step by step." at the end of the question.</p>
<p>The baseline methods and our InfoRE both adopt a zero-shot setting to counteract the potential randomness associated with demonstrations in a few-shot setting.We also design an answer-format instruction within the prompts for various tasks to standardize the structure of the final answer, thereby enhancing the precision of answer extraction.Moreover, all results reported in the paper use only the reorganized contextual information to reason.Comprehensive details about prompts and answer-format instruction are available in Appendix C. Following previous methods [17,18,25], we run the official evaluation scripts of each dataset to get the F1 to measure the results.</p>
<p>Implementation Details</p>
<p>In our paper, the large language models employed in the extraction and reasoning process include Llama2-70B [2], GPT-3.5 (text-davinci-003) [32] and GPT-4 [3].We configure all models with top_p parameter as 1.0 and temperature as 0.0.In the policy model, we use the BERT-base version on all tasks and datasets.In RL training, we calculate the F1 score between the generated answer and the reference answer as the reward, with a rescaling coefficient of 10.We train the model for 1000 episodes.We conduct training for epoch 5, a batch size of 4, and a learning rate of 2e-6.The parameter of ϵ is set to 0.2.All experiments are conducted on an NVIDIA RTX A6000.</p>
<p>Results and Analysis</p>
<p>Main Results</p>
<p>Claim Verification Table 1 presents a comprehensive performance comparison between our InfoRE and existing zero-shot techniques.For the HOVER dataset, we segment it into the 2-hop, 3-hop, and 4-hop levels following previous methods [17].As depicted in Table 1, our InfoRE demonstrates significant improvements in the zero-shot claim verification task.The CoT [6] approach offers a lightly increase of 0.62% in the HOVER 4-hop using GPT-4, which indicates its marginal utility in more complex reasoning scenarios.Yet, our InfoRE achieves 3.02% improvement on the HOVER 4hop using GPT-4 showing remarkable performance on contextual aware understanding and reasoning.This improvement is further increased to 73.62% in combination with CoT, suggesting that the methods complement each other effectively.In the case of Llama2-70B, the combined application of InfoRE and CoT yields a score of 53.20% on the 2-hop HOVER task, surpassing its CoT-only score of 50.02%.This pattern of improvement is consistent with GPT-3.5.GPT-4 shows superior performance across all methods and datasets, suggesting an inherent advanced reasoning ability.This performance is mirrored in the specialized benchmarks, where GPT-4 with InfoRE attains near-perfect accuracy on FEVEROUS (95.62%) and very high accuracy on SCIFACT (93.67%).These figures solidify the notion that the InfoRE indeed enhances the reasoning capabilities of LLMs.</p>
<p>Table 1: Zero-shot performance on claim verification task across three large language models.</p>
<p>LLMs</p>
<p>Analysis</p>
<p>Ablation studies In our paper, the re-organization comprises two components: extraction and pruning.To investigate the impact of each component in detail, we conduct a series of ablation experiments using GPT-3.5 on the 2WikiMultiHopQA dataset.First, we directly remove extraction and pruning from our method, and the results are shown in the second and third rows of Table 3, respectively.It is worth mentioning that in the experiment where extraction is removed, we directly prune the sentences in the original context.Furthermore, we replace the reinforcement learning-based pruning method with a similarity-based pruning method to demonstrate its effectiveness.Specifically, the similarity-based pruning method uses Siamese-BERT, which takes the original question and each logical relationship as inputs separately, and then generates the corresponding representations.Then, we calculate the cosine similarity between these two representations.Finally, we removed the 30% of logical relationships with the lowest similarity, the result is shown in the last row of Table 3.The results in Table 3 show that removing the extraction and pruning operations leads to performance drops of 2.94% and 1.53%, respectively.This demonstrates the effectiveness of both components in our methods.The larger performance drop after removing extraction highlights the importance of extracting logical relationships for effective reasoning.After replacing the pruning model, the performance dropped by 1.26%, but the results were still better than without pruning.</p>
<p>This not only demonstrates the necessity of pruning but also highlights the effectiveness of the reinforcement learning-based pruning method.</p>
<p>Quality of Re-Organized Information</p>
<p>To assess the quality of the re-organized information, we perform a quantitative evaluation of the re-organized information with GPT-4 (gpt-4-32k) on the 2WikiMultiHopQA dataset.Specifically, we select 100 samples from the dataset, GPT-4 (gpt-4-32k) is asked to rank re-organized information produced by GPT-3.5 (text-davinci-003) [1] and GPT-4, as well as original context following criteria: (1) Depth: The information present multiple relationships of a topic, offering insightful perspectives or in-depth understanding of the subject.(2) Clarity: Information is clear and precise, making it easy to understand without ambiguity.All of the information is ranked 1, 2, and 3 with 3, 2, and 1 scores, respectively.Finally, we get a weighted average score for each information to measure the overall quality.The results in Table 4 demonstrate that the reorganized information outperforms the original textual context in terms of depth and clarity, which justifies the motivation of our paper.In addition, the re-organized information from GPT-4 outperforms GPT-3.5, which proves in another way that GPT-4 is indeed more capable than GPT-3.5.The evaluation results further validate the effectiveness of our method.Furthermore, the 22.22% improvement in depth is more obvious than the 15.14% improvement in clarity, which indicates that the re-organization of the information has been particularly effective in enhancing the logical relationships of the information.The improvement in clarity suggests that the information is now presented in a more direct and streamlined manner, making it easier for LLMs to grasp the essential points without wading through unnecessary details.</p>
<p>Effect of Re-organized Information Quality</p>
<p>To explore the effects of the quality of re-organized context on model reasoning capabilities, we utilize both GPT-3.5 (text-davinci-003) and GPT-4 to re-organize context information.Reasoning processes are then independently executed on each model.Moreover, employing this cross-validation technique also allows us to effectively evaluate the robustness of our method.The cross-validation results, shown in Table 5, indicate that the use of either GPT-3.5 or GPT-4 for information re-organization leads to a marked improvement in the performance of LLMs on reasoning tasks.Additionally, it is observed that using GPT-4 for information re-organization while reasoning with the GPT-3.5 results in a further increase in reason results by 1.19% and 2.03% on FEVEROUS and 2WikiMultiHopQA datasets, respectively.This implies that enhancing the quality of re-organized information can improve the performance of LLMs, pointing to a promising direction for future research in refining information synthesis and re-organization.Conversely, when GPT-3.5 is employed for information re-organization in conjunction with reasoning using the GPT-4 model, there is a decrease in reason ability by 0.95% and 1.45% respectively.In addition, the magnitude of the decrease is lower than the increase, which implies that our re-organization strategy may have a greater impact on models with weaker reasoning abilities.This finding aligns with our understanding that models with inherently weaker reasoning abilities tend to rely more heavily on external strategy.For models with stronger inherent capabilities, our method still further improve its reasoning ability.</p>
<p>Error Analysis To better comprehend where the errors in our InfoRE methodology come from and where they are fixed, we annotate 100 wrong predictions made by both InfoRE and Standard methods with GPT-3.5 on 2WikiMultiHopQA dataset.We categorize the errors into 4 classes:</p>
<p>• Contextual Misunderstanding (CM): This happens when the model fails to interpret or connect multiple pieces of information from different parts of the documents.Multi-hop reasoning requires synthesizing information from various segments, and recognizing logical relations, and any misunderstanding can lead to incorrect conclusion.</p>
<p>• Factual Error (FE): The model may provide an answer that is factually incorrect or not supported by the given documents.This is often due to the model's reliance on its training data, which may not always align with the specific facts in the context.</p>
<p>• Mathematical Error (ME): The error occurs when math calculations are involved in deriving the final answer.</p>
<p>• Unanswerable Question (UQ): It's a specific type of error or limitation in dataset design, where the context does not contain enough information to provide a valid answer to the posed question.The first four rectangles are error categories, while "Corrected" on the far right denotes the percentage of errors originally made by the baseline method that our method InfoRE has successfully corrected.</p>
<p>When engaging in reasoning with large language models, all four error categories are present.As shown in Figure 4, there are 6% unanswerable question errors in the dataset, and more than 90% errors occur in the reasoning in the baseline method.Among the four types of error, contextual misunderstanding is the primary source of errors in the baseline, highlighting the importance of an in-depth understanding of context in solving reasoning tasks with large language models.This finding is consistent with our motivation presented in the introduction section.Moreover, comparing the results of errors between our method and the baseline method, our method mainly corrects 14% of errors coming from the baseline method, most of the corrected errors are contextual misunderstanding errors.This further indicates that our InfoRE method assists Large Language Models (LLMs) in understanding context, signifying the necessity and effectiveness of conducting information re-organization before directly addressing the original question.</p>
<p>Conclusion</p>
<p>In this paper, we propose an information re-organization method to improve the reasoning ability of large language models (LLMs).Compared with previous approaches primarily focus on improving the quality of intermediate steps, our method emphasizes uncovering the logical relationships, multihop connections, and pruning the irrelevant information through information re-organization.This approach enables LLMs to explicitly perceive the logical relationships and multi-hop connections of concepts within the context, promoting a deeper integration and understanding of the context, which results in more robust reasoning outcomes.To verify the effectiveness of our method, we conduct experiments using various LLMs across a range of contextually aware multi-hop reasoning tasks.The experiment results demonstrate the potential of our method to improve the reasoning ability of LLMs.Additionally, our method has a positive impact on various tasks involving context understanding, such as academic research, legal analysis, and medical diagnostics.However, it is also important to be aware of potential negative impacts, such as the propagation of misinformation.[QUESTION] please answer the question based on the documents.Answer:</p>
<p>The specific prompts for Standard, chain-of-thought (CoT), our InfoRE, and InfoRE + CoT to reason are shown in Table 7.In the prompt, the content within the brackets [] should be replaced with specific example content.</p>
<p>D Detailed Dataset Information</p>
<p>In our experiments, due to the resource limitations of large language models, we sample a portion from each dataset, following previous methods [6,16].</p>
<p>Claim Verification</p>
<p>The task involves assessing a given claim against a set of evidence documents to determine whether they support or refute the claim [12].We consider HOVER [24] and FEVEROUS [25], which comprise complex claims that necessitate multi-hop reasoning for verification.Besides, we also take into account the SCIFACT [26] dataset, notable for its inclusion of scientific claims.</p>
<p>Question Answering For this task, we consider the following datasets: 2WikiMultiHopQA [27], StrategyQA [28], MuSiQue [29], and HotpotQA [30].To answer the questions in these datasets requires not only multi-hop reasoning but also cross-document analysis.Reading Comprehension Machine reading comprehension task requires a model to process documents and select an answer from the provided candidates to a question about the content [14].</p>
<p>We primarily consider WIKIHOP [31] in the task, which necessitates multi-hop reasoning to derive the final answer.Additionally, HotpotQA dataset [30] is also frequently considered as part of the question answering domain.</p>
<p>E Limitations</p>
<p>We propose an information re-organization approach to improve the reasoning of large language models, which performs well on some context-aware reasoning tasks but still has some limitations.</p>
<p>Firstly, the structures of information re-organization are limited.Generally, there are multiple structures for information reorganization, such as tables, timelines, etc. Next, we will extend the re-organization structure to include more types.Secondly, the re-organization process relies on large language models.If we can implement this re-organization using smaller language models, our method will become more generalizable.This is another direction we need to focus on in the future.</p>
<p>Answer • The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: The paper poses no such risks.Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: Introduction, Related Work, Methodology, Experimental, Results and Analysis.Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New Assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Context:Figure 2 :
2
Figure 2: Illustration of our information re-organization method with two modules: 1)Information Re-Organization, which includes logic relationship extraction and noise pruning.2) Reasoning using re-organized context.The re-organized context in black italicized text is relevant to the question.</p>
<p>Figure 3 :
3
Figure 3: Illustration of Pruning model.The representation of [CLS] is used to obtain action probabilities.</p>
<p>Figure 4 :
4
Figure 4: Error Analysis of InfoRE on 2Wiki-MultiHopQA against Standard baseline method.The first four rectangles are error categories, while "Corrected" on the far right denotes the percentage of errors originally made by the baseline method that our method InfoRE has successfully corrected.</p>
<p>Table 2 :
2
Question Answer and Reading Comprehension Different from claim verification task, this task involves using multiple documents as context, presenting a challenge in cross-document reasoning.Intuitively, when the reasoning process involves multiple documents, information re-organization can effectively merge the information from different documents and uncover logical relationships that are not apparent in plain text.Performance in Table2verifies the intuitive.We can see that after applying information re-organization, all the results have a significant improvement.GPT-4 outperforms the other models, with the highest F1 score of 76.52%, 71.20%, and 83.22% when employing InfoRE on 2WikiMultiHopQA, StrategyQA, and HotpotQA, respectively.This is consistent with the performance improvements observed with GPT3.5 (text-davinci-003) and Llama2-70B when applying InfoRE.Different from conventional QA tasks, reading comprehension tasks require LLMs to not only deeply understand the context but also identify distractors among the candidates, increasing the reasoning challenge.Our InfoRE consistently shows improvements on this task.Specifically, in the WIKIHOP dataset, GPT-3.5 with InfoRE outperforms other methods with an F1 of 51.87%.This improvement further verifies the effectiveness of our method.Zero-shot results on Question Answering and Reading Comprehension tasks.2WMHQA, SQA, and HQA are abbreviations for 2WikiMultiHopQA, StrategyQA, and HotpotQA, respectively.
MethodsHOVERFEVEROUS SCIFACT2-hop 3-hop 4-hopStandard49.41 48.35 47.8263.3960.70LLAMA2-70BInfoRE52.83 51.42 50.04 ↑ 3.42 ↑ 3.07 ↑ 2.2267.84 ↑ 4.4563.81 ↑ 3.11CoT50.02 48.76 48.0164.5361.24InfoRE + CoT 53.20 51.70 50.1568.1264.02↑ 3.18 ↑ 2.94 ↑ 2.14↑ 3.59↑ 2.78Standard64.74 63.04 61.5487.6777.42GPT-3.5InfoRE68.21 66.45 64.91 ↑ 3.47 ↑ 3.41 ↑ 3.3791.31 ↑ 3.6481.54 ↑ 4.12CoT66.70 64.52 62.6988.6778.49InfoRE + CoT 69.02 67.53 65.6691.5382.26↑ 2.32 ↑ 3.01 ↑ 2.97↑ 2.86↑ 3.77Standard72.40 71.02 70.0692.3391.40GPT-4InfoRE75.87 74.06 73.08 ↑ 3.47 ↑ 3.04 ↑ 3.0295.62 ↑ 3.2993.67 ↑ 2.27CoT73.82 72.07 70.6892.6792.47InfoRE + CoT 76.69 75.16 73.6295.6794.32↑ 2.87 ↑ 3.09 ↑ 2.94↑ 3.00↑ 1.85</p>
<p>Table 3 :
3
F1 performance of ablation studies.
Methods2WikiMultiHopQAFull model64.58w/o extraction61.64w/o pruning63.05similarity-based pruning63.32</p>
<p>Table 4 :
4
Qualitative evaluation results on 2Wiki-MultiHopQA dataset.Avg R denotes the weighted average ranking score.The larger ranking score denotes better information quality.
MethodsDepth1st2nd 3rd Avg R.Original0.22 0.36 0.421.80GPT-3.50.32 0.36 0.322.00GPT-40.46 0.28 0.262.20MethodsClarity1st2nd 3rd Avg R.Original0.25 0.35 0.401.85GPT-3.50.35 0.32 0.332.02GPT-40.40 0.33 0.272.13</p>
<p>Table 5
5: F1 performance of cross-validation, where InfoRE <em>denotes reason with GPT-3.5 but information re-organizationwith GPT-4, InfoRE  † denotes reason with GPT-4 but infor-mation re-organization with GPT-3.5 (text-davinci-003).MethodsFEVEROUS 2WikiMultiHopQAGPT-3.5 ( †)Standard87.6758.25InfoRE91.3164.58InfoRE </em>92.5066.61GPT-4 (*)Standard92.3372.69InfoRE95.6276.52InfoRE  †94.6775.07</p>
<p>Table 7 :
7
Specific prompts and answer format instructions of Standard, CoT, InforRE, and InfoRE + CoT in our paper.During execution, "[EVIDENCE]" needs to be replaced with the specific document, "[CLAIM]" is replaced with the specific question, and "[MindMap]" is replaced with the specific MindMap.
MethodsPrompt ContentDocuments: [EVIDENCE]Question: [CLAIM]?StandardPlease answer the question based on Documents. Your final answer should be enclosed in XML tag <answer></answer>, like this:<answer>{final_answer}</answer>, at the end of your response.Answer:Documents: [EVIDENCE]Question: [CLAIM]?Please answer the question based on Documents.CoTYour final answer should be enclosed in XML tag <answer></answer>, like this:<answer>{final_answer}</answer>, at the end of your response.Let's think step by step.Answer:Documents: [MindMap]Question: [CLAIM]?InfoREPlease answer the question based on Documents. Your final answer should be enclosed in XML tag <answer></answer>, like this:<answer>{final_answer}</answer>, at the end of your response.Answer:Documents: [MindMap]Question: [CLAIM]?Please answer the question based on Documents.InfoRE + CoTYour final answer should be enclosed in XML tag <answer></answer>, like this:<answer>{final_answer}</answer>, at the end of your response.Let's think step by step.Answer:[Re-Organized TEXT][TEXT][Optional]Question:</p>
<p>Table 8 :
8
Details statics information of evaluation datasets we used in the paper.Pairs denote the number of examples.2WMHopQA is the short name of 2WikiMultiHopQA.
TasksDatasetPairstrain testFEVEROUS2000 2959Claim VerificationHOVER2000 4000SCIFACT2002122WikiMultiHopQA 2000 500Question AnsweringStrategyQA1000 229MusiQue2000 2417Reading ComprehensionHotpotQA WIKIHOP2000 500 2000 500</p>
<dl>
<dt>The answer NA means that paper does not include experiments requiring code.•Pleasesee the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.•Whileweencourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).•Theinstructionsshould contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.•Theauthors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.•At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).•Providingasmuch information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.6.Experimental Setting/DetailsQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</dt>
<dd>[Yes]Justification: Supplementary materialGuidelines:• results? Answer: [Yes] Justification: Experimental Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer "Yes" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Experimental Guidelines: • Answer: [Yes] Justification: Supplementary material Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid-eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Conclusion • 8. Experiments Compute Resources Guidelines:
Acknowledgments and Disclosure of FundingUse unnumbered first level headings for the acknowledgments.All acknowledgments go at the end of the paper before the list of references.Moreover, you are required to declare funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).More information about this disclosure can be found at: https: //neurips.cc/Conferences/2024/PaperInformation/FundingDisclosure.Do not include this section in the anonymized submission, only in the final paper.You can use the ack environment provided in the style file to automatically hide this section in the anonymized submission.A Prompt for ExtractionThe specific prompt of extraction is shown in Table6.The purpose of the prompt is to obtain logical relationships and multi-hop connections from the context.In the prompt, the content within the brackets should be replaced with specific example content.Table6: Specific prompt for obtaining logical relationships.During execution, "[EVIDENCE]" needs to be replaced with the specific document, and "[CLAIM]" is replaced with the specific question.Prompt Content for Logical Relationship Extraction Given a claim and corresponding evidence, please summarize the evidence as a mind map according to the claim.The output must be in a strict JSON format: {"mind_map": "mind_map"}.CLAIM: [CLAIM] EVIDENCE:[EVIDENCE].B Input Format For PruningThe extracted context g includes logical relationships and corresponding attribute values.First, we iterate through all logical relationships and attribute values, and following the previous method[33], we concatenate them using[SEP][CLS], combining them with the question to input into the pruning model.Then, we use the representation of the [CLS] token to represent the logical relationships for subsequent operations.We use an example in Figure2C Prompt for Multi-hop ReasonDuring the reasoning stage using large language models, to accommodate more context-aware reasoning tasks while ensuring comparability of results, we designed a universal prompt template.The prompt template consists of three components: original context, e.g., documents or paragraphs, reorganized information, and a question.The prompt template is as follows: • The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes]Justification: Limitations in Appendix Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory Assumptions and ProofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer:[NA]Justification: The paper does not include theoretical results Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental Result ReproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: Experimental Guidelines:• The answer NA means that the paper does not include experiments.• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</dd>
</dl>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>OpenAI. Gpt-4 technical report. 2024</p>
<p>Gpt-4 can't reason. Konstantine Arkoudas, 2023</p>
<p>Can gpt-3 perform statutory reasoning?. Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme, 10.1145/3594536.3595163Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law, ICAIL '23. the Nineteenth International Conference on Artificial Intelligence and Law, ICAIL '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Tree of Thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michał Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 10.1609/aaai.v38i16.29720Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar 202438</p>
<p>On the relation between the natural logic of reasoning and standard logic. M D S Braine, 10.1037/0033-295x.85.1.1Psychological Review. 851978</p>
<p>The mind map book: Unlock your creativity, boost your memory, change your life. Tony Buzan, Barry Buzan, James Harrison, 2010141986951</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>A survey on automated factchecking. Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos, 10.1162/tacl_a_00454Transactions of the Association for Computational Linguistics. 102022</p>
<p>Unsupervised question decomposition for question answering. Ethan Perez, Patrick Lewis, Wen-Tau Yih, Kyunghyun Cho, Douwe Kiela, 10.18653/v1/2020.emnlp-main.713Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Chi, Denny Quoc V Le, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Fact-checking complex claims with program-guided reasoning. Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, Preslav Nakov, 10.18653/v1/2023.acl-long.386Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Question decomposition improves the faithfulness of model-generated reasoning. Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam Mccandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, R Samuel, Ethan Bowman, Perez, 2023</p>
<p>BASS: Boosting abstractive summarization with unified semantic graph. Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, Haifeng Wang, 10.18653/v1/2021.acl-long.472Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>Dialogue graph modeling for conversational machine reading. Siru Ouyang, Zhuosheng Zhang, Hai Zhao, doi: 10.18653Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, Association for Computational LinguisticsAugust 2021</p>
<p>URL. </p>
<p>Text to multi-level mindmaps: A new way for interactive visualization and summarization of natural language text. Mohamed Elhoseiny, Ahmed M Elgammal, CoRR, abs/1408.10312014</p>
<p>Freepub: Collecting and organizing scientific material using mindmaps. Theodore Dalamagas, Tryfon Farmakakis, Manolis Maragkakis, Artemis G Hatzigeorgiou, ArXiv, abs/1012.16232010</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2017</p>
<p>HoVer: A dataset for many-hop fact extraction and claim verification. Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, Mohit Bansal, 10.18653/v1/2020.findings-emnlp.309Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational LinguisticsNovember 2020</p>
<p>Feverous: Fact extraction and verification over unstructured and structured information. Rami Aly, Zhijiang Guo, Michael Schlichtkrull, James Thorne, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. J Vanschoren, S Yeung, the Neural Information Processing Systems Track on Datasets and Benchmarks20211Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal</p>
<p>SciFact-open: Towards open-domain scientific claim verification. David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi, 10.18653/v1/2022.findings-emnlp.347Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of the 28th International Conference on Computational Linguistics. Donia Scott, Nuria Bel, Chengqing Zong, the 28th International Conference on Computational LinguisticsBarcelona, SpainDecember 2020</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 2021</p>
<p>MuSiQue: Multihop questions via single-hop question composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 10.1162/tacl_a_00475Transactions of the Association for Computational Linguistics. 102022</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober-November 2018</p>
<p>Constructing datasets for multihop reading comprehension across documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, CoRR, abs/1710.064812017</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Text summarization with pretrained encoders. Yang Liu, Mirella Lapata, 10.18653/v1/D19-1387Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8809 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8809</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8809</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-252280335</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.coling-1.534.pdf" target="_blank">Graph-to-Text Generation with Dynamic Structure Pruning</a></p>
                <p><strong>Paper Abstract:</strong> Most graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8809.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8809.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeviGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi graph conversion (edge-to-node transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversion that turns a labeled multi-relational directed graph into an unlabeled bipartite graph by turning each labeled edge into an intermediate relation node and adding reverse/default edge types; used as the basis for downstream token-level graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph (edge-as-node) conversion</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms each labeled edge (v_i, r, v_j) of the original multi-relational directed graph G0 into two unlabeled edges (v_i, r) and (r, v_j) in a bipartite Levi graph G_l; additionally adds reverse edges for default edges so the Levi graph contains two relation types {d, r} (default and reverse). This preserves labeled relations by turning relation labels into explicit intermediate nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and Knowledge Graphs (multi-relational directed graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each labeled edge (v_i, r, v_j) create a relation-node r and edges (v_i, r) and (r, v_j); add reverse edges for default edges; resulting graph is an unlabeled bipartite Levi graph.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text and KG-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of the pipeline for the paper's models; the full model (using Levi conversion + token-graph + SACA + DGP) achieved BLEU=47.85 and METEOR=45.80 on LDC2020T02 (reported mean over 4 seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Levi conversion is the preprocessing step used by this paper and follows previous practice (cited Beck et al., 2018); compared to simple sequence linearization (FINETUNE/SPRING), Levi graph preserves relational structure explicitly rather than flattening it.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly preserves relation labels as nodes so relation information is available to graph encoders; keeps structural information instead of flattening into a sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces additional nodes (relation nodes), increasing graph size; requires GNN/RGAT-style encoders to exploit structure (not directly consumable by vanilla sequence LMs without further conversion).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No direct failure cases reported for Levi conversion alone; overall larger graphs (which Levi conversion increases further) can be more challenging—model performance degrades with very large or very noisy graphs unless further mechanisms (like SACA/DGP) are used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8809.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8809.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TokenGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level graph representation (token graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that converts each token of a node in the Levi graph into a separate node, producing a token-granular graph G that is directly used with PLM token embeddings and structural adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level graph (token graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Starting from the Levi graph G_l, each token belonging to a node in V_l is made into its own node in a new graph G=(V,E,R), so nodes correspond to tokens rather than higher-level graph items; this enables closer integration with token-based pretrained language models (PLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and Knowledge Graphs (represented at token granularity)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>After Levi conversion, decompose node surface forms into tokens; create a node per token and retain the Levi graph connectivity among token-nodes (with relation types R inherited), producing a token graph aligned to PLM token inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation with PLMs (T5-based models in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in the paper's T5-based models; full model using token graph + SACA + DGP reported BLEU=47.85 and METEOR=45.80 on LDC2020T02 (mean over 4 seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared implicitly with sequence-linearization approaches: token graph keeps graph topology at token level for adapters and SACA, while linearization (FINETUNE/SPRING) discards explicit topology and feeds a sequence to the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Aligns graph nodes to PLM token-level inputs facilitating use of PLM token embeddings and structural adapters; preserves token-level connectivity and relation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Expands number of nodes (token-level granularity increases graph size), which can raise computational cost for GNN/RGAT layers and attention operations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Larger token graphs can lead to higher complexity; no single failure mode isolated, but empirical results show poorer performance on very large or very noisy graphs unless complemented by SACA/DGP.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8809.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8809.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization (FINETUNE / SPRING)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization / sequence serialization (FINETUNE; SPRING uses linearization for AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flatten the input graph into a sequence of tokens and fine-tune a pre-trained sequence model (e.g., T5) on the serialized input instead of injecting structural bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural adapters in pretrained language models for AMR-to-Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / Serialization (sequence conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert the entire input graph into a serialized sequence (a linear string) and feed it to a text-to-text pretrained LM (e.g., T5) for fine-tuning; SPRING performs graph linearization for AMR tasks and FINETUNE denotes direct sequence fine-tuning of T5 on serialized graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR and Knowledge Graphs (flattened into sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Custom graph linearization strategies (e.g., flattening triples or AMR graph traversal) to produce a single token sequence that represents nodes/relations; the exact linearization variant depends on method (SPRING proposes a specific AMR linearization).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text and KG-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that FINETUNE performed well on ENT-DESC (FINETUNE outperformed previous methods on ENT-DESC according to the paper), but exact numbers are not provided in-text for FINETUNE; by contrast, the paper's structured models further improve over structure-less FINETUNE in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Linearization (FINETUNE/SPRING) is simpler but discards explicit graph topology; the paper shows that injecting structural bias (SA-RGCN, SACA, DGP) typically improves faithfulness and performance on complex graphs compared to pure linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; directly compatible with sequence PLMs (no graph encoder required); can perform competitively on some datasets (e.g., ENT-DESC).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ignores explicit graph topology which can hurt performance on complex graphs; may allow the model to hallucinate relations not present in the original graph (less faithful).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>On complex graphs with high reentrancy or large diameter, linearized approaches tend to underperform structure-aware methods; ENT-DESC shows sensitivity to noise where linearization may be less robust.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8809.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8809.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SA-RGCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Adapters in PLMs (SA-RGCN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapter-based method that injects graph structural bias into pretrained language model encoders by adding structural adapter layers that aggregate neighbor information per relation type.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural adapters in pretrained language models for AMR-to-Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural adapter (RGCN-based adapter) for PLMs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Adds a structural adapter after each Transformer encoder block that aggregates representations from immediate neighbors under each relation r using learnable relation-specific matrices W^l_r, followed by a gating/FFN adapter; this encodes graph topology into encoder token representations while freezing base PLM weights.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and Knowledge Graphs (multi-relational)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operates on token-level graph input (e.g., Levi/token graph); computes neighbor-aggregated features per relation at each adapter layer (Equations 1-2 in the paper) and injects these into PLM encoder representations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text and KG-to-text generation using PLMs (T5 in the experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as the main baseline; the authors report that their final model (SA-RGCN + SACA + DGP) improves over their implemented SA-RGCN by +1.01 METEOR and +0.38 M-score on LDC2020T02; specific SA-RGCN absolute metrics are reported in tables (SA-RGCN reimplementation numbers shown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>SA-RGCN injects structural bias into PLMs as an adapter approach and outperforms pure sequence FINETUNE in many cases; the paper's SACA/DGP components are built on top of SA-RGCN and further improve performance, showing that decoding-time structure-aware re-encoding adds complementary gains.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encodes graph topology directly into PLMs without fine-tuning the base model (avoids catastrophic forgetting); effective baseline that improves over sequence-only methods.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still relies on a static encoder representation computed once; the encoder may not capture all decoding-step-specific specialized information, motivating SACA re-encoding at decode time.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>SA-RGCN alone is less effective on very complex graphs (high node count, large diameter, many reentrancies) compared to when combined with SACA/DGP; removing structural adapters reduces performance (paper's ablation shows drop when w/o StrucAdapt).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8809.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8809.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SACA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-Aware Cross-Attention (SACA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-side mechanism that re-encodes the input graph at every decoding step by augmenting the graph with a generated-text context node and running relational graph attention (RGAT) so the decoder gets a structure-aware, context-conditioned graph context vector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structure-Aware Cross-Attention (SACA) re-encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each decoding time step, create a joint graph by adding a node v_d representing the current generated-text context and connect it bidirectionally to all input graph nodes; run multi-layer relational graph attention (RGAT) over the joint graph to re-encode node representations conditioned on v_d; use the final v_d embedding as the structure-enhanced input-graph context for token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and Knowledge Graphs (token-level Levi graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a linearization — SACA performs a dynamic re-encoding: build joint graph (V ∪ {v_d}), connect v_d to every input node, run L layers of RGAT with relation embeddings E_r, compute attention scores and relation-aware neighbor aggregation (Equations 4–6), and fetch h^L_{v_d} as cross-attention context.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text and KG-to-text) in PLM decoders (T5) and in RNN decoders when generalized</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Integrated with SA-RGCN and DGP, the full system achieved BLEU=47.85 and METEOR=45.80 on LDC2020T02 (mean over 4 seeds). Ablation: full model 47.85 BLEU; without DGP 47.68 BLEU; without SACA & DGP 47.20 BLEU; without StrucAdapt 45.43 BLEU (showing SACA contributes to the top performance). On generalization (GraphWriter integration) adding SACA improved BLEU from 14.13±0.10 to 15.59±0.35 on AGENDA (see Table 7 where SACA was applied).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to Graph-Summ (a multi-document summarization model that adds only one graph attention layer per decoder layer), SACA explicitly re-encodes the input graph conditioned on generated context and stacks multiple RGAT layers to allow deep interaction; SACA yields better specialized per-step representations than plain cross-attention and outperforms SA-RGCN alone in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Produces decoder-step-specific, structure-aware input graph contexts that improve faithfulness and performance, especially on complex graphs; can be stacked multiple layers; generalizes to RNN decoders (improves GraphWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces additional computation at each decoding step (minor but non-zero inference-time increase reported); relies on quality of encoder initial representations (SACA performs best when the encoder also encodes structure).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If encoder representations are poor (e.g., no structural adapter), SACA's gains are diminished; SACA alone without appropriate encoder structure gives smaller improvements. No catastrophic failure modes reported, but benefits are largest on complex graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8809.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8809.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Graph Pruning (DGP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time mechanism that applies learned gates to node connections in the joint graph to sparsify/prune irrelevant nodes dynamically based on the generated-text context, reducing distraction from redundant nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dynamic Graph Pruning (gated sparsification of joint graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each decoding step t, compute a gate g_v = sigmoid(W_g^T tanh(W_e h_v + W_d h^d_t)) for each node v (h_v is node representation, h^d_t is decoder context). Apply gate values multiplicatively to neighbor attention weights in RGAT (α_{v,u} <- g_u * raw_attention), thereby weakening edges of nodes with low gates (pruned). An L1 regularizer across gates encourages sparsity (Equation 9).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and Knowledge Graphs (token-level Levi graphs / joint graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a conversion to text per se, but a per-decode-step graph-structure modification: compute gates per node using decoder context, apply gates to attention weights in SACA RGAT layers, and optionally add L1 regularization to encourage pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (decoder-side pruning to improve generation relevance and faithfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation shows small but measurable improvements: full model BLEU=47.85; without DGP BLEU=47.68 (a drop of 0.17 BLEU) on LDC2020T02; human evaluation shows improved Authenticity over baseline SA-RGCN. In GraphWriter generalization experiments, adding DGP+SACA produced improvements over GraphWriter baseline (BLEU up to 15.59 ±0.35).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Unlike static encoders or plain SACA, DGP actively prunes irrelevant nodes during decoding; authors found naive gates remain near 1 without regularization, so they introduced an L1 penalty inspired by gated-attention work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces distraction from nodes that have already been described or are irrelevant, improving faithfulness and performance on complex graphs; introduces only minor parameter and inference-time increases.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Gate values initially tend to be ≈1 (no pruning) unless an additional sparsity regularizer (L1) is used; pruning decisions depend on gate learning which may be sensitive to hyperparameters (λ).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Model often doesn't learn to prune (gates ≈1) without the L1 regularizer; overly aggressive pruning (if λ too large) could remove needed context, and the paper uses λ tuning to avoid pruning too much.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8809.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8809.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphWriter+SACA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphWriter with SACA/DGP applied (generalization study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SACA and DGP to an existing RNN-based graph-to-text model (GraphWriter), showing the proposed decoder-side techniques generalize beyond PLMs and improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text generation from knowledge graphs with graph transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphWriter (graph-transformer/RNN encoder-decoder) augmented with SACA and DGP</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GraphWriter is a graph-to-text architecture with a multi-layer graph transformer encoder and attention-based decoder; the paper replaces GraphWriter's plain decoder cross-attention with SACA and inserts DGP before SACA to prune nodes dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge Graphs (KGs) used in AGENDA dataset</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>GraphWriter's encoder encodes KG structure (no sequence linearization); SACA/DGP are inserted at decoding time to re-encode and prune the encoder-produced node representations conditioned on decoder context.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (AGENDA dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reimplementation baseline GraphWriter: BLEU=14.13±0.10, METEOR=18.92±0.28, ROUGE-L=27.61±0.16. GraphWriter + SACA (+DGP) (Ours): BLEU=15.59±0.35, METEOR=19.70±0.21, ROUGE-L=28.47±0.14 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Applying SACA/DGP to GraphWriter yields a statistically meaningful improvement over the GraphWriter baseline, demonstrating that SACA/DGP generalize to RNN/transformer-based graph encoders beyond PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Demonstrates cross-architecture applicability: decoder-side re-encoding and pruning help RNN-based systems as well as PLM-based ones; provides consistent gains on AGENDA.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Integration requires changes in decoder cross-attention to run per-step RGATs and gating, adding complexity and some inference overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases reported for this integration; gains are modest on some datasets and require careful tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Structural adapters in pretrained language models for AMR-to-Text generation <em>(Rating: 2)</em></li>
                <li>One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 2)</em></li>
                <li>Graph transformer for graph-to-sequence learning <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning <em>(Rating: 1)</em></li>
                <li>A graph-to-sequence model for amr-to-text generation <em>(Rating: 1)</em></li>
                <li>Generating fluent descriptions for knowledge graph (G2T) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8809",
    "paper_id": "paper-252280335",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "LeviGraph",
            "name_full": "Levi graph conversion (edge-to-node transformation)",
            "brief_description": "A conversion that turns a labeled multi-relational directed graph into an unlabeled bipartite graph by turning each labeled edge into an intermediate relation node and adding reverse/default edge types; used as the basis for downstream token-level graph representations.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "use",
            "representation_name": "Levi graph (edge-as-node) conversion",
            "representation_description": "Transforms each labeled edge (v_i, r, v_j) of the original multi-relational directed graph G0 into two unlabeled edges (v_i, r) and (r, v_j) in a bipartite Levi graph G_l; additionally adds reverse edges for default edges so the Levi graph contains two relation types {d, r} (default and reverse). This preserves labeled relations by turning relation labels into explicit intermediate nodes.",
            "graph_type": "AMR graphs and Knowledge Graphs (multi-relational directed graphs)",
            "conversion_method": "For each labeled edge (v_i, r, v_j) create a relation-node r and edges (v_i, r) and (r, v_j); add reverse edges for default edges; resulting graph is an unlabeled bipartite Levi graph.",
            "downstream_task": "Graph-to-text generation (AMR-to-text and KG-to-text)",
            "performance_metrics": "Used as part of the pipeline for the paper's models; the full model (using Levi conversion + token-graph + SACA + DGP) achieved BLEU=47.85 and METEOR=45.80 on LDC2020T02 (reported mean over 4 seeds).",
            "comparison_to_others": "Levi conversion is the preprocessing step used by this paper and follows previous practice (cited Beck et al., 2018); compared to simple sequence linearization (FINETUNE/SPRING), Levi graph preserves relational structure explicitly rather than flattening it.",
            "advantages": "Explicitly preserves relation labels as nodes so relation information is available to graph encoders; keeps structural information instead of flattening into a sequence.",
            "disadvantages": "Introduces additional nodes (relation nodes), increasing graph size; requires GNN/RGAT-style encoders to exploit structure (not directly consumable by vanilla sequence LMs without further conversion).",
            "failure_cases": "No direct failure cases reported for Levi conversion alone; overall larger graphs (which Levi conversion increases further) can be more challenging—model performance degrades with very large or very noisy graphs unless further mechanisms (like SACA/DGP) are used.",
            "uuid": "e8809.0"
        },
        {
            "name_short": "TokenGraph",
            "name_full": "Token-level graph representation (token graph)",
            "brief_description": "A representation that converts each token of a node in the Levi graph into a separate node, producing a token-granular graph G that is directly used with PLM token embeddings and structural adapters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Token-level graph (token graph)",
            "representation_description": "Starting from the Levi graph G_l, each token belonging to a node in V_l is made into its own node in a new graph G=(V,E,R), so nodes correspond to tokens rather than higher-level graph items; this enables closer integration with token-based pretrained language models (PLMs).",
            "graph_type": "AMR graphs and Knowledge Graphs (represented at token granularity)",
            "conversion_method": "After Levi conversion, decompose node surface forms into tokens; create a node per token and retain the Levi graph connectivity among token-nodes (with relation types R inherited), producing a token graph aligned to PLM token inputs.",
            "downstream_task": "Graph-to-text generation with PLMs (T5-based models in experiments)",
            "performance_metrics": "Used in the paper's T5-based models; full model using token graph + SACA + DGP reported BLEU=47.85 and METEOR=45.80 on LDC2020T02 (mean over 4 seeds).",
            "comparison_to_others": "Compared implicitly with sequence-linearization approaches: token graph keeps graph topology at token level for adapters and SACA, while linearization (FINETUNE/SPRING) discards explicit topology and feeds a sequence to the LM.",
            "advantages": "Aligns graph nodes to PLM token-level inputs facilitating use of PLM token embeddings and structural adapters; preserves token-level connectivity and relation signals.",
            "disadvantages": "Expands number of nodes (token-level granularity increases graph size), which can raise computational cost for GNN/RGAT layers and attention operations.",
            "failure_cases": "Larger token graphs can lead to higher complexity; no single failure mode isolated, but empirical results show poorer performance on very large or very noisy graphs unless complemented by SACA/DGP.",
            "uuid": "e8809.1"
        },
        {
            "name_short": "Linearization (FINETUNE / SPRING)",
            "name_full": "Graph linearization / sequence serialization (FINETUNE; SPRING uses linearization for AMR)",
            "brief_description": "Flatten the input graph into a sequence of tokens and fine-tune a pre-trained sequence model (e.g., T5) on the serialized input instead of injecting structural bias.",
            "citation_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "mention_or_use": "mention",
            "representation_name": "Linearization / Serialization (sequence conversion)",
            "representation_description": "Convert the entire input graph into a serialized sequence (a linear string) and feed it to a text-to-text pretrained LM (e.g., T5) for fine-tuning; SPRING performs graph linearization for AMR tasks and FINETUNE denotes direct sequence fine-tuning of T5 on serialized graphs.",
            "graph_type": "AMR and Knowledge Graphs (flattened into sequences)",
            "conversion_method": "Custom graph linearization strategies (e.g., flattening triples or AMR graph traversal) to produce a single token sequence that represents nodes/relations; the exact linearization variant depends on method (SPRING proposes a specific AMR linearization).",
            "downstream_task": "Graph-to-text generation (AMR-to-text and KG-to-text)",
            "performance_metrics": "Paper reports that FINETUNE performed well on ENT-DESC (FINETUNE outperformed previous methods on ENT-DESC according to the paper), but exact numbers are not provided in-text for FINETUNE; by contrast, the paper's structured models further improve over structure-less FINETUNE in some settings.",
            "comparison_to_others": "Linearization (FINETUNE/SPRING) is simpler but discards explicit graph topology; the paper shows that injecting structural bias (SA-RGCN, SACA, DGP) typically improves faithfulness and performance on complex graphs compared to pure linearization.",
            "advantages": "Simple to implement; directly compatible with sequence PLMs (no graph encoder required); can perform competitively on some datasets (e.g., ENT-DESC).",
            "disadvantages": "Ignores explicit graph topology which can hurt performance on complex graphs; may allow the model to hallucinate relations not present in the original graph (less faithful).",
            "failure_cases": "On complex graphs with high reentrancy or large diameter, linearized approaches tend to underperform structure-aware methods; ENT-DESC shows sensitivity to noise where linearization may be less robust.",
            "uuid": "e8809.2"
        },
        {
            "name_short": "SA-RGCN",
            "name_full": "Structural Adapters in PLMs (SA-RGCN)",
            "brief_description": "An adapter-based method that injects graph structural bias into pretrained language model encoders by adding structural adapter layers that aggregate neighbor information per relation type.",
            "citation_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "mention_or_use": "use",
            "representation_name": "Structural adapter (RGCN-based adapter) for PLMs",
            "representation_description": "Adds a structural adapter after each Transformer encoder block that aggregates representations from immediate neighbors under each relation r using learnable relation-specific matrices W^l_r, followed by a gating/FFN adapter; this encodes graph topology into encoder token representations while freezing base PLM weights.",
            "graph_type": "AMR graphs and Knowledge Graphs (multi-relational)",
            "conversion_method": "Operates on token-level graph input (e.g., Levi/token graph); computes neighbor-aggregated features per relation at each adapter layer (Equations 1-2 in the paper) and injects these into PLM encoder representations.",
            "downstream_task": "AMR-to-text and KG-to-text generation using PLMs (T5 in the experiments)",
            "performance_metrics": "Used as the main baseline; the authors report that their final model (SA-RGCN + SACA + DGP) improves over their implemented SA-RGCN by +1.01 METEOR and +0.38 M-score on LDC2020T02; specific SA-RGCN absolute metrics are reported in tables (SA-RGCN reimplementation numbers shown in paper).",
            "comparison_to_others": "SA-RGCN injects structural bias into PLMs as an adapter approach and outperforms pure sequence FINETUNE in many cases; the paper's SACA/DGP components are built on top of SA-RGCN and further improve performance, showing that decoding-time structure-aware re-encoding adds complementary gains.",
            "advantages": "Encodes graph topology directly into PLMs without fine-tuning the base model (avoids catastrophic forgetting); effective baseline that improves over sequence-only methods.",
            "disadvantages": "Still relies on a static encoder representation computed once; the encoder may not capture all decoding-step-specific specialized information, motivating SACA re-encoding at decode time.",
            "failure_cases": "SA-RGCN alone is less effective on very complex graphs (high node count, large diameter, many reentrancies) compared to when combined with SACA/DGP; removing structural adapters reduces performance (paper's ablation shows drop when w/o StrucAdapt).",
            "uuid": "e8809.3"
        },
        {
            "name_short": "SACA",
            "name_full": "Structure-Aware Cross-Attention (SACA)",
            "brief_description": "A decoder-side mechanism that re-encodes the input graph at every decoding step by augmenting the graph with a generated-text context node and running relational graph attention (RGAT) so the decoder gets a structure-aware, context-conditioned graph context vector.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structure-Aware Cross-Attention (SACA) re-encoding",
            "representation_description": "At each decoding time step, create a joint graph by adding a node v_d representing the current generated-text context and connect it bidirectionally to all input graph nodes; run multi-layer relational graph attention (RGAT) over the joint graph to re-encode node representations conditioned on v_d; use the final v_d embedding as the structure-enhanced input-graph context for token prediction.",
            "graph_type": "AMR graphs and Knowledge Graphs (token-level Levi graphs)",
            "conversion_method": "Not a linearization — SACA performs a dynamic re-encoding: build joint graph (V ∪ {v_d}), connect v_d to every input node, run L layers of RGAT with relation embeddings E_r, compute attention scores and relation-aware neighbor aggregation (Equations 4–6), and fetch h^L_{v_d} as cross-attention context.",
            "downstream_task": "Graph-to-text generation (AMR-to-text and KG-to-text) in PLM decoders (T5) and in RNN decoders when generalized",
            "performance_metrics": "Integrated with SA-RGCN and DGP, the full system achieved BLEU=47.85 and METEOR=45.80 on LDC2020T02 (mean over 4 seeds). Ablation: full model 47.85 BLEU; without DGP 47.68 BLEU; without SACA & DGP 47.20 BLEU; without StrucAdapt 45.43 BLEU (showing SACA contributes to the top performance). On generalization (GraphWriter integration) adding SACA improved BLEU from 14.13±0.10 to 15.59±0.35 on AGENDA (see Table 7 where SACA was applied).",
            "comparison_to_others": "Compared to Graph-Summ (a multi-document summarization model that adds only one graph attention layer per decoder layer), SACA explicitly re-encodes the input graph conditioned on generated context and stacks multiple RGAT layers to allow deep interaction; SACA yields better specialized per-step representations than plain cross-attention and outperforms SA-RGCN alone in experiments.",
            "advantages": "Produces decoder-step-specific, structure-aware input graph contexts that improve faithfulness and performance, especially on complex graphs; can be stacked multiple layers; generalizes to RNN decoders (improves GraphWriter).",
            "disadvantages": "Introduces additional computation at each decoding step (minor but non-zero inference-time increase reported); relies on quality of encoder initial representations (SACA performs best when the encoder also encodes structure).",
            "failure_cases": "If encoder representations are poor (e.g., no structural adapter), SACA's gains are diminished; SACA alone without appropriate encoder structure gives smaller improvements. No catastrophic failure modes reported, but benefits are largest on complex graphs.",
            "uuid": "e8809.4"
        },
        {
            "name_short": "DGP",
            "name_full": "Dynamic Graph Pruning (DGP)",
            "brief_description": "A decoding-time mechanism that applies learned gates to node connections in the joint graph to sparsify/prune irrelevant nodes dynamically based on the generated-text context, reducing distraction from redundant nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Dynamic Graph Pruning (gated sparsification of joint graph)",
            "representation_description": "At each decoding step t, compute a gate g_v = sigmoid(W_g^T tanh(W_e h_v + W_d h^d_t)) for each node v (h_v is node representation, h^d_t is decoder context). Apply gate values multiplicatively to neighbor attention weights in RGAT (α_{v,u} &lt;- g_u * raw_attention), thereby weakening edges of nodes with low gates (pruned). An L1 regularizer across gates encourages sparsity (Equation 9).",
            "graph_type": "AMR graphs and Knowledge Graphs (token-level Levi graphs / joint graphs)",
            "conversion_method": "Not a conversion to text per se, but a per-decode-step graph-structure modification: compute gates per node using decoder context, apply gates to attention weights in SACA RGAT layers, and optionally add L1 regularization to encourage pruning.",
            "downstream_task": "Graph-to-text generation (decoder-side pruning to improve generation relevance and faithfulness)",
            "performance_metrics": "Ablation shows small but measurable improvements: full model BLEU=47.85; without DGP BLEU=47.68 (a drop of 0.17 BLEU) on LDC2020T02; human evaluation shows improved Authenticity over baseline SA-RGCN. In GraphWriter generalization experiments, adding DGP+SACA produced improvements over GraphWriter baseline (BLEU up to 15.59 ±0.35).",
            "comparison_to_others": "Unlike static encoders or plain SACA, DGP actively prunes irrelevant nodes during decoding; authors found naive gates remain near 1 without regularization, so they introduced an L1 penalty inspired by gated-attention work.",
            "advantages": "Reduces distraction from nodes that have already been described or are irrelevant, improving faithfulness and performance on complex graphs; introduces only minor parameter and inference-time increases.",
            "disadvantages": "Gate values initially tend to be ≈1 (no pruning) unless an additional sparsity regularizer (L1) is used; pruning decisions depend on gate learning which may be sensitive to hyperparameters (λ).",
            "failure_cases": "Model often doesn't learn to prune (gates ≈1) without the L1 regularizer; overly aggressive pruning (if λ too large) could remove needed context, and the paper uses λ tuning to avoid pruning too much.",
            "uuid": "e8809.5"
        },
        {
            "name_short": "GraphWriter+SACA",
            "name_full": "GraphWriter with SACA/DGP applied (generalization study)",
            "brief_description": "Application of SACA and DGP to an existing RNN-based graph-to-text model (GraphWriter), showing the proposed decoder-side techniques generalize beyond PLMs and improve performance.",
            "citation_title": "Text generation from knowledge graphs with graph transformers",
            "mention_or_use": "use",
            "representation_name": "GraphWriter (graph-transformer/RNN encoder-decoder) augmented with SACA and DGP",
            "representation_description": "GraphWriter is a graph-to-text architecture with a multi-layer graph transformer encoder and attention-based decoder; the paper replaces GraphWriter's plain decoder cross-attention with SACA and inserts DGP before SACA to prune nodes dynamically.",
            "graph_type": "Knowledge Graphs (KGs) used in AGENDA dataset",
            "conversion_method": "GraphWriter's encoder encodes KG structure (no sequence linearization); SACA/DGP are inserted at decoding time to re-encode and prune the encoder-produced node representations conditioned on decoder context.",
            "downstream_task": "KG-to-text generation (AGENDA dataset)",
            "performance_metrics": "Reimplementation baseline GraphWriter: BLEU=14.13±0.10, METEOR=18.92±0.28, ROUGE-L=27.61±0.16. GraphWriter + SACA (+DGP) (Ours): BLEU=15.59±0.35, METEOR=19.70±0.21, ROUGE-L=28.47±0.14 (Table 7).",
            "comparison_to_others": "Applying SACA/DGP to GraphWriter yields a statistically meaningful improvement over the GraphWriter baseline, demonstrating that SACA/DGP generalize to RNN/transformer-based graph encoders beyond PLMs.",
            "advantages": "Demonstrates cross-architecture applicability: decoder-side re-encoding and pruning help RNN-based systems as well as PLM-based ones; provides consistent gains on AGENDA.",
            "disadvantages": "Integration requires changes in decoder cross-attention to run per-step RGATs and gating, adding complexity and some inference overhead.",
            "failure_cases": "No explicit failure cases reported for this integration; gains are modest on some datasets and require careful tuning.",
            "uuid": "e8809.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "rating": 2,
            "sanitized_title": "structural_adapters_in_pretrained_language_models_for_amrtotext_generation"
        },
        {
            "paper_title": "One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline",
            "rating": 2,
            "sanitized_title": "one_spring_to_rule_them_both_symmetric_amr_semantic_parsing_and_generation_without_a_complex_pipeline"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Graph transformer for graph-to-sequence learning",
            "rating": 2,
            "sanitized_title": "graph_transformer_for_graphtosequence_learning"
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "rating": 1,
            "sanitized_title": "densely_connected_graph_convolutional_networks_for_graphtosequence_learning"
        },
        {
            "paper_title": "A graph-to-sequence model for amr-to-text generation",
            "rating": 1,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Generating fluent descriptions for knowledge graph (G2T)",
            "rating": 1,
            "sanitized_title": "generating_fluent_descriptions_for_knowledge_graph_g2t"
        }
    ],
    "cost": 0.02069375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph-to-Text Generation with Dynamic Structure Pruning
October 12-17, 2022</p>
<p>Liang Li liliang@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Cyber Security
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Ruiying Geng ruiying.gry@alibaba-inc.com 
DAMO Academy
Alibaba Group</p>
<p>Bowen Li 
Can Ma macan@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>DAMO Academy
Alibaba Group</p>
<p>Yinliang Yue yueyinliang@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>Binhua Li binhua.lbh@alibaba-inc.com 
DAMO Academy
Alibaba Group</p>
<p>Yongbin Li 
DAMO Academy
Alibaba Group</p>
<p>Graph-to-Text Generation with Dynamic Structure Pruning</p>
<p>Proceedings of the 29th International Conference on Computational Linguistic
the 29th International Conference on Computational LinguisticOctober 12-17, 20226115
Most graph-to-text works are built on the encoder-decoder framework with crossattention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.</p>
<p>Introduction</p>
<p>Data-to-text task aims to generate a natural language description from structural or semi-structural data, such as tables (Wiseman et al., 2017), Abstract Meaning Representation (AMR) graphs (Banarescu et al., 2013), and Knowledge Graphs (KG) (Cheng et al., 2020). It helps people get the key points of the input data and makes the stored information accessible to a broader audience of endusers. There have been several practical application scenarios in this field, such as biography generation (Lebret et al., 2016), basketball news generation (Wiseman et al., 2017), and advertising text generation (Shao et al., 2019). This paper focuses on generation from graph structures in AMR and KG, referred to as graph-to-text.</p>
<p>In recent years, encoder-decoder with the crossattention mechanism has been the de facto framework for graph-to-text tasks (shown in Figure 1(a)). Given an input graph, the encoder first computes vector representations for the graph nodes. On the decoding side, Input Graph (IG) context vector is obtained via cross-attention based on the partially Generated Text (GT) at each time step, then the next target token is finally predicted. Unlike conventional text-to-text tasks, the structural nature of the input graph makes it unsuitable to naively apply sequential encoder-decoder architecture to the graph-to-text task. To alleviate this issue, recent studies (Song et al., 2018;Damonte and Cohen, 2019;Cai and Lam, 2020) proposed to utilize the graph encoder to capture the input graph structure. These works have demonstrated that explicitly modeling the graph structure can bring benefits to the model performance.</p>
<p>Although equipped with the structure-aware modeling, it is still hard for the encoder to capture all specialized information for graph-to-text generation. It is evidenced by recent studies Li et al., 2021) that a vanilla structural encoder cannot capture the accurate semantic representation of the input structural data effectively. Auxiliary supervision has been shown to be helpful, but effective auxiliary tasks are not easy to design and may not generalize well to different datasets. We suspect that it is challenging for the encoder to encode all relevant information into node representations in a single forward pass for all the decoding steps, especially if the input graph structure is complex. Besides the encoder side, few works have focused on the decoder side for graph-to-text tasks. Considering the ordinary cross-attention mechanism, the representations of input data obtained from the encoder are still treated as an unordered node representation sequence. We conjuncture that this plain cross-attention does not take full advantage of the input graph structure and therefore may harm the model performance.</p>
<p>Current models with graph encoder and crossattention may yield inaccurate input graph context representation due to the deficiency on both encoder and decoder as we discussed before. To tackle the above problems and avoid introducing auxiliary tasks, we propose a novel Structure-Aware Cross-Attention (SACA) mechanism. Apart from the plain cross-attention, our SACA re-encodes the input graph conditioning on the newly generated context in a structure-aware fashion. Other than a single forward pass, specialized representations from the source side are built adaptively at each decoding step, which makes the decoder easily exploit relevant-only information for prediction. More specifically, as shown in Figure 1(b), we construct a joint graph, in which we explicitly treat the generated text context vector as an additional node and connect it with nodes in the input graph at each decoding step. We implement SACA using the relational graph attention network (RGAT, Shaw et al. 2018). Furthermore, we stack multiple layers of SACAs to perform deep interactions between the generated text context vector and input node representations. Finally, we fetch the node representation corresponding to the newly added node as the structure-enhanced input graph context to predict the target token.</p>
<p>In practice, we notice that some nodes become ir-relevant and uninformative as the decoding goes on. These nodes are distracting and can disturb the generation process. Intuitively, the decoder should dynamically discard the unrelated parts of the graph at different decoding steps. In other words, the joint graph structure should be dynamically adjusted. To this end, we adapt SACA and propose its variant Dynamic Graph Pruning (DGP) mechanism (shown in Figure 1(c)). DGP prunes the structure of the joint graph via the gate mechanism to achieve sparse connections between the nodes based on the generated text context.</p>
<p>We conduct experiments on two graph-to-text datasets, LDC2020T02 1 and ENT-DESC (Cheng et al., 2020), to verify the effectiveness of the proposed approach. Empirical results show that our proposed methods achieve new state-of-the-art results on the two datasets. Further experiments indicate that SACA and DGP do not reduce the diversity of the generated text and can better handle complex graphs. Meanwhile, additional investigation reveals that SACA and DGP only bring minor increase on the model size and inference time.</p>
<p>Related Works</p>
<p>Graph-to-text is a challenging task which aims at generating a descriptive text from the structured knowledge, such Knowledge Graph (KG), and Abstract Meaning Representation (AMR) graphs. It is helpful for interpretability of KGs in general (Schmitt et al., 2020) and knowledge-based question answering Fu et al., 2020;.</p>
<p>In recent years, most graph-to-text methods have been built based on the encoder-decoder architecture. This kind of method usually consists of a structural encoder and a decoder. The structural encoder aims to model the structure information into the representation of the input graph. Song et al. (2018) first propose the graph recurrent networks (GRNs) to encode the AMR node directly. And then, some works (Shi et al., 2020; introduce the Graph Neural Networks (GNNs) as the structural encoder, which updates the representations of nodes based on their immediate neighbors. To integrate both local and non-local features and learn a better structural representation of a graph, Guo et al. (2019) introduce the dense connection, allowing deeper GCNs. Unlike the local information aggregation scheme, ; Cai and Lam (2020) propose the Graph Transformer that uses explicit relation encoding and allows direct communication between two distant nodes.</p>
<p>A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ , also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step.</p>
<p>Recent approaches try to apply the Pre-trained Language Models (PLMs) (Kenton and Toutanova, 2019; Raffel et al., 2019) into the graph-to-text generation. Particularly, Ribeiro et al. (2021) propose to utilize the adapter method (Pfeiffer et al., 2020) to encode graph structure into PLMs and only train graph structure-aware adapter parameters. In this way, they avoid catastrophic forgetting while maintaining the topological structure of the graph.</p>
<p>Approach</p>
<p>We expect that developing graph-to-text generation should benefit from the recent advance on pre-trained language models (PLMs) (Lewis et al., 2020;Raffel et al., 2019). To explicitly encode the input graph structure into PLMs while alleviating the catastrophic forgetting problem, we consider SA- RGCN (Ribeiro et al., 2021) as our baseline model. SA-RGCN is an adapter method to encode graph structure into PLMs. The overall illustration of our model architecture is shown in Figure  2(a). In this section, we first introduce how to represent the input graph and the architecture of our baseline SA-RGCN. Then, we depict our proposed Structure-Aware Cross-Attention (SACA) in details. Lastly, we adapt SACA and propose its variant Dynamic Graph Pruning (DGP).</p>
<p>Graph Representation</p>
<p>Let G 0 = (V 0 , E 0 , R 0 ) denote a multi-relational and directed graph with nodes v i ∈ V 0 and labeled edges (v i , r, v j ) ∈ E 0 , where r ∈ R 0 is the relation type. Following previous work (Beck et al., 2018), we convert each input graph into a Levi graph G l = (V l , E l ), which is an unlabeled and con-nected bipartite graph. Specifically, each labeled edge (v i , r, v j ) ∈ E 0 is transformed into two unlabeled edges (v i , r), (r, v j ) ∈ E l . In addition, we add a reverse edge (v j , v i ) for each default edge (v i , v j ). Therefore, each Levi graph G l contains two type relations R l = {d, r}, where d and r denote the default and reverse edge, respectively. To better take advantage of the PLMs, we convert each G l into a new token graph G = (V, E, R), where each token of a node in V l becomes a node v ∈ V.</p>
<p>Pretrained LMs with Structural Adapters</p>
<p>To inject graph structural bias into PLMs, we incorporate the structural adapter (Ribeiro et al., 2021) into the PLMs encoder. As shown in Figure 2 (a), we add a structural adapter after each transformer encoder block on the encoder. Figure 2  Formally, at each layer l, given the encoder layer representation h l v , a structural adapter computes the representation for v by the following:
g l v = r∈R u∈Nr(v) 1 |N r (v)| W l r LN(h l v ), (1) z l v = W l e (σ(g l v )) + h l v ,(2)
where LN(·) denotes layer normalization. N r (v) is the sef of immediate neighbors under relation r ∈ R. W l r encodes the edge type between the nodes u and v. σ is the activation function.</p>
<p>We add an FNN adapter after each transformer decoder block to adapt the language model to the graph-to-text task. Given the outputĥ l v of the l th transformer decoder block, the adapter representation is computed as:
z l v = W l o (σ(W l p LN(ĥ l v ))) +ĥ l v ,(3)
where W l o and W l d denote learnable parameters.</p>
<p>Structure-Aware Cross-Attention</p>
<p>We argue that the input graph context representation obtained by the plain cross-attention may be inaccurate. The reason is twofold. First, it is not easy for the graph encoder to capture all specialized information required for generation in a single forward pass. Therefore, a single encoder without any auxiliary assistant may not be effective in capturing the accurate semantic representation (Liu  Li et al., 2021). In other words, the graph representation encoded by the graph encoder may be inaccurate. Second, during decoding, the decoder treats structural data as an unordered node sequence, which ignores the input graph structure. However, the graph structure has been proven to play an essential role in the graph representation and may offer clues about which nodes are more related to the generated text context. To tackle the above challenge, we propose a Structure-Aware Cross-Attention (SACA) mechanism, which re-encodes the input graph representation by conditioning on the newly generated context. Specifically, we first build a joint graph, in which we view the generated text (GT) context as a new node v d and explicitly connect it to each node in the input graph G at each decoding step. The corresponding reverse edges are also added. The joint graph can be formulated as
G joint = (V joint , E joint , R), where V joint = {v d } ∪ V and E joint = {(v i , v d ), (v d , v i ); v i ∈ V} ∪ E.
We use the representations from the encoder for the node from V and the hidden state from the last transformer decoder block as the representation for the GT context node.</p>
<p>To induce the representations for the nodes in the joint graph G joint and facilitate introducing Dynamic Graph Pruning (in Section 3.4), we consider graph neural network built on graph attention framework (GAT) (Shaw et al., 2018). Moreover, we employ the relational graph attention network (RGAT) implemented by Shaw et al. (2018) to model the relation between neighbor nodes. Specifically, at each RGAT layer l, we update the repre-
sentation h l v of each node v ∈ G joint by: s v,u = W q h l v T (W k h l u + E r R(v,u) ) √ m ,(4)α v,u = e sv,u u ′ ∈Nv e s v,u ′ ,(5)h l+1 v = σ( u∈Nv α v,u W v h l u ),(6)
where E r R(v,u) means the embedding of the relation between node v and u. m denotes the hidden dimension of RGAT. Finally, the representation vector h L v d corresponding to the GT context node v d is fetched and used as the structure-enhanced input graph context vector for token prediction.</p>
<p>In conclusion, SACA provides two advantages. First, it re-encodes the input graph by conditioning its representation on the newly generated context. As a result, we build specialized representations which make it easier for the decoder to exploit relevant-only information for prediction at each decoding step. Second, the re-encoding explicitly injects structural bias into input graph context modeling, helping the decoder obtain a more accurate input graph context vector. The proposed SACA can be plugged after the last transformer decoder block as shown in Figure 2 (a).</p>
<p>Dynamic Graph Pruning</p>
<p>In practice, we notice that some nodes become irrelevant and uninformative as the decoding goes on. These unrelated nodes are distracting and can even disturb the subsequent generation. Intuitively, the decoder should dynamically prune the joint graph at different decoding steps. For this purpose, we adapt SACA and propose its variant Dynamic Graph Pruning (DGP) mechanism, which aims to dynamically drop the redundant nodes in the joint graph according to the generated text during decoding. The DGP employs the gate mechanism to sparse the connection between a node and its immediate neighbors in the joint graph to achieve graph pruning. Specifically, at each decoding step t, for each node v in the joint graph, we formulate its gate as bellow:
g v = sigmoid(W T g tanh(W e h v + W d h d t )),(7)
where W g , W e , and W d are learnable parameters. And h v is the representation of node v and h d t is the decoder hidden state at decoding step t, which is usually considered as the representation of the generated text context. The value of gate g v ∈ R decides whether the node v i should be dropped or not. Correspondingly, we apply the gate value to multiple SACA layers invariably by modifying the attention weights in SACA (Equation 5) as follows:
α v,u = g u ⊙ e sv,u u ′ ∈Nv g u ′ ⊙ e s v,u ′ .(8)
Intuitively, if the value of gate g u is close to 0, the connections between node u with all its immediate neighbors will be largely weaken. That is, the node is removed from the joint graph. Specifically, the attention score α v,u measures the relevance between any two nodes, v and u, in the joint graph, while the gate g v models the relevance between the node v and the generated text context h t .</p>
<p>As a shown example in Figure 2 (c), the red node represents the main entity. Initially, the main entity connects with all its neighbor nodes. As the decoding goes on, some nodes are redundant for the subsequent generation. For example, the nodes "actor" has been described, and node "voice actor" is also covered by the generated text. Therefore, DGP discards these nodes by giving them gates with small values.</p>
<p>We observed that the values of the gates calculated by Equation 7 are almost equal to 1, indicating that the model does not actively learn to prune a graph. Inspired by Xue et al. (2020), we further introduce a regularization item, encouraging the network to turn off more gates and generate more sparse connections between nodes in the in- put graph. We formulate it as follows:
L DGP = |y| t=1 ∥Gate t ∥ 1 |y| ,(9)
where Gate t = {g v |v ∈ V}. ∥ * ∥ 1 means L1 norm regularizer.</p>
<p>Training</p>
<p>Given a reference output y = {y 1 , y 2 , ..., y T } and an input graph G, we use the cross-entropy loss as the objective function of graph-to-text generation:
L lm = |y| t=1
log p(y t |y 1:t−1 , G, θ).</p>
<p>Finally, the overall objective function consists of two parts:
L = L lm + λL DGP ,(11)
where λ is a tunable hyper-parameter and is used to make a trade-off between the cross-entropy loss and the regularization item. Intuitively, the L DGP object encourages the model to learn how to prune the graph, and the L lm trains the model to generate the text according to the graph and restrains DGP from pruning too much.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>We demonstrate the effectiveness of our models on two graph-to-text datasets: LDC2020T02 and ENT-DESC (Cheng et al., 2020)   target text consists of sentences that verbalize the main entity in KG. ENT-DESC lacks explicit alignment between the input and the output. Therefore, some knowledge in the input graph may be noise. We follow official training, development, and test splits of 88,650/11,081/11,081 instances. Table 1 summarizes the detailed statistics of LDC2020T02 and ENT-DESC.</p>
<p>Settings</p>
<p>Our implementation is based on Hugging Face (Wolf et al., 2019). The RGCN and RGAT are implemented based on PyTorch Geometric (Fey and Lenssen, 2019). We initialize our models by T5 (Raffel et al., 2019). To make a fair comparision, we following the same experimental setting with SA-RGCN (Ribeiro et al., 2021). We set the hidden dimensions of both Structural Adapter and SACA to 256. And we use T5 base for all experiments on ENT-DESC and T5 large on LDC2020T02 for a fair comparison with baselines. We use the AdamW optimizer (Loshchilov and Hutter, 2018) and employ a linearly decreasing learning rate schedule without warm-up. The learning rate is fixed as 10 −4 . We set the training batch size as 4 for all experiments. We freeze the T5 parameters and only update the newly added parameters during training. We tune the hyper-parameter λ in Equation 11 from the set [1 −2 , 5 −3 , 1 −3 , 5 −4 ], and select the best one on the development set. We stack L = 2 RGAT layers in Structure-Aware Cross-Attention. During de-coding, we use beam search with a beam size 5. We use BLEU (Papineni et al., 2002) for the early stopping criterion. All experiments are trained on Nvidia Tesla V100 32GB GPUs. Following previous works, on both datasets, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and ChRF++ (Popović, 2015) on both datasets. On LDC2020T02, following Ribeiro et al. (2021), we utilize the meaning (M) component of the MF-score (Opitz and Frank, 2021) to measure how well the source AMR graph can be reconstructed from the generated sentence (refer to A.1 for more details). We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. On ENT-DESC, We add ROUGE-L (Lin, 2004) and employ PARENT (Dhingra et al., 2019) for evaluating the faithfulness. We conduct experiments over 4 different seeds and report the average scores on them.</p>
<p>Main Results</p>
<p>We compare our method with recent state-of-theart methods (please refer to A.4 for more details).  </p>
<p>Analysis and Discussion</p>
<p>Ablation Study The overall performance on the two datasets shows the superiority of our proposed Structure-Aware Cross-Attention (SACA) and Dynamic Graph Pruning (DGP  as well as our method are all initialized by T5. And the AMR paring and AMR-to-Text are dual tasks actually. Therefore, the M score is biased and the results of our model and ADAPT are somehow inflated. Additionally, we utilize Distinct-1 and Distinct-2 (Li et al., 2016) to evaluate the diversity of the output text. We observe that SACA and DGP have little effect on Distinct-1 and Distinct-2. This implies that they will not reduce the diversity of the output text. We notice that, compared with ADAPT, w/o StrucAdapt shows a slight improvement. This indicates it is necessary to explicitly model the graph structure in the encoder, even though structural bias has been injected into the input graph context modeling during decoding. We believe this may be attributed to SACA relying on the input graph representation encoded by the encoder. Because our SACA is designed to exploit the relevant-only information for prediction, it re-encodes the input graph by conditioning its representation on the newly generated context. Therefore, the initial representation for the input graph is important.</p>
<p>Impact on the parameter and speed Furthermore, we investigate the impact of SACA and DPG on the model parameters and inference speed on LDC2020T02 development. Specifically, we calculate the additional parameters of each model with respect to T5 large . And we set the batch size to 1 to calculate the average decoding time for generating all examples. The results summarized in Table 4 indicate that SACA and DGP only bring minor increase on the model size and inference time.</p>
<p>Impact on the Graph Properties To examine the robustness of our proposed methods, we investigate the model's performance concerning different graph properties (graph size, graph diameter, and reentrancies) on LDC2020T02 and ENT-DESC. Following previous works (Cheng et al., 2020;Ribeiro et al., 2021), we use BLEU as the metric. The results are summarized in Table 5 and Table 6, respectively. For LDC2020T02, we firstly note that the BLEU scores decrease as the graph size increases since the larger graph is often  complex. Our method achieves a clear improvement when handling graphs with &gt; 30 nodes. And then we observe that the BLEU gap between our method and SA-RGCN becomes larger for a relatively larger graph diameter. Reentrancies are the nodes with multiple parents. A graph with more reentrancies is typically more complex . As shown in the last section in Table 5, our method has an improvement of +1.23 BLEU points compared to SA-RGCN when graphs contain &gt; 2 reentrancies. To sum up, the results on the LDC2020T02 dataset show the advantage of our model in dealing with the AMR graph with more complex structures. As shown in Table 6, both models perform differently on ENT-DESC than on LDC2020T02. First, we notice that both models perform the best when the graph size is between 31 and 50, and they perform poorly when the graph size is too small or too large. Cheng et al. (2020) also observed the finding, and they believe this is due to the insufficient or very noisy input information for generation. Additionally, both models perform better when graph diameter or number of the reentrancies increase. The reason is that, in the ENT-DESC, the knowledge graph with a small diameter or number of the reentrancies contains more noisy information for the generation. Please refer to A.2 for more details. The BLEU gap between our method and SA-RGCN is the largest when the graph diameter &gt; 5 or the number of reentrancies &gt; 10. The above results demonstrate that our approach makes SA-RGCN better at handling complex knowledge graphs.</p>
<p>We investigate how the model behaves on different types of graphs (AMR and KG). And the results demonstrate that our model deals better with   complex structures. We believe the improvement comes from two aspects. First, on the one hand, it is challenging for an encoder to encode all relevant information into node representations in a single forward pass, especially if the graph structure is complex. On the other hand, the re-encoding in SACA makes the decoder easily exploit the relevant-only information for prediction and explicitly injects the structural information at each decoding step. Second, DGP dynamically removes the nodes which are redundant for the subsequent generation, which makes the decoder pay more attention to the relevant nodes.</p>
<p>Generalization Study</p>
<p>Institutionally, our proposed methods can not only be applied to PLMs but also RNN based models.</p>
<p>In other words, we can easily combine the SACA and DGP with previous RNN based works. To examine the generalization of SACA and DGP, we choose GraphWriter (Koncel-Kedziorski et al., 2019) as the baseline, which consists of a multilayer graph transformer encoder and an attentionbased decoder with a copy mechanism. Further, to make a fair comparison, we conduct the generalization experiment on AGENDA dataset (Koncel-Kedziorski et al., 2019). We simply replace the plain cross-attention in GraphWriter with our proposed SACA. Additionally, we add the DGP layer before the SACA. The experiments are under the same settings as described in GraphWriter. As shown in Table 7, we observe that our proposed model significantly improves the performance of GraphWriter. The results indicate that SACA and DGP are not only effective well on PLMs-based models but also potent for RNN-based models.</p>
<p>Human Evaluation</p>
<p>Considering that the knowledge graph is more readable than AMR, we do human evaluations on the ENT-DESC test set to examine whether human judgments corroborate improvements in automatic evaluation metrics. Following Cheng et al. (2020), from outputs generated by the baseline model SA-RGCN and our final model (Ours). We distribute the outputs of different systems to three annotators with linguistic backgrounds. The annotators have no knowledge in advance about which model the generated text comes from. Specifically, we give each participant all main entities' neighbors, 1-hop and 2-hop connections between main entities, and topic-related entities as references. They are required to score the generated text from 1 to 5 in terms of three criteria: Fluency (is the sentence fluent?), Grammar (is the sentence grammatical?), and Authenticity (is the sentence more related to the input graph?). For each criterion, we calculate the final score by averaging the scores from all annotators. As shown in Figure 3, our model outperforms the baseline SA-RGCN on Fluency and Grammar metrics. For Authenticity, the improvement is more significant. The performance validates the benefit of our proposed SACA and DGP modules in capturing more accurate input graph context representations. We supply a case study in A.3.</p>
<p>Conclusions</p>
<p>In this work, we make two main contributions. First, we propose Structure-Aware Cross-Attention (SACA) to make decoder easily exploit relevant-only information for prediction. Apart from the plain cross-attention, SACA re-encodes the input graph conditioning on the newly generated context while explicitly considering the input graph structure. The second one is that we adapt SACA and propose its variant Dynamic Graph Pruning (DGP) mechanism. In detail, the DGP dynamically prunes the structure of the joint graph at different decoding steps according to the generated text. Experimental results conducted on two graph-to-text datasets, LDC2020T02 and ENT-DESC, show the effectiveness of our method. The empirical and analysis results on both datasets show that the proposed methods can improve the model's performance on complex graphs while only bringing minor increase on the model size and inference time.</p>
<p>A Appendix</p>
<p>A.1 MF-score</p>
<p>The M (Meaning Preservation) component of the MF-score (Opitz and Frank, 2021) is utilized to measure how well the source AMR graph can be reconstructed from the generated sentence. It reconstructs the AMR with a SOTA parser and computes the relative graph overlap of the reconstruction and the source AMR using graph matching. M employs the python library amrlib 2 (version 0.5.0) to make AMR parse, where the parser is a T5-based model.</p>
<p>A.2 Distribution on Graph Size</p>
<p>On the ENT-DESC test set, previous study (Cheng et al., 2020) and our experimental results (in Table 6) suggest that the model performs the best when the graph size lies in the range of 21 − 40 and has a poorer performance when the number of triples is too small or too large. It should be due to the fact that the input information is insufficient or very noisy. However, we find that the model performance increases as the graph diameter and reentrancies increase. For further investigation, we calculate the distribution of graph diameter and reentrancies broken down by graph size, respectively. The results are summarized in Figure 4. As shown in Figure 4(a), the proportion of graphs with size 21 − 40 increases as the graph diameter increases. As shown in Figure 4(b), the results on graph reentrancy follow a pattern similar to graph diameter. In a word, in ENT-DESC, the noise decreases as the graph diameter and reentrancies increase, so the model performs better.</p>
<p>A.3 Case Study</p>
<p>As shown in Figure 5, we further take a typical example from our human study to better understand how our method improves the mode's performance. Given the Knowledge Graph containing the main entity "Andrew Lawrence" and all its related entities, we aim to generate a description about the main entity. We notice that both the baseline and our model can identify the main entity. However, the baseline outputs a sentence describing the relation between "Andrew Lawrence" and "Matthew Lawrence". The relation is not existing in the input graph. Moreover, it repeatedly generates the entity "Brotherly Love" and misses the related entity "Recess". Compared with it, our model generates the 2 https://github.com/bjascob/amrlib/tree/0.5.0 sentences faithful to the input graph and correctly covers the main entity and most topic-related entities. We consider this is because the SACA helps the decoder obtain a more accurate input graph context, and the DGP removes the redundant nodes as the decoding stage progresses.</p>
<p>A.4 Baseline Models</p>
<p>On the AMR-to-Text task LDC2020T02, we compare our method with several baselines including:</p>
<p>• LDGCN (Zhang et al., 2020b) is a a dynamic fusion mechanism, which captures richer nonlocal interactions by synthesizing higher order information from the input graphs. A weight tied convolutions to reduce memory usage is applied.</p>
<p>• SPRING (Bevilacqua et al., 2021) casts Textto-AMR and AMR-to-Text as a symmetric transduction task and proposes a graph linearization and extending a pretrained encoderdecoder model.</p>
<p>On the KG-to-Text task ENT-DESC, we compare our method with several baselines including:</p>
<p>• s2s (Bahdanau et al., 2015) is a encoderdecoder based model, which allows a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.</p>
<p>• GraphTransformer (Koncel-Kedziorski et al., 2019) introduces a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints.</p>
<p>• GRN (Beck et al., 2018) couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations.</p>
<p>• GCN (Marcheggiani and Perez-Beltrachini, 2018) proposes an alternative encoder based on graph convolutional networks that directly exploits the input structure.</p>
<p>• DeepGCN (Guo et al., 2019) introduces a dense connection strategy, which is able to integrate both local and non-local features to </p>
<p>Reentrancies</p>
<p>Graph Size &gt;40</p>
<p>Graph Size 21-40</p>
<p>Graph Size 1-20 (b) The distribution of graph reentrancies by graph size. Figure 4: The clustered column charts of graph diameter and reentrancies by graph size.</p>
<p>Gold</p>
<p>Andrew James Lawrence (born January 12, 1988) is an American actor and singer. He is known for his roles as Andy Roman in "Brotherly Love" and T.J. Detweiler in "Recess". SA-RGAT Andrew Lawrence (born January 12, 1988) is an American actor, voice actor, and singer. He is best known for his roles in the films "Brotherly Love" and "Brotherly Love". He is the younger brother of Matthew Lawrence.</p>
<p>Ours</p>
<p>Andrew Lawrence (born January 12, 1988) is an American actor and singer. He is best known for his roles in "Recess" and "Brotherly Love". learn a better structural representation of a graph.</p>
<p>• MGCN + CNN (Cheng et al., 2020) is a multi-graph structure that is able to represent the original graph information more comprehensively. We do not report the results of MGCN + CNN + delex. Because it applies the delexicalization technique on the ENT-DESC dataset, which delexicalizes the main entity and topic-related entities by replacing these entities with tokens indicating the entity types and indices. The delexicalization technique greatly boosts their performance on ROUGE-L. They do not release the code about delexicalization, and we can not reproduce it.</p>
<p>What's more, FINETUNE, ADAPT and SA-RGCN are T5-based models proposed in (Ribeiro et al., 2021).</p>
<p>Figure 1 :
1(a) denotes an encoder-decoder framework with the cross-attention mechanism where IG and GT contexts denote the input graph and generated text graph contexts, respectively. (b) is an example of Structure-Aware Cross-Attention. The dotted lines in (c) denote the pruned edges and nodes.</p>
<p>(b) illustrates the architecture of a structural adapter, in where a relational GCN (RGCN) (Schlichtkrull et al., 2018) layer computes the node representation based on the local neighborhood of node v ∈ V.</p>
<p>Figure 2 :
2Lawrence … known for his roles in " Recess" … Andrew Lawrence … is an American actor and singer. He is best known for his roles in " Recess" and …Feed-Forward Layer Illustration of the proposed model architecture. (a) is an overview of our model. (b) is the architecture of a structural adapter. (c) is an example of Dynamic Graph Pruning, where r 1 ∼ r 4 denote the relations: "country of citizenship", "occupation", "sibling", and "cast member", respectively. The dummy lines in (c) denote the pruned edges. et al., 2019;</p>
<p>Figure 3 :
3Human evaluation results on ENT-DESC test set.</p>
<p>Figure 5 :
5An example of generated sentences. The main entity is highlighted in red, topic-related entities are highlighted in blue, and the sentence that is not faithful to the input graph is in green.</p>
<p>Table 2 :
2Main results of models on LDC2020T02 and ENT-DESC test datasets. ‡ means our reimplementation. The other results are copied from the original paper. Mean (±s.d.) over 4 seeds.</p>
<p>Table 2
2summarizes the results on LDC2020T02 and ENT-DESC test sets. FINETUNE is a method that transforms the input graph into a sequence and finetunes T5 directly. It does not consider the input graph structure. For LDC2020T02, our method outperforms the previous state-of-the-art model by 0.78 BLEU and 1.15 ChRF++. Com-Models 
BLEU METEOR 
M 
Dis-1 Dis-2 
GOLD 
-
-
81.00 23.82 71.76 
ADAPT 
45.22 
43.28 
79.56 23.20 71.40 
Ours 
47.85 
45.80 
80.37 23.46 71.75 
w/o DGP 
47.68 
45.51 
80.21 23.51 72.08 
w/o SACA &amp; DGP 47.20 
45.05 
80.01 23.38 71.69 
w/o StrucAdapt 
45.43 
43.54 
79.75 23.32 71.65 </p>
<p>Table 3 :
3And our model exceeds all previous works and achieves new state-of-the-art results on all metrics. The above results indicate that our proposed methods can improve the model on fluency and faithfulness.Ablation study of models on LDC2020T02 
development dataset. GOLD indicates the ground-truth 
sentences. Dis-1 and Dis-2 denote Distinct1 and Dis-
tinct2, respectively. </p>
<p>pared with our implemented SA-RGCN, we im-
prove 1.01 METEOR. Moreover, our method raises 
0.38 M, which indicates that it can generate more 
faithful sentences to the input graphs. The improve-
ment on BERTScore shows that the sentence gener-
ated by our method is more similar to the ground 
truth on the semantic level. For ENT-DESC, we 
notice FINETUNE performs better than all previ-
ous methods. SA-RGCN, which encodes graph 
structure into T5, furtherly improves the perfor-
mance. </p>
<p>Table 4 :
4Impact on parameter and speed.</p>
<p>Table 5 :
5BLEU scores with respect to graph size, graph 
diameter and number of reentrancies on LDC2020T02 
test set. </p>
<p>Table 6 :
6BLEU scores with respect to graph size, graph 
diameter and number of reentrancies on ENT-DEST test 
set. </p>
<p>Models 
BLEU 
METEOR 
ROUGE-L 
GraphWriter 
14.30 
18.80 
-
GraphWriter  ‡ 14.13 ± 0.10 18.92 ± 0.28 27.61 ± 0.16 
Ours 
15.59 ±0.35 
19.70 ±0.21 
28.47 ±0.14 </p>
<p>Table 7 :
7Generalization Study on AGENDA test dataset. ‡ means our reimplementation.</p>
<p>(a) The distribution of graph diameter by graph size.Graph Diameter 
Graph Size 
Reentrancies 
Graph Size </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>1-3 
4-5 </p>
<blockquote>
<p>5 </p>
</blockquote>
<h1>Example</h1>
<p>Graph Diameter </p>
<p>Graph Size 1-20 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.58% </p>
<p>46.84% </p>
<p>52.81% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>Graph Diameter </p>
<p>Graph Size &gt;40 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size 1-20 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1-3 
4-5 </p>
<blockquote>
<p>5 </p>
</blockquote>
<h1>E</h1>
<p>Graph Diameter </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.58% </p>
<p>46.84% </p>
<p>52.81% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>Graph Diameter </p>
<p>Graph Size &gt;40 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size 1-20 </p>
<p>21-40 </p>
<blockquote>
<p>40 
0.31773 
0.06695 
0.498832 0.191032 
0.553757 0.427306 
0.45745 
0.22137 </p>
</blockquote>
<p>21-40 </p>
<blockquote>
<p>40 
0.315766 0.075538 
0.468407 0.199322 
0.528123 0.338262 </p>
</blockquote>
<p>Graph Size </p>
<p>Graph Size </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<h1>Example</h1>
<p>Reentrancies </p>
<p>Graph Size 1-20 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.77% </p>
<p>49.88% </p>
<p>55.38% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>Reentrancies </p>
<p>Graph Size &gt;40 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size 1-20 </p>
<p>21-40 </p>
<blockquote>
<p>40 
0.315766 0.075538 
0.468407 0.199322 
0.528123 0.338262 </p>
</blockquote>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<h1>Example</h1>
<p>Reentrancies </p>
<p>Graph Size 1-20 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.77% </p>
<p>49.88% </p>
<p>55.38% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>https://catalog.ldc.upenn.edu/LDC2020T02</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyung Hyun Cho, Yoshua Bengio, Proc. of ICLR. of ICLRDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR.</p>
<p>Abstract meaning representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th linguistic annotation workshop and interoperability with discourse. the 7th linguistic annotation workshop and interoperability with discourseLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th linguis- tic annotation workshop and interoperability with discourse.</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, In ACLDaniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In ACL.</p>
<p>One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline. Michele Bevilacqua, Rexhina Blloshmi, Roberto Navigli, Proc. of AAAI. of AAAIMichele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One SPRING to rule them both: Sym- metric AMR semantic parsing and generation without a complex pipeline. In Proc. of AAAI.</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, Proc. of AAAI. of AAAIDeng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. In Proc. of AAAI.</p>
<p>Kgpt: Knowledge-grounded pretraining for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang, Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020. Kgpt: Knowledge-grounded pre- training for data-to-text generation.</p>
<p>ENT-DESC: Entity description generation by exploring knowledge graph. Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, Luo Si, EMNLP. Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, and Luo Si. 2020. ENT- DESC: Entity description generation by exploring knowledge graph. In EMNLP.</p>
<p>Structural neural encoders for amr-to-text generation. Marco Damonte, Shay B Cohen, Proc. of AACL. of AACLMarco Damonte and Shay B Cohen. 2019. Structural neural encoders for amr-to-text generation. In Proc. of AACL.</p>
<p>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. Michael Denkowski, Alon Lavie, Proceedings of the sixth workshop on statistical machine translation. the sixth workshop on statistical machine translationMichael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Pro- ceedings of the sixth workshop on statistical machine translation.</p>
<p>. Bhuwan Dhingra, Manaal Faruqui, Ankur P Parikh, Ming-Wei Chang, Dipanjan Das, William W , Bhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh, Ming-Wei Chang, Dipanjan Das, and William W.</p>
<p>Handling divergent reference texts when evaluating table-to-text generation. Cohen, Proc. of ACL. of ACLCohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proc. of ACL.</p>
<p>Fast graph representation learning with PyTorch Geometric. Matthias Fey, Jan E Lenssen, ICLR Workshop on Representation Learning on Graphs and Manifolds. Matthias Fey and Jan E. Lenssen. 2019. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds.</p>
<p>Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, Jian Sun, arXiv:2007.130692020. A survey on complex question answering over knowledge base: Recent advances and challenges. arXiv preprintBin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. 2020. A survey on complex question answering over knowledge base: Recent advances and challenges. arXiv preprint arXiv:2007.13069.</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, Transactions of the Association for Computational Linguistics. Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional net- works for graph-to-sequence learning. Transactions of the Association for Computational Linguistics.</p>
<p>S 2 SQL: Injecting syntax to question-schema interaction graph encoder for text-to-SQL parsers. Binyuan Hui, Ruiying Geng, Lihan Wang, Yanyang Bowen Qin, Bowen Li, Jian Li, Yongbin Sun, Li, Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsBinyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Yanyang Li, Bowen Li, Jian Sun, and Yongbin Li. 2022. S 2 SQL: Injecting syntax to question-schema interaction graph encoder for text-to-SQL parsers. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1254-1262, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proc. of AACL. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanovaof AACLJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec- tional transformers for language understanding. In Proc. of AACL.</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, Proc. of AACL. of AACLRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph trans- formers. In Proc. of AACL.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, EMNLP. Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with ap- plication to the biography domain. In EMNLP.</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, and comprehensionMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020. Bart: De- noising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.</p>
<p>A diversity-promoting objective function for neural conversation models. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, Proc. of NAACL. of NAACLJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objec- tive function for neural conversation models. In Proc. of NAACL.</p>
<p>Improving encoder by auxiliary supervision tasks for table-to-text generation. Liang Li, Can Ma, Yinliang Yue, Dayong Hu, ACL. Liang Li, Can Ma, Yinliang Yue, and Dayong Hu. 2021. Improving encoder by auxiliary supervision tasks for table-to-text generation. In ACL.</p>
<p>Leveraging graph to improve abstractive multi-document summarization. Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du, Proc. of ACL. of ACLWei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and Junping Du. 2020. Leveraging graph to improve abstractive multi-document summarization. In Proc. of ACL.</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.</p>
<p>Hierarchical encoder with auxiliary supervision for neural tableto-text generation: Learning better representation for tables. Tianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, Zhifang Sui, Proc. of AAAI. of AAAITianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, and Zhifang Sui. 2019. Hierarchical encoder with auxiliary supervision for neural table- to-text generation: Learning better representation for tables. In Proc. of AAAI.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In ICLR.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Gen- eration.</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from AMR. Juri Opitz, Anette Frank, Proc. of EACL. of EACLJuri Opitz and Anette Frank. 2021. Towards a decom- posable metric for explainable evaluation of text gen- eration from AMR. In Proc. of EACL.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, ACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In ACL.</p>
<p>MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder, EMNLP. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Se- bastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In EMNLP.</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. Maja Popović, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationMaja Popović. 2015. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation.</p>
<p>Binyuan Bowen Qin, Lihan Hui, Min Wang, Jinyang Yang, Binhua Li, Ruiying Li, Rongyu Geng, Jian Cao, Luo Sun, Si, arXiv:2208.13629A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprintBowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, et al. 2022. A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprint arXiv:2208.13629.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.</p>
<p>Structural adapters in pretrained language models for AMR-to-Text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, Proc. of EMNLP. of EMNLPLeonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021. Structural adapters in pretrained language models for AMR-to-Text generation. In Proc. of EMNLP.</p>
<p>Modeling relational data with graph convolutional networks. Michael Sejr, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, The Semantic Web -15th International Conference. Heraklion, Crete, GreeceProceedingsMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In The Semantic Web -15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings.</p>
<p>An unsupervised joint system for text generation from knowledge graphs and semantic parsing. Martin Schmitt, Sahand Sharifzadeh, Hinrich Volker Tresp, Schütze, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsMartin Schmitt, Sahand Sharifzadeh, Volker Tresp, and Hinrich Schütze. 2020. An unsupervised joint sys- tem for text generation from knowledge graphs and semantic parsing. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7117-7130, Online. As- sociation for Computational Linguistics.</p>
<p>Long and diverse text generation with planning-based hierarchical variational model. Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, EMNLP-IJCNLP. Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. 2019. Long and diverse text gen- eration with planning-based hierarchical variational model. In EMNLP-IJCNLP.</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, Proc. of NAACL. of NAACLPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. of NAACL.</p>
<p>G2t: Generating fluent descriptions for knowledge graph. Yunzhou Shi, Zhiling Luo, Pengcheng Zhu, Feng Ji, Wei Zhou, Haiqing Chen, Yujiu Yang, Proc. of SIGIR. of SIGIRYunzhou Shi, Zhiling Luo, Pengcheng Zhu, Feng Ji, Wei Zhou, Haiqing Chen, and Yujiu Yang. 2020. G2t: Generating fluent descriptions for knowledge graph. In Proc. of SIGIR.</p>
<p>A graph-to-sequence model for amrto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, ACL. Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr- to-text generation. In ACL.</p>
<p>Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing. Lihan Wang, Binyuan Bowen Qin, Bowen Hui, Min Li, Bailin Yang, Binhua Wang, Jian Li, Fei Sun, Luo Huang, Si, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningLihan Wang, Bowen Qin, Binyuan Hui, Bowen Li, Min Yang, Bailin Wang, Binhua Li, Jian Sun, Fei Huang, Luo Si, et al. 2022. Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1889-1898.</p>
<p>Amr-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, Transactions of the Association for Computational Linguistics. Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. Amr-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics.</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart M Shieber, Alexander M Rush, Proc. of EMNLP. of EMNLPSam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in data-to-document genera- tion. In Proc. of EMNLP.</p>
<p>Huggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.03771arXiv preprintThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of- the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Not all attention is needed: Gated attention network for sequence data. Lanqing Xue, Xiaopeng Li, Nevin L Zhang, Proc. of AAAI. of AAAILanqing Xue, Xiaopeng Li, and Nevin L Zhang. 2020. Not all attention is needed: Gated attention network for sequence data. In Proc. of AAAI.</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Proc. of ICLR. of ICLRTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: Eval- uating text generation with BERT. In Proc. of ICLR.</p>
<p>Lightweight, dynamic graph convolutional networks for amr-to-text generation. Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, B Shay, Zuozhu Cohen, Lidong Liu, Bing, EMNLP. Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B Cohen, Zuozhu Liu, and Lidong Bing. 2020b. Lightweight, dynamic graph convolutional networks for amr-to-text generation. In EMNLP.</p>
<p>Modeling graph structure in transformer for better amr-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, EMNLP-IJCNLP. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better amr-to-text genera- tion. In EMNLP-IJCNLP.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9189 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9189</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9189</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-272600407</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.07641v1.pdf" target="_blank">SimulBench: Evaluating Language Models with Creative Simulation Tasks</a></p>
                <p><strong>Paper Abstract:</strong> We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\% more cases.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9189.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9189.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemistry (Eq. balancing / reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemical equation balancing and chemical-reaction prediction simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SIMULBENCH includes stateless chemistry-related simulation tasks (e.g., chemical equation balancing and predicting chemical reactions) intended to test LLMs' ability to produce precise, character- and syntax-sensitive outputs for chemistry problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo, GPT-4o, LLaMA-3-70B-Chat, Qwen1.5-110B-Chat, Mixtral variants, LLaMA-2 series, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mix of proprietary (OpenAI GPT-4-turbo, GPT-4o) and open-source LLMs (LLaMA-2/3 family, Qwen1.5, Mixtral/Mistral instruction-tuned models) evaluated as text-based simulators in SIMULBENCH. Models vary from ~7B to 110B parameters (when reported) and differ in training/fine-tuning (instruction-tuning, mixture-of-experts variants).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Chemistry (computational/educational chemistry; chemical equation balancing / reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Stateless tasks such as balancing chemical equations and predicting chemical reactions from text specifications (one-shot textual interface: input a reaction or unbalanced equation, output a balanced equation or predicted products).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4 judge scoring on a 1–10 scale (automated), aggregated average scores per script and per-model; human verification of judge outputs on a sample (83% considered reasonable).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No per-model per-chemistry-task scores reported in the paper; chemistry-related tasks are grouped among the hardest stateless tasks (paper lists 'Chemical Equation Balancer' in the top-10 hardest stateless tasks), indicating low/poor performance across many models (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Task objectivity and character-level constraints (need for exact syntax and symbols); training data coverage for chemistry formats; model size and family (proprietary vs open-source gap); ability to produce rigorous, exact outputs rather than free-form text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons are across evaluated LLMs (proprietary vs open-source); no human-expert or classical chemistry solver baselines were used in the paper for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports that objective stateless tasks with character-level constraints are where LLMs (especially open-source ones) perform worst; stronger LLMs still make encoding or attribute errors on structured outputs (e.g., SVG example analogously), and weaker models may 'get out of control' formatting-wise. No per-model chemistry numeric accuracy is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight the need for models to better handle precise, constrained outputs and suggest more attention to tasks that require rigorous, character-level correctness when designing future LLMs and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimulBench: Evaluating Language Models with Creative Simulation Tasks', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9189.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9189.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cryptography (classical ciphers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based cryptographic system simulation (classical encryption tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SIMULBENCH includes stateless cryptographic simulation tasks (e.g., encrypting plaintext with Caesar cipher) to evaluate LLMs' ability to follow exact algorithmic/output constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo, GPT-4o, LLaMA-3-70B-Chat and other open-source LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of proprietary and open-source LLMs evaluated as simulators; models differ in size and architecture as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computer security / cryptography (classical cipher simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Stateless simulation of cryptographic operations, e.g., encrypting a plaintext message with a specified classical cipher and rotation (the paper gives the example: encrypting 'Hello World' with a Caesar cipher rotation 5).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4 judge 1–10 scoring per script; qualitative listing of tasks where models failed; no separate cryptography-specific numeric accuracy breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Paper states open-source LLMs mostly failed on the Caesar cipher example; overall stateless objective tasks (including cryptographic system) are among the hardest in SIMULBENCH. No per-model numeric accuracy for the specific cryptography task is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Need for exact, character-level transformations; sensitivity to formatting and exact algorithmic application; lack of explicit symbolic algorithm execution in language-model training; model family and size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across LLMs only (no cryptographic-tool or human baseline reported). Proprietary models perform better on objective, exact tasks than many open-source models but failure still observed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Open-source models often fail to produce correct encrypted outputs; even stronger models struggle on tasks requiring strict algorithmic output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note LLMs are knowledgeable but struggle to apply knowledge flexibly for rigorous outputs; they highlight the need to improve ability for exact algorithmic/textual transformations and careful use of context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimulBench: Evaluating Language Models with Creative Simulation Tasks', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9189.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9189.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Board games / strategic games</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Board-game simulation (Tic-Tac-Toe, Gomoku, Chess) and game-playing environments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SIMULBENCH contains stateful game simulations where the LLM must maintain and update an evolving game state, enforce game rules, and possibly plan strategies across multiple turns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo, GPT-4o, LLaMA-3-70B-Chat, LLaMA-3-8B-Chat, Qwen1.5-110B-Chat, Mixtral family, LLaMA-2 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Range of proprietary and open-source LLMs, tested with multi-turn script-based evaluation from user-agent dialogues; models vary in parameter counts and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Game theory / AI for games (multi-turn stateful simulations requiring state update and strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Stateful multi-turn text simulations of board games (e.g., Tic-Tac-Toe, Chess, Gomoku) where the LLM must maintain board state, apply rules, and produce the next move or updated board representation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4 judge 1–10 scoring on the final response in extracted multi-turn scripts; pairwise win/tie/lose comparisons on SIMULBENCH-Hard for top models (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Paper reports generally poor performance across models on board-game simulations: models know rules when asked but often fail to maintain correct board state and produce orderly game play; no per-model per-game win rates vs. correct ground truth given, but SIMULBENCH-Hard pairwise comparisons show GPT-4-turbo outperforms LLaMA-3-70B-Chat on 18.55% more cases (hard subset).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Statefulness (long-horizon memory) and the need to correctly track and update discrete state across turns; ability to selectively use history; model size and architecture; training data emphasis (code-heavy training helps code-like tasks more than strategic game play); presence of erroneous dialogue history negatively impacts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across LLMs; no classical game engine baseline or human expert baseline presented. Proprietary GPT-4 variants outperform top open-source LLMs on hard scripts according to pairwise comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Common failures include starting with incorrect board states, losing track of moves, and inability to implement winning tactics despite knowledge of rules — indicates weakness in applying knowledge in long-horizon, stateful simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest more focus on models' selective use of history and improving long-horizon strategic planning; they note that stateful tasks expose gaps in current LLM capabilities and recommend research into better history handling and memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimulBench: Evaluating Language Models with Creative Simulation Tasks', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9189.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9189.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Systems / Programming (terminals, interpreters, SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linux terminal, Python interpreter, SQL terminal and other code-execution style simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SIMULBENCH includes stateful or stateless simulations that emulate programming language interpreters, shells, and database terminals, requiring precise code or command outputs and possibly multi-turn interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo, GPT-4o, LLaMA-3 series, Qwen1.5, Mixtral/Mistral variants, LLaMA-2 series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various proprietary and open-source transformer LLMs of differing scales and fine-tuning; evaluated via scripted multi-turn interactions where a user-agent issues commands and the model must respond as a terminal or interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computer systems and software engineering (terminal and interpreter simulation; programming assistance and code execution emulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate terminals and language interpreters (e.g., Linux terminal, Python interpreter, SQL executor) in text: accept commands and return correct outputs, including stateful environment effects across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4 judge 1–10 scoring; scripts extracted to test multi-turn behavior and state management.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Paper notes that code-related tasks (programming language interpreters and Linux terminal) perform slightly better than board games and other stateful strategic tasks, attributed to code being common in training data; no per-task numeric accuracies provided beyond aggregated model scores.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Training-data coverage (code is abundant in LLM training sets, improving performance); model size and instruction fine-tuning; stateful context handling; presence of erroneous prior history.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cross-model comparison; no dedicated interpreter or dynamic execution baseline used. Stronger LLMs outperform weaker ones on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although better than many other stateful tasks, LLMs can still falter on complex multi-step terminal sessions or when history contains errors; exact reproducible command outputs may still be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight that code presence in training data improves simulation ability on code-like tasks and recommend careful exploitation of history; suggest expanding benchmarks and more diverse user-agent behaviors for future evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimulBench: Evaluating Language Models with Creative Simulation Tasks', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9189.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9189.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Space / Robotics simulations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Space mission and rover simulations (Mars Rover/Colony, Space Station, Space Mission Commander)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SIMULBENCH contains stateful simulation tasks modeling space/robotic scenarios (e.g., Mars Rover Simulator, Space Station Simulation, Mars Colony Simulator) where the LLM must manage an evolving environment and return relevant operational decisions or state updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo, GPT-4o, LLaMA variants, Qwen, Mixtral family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary and open-source LLMs across different parameter scales and instruction fine-tunings used to simulate multi-turn operational scenarios in space/robotics contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Aerospace / robotic systems (operational simulation of missions and rover control in text form)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Stateful multi-turn textual simulation of space missions (e.g., managing a Mars colony, commanding a rover or space mission), requiring maintenance of environmental state across turns and producing plans or commands.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Automated GPT-4 judge (1–10) scoring of final responses in extracted multi-turn scripts. No per-task numeric breakdown in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Space/mission tasks are listed among stateful examples in the 'simplest' tasks list (e.g., 'Space Station Simulation', 'Mars Colony Simulator' appear in simplest set), indicating relatively better performance in some cases; however, aggregated per-model scores vary and no exact per-task accuracies are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Statefulness and long-horizon memory demands; requirement for strategic planning; availability of relevant training data; model's ability to use history selectively and to handle complex, multi-step instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cross-model comparisons only; no domain-specific simulator or human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Challenges arise for tasks requiring long-run strategy and reliable memory; models may fail to maintain coherent multi-step plans across scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest more research into selective history utilization and longer dialogue lengths, and advocate expanding benchmark tasks to include more realistic user queries and diverse personas to better probe such simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimulBench: Evaluating Language Models with Creative Simulation Tasks', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9189.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9189.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Urban planning / City design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>City planner / city design simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SIMULBENCH includes stateful simulation tasks such as 'City Planner' that require structured, often objective outputs related to spatial planning and design constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo, GPT-4o, LLaMA family, Qwen, Mixtral family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Set of evaluated LLMs spanning proprietary and open-source models with different capacities and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Urban planning / civil systems (textual simulation of city planning and design decisions)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Stateful or stateless simulations requiring the model to produce plans, structured outputs, or follow constraints for city planning problems (e.g., city layout, zoning suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-4 judge scoring (1–10) on extracted multi-turn scripts; tasks appear in curated simplest/hardest lists but no per-task numeric breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>City planning tasks appear in the paper's 'simplest' stateful tasks list (e.g., 'City Planner' listed among simpler stateful tasks), suggesting models handled some of these tasks relatively well in aggregate; exact numeric accuracy per model not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Task specification clarity (free-form vs constrained), requirement for structured outputs, need to maintain state, and model's training coverage for planning/domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Across LLMs; no domain-specific urban-planning baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Objective, constrained planning tasks that require precise structured outputs remain challenging; models may produce plausible but unrigorous or inconsistent plans.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend expanding task diversity and improving history handling and evaluation fidelity to better capture performance in structured domain simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimulBench: Evaluating Language Models with Creative Simulation Tasks', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4. <em>(Rating: 1)</em></li>
                <li>Judging llm-as-a-judge with mtbench and chatbot arena. <em>(Rating: 2)</em></li>
                <li>MT-Bench <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9189",
    "paper_id": "paper-272600407",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Chemistry (Eq. balancing / reaction prediction)",
            "name_full": "Chemical equation balancing and chemical-reaction prediction simulations",
            "brief_description": "SIMULBENCH includes stateless chemistry-related simulation tasks (e.g., chemical equation balancing and predicting chemical reactions) intended to test LLMs' ability to produce precise, character- and syntax-sensitive outputs for chemistry problems.",
            "citation_title": "SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo, GPT-4o, LLaMA-3-70B-Chat, Qwen1.5-110B-Chat, Mixtral variants, LLaMA-2 series, etc.)",
            "model_description": "A mix of proprietary (OpenAI GPT-4-turbo, GPT-4o) and open-source LLMs (LLaMA-2/3 family, Qwen1.5, Mixtral/Mistral instruction-tuned models) evaluated as text-based simulators in SIMULBENCH. Models vary from ~7B to 110B parameters (when reported) and differ in training/fine-tuning (instruction-tuning, mixture-of-experts variants).",
            "scientific_subdomain": "Chemistry (computational/educational chemistry; chemical equation balancing / reaction prediction)",
            "simulation_task": "Stateless tasks such as balancing chemical equations and predicting chemical reactions from text specifications (one-shot textual interface: input a reaction or unbalanced equation, output a balanced equation or predicted products).",
            "evaluation_metric": "GPT-4 judge scoring on a 1–10 scale (automated), aggregated average scores per script and per-model; human verification of judge outputs on a sample (83% considered reasonable).",
            "simulation_accuracy": "No per-model per-chemistry-task scores reported in the paper; chemistry-related tasks are grouped among the hardest stateless tasks (paper lists 'Chemical Equation Balancer' in the top-10 hardest stateless tasks), indicating low/poor performance across many models (qualitative).",
            "factors_affecting_accuracy": "Task objectivity and character-level constraints (need for exact syntax and symbols); training data coverage for chemistry formats; model size and family (proprietary vs open-source gap); ability to produce rigorous, exact outputs rather than free-form text.",
            "comparison_baseline": "Comparisons are across evaluated LLMs (proprietary vs open-source); no human-expert or classical chemistry solver baselines were used in the paper for these tasks.",
            "limitations_or_failure_cases": "Paper reports that objective stateless tasks with character-level constraints are where LLMs (especially open-source ones) perform worst; stronger LLMs still make encoding or attribute errors on structured outputs (e.g., SVG example analogously), and weaker models may 'get out of control' formatting-wise. No per-model chemistry numeric accuracy is provided.",
            "author_recommendations_or_insights": "Authors highlight the need for models to better handle precise, constrained outputs and suggest more attention to tasks that require rigorous, character-level correctness when designing future LLMs and benchmarks.",
            "uuid": "e9189.0",
            "source_info": {
                "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Cryptography (classical ciphers)",
            "name_full": "Text-based cryptographic system simulation (classical encryption tasks)",
            "brief_description": "SIMULBENCH includes stateless cryptographic simulation tasks (e.g., encrypting plaintext with Caesar cipher) to evaluate LLMs' ability to follow exact algorithmic/output constraints.",
            "citation_title": "SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo, GPT-4o, LLaMA-3-70B-Chat and other open-source LLMs)",
            "model_description": "Same set of proprietary and open-source LLMs evaluated as simulators; models differ in size and architecture as reported in the paper.",
            "scientific_subdomain": "Computer security / cryptography (classical cipher simulation)",
            "simulation_task": "Stateless simulation of cryptographic operations, e.g., encrypting a plaintext message with a specified classical cipher and rotation (the paper gives the example: encrypting 'Hello World' with a Caesar cipher rotation 5).",
            "evaluation_metric": "GPT-4 judge 1–10 scoring per script; qualitative listing of tasks where models failed; no separate cryptography-specific numeric accuracy breakdown provided.",
            "simulation_accuracy": "Paper states open-source LLMs mostly failed on the Caesar cipher example; overall stateless objective tasks (including cryptographic system) are among the hardest in SIMULBENCH. No per-model numeric accuracy for the specific cryptography task is provided.",
            "factors_affecting_accuracy": "Need for exact, character-level transformations; sensitivity to formatting and exact algorithmic application; lack of explicit symbolic algorithm execution in language-model training; model family and size.",
            "comparison_baseline": "Compared across LLMs only (no cryptographic-tool or human baseline reported). Proprietary models perform better on objective, exact tasks than many open-source models but failure still observed.",
            "limitations_or_failure_cases": "Open-source models often fail to produce correct encrypted outputs; even stronger models struggle on tasks requiring strict algorithmic output formats.",
            "author_recommendations_or_insights": "Authors note LLMs are knowledgeable but struggle to apply knowledge flexibly for rigorous outputs; they highlight the need to improve ability for exact algorithmic/textual transformations and careful use of context.",
            "uuid": "e9189.1",
            "source_info": {
                "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Board games / strategic games",
            "name_full": "Board-game simulation (Tic-Tac-Toe, Gomoku, Chess) and game-playing environments",
            "brief_description": "SIMULBENCH contains stateful game simulations where the LLM must maintain and update an evolving game state, enforce game rules, and possibly plan strategies across multiple turns.",
            "citation_title": "SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo, GPT-4o, LLaMA-3-70B-Chat, LLaMA-3-8B-Chat, Qwen1.5-110B-Chat, Mixtral family, LLaMA-2 variants)",
            "model_description": "Range of proprietary and open-source LLMs, tested with multi-turn script-based evaluation from user-agent dialogues; models vary in parameter counts and instruction tuning.",
            "scientific_subdomain": "Game theory / AI for games (multi-turn stateful simulations requiring state update and strategy)",
            "simulation_task": "Stateful multi-turn text simulations of board games (e.g., Tic-Tac-Toe, Chess, Gomoku) where the LLM must maintain board state, apply rules, and produce the next move or updated board representation.",
            "evaluation_metric": "GPT-4 judge 1–10 scoring on the final response in extracted multi-turn scripts; pairwise win/tie/lose comparisons on SIMULBENCH-Hard for top models (Table 1).",
            "simulation_accuracy": "Paper reports generally poor performance across models on board-game simulations: models know rules when asked but often fail to maintain correct board state and produce orderly game play; no per-model per-game win rates vs. correct ground truth given, but SIMULBENCH-Hard pairwise comparisons show GPT-4-turbo outperforms LLaMA-3-70B-Chat on 18.55% more cases (hard subset).",
            "factors_affecting_accuracy": "Statefulness (long-horizon memory) and the need to correctly track and update discrete state across turns; ability to selectively use history; model size and architecture; training data emphasis (code-heavy training helps code-like tasks more than strategic game play); presence of erroneous dialogue history negatively impacts performance.",
            "comparison_baseline": "Compared across LLMs; no classical game engine baseline or human expert baseline presented. Proprietary GPT-4 variants outperform top open-source LLMs on hard scripts according to pairwise comparison.",
            "limitations_or_failure_cases": "Common failures include starting with incorrect board states, losing track of moves, and inability to implement winning tactics despite knowledge of rules — indicates weakness in applying knowledge in long-horizon, stateful simulations.",
            "author_recommendations_or_insights": "Authors suggest more focus on models' selective use of history and improving long-horizon strategic planning; they note that stateful tasks expose gaps in current LLM capabilities and recommend research into better history handling and memory mechanisms.",
            "uuid": "e9189.2",
            "source_info": {
                "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Systems / Programming (terminals, interpreters, SQL)",
            "name_full": "Linux terminal, Python interpreter, SQL terminal and other code-execution style simulations",
            "brief_description": "SIMULBENCH includes stateful or stateless simulations that emulate programming language interpreters, shells, and database terminals, requiring precise code or command outputs and possibly multi-turn interactions.",
            "citation_title": "SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo, GPT-4o, LLaMA-3 series, Qwen1.5, Mixtral/Mistral variants, LLaMA-2 series)",
            "model_description": "Various proprietary and open-source transformer LLMs of differing scales and fine-tuning; evaluated via scripted multi-turn interactions where a user-agent issues commands and the model must respond as a terminal or interpreter.",
            "scientific_subdomain": "Computer systems and software engineering (terminal and interpreter simulation; programming assistance and code execution emulation)",
            "simulation_task": "Simulate terminals and language interpreters (e.g., Linux terminal, Python interpreter, SQL executor) in text: accept commands and return correct outputs, including stateful environment effects across turns.",
            "evaluation_metric": "GPT-4 judge 1–10 scoring; scripts extracted to test multi-turn behavior and state management.",
            "simulation_accuracy": "Paper notes that code-related tasks (programming language interpreters and Linux terminal) perform slightly better than board games and other stateful strategic tasks, attributed to code being common in training data; no per-task numeric accuracies provided beyond aggregated model scores.",
            "factors_affecting_accuracy": "Training-data coverage (code is abundant in LLM training sets, improving performance); model size and instruction fine-tuning; stateful context handling; presence of erroneous prior history.",
            "comparison_baseline": "Cross-model comparison; no dedicated interpreter or dynamic execution baseline used. Stronger LLMs outperform weaker ones on these tasks.",
            "limitations_or_failure_cases": "Although better than many other stateful tasks, LLMs can still falter on complex multi-step terminal sessions or when history contains errors; exact reproducible command outputs may still be challenging.",
            "author_recommendations_or_insights": "Authors highlight that code presence in training data improves simulation ability on code-like tasks and recommend careful exploitation of history; suggest expanding benchmarks and more diverse user-agent behaviors for future evaluation.",
            "uuid": "e9189.3",
            "source_info": {
                "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Space / Robotics simulations",
            "name_full": "Space mission and rover simulations (Mars Rover/Colony, Space Station, Space Mission Commander)",
            "brief_description": "SIMULBENCH contains stateful simulation tasks modeling space/robotic scenarios (e.g., Mars Rover Simulator, Space Station Simulation, Mars Colony Simulator) where the LLM must manage an evolving environment and return relevant operational decisions or state updates.",
            "citation_title": "SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo, GPT-4o, LLaMA variants, Qwen, Mixtral family)",
            "model_description": "Proprietary and open-source LLMs across different parameter scales and instruction fine-tunings used to simulate multi-turn operational scenarios in space/robotics contexts.",
            "scientific_subdomain": "Aerospace / robotic systems (operational simulation of missions and rover control in text form)",
            "simulation_task": "Stateful multi-turn textual simulation of space missions (e.g., managing a Mars colony, commanding a rover or space mission), requiring maintenance of environmental state across turns and producing plans or commands.",
            "evaluation_metric": "Automated GPT-4 judge (1–10) scoring of final responses in extracted multi-turn scripts. No per-task numeric breakdown in paper.",
            "simulation_accuracy": "Space/mission tasks are listed among stateful examples in the 'simplest' tasks list (e.g., 'Space Station Simulation', 'Mars Colony Simulator' appear in simplest set), indicating relatively better performance in some cases; however, aggregated per-model scores vary and no exact per-task accuracies are provided.",
            "factors_affecting_accuracy": "Statefulness and long-horizon memory demands; requirement for strategic planning; availability of relevant training data; model's ability to use history selectively and to handle complex, multi-step instructions.",
            "comparison_baseline": "Cross-model comparisons only; no domain-specific simulator or human baseline provided.",
            "limitations_or_failure_cases": "Challenges arise for tasks requiring long-run strategy and reliable memory; models may fail to maintain coherent multi-step plans across scripts.",
            "author_recommendations_or_insights": "Authors suggest more research into selective history utilization and longer dialogue lengths, and advocate expanding benchmark tasks to include more realistic user queries and diverse personas to better probe such simulations.",
            "uuid": "e9189.4",
            "source_info": {
                "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Urban planning / City design",
            "name_full": "City planner / city design simulations",
            "brief_description": "SIMULBENCH includes stateful simulation tasks such as 'City Planner' that require structured, often objective outputs related to spatial planning and design constraints.",
            "citation_title": "SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo, GPT-4o, LLaMA family, Qwen, Mixtral family)",
            "model_description": "Set of evaluated LLMs spanning proprietary and open-source models with different capacities and fine-tuning.",
            "scientific_subdomain": "Urban planning / civil systems (textual simulation of city planning and design decisions)",
            "simulation_task": "Stateful or stateless simulations requiring the model to produce plans, structured outputs, or follow constraints for city planning problems (e.g., city layout, zoning suggestions).",
            "evaluation_metric": "GPT-4 judge scoring (1–10) on extracted multi-turn scripts; tasks appear in curated simplest/hardest lists but no per-task numeric breakdown provided.",
            "simulation_accuracy": "City planning tasks appear in the paper's 'simplest' stateful tasks list (e.g., 'City Planner' listed among simpler stateful tasks), suggesting models handled some of these tasks relatively well in aggregate; exact numeric accuracy per model not reported.",
            "factors_affecting_accuracy": "Task specification clarity (free-form vs constrained), requirement for structured outputs, need to maintain state, and model's training coverage for planning/domain knowledge.",
            "comparison_baseline": "Across LLMs; no domain-specific urban-planning baseline reported.",
            "limitations_or_failure_cases": "Objective, constrained planning tasks that require precise structured outputs remain challenging; models may produce plausible but unrigorous or inconsistent plans.",
            "author_recommendations_or_insights": "Authors recommend expanding task diversity and improving history handling and evaluation fidelity to better capture performance in structured domain simulations.",
            "uuid": "e9189.5",
            "source_info": {
                "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations.",
            "rating": 2,
            "sanitized_title": "gtbench_uncovering_the_strategic_reasoning_limitations_of_llms_via_gametheoretic_evaluations"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4.",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mtbench and chatbot arena.",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "MT-Bench",
            "rating": 2
        }
    ],
    "cost": 0.016902,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks
11 Sep 2024</p>
<p>Qi Jia jia_qi@nus.edu.sg 
National University of Singapore</p>
<p>Xiang Yue 
Carnegie Mellon University</p>
<p>Tianyu Zheng 
University of Waterloo</p>
<p>Jie Huang 
University of Illinois
Urbana Champaign</p>
<p>Bill Yuchen Lin 
Allen Institute for AI</p>
<p>SIMULBENCH: Evaluating Language Models with Creative Simulation Tasks
11 Sep 20242D22059C5129DC0F93FBB159527020E8arXiv:2409.07641v1[cs.CL]
We introduce SIMULBENCH, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation tasks, such as acting as a Linux terminal or playing text games with users.While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks.A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI.To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks.Then, challenging dialogue scripts are extracted for evaluating different target LLMs.To facilitate automatic assessment on SIMULBENCH, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts.Our comprehensive experiments indicate that these creative simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs.For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55% more cases.</p>
<p>Introduction</p>
<p>The ability of large language models (LLMs) to simulate complex tasks is pivotal in driving the evolution of AI towards achieving general intelligence [2].These models exhibit remarkable versatility by adeptly assuming a wide range of roles-from acting as a Linux terminal to serving as an investment manager-highlighting their adaptability across various domains.Such flexibility underscores their potential for broad implementation.Consequently, the development of a benchmark dataset for simulation tasks is imperative in nurturing LLMs' progression toward becoming true generalists.</p>
<p>Nonetheless, existing benchmarks do not fully evaluate this potential.Current evaluations mainly focus on single-turn, static interactions between users and LLMs [6,14].While MT-bench [17] attempts to consider multi-turn interactions with 80 examples, its reliance on predefined second queries fails to effectively examine the dynamic responses of different LLMs when engaging with users in complex, long-horizon simulation tasks.In addition, these benchmarks primarily concentrate on tasks related to general information retrieval and creative writing, with less emphasis on complex simulation abilities.</p>
<p>Based on whether the simulation target is a human or not, simulation tasks can be divided into two groups.The former groups correlate to existing role-playing benchmarks focusing on replicating the language styles and knowledge of famous characters or professions [5,12,18,9] and have been widely investigated.However, the second group of tasks are under consideration.Recent work from Duan et al. [3] introducing GTBench to explore LLM's ability on some language-driven games, is barely beginning to explore this kind of simulation abilities.A comprehensive benchmark covering wide-ranging non-human centered tasks for thoroughly assessing the simulation potential of LLMs is in urgent need.</p>
<p>Tasks for SIMULBENCH.We have gathered 109 distinct simulation tasks that require LLMs to perform in a variety of interfaces.These interfaces include acting as a Linux terminal, an SQL executor, text-based games such as tic-tac-toe, a generator for passwords with particular constraints, an ASCII art creator, a predictor of chemical reactions, and more.Each task specification comes with an interface description, some output requirements and an initial user request.Some examples of simulation tasks are presented in Figure 1.</p>
<p>Multi-Turn Script-based Evaluation.MT-bench [17] is designed to test LLMs in a two-turn conversation, where the second turn is predefined.However, our SIMULBENCH necessitates multiple turns between users and LLMs.Depending on the task types and context window limit, some tasks may involve conversations exceeding 5 turns, with the majority spanning over 2 turns.To replicate realistic usage scenarios of LLMs, we employ OpenAI's GPT-3.5 to simulate a user interacting continuously with an LLM.To ensure fairness among different test models, we extract challenging histories from the collected dialogues to form the final test scripts.Finally, after gathering reactions from each target LLM, we follow the methodology of previous studies [17,19,15], using GPT-4 to assess and rate the quality of these responses.We also conduct pairwise comparisons for a more detailed analysis.</p>
<p>Experimental Results and Findings.Our study involved an analysis of 2 proprietary LLMs and 12 widely used open-source LLMs, specifically series of models in LLaMA [11], Qwen [1] and Mixtral [4].These models are often ranked highly on several existing leaderboards, such as the Chatbot Arena2 .Although the performance of these open-source LLMs is approaching GPT-4-turbo, there is still a conspicuous gap between them.Even the strongest open LLM, LLaMA-3-70B-Chat, was surpassed by GPT-4-turbo on 18.55% more cases on the hard subset of SIMULBENCH.</p>
<p>We noticed that recent LLMs can take advantage of history information much better than the previous ones, showing superior performance on stateful tasks than the stateless ones.However, we also highlighted the importance of utilizing the context information cautiously and selectively, and showed that even the performance of GPT-4o drops from 9.40 to 7.57 on the most challenging scripts possibly containing erroneous dialogue history.In addition, we observed that although LLMs are knowledgeable and good at question answering, they face obstacles to applying knowledge flexibly and tend to exhibit poorer performance in simulation tasks that necessitate more rigorous outputs (such as classical encryption algorithms) and strategic plans along with long-horizon memory (such as different board games).</p>
<p>SIMULBENCH</p>
<p>In this section, we first describe how we collect the tasks for SIMULBENCH, next introduce the user agent for collecting LLM interactions, then explain how we extracted challenging conversation histories and finally present our evaluation metrics.</p>
<p>Overview</p>
<p>The complexity of simulation tasks, characterized by their multi-round nature and the diverse conversation paths influenced by differing model responses, renders script-based single-turn evaluation [16,12] and pre-defined multi-turn dialogue templates [17] inappropriate.Previous studies [18,13] have employed human volunteers to interact with models and perform evaluations.This approach, however, inherently necessitates that the volunteer possess a thorough understanding of the testing role or be able to readily acquire the needed knowledge through search engines.Yet, in simulation tasks, some of the required knowledge is highly specialized, such as diverse programming languages in language interpreters or terminal simulators, and algebraic notation in chess player simulations.Consequently, recruiting knowledgeable volunteers for various scenarios is not only impractical but also challenging to replicate for subsequent research.</p>
<p>To address this, we propose a three-stage evaluation framework leveraging the exceptional proficiency of proprietary models in diverse text-based generation tasks.The first stage involves the collection of multi-turn dialogues between a fixed user agent and an LLM.Subsequently, challenging conversation histories will be extracted as testing scripts for fair comparisons.Finally, an LLM Judge is utilized to rate the performance of the LLMs' response in each script.The mean score across numerous testing scripts covering diverse simulation tasks serves as an indicator of the viability of using LLMs as simulators.</p>
<p>Collecting tasks for SIMULBENCH</p>
<p>In order to encompass a broad range of simulation scenarios, we utilize tasks found in a publicly accessible Github repository named "Awesome ChatGPT Prompts" 3 .This repository is a platform where community users share real-world applications of ChatGPT.It contains 168 prompts that represent a wide array of scenarios.</p>
<p>We filtered out role-playing cases manually, modified the serious mistakes, filled the placeholders in some samples, and collected 59 prompts as the seed data in the end.We treated the prompts as simulation specifications.Each simulation task specification primarily consists of a brief paragraph detailing the task description, output requirements, and an initial user request.</p>
<p>To improve the diversity of the testing data, we adopted a 5-shot prompting strategy and prompted GPT-4 to generate new simulation tasks.5 samples are randomly selected from the seed data to form the task generation prompt.We carefully checked the quality of generated prompts and only reserved the non-repetitive ones.Finally, 109 simulation tasks were collected.</p>
<p>LLM interactions with an user agent</p>
<p>To assess the capability of LLMs as simulators, interaction with a user is required.To automate this interaction, we developed a user agent leveraging the capabilities of GPT-3.5-turbo.The model was tasked with emulating a real human, generating diverse requests that engage in dialogue with the simulators.To maintain the user agent's character consistency, we suggested four distinct generic response strategies, as follows:</p>
<p>• Improvement: Identifying errors, ambiguities, or other dis-satisfactions for improvements.</p>
<p>• NextStep: Proceeding to the subsequent step or diving deeper into the current topic.</p>
<p>• NewRequest: Initiating a new request which is more long-tailed or more difficult.</p>
<p>• Others: Other feasible strategies.The finalized prompt for the user agent is designed to accommodate most kinds of tasks.It can also be modified slightly by incorporating task-specific configurations to improve the user agent's stability.Each time, the dialogue history will be inserted into the placeholder of the prompt, the user agent is expected to generate the next utterance together with a brief description of their adopted strategy.The utterance will be extracted as a reply to the current dialogue.</p>
<p>We utilized the default prompts for simulators provided by the corresponding LLMs.Throughout the conversation, the initial utterance from the user bot is designated as the simulation task specification.The simulator and the user bot alternate turns speaking until the maximum turn limit is reached.</p>
<p>Test script extraction</p>
<p>Intuitively, we can assess the performance of a test model in each dialogue between itself and the user agent across the simulation tasks.Unfortunately, based on our pilot experiments, it suffers from unfair comparisons due to the dynamics involved by the user agent.Even though the model's temperature is set to 0.0 without sampling strategies during decoding, the agent may still raise queries with various levels of complexity among different test models.This divergence becomes more severe as the conversation goes on.For example, at the 4th turn of simulating a password generator, the user agent only queried gpt-4-0124-preview to generate a password with "length=11", but challenged LLaMA-2-70B and Mixtral-8x7B with "length=16" and "length=28" respectively.</p>
<p>One possible solution is to collect multiple dialogues for each test model, and use the averaged performance as the final result.However, it's hard to guarantee that the user agent will not be biased toward some models all the time, and it largely aggravates the evaluation cost.</p>
<p>Instead, we propose to extract challenging dialogue histories as a test script from the user-simulator dialogues, and do the script-based evaluation.It imitates the endgames in chess, where the history interactions are provided and the test model is expected to continue the dialogue and generate a response to the latest user's query.In this way, our evaluation pipeline assures fair comparisons among different models while maintaining the multi-round characteristic of simulation tasks, with better reproducibility and lower computation costs.Specifically, we chose gpt-3.5-turboas the simulator, and collected the dialogue with the user agent on each simulation task 3 times.Two strategies were adopted to identify challenging test scripts:</p>
<p>• We regard the last turn in a dialogue as challenging.The turns before it and the latest user query form a test script.</p>
<p>• We adopt GPT-4 to identify whether there is a turn in the given dialogue, where the user's request possesses extreme complexity and difficulty, resulting in an inaccurate response from the simulator.If it exists, the turn and turns after it are all recognized as challenging and extracted as test scripts.</p>
<p>Finally, 500 test scripts are selected with the above strategies 4 , denoted as SIMULBENCH-All.A hard subset containing 275 test scripts collected only by the second strategy is denoted as SIMUL-BENCH-Hard.SimulBench is licensed by CC BY NC 4.0 (allowing only non-commercial use).</p>
<p>GPT-4 as judge for scoring &amp; comparing</p>
<p>Evaluation based on GPT-4 has been widely discussed and adopted in recent works [17,7,10], since it is more affordable and convenient for re-implementation than hiring human annotators.Considering the diversity and complexity of simulation tasks, we also adopted GPT-4 as an evaluator to assess the performance of a test model in each test script.The evaluation was conducted on a scale of 1 to 10.We modified the evaluation prompt from MT-Bench [17] to suit the task-specific specifications and incorporated definitions for each score.Besides, we also compared the performance of two different models in relation to a script of a simulation task.The comparison was categorized as a "win", "lose", or "tie".Follow Zheng et al. [17], we swap the order of two responses to avoid position bias and only declare a win when a response is preferred in both orders.</p>
<p>3 Evaluation setup</p>
<p>Models</p>
<p>The following models with different scales are mainly considered in current work:</p>
<p>GPT-4-turbo [8] and GPT-4o5 are proprietary models provided by OpenAI through API requests.</p>
<p>The specific versions we used are gpt-4-0125-turbo and gpt-4o-2024-05-13.</p>
<p>LLaMA [11] is a collection of publicly released pre-trained and fine-tuned generative text models.We experimented with both LLaMA-2 and LLaMA-3 with different sizes: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, LLaMA-2-70B-Chat, LLaMA-3-8B-Chat, and LLaMA-3-70B-Chat.</p>
<p>Qwen [1] is another series of publicly available foundation models.We chose Qwen1.5-110B-Chat and Qwen1.5-7B-Chat for comparison.</p>
<p>Mixtral [4] includes Mixtral-8x7B-Instruct-v0.1 and Mixtral-8x22B-Instruct-v0.1 which are pretrained generative Sparse Mixture of Experts.Their latest instruction fine-tuned model, Mistral-7B-Instruct-v0.3, is also considered.</p>
<p>Implementation details</p>
<p>We set the maximum number of turns as 4 when collecting user-simulator dialogues.All of the model's temperatures are equal to 0.0 for better reproduction and fair comparison except the user agent's.It's equal to 1.2, enabling sampling strategies to achieve diverse user reactions among multiple dialogue sessions given the same simulation task.The maximum token from the user agent and the simulation models in each utterance is 300 and 1024 tokens respectively.</p>
<p>Results</p>
<p>This section analyzes the performances of different LLMs.</p>
<p>Main results</p>
<p>Figure 2 presents the results on all of the tasks in SIMULBENCH by the score (see specific numbers in Table 4).</p>
<p>GPT-4-turbo achieves the highest score, followed by GPT-4o and LLaMA-3-70B-Chat.It's abnormal that GPT-4o lags behind GPT-4-turbo on our SIMULBENCH, contradicting their rankings on Chatbot Arena Leaderboard 6 .More analysis and explanations are in Sec.4.2.2.Besides, the performance of open-source models is gradually approaching that of proprietary ones, where LLaMA-3-70B-Chat outperforms LLaMA-2-70B-Chat by a margin of 16.35% and 25.06% on all test scripts and the hard subset.</p>
<p>In the same series of models, the larger ones always perform better than their smaller counterparts.However, it doesn't hold among different series.LLaMA-3-70B-Chat is superior to Qwen1.5-110B-Chat with less than 36% numbers of parameters.Meanwhile, LLaMA-3-8B-Chat shows favorable performance than both the mixture-of-experts version and the instruction fune-tuned model from the Mixtral family, and its pioneers based on LLaMA-2.</p>
<p>Comparing the performance between SIMULBENCH-All and SIMULBENCH-Hard, we can see the trend that the stronger the model, the less its score declines.However, there an only outliers: Qwen1.5-7B-Chat.See more discussions in Sec.4.2.1.To save the API costs, we only carried out the pairwise comparison on SIMULBENCH-Hard among the top-3 models shown in Fig. 2. According to the results in Table 1, GPT-4-turbo indeed outperforms GPT-4o and Llama-3-70B-Chat on 16.36% and 18.55% more cases respectively.GPT-4o performs similarly to Llama-3-70B-Chat, which is in accord with the minor differences between them in Fig 4.
G P T -4 -t u r b o L L a M A -3 -7 0 B -C h a t G P T -4 o Q w e n 1 .5 -1 1 0 B -C h a t L L a M A -3 -8 B -C h a t M ix t r a l-8 x 2 2 B -I n s t r u c t -v 0 .1 Q w e n 1 .5 -7 B -C h a t M ix t r a l-8 x 7 B -I n s t r u c t -v 0 .1 L L a M A -2 -7 0 B -C h a t M is t r a l-7 B -I n s t r u c t -v 0 .3 L L a M A -2 -1 3 B -C h a t L L a M A -2 -7 B -C h a t All Hard</p>
<p>Detailed analysis on model performances</p>
<p>To delve deeper into the complexities of SIMULBENCH, we focused on its hard subset, and categorized simulation tasks and test scripts into different categories, with in-depth analysis as follows.</p>
<p>Performances on different types of simulation tasks</p>
<p>We categorize the simulation tasks into two types based on their characteristics:</p>
<p>• Stateless refers to a simulation task which is a one-time call tool that accepts a user request as input and returns the output by following the underlying mechanism or rules of the task, such as password generation and song recommender.</p>
<p>• Stateful refers to the tasks that have a real underlying environment or state that evolves as the user's request is processed at each turn, such as the Linux terminal and Tic-Tac-Toe games.</p>
<p>We collected 51 stateless tasks and 59 stateful tasks in total.To eliminate the influence of different script types defined in Sec.4.2.2, we focus on the scripts in FirstChan, containing 38 stateless and 55 stateful scripts.</p>
<p>The results are shown in Fig. 3</p>
<p>(a)</p>
<p>. There is a clear difference in the figure where top-5 LLMs perform better on stateful tasks while the weaker LLMs prefer stateless tasks regardless of LLaMA-2-7B-Chat.Comparing LLaMA-3-8B-Chat with other LLMs with less than 10B parameters, it achieves superior gains on stateful tasks, increased by 19.38%, 24.03% and 36.92% over Qwen, Mistral and LLaMA-2 respectively.Qwen1.5 with parameters increased from 7B to 110B also transforms from a better stateless simulator to a better stateful simulator.Overall, strong LLMs have better abilities in utilizing history information, and show more stable performances on different simulation tasks.</p>
<p>Besides, the poor performances of Qwen1.5-7B-Chat on both kinds of tasks explains why it performs poorly on SIMULBENCH-Hard, suffering more on stateful simulation tasks.</p>
<p>Performances on different kinds of scripts</p>
<p>Based on the script extraction strategy introduced in Sec.2.4, we classify all the scripts into three categories:</p>
<p>• LastOnly refers to 225 scripts extracted by the first strategy and not considered by the second strategy.</p>
<p>• FirstChan has 93 scripts from the second strategy while only considering the first recognized challenging turn.</p>
<p>• SubseqChan contains 182 scripts with subsequent challenging turns after the first one.FirstChan and SubseqChan constitute SIMULBENCH-Hard.</p>
<p>According to results in Fig. 3(b), LastOnly is much easier than the other two types.All of the LLMs achieve scores above 8, with top-5 of them more than 9. LLaMA-3-70B-Chat even slightly outperforms GPT-4-Turbo among this group of easier test samples.</p>
<p>SubseqChan is generally more challenging than FirstChain.It should be noted that scripts in Sub-seqChan may contain content or format errors in their dialogue history.Simulators are supposed to avoid the impact of previous error information and always provide a high-quality answer to the latest user query.Most of the LLMs perform more poorly in this category, where GPT-4o and Qwen1.5-110B-Chat drop dramatically by around 1 point.It reflects that both of them are truly aware of the history messages, but have not learned to take the essence and discard the dross so far.Even though some smaller models do perform better on SubseqChan, it doesn't mean that they have better history modeling ability considering their overall poor scores.How to selectively utilize historical information should be paying more attention in the further model design.</p>
<p>GPT-4o outperforms GPT-4-Turbo on both LastOnly and FirstChan, but performs much weaker on SubseqChan.This also explains why it lags behind GPT-4-Turbo on SIMULBENCH-Hard.</p>
<p>Which specific simulation tasks are harder?</p>
<p>We also wondering if there are any common characteristics among the specific simulation tasks that LLMs are good at or poor at.The average score of all LLMs considered above is averaged for each simulation task to present its complexity.We list the 10 simplest tasks and the 10 hardest ones that belong to different simulation types in Table 2. Overall, LLMs perform well on subjective tasks where the output is more free-formed, such as Startup Idea Generator and Text Based Adventure Game.However, it exhibits weaker performance on objective tasks which have underlying rules and output requirements.</p>
<p>For stateless tasks, most of the hardest simulations present character-level constraints and require an accurate response.Cryptographic system is a representative task that asks the model to encrypt a plain text message with a cryptographic method.LLMs, especially the open source ones, mostly failed on this task when asked to encrypt "Hello World" using a Caesar cipher with rotation 5. SVG designer, requiring the simulator to create images with SVG codes and convert the codes to a base64 data URL, is more complicated.Stronger LLMs encode wrong attributes of the image, while outputs of weaker LLMs may even get out of control.</p>
<p>For stateful tasks, the simulation of board games is extremely difficult.It not only requires characterlevel updates as the dialogue progresses, but also expects to follow complicated rules and even manage winning tactics.All of the models are clear about game rules when asked but failed to maintain an orderly and challenging gaming environment, and even start a wrong game board as shown in Fig. 1, indicating that knowledgeable LLMs are not good at applying knowledge.Tasks related to codes, such as different programming language interpreters and Linux Terminal, perform slightly better in board games.The major reason is that codes have been widely considered as an important part of the training data while more strategic chess journals are not specially considered.</p>
<p>Human evaluation</p>
<p>To ensure the quality of GPT-4 Judge's outputs for scoring, we randomly sampled 100 outputs and asked a human annotator to verify if the output was reasonable.The annotator was required to try their best to understand the complicated simulation tasks with the help of searching on the Internet.83% of samples are considered reasonable for both their short explanations and scores in the outputs, showing the reliability of GPT-4 as a judge for simulation tasks.Most scores in the rest samples are higher than the annotator's expectation, showing that the judge may be too optimistic in some cases.</p>
<p>Ethical concerns</p>
<p>We use the Perspective API 7 to assess the potential toxicity in our data.The simulation task specification and all of the dialogue scripts are scored for toxicity across six attributes.The score for each attribute ranges from 0 to 1, which is the lower the safer.The averaged scores of all samples are summarized in Table 3.None of the scores is higher than 0.07, exhibiting little toxicity.</p>
<p>Conclusion</p>
<p>We present a hard benchmark, SIMULBENCH, specifically aimed to evaluate LLMs' performance across different simulation tasks.Our evaluation framework is uniquely designed to incorporate GPTs as user agents, collect challenging dialogue history and do a script-based evaluation, facilitating automatic evaluation of multi-turn simulations under the guarantee of fair comparisons.</p>
<p>Our findings reveal that although open-source models are approaching the performance of proprietary APIs, GPT-4 is still topping the rank.By categorizing tasks from different aspects, we highlight that the model should cautiously attend to their historical context.We also observe that LLMs perform sub-optimally in objective simulation tasks, especially those that require an accurate response with complex character-level constraints and scenarios requiring a stateful strategy system to be built within LLMs' simulations, pointing to important future research.</p>
<p>The simulation scenarios in the current benchmark are still limited by 109 simulation tasks and a single user agent.In the future, we consider incorporating more diverse tasks by exploiting the wild user queries, incorporate various personas in user agents to better mimic real users, and extending the length of dialogue with the user bot.</p>
<p>A Results</p>
<p>We list the scores of different LLMs in Table 4.</p>
<p>B Prompts B.1 Prompts for simulation task generation</p>
<p>To collect a more diverse and balanced set of simulation tasks, we manually classified the seed data into stateless tasks and stateful tasks first during implementation.Two prompts are used to generation each kind of tasks respectively.</p>
<p>The prompt for generating stateless tasks:</p>
<p>Please act as a task creator and it is your job to create challenging and diverse tasks that require <strong>expertise</strong> of diverse domains.Each task is defined as a practical <strong>stateless</strong> one-time call interface that accept an user request(textual and self-contained) as input and return the output by following the underlying mechanism or rules of the task.Please note that the tasks you create should not be without evaluation criteria and should not be entirely creative.The prompt of the User Agent is as follows:</p>
<p>Please act as a user who is fond of posing requests, given the interaction history between the user and an AI assistant for a given task.The name and description of the task are provided in the user's <strong>first</strong> utterance.The requests you generate should be <strong>diverse</strong> and <strong>complicated</strong> enough and be executable commands or instructions applicable to the given task.</p>
<h2>Interaction History {DIALOGUE} ## Strategy</h2>
<p>Here are some useful strategies for generating the next request: -1: A request that points out any errors, ambiguities, or other <strong>dissatisfactions</strong> of the previous response from the AI assistant.-2: A request that never appeared previously but is <strong>related</strong> to previous requests by either conditioning on previous requests' outcome or will have an impact upon previous outcomes.-3: A request that never appeared previously, still belongs to the same domain as previous requests but has a much higher <strong>rarity</strong> (i.e., being more long-tailed), or a much higher <strong>difficulty</strong> and <strong>complexity</strong>.</p>
<p>-4: Others Now, please first select one strategy from the above options (if Others is selected, please further elaborate on your strategy) and then generate a new request using the selected strategy.Put the strategy number and request in JSON format by filling in the placeholders in [] in the following code block.Only output the JSON code block, nothing else! "' { "user": { "strategy": "[{STRATEGY_TYPE}]", "request": "[{REQUEST_TYPE}]" } } "'</p>
<p>B.3 Prompt for identifying challenging turns</p>
<p>The prompt used in Sec.2.4 is:</p>
<p>[Instruction] Please act as a keen observer and a sharp-eyed judge.You will be presented with a multi-turn dialogue history between a user and an AI assistant.In the dialogue, the user may pose various types of requests to the AI assistant and the AI assistant should (but may fail to) provide relevant and accurate response.Your job is to carefully look through each turn in the dialogue and identify the first <strong>challenging</strong> turn in the dialogue.Challenging implies that, at that turn, the user's request possesses extreme complexity and difficulty, resulting in an inaccurate response(content errors, format errors, ambiguities, etc.) from the AI assistant.</p>
<p>[The Start of the Dialogue] {DIALOGUE} [The End of the Dialogue] Now, please carefully look through each turn in the above dialogue and identify the first <strong>challenging</strong> turn in it.Your decision should have an objective and rigorous rationale to support it.Begin your evaluation by providing a short explanation.After providing the explanation, you must output the turn number(1 to {DIALOGUE_LEN}, and 0 represents no challenging turns), and strictly follow this format: [[TURN_NUMBER]] , for example: Turn: [ [3]] .</p>
<p>B.4 Prompts for the LLM Judge</p>
<p>The prompt of the LLM judge for scoring a single LLM is as follows:</p>
<p>[Instruction] Please act as an impartial and sharp-eyed judge.You will be presented with a multi-turn dialogue history between a user and an AI assistant.In the dialogue, the user may pose various types of requests to the AI assistant, and the AI assistant should (but may fail to) provide a high-quality response to satisfy the user's need.Your job is to evaluate the quality of the <strong>last response</strong> provided by the AI assistant based on the earlier dialogue history.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the AI assistant's responses to the user's requests and commands regarding the simulation task of "{SIMULATION}".</p>
<p>Begin your evaluation by providing a short explanation.Be as objective as possible.Ignore the words of praise from the user.After providing your explanation, you must rate the response on a scale of 1 to 10.The scoring standard is as follows: -1 to 2: The AI's response terribly fulfills the user's request, completely deviating from its simulation task and containing garbage content and format errors.</p>
<p>-3 to 4: The AI's response poorly fulfills the user's request, containing inaccurate and useless content given its simulation task, or having format errors such as redundant explanations.</p>
<p>-5 to 6: The AI's response moderately fulfills the user's request, containing no content errors but still showing errors such as incorrect format and repetition.</p>
<p>-7 to 8: The AI's response well fulfills the user's request, containing no content errors and strictly following format requirements.</p>
<p>-9 to 10: The AI's response perfectly fulfills the user's request, containing no content or format errors at all while exhibiting extremely good relevance, helpfulness, accuracy, and creativity.Your rating should strictly follow this format: [[rating]] , for example: Rating: [ [5]] .The prompt of the LLM judge for comparing two LLMs is as follows:</p>
<p>[Instruction] Please act as an impartial and sharp-eyed judge.You will be presented with a multi-turn dialogue history between a user and an AI assistant, and two candidates of the <strong>next response</strong> from the AI assistant.In the dialogue, the user may pose various types of requests to the AI assistant, and the AI assistant should provide a high-quality response to satisfy the user's need.Your job is to evaluate which candidate response is better considering factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the AI assistant's responses to the user's requests and commands regarding the simulation task of "{SIMULATION}".The AI should adhere to the user's instructions regarding both the content and format of the responses.Additional explanations not required unless been asked, and should be punished if they are unallowed by the simulation task.It should be noted that an empty response is sometimes expected in accordance with the design of a true system.</p>
<p>Begin your evaluation by comparing the two responses and provide a short explanation.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain indexes of the responses.Be as objective as possible.Ignore the words of praise from the user.After providing your explanation, output your final verdict by strictly following this format:</p>
<p>Figure 1 :
1
Figure 1: Examples of creative simulation tasks in SIMULBENCH.</p>
<p>Figure 2 :
2
Figure 2: Performances of LLMs on SIMULBENCH.</p>
<p>Figure 3 :
3
Figure 3: Performances of LLMs on SIMULBENCH in different categories.</p>
<p>[</p>
<p>The Start of the Dialogue History] {DIALOGUE} [The End of the Dialogue History] [The Start of the Last Response] {RESPONSE} [The End of the Last Response]</p>
<p>[[A]]ïf response A is better, [[B]]ïf response B is better, and [[C]] for a tie.[The Start of the Dialogue History] {DIALOGUE} [The End of the Dialogue History] [The Start of Candidate Response A] {RESPONSE_1} [The End of Candidate Response A] [The Start of Candidate Response B] {RESPONSE_2} [The End of Candidate Response B]</p>
<p>Table 1 :
1
Pairwise comparisons among top-3 LLMs on SIMULBENCH-Hard.The rate of win/tie/lose(%) regards to the first model.∆ calculates the value by which the win rate exceeds the loss rate.
Pair of ModelsWinTieLose∆GPT-4-turbo v.s. GPT-4o40.36 35.64 24.00 16.36GPT-4-turbo v.s. Llama-3-70B-Chat 38.91 40.73 20.36 18.55GPT-4o v.s. Llama-3-70B-Chat35.64 32.72 31.64 4.00</p>
<p>Table 2 :
2
The simpliest and hardest simulation tasks.
TasksStateless TasksStateful TasksSimplest Educational Content Creator, PromptDungeon Master, Text Based Adven-Enhancer, Virtual Veterinarian, Artifi-ture Game, Project Manager Simulator,cial Sommelier, Urban Myth Debunker,JSON Data Store, SQL terminal, Vir-Startup Idea Generator, Wikipediatual Detective, Space Station Simula-page, Fancy Title Generator, Historicaltion, Mars Colony Simulator, VirtualError Corrector, Etiquette ExpertSpace Mission Commander, Car Navi-gation SystemHardest SVG designer, Cryptographer, Plagia-Chess Game Simulator, Redux Staterism Checker, Smart Domain NameManager, Excel Sheet, City Plan-Generator, Cryptographic System,ner, Tic-Tac-Toe Game, Mars RoverAscii Artist, Chemical Equation Bal-Simulator, Chess Player, Japaneseancer, English Pronunciation Helper,Kanji quiz machine, Python Interpreter,Prompt Generator, Diagram Generator,Gomoku playerNew Language Creator</p>
<p>Table 3 :
3
Perspective API results of toxicity assessment.
Attributestoxicity severe toxicity identity attack insult profanity threatSimulation Task0.060.000.010.020.030.01Testing Script0.070.000.020.030.040.02</p>
<p>Table 4 :
4
Performances of different models.
ModelsLast OnlyScript Type First Subseq Chan ChanSimulation Type Hard All State State -less -fulGPT-4-Turbo9.29 8.628.138.458.758.29 8.74GPT-4o9.40 8.657.578.558.727.93 8.59LLaMA-3-70B-Chat9.32 8.137.987.748.408.03 8.61LLaMA-3-8B-Chat9.03 7.287.206.767.647.23 8.04Qwen1.5-110B-Chat9.16 8.257.257.928.487.59 8.30Qwen1.5-7B-Chat8.73 6.536.606.716.406.58 7.55Mixtral-8x22B-Instruct-v0.1 8.99 7.626.837.757.537.10 7.95Mixtral-8x7B-Instruct-v0.18.39 6.906.766.796.986.81 7.52Mixtral-7B-Instruct-v0.38.37 6.416.146.766.166.23 7.19LLaMA-2-70B-Chat8.59 6.466.406.556.406.42 7.40LLaMA-2-13B-Chat8.48 5.846.116.05.746.02 7.13LLaMA-2-7B-Chat8.37 5.466.095.295.585.88 7.00
https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
https://github.com/f/awesome-chatgpt-prompts
We discarded 10 samples where all of the models achieved full scores.
https://openai.com/index/hello-gpt-4o/
https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu, arXiv:2402.123482024arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, M I Weishi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Reviving anime character in reality via large language model. 2023arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instructionfollowing models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, </p>
<p>Geval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>. OpenAI. Gpt-4. OpenAI Blog. 2023</p>
<p>Roleeval: A bilingual role evaluation benchmark for large language models. Tianhao Shen, Sun Li, Deyi Xiong, arXiv:2312.161322023arXiv preprint</p>
<p>Evaluation metrics in the era of GPT-4: Reliably evaluating large language models on sequence to sequence tasks. Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023arXiv preprint</p>
<p>Multi-party chat: Conversational agents in group settings with humans and models. Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, Mojtaba Komeili, arXiv:2304.138352023arXiv preprint</p>
<p>Mmmu: A massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, arXiv:2311.165022023arXiv preprint</p>
<p>A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. Chen Zhang, Luis Fernando, D' Haro, Yiming Chen, Malu Zhang, Haizhou Li, arXiv:2312.154072023arXiv preprint</p>
<p>Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, Lin Gui, arXiv:2310.01459Narrativeplay: Interactive narrative understanding. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mtbench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, arXiv:2311.16832Customizing chinese conversational ai characters with large language models. 2023arXiv preprint</p>
<p>Sotopia: Interactive evaluation for social intelligence in language agents. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Second Agent Learning in Open-Endedness Workshop. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4772 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4772</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4772</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-f8359e7d74b2be343379472be3d2b452fcfa4801</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f8359e7d74b2be343379472be3d2b452fcfa4801" target="_blank">How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GAMA ($\gamma$)-Bench is introduced, a new framework for evaluating LLMs' Gaming Ability in Multi-Agent environments that includes eight classical game theory scenarios and a dynamic scoring scheme specially designed to quantitatively assess LLMs' performance.</p>
                <p><strong>Paper Abstract:</strong> Decision-making is a complex process requiring diverse abilities, making it an excellent framework for evaluating Large Language Models (LLMs). Researchers have examined LLMs' decision-making through the lens of Game Theory. However, existing evaluation mainly focus on two-player scenarios where an LLM competes against another. Additionally, previous benchmarks suffer from test set leakage due to their static design. We introduce GAMA($\gamma$)-Bench, a new framework for evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes eight classical game theory scenarios and a dynamic scoring scheme specially designed to quantitatively assess LLMs' performance. $\gamma$-Bench allows flexible game settings and adapts the scoring system to different game parameters, enabling comprehensive evaluation of robustness, generalizability, and strategies for improvement. Our results indicate that GPT-3.5 demonstrates strong robustness but limited generalizability, which can be enhanced using methods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2. Gemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by LLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental results are publicly available at https://github.com/CUHK-ARISE/GAMABench.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4772.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4772.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 multi-agent (episodic history)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (0125) multi-agent instantiation with in-prompt historical game state</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ten agents instantiated from GPT-3.5 (0125) play multi-round, multi-player games in γ-Bench where the full history of past rounds (previous actions, outcomes) is provided in the prompt, enabling agents to condition decisions on episodic history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-3.5 (0125) multi-agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multiple independent agents produced by the GPT-3.5 (0125) model; each agent receives game instructions and the formatted history of previous rounds in the prompt and responds in JSON to select actions each round.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic in-prompt history (short-term / episodic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Previous rounds' actions, aggregated statistics (e.g., average chosen number, winners, votes) and outcomes are included in the prompt as 'Game Results for Round I' entries; agents read that history each round and can adapt choices accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-agent multi-round game playing (γ-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Eight classical game-theory scenarios (Guess 2/3 of the Average, El Farol Bar, Divide the Dollar, Public Goods Game, Diner's Dilemma, Sealed-Bid Auction, Battle Royale, Pirate Game) run in multiplayer (N>2), multi-round (K rounds) settings; agents must maximize individual utility over rounds and may exploit historical information.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>γ-Bench (GAMA(γ)-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Qualitative/within-run findings: agents leverage the provided historical rounds to adapt and typically improve performance over successive rounds (e.g., averages in Guess 2/3 decrease toward better answers; Divide the Dollar proposals converge); the paper reports that GPT-3.5 'can learn from historical data and enhance its performance over time', but it does not report a controlled ablation with memory removed vs with memory for aggregated numeric comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit numerical ablation comparing with/without history; observed behaviors indicate agents often rely heavily on the immediate previous round rather than deriving or following optimal long-horizon strategies; in some games (sequential games, complex rules) agents still perform poorly despite access to history.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Providing per-round history in the prompt enables emergent learning/adaptation across rounds, improving social-welfare-oriented games' outcomes in practice, but simply exposing history does not guarantee convergence to Nash equilibria or strategic sophistication in sequential/complex-rule games; history is used but often myopically (reacting to last round) rather than for deep strategic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4772.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4772.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explicit vs Implicit observation (shared memory manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit vs Implicit observation condition for historical outcomes in El Farol Bar</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental manipulation of which agents receive per-round observational history: 'Explicit' (everyone sees end-of-round results) versus 'Implicit' (those who stayed home do not see bar outcomes), used to probe how access to shared past information affects learning and stabilization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-3.5 agents under Explicit/Implicit observation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same GPT-3.5 agents, run in two information-availability conditions that change which agents receive the per-round outcome information in the prompt, thereby altering the effective shared memory available to each agent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>public vs private episodic observation (visibility of past rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>In 'Explicit' setting the end-of-round results (who went to the bar, overcrowding outcome) are included in the prompt for all agents; in 'Implicit' setting agents who stayed home do not receive those observations (their prompt omits that outcome), creating asymmetric/missing episodic information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>El Farol Bar (multi-round coordination with capacity R)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Each round, players independently decide to go or stay; if proportion going ≤ R, going yields higher utility; otherwise going is worse than staying. Agents must learn a mixed strategy to stabilize attendance near capacity R across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>γ-Bench — El Farol Bar scenario</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Reported qualitative effect: when historical outcomes are explicitly observable by all agents ('Explicit'), probabilities of going stabilize faster and at higher levels than in the 'Implicit' setting; in 'Implicit', agents need more rounds (Rounds 2–6) to infer bar availability and their average probability of going is lower. No precise numeric head-to-head performance metrics (e.g., final score per condition) are provided in the main scoring table (the paper states they evaluated only the Implicit setting for the main score), so only qualitative differences are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No full quantitative ablation with both conditions across all models; the asymmetric information design shows that missing observations slow learning and produce lower participation probabilities, but exact effect sizes are not reported in a comparable numeric form in the paper's main results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Access to shared episodic outcomes (public memory) speeds up coordination and stabilizes mixed strategies; asymmetric or missing observations act like restricted memory and degrade adaptation speed, indicating that information visibility is a critical component of effective memory usage in multi-agent LLM systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4772.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4772.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training-data memory / test contamination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model memorization of training examples and test-set leakage concerns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discussion and experiments probing whether vanilla game settings are present in training data (data contamination) and whether LLMs 'remember' canonical game outcomes, affecting measured generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Various LLMs (discussion of training memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Paper-wide discussion refers to multiple LLMs (GPT-3.5, GPT-4, Gemini, LLaMA variants) and the risk that canonical/static game settings may have been encountered during pretraining, enabling models to recall solutions rather than reason.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term training memorization (pretraining data memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The paper points out that fixed, classical game settings increase the likelihood that LLMs encountered similar scenarios during pretraining (memorized training examples), and thus γ-Bench uses dynamic parameterization to reduce test-set leakage; this is a mention of long-term memory effects due to dataset memorization, not an implemented retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Generalizability evaluation across varied game parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The paper varies game parameters to test whether models' correct responses generalize beyond canonical settings or reflect memorized answers from pretraining; experiments include varying ratios in Guess 2/3, bar capacity R, return rates in Public Goods, gold totals in Divide the Dollar, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>γ-Bench (parameterized settings to test generalization / contamination)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper reports variable generalizability: models sometimes behave correctly across parameter changes (games 1,3,5,6,8) suggesting reasoning; in games 2 and 4 (El Farol Bar and Public Goods) models show low generalizability and consistent patterns indicative of either poor reasoning or memorized heuristics; the authors do not provide a direct experimental ablation isolating training-data memorization vs reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No definitive decomposition of whether correct answers stem from reasoning vs memorized pretraining examples; detecting and eliminating test-set leakage remains challenging and is cited as a motivation for γ-Bench's dynamic generation of scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Dynamic, parameterized scenario generation reduces risk of test-set leakage and is necessary to evaluate whether LLMs use memorized training artifacts or genuinely reason across varied environments; vigilance against training-data memory impacting evaluation is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments", 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GTBench: Uncovering the strategic reasoning capabilities of llms via game-theoretic evaluations <em>(Rating: 2)</em></li>
                <li>MAGIC: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration <em>(Rating: 2)</em></li>
                <li>GameEval: Evaluating llms on conversational games <em>(Rating: 1)</em></li>
                <li>SmartPlay: A benchmark for llms as intelligent agents <em>(Rating: 1)</em></li>
                <li>Long-horizon dialogue understanding for role identification in the game of Avalon with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4772",
    "paper_id": "paper-f8359e7d74b2be343379472be3d2b452fcfa4801",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 multi-agent (episodic history)",
            "name_full": "GPT-3.5 (0125) multi-agent instantiation with in-prompt historical game state",
            "brief_description": "Ten agents instantiated from GPT-3.5 (0125) play multi-round, multi-player games in γ-Bench where the full history of past rounds (previous actions, outcomes) is provided in the prompt, enabling agents to condition decisions on episodic history.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-3.5 (0125) multi-agent",
            "agent_description": "Multiple independent agents produced by the GPT-3.5 (0125) model; each agent receives game instructions and the formatted history of previous rounds in the prompt and responds in JSON to select actions each round.",
            "memory_type": "episodic in-prompt history (short-term / episodic memory)",
            "memory_description": "Previous rounds' actions, aggregated statistics (e.g., average chosen number, winners, votes) and outcomes are included in the prompt as 'Game Results for Round I' entries; agents read that history each round and can adapt choices accordingly.",
            "task_name": "Multi-agent multi-round game playing (γ-Bench)",
            "task_description": "Eight classical game-theory scenarios (Guess 2/3 of the Average, El Farol Bar, Divide the Dollar, Public Goods Game, Diner's Dilemma, Sealed-Bid Auction, Battle Royale, Pirate Game) run in multiplayer (N&gt;2), multi-round (K rounds) settings; agents must maximize individual utility over rounds and may exploit historical information.",
            "benchmark_name": "γ-Bench (GAMA(γ)-Bench)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Qualitative/within-run findings: agents leverage the provided historical rounds to adapt and typically improve performance over successive rounds (e.g., averages in Guess 2/3 decrease toward better answers; Divide the Dollar proposals converge); the paper reports that GPT-3.5 'can learn from historical data and enhance its performance over time', but it does not report a controlled ablation with memory removed vs with memory for aggregated numeric comparison.",
            "limitations_or_challenges": "No explicit numerical ablation comparing with/without history; observed behaviors indicate agents often rely heavily on the immediate previous round rather than deriving or following optimal long-horizon strategies; in some games (sequential games, complex rules) agents still perform poorly despite access to history.",
            "key_insights": "Providing per-round history in the prompt enables emergent learning/adaptation across rounds, improving social-welfare-oriented games' outcomes in practice, but simply exposing history does not guarantee convergence to Nash equilibria or strategic sophistication in sequential/complex-rule games; history is used but often myopically (reacting to last round) rather than for deep strategic planning.",
            "uuid": "e4772.0",
            "source_info": {
                "paper_title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Explicit vs Implicit observation (shared memory manipulation)",
            "name_full": "Explicit vs Implicit observation condition for historical outcomes in El Farol Bar",
            "brief_description": "An experimental manipulation of which agents receive per-round observational history: 'Explicit' (everyone sees end-of-round results) versus 'Implicit' (those who stayed home do not see bar outcomes), used to probe how access to shared past information affects learning and stabilization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-3.5 agents under Explicit/Implicit observation",
            "agent_description": "Same GPT-3.5 agents, run in two information-availability conditions that change which agents receive the per-round outcome information in the prompt, thereby altering the effective shared memory available to each agent.",
            "memory_type": "public vs private episodic observation (visibility of past rounds)",
            "memory_description": "In 'Explicit' setting the end-of-round results (who went to the bar, overcrowding outcome) are included in the prompt for all agents; in 'Implicit' setting agents who stayed home do not receive those observations (their prompt omits that outcome), creating asymmetric/missing episodic information.",
            "task_name": "El Farol Bar (multi-round coordination with capacity R)",
            "task_description": "Each round, players independently decide to go or stay; if proportion going ≤ R, going yields higher utility; otherwise going is worse than staying. Agents must learn a mixed strategy to stabilize attendance near capacity R across rounds.",
            "benchmark_name": "γ-Bench — El Farol Bar scenario",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Reported qualitative effect: when historical outcomes are explicitly observable by all agents ('Explicit'), probabilities of going stabilize faster and at higher levels than in the 'Implicit' setting; in 'Implicit', agents need more rounds (Rounds 2–6) to infer bar availability and their average probability of going is lower. No precise numeric head-to-head performance metrics (e.g., final score per condition) are provided in the main scoring table (the paper states they evaluated only the Implicit setting for the main score), so only qualitative differences are reported.",
            "limitations_or_challenges": "No full quantitative ablation with both conditions across all models; the asymmetric information design shows that missing observations slow learning and produce lower participation probabilities, but exact effect sizes are not reported in a comparable numeric form in the paper's main results.",
            "key_insights": "Access to shared episodic outcomes (public memory) speeds up coordination and stabilizes mixed strategies; asymmetric or missing observations act like restricted memory and degrade adaptation speed, indicating that information visibility is a critical component of effective memory usage in multi-agent LLM systems.",
            "uuid": "e4772.1",
            "source_info": {
                "paper_title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Training-data memory / test contamination",
            "name_full": "Model memorization of training examples and test-set leakage concerns",
            "brief_description": "Discussion and experiments probing whether vanilla game settings are present in training data (data contamination) and whether LLMs 'remember' canonical game outcomes, affecting measured generalizability.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "Various LLMs (discussion of training memory)",
            "agent_description": "Paper-wide discussion refers to multiple LLMs (GPT-3.5, GPT-4, Gemini, LLaMA variants) and the risk that canonical/static game settings may have been encountered during pretraining, enabling models to recall solutions rather than reason.",
            "memory_type": "long-term training memorization (pretraining data memory)",
            "memory_description": "The paper points out that fixed, classical game settings increase the likelihood that LLMs encountered similar scenarios during pretraining (memorized training examples), and thus γ-Bench uses dynamic parameterization to reduce test-set leakage; this is a mention of long-term memory effects due to dataset memorization, not an implemented retrieval mechanism.",
            "task_name": "Generalizability evaluation across varied game parameters",
            "task_description": "The paper varies game parameters to test whether models' correct responses generalize beyond canonical settings or reflect memorized answers from pretraining; experiments include varying ratios in Guess 2/3, bar capacity R, return rates in Public Goods, gold totals in Divide the Dollar, etc.",
            "benchmark_name": "γ-Bench (parameterized settings to test generalization / contamination)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The paper reports variable generalizability: models sometimes behave correctly across parameter changes (games 1,3,5,6,8) suggesting reasoning; in games 2 and 4 (El Farol Bar and Public Goods) models show low generalizability and consistent patterns indicative of either poor reasoning or memorized heuristics; the authors do not provide a direct experimental ablation isolating training-data memorization vs reasoning.",
            "limitations_or_challenges": "No definitive decomposition of whether correct answers stem from reasoning vs memorized pretraining examples; detecting and eliminating test-set leakage remains challenging and is cited as a motivation for γ-Bench's dynamic generation of scenarios.",
            "key_insights": "Dynamic, parameterized scenario generation reduces risk of test-set leakage and is necessary to evaluate whether LLMs use memorized training artifacts or genuinely reason across varied environments; vigilance against training-data memory impacting evaluation is recommended.",
            "uuid": "e4772.2",
            "source_info": {
                "paper_title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GTBench: Uncovering the strategic reasoning capabilities of llms via game-theoretic evaluations",
            "rating": 2
        },
        {
            "paper_title": "MAGIC: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration",
            "rating": 2
        },
        {
            "paper_title": "GameEval: Evaluating llms on conversational games",
            "rating": 1
        },
        {
            "paper_title": "SmartPlay: A benchmark for llms as intelligent agents",
            "rating": 1
        },
        {
            "paper_title": "Long-horizon dialogue understanding for role identification in the game of Avalon with large language models",
            "rating": 1
        }
    ],
    "cost": 0.01260375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming Ability in Multi-Agent Environments</h1>
<p>Jen-tse Huang ${ }^{1,2}$ Eric John Li ${ }^{1}$ Man Ho Lam ${ }^{1}$ Tian Liang ${ }^{4,2}$ Wenxuan Wang ${ }^{1,2 <em>}$<br>Youliang Yuan ${ }^{3,2}$ Wenxiang Jiao ${ }^{2 </em>}$ Xing Wang ${ }^{2}$ Zhaopeng Tu ${ }^{2}$ Michael R. Lyu ${ }^{1}$<br>${ }^{1}$ The Chinese University of Hong Kong ${ }^{2}$ Tencent AI Lab<br>${ }^{3}$ The Chinese University of Hong Kong, Shenzhen ${ }^{4}$ Tsinghua University</p>
<h4>Abstract</h4>
<p>Decision-making is a complex process requiring diverse abilities, making it an excellent framework for evaluating Large Language Models (LLMs). Researchers have examined LLMs' decision-making through the lens of Game Theory. However, existing evaluation mainly focus on two-player scenarios where an LLM competes against another. Additionally, previous benchmarks suffer from test set leakage due to their static design. We introduce GAMA( $\gamma$ )-Bench, a new framework for evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes eight classical game theory scenarios and a dynamic scoring scheme specially designed to quantitatively assess LLMs' performance. $\gamma$-Bench allows flexible game settings and adapts the scoring system to different game parameters, enabling comprehensive evaluation of robustness, generalizability, and strategies for improvement. Our results indicate that GPT-3.5 demonstrates strong robustness but limited generalizability, which can be enhanced using methods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2. Gemini-1.5-Pro outperforms others, scoring of 69.8 out of 100, followed by LLaMA-3.1-70B (65.9) and Mixtral-8x22B (62.4). Our code and experimental results are publicly available at https://github.com/CUHK-ARISE/GAMABench.</p>
<h2>1 INTRODUCTION</h2>
<p>We have recently witnessed the advancements in artificial intelligence made by Large Language Models (LLMs), which have marked a significant breakthrough in the field. ChatGPT ${ }^{1}$, a leading LLM, has demonstrated its proficiency in a variety of natural language processing tasks, including machine translation (Jiao et al., 2023), sentence revision (Wu et al., 2023), information retrieval (Zhu et al., 2023), and program repair (Surameery \&amp; Shakor, 2023). Beyond the academic sphere, LLMs have entered diverse aspects of our everyday life, such as education (Baidoo-Anu \&amp; Ansah, 2023), legal service (Guha et al., 2023), product design (Lanzi \&amp; Loiacono, 2023), and healthcare (Johnson et al., 2023). Given their extensive capabilities, evaluating LLMs demands more than simple, isolated tasks. A comprehensive and multifaceted approach is highly in demand to assess the efficacy of these advanced models.</p>
<p>With the broad knowledge encoded in LLMs, their intelligence (Liang et al., 2024), and capabilities in general-purpose task solving (Qin et al., 2023), a question emerges: Can LLMs assist in everyday decision-making? Many real-world decision-making scenarios can be modeled using Game Theory (Koller \&amp; Pfeffer, 1997). Furthermore, individuals' ability to achieve Nash equilibrium (Nash, 1950) reflects their capacity in decision-making (Risse, 2000). Therefore, many studies have drawn on the principles of game theory (Duan et al., 2024; Xie et al., 2024; Xu et al., 2024a), which has several advantages: (1) Scope: Game theory allows for the abstraction of diverse real-life scenarios into simple mathematical models, facilitating a broad range of evaluations. (2) Quantifiability: By</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Performance (scores) of different LLMs on $\gamma$-Bench.</p>
<table>
<thead>
<tr>
<th>$\gamma$-Bench Leaderboard</th>
<th>GPT-3.5</th>
<th></th>
<th></th>
<th>GPT-4</th>
<th></th>
<th>Gemini-Pro</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>0613</td>
<td>1106</td>
<td>0125</td>
<td>t-0125</td>
<td>o-0806</td>
<td>1.0</td>
<td>1.5</td>
</tr>
<tr>
<td>Guess 2/3 of the Average</td>
<td>$41.4_{ \pm 0.5}$</td>
<td>$68.5_{ \pm 0.5}$</td>
<td>$63.4_{ \pm 3.4}$</td>
<td>$91.6_{ \pm 0.6}$</td>
<td>$94.3_{ \pm 0.6}$</td>
<td>$77.3_{ \pm 6.2}$</td>
<td>$95.4_{ \pm 0.5}$</td>
</tr>
<tr>
<td>El Farol Bar</td>
<td>$74.8_{ \pm 4.5}$</td>
<td>$64.3_{ \pm 3.1}$</td>
<td>$68.7_{ \pm 2.7}$</td>
<td>$23.0_{ \pm 8.0}$</td>
<td>$70.0_{ \pm 22.1}$</td>
<td>$33.5_{ \pm 10.3}$</td>
<td>$37.2_{ \pm 4.2}$</td>
</tr>
<tr>
<td>Divide the Dollar</td>
<td>$42.4_{ \pm 7.7}$</td>
<td>$70.3_{ \pm 3.3}$</td>
<td>$68.6_{ \pm 2.4}$</td>
<td>$98.1_{ \pm 1.9}$</td>
<td>$95.2_{ \pm 0.7}$</td>
<td>$77.6_{ \pm 3.6}$</td>
<td>$93.8_{ \pm 0.3}$</td>
</tr>
<tr>
<td>Public Goods Game</td>
<td>$17.7_{ \pm 1.7}$</td>
<td>$43.5_{ \pm 12.6}$</td>
<td>$38.9_{ \pm 8.1}$</td>
<td>$89.2_{ \pm 1.8}$</td>
<td>$90.9_{ \pm 3.0}$</td>
<td>$68.5_{ \pm 7.6}$</td>
<td>$100.0_{ \pm 0.0}$</td>
</tr>
<tr>
<td>Diner’s Dilemma</td>
<td>$67.0_{ \pm 4.9}$</td>
<td>$1.4_{ \pm 1.3}$</td>
<td>$2.8_{ \pm 2.8}$</td>
<td>$0.9_{ \pm 0.7}$</td>
<td>$10.7_{ \pm 8.3}$</td>
<td>$3.1_{ \pm 1.5}$</td>
<td>$35.9_{ \pm 5.3}$</td>
</tr>
<tr>
<td>Sealed-Bid Auction</td>
<td>$10.3_{ \pm 0.2}$</td>
<td>$7.6_{ \pm 1.8}$</td>
<td>$13.0_{ \pm 1.5}$</td>
<td>$24.2_{ \pm 1.1}$</td>
<td>$20.8_{ \pm 3.2}$</td>
<td>$31.6_{ \pm 12.2}$</td>
<td>$26.9_{ \pm 9.4}$</td>
</tr>
<tr>
<td>Battle Royale</td>
<td>$19.5_{ \pm 7.7}$</td>
<td>$35.7_{ \pm 6.8}$</td>
<td>$28.6_{ \pm 11.0}$</td>
<td>$86.8_{ \pm 9.7}$</td>
<td>$67.3_{ \pm 14.8}$</td>
<td>$16.5_{ \pm 6.9}$</td>
<td>$81.3_{ \pm 7.7}$</td>
</tr>
<tr>
<td>Pirate Game</td>
<td>$68.4_{ \pm 19.9}$</td>
<td>$69.5_{ \pm 14.6}$</td>
<td>$71.6_{ \pm 7.7}$</td>
<td>$85.4_{ \pm 8.7}$</td>
<td>$84.4_{ \pm 6.7}$</td>
<td>$57.4_{ \pm 14.3}$</td>
<td>$87.9_{ \pm 5.6}$</td>
</tr>
<tr>
<td>Overall</td>
<td>$42.7_{ \pm 2.0}$</td>
<td>$45.1_{ \pm 1.6}$</td>
<td>$44.4_{ \pm 2.1}$</td>
<td>$62.4_{ \pm 2.7}$</td>
<td>$66.7_{ \pm 4.7}$</td>
<td>$45.7_{ \pm 3.4}$</td>
<td>$69.8_{ \pm 1.6}$</td>
</tr>
</tbody>
</table>
<p>(a) Closed-source LLMs: Gemini-1.5-Pro outperforms. For GPT-4: t denotes Turbo and o denotes Omni.</p>
<table>
<thead>
<tr>
<th>$\gamma$-Bench Leaderboard</th>
<th>LLaMA-3.1</th>
<th></th>
<th></th>
<th>Mixtral</th>
<th></th>
<th>Qwen-2</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>8B</td>
<td>70B</td>
<td>405B</td>
<td>8x7B</td>
<td>8x22B</td>
<td>72B</td>
<td></td>
</tr>
<tr>
<td>Guess 2/3 of the Average</td>
<td>$85.5_{ \pm 3.0}$</td>
<td>$84.0_{ \pm 1.7}$</td>
<td>$94.3_{ \pm 0.6}$</td>
<td>$91.8_{ \pm 0.4}$</td>
<td>$83.6_{ \pm 4.6}$</td>
<td>$93.2_{ \pm 1.3}$</td>
<td></td>
</tr>
<tr>
<td>El Farol Bar</td>
<td>$75.7_{ \pm 2.2}$</td>
<td>$59.7_{ \pm 3.5}$</td>
<td>$20.5_{ \pm 24.2}$</td>
<td>$66.8_{ \pm 5.8}$</td>
<td>$39.3_{ \pm 12.2}$</td>
<td>$17.0_{ \pm 25.5}$</td>
<td></td>
</tr>
<tr>
<td>Divide the Dollar</td>
<td>$56.4_{ \pm 8.4}$</td>
<td>$87.0_{ \pm 4.1}$</td>
<td>$94.9_{ \pm 1.0}$</td>
<td>$1.2_{ \pm 2.8}$</td>
<td>$79.0_{ \pm 9.6}$</td>
<td>$91.9_{ \pm 2.4}$</td>
<td></td>
</tr>
<tr>
<td>Public Goods Game</td>
<td>$19.6_{ \pm 1.0}$</td>
<td>$90.6_{ \pm 3.6}$</td>
<td>$97.0_{ \pm 0.8}$</td>
<td>$27.6_{ \pm 11.7}$</td>
<td>$83.7_{ \pm 3.5}$</td>
<td>$81.3_{ \pm 5.9}$</td>
<td></td>
</tr>
<tr>
<td>Diner’s Dilemma</td>
<td>$59.3_{ \pm 2.4}$</td>
<td>$48.1_{ \pm 5.7}$</td>
<td>$14.4_{ \pm 4.5}$</td>
<td>$76.4_{ \pm 7.1}$</td>
<td>$79.9_{ \pm 5.8}$</td>
<td>$0.0_{ \pm 0.0}$</td>
<td></td>
</tr>
<tr>
<td>Sealed-Bid Auction</td>
<td>$37.1_{ \pm 3.1}$</td>
<td>$15.7_{ \pm 2.7}$</td>
<td>$14.7_{ \pm 3.2}$</td>
<td>$3.1_{ \pm 1.6}$</td>
<td>$13.2_{ \pm 3.7}$</td>
<td>$2.5_{ \pm 0.7}$</td>
<td></td>
</tr>
<tr>
<td>Battle Royale</td>
<td>$35.9_{ \pm 12.1}$</td>
<td>$77.7_{ \pm 26.0}$</td>
<td>$92.7_{ \pm 10.1}$</td>
<td>$12.6_{ \pm 9.4}$</td>
<td>$36.0_{ \pm 21.0}$</td>
<td>$81.7_{ \pm 9.6}$</td>
<td></td>
</tr>
<tr>
<td>Pirate Game</td>
<td>$78.3_{ \pm 10.0}$</td>
<td>$64.0_{ \pm 15.5}$</td>
<td>$65.6_{ \pm 22.3}$</td>
<td>$67.3_{ \pm 7.6}$</td>
<td>$84.3_{ \pm 8.8}$</td>
<td>$86.1_{ \pm 6.4}$</td>
<td></td>
</tr>
<tr>
<td>Overall</td>
<td>$56.0_{ \pm 3.1}$</td>
<td>$65.9_{ \pm 3.3}$</td>
<td>$61.8_{ \pm 4.7}$</td>
<td>$43.4_{ \pm 2.2}$</td>
<td>$62.4_{ \pm 2.2}$</td>
<td>$56.7_{ \pm 3.4}$</td>
<td></td>
</tr>
</tbody>
</table>
<p>(b) Open-source LLMs: LLaMA-3.1-70B outperforms.
examining the Nash equilibrium within these models, we gain a measurable metric for comparing LLMs’ decision-making performance. (3) Variability: The adjustable parameters of these models enable the creation of variant scenarios, enhancing the diversity and robustness of our assessments. However, existing research is often limited to two-player or two-action settings, such as the classical Prisoner’s Dilemma and Ultimatum Game <em>(Guo, 2023; Phelps &amp; Russell, 2023; Akata et al., 2023; Aher et al., 2023; Brookins &amp; DeBacker, 2024)</em>. Moreover, prior work relies on fixed, classical game settings, increasing the likelihood that LLMs have encountered these scenarios during training, facing the risk of test set leakage. In this paper, we assess LLMs in more complex scenarios involving multiple players, actions, and rounds, across classical game theory scenarios with dynamically adjustable game parameters.</p>
<p>We include eights games and divide them into three categories based on their characteristics. The first category in our framework evaluates LLMs’ ability to make optimal decisions by understanding game rules and recognizing patterns in other players’ behavior. A distinctive characteristic of these games is that individual players cannot achieve higher gains without cooperation, provided that other participants cooperate. Essentially, these games’ Nash equilibrium aligns with maximizing overall social welfare. We name such games as I. Cooperative Games, including (1) Guess 2/3 of the Average, (2) El Farol Bar, and (3) Divide the Dollar. The second category assesses the propensity of LLMs to prioritize self-interest, potentially betraying others for greater gains. In contrast to the first category, games in this category incentivize higher rewards for participants who betray their cooperative counterparts. Typically, the Nash equilibrium in these games leads to reduced social welfare. This category is termed II. Betraying Games, including (4) Public Goods Game, (5) Diner’s Dilemma, (6) Sealed-Bid Auction. Last but not least, we focus specifically on two games characterized by sequential decision-making processes, distinguishing them from the previous six games based on simultaneous decision-making. III. Sequential Games are the (7) Battle Royale and (8) Pirate Game.</p>
<p>Decision-making is a complex task requiring various abilities. Several common ones are evaluated across all games: (1) Perception: the ability to understand situations, environments, and rules, and extends to long-text understanding for LLMs. (2) Arithmetic Reasoning: the ability to quantify real-world options and perform calculations. (3) ToM Reasoning: the Theory of Mind *(Kosinski,</p>
<p>2024; Bubeck et al., 2023; Huang et al., 2024a) refers to the ability to infer others' intentions and beliefs. (4) Strategic Reasoning: the ability to integrate all available information to arrive at the best decision. Certain games involve specialized abilities, such as K-level reasoning in the "Guess $2 / 3$ of the Average" game and mixed strategy adoption in the "El Farol Bar" game.
In this paper, we instruct ten agents, based on the GPT-3.5 (0125) model, to engage in the eight games, followed by an analysis of the results obtained. Subsequently, we assess the model's robustness against multiple runs, temperature parameter alterations, and prompt template variations. Further exploration is conducted to ascertain if instructional prompts, such as Chain-of-Thought (CoT) (Kojima et al., 2022), enhance the model's decision-making capabilities. Additionally, the model's capacity to generalize across diverse game settings is examined. Finally, we evaluate the performance of thirteen LLMs, including GPT-3.5-Turbo (0613, 1106, 0125) (OpenAI, 2022), GPT4 (Turbo-0125, 40-0806) (OpenAI, 2023), Gemini-1.0-Pro (Pichai \&amp; Hassabis, 2023), Gemini-1.5Pro (Pichai \&amp; Hassabis, 2024), LLaMA-3.1 (8B, 70B, 405B) (Dubey et al., 2024), Mixtral (8x7B, 8x22B) (Jiang et al., 2024), and Qwen-2-72B (Yang et al., 2024). We compare the performance of different LLMs by creating multiple agents from the same model to participate in the games, then calculate the average performance of these agents. Our contributions include:</p>
<ul>
<li>We provide a comprehensive review and comparison of existing literature on evaluating LLMs using game theory scenarios, as summarized in Table 3. The review includes key aspects such as models, games, temperature settings, and other game parameters, highlighting our emphasis on the multi-player setting and the generalizability of LLMs.</li>
<li>Starting from the multi-player setting, we collect eight classical game theory scenarios to measure LLMs' Gaming Ability in Multi-Agent environments, and implement our framework, GAMA( $\gamma$ )Bench. It enables dynamic game scene generation with diverse profiles, offering unlimited scenarios to assess LLM generalizability while minimizing test set leakage risk.</li>
<li>We apply $\gamma$-Bench to thirteen LLMs to provide an in-depth analysis of their performance in multiagent gaming scenarios, indicating their potential as assistants in decision-making process.</li>
</ul>
<h1>2 INTRODUCTION TO GAMES</h1>
<p>We collect eight games well studied in Game Theory and propose $\gamma$-Bench, a framework with multiplayer, multi-round, and multi-action settings. Notably, $\gamma$-Bench allows the simultaneous participation of both LLMs and humans, enabling us to evaluate LLMs' performance when playing against humans or fixed strategies. This section details each game with their classical settings (parameters).</p>
<h3>2.1 COOPERATIVE GAMES</h3>
<p>(1) Guess 2/3 of the Average Initially introduced by Ledoux (1981), the game involves players independently selecting an integer between 0 and 100 (inclusive). The winner is the player(s) choosing the number closest to two-thirds of the group's average. A typical initial strategy might lead players to assume an average of 50, suggesting a winning number around $50 \times \frac{2}{3} \approx 33$. However, if all participants adopt this reasoning, the average shifts to 33, thereby altering the winning number to approximately 22. The game has a Pure Strategy Nash Equilibrium (PSNE) where all players selecting zero results in a collective win.
(2) El Farol Bar Proposed by Arthur (1994) and Huberman (1988), this game requires players to decide to either visit a bar for entertainment or stay home without communication. The bar, however, has a limited capacity and can only accommodate part of the population. In a classical scenario, the bar becomes overcrowded and less enjoyable if more than $60 \%$ of the population decides to go there. Conversely, if $60 \%$ or fewer people are present, the experience is more enjoyable than staying home. Imagine that if everyone adopts the same pure strategy, i.e., either everyone going to the bar or everyone staying home, then the social welfare is not maximized. Notably, the game lacks a PSNE but presents an Mixed Strategy Nash Equilibrium (MSNE), where the optimal strategy involves going to the bar with a $60 \%$ probability and staying home with a $40 \%$ probability.
(3) Divide the Dollar Firstly mentioned in Shapley \&amp; Shubik (1969), the game involves two players independently bidding up to 100 cents for a dollar. Ashlock \&amp; Greenwood (2016) further</p>
<p>generalized the game into a multi-player setting. If the sum of bids is at most one dollar, each player is awarded their respective bid; if the total exceeds a dollar, no player receives anything. The NE of this game occurs when each player bids exactly $\frac{100}{N}$ cents.</p>
<h1>2.2 Betraying Games</h1>
<p>(4) Public Goods Game Studied since the early 1950s (Samuelson, 1954), the game requires $N$ players to secretly decide how many of their private tokens to contribute to a public pot. The tokens in the pot are then multiplied by a factor $R(1<R\<N)$, and the resulting "public good" is evenly distributed among all players. Players retain any tokens they do not contribute. A simple calculation reveals that for each token a player contributes, their net gain is $\frac{R}{N}-1$, which is less than zero. This suggests that the rational strategy for each player is to contribute no tokens, which reaches an NE of this game. The game serves as a tool to investigate tendencies towards selfish behavior and free-riding among participants.
(5) Diner's Dilemma This game is the multi-player variant of the Prisoner's Dilemma (Glance \& Huberman, 1994). The game involves $N$ players dining together, with their decision to split all the costs. Each player needs to independently choose whether to order the expensive or the cheap dish, priced at $x$ and $y(x>y)$, respectively. The expensive offers $a$ utility per individual, surpassing the $b$ utility of another choice $(a>b)$. The game satisfies two assumptions: (1) $a-x<b-y$ : Although the expensive dish provides a greater utility, the benefit does not justify its higher cost, leading to a preference for the cheap one when dining alone. (2) $a-\frac{x}{N}>b-\frac{y}{N}$ : Individuals are inclined to choose the expensive dish when the cost is shared among all diners. The assumptions lead to an NE where all players opt for the more expensive meal. However, this PSNE results in a lower total social welfare of $N(a-x)$ compared to $N(b-y)$, which is the utility if all choose the cheap one. This game evaluates the long-term perspective and the capacity to establish sustained cooperation.
(6) Sealed-Bid Auction The Sealed-Bid Auction (SBA) involves players submitting their bids confidentially and simultaneously, different from the auctions where bids are made openly in a sequential manner. We consider two variants of SBA: the First-Price Sealed-Bid Auction (FPSBA) and the Second-Price Sealed-Bid Auction (SPSBA). In FPSBA, also known as the Blind Auction, if all players bid their true valuation $v_{i}$ of the item, the winner achieves a net gain of $b_{i}-v_{i}=0$ while others also gain nothing (McAfee \&amp; McMillan, 1987). Moreover, the highest bidder will discover that to win the auction, it is sufficient to bid marginally above the second-highest bid. Driven by these two factors, FPSBA is often deemed inefficient in practical scenarios, as bidders are inclined to submit bids significantly lower than their actual valuation, resulting in suboptimal social welfare. In contrast, SPSBA, commonly called the Vickrey auction, requires the winner to pay the second-highest bid, encouraging truthful bidding by all players (Vickrey, 1961). It can be proven that bidding true valuations in SPSBA represents an NE. This auction evaluates agent performance in imperfect information games, where agents lack knowledge of other players' valuations.</p>
<h3>2.3 SEQUENTIAL GAMES</h3>
<p>(7) Battle Royale Extended from the Truel (Kilgour, 1975) involving three players, the Battle Royale involves $N$ players shooting at each other. In the widely studied form (Kilgour \&amp; Brams, 1997), players have different probabilities of hitting the target, with the turn order set by increasing hit probabilities. The game allows for unlimited bullets and the tactical option of intentionally missing shots. The objective for each participant is to emerge as the sole survivor, with the game ending when only one player remains. While the NE has been identified for infinite sequential truels (Kilgour, 1977), the complexity of these equilibria escalates exponentially with an increased number of players.
(8) Pirate Game This game is a multi-player version of the Ultimatum Game (Goodin, 1998; Stewart, 1999). Each player is assigned a "pirate rank", determining their action order. The game involves $N$ pirates discussing the division of $G$ golds they have discovered. The most senior pirate first proposes a distribution method. If the proposal is approved by at least half of the pirates, including the proposer, the game ends, and the gold is distributed as proposed. Otherwise, the most senior pirate is thrown overboard, and the next in rank assumes the proposer role until the game</p>
<p>ends. Each pirate's objectives are prioritized as (1) survival, (2) maximizing their share of gold, and (3) the opportunity to eliminate others from the game. Stewart (1999) identifies the optimal strategy, where the most senior pirate allocates one gold to each odd-ranked pirate and keeps the remainder.</p>
<h1>3 GAMA-Bench Scoring Scheme</h1>
<p>This section presents experiments conducted using the default settings for each game on the GPT-3.5 (0125) model. Utilizing this model as a case study, we illustrate our methodology for benchmarking an LLM with $\gamma$-Bench. The prompt and its design method can be found in $\S \mathrm{C}$ in the appendix. Each game involves ten agents based on GPT-3.5, with the temperature parameter set to one. For simultaneous games, there will be twenty rounds. We run each game five times to enhance the reliability of our findings and mitigate the impact of variance. For clarity and conciseness, this section presents one of the five runs while $\S 4.1$ details quantitative results. Our findings of GPT3.5's behaviors on $\gamma$-Bench include:</p>
<h2>Key Findings:</h2>
<ul>
<li>The model's decisions are mainly influenced by the outcomes of the preceding round rather than deriving from the reasoning of the optimal strategy.</li>
<li>Although initially demonstrating suboptimal performance, the model can learn from historical data and enhance its performance over time. A larger fluctuation is observed in games that are difficult to optimize from historical data, such as the El Farol Bar game.</li>
<li>The model demonstrates the ability to engage in spontaneous cooperation, leading to increased social welfare beyond mere self-interest, without the necessity for explicit communication. However, this phenomenon also results in low performance in Betraying Games.</li>
<li>The model shows limitations in sequential games with more complicated rules.</li>
<li>The aggregate score of the model on $\gamma$-Bench is 45.9 .</li>
</ul>
<h3>3.1 COOPERATIVE GAMES</h3>
<p>(1) Guess 2/3 of the Average [TO PROMPT] The vanilla setting for this game is $M I N=0$, $M A X=100$, and $R=\frac{2}{3}$. We show the choices made by all agents as well as the average and the winning numbers in Fig. 1(1). Key observations are: (1) In the first round, agents consistently select 50 (or close to 50), corresponding to the mean of a uniform distribution ranging from 0 to 100 . This behavior suggests that the model fails to recognize that the winning number is $\frac{2}{3}$ of the average. (2) As rounds progress, the average number selected decreases noticeably, demonstrating that agents are capable of adapting based on historical outcomes. Since the optimal strategy is to choose the $M I N$, the score in this game is given by $S_{1}=\frac{1}{N R} \sum_{i j}\left(C_{i j}-M I N\right)$, where $C_{i j}$ is the chosen number of player $i$ in round $j$. The model scores ${ }^{2} 65.4$ on this game.
(2) El Farol Bar [TO PROMPT] The vanilla setting for this game is $M I N=0, M A X=10$, $H O M E=5$, and $R=60 \%$. To explore the influence of incomplete information, we introduce two settings: Explicit indicates that everyone can see the results at the end of each round, while Implicit indicates that those staying at home cannot know what happened in the bar after the round ends. Fig. 1(2) illustrates the probability of agents deciding to go to the bar and the total number of players in the bar. We find that: (1) In the first round, there is an inclination among agents to visit the bar. Observations of overcrowding lead to a preference for staying home, resulting in fluctuations shown in both Fig. 1(2-1) and Fig. 1(2-2). In the Implicit setting, due to the lack of direct observations of the bar's occupancy, agents require additional rounds (Rounds 2 to 6) to discern the availability of space in the bar. (2) The probability of agents going to the bar gradually stabilizes, with the average probability in the Implicit setting being lower than in the Explicit setting. Since the optimal strategy is to choose the go with a probability of $R$, the raw score ${ }^{3}$ in this game is given by</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance of GPT-3.5 (0125) in Cooperative and Betraying games. $S_{2}=\left.\frac{1}{K} \sum_{j}\right|<em i="i">{N} ^{1} \sum</em>=0$ when player $i$ chose to stay. The model scores 73.3 on this game.
(3) Divide the Dollar [TO PROMPT] The vanilla setting for this game is $G=100$. We plot the proposals by all agents and the sum of their proposals in Fig. 1(3). Our analysis reveals the following insights: (1) In the first round, agents' decisions align with the NE predictions of the game. However, after gaining golds, agents exhibit increased greed, proposing allocations that exceed the NE-prescribed amounts. Upon receiving nothing, they tend to propose a "safer" amount. The trend continues and causes fluctuations across subsequent rounds. (2) Despite these fluctuations, the average of proposed golds converges to approximately 100 . Since the optimal strategy is to propose $G / N$, the raw score in this game is given by $S_{3}=\frac{1}{K} \sum_{j}\left|\sum_{i} B_{i j}-G\right|$, where $B_{i j}$ is the proposed amount number of player $i$ in round $j$. The model scores 68.1 on this game.} D_{i j}-R \mid$, where $D_{i j}=1$ when player $i$ chose to go in round $j$ and $D_{i j</p>
<h1>3.2 Betraying Games</h1>
<p>(4) Public Goods Game [TO PROMPT] The vanilla setting for this game is $R=2$. Each player has $T=20$ to contribute in each round. Fig. 1(4) shows the contributed tokens by each agent and their corresponding gains per round. The observations reveal the following: (1) Despite an investment return of $-80 \%$, agents display a pattern of alternating between free-riding and contributing all their tokens. (2) As the rounds progress, there is an evident increase in the number of tokens contributed to the public pot, leading to an overall enhancement in social welfare gains. These findings suggest that the LLM exhibits cooperative behavior, prioritizing collective benefits over individual self-interest. Since we expect the model to infer the optimal strategy, i.e., contributing zero tokens, the raw score in this game is given by $S_{4}=\frac{1}{N K} \sum_{i j} C_{i j}$, where $C_{i j}$ is the proposed contribution amount of player $i$ in round $j$. The model scores 41.2 on this game.
(5) Diner's Dilemma [TO PROMPT] The vanilla setting for this game is $P_{h}=20, P_{l}=10$, $U_{h}=20, U_{l}=15$. We show the probability of agents choosing the costly dish, their resulting utilities, and the average bill in Fig. 1(5). Analysis of the figure reveals the following insights: (1) Contrary to the NE predictions for this game, agents predominantly prefer the cheap dish, which maximizes total social welfare. (2) Remarkably, a deviation from cooperative behavior is observed wherein one agent consistently chooses to betray others, thereby securing a higher utility. This pattern of betrayal by this agent persists across subsequent rounds. Since we expect the model to infer the the optimal strategy, i.e., choosing the costly dish, the raw score in this game is given by $S_{5}=\frac{1}{N K} \sum_{i j} D_{i j}$, where $D_{i j}=1$ when player $i$ chose the cheap dish in round $j$ and $D_{i j}=0$ when player $i$ chose the costly dish. The model scores 4.0 on this game.
(6) Sealed-Bid Auction [TO PROMPT] For the vanilla setting in this game, we randomly assign valuations to each agent in each round, ranging from 0 to 200 . We fix the seed for random number generation to ensure fair comparisons across various settings and models. We evaluate LLMs' performance under both First-Price and Second-Price settings. Fig. 1(6) depicts the subtraction</p>
<p>between valuations and bids and bid amounts of each agent. Our key findings include: (1) As introduced in $\S 2.2$, we note that agents generally submit bids that are lower than their valuations in the First-Price auction, a tendency indicated by the positive discrepancies between valuations and bids depicted in Fig. 1(6-1). (2) Though the NE suggests that everyone bids the amount of their valuation in the Second-Price setting, we find a propensity for bidding below valuation levels, as demonstrated in Fig. 1(6-2). Since the optimal strategy is to bid the prices lower than their true valuations, ${ }^{4}$ the raw score in this game is given by $S_{6}=\frac{1}{N K} \sum_{i j} \frac{v_{i j}-b_{i j}}{v_{i j}}$, where $v_{i j}$ and $b_{i j}$ are player $i$ 's valuation and bid in round $j$, respectively. The model scores 14.6 on this game.</p>
<h1>3.3 SEQUENTIAL GAMES</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GPT-3.5 (0125)'s performance in "Battle Royale." (a): Agents' actions and outcomes of each round. For example, in round 11, player 6 shot at player 7 but missed.
(7) Battle Royale [TO PROMPT] For the vanilla setting in this game, we assign varied hit rates to each agent, spanning from $35 \%$ to $80 \%$ in increments of $5 \%$. This setting covers a broad spectrum of hit rates, avoiding extremes of $0 \%$ or $100 \%$. Fig. 2 illustrates the actions and outcomes of each round, along with the tally of participants remaining. Our observations reveal: (1) Unlike our expectations, agents rarely target the player with the highest hit rate. (2) Agents neglect to utilize the strategy of "intentionally missing." For example, in round 19, with players 7,8 , and 10 remaining, it was player 7's turn to act. The optimal strategy for player 7 would have been to intentionally miss the shot, thereby coaxing player 8 into eliminating player 10 , enabling player 7 to target player 8 in the following round for a potential victory. Instead, player 7 opted to target player 10 , resulting in player 8 firing upon itself. For simplicity, we evaluate whether agents target the player with the highest hit rate (excluding themselves). Therefore, the raw score in this game is given by $S_{7}=\frac{1}{N k} \sum_{i j} I_{i j}$, where $k$ represents the number of rounds played and $I_{i j}=1$ if player $i$ targets the player with the highest hit rate in round $j$, and $I_{i j}=0$ otherwise. The model scores 20.0 on this game.</p>
<p>Table 2: Performance of GPT-3.5 (0125) in the "Pirate Game." Each row shows the proposed gold distribution in the specific round and whether each pirate accepts (" $\checkmark$ ") or rejects (" $\boldsymbol{X}$ ") the proposal. $S_{8 P}$ shows the score of the proposer while $S_{8 V}$ shows the score of all voters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Pirate Rank</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">$S_{8 P}$</th>
<th style="text-align: center;">$S_{8 V}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Round 1</td>
<td style="text-align: center;">$100 \checkmark$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Round 2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$99 \checkmark$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$0 \checkmark$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">$0 \times$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Round 3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$50 \checkmark$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$1 \checkmark$</td>
<td style="text-align: center;">$44 \checkmark$</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">0.57</td>
</tr>
</tbody>
</table>
<p>(8) Pirate Game [TO PROMPT] The vanilla setting for this game is $G=100$. As introduced in $\S 2.3$, the optimal strategy for the first proposer is to allocate 96 golds to itself and one gold each to the third, fifth, seventh, and ninth pirates. Stewart (1999) has elucidated the optimal strategy for voters: (1) accept if allocated two or more golds; (2) reject if no golds are allocated; (3) accept if one gold is allocated and it shares the same parity as the proposer, otherwise, reject. Table 2 presents a sample game's proposals and voting results. The key conclusion is that agents fail to propose optimal proposals and frequently cast incorrect votes, suggesting that the LLM demonstrates suboptimal performance in this game. Two aspects are considered to comprehensively evaluate a model's performance: (1) whether proposers give a reasonable proposal and (2) whether voters act correctly towards a given proposal. For (1), we calculate the $L_{1}$ norm between the given proposal and the optimal strategy, defined as $S_{8 P}=\frac{1}{k} \sum_{j}\left|P_{j}-O_{j}\right|<em j="j">{1}$, where $P</em>$ represents the model's</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>proposal and $O_{j}$ denotes the optimal proposal in round $j$, with the game ending at round $k$. For (2), we calculate the accuracy of choosing the right action elucidated above, which is: $S_{8 V}=$ $\frac{1}{k\left(2 N-k-1\right)} \sum_{i j} I_{i j}$, where $I_{i j}=1$ if player $i$ votes correctly in round $j$ and $I_{i j}=0$ otherwise, excluding the proposer from the calculation. The model scores 80.6 on this game.</p>
<h1>4 BEYOND DEFAULT SETTINGS</h1>
<p>This section explores deeper into several following Research Questions (RQs). RQ1 Robustness: Is there a significant variance in multiple runs? Is the performance sensitive to different temperatures and prompt templates? RQ2 Reasoning Strategies: Are strategies to enhance reasoning skills applicable to game scenarios? This includes implementing Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022) reasoning and assigning unique personas to LLMs. RQ3 Generalizability: How does LLM performance vary with different game settings? Do LLMs remember answers learned during the training phase? RQ4 Leaderboard: How do various LLMs perform on $\gamma$-Bench? Unless otherwise specified, we apply the vanilla settings described in $\S 3$.</p>
<h3>4.1 RQ1: ROBUSTNESS</h3>
<p>This RQ examines the stability of LLMs' responses, assessing the impact of three critical factors on model performance: (1) randomness introduced by the model's sampling strategy, (2) the temperature parameter setting, and (3) the prompt used for game instruction.</p>
<p>Multiple Runs Firstly, we run all games five times under the same settings. Fig. 4 illustrates the average performance across tests, while Table 4 lists the corresponding scores. The analysis reveals that, except for the two sequential games and the "Public Goods Game," the model demonstrates a consistent performance, as evidenced by the low variance in scores for each game.</p>
<p>Temperatures As discussed in our literature review in §B, prior research incorporates varying temperature parameters from 0 to 1 yet omits to explore their impacts. This study conducts experiments across games employing a range of temperatures ${0.0,0.2,0.4,0.6,0.8,1.0}$ under vanilla settings. The results, both visual and quantitative, are documented in Fig. 5 and Table 5, respectively. The small overall variance of 3.4 indicates that, for the majority of games, temperature adjustments yield negligible effects. A notable exception is observed in "Guess 2/5 of the Average," where increased temperatures correlate with enhanced scores ( 48.0 to 65.4 ), contrasting starkly with the near-random performance at zero temperature.</p>
<p>Prompt Templates We also investigate the impact of prompt phrasing on model performance. We leverage GPT-4 to rewrite our default prompt templates, generating four additional versions. We perform a manual checking process on the generated versions to ensure GPT-4's adherence to game rules without altering critical data. The prompt templates can be found in §D. We plot the results of using these templates in Fig. 6 and record the quantitative scores in Table 6. Notably, we find that prompt wording can significantly affect performance, as shown by the high variances in the "Public Goods Game" (11.5), "Diner's Dilemma" (23.7), and "Pirate Game" (14.7).</p>
<p>Answer to RQ1: GPT-3.5 exhibits consistency in multiple runs and shows robustness against different temperature settings. However, inappropriate prompt designs resulting from potential misinformation during rephrasing can significantly impair performance.</p>
<h3>4.2 RQ2: REASONING STRATEGIES</h3>
<p>This RQ focuses on improving the model's performance through prompt instructions. We investigate two strategies: Chain-of-Thought (CoT) prompting (Kojima et al., 2022) and persona assignment (Kong et al., 2024). We show the visualized and quantitative results in Fig. 7 and Table 7.</p>
<p>CoT According to Kojima et al. (2022), introducing a preliminary phrase, "Let's think step by step," encourages the model to sequentially analyze and explain its reasoning before presenting its conclusion. This approach has proven beneficial in specific scenarios, such as games (1), (3), (4),</p>
<p>and (5), improving the overall score from 45.9 to 57.9 , by 12.0 . In the "(3) Divide the Dollar" game, incorporating CoT reduces the model's propensity to suggest disproportionately large allocations, increasing the score by 15.3 . Similarly, in the "(4) Public Goods Game" and "(5) Diner's Dilemma," CoT prompts the model to recognize being a free-rider as the optimal strategy, increasing the scores by 14.9 and 78.5 , respectively.</p>
<p>Persona Studies (Kong et al., 2024; Huang et al., 2024b) have demonstrated that assigning roles to models influences performance across various downstream tasks. Inspired by this discovery, our study initiates with a prompt that specifies the model's role, such as "You are [ROLE]," where the role could be a cooperative and collaborative assistant, a selfish and greedy assistant, or a mathematician. Our findings reveal that assigning the "cooperative" role enhances model performance in games (1), (2), and (3), notably outperforming the CoT method in the "(3) El Farol Bar" game. Conversely, the "selfish" role markedly diminishes performance almost all the games, with the only exception of the "(7) Battle Royale" game. The "mathematician" role improves the model's overall score by 0.6 , which is small and does not surpass the CoT method's effectiveness.</p>
<p>Answer to RQ2: It is possible to improve GPT-3.5 through simple prompt instructions. Among the methods we explore, the CoT prompting performs the best, achieving a performance close to GPT-4 (57.9 vs. 62.4).</p>
<h1>4.3 RQ3: GENERALIZABILITY</h1>
<p>Considering the extensive exploration of games in domains such as mathematics, economics, and computer science, it is probable that the vanilla settings of these games are included within the training datasets of LLMs. To ascertain the presence of data contamination in our chosen games, we subjected them to various settings. The specifics of the parameters selected for each game are detailed in Table 8, and the experimental outcomes are visually represented in Fig. 8. Our findings indicate variability in model generalizability across different games. Specifically, in games (1), (3), (5), (6), and (8), the model demonstrated correct performance under diverse settings. In the "(3) Divide the Dollar" game, the model's performance improved with an increase in total golds $(G)$, suggesting that higher allocations of golds satisfy the demands of all players. Conversely, the model exhibited low generalizability in games (2) and (4). An analysis of the game "(2) El Farol Bar" reveals a consistent decision-making pattern by the model, opting to participate with approximately a $50 \%$ probability regardless of varying bar capacities $(R)$, indicating that the model is acting randomly. Similarly, in the "(4) Public Goods Game," the model consistently contributes similar amounts, even when the return rate is nil, indicating a lack of understanding of the game rules. A possible reason for this poor performance is the model's inability to adjust its performance incrementally based on historical data.</p>
<p>Nagel (1995) conducted experiments with 15 to 18 human subjects participating in the "(1) Guess $2 / 3$ of the Average" game, using ratios of $\frac{1}{2}, \frac{2}{3}$, and $\frac{4}{3}$. The average numbers were $27.05,36.73$, and 60.12 for each ratio, respectively. In a similar vein, Rubinstein (2007) explored the $\frac{2}{3}$ ratio on a larger population involving 2,423 subjects, yielding a comparable mean of 36.2 , aligning with the finding in Nagel (1995). The model produces average numbers of $34.59,34.59$, and 74.92 for the same ratios, indicating its predictions are more aligned with human behavior than the game's NE.</p>
<p>Answer to RQ3: GPT-3.5 demonstrates variable performance across different game settings, exhibiting notably lower efficacy in "(2) El Farol Bar" and "(4) Public Goods Game." It is noteworthy that, $\gamma$-Bench provides a test bed to evaluate the ability of LLMs in complex reasoning scenarios. As model's ability improves (e.g., achieving more than 90 on $\gamma$-Bench), we can increase the difficulty by varying game settings.</p>
<h3>4.4 RQ4: LEADERBOARD</h3>
<p>This RQ investigates the variance in decision-making capabilities among different LLMs, using $\gamma$-Bench. We first focus on closed-source models, including OpenAI's GPT-3.5 (0613, 1106, and 0125), GPT-4 (Turbo-0125, 4o-0806), and Google's Gemini Pro (1.0, 1.5). The results are organized in Table 1a, with model performance visualized in Fig. 9 in the appendix. Gemini-1.5-Pro scores 69.8, markedly surpassing other models, particularly in games (1), (4), and (5). GPT-4o follows</p>
<p>closely behind Gemini-Pro, achieving 66.75. GPT-4's lowered performance in the "(2) El Farol Bar" game (23.0) and the "(5) Diner's Dilemma" game (0.9) stems from its conservative strategies favoring staying at home and spending less money. Similarly, the "(6) Sealed-Bid Auction" (24.2) is attributed to a strategy of not risking bidding high or low. The risk-averse preference also explains the relatively good score on the "(4) Public Goods Game," where the GPT-4 does not take the risk to invest. Furthermore, an evaluation of three GPT-3.5 updates shows similar performance.</p>
<p>Next, we focus on open-source models, whose performance is detailed in Table 1b and visualized in Fig. 10. The top-two open-source model, LLaMA-3.1-70B and Mixtral-8x22B, closely follows Gemini-1.5-Pro with a score of 65.9 and 62.4 , surpassing GPT-4. Most open-source models, including Qwen-2, LLaMA-3.1-405B, and LLaMA-3.1-8B, outperform GPT-3.5 and Gemini-1.0-Pro. Mixtral-8x7B exhibits the lowest performance, likely due to its smaller size and weaker reasoning capabilities. Interestingly, LLaMA-3.1-405B underperforms compared to its smaller counterpart, the 70B version, which we attribute to its overly conservative strategy in the "(2) El Farol Bar" game, a challenge similar to the one faced by GPT-4.</p>
<p>Answer to RQ4: Currently, Gemini-1.5-Pro outperforms all other models evaluated in this study. LLaMA-3.1-70B performs closely, being in the second place.</p>
<h1>5 Related Work</h1>
<p>Evaluating LLMs through game theory models has become a popular research direction. An overview on recent studies is summarized in Table 3. We find: (1) Many studies examine the PSNE on two-player, single-round settings, focusing on the Prisoner's Dilemma and the Ultimatum Game. (2) Varying temperatures are employed without discussing the impact on LLMs' performance.</p>
<h3>5.1 SPECIFIC GAMES</h3>
<p>Researchers have explored diverse game scenarios. Using the complex and deceptive environments of Avalon game as a test bed, recent work focuses on long-horizon multi-party dialogues (Stepputtis et al., 2023), social behaviors (Lan et al., 2024), social intelligence (Liu et al., 2024), and recursive contemplation (Wang et al., 2023) for identifying deceptive information. Other papers have investigated communication games like Werewolf, with a focus on tuning-free frameworks (Xu et al., 2023) and reinforcement learning-powered approaches (Xu et al., 2024b). O'Gara (2023) found that advanced LLMs exhibit deception and lie detection capabilities in the text-based game, Hoodwinked. Meanwhile, Liang et al. (2023) evaluated LLMs' intelligence and strategic communication skills in the word guessing game, Who Is Spy? In the game of Water Allocation Challenge, Mao et al. (2025) constructed a scenario highlighting unequal competition for limited resources.</p>
<h3>5.2 GAME Benchmarks</h3>
<p>Another line of studies collects games to build more comprehensive benchmarks to assess the artificial general intelligence of LLMs. Tsai et al. (2023) found that while LLMs perform competitively in text games, they struggle with world modeling and goal inference. GameEval (Qiao et al., 2023) introduced three goal-driven conversational games (Ask-Guess, SpyFall, and TofuKingdom) to assess the problem-solving capabilities of LLMs in cooperative and adversarial settings. MAgIC (Xu et al., 2024a) proposed the probabilistic graphical modeling method for evaluating LLMs in multi-agent game settings. LLM-Co (Agashe et al., 2023) assesses LLMs in multi-agent coordination scenarios, showcasing their capabilities in partner intention inference and proactive assistance. SmartPlay (Wu et al., 2024) evaluated LLMs as agents across six games, emphasizing reasoning, planning, and learning capabilities. Abdelnabi et al. (2024) designed negotiation games involving six parties with distinct objectives to evaluate LLMs' ability to reach agreement.</p>
<h2>6 CONCLUSION</h2>
<p>This paper presents $\gamma$-Bench, a benchmark designed to assess LLMs' Gaming Ability in MultiAgent environments. $\gamma$-Bench incorporates eight classic game theory scenarios, emphasizing multiplayer interactions across multiple rounds and actions. Our findings reveal that GPT-3.5 (0125)</p>
<p>demonstrates a limited decision-making ability on $\gamma$-Bench, yet it can improve itself by learning from the historical results. Leveraging the carefully designed scoring scheme, we observe that GPT-3.5 (0125) exhibits commendable robustness across various temperatures and prompts. It is noteworthy that strategies such as CoT prove effective in this context. Nevertheless, its capability to generalize across various game settings remains restricted. Finally, Gemini-1.5-Pro outperforms all tested models, achieving the highest ranking on the $\gamma$-Bench leaderboard, with the open-source LLaMA-3.1-70B following closely behind.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>The paper is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14206921 of the General Research Fund).</p>
<h2>REFERENCES</h2>
<p>Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, and Mario Fritz. Cooperation, competition, and maliciousness: Llm-stakeholders interactive negotiation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.</p>
<p>Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903, 2023.</p>
<p>Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pp. 337-371. PMLR, 2023.</p>
<p>Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. arXiv preprint arXiv:2305.16867, 2023.</p>
<p>W Brian Arthur. Inductive reasoning and bounded rationality. The American economic review, 84 (2):406-411, 1994.</p>
<p>Daniel Ashlock and Garrison Greenwood. Generalized divide the dollar. In 2016 IEEE Congress on Evolutionary Computation (CEC), pp. 343-350. IEEE, 2016.</p>
<p>David Baidoo-Anu and Leticia Owusu Ansah. Education in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning. Journal of AI, 7(1):52-62, 2023.</p>
<p>Philip Brookins and Jason DeBacker. Playing games with gpt: What can we learn about a large language model from canonical strategic games? Economics Bulletin, 44(1):25-37, 2024.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Valerio Capraro, Roberto Di Paolo, and Veronica Pizziol. Assessing large language models' ability to predict how humans balance self-interest and the interest of others. arXiv preprint arXiv:2307.12776, 2023.</p>
<p>Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put your money where your mouth is: Evaluating strategic planning and execution of llm agents in an auction arena. In NeurIPS 2024 Workshop on Open-World Agents, 2024.</p>
<p>Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias StengelEskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench: Uncovering the strategic reasoning capabilities of llms via game-theoretic evaluations. Advances in Neural Information Processing Systems, 37:28219-28253, 2024.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational players in game theory? a systematic analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, number 16 in 38, pp. 17960-17967, 2024.</p>
<p>Natalie S Glance and Bernardo A Huberman. The dynamics of social dilemmas. Scientific American, 270(3):76-81, 1994.</p>
<p>Robert E Goodin. The theory of institutional design. Cambridge University Press, 1998.
Neel Guha, Julian Nyarko, Daniel E Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, et al. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. In Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.</p>
<p>Fulin Guo. Gpt agents in game theory experiments. arXiv preprint arXiv:2305.05516, 2023.
Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo. Suspicion agent: Playing imperfect information games with theory of mind aware gpt-4. In First Conference on Language Modeling, 2024.</p>
<p>Babak Heydari and Nunzio Lorè. Strategic behavior of large language models: Game structure vs. contextual framing. Contextual Framing (September 10, 2023), 2023.</p>
<p>John J Horton. Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research, 2023.</p>
<p>Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. Apathetic or empathetic? evaluating LLMs' emotional alignments with humans. In Advances in Neural Information Processing Systems 37, 2024a.</p>
<p>Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. On the humanity of conversational ai: Evaluating the psychological portrayal of llms. In Proceedings of the Twelfth International Conference on Learning Representations, 2024b.</p>
<p>Bernardo A. Huberman. The Ecology of Computation. North-Holland, 1988.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. Is chatgpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745, 2023.</p>
<p>Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone, Eli Zimmerman, Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir, et al. Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model. Research square, 2023.</p>
<p>D Marc Kilgour. Equilibrium points of infinite sequential truels. International Journal of Game Theory, 6:167-180, 1977.</p>
<p>D Marc Kilgour and Steven J Brams. The truel. Mathematics Magazine, 70(5):315-326, 1997.
D Mark Kilgour. The sequential truel. International Journal of Game Theory, 4:151-174, 1975.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems, $35: 22199-22213,2022$.</p>
<p>Daphne Koller and Avi Pfeffer. Representations and solutions for game-theoretic problems. Artificial intelligence, 94(1-2):167-215, 1997.</p>
<p>Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang Dong. Better zero-shot reasoning with role-play prompting. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 4099-4113, 2024.</p>
<p>Michal Kosinski. Evaluating large language models in theory of mind tasks. Proceedings of the National Academy of Sciences, 121(45):e2405460121, 2024.</p>
<p>Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, and Hao Wang. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 128-145, 2024.</p>
<p>Pier Luca Lanzi and Daniele Loiacono. Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1383-1390, 2023.</p>
<p>Alain Ledoux. Concours résultats complets. les victimes se sont plu à jouer le 14 d'atout. Jeux \&amp; Stratégie, 2(10):10-11, 1981.</p>
<p>Jiatong Li, Rui Li, and Qi Liu. Beyond static datasets: A deep interaction approach to llm evaluation. arXiv preprint arXiv:2309.04369, 2023.</p>
<p>Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Leveraging word guessing games to assess the intelligence of large language models. arXiv preprint arXiv:2310.20499, 2023.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multiagent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024.</p>
<p>Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, and Jieyu Zhao. Interintent: Investigating social intelligence of llms via intention understanding in an interactive game context. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024.</p>
<p>Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Qiang Guan, Tao Ge, and Furu Wei. Olympics: Llm agents meet game theory. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 2845-2866, 2025.</p>
<p>R Preston McAfee and John McMillan. Auctions and bidding. Journal of economic literature, 25 (2):699-738, 1987.</p>
<p>Roger B Myerson. Game theory. Harvard university press, 2013.
Rosemarie Nagel. Unraveling in guessing games: An experimental study. The American economic review, 85(5):1313-1326, 1995.</p>
<p>John F Nash. Equilibrium points in n-person games. Proceedings of the national academy of sciences, 36(1):48-49, 1950.</p>
<p>John F Nash. Non-cooperative games. Annals of Mathematics, 54(2):286-295, 1951.
Aidan O'Gara. Hoodwinked: Deception and cooperation in a text-based game for language models. arXiv preprint arXiv:2308.01404, 2023.</p>
<p>OpenAI. Introducing chatgpt. OpenAI Blog Nov 30 2022, 2022. URL https://openai.com/ index/chatgpt/.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Joseph Persky. Retrospectives: The ethology of homo economicus. Journal of Economic Perspectives, 9(2):221-231, 1995.</p>
<p>Steve Phelps and Yvan I Russell. Investigating emergent goal-like behaviour in large language models using experimental economics. arXiv preprint arXiv:2305.07970, 2023.</p>
<p>Sundar Pichai and Demis Hassabis. Introducing gemini: our largest and most capable ai model. Google Blog Dec 06 2023, 2023. URL https://blog.google/technology/ai/ google-gemini-ai/.</p>
<p>Sundar Pichai and Demis Hassabis. Our next-generation model: Gemini 1.5. Google Blog Feb 15 2024, 2024. URL https://blog.google/technology/ai/ google-gemini-next-generation-model-february-2024/.</p>
<p>Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, and Nan Duan. Gameeval: Evaluating llms on conversational games. arXiv preprint arXiv:2308.10032, 2023.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339-1384, 2023.</p>
<p>Mathias Risse. What is rational about nash equilibria? Synthese, 124:361-384, 2000.
Ariel Rubinstein. Instinctive and cognitive reasoning: A study of response times. The Economic Journal, 117(523):1243-1259, 2007.</p>
<p>Paul A Samuelson. The pure theory of public expenditure. The review of economics and statistics, 36(4):387-389, 1954.</p>
<p>Lloyd S Shapley and Martin Shubik. Pure competition, coalitional power, and fair division. International Economic Review, 10(3):337-362, 1969.</p>
<p>Simon Stepputtis, Joseph P Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, and Katia Sycara. Long-horizon dialogue understanding for role identification in the game of avalon with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 11193-11208, 2023.</p>
<p>Ian Stewart. A puzzle for pirates. Scientific American, 280(5):98-99, 1999.
Nigar M Shafiq Surameery and Mohammed Y Shakor. Use chat gpt to solve programming bugs. International Journal of Information Technology \&amp; Computer Engineering (IJITC) ISSN: 2455$5290,3(01): 17-22,2023$.</p>
<p>Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, and Hongyuan Mei. Can large language models play text games well? current state-of-the-art and open questions. arXiv preprint arXiv:2304.02868, 2023.</p>
<p>William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance, 16(1):8-37, 1961.</p>
<p>Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon's game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. arXiv preprint arXiv:2303.13648, 2023.</p>
<p>Yue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as intelligent agents. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, and Guohao Li. Can large language model agents simulate human trust behaviors? Advances in neural information processing systems, 37, 2024.</p>
<p>Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 7315-7332, 2024a.</p>
<p>Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.</p>
<p>Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. In Proceedings of the Forty-first International Conference on Machine Learning, 2024b.</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.</p>
<p>Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, and Furu Wei. K-level reasoning with large language models. arXiv preprint arXiv:2402.01521, 2024.</p>
<p>Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107, 2023.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: $\gamma$-Bench enables multiple LLMs and humans to engage in multi-round games. The framework comprises three categories of games, each targeting different LLM abilities, and includes eight classic games from Game Theory.</p>
<h1>CONTENTS</h1>
<p>A More Information on Game Theory ..... 18
A. 1 Formulation ..... 18
A. 2 Nash Equilibrium ..... 18
A. 3 Human Behaviors ..... 18
B Literature Review: Evaluating LLMs with Game Theory ..... 19
C Details about Prompts ..... 20
C. 1 Design Methodology ..... 20
C. 2 Cooperative Games ..... 20
C. 3 Betraying Games ..... 22
C. 4 Sequential Games ..... 24
D Examples of GPT-4-Rephrased Prompts ..... 26
E Rescale Method for Raw Scores ..... 29
F Detailed Results ..... 30
F. 1 Robustness: Multiple Runs ..... 30
F. 2 Robustness: Temperatures ..... 31
F. 3 Robustness: Prompt Templates ..... 32</p>
<p>F. 4 Generalizability ..... 34
F. 5 Leaderboard ..... 35
F. 6 Detailed Player Actions of GPT-3.5 (0125) ..... 36
G LLM vs. Specific Strategies ..... 36
H Jailbreak Influence ..... 37
I Limitations ..... 37
J Ethics Statement and Broader Impacts ..... 38
K LLaMA-3.1-70B ..... 38
K. 1 Robustness: Multiple Runs ..... 38
K. 2 Robustness: Temperatures ..... 38
K. 3 Robustness: Prompt Templates ..... 39
K. 4 Generalizability ..... 39
L Gemini-1.5-Pro ..... 40
L. 1 Robustness: Multiple Runs ..... 40
L. 2 Robustness: Temperatures ..... 40
L. 3 Robustness: Prompt Templates ..... 41
L. 4 Generalizability ..... 41
M GPT-4o ..... 42
M. 1 Robustness: Multiple Runs ..... 42
M. 2 Robustness: Temperatures ..... 42
M. 3 Robustness: Prompt Templates ..... 43
M. 4 Generalizability ..... 43</p>
<h1>A MORE INFORMATION ON GAME THEORY</h1>
<h2>A. 1 FORMULATION</h2>
<p>Game theory involves analyzing mathematical models of strategic interactions among rational agents (Myerson, 2013). A game can be modeled using these key elements:</p>
<ol>
<li>Players, denoted as $\mathcal{P}={1,2, \cdots, N}$ : A set of $N$ participants.</li>
<li>Actions, represented as $\mathcal{A}=\left{\mathcal{A}<em 1="1">{i}\right}: N$ sets of actions available to each player. For instance, $\mathcal{A}=\left{\mathcal{A}</em>}={C, D}, \mathcal{A<em N="N">{2}={D, F}, \cdots, \mathcal{A}</em>={C, F}\right}$</li>
<li>Utility functions, denoted as $\mathcal{U}=\left{\mathcal{U}<em j="1">{i}: \times</em>\right}$ : A set of $N$ functions that quantify each player's preferences over all possible outcomes.}^{N} \mathcal{A}_{j} \mapsto \mathbb{R</li>
<li>Information, represented as $\mathcal{I}=\left{\mathcal{I}_{i}\right}: N$ sets of information available to each player, including other players' action sets, utility functions, historical actions, and other beliefs.</li>
<li>Order, indicated by $\mathcal{O}=\mathcal{O}<em 2="2">{1}, \mathcal{O}</em>$ implies that all players take actions simultaneously.}, \cdots, \mathcal{O}_{k}$ : A sequence of $k$ sets specifying the $k$ steps to take actions. For example, $\mathcal{O}=\mathcal{P</li>
</ol>
<p>In this study, Multi-Player games are defined as those with $|\mathcal{P}|&gt;2$ since game theory models have at least two players. Similarly, Multi-Action games are those where $\forall_{i \in \mathcal{P}}\left|\mathcal{A}<em _in="\in" _mathcal_P="\mathcal{P" i_="i," j="j">{i}\right|&gt;2$. Meanwhile, Multi-Round games involve the same set of players repeatedly engaging in the game, with a record of all previous actions being maintained. Simultaneous games satisfy that $k=1$, whereas Sequential games have $k&gt;1$, indicating players make decisions in a specific order. Games of Perfect Information are characterized by the condition $\forall</em>} \mid i \neq j} \mathcal{I<em j="j">{i}=\mathcal{I}</em>$. Since every player can see their own action, the above condition indicates that all players are visible to the complete information set in the game. Conversely, games not meeting this criterion are classified as Imperfect Information games, where players have limited knowledge of others' actions.</p>
<h2>A. 2 NASH Equilibrium</h2>
<p>Studying game theory models often involves analyzing their Nash Equilibria (NE) (Nash, 1950). An NE is a specific set of strategies where no one has anything to gain by changing only one's own strategy. This implies that given one player's choice, the strategies of others are constrained to a specific set, which in turn limits the original player's choice to the initial one. When each player's strategy contains only one action, the equilibrium is identified as a Pure Strategy Nash Equilibrium (PSNE) (Nash, 1950). However, in certain games, such as rock-paper-scissors, an NE exists only when players employ a probabilistic approach to their actions. This type of equilibrium is known as a Mixed Strategy Nash Equilibrium (MSNE) (Nash, 1951), with PSNE being a subset of MSNE where probabilities are concentrated on a single action. According to Thm. A. 1 shown below, we can analyze the NE of each game and evaluate whether LLMs' choices align with the NE.</p>
<p>Theorem A. 1 (Nash's Existence Theorem) Every game with a finite number of players in which each player can choose from a finite number of actions has at least one mixed strategy Nash equilibrium, in which each player's action is determined by a probability distribution.</p>
<h2>A. 3 HUMAN BEHAVIORS</h2>
<p>The attainment of NE presupposes participants as Homo Economicus, who are consistently rational and narrowly self-interested, aiming at maximizing self goals (Persky, 1995). However, human decision-making often deviates from this ideal. Empirical studies reveal that human choices frequently diverge from what the NE predicts (Nagel, 1995). This deviation is attributed to the complex nature of human decision-making, which involves not only rational analysis but also personal values, preferences, beliefs, and emotions. By comparing human decision patterns documented in prior studies, together with the NE, we can ascertain whether LLMs exhibit tendencies more akin to homo economicus or actual human decision-makers, thus shedding light on their alignment with human-like or purely rational decision-making processes.</p>
<h1>B Literature Review: Evaluating LLMs with Game Theory</h1>
<p>Evaluating LLMs through game theory models has become a popular research direction. An overview on recent studies is summarized in Table 3. From our analysis, several key observations emerge: (1) The majority of these studies are concentrated on two-player settings. (2) There is a predominant focus on two-action games; notably, half of the studies examine the Prisoner's Dilemma and the Ultimatum Game (the Dictator Game is one of the variants of the Ultimatum Game). (3) A notable gap in the literature is the lack of the comparative studies between LLMs' decision-making across multiple rounds and the action probability distributions predicted by the MSNE. (4) The studies exhibit variability in the temperatures used, which precludes definitive conclusions regarding their impact on LLMs' performance.</p>
<p>Table 3: A Comparison of existing studies that evaluate LLMs using game theory models. T denotes the temperature employed in each experiment. MP refers to a multi-player setting, whereas MR indicates multi-round interactions. Role specifies whether a specific role is assigned to the LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">T</th>
<th style="text-align: center;">MP</th>
<th style="text-align: center;">MR</th>
<th style="text-align: center;">Role</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">Games</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Horton (2023)</td>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Dictator Game</td>
</tr>
<tr>
<td style="text-align: center;">Guo (2023)</td>
<td style="text-align: center;">gpt-4-1106-preview</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ultimatum Game, Prisoner's Dilemma</td>
</tr>
<tr>
<td style="text-align: center;">Phelps \&amp; Russell (2023)</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Prisoner's Dilemma</td>
</tr>
<tr>
<td style="text-align: center;">Akata et al. (2023)</td>
<td style="text-align: center;">text-davinci-003, <br> gpt-3.5-turbo, gpt-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Prisoner's Dilemma, Battle of the Sexes</td>
</tr>
<tr>
<td style="text-align: center;">Aher et al. (2023)</td>
<td style="text-align: center;">text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002, text-davinci-003, gpt-3.5-turbo, gpt-4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Ultimatum Game</td>
</tr>
<tr>
<td style="text-align: center;">Capraro et al. (2023)</td>
<td style="text-align: center;">ChatGPT-4, Bard, Bing Chat</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Dictator Game <br> (Three Variants)</td>
</tr>
<tr>
<td style="text-align: center;">Brookins \&amp; DeBacker (2024)</td>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Dictator Game, Prisoner's Dilemma</td>
</tr>
<tr>
<td style="text-align: center;">Li et al. (2023)</td>
<td style="text-align: center;">gpt-3.5-turbo-0613, <br> gpt-4-0613, claude-2.0, chat-bison-001</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Public Goods Game</td>
</tr>
<tr>
<td style="text-align: center;">Heydari \&amp; Lorè (2023)</td>
<td style="text-align: center;">gpt-3.5-turbo-16k, gpt-4, llama-2</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Prisoner's Dilemma, <br> Stag Hunt, Snowdrift, Prisoner's Delight</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2024)</td>
<td style="text-align: center;">gpt-3.5, gpt-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Leduc Hold'em</td>
</tr>
<tr>
<td style="text-align: center;">Chen et al. (2024)</td>
<td style="text-align: center;">gpt-3.5-turbo-0613, <br> gpt-4-0613, claude-instant-1.2, claude-2.0, chat-bison-001</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">English Auction</td>
</tr>
<tr>
<td style="text-align: center;">Xu et al. (2024a)</td>
<td style="text-align: center;">gpt-3.5-turbo, gpt-4, <br> llama-2-70b, claude-2.0, palm-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Cost Sharing, Prisoner's Dilemma, Public Goods Game</td>
</tr>
<tr>
<td style="text-align: center;">Fan et al. (2024)</td>
<td style="text-align: center;">text-davinci-003, <br> gpt-3.5-turbo, gpt-4</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Dictator Game, Rock-Paper-Scissors, Ring-Network Game</td>
</tr>
<tr>
<td style="text-align: center;">Zhang et al. (2024)</td>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Guess 0.8 of the Average Survival Auction Game</td>
</tr>
<tr>
<td style="text-align: center;">Duan et al. (2024)</td>
<td style="text-align: center;">gpt-3.5-turbo, gpt-4, <br> llama-2-70b, codellama-34b, mistral-7b-orca</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ten Games ${ }^{a}$</td>
</tr>
<tr>
<td style="text-align: center;">Xie et al. (2024)</td>
<td style="text-align: center;">text-davinci-003, <br> gpt-3.5-turbo-instruct, <br> gpt-3.5-turbo-0613, gpt-4, <br> llama-2-(7/13/70)b, <br> vicuna-(7/13/33)b-v1.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Seven Games ${ }^{\text {b }}$</td>
</tr>
<tr>
<td style="text-align: center;">This Study</td>
<td style="text-align: center;">gpt-3.5-turbo, gpt-4 <br> gemini-pro</td>
<td style="text-align: center;">$0 \sim 1$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Eight Games ${ }^{\text {c }}$</td>
</tr>
</tbody>
</table>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C Details about Prompts</h1>
<h2>C. 1 Design Methodology</h2>
<p>We adopt a cohesive approach to ensure the prompt design is systematic and not arbitrary. Game descriptions are gathered from verified sources, including academic papers referenced in $\S 2$ and Wikipedia entries. Using these descriptions, we instruct GPT-4 to generate prompts to guide LLMs in engaging in the specified games. These prompts are structured to encompass four essential elements: the rules of the game, objectives for the players, a template for announcing game outcomes (for displaying historical results), and instructions for formatting responses in JSON. A manual checking process is conducted to ascertain that GPT-4's comprehension of the game descriptions is correct. The prompts are detailed in the rest part of this section.</p>
<h2>C. 2 COOPERATIVE GAMES</h2>
<h2>Guess $2 / 3$ of the Average</h2>
<p>SYSTEM You are participating in a game played by $N$ players over $K$ rounds. Game Rules:</p>
<ol>
<li>Each player selects an integer number between $M I N$ and $M A X$, inclusive.</li>
<li>After all selections are made, the average of all chosen numbers is calculated.</li>
<li>The target number is $R$ of this average.</li>
<li>The winner is the player(s) who selected a number closest to the target number.
$\cdots$
USER Game Results for Round $I$ :
Average Number Chosen: $M_{I}$
Target Number ( $R$ of Average): $T_{I}$
Winning Number: $W_{I}$
You chose:
ASSISTANT { "chosen_number": " $C_{I J}$ " $}$
USER [Congratulation you won]/[Unfortunately you lost]. . .
USER Now round $I$ starts.
Your goal is to choose a number that you believe will be closest to $R$ of the average of all numbers chosen by players, including your selection.
Please provide your chosen number in the following JSON format:
{"chosen_number": "integer_between._MIN_and._MAX"}.</li>
</ol>
<h2>[BACK TO GAME DESCRIPTION]</h2>
<p>[BACK TO VANILLA EXPERIMENT]</p>
<h1>El Farol Bar</h1>
<p>SYSTEM You are participating in a game played by $N$ players over $K$ rounds. Game Rules:</p>
<ol>
<li>Every round, you and the other players decide independently whether to go to a bar.</li>
<li>If equal to or less than $R$ of the players go to the bar, everyone who goes has more fun than staying home, receiving a utility of $M A X$.</li>
<li>If more than $R$ of the players go to the bar, everyone who goes has less fun than staying home, receiving a utility of $M I N$.</li>
<li>Everyone who stays home receives a utility of $H O M E$.
...
USER Game Results for Round $I$ :
(Only for Explicit) $G_{I}$ players went to the bar, while $S_{I}$ players stayed home. $G_{I} / N$, which is [more]/[equal to or less] than $R$ of the players went to the bar. It was [less]/[more] fun to go to the bar this round.
You chose:
ASSISTANT $\left{\right.$ "decision": " $D_{I J}$ " $}$
USER You gained $G_{I J}$.
...
USER Now round $I$ starts.
Your goal is to maximize your fun. Choose to go to the bar when you predict fewer than $R$ of the players will go, and choose to stay home otherwise.
Please provide your decision in the following JSON format:
{"decision": "go_or_stay"}.</li>
</ol>
<h2>Divide the Dollar</h2>
<p>SYSTEM You are participating in a game played by $N$ players over $K$ rounds. Game Rules:</p>
<ol>
<li>You are dividing $G$ golds. Each player independently proposes a bid.</li>
<li>If the sum of all bids does not exceed $G$, each player receives their bid amount.</li>
<li>If the sum exceeds $G$, all players receive nothing.
...
USER Game Results for Round $I$ :
Your bid amount was:
ASSISTANT {"bid_amount": " $B_{I J}$ "}
USER The sum of all bids was $S_{I}$.
The sum [does not exceed]/[exceeds] $G$.
You received $\left[B_{I J}\right] /[0]$ golds.
...
USER Now round $I$ starts.
Your goal is to maximize your individual gain without causing the total sum of bids to exceed $G$ golds.
Please provide your bid amount in the following JSON format:
{"bid_amount": "integer_between_0_and_G"}.</li>
</ol>
<h2>[BACK TO GAME DESCRIPTION]</h2>
<p>[BACK TO VANILLA EXPERIMENT]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{a}$ Tie-Tac-Toe, Connect-4, Kuhn Poker, Breakthrough, Liar's Dice, Blind Auction, Negotiation, Nim, Pig, Iterated Prisoner's Dilemma,
${ }^{\text {b }}$ Trust Game, Minimum Acceptable Probabilities Trust Game, Repeated Trust Game, Dictator Game, Risky Dictator Game, Lottery People Game, Lottery Gamble Game.
${ }^{\text {c }}$ Guess 2/3 of the Average, El Farol Bar, Divide the Dollar, Public Goods Game, Diner's Dilemma, Sealed-Bid Auction, Battle Royale, Pirate Game.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8994 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8994</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8994</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-277780144</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.08781v1.pdf" target="_blank">Efficient Evaluation of Large Language Models via Collaborative Filtering</a></p>
                <p><strong>Paper Abstract:</strong> With the development of Large Language Models (LLMs), numerous benchmarks have been proposed to measure and compare the capabilities of different LLMs. However, evaluating LLMs is costly due to the large number of test instances and their slow inference speed. In this paper, we aim to explore how to efficiently estimate a model's real performance on a given benchmark based on its evaluation results on a small number of instances sampled from the benchmark. Inspired by Collaborative Filtering (CF) in Recommendation Systems (RS), we treat LLMs as users and test instances as items and propose a two-stage method. In the first stage, we treat instance selection as recommending products to users to choose instances that can easily distinguish model performance. In the second stage, we see performance prediction as rating prediction problem in RS to predict the target LLM's behavior on unselected instances. Experiments on multiple LLMs and datasets imply that our method can accurately estimate the target model's performance while largely reducing its inference overhead.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8994.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8994.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TinyBenchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TinyBenchmarks: evaluating LLMs with fewer examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (Polo et al., 2024) that leverages psychometric / educational-assessment models to evaluate large language models using a small subset of test instances, aiming to preserve measurement fidelity while reducing evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>tinybenchmarks: evaluating llms with fewer examples.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Psychometrics-based TinyBenchmark procedure</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A reduced-example evaluation approach that applies models from educational measurement / psychometrics (e.g., item-response or adaptive-testing inspired methods) to select and weight a small set of test items to estimate LLM performance; intended to preserve accuracy of estimates while using far fewer instances.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Mentioned in Related Work: the current paper states TinyBenchmarks leverages models of educational assessments from psychometrics to accurately assess LLM capabilities with a fraction of standard benchmark instances. The current paper treats TinyBenchmark as a baseline in experiments (compares MAE and runtime against it) but does not reproduce TinyBenchmark's internal numeric results or human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>This paper only cites TinyBenchmarks in related work and uses it as a baseline; no cognitive-psychology-specific tests, per-item human baselines, or numeric outcomes from TinyBenchmarks are reported here. No details on psychometric model parameters, subject populations, or direct LLM-vs-human comparisons are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Evaluation of Large Language Models via Collaborative Filtering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8994.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8994.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computerized Adaptive Testing (CAT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computerized adaptive testing: Theory and practice</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundational psychometrics reference (van der Linden & Glas, 2000) on adaptive testing algorithms and item selection that can inform reduced-sample evaluation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Computerized adaptive testing: Theory and practice.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Computerized adaptive testing (general psychometric approach)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A family of item-response-based adaptive testing procedures from psychometrics that select items dynamically based on a test-taker's estimated ability to achieve efficient and informative measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Referenced in the Introduction/Related Work as background motivating reduced-sample and adaptive approaches for evaluating models (cited as inspiration for efficiency in test design). The paper does not apply CAT to any specific cognitive psychology test nor report LLM or human scores under CAT in its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The reference is cited as conceptual background; the present paper does not implement a full CAT pipeline or report any LLM-vs-human CAT results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Evaluation of Large Language Models via Collaborative Filtering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>tinybenchmarks: evaluating llms with fewer examples. <em>(Rating: 2)</em></li>
                <li>Computerized adaptive testing: Theory and practice. <em>(Rating: 1)</em></li>
                <li>tinybenchmarks: evaluating llms with fewer examples <em>(Rating: 1)</em></li>
                <li>Chatgpt as a math questioner? evaluating chatgpt on generating pre-university math questions. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8994",
    "paper_id": "paper-277780144",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "TinyBenchmarks",
            "name_full": "TinyBenchmarks: evaluating LLMs with fewer examples",
            "brief_description": "A method (Polo et al., 2024) that leverages psychometric / educational-assessment models to evaluate large language models using a small subset of test instances, aiming to preserve measurement fidelity while reducing evaluation cost.",
            "citation_title": "tinybenchmarks: evaluating llms with fewer examples.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Psychometrics-based TinyBenchmark procedure",
            "test_description": "A reduced-example evaluation approach that applies models from educational measurement / psychometrics (e.g., item-response or adaptive-testing inspired methods) to select and weight a small set of test items to estimate LLM performance; intended to preserve accuracy of estimates while using far fewer instances.",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "experimental_details": "Mentioned in Related Work: the current paper states TinyBenchmarks leverages models of educational assessments from psychometrics to accurately assess LLM capabilities with a fraction of standard benchmark instances. The current paper treats TinyBenchmark as a baseline in experiments (compares MAE and runtime against it) but does not reproduce TinyBenchmark's internal numeric results or human baselines.",
            "limitations_or_caveats": "This paper only cites TinyBenchmarks in related work and uses it as a baseline; no cognitive-psychology-specific tests, per-item human baselines, or numeric outcomes from TinyBenchmarks are reported here. No details on psychometric model parameters, subject populations, or direct LLM-vs-human comparisons are provided in this paper.",
            "uuid": "e8994.0",
            "source_info": {
                "paper_title": "Efficient Evaluation of Large Language Models via Collaborative Filtering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Computerized Adaptive Testing (CAT)",
            "name_full": "Computerized adaptive testing: Theory and practice",
            "brief_description": "A foundational psychometrics reference (van der Linden & Glas, 2000) on adaptive testing algorithms and item selection that can inform reduced-sample evaluation strategies.",
            "citation_title": "Computerized adaptive testing: Theory and practice.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Computerized adaptive testing (general psychometric approach)",
            "test_description": "A family of item-response-based adaptive testing procedures from psychometrics that select items dynamically based on a test-taker's estimated ability to achieve efficient and informative measurement.",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "experimental_details": "Referenced in the Introduction/Related Work as background motivating reduced-sample and adaptive approaches for evaluating models (cited as inspiration for efficiency in test design). The paper does not apply CAT to any specific cognitive psychology test nor report LLM or human scores under CAT in its experiments.",
            "limitations_or_caveats": "The reference is cited as conceptual background; the present paper does not implement a full CAT pipeline or report any LLM-vs-human CAT results.",
            "uuid": "e8994.1",
            "source_info": {
                "paper_title": "Efficient Evaluation of Large Language Models via Collaborative Filtering",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "tinybenchmarks: evaluating llms with fewer examples.",
            "rating": 2,
            "sanitized_title": "tinybenchmarks_evaluating_llms_with_fewer_examples"
        },
        {
            "paper_title": "Computerized adaptive testing: Theory and practice.",
            "rating": 1,
            "sanitized_title": "computerized_adaptive_testing_theory_and_practice"
        },
        {
            "paper_title": "tinybenchmarks: evaluating llms with fewer examples",
            "rating": 1,
            "sanitized_title": "tinybenchmarks_evaluating_llms_with_fewer_examples"
        },
        {
            "paper_title": "Chatgpt as a math questioner? evaluating chatgpt on generating pre-university math questions.",
            "rating": 1,
            "sanitized_title": "chatgpt_as_a_math_questioner_evaluating_chatgpt_on_generating_preuniversity_math_questions"
        }
    ],
    "cost": 0.0083195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Efficient Evaluation of Large Language Models via Collaborative Filtering
5 Apr 2025</p>
<p>Xu-Xiang Zhong 
School of Artificial Intelligence
Nan-jing University</p>
<p>National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Chao Yi 
School of Artificial Intelligence
Nan-jing University</p>
<p>National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Han-Jia Ye 
School of Artificial Intelligence
Nan-jing University</p>
<p>National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>Han -Jia 
Efficient Evaluation of Large Language Models via Collaborative Filtering
5 Apr 2025966A007121492BA514035B55775C9217arXiv:2504.08781v1[cs.CL]
With the development of Large Language Models (LLMs), numerous benchmarks have been proposed to measure and compare the capabilities of different LLMs.However, evaluating LLMs is costly due to the large number of test instances and their slow inference speed.In this paper, we aim to explore how to efficiently estimate a model's real performance on a given benchmark based on its evaluation results on a small number of instances sampled from the benchmark.Inspired by Collaborative Filtering (CF) in Recommendation Systems (RS), we treat LLMs as users and test instances as items and propose a twostage method.In the first stage, we treat instance selection as recommending products to users to choose instances that can easily distinguish model performance.In the second stage, we see performance prediction as rating prediction problem in RS to predict the target LLM's behavior on unselected instances.Experiments on multiple LLMs and datasets imply that our method can accurately estimate the target model's performance while largely reducing its inference overhead.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have garnered widespread attention, with numerous LLMs (Bai et al., 2023;Touvron et al., 2023a;b;Zeng et al., 2023) being released and rapidly iterated.Due to their powerful general capabilities, these LLMs are expected to perform a diverse and broad range of tasks (Zhou et al., 2023;Pham et al., 2024;Gao et al., 2023;Qin et al., 2023).To fairly assess and compare different LLMs' capabilities, many benchmarks for evaluating LLMs have emerged and developed continuously (Liu et al., 2024;Hendrycks et al., 2021;Li et al., 2024;Zhong et al., 2024;Liang et al., 2023).To comprehensively and accurately On the left, the red dashed line represents the real performance of a new model, and the gray area indicates the gap between the estimated performance of our method and the real performance, which is smaller.On the right is the problem setup, where the goal is to extract a subset from each task, use the new model's evaluation on it to predict performance on each task and minimize the gap between estimated and real performance.</p>
<p>assess the diverse capabilities of LLMs, LLM benchmarks typically contain a variety of tasks (or scenarios), with each task corresponding to a sub-dataset.Furthermore, even within a specific task, such as code generation (Lu et al., 2021), there may be multiple programming languages, and each language has its own sub-dataset.So, a well-designed benchmark often contains many test instances.</p>
<p>Given the large number of test instances and the relatively slow inference speed of LLMs, it is impractical to let LLMs infer on all instances to obtain their performance.For example, when selecting the best model for a target task from an LLM Zoo, evaluating all models on a benchmark would result in substantial time and computational overhead.Furthermore, inference with certain closed-source large models incurs fees, making it costly to run these models on a large volume of test samples.In summary, performing inference on all instances of the target benchmark for each LLM in a large model zoo is not viable in resource-constrained scenarios.We hope to accurately evaluate the capabilities of an LLM in one specific task at a low cost.Some previous researches (Bommasani et al., 2021;Prabhu et al., 2024;Polo et al., 2024) have proposed their solutions to this problem.They only need to conduct inference on a small number of test instances to estimate the target models' performance.However, most of the previous researches focus only on the overall performance and ranking of LLMs across the entire benchmark, without considering the taskspecific performance and ranking of LLMs on individual tasks within the benchmark.Considering only overall performance cannot comprehensively and fairly evaluate the capabilities of large language models (LLMs), as different LLMs with similar overall performance and rankings may exhibit significant performance differences between tasks.And in practical applications, we often focus more on an LLM's task-level capabilities rather than overall capabilities.Therefore, a low-cost method is urgently needed to evaluate the capabilities of large models in various tasks.</p>
<p>To meet the above need, we propose a two-stage method inspired by Collaborative Filtering (CF) in recommendation systems (Goldberg et al., 1992;Schafer et al., 2007;Su and Khoshgoftaar, 2009).We apply collaborative filtering in both stages, treating LLMs as users and instances as items.</p>
<p>Processes in two stages focus on determining the value of items for a new user.They utilize the interaction history of other users with items and the new user's interaction history with some items.And Figure 1 is a conceptual diagram that integrates different tasks.We can observe that our method is closer to real performance compared to other methods.</p>
<p>In the first stage, we treat instance selection as recommending products to users.And we use CF to select a given number of instances, such as 10% for each scenario according to the importance score to get the real evaluation results of the target model.In the second stage, we view performance prediction as rating prediction problem in RS to predict the target LLM's behavior on unselected instances.Specifically, we predict performance using CF, based on the results of similar LLMs on remaing 90% unselected instances and the results of the target LLM on selected instances and synthesized instances by optimal transport (Peyré and Cuturi, 2019).Our contributions are as follows:</p>
<p>• We propose an efficient evaluation method based on the idea of collaborative filtering, which can efficiently give the performance of LLMs on different tasks.• We analyze the similarities and differences between efficient evaluation methods and Recommendation Systems (RS), which inspire us to apply the methods of RS to address the efficient evaluating problem.• Experiments on benchmarks composed of various LLMs and tasks demonstrate the effectiveness of our method.</p>
<p>Related Work</p>
<p>Efficient Benchmarking of PFMs.With the development of Pre-trained Foundation Models (PFMs), multiple benchmarks are introduced to quantify PFM's abilities and compare different PFMs.However, the continuous growth in the size of models and datasets has increased evaluation costs.Some researchers focus on designing efficient benchmarks to accommodate the costs.Perlitz et al. (2024) find that while diversity across datasets is crucial for evaluation in HELM (Liang et al., 2023), the quantity of examples presently utilized is unnecessarily large.They also design a coarse-to-fine tournament algorithm to get LLM's ranking.Vivek et al. (2024) suggest grouping evaluation samples according to LLM's confidence in the correct class to accelerate evaluation processes for classification tasks.TinyBenchmarks (Polo et al., 2024) leverages models of educational assessments from psychometrics to accurately assess the capabilities of LLMs with a fraction of the test instances in standard benchmark datasets.Lifelong Benchmarks (Prabhu et al., 2024)</p>
<p>Preliminary</p>
<p>Here we will introduce the process of original LLM evaluation and efficient evaluation methods, as shown in Figure 2. We will also present the evaluation metrics for comparing these methods and insights from a simple baseline.</p>
<p>Evaluation for LLMs</p>
<p>Assume there is an LLM Benchmark, whose dataset is
T = {T 1 , • • • , T N } consisting of N different tasks' datasets.
Here T i = {(x j i , y j i )} |Ti| j=1 refers to the i-th task's dataset.x j i and y j i represent the instance text and label text of the j-th instance in T i .We input the instance text x j i to the LLM f and perform post-processing on the answer returned by the LLM to obtain the final answer ŷj i :
ŷj i = PO(f (x j i )).(1)
where PO represents POSTPROCESS.After obtaining the final answer ŷj i , we can leverage evaluation metric to acquire the real performance p i of LLM f on the task T i :
p i = 1 |T i | |Ti| j=1 ME(ŷ j i , y j i ).(2)
where ME represents a METRIC, such as Exact Match Accuracy, ROUGE (Lin, 2004), and so on.By testing f on all the tasks in the LLM Benchmark T , we can obtain an assessment of the LLM f 's capabilities on various tasks
p = [p 1 , • • • , p N ].
After evaluating all the LLMs in the model zoo, we can also obtain the ranking r = [r 1 , • • • , r N ] of the LLM f .p and r respectively reflect the LLM's absolute and relative performance.We need k = N i=1 |T i | forward inferences to evaluate the LLM f on LLM Benchmark T .With each inference taking an average of t seconds, the total evaluation time is k × t.Given the numerous test instances in LLM Benchmark and the relatively slow inference speed of LLMs, both k and t tend to be high, significantly increasing the time and computational resources required for evaluation.Our work is aimed to decrease k.</p>
<p>Efficient Evaluation Method for LLMs</p>
<p>In practical scenarios, different LLMs can be organized into an ordered list [f 1 , f 2 , f 3 , • • • ] based on their release dates.We assume that we can get the evaluation results
D ∈ R B×|T | cosisting of ME(ŷ j i , y j i ) for the earliest re- leased B LLMs [f 1 , • • • , f B ]
on all instances of the LLM Benchmark.We refer to this collection of B LLMs as the initial model set.Without loss of generality, we assume that larger values in D correspond to higher quality answers from the LLM on the respective instances, indicating better performance, conversely, smaller values reflect lower answer quality.We now focus on evaluating a new LLM on the Benchmark, mainly on its absolute performance and its ranking relative to the initial LLMs for each task.The previous method tests the LLM on all instances in the Benchmark to obtain the ground truth values of model performance p i and rankings r i , as shown in Equation 2. While efficient evaluation method aims to obtain precise estimates of p i and r i by utilizing only a subset of instances on the Benchmark.</p>
<p>A well-designed efficient evaluation method consists of two core components: the instance selection function g and the performance prediction function h.The instance sampling function g leverages the evaluation results of initial LLMs D i ∈ R B×|Ti| on all instances for each task to select the important test instances, which forms a subset T s i of T i :
T s i = g(D i , T i ), |T s i | &lt; |T i |, T s i ⫋ T i .(3)
After getting the subset benchmark T s = {T s 1 , • • • , T s N }, the performance prediction function h uses these subsets and the evaluation result of initial LLMs on the i-th task D i ∈ R B×|Ti| to get estimate performance pi of the real performance p i of the new model for each task.
pi = h(D i , T s i ).(4)
After getting pi , we can compare it with the performance of historical LLMs to determine its ranking ri on each task.</p>
<p>Different efficient evaluation methods have different implementations of g and h.To compare these methods, we use the Mean Absolute Error (MAE) between the LLM's predicted performances p = [p 1 , • • • , pN ] and real performances p as shown in Equation 5.The MAE between predicted rankings r = [r 1 , • • • , rN ] and real rankings r across the tasks can be computed in a similar way.
MAE(p, p) = 1 N N i=1 |p i − pi |.(5)
When comparing the MAE of different methods, we ensure that the numbers of selected instance subsets |T s | for different methods are consistent.A smaller MAE indicates better performance of the corresponding efficient evaluation method.Note that we do not use Kendall's τ or weighted Kendall's τ coefficient because the ranking error of a new model will not cause a significant change in the above metrics, making it less effective for further comparisons.</p>
<p>When designing an efficient evaluation method, the following criteria should be met: 1) High Efficiency.The method should give the predicted result accurately and quickly to satisfy the low-cost requirements mentioned in Section 1.</p>
<p>2) Low Overhead.The method should be easily deployed without additional overhead so that can be used quickly for new scenarios.3) Commonness.We need to quickly evaluate the overall capability of the model on a task with a limited number of samples, which means reducing the MAE of performances in Equation 5. So, the method needs to reasonably handle instances with redundant information and keep only a small number of them.4) Personalization.</p>
<p>A good benchmark (OpenCompass, 2023;HuggingFace, 2024) should not only offer the absolute performance of models, but also provide their relative differences, such as their rankings r.Hence, efficient evaluation methods should consider the differences of each model to give an accurate ranking between models.5) Complementation.Given that the capabilities evaluated across different tasks may sometimes be similar, a good method should use information from other tasks to improve their performance or speed.</p>
<p>Insights from A Simple Baseline</p>
<p>In this subsection, we offer a simple implementation of an efficient evaluation method as a baseline.We also do a toy experiment and find that semantic information is not helpful.</p>
<p>In this baseline, we implement the instance selection function g using the K-means clustering algorithm on instance embeddings.Specifically, we perform the clustering algorithm on each task and select instances closest to the cluster center as selected instances.For performance prediction function h, we use the following equation to get the predicted peformances p:
pi = K j=1 |C ij |f (x ij ) K j=1 |C ij | (6)
where |C ij | is instance number of the j-the cluster in task i and x ij is the instance colsest to the center of c ij .This method assumes that the evaluation results of the instances of the same cluster are consistent with the center points and the weighted average of the center points can be used as an estimate of the real performance.</p>
<p>We implement two most common instance embedding methods for clustering, the first is semantic embedding given by Sentence-Bert (Reimers and Gurevych, 2019) and the other is the historical evaluation results of initial LLMs, which is the same as D i in Section 3.2.We do a toy experiment on a subset of Opencompass Benchmark (OpenCompass, 2023) to compare two instance embedding methods and the result is in Table 1, we can see that the efficient evaluation method with LLMs' historical evaluation results is better than that with semantic embedding.We guess the reason is that there is a large gap between the embedding space and the evaluation result space.Using historical evaluation results can better reflect the difficulty of samples than semantic information, so it is easier to predict the results of unselected instances.Experimental details and testing of hypotheses can be viewed in Appendix A.</p>
<p>In addition to the poor performance of semantic embedding, there are other difficulties in using semantic information: 1) High Consumption of Semantic Embedding Models.At least one embedding model infers on all the instances causing high consumption.2) Large Space to Store Semantic Embeddings.A lot of storage space is needed to store semantic embeddings compared to historical model performance.</p>
<p>3) Privacy Requirements for Benchmark.Some benchmark providers only provide a subset of the benchmark dataset.It is unfair and unsafe for model developers to provide their models to benchmark providers.In summary, we do not use semantic information in our method but only the evaluation results of historical models as in the previous methods.</p>
<p>Method</p>
<p>This section presents a two-stage method based on Collaborative Filtering (CF) and highlights the advantages of our method over previous approaches.By treating LLMs as users and test instances as items, we construct user and item features from user-item interactions.These features help calculate similarities between new and previous LLMs, and between selected and unselected instances.These similarities are then used for instance selection and performance prediction.In addition, we use historical information of similar scenes to obtain extra information to improve the performance of our method, which can be seen as metainformation in cold start problems.</p>
<p>Stage 1: Select Test Instances via CF</p>
<p>As described in Section 3.2, the instance selection function g aims to select important instances for each task.We first define the importance score, and then design an iterative sampling process for each task as shown in Figure 3.</p>
<p>DEFINITION OF IMPORTANT SCORE</p>
<p>We first define the importance score of a test instance for a model set.We think instances that can easily distinguish model performance are important.This is consistent with research in educational measurement (der Linden and Glas, 2000), which shows that overly difficult or easy instances fail to differentiate between test takers of varying abilities, as all test takers may answer correctly or incorrectly.Intuitively, the most important test instances are those where half of the LLMs provide high-quality answers and the other half provide low-quality answers.Such a design ensures that any LLM can be distinctly differentiated from half of the other LLMs in the model set on the current instance.</p>
<p>We define the importance score v(x|F ) for instance x given model set
F = {f 1 , • • • , f M } as: v(x|F ) = 1 M − 1 M m=1 (ME(ŷ (fm) , y) − ME(ŷ, y)) 2 . (7)
where ŷ(fm) = PO(f m (x)), y and ME indicate the m-th LLM's answer, ground truth answer and average performance for instance x, respectively.The metric represents the variability in the LLM's responses for a single instance.The larger this value is, the higher the importance score of the instance is.</p>
<p>PROCESS FOR INSTANCE SAMPLING</p>
<p>To choose instances that really matter, the selected instances should meet two conditions: (1) The instances should be able to assess the capabilities of all LLMs in the model set, meaning that the instances are required to have a high importance score v(x|F B ∪ {f t }) for the model set F B ∪ {f t } composed of the B initial LLMs F B and the target LLM f t ; (2) The instances should primarily differentiate the target LLM from its similar LLMs in F B , which requires the instances to have a high importance score v(x|F S ∪ {f t }) for the model set F S ∪ {f t } composed of the target LLM f t and it's similar LLMs F S .</p>
<p>To meet the above requirements, we design an iterative process.As Figure 3 shows, given target LLM f t , the ith task T i , initial models F B = {f 1 , • • • , f B }, evaluation results of initial models D i ∈ R B×|Ti| , the process can select important instances with three steps.</p>
<p>In the first step, we get a Probe Set P by calculating the important score for each instance in T i and selecting instances with higher important scores.For example, important score of the j-th instance in T i can be calculated as follows:
v(x j i |F B ) = 1 B − 1 B k=1 (D kj i − D •j i ) 2 . (8)
where D kj i refers to the element in the k-th row and j-th column of D i , which corresponds to the evaluation result of the k-th LLM on the j-th instance.And D •j i represents the average of the j-th column of D i .This is to find instances that can easily distinguish performance between models.</p>
<p>In the second step, we let target model f t infer on the Probe Set P to get the evaluation result vector d P t ∈ R |P | .Subsequently, we extract the evaluation results D i,P ∈ R B×|P | of the initial LLMs F B on the instances in P from D i .We treat d p t and each row of D i,P as the features of the f t and F B , respectively.By calculating the cosine similarity between each row of D i,P and d p t , and selecting the top n LLMs most similar to the target LLM f t , we can form a set of LLMs F S with capabilities similar to the current LLM.We use S = {k|f k ∈ F S } to represent the index set of similar LLMs.We calculate the importance score of the instance x j i on the similar model set F S :
v(x j i |F S ) = 1 |S| − 1 k∈S (D kj i − D •j i ) 2 . (9)
This is to select instances that should primarily differentiate the target LLM from its similar LLMs in F B .</p>
<p>In the third step, we perform a weighted sum of v(x j i |F B ) and v(x j i |F S ) like Equation 10to obtain the instance's final importance score v(x j i ) thus meeting two conditions talked above.Then we select new q instances with higher v(x j i ) to expand the Probe Set P .</p>
<p>v(x
j i ) = α × v(x j i |F B ) + (1 − α) × v(x j i |F S ). (10)
Finally, we can repeat step 2 and 3 until |P | meets the expected subset size |T s i |.In Equation 10, the first term remains constant across different target LLMs, while the second term varies because the sets of similar LLMs F S differ among different target LLMs.Hence, it achieves personalized instance selection.</p>
<p>RELATIONSHIP WITH RECOMMEND SYSTEMS</p>
<p>The process is similar to user-based collaborative filtering, where LLMs and instances act as users and items in a recommendation system, respectively.The evaluation matrix D B of LLMs on instances is similar to the user-item interactions matrix.The difference is that the values in our matrix reflect the quality of LLM responses, not user preferences.Our goal is to recommend the most important instances (items) to a new LLM (user).Our method starts by testing the target LLM on a Probe Set, like recommending popular items to a new user in cold start problems.Our instances sampling process resembles the user-based collaborative filtering while also considering the popularity of products to recommend items to new users.</p>
<p>Stage 2: Predict LLM's Performance</p>
<p>After getting the subset benchmark T s = {T s 1 , • • • , T s N }, we need to design the performance prediction function h to predict the performance p i and ranking r i of the new LLM.</p>
<p>PURPOSE OF PERFORMANCE PREDICTION</p>
<p>For performance prediction, a direct method is to use the new LLM's performance p s i on the sampled subset T s i .However, since our instance selecting method excludes those that are either too difficult or too easy, p s i may not be an exact estimate of p i .To obtain an accurate estimate of p i , we reexamine each component of its calculation formula (Equation 11).We use T s i and T ns i to represent the instances set we have selected and not selected, respectively.Similarly, a s i and a ns i denote the sum of the real performances of the target LLM f t for instances on sets T s i and T ns i .
p i = a s i + a ns i |T s i | + |T ns i | . (11)
Since we can obtain the true value of |T s i |, |T ns i | and a s i , we only need to predict the value of a ns i to get p i .</p>
<p>PROCESS FOR PERFORMANCE PREDICTION</p>
<p>However, |T s i | is usually small making predicting the value of a ns i difficult.Fortunately, this is similar to the cold start problem in recommendation systems, and we decide to draw inspiration from the idea of using meta-information to assist recommendations.However, in light of the issues mentioned in Section 3.3, we choose not to rely on semantic information.Instead, we leverage the performance of sampled instances in similar tasks combined with Optimal Transport (OT) to derive meta-information, in other words, synthetic data, thereby enhancing evaluation performance.</p>
<p>For the first step in Figure 4, for the i-th task, supposing the evaluation results of initial LLMs D i ∈ R B×|Ti| and the LLM f t on sampled dataset d t =
[ME(ŷ 1 i , y 1 i ), • • • , ME(ŷ |T s i | i , y |T s i | i )] ∈ R 1×|T s i | .
We can get the mean performance vector
v i = |T i | j=1 D •j i |Ti|
of initial LLMs for each task and calculate cosine similarity matrix C s ∈ R N ×N .With the help of C s and threshold value τ 0 , similar tasks can be found and we can combine the selected data from similar tasks to get T st i and D P .Because the real value of f t on T s i , the value of f t on the synthetic dataset can be calculated in the same way.Now come to the second step in Figure 4.For the sake of simplicity, we treat the synthetic data just like sampled instance, that is to say, T i := T s i ∪T ns i ∪T sy i and T s i := T s i ∪ T sy i .We denote the matrices composed of feature vectors of selected and unselected instances as
(st) i ∈ R B×|T st i | . WeD (s) i ∈ R B×|T s i | and D (ns) i ∈ R B×|T ns i | .
We then calculate the average value of importance scores v use Equation 7 .We set a threshold τ 1 .For instances whose importance score is smaller than v τ1 , we use the average performance of init LLMs as predicted
✓ ✓ ✓ Low Overhead ✓ ✓ ✓ ✗ Commonness ✓ ✓ ✓ ✓ Personalization ✓ ✗ ✓ ✗ Complementation ✓ ✗ ✗ ✓
performance ĉns i0 and remove them from D (ns) i</p>
<p>:
ĉns i0 = 1 B B k=1   j∈S D (ns)kj i   . T ns i := T ns i − S D (ns) i := D (ns) i − S ∈ R B×(|T (ns) i |−|S|) (13)
where S = {k|v
(ns)k i &lt; v τ1 }.
As discussed in Section 4.1.1,We do this because we believe that too difficult or easy instances fail to differentiate models.</p>
<p>After that, we can calculate the cosine similarity matrix
C i ∈ R |T ns i |×|T s i | between the selected instances' feature D (s) i
and unselected instances D (ns) i for each task.And for each unselected instance, we find the top 3 most similar instances and compute their average similarity c j i .We set an adaptive threshold τ 3 .Based on the average similarity c j i and τ 3 , the instances can be divided into two parts.</p>
<p>For each instance in {x j i |c j i ≥ τ 3 }, we use the average evaluation results of the target LLM f t on the three most similar sampled instances as the predicted performance.Hence, we can get the estimated sum of f t 's performances ĉns i1 on {x j i |c j i ≥ τ 3 }.This method is similar to the idea of item-based collaborative filtering.</p>
<p>For each instance in {x j i |c j i &lt; τ 3 }, we use the average results of f t 's similar LLMs as the predicted performance.And we can get the estimated sum of f t 's performances ĉns i2 .This is similar to user-based collaborative filtering.Thus, we can replace a ns i with ĉns i = ĉns i0 + ĉns i1 + ĉns i2 in Equation 11to obtain an estimate of model performance pi .Through experiments, we find that pi yields more precise results compared to p s i .For LLM's ranking prediction, we compare the estimated performance pi of the target LLM f t with the performances of initial LLMs to obtain the ranking prediction ri .</p>
<p>Comparison with Previous Methods</p>
<p>As shown in Table 2, our method meets all the criteria described in Section 3.2 which was not achievable with previous approaches.Specifically, our method only requires instances that are discriminative for models, which makes it efficient and personalized.For instances that are too simple or too difficult, our method uses the average results of the initial models for estimation.This ensures that our approach meets the commonness criterion.In addition, our method uses OT to leverage information from other tasks for prediction, thus meeting the complementation criterion.</p>
<p>Experiments</p>
<p>Setups.Our experiment mainly focuses on efficiently evaluating the performance of new LLMs on Benchmarks, given the evaluation results of some initial LLMs on the Benchmark datasets.This aligns with real-world scenarios.Based on their release dates, we select some of the earliest released LLMs for each Benchmark as initial LLMs.As described in Section 1, the overall performance of an LLM cannot reflect its task-level performance.Therefore, we focus on whether the efficient evaluation method can accurately predict the LLM's performance on each task.To compare the adaptability of the efficient evaluation method to different sample sizes, we set 5 ratios ([0.1, 0.2, 0.3, 0.4, 0.5]) to select corresponding subsets from each task's dataset.</p>
<p>Benchmark.We choose two commonly used LLM Benchmarks for our experiments.(1) HuggingFace's Open LLM Leaderboard is a public ranking platform designed to compare and showcase the performance of open-source large language models (LLMs).It provides a standardized evaluation framework across multiple benchmarks, including tasks like natural language understanding, generation, and reasoning.We follow TinyBenchmark (Polo et al., 2024) to collect the evaluation results of 395 LLMs and get approximately 50K instances.We divide the 395 LLMs into initial LLMs and test LLMs based on their release dates in a 3:1 ratio.(2) MMLU: MMLU is a multiple-choice QA benchmark consisting of 57 tasks.It comprises approximately 14K instances and we consider the same set of 395 LLMs and train-test splits.The reason to consider it is its immense popularity when comparing LLMs and inclusion into several other benchmarks.</p>
<p>Baselines.We adopt four baselines as our comparison methods, which are Random Sampling, Baseline with Clustering, TinyBenchmark, and Sort&amp;Search.Since Baseline with Clustering has been introduced in Section 3.3, we do not repeat it here.For Random Sampling, we randomly sample instances from each task's dataset to form its subset.Then we use the LLM's performance on these subsets to act as the estimation for its performance on each task's dataset.We rank different LLMs based on their performances on the sampled subsets.For TinyBenchmark (Polo et al., 2024) and Sort&amp;Search (Prabhu et al., 2024), we compare the target LLM's estimated performance with the real performance of other LLMs on each task's dataset to obtain the ranking of the target LLM.For methods with randomness, we repeat the above experiment 5 times and report the mean as result.</p>
<p>Evaluations.For each task, we calculate the Mean Absolute Error (MAE) between the predicted performance and the actual performance for each model.Then, we take the mean of MAE for all tasks as the final performance of the Efficient Benchmark Method.For example, for the MMLU Benchmark, we should calculate the average MAE for 395 test models on 57 datasets, totaling 22,515 test cases.Additionally, considering that we are more concerned with evaluating strong models in practice, we calculate the weighted MAE by using the scaling factor 1 ri to weight the MAE of different LLMs based on their true rankings r i .</p>
<p>Results Analysis.Figure 5 shows the MAE and weighted MAE between the estimated LLM's performance metrics (performance and ranking) by different Efficient Benchmark methods and the actual performance metrics of LLMs.The smaller the values of MAE and Weighted MAE, the more accurate the performance metrics of LLMs estimated by Efficient Benchmark are.From the Figure 5, we can see that on benchmarks called Open LLM Leaderboard and MMLU our method obtains the most accurate estimates of LLM's performance.Moreover, surprisingly, Baseline with Clustering performs very well, and only our method consistently outperforms it across different ratios.</p>
<p>Running Time Analysis The method's runtime is also an important metric to compare efficient evaluation methods.For simplicity, we assume that the inference time of a LLM is the same for different instances.We show the time taken by different methods to evaluate 1 model on MMLU at a sampling ratio of 0.1.Table 3 shows the results, which include the time for methods to deploy (Deployment Time), the time to select instances (Select Time), the time to predict performance (Predict Time) and the sum of Select time and Predict Time (Total Time).Compared to TinyBenchmark and Clustering Baseline, our method demonstrates a smaller time cost while providing accurate LLM performance estimates.Although our method is slower than the Random and Sort&amp;Search methods, the slight increase in time is acceptable considering their poor performance.</p>
<p>Ablation Study</p>
<p>We conduct an ablation study on the components and parameters of the method in Section 4. The specific roles and details of the parameters can be found in Appendix D. The experiments demonstrate that optimal transport is an indispensable module in our method, and our method is robust to different parameter settings.Furthermore, it shows that the performance reported in Section 5 is not the best, indicating that our method has further potential.</p>
<p>Conclusion</p>
<p>In this work, we focus on designing an efficient evaluation method to evaluate the target LLM's task-level capacities at a low cost.We re-examine the issue from the perspectives of collaborative filtering in recommendation systems and propose a two-stage method, which includes instance selection stage and performance estimation stage.The experimental results across multiple LLMs and datasets demonstrate the effectiveness of our method.</p>
<p>C. Instance Sampling Details</p>
<p>Assume |Ti| is the number of test instance in the dataset for the i-th task, and let α be the sampling ratio, then α × |Ti| represents the number of sampled instances.To ensure an accurate estimation of LLM performance, we set a minimum sample size of 20 for each dataset.When |Ti| is less than 20, we will use all samples from the current dataset.When |Ti| is greater than or equal to 20 but α × |Ti| is less than 20, we will set the number of sampled instances for the current dataset to 20.</p>
<p>D. Ablation Study</p>
<p>D.1. Optimal Transport Module</p>
<p>We plot the MAE metric on different ratios on MMLU. Figure 7 shows the result that our method with optimal transport is consistently better than that without optimal transport.All in all, introducing information from other tasks into efficient evaluation methods can improve method performance.</p>
<p>D.2. Hyperparameter</p>
<p>Next, we will analyze the hyperparameters involved in the method in Section 4. They are |S|, α, number of iterations in Section 4.1 and τ0, τ1, τ2, q in Section 4.2.Here τ2 and q are hyperparameters related to τ3 in Section 4.2.Specifically, τ3 = max(τ2, [ci]q).Here [ci]q refers to the q quantile of ci and ci a vector formed by the average similarities of different tasks.</p>
<p>The specific roles of these hyperparameters are shown in Table 4. Efficient Large Language Models Evaluation via Collaborative Filtering of iterations, α, q, τ2, τ1 and τ0, respectively.From the figure, we can find that our method is robust to different hyperparameters.</p>
<p>Figure 1 .
1
Figure 1.Comparison between Methods and Problem Setting.On the left, the red dashed line represents the real performance of a new model, and the gray area indicates the gap between the estimated performance of our method and the real performance, which is smaller.On the right is the problem setup, where the goal is to extract a subset from each task, use the new model's evaluation on it to predict performance on each task and minimize the gap between estimated and real performance.</p>
<p>Figure 2 .
2
Figure 2. The Paradigms of Original and Efficient LLM Benchmark.The left part illustrates the evaluation process of the Original LLM Benchmark.The right part shows the process of an efficient evaluating method, which consists of two main components: the Instance Selection Function g and the Performance Estimation Function h.The goal of efficient evaluation methods is to design effective g and h to minimize the difference between real performance p and predicted performance p.</p>
<p>Figure 3 .
3
Figure 3. Steps in Instance Selection Process.We select instances that can easily distinguish models through an iterative process.</p>
<p>Figure 4 .
4
Figure 4. Steps in Performance Prediction Process.We predict performance based on optimal transport and collaborative filtering.</p>
<p>for D (s) i and importance score for each instance in D (ns) i</p>
<p>Figure 5 .
5
Figure 5.The Mean Absolute Error (MAE) and weighted MAE between the estimated LLM's performance by different efficient evaluation methods and the real performance of LLMs.The first and second lines represent the results on MMLU and LB, respectively.</p>
<p>Figure 7 .
7
Figure 7.The MAE and Weighted MAE of Method with OT and Method without OT.</p>
<p>Figure 8 .
8
Figure 8.The MAE and Weighted MAE with Different Hyperparameters.</p>
<p>focuses on vision models.It proposes a method called Sort&amp;Search and this method leverages previous model predictions to get each test instance's difficulty and then ranks and selectively evaluates test instances.In this paper, we design a new efficient benchmark method for LLMs based on the idea of collaborative filtering, which has superior performance.
Xie et al. (2023b) design Data Selection with ImportanceResampling (DSIR) to select a tailored subset from thepretraining dataset for a specific target instance distribu-tion, aiming to maximize the performance of the pre-trainedmodel while adhering to a fixed compute budget. DSIRestimates importance weights in a reduced feature spacefor tractability and selects data with importance resamplingaccording to these weights. Xie et al. (2023a) leverage dis-tributionally robust optimization (DRO) to tune the domainweights without knowledge of downstream tasks. These do-main weights decide the mixture proportions of pretrainingdata domains. In this work, we primarily focus on instanceselection during the testing phase of large language models.
Data Selection for LLM.Some previous work has attempted to select training data for LLM during the training phase to reduce the impact of low-quality training instances on model performance and improve training speed and efficiency.Schoch et al. (2023) propose TS-DSHAPLEY to utilize Shapley Values to filter out harmful training data, thereby improving the performance after model fine-tuning.</p>
<p>Table 1 .
1
Performance between Embedding Methods.
Embedding MethodMAE of PerformanceMAE of RankingHistorical Results0.0352.9Semantic0.2106.7</p>
<p>can also extract a submatrix D
(ns) i∈ R B×|T ns i | whichrepresents unselected samples from D i . Then we can do theOptimal Transport (OT) problem as follows:argmin⟨C c , P ⟩P ≥0subject to|T st i | j=1P jk =1 |T ns i |,(12)|T ns i | k=1P jk =1 |T st i |where C c ∈ R |T st i |×|T ns i | is cost matrix between columns of D (st) i and columns of D (ns) i caculated by euler distance.With P , we can get synthetic dataset T sy iand D(sy) i∈R B×|T ns i | , where D(sy) i(st) i = D</p>
<p>Table 2 .
2
Comparison of Different Evaluation Methods.
MethodOursCluster-ingSort&amp; SearchTiny BenchmarkHigh Efficiency✓</p>
<p>Table 3 .
3
Comparison of Time Between Methods.
MethodDT(s)ST(s)PT(s)TT(s)Random00.00149 0.000782 0.00227Sort&amp;Search06.050.01676.07Clustering026.80.23227.0TinyBenchmark30926.83.3630.2Ours00.71324.525.2</p>
<p>Table 4 .
4
Roles of Different Hyperparameters.Figure 8 shows the MAE and weighted MAE with different hyperparameters on MMLU benchmark.Each row of the figure represents the performance change if only one hyperparameter is changed and the rest is unchanged.From top to bottom, they represent the |S|, number
HyperparameterRoles|S|the number of similar models for a newtarget modelαa hyperparameter that determines the im-portance score of the samplenumber of iterationsthe number of iterations in instance se-lection processτ0a hyperparameter used to select similartasksτ1a hyperparameter used to determineunimportant instancesτ2a hyperparameter used to determine theprediction modeqa hyperparameter used to determine theprediction mode
In the Appendix, we introduce more details about the Experiments.A. Experiments for Clustering BaselineA.1. Experiment SettingsWe sample 187 tasks(Zellers et al., 2019;Clark et al., 2018;Bisk et al., 2020;Mihaylov et al., 2018;Lai et al., 2017;Xu et al., 2021;2020;Huang et al., 2023)from OpenCompass (OpenCompass, 2023), a large language model evaluation benchmark.We collect the evaluation results of 32 widely used LLMs (e.g., LLAMA(Touvron et al., 2023a), Qwen(Bai et al., 2023), ChatGLM(Zeng et al., 2023), Gemma(Mesnard et al., 2024)).We select the first 21 LLMs based on their release dates as the initial LLMs and use the remaining 11 LLMs as the new LLMs to be tested.We do the toy experiment with ratio 0.1, which is described in Section 5.A.2. Hypotheses TestingTo verify the hypothesis that there is a gap between the semantic embedding space and the evaluation results space.We plot Figure6on one task of MMLU benchmark.We can see that there is no obvious relationship between the two distances meaning that the two spaces are not aligned.B. Something for Binary Evaluation MatrixIf the ME in Section 3.1 yields binary results of 0 or 1, in other words, the evaluation results matrix Di in Section 3.2 is a binary matrix, then some equations can be calculated using alternative methods or may require further steps.B.1. Another Way to Calculate Important ScoreWith the ME yields binary results of 0 or 1, we can alternatively calculate v(x|F ) in Section 4.1 using the quantity difference between 0 and 1 values shown in Equation14.where I(•) denotes the indicator function, which takes the value 1 if the condition inside the parentheses is true, and 0 otherwise.B.2. Further steps for Optimal TransportIf Di in Section 4.2 is a binary matrix, further steps should be done after getting matrix D (sy) i in Section 4.2.Specifically, the elements in D (sy) i should be divided into 0-1 by 0.5 as the threshold.
. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu, Qwen technical report. 2023CoRR</p>
<p>PIQA: reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, AAAI. 2020</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ B Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen Chen, Jared Quincy Creel, Dorottya Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark S Koh, Ranjay Krass, Rohith Krishna, Kuditipudi, 2021CoRR</p>
<p>Think you have solved question answering? try arc, the AI2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, 2018CoRR</p>
<p>Computerized adaptive testing: Theory and practice. J Wim, Van Der Linden, Cees, Glas, 2000Springer</p>
<p>Text-to-sql empowered by large language models: A benchmark evaluation. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, Jingren Zhou, 2023VLDB</p>
<p>Using collaborative filtering to weave an information. David Goldberg, David A Nichols, Brian M Oki, Douglas B Terry, Commun. ACM. 1992</p>
<p>Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt ; Yuzhen, Yuzhuo Huang, Zhihao Bai, Junlei Zhu, Jinghan Zhang, Tangjun Zhang, Junteng Su, Chuancheng Liu, Yikai Lv, Jiayi Zhang, Yao Lei, Fu, NeurIPS. 2021. 2023ICLR</p>
<p>. HuggingFace. Open llm leaderboard. 2024</p>
<p>RACE: large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard H Hovy, EMNLP. 2017</p>
<p>CMMLU: measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, ACL. 2024</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, 2023TMLR</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang, ICLR. 2024</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shengyu Shao Kun Deng, Shujie Fu, Liu, NeurIPS. 2021</p>
<p>. Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin</p>
<p>Chiu, Open models based on gemini research and technology. CoRR2024</p>
<p>Can a suit of armor conduct electricity? A new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, EMNLP. 2018</p>
<p>Opencompass: A universal evaluation platform for foundation models. Opencompass, 2023</p>
<p>Efficient benchmarking (of language models). Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen, NAACL. 2024</p>
<p>Computational optimal transport: With applications to data science. Gabriel Peyré, Marco Cuturi, Foundations and Trends® in Machine Learning. 201911</p>
<p>Chatgpt as a math questioner? evaluating chatgpt on generating pre-university math questions. Van Long, Phuoc Pham, Duc , Anh Vu, Nhat , Minh Hoang, Xuan Long Do, Anh Tuan Luu, SAC. 2024</p>
<p>tinybenchmarks: evaluating llms with fewer examples. Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin, ICML. 2024</p>
<p>Lifelong benchmarks: Efficient model evaluation in an era of rapid progress. Ameya Prabhu, Vishaal Udandarao, H S Philip, Matthias Torr, Adel Bethge, Samuel Bibi, Albanie, ICLR2024</p>
<p>Toolllm: Facilitating large language models to master 16000+ real-world apis. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, ICLR. 2023</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, EMNLP-IJCNLP. 2019</p>
<p>Collaborative filtering recommender systems. Ben Schafer, Dan Frankowski, Jon Herlocker, Shilad Sen, The adaptive web: methods and strategies of web personalization. Springer2007</p>
<p>Data selection for fine-tuning large language models using transferred shapley values. Stephanie Schoch, Ritwick Mishra, Yangfeng Ji, ACL. 2023</p>
<p>A survey of collaborative filtering techniques. Xiaoyuan Su, Taghi M Khoshgoftaar, 421425:1- 421425:19Adv. Artif. Intell. 2009. 2009</p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, 2023aCoRR</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, 2023bCoRRRobert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Anchor points: Benchmarking models with much fewer examples. Rajan Vivek, Kawin Ethayarajh, Diyi Yang, Douwe Kiela, EACL. 2024</p>
<p>Doremi: Optimizing data mixtures speeds up language model pretraining. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, Adams Wei Yu, NeurIPS. 2023a</p>
<p>Data selection for language models via importance resampling. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang, NeurIPS2023b</p>
<p>CLUE: A chinese language understanding evaluation benchmark. Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan, COLING. 2020</p>
<p>Fewclue: A chinese few-shot learning evaluation benchmark. Liang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei Zhang, Hu Yuan, Huilin Xu, Guoao Wei, Xiang Pan, Hai Hu, 2021CoRR</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, ACL. 2019</p>
<p>GLM-130B: an open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, ICLR. 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, NAACL. 2024</p>
<p>A survey of large language models in medicine: Progress, application, and challenge. Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Peilin Sam S Chen, Junling Zhou, Yining Liu, Chengfeng Hua, Xian Mao, Wu, 2023CoRR</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6516 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6516</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6516</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276259248</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.07191v4.pdf" target="_blank">Bag of Tricks for Inference-time Computation of LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/usail-hkust/benchmark_inference_time_computation_LLM</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6516.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6516.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-of-N sampling with external verifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble decoding method that samples N candidate chain-of-thought outputs and selects the highest-scoring candidate according to a reward/verifier model; used as the paper's default inference-time computation baseline (N=32).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle (among others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>open-domain question answering / knowledge-based QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>48.4</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-6.4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Used as the default decoding/selection pipeline (sample N candidates then use reward model). The method benefits from candidate diversity produced by sampling hyperparameters; sensitivity to reward model generalization noted. Reported to work well on knowledge-based tasks but less consistently on complex math/code.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6516.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over multiple reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generates multiple reasoning paths/answers and picks the most frequently produced final answer (majority voting) to improve robustness of chain-of-thought outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle (reported as representative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based question answering / multi-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>54.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-Consistency produced higher accuracy than Best-of-N on the reported knowledge-based benchmark (Bamboogle) under equal token budget; authors note Self-Consistency and Self-Refine scale better with token consumption for some tasks. The approach relies on diversity of sampled reasoning paths to produce a robust majority answer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6516.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-level Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-level Best-of-N sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applies Best-of-N selection at each generation step (selecting promising intermediate tokens/steps) rather than only at full-sequence level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Step-level Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>52.4</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Step-level Best-of-N outperformed sequence-level Best-of-N on the reported benchmark, indicating that selecting higher-quality intermediate steps can increase final answer accuracy; improvement depends on sampling hyperparameters and budget.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6516.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam Search decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deterministic breadth-first decoding that expands top candidates layer-by-layer (beam width) to find high-likelihood outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / deterministic search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>33.1</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-15.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Beam Search showed minimal gains and low token-efficiency for many reasoning tasks; authors note adding more completion tokens does not always improve accuracy and Beam Search is token-inefficient compared to sampling-based ensemble techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6516.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search (MCTS) decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Search-based inference that grows a tree of partial solutions using stochastic rollouts to identify promising branches (used for planning-style reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based QA / planning-like reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>49.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>MCTS yields moderate improvements but shows slower gains with increased token budgets compared to Self-Consistency; performance is task-specific and often lags in token-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6516.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative approach where the model iteratively critiques and refines its own outputs during inference to improve final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative / refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>41.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-12.9</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-Refine underperformed relative to Self-Consistency in many experiments; authors hypothesize overly restrictive self-feedback or poor self-evaluation limits improvement unless external/verifier signals are accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6516.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction prompting method that elicits step-by-step intermediate reasoning steps from the model to aid multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA / Qwen / Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–72B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / decompositional prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (stepwise)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (representative arithmetic dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems requiring multi-step calculation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Reflect CoT, IO prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT generally improves reasoning accuracy compared to direct IO prompting; however, self-correction / reflection-based CoT yields mixed results across datasets. Authors emphasize prompt type strongly influences reasoning trajectories and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6516.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflect CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of CoT where each reasoning step is followed by an explicit reflection/verification to catch errors during the chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Reflect CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / reflective prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Bamboogle / MATH (reported examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based QA and complex mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reflect CoT sometimes fails to improve and in some reported cases does not lead to improvements for Bamboogle and MATH using temperature=0.8 and top-p=0.9. Reflection/self-correction is task-sensitive and often unreliable without external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6516.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature (τ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling temperature hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding hyperparameter that scales logits to control randomness of token sampling; higher τ increases output diversity and lower τ yields more deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA / Qwen / Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>sampling temperature (τ)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sampling / diversity control</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (controls degree of diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multiple (e.g., GSM8K, MATH, HumanEval, Bamboogle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>various reasoning tasks (arithmetic, math, code, QA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy improvement (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>4.83</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>lower/higher temperature settings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.83</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Tuning temperature produced improvements of ~2.32%–4.83% across four datasets; optimal observed τ ≈ 0.8 stabilizes performance. Higher τ increases diversity of reasoning styles, which can help ensemble selection methods but may also inject noise.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6516.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-p</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nucleus sampling (top-p) hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling strategy that restricts the candidate token set to the smallest set whose cumulative probability exceeds p, balancing diversity and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA / Qwen / Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>top-p (nucleus sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sampling / diversity control</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (controls effective vocabulary)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multiple (representative tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>various reasoning tasks (arithmetic, math, code, QA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy improvement (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>5.88</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>lower/higher top-p settings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.88</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Optimal top-p reported near 0.9, with improvements in the range 2.32%–5.88%; increasing top-p generally increased diversity and improved reasoning up to a point, beyond which coherence dropped.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6516.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Process Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process-based reward model (step-level evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reward model that evaluates intermediate reasoning steps (process) rather than only the final answer; designed to encourage correct decomposition and reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B (used as reward/evaluator in sections)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>process reward</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>external verifier / reward model</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>supports process-aware evaluation (mixed)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH / HumanEval (complex reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>complex mathematical reasoning and code generation requiring correct intermediate steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / pass@1 (task-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Result Reward, RLHF, Proof-Critical, LLM-as-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Process rewards often excel in complex tasks (mathematics, code) by evaluating step correctness; however, reward models suffer from generalization issues and can inflate performance on some tasks. Authors used process reward (prompt-driven) and found inconsistent scaling behavior across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6516.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLHF Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from Human Feedback (RLHF) reward model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned scalar reward model trained on human preference data used to score candidate solutions during inference selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Chat-1.8B-SFT-derived RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.8B (reward model architecture reference)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RLHF reward</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>external verifier / reward model</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>knowledge-based reasoning (e.g., Bamboogle, HotpotQA, FEVER)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>knowledge-based QA and fact verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Process Reward, Proof-Critical, LLM-as-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>RLHF rewards sometimes improve performance on knowledge-based tasks but have inconsistent effectiveness across complex reasoning; reward models can produce inflated performance estimates due to poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6516.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proof-Critical Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proof-Critical reward model (proof-aware numerical reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward type tailored to formal mathematical proof tasks that evaluates proof-critical correctness and assigns scalar scores to candidate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-5-Step-Prover-Critic (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Proof-Critical reward</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>external verifier / reward model (formal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>complex mathematical problem solving / proof validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RLHF, LLM-as-Judge, Process Reward</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Proof-Critical reward can sometimes degrade performance (not monotonic improvement) on challenging math tasks due to generalization limits; authors observed declines with scaling test-time computation in MATH500 when using Proof-Critical reward.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6516.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e6516.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-evaluation (LLM evaluating its own outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The model evaluates its own candidates via prompts mapping fuzzy judgments (e.g., 'sure/likely/impossible') to probabilities to select outputs without an external verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7B / LLaMA variants (used in analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–8B (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>self-evaluation (process/result)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>internal verifier / self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (self-assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multiple (presented figures across datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>selection of optimal candidate solution in inference-time pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy relative to random/majority</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>random selection, majority voting, external reward models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-evaluation often fails to improve selection quality and in some worst-case scenarios performs worse than random selection; LLMs struggle to reliably judge their own reasoning without external signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bag of Tricks for Inference-time Computation of LLM Reasoning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Best-of-N: Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>A survey of monte carlo tree search methods <em>(Rating: 1)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>ProcessBench: identifying process errors in mathematical reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6516",
    "paper_id": "paper-276259248",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "Best-of-N",
            "name_full": "Best-of-N sampling with external verifier",
            "brief_description": "An ensemble decoding method that samples N candidate chain-of-thought outputs and selects the highest-scoring candidate according to a reward/verifier model; used as the paper's default inference-time computation baseline (N=32).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Best-of-N",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Bamboogle (among others)",
            "task_description": "open-domain question answering / knowledge-based QA",
            "performance_metric": "accuracy (%)",
            "performance_value": 48.4,
            "comparison_target_method": "Self-Consistency",
            "performance_difference": -6.4,
            "statistical_significance": false,
            "analysis_notes": "Used as the default decoding/selection pipeline (sample N candidates then use reward model). The method benefits from candidate diversity produced by sampling hyperparameters; sensitivity to reward model generalization noted. Reported to work well on knowledge-based tasks but less consistently on complex math/code.",
            "ablation_study_present": true,
            "uuid": "e6516.0",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over multiple reasoning paths)",
            "brief_description": "Generates multiple reasoning paths/answers and picks the most frequently produced final answer (majority voting) to improve robustness of chain-of-thought outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Bamboogle (reported as representative)",
            "task_description": "knowledge-based question answering / multi-domain QA",
            "performance_metric": "accuracy (%)",
            "performance_value": 54.8,
            "comparison_target_method": "Best-of-N",
            "performance_difference": 6.4,
            "statistical_significance": false,
            "analysis_notes": "Self-Consistency produced higher accuracy than Best-of-N on the reported knowledge-based benchmark (Bamboogle) under equal token budget; authors note Self-Consistency and Self-Refine scale better with token consumption for some tasks. The approach relies on diversity of sampled reasoning paths to produce a robust majority answer.",
            "ablation_study_present": true,
            "uuid": "e6516.1",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Step-level Best-of-N",
            "name_full": "Step-level Best-of-N sampling",
            "brief_description": "Applies Best-of-N selection at each generation step (selecting promising intermediate tokens/steps) rather than only at full-sequence level.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Step-level Best-of-N",
            "reasoning_method_type": "sequential / ensemble",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Bamboogle",
            "task_description": "knowledge-based QA",
            "performance_metric": "accuracy (%)",
            "performance_value": 52.4,
            "comparison_target_method": "Best-of-N",
            "performance_difference": 4.0,
            "statistical_significance": false,
            "analysis_notes": "Step-level Best-of-N outperformed sequence-level Best-of-N on the reported benchmark, indicating that selecting higher-quality intermediate steps can increase final answer accuracy; improvement depends on sampling hyperparameters and budget.",
            "ablation_study_present": true,
            "uuid": "e6516.2",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Beam Search",
            "name_full": "Beam Search decoding",
            "brief_description": "Deterministic breadth-first decoding that expands top candidates layer-by-layer (beam width) to find high-likelihood outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Beam Search",
            "reasoning_method_type": "sequential / deterministic search",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "Bamboogle",
            "task_description": "knowledge-based QA",
            "performance_metric": "accuracy (%)",
            "performance_value": 33.1,
            "comparison_target_method": "Best-of-N",
            "performance_difference": -15.3,
            "statistical_significance": false,
            "analysis_notes": "Beam Search showed minimal gains and low token-efficiency for many reasoning tasks; authors note adding more completion tokens does not always improve accuracy and Beam Search is token-inefficient compared to sampling-based ensemble techniques.",
            "ablation_study_present": true,
            "uuid": "e6516.3",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MCTS",
            "name_full": "Monte Carlo Tree Search (MCTS) decoding",
            "brief_description": "Search-based inference that grows a tree of partial solutions using stochastic rollouts to identify promising branches (used for planning-style reasoning).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "MCTS",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Bamboogle",
            "task_description": "knowledge-based QA / planning-like reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": 49.2,
            "comparison_target_method": "Best-of-N",
            "performance_difference": 0.8,
            "statistical_significance": false,
            "analysis_notes": "MCTS yields moderate improvements but shows slower gains with increased token budgets compared to Self-Consistency; performance is task-specific and often lags in token-efficiency.",
            "ablation_study_present": true,
            "uuid": "e6516.4",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "Iterative approach where the model iteratively critiques and refines its own outputs during inference to improve final answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Self-Refine",
            "reasoning_method_type": "iterative / refinement",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Bamboogle",
            "task_description": "knowledge-based QA",
            "performance_metric": "accuracy (%)",
            "performance_value": 41.9,
            "comparison_target_method": "Self-Consistency",
            "performance_difference": -12.9,
            "statistical_significance": false,
            "analysis_notes": "Self-Refine underperformed relative to Self-Consistency in many experiments; authors hypothesize overly restrictive self-feedback or poor self-evaluation limits improvement unless external/verifier signals are accurate.",
            "ablation_study_present": true,
            "uuid": "e6516.5",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "Instruction prompting method that elicits step-by-step intermediate reasoning steps from the model to aid multi-step problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA / Qwen / Mistral (various)",
            "model_size": "various (7B–72B)",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential / decompositional prompting",
            "reasoning_style_diversity": "single style (stepwise)",
            "benchmark_name": "GSM8K (representative arithmetic dataset)",
            "task_description": "grade-school arithmetic word problems requiring multi-step calculation",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Reflect CoT, IO prompt",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "CoT generally improves reasoning accuracy compared to direct IO prompting; however, self-correction / reflection-based CoT yields mixed results across datasets. Authors emphasize prompt type strongly influences reasoning trajectories and performance.",
            "ablation_study_present": true,
            "uuid": "e6516.6",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Reflect CoT",
            "name_full": "Reflective Chain-of-Thought prompting",
            "brief_description": "A variant of CoT where each reasoning step is followed by an explicit reflection/verification to catch errors during the chain-of-thought.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-7B",
            "model_size": "7B",
            "reasoning_method_name": "Reflect CoT",
            "reasoning_method_type": "sequential / reflective prompting",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Bamboogle / MATH (reported examples)",
            "task_description": "knowledge-based QA and complex mathematics",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "CoT",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Reflect CoT sometimes fails to improve and in some reported cases does not lead to improvements for Bamboogle and MATH using temperature=0.8 and top-p=0.9. Reflection/self-correction is task-sensitive and often unreliable without external verification.",
            "ablation_study_present": true,
            "uuid": "e6516.7",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Temperature (τ)",
            "name_full": "Sampling temperature hyperparameter",
            "brief_description": "A decoding hyperparameter that scales logits to control randomness of token sampling; higher τ increases output diversity and lower τ yields more deterministic outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA / Qwen / Mistral (various)",
            "model_size": "various",
            "reasoning_method_name": "sampling temperature (τ)",
            "reasoning_method_type": "sampling / diversity control",
            "reasoning_style_diversity": "diverse (controls degree of diversity)",
            "benchmark_name": "multiple (e.g., GSM8K, MATH, HumanEval, Bamboogle)",
            "task_description": "various reasoning tasks (arithmetic, math, code, QA)",
            "performance_metric": "accuracy improvement (%)",
            "performance_value": 4.83,
            "comparison_target_method": "lower/higher temperature settings",
            "performance_difference": 4.83,
            "statistical_significance": false,
            "analysis_notes": "Tuning temperature produced improvements of ~2.32%–4.83% across four datasets; optimal observed τ ≈ 0.8 stabilizes performance. Higher τ increases diversity of reasoning styles, which can help ensemble selection methods but may also inject noise.",
            "ablation_study_present": true,
            "uuid": "e6516.8",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Top-p",
            "name_full": "Nucleus sampling (top-p) hyperparameter",
            "brief_description": "Sampling strategy that restricts the candidate token set to the smallest set whose cumulative probability exceeds p, balancing diversity and coherence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA / Qwen / Mistral (various)",
            "model_size": "various",
            "reasoning_method_name": "top-p (nucleus sampling)",
            "reasoning_method_type": "sampling / diversity control",
            "reasoning_style_diversity": "diverse (controls effective vocabulary)",
            "benchmark_name": "multiple (representative tasks)",
            "task_description": "various reasoning tasks (arithmetic, math, code, QA)",
            "performance_metric": "accuracy improvement (%)",
            "performance_value": 5.88,
            "comparison_target_method": "lower/higher top-p settings",
            "performance_difference": 5.88,
            "statistical_significance": false,
            "analysis_notes": "Optimal top-p reported near 0.9, with improvements in the range 2.32%–5.88%; increasing top-p generally increased diversity and improved reasoning up to a point, beyond which coherence dropped.",
            "ablation_study_present": true,
            "uuid": "e6516.9",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Process Reward",
            "name_full": "Process-based reward model (step-level evaluation)",
            "brief_description": "Reward model that evaluates intermediate reasoning steps (process) rather than only the final answer; designed to encourage correct decomposition and reasoning trajectories.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B (used as reward/evaluator in sections)",
            "model_size": "32B",
            "reasoning_method_name": "process reward",
            "reasoning_method_type": "external verifier / reward model",
            "reasoning_style_diversity": "supports process-aware evaluation (mixed)",
            "benchmark_name": "MATH / HumanEval (complex reasoning tasks)",
            "task_description": "complex mathematical reasoning and code generation requiring correct intermediate steps",
            "performance_metric": "accuracy / pass@1 (task-dependent)",
            "performance_value": null,
            "comparison_target_method": "Result Reward, RLHF, Proof-Critical, LLM-as-Judge",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Process rewards often excel in complex tasks (mathematics, code) by evaluating step correctness; however, reward models suffer from generalization issues and can inflate performance on some tasks. Authors used process reward (prompt-driven) and found inconsistent scaling behavior across tasks.",
            "ablation_study_present": true,
            "uuid": "e6516.10",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RLHF Reward",
            "name_full": "Reinforcement Learning from Human Feedback (RLHF) reward model",
            "brief_description": "A learned scalar reward model trained on human preference data used to score candidate solutions during inference selection.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternLM2-Chat-1.8B-SFT-derived RLHF",
            "model_size": "1.8B (reward model architecture reference)",
            "reasoning_method_name": "RLHF reward",
            "reasoning_method_type": "external verifier / reward model",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "knowledge-based reasoning (e.g., Bamboogle, HotpotQA, FEVER)",
            "task_description": "knowledge-based QA and fact verification",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Process Reward, Proof-Critical, LLM-as-Judge",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "RLHF rewards sometimes improve performance on knowledge-based tasks but have inconsistent effectiveness across complex reasoning; reward models can produce inflated performance estimates due to poor generalization.",
            "ablation_study_present": true,
            "uuid": "e6516.11",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Proof-Critical Reward",
            "name_full": "Proof-Critical reward model (proof-aware numerical reward)",
            "brief_description": "A reward type tailored to formal mathematical proof tasks that evaluates proof-critical correctness and assigns scalar scores to candidate solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternLM2-5-Step-Prover-Critic (reference)",
            "model_size": "unspecified",
            "reasoning_method_name": "Proof-Critical reward",
            "reasoning_method_type": "external verifier / reward model (formal)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "MATH500",
            "task_description": "complex mathematical problem solving / proof validation",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "RLHF, LLM-as-Judge, Process Reward",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Proof-Critical reward can sometimes degrade performance (not monotonic improvement) on challenging math tasks due to generalization limits; authors observed declines with scaling test-time computation in MATH500 when using Proof-Critical reward.",
            "ablation_study_present": true,
            "uuid": "e6516.12",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Evaluation",
            "name_full": "Self-evaluation (LLM evaluating its own outputs)",
            "brief_description": "The model evaluates its own candidates via prompts mapping fuzzy judgments (e.g., 'sure/likely/impossible') to probabilities to select outputs without an external verifier.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-7B / LLaMA variants (used in analyses)",
            "model_size": "7B–8B (representative)",
            "reasoning_method_name": "self-evaluation (process/result)",
            "reasoning_method_type": "internal verifier / self-critique",
            "reasoning_style_diversity": "single style (self-assessment)",
            "benchmark_name": "multiple (presented figures across datasets)",
            "task_description": "selection of optimal candidate solution in inference-time pipeline",
            "performance_metric": "accuracy relative to random/majority",
            "performance_value": null,
            "comparison_target_method": "random selection, majority voting, external reward models",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Self-evaluation often fails to improve selection quality and in some worst-case scenarios performs worse than random selection; LLMs struggle to reliably judge their own reasoning without external signals.",
            "ablation_study_present": true,
            "uuid": "e6516.13",
            "source_info": {
                "paper_title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Best-of-N: Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "bestofn_training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "A survey of monte carlo tree search methods",
            "rating": 1,
            "sanitized_title": "a_survey_of_monte_carlo_tree_search_methods"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "ProcessBench: identifying process errors in mathematical reasoning",
            "rating": 2,
            "sanitized_title": "processbench_identifying_process_errors_in_mathematical_reasoning"
        }
    ],
    "cost": 0.016604999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bag of Tricks for Inference-time Computation of LLM Reasoning
17 Feb 2025</p>
<p>Fan Liu 
Hong Kong University of Science and Technology</p>
<p>Wenshuo Chao 
Hong Kong University of Science and Technology</p>
<p>Naiqiang Tan 
Hong Kong University of Science and Technology</p>
<p>Hao Liu 
Hong Kong University of Science and Technology</p>
<p>Hong Kong University of Science and Technology</p>
<p>Bag of Tricks for Inference-time Computation of LLM Reasoning
17 Feb 20257BBE85C85224DFC9203318871761BA77arXiv:2502.07191v4[cs.AI]
With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention.Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training.However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks.In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity.Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types).Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%).Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks.These findings provide a stronger foundation for future research.The code is available at https://github.com/</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated remarkable reasoning capabilities, enabling them to tackle increasingly sophisticated tasks in fields such as science, mathematics, and coding (Zhang et al., 2024a;Chen et al., 2021).While scaling model size and expanding high-quality training datasets have significantly driven these advancements, researchers are actively exploring complementary approaches to further enhance model performance.Inspired by human problem-solving behavior-where individuals often dedicate more time deliberating on complex problems to improve their decisions-there is growing interest (Snell et al., 2024) in leveraging inference-time computation (e.g., utilizing additional computation during testing to enhance the performance of reasoning tasks) to strengthen the reasoning abilities of LLMs.</p>
<p>While inference-time computation holds significant potential for enhancing the reasoning performance of LLMs (Wang et al., 2022), existing studies reveal mixed results in inference-time computation (e.g., limited selfcorrection capabilities (Huang et al., 2023)).Its effectiveness on broader reasoning tasks (e.g., logical reasoning, code generation, question answering, and fact verification) remains limited, with most research narrowly focused on domains like math problems.Moreover, inference-time methods are sensitive to hyperparameters, such as temperature and top-p sampling, where small adjustments can lead to notable performance differences (e.g., a 5% improvement in solving math problems by tuning temperature).These challenges underscore the critical role of inference-time techniques (e.g., instructing prompt, sampling strategies, reward models), as shown in Table 1.Despite recent advancements, these gaps indicate that the field remains nascent, with many challenges yet to be addressed.</p>
<p>In this study, we investigate key tricks that influence the effectiveness of inference-time computation methods in LLM reasoning.Since most current methods rely on a proposer-verifier pipeline that first generates candidate solu-Table 1: Configuration of inference-time computation methods.Inference-time computation involves two main steps: generating candidate solutions (e.g., chain-of-thought reasoning) and selecting the optimal solution.These configurations, though significant, often receive less attention and lack standardization.</p>
<p>Methods</p>
<p>Domain *N Prompt</p>
<p>Temperature Top-p Trajectory Selection Verification Reward Model Q<em> (Wang et al., 2024) Math/Code 6 COT 0.9/0.2</em> Best-of-N</p>
<p>Step-level reward (score value) Policy model MALT (Motwani et al., 2024) Math/CommonsenseQA 27 COT 0.3 * Best-of-N Trajectory-level reward Verify agent GenRM (Zhang et al., 2024a) Math/Algorithmic 1-32 COT * * Best-of-N</p>
<p>Step-level reward (yes or no) Verifier HiAR-ICL (Wu et al., 2024a) Math/CommonsenseQA 5 Automatic COT 0.8 0.9 MCTS</p>
<p>Step-level reward Self-Verify CPO (Zhang et al., 2024b) Math/CommonsenseQA/Fact Verification 5 COT 0.9/0.40.9 Tree of Search</p>
<p>Step-level reward (score value) Self-Verify AutoMathCritique (Xi et al., 2024) Math 1-128 COT 0.7 *</p>
<p>Step-level refine</p>
<p>Step-level reward ( correct or wrong) Critique model tions (e.g., chain-of-thought candidate solutions) and then selects the optimal solution based on specific reward signals (e.g., RLHF rewards, process rewards), our research focuses on strategies for both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types) across broader reasoning tasks, including logical reasoning, code generation, fact verification, complex mathematics, and arithmetic.Through ablation studies, we evaluate simple techniques in inference-time computation and empirically analyze their impact on reasoning performance.Our analysis reveals that inference-time computation methods are highly sensitive to the experimental setup.</p>
<p>With proper configurations, our results demonstrate that previously overlooked techniques can significantly enhance performance.Furthermore, we emphasize the need for a standardized benchmarking framework for inference-time computation in LLM reasoning.</p>
<p>Our main contributions and observations include the following: 1) Evaluation of Key Tricks: We evaluate the impact of a wide range of key tricks (e.g., prompt type, temperature, top-p, reward model type) on LLMs.Key insights include: Instruction prompts significantly influence LLM reasoning, with self-correction often yielding mixed results compared to CoT prompting; Self-evaluation frequently fails to assess solution quality effectively; Reward models can cause performance inflation in LLM reasoning, with inconsistent effectiveness across tasks due to generalization issues; 2) Combination of Techniques: We further explore the effects of combining selected useful tricks on inference-time computation methods.Our empirical results suggest that improvements are not always additive when combining different techniques, although methods such as prompt design, temperature, and reward models can be effective, as demonstrated in Section 4.2.3.3) Comprehensive Benchmarks:</p>
<p>We conduct extensive experiments, evaluating six representative inference-time computation methods across eight reasoning tasks, utilizing over 20,000 A100-80G GPU hours and more than 1,000 individual tests.Our results establish a baseline benchmark for inference-time computation, as shown in Table 2.</p>
<p>Related Work</p>
<p>We briefly introduce related work, including reasoning with LLMs, inference-time computation methods for LLM reasoning, and benchmarks of LLM reasoning.</p>
<p>Reasoning with LLMs.LLMs have demonstrated strong reasoning abilities in complex tasks such as code generation, mathematical problem-solving, and research ideation (Zhou et al., 2022).Existing methods for enhancing LLM reasoning include: 1) Prompt engineering -Activates latent multi-step reasoning capabilities.For example, Chain of Thought (CoT) (Wei et al., 2022) guides step-by-step problem-solving but relies heavily on high-quality demonstrations for analogical learning.2) Post-training techniques (Chen et al., 2024a;b) -Iteratively enrich training datasets to improve model performance.Self-training methods (Chen et al., 2024a) curate new high-quality examples to enhance reasoning, but these approaches demand significant computational resources.3)Search-based methods (Browne et al., 2012;Feng et al., 2023a;Liu et al., 2023) -Optimize reasoning paths at inference time using search algorithms.For instance, Tree of Thought (Yao et al., 2024) employs breadth-first search to refine solutions.This work focuses on test-time computation, leveraging inference-time optimization to enhance LLM reasoning without additional training overhead.</p>
<p>Inference-Time Computation of LLM Reasoning.Scaling inference-time computation has proven more effective than merely increasing model parameters (Snell et al., 2024).</p>
<p>Recently, research has focused on optimizing reasoning efficiency during inference rather than solely scaling trainingtime computation.Best-of-N (Cobbe et al., 2021a) enhances LLM reasoning by sampling N candidate solutions, evaluating them with a learned verifier or reward model, and selecting the highest-scoring one.Similarly, MCTS (Tian et al., 2024) improves inference by actively planning and selecting higher-quality responses.These advancements highlight inference-time optimization as crucial for enhancing LLM reasoning beyond scaling training computation.</p>
<p>Benchmarks of LLM Reasoning.LLMs have made remarkable progress in solving complex tasks in a zero-shot manner (Hendrycks et al., 2021;Press et al., 2022;Liu et al., 2024a), positioning them as a key milestone toward artificial general intelligence.Consequently, benchmarking their reasoning abilities has become a central challenge.Recent studies have evaluated LLM reasoning across various domains, including mathematical reasoning (Hendrycks et al., 2021), code generation (Chen et al., 2021), and factual QA (Thorne et al., 2018), etc (Liu et al., 2024c;b).While these benchmarks enhance our understanding of LLM reasoning, most research has focused on task performance rather than inference-time computation, leaving key optimization techniques underexplored.</p>
<p>Unique to this paper, we are the first to comprehensively study how LLM reasoning performance changes with the incorporation of previously overlooked key techniques.We hope that our work will provide valuable insights into the role of inference-time computation.</p>
<p>Preliminares</p>
<p>LLMs.Given an input context x (e.g., math problem, commonsense QA, etc.), the LLM aims to autoregressively predict the next token (Dubey et al., 2024),
π θ (y|x) = n t=1 π θ (y t |x, y &lt;t ),(1)
where π θ (•) is the LLM parameterized by θ, and y = (y 1 , y 2 , • • • , y n ) is the output sequence.Here, y &lt;1 = ∅ and y &lt;t = (y 1 , y 2 , • • • , y t−1 ).For a vocabulary size M , the probability of predicting the t-th token is determined using a softmax with temperature τ on logit scores z of all tokens, combined with top-p (nucleus sampling) to control the randomness and diversity of the sampling process.</p>
<p>Chain of Thought Prompting.</p>
<p>Chain-of-thought (CoT) (Wei et al., 2022) is a method that prompts LLMs to generate a series of reasoning steps leading to the final answer.These intermediate steps, denoted as y 1 , . . ., y n−1 , connect the input x to the output y (omit n for simplicity), where n represents the total number of steps.For example, given an instruction I (e.g., "Let's solve this step by step") along with demonstration examples and the input question x, the final answer is y.Each intermediate thought y i is a part of the reasoning process that leads to the final answer.These thoughts are sequentially generated from the distribution y i ∼ π θ (• | I, x, y &lt;i−1 ), and the final output is sampled from: y ∼ π θ (• | I, x, y &lt;n−1 ).</p>
<p>Temperature.The temperature (Hinton, 2015) τ of LLM controls the level of randomness in the generated outputs, influencing their diversity.Instead of directly calculating the softmax, the logits are scaled by the temperature value.The conditional probability of generating a token in the sequence can be expressed as: π θ (y t | x, y &lt;t ) = exp(zt/τ ) M i=1 exp(zi/τ ) , where z t represents the logit score: logit θ (y t | x, y &lt;t ), and τ is the temperature parameter.A higher temperature τ results in a smoother probability distribution (introducing more randomness), while a lower temperature makes the distribution sharper, leading to more deterministic behavior.</p>
<p>Top-p.The top-p (Holtzman et al., 2019) controls the LLM output by augmenting the vocabulary size M as only those tokens are considered for which the cumulative probability (C k = k i=1 p (i) ) is greater than the Top-p value (the cumulative probability is calculated by sorting the tokens by their probability in a descending order and then adding them up, where: probabilities: {p (1) , p (2) , . . ., p (M ) }, where:
p (1) ≥ p (2) ≥ • • • ≥ p (M )
).After the tokens are selected, it would be re-calculated their softmax with reduced vocab size.The truncated probability distribution can be defined as:
π θ (y t | x, y <t ) = p (i) k j=1 p (j) , if i ≤ k, 0, if i > k.
. Inference-Time Computation Methods for LLM Reasoning.Inference-time computation methods (Ott et al., 2018) typically follows a pipeline comprising two main steps: generating candidate solutions (e.g., chain-of-thought reasoning candidates) and selecting the optimal solution based on specific reward signals (e.g., numerical reward, self-consistency, process reward, or binary feedback such as "Yes" or "No").Formally, given a problem x, the inferencetime computation methods sample K candidate solutions: y (k) ∼ π θ (y | I, x, y &lt;n ),for k = 1, 2, . . ., K, where y (k) represents the k-th candidate solution.After sampling, each candidate is evaluated using a reward model to produce a reward signal:r (k) = reward(I, x, y &lt;n−1 , y (k) ), where the reward model can take various forms.For example, it may be a general LLM that evaluates solutions using instructions I (e.g., "Let's verify the step-by-step reasoning.Is the answer correct (Yes/No)?").Alternatively, the reward model could be specifically trained to output a scalar value between 0 and 1, with higher values indicating better solutions.The final solution ŷ is then selected based on the reward signals.</p>
<p>For numerical rewards, the solution with the highest reward is chosen:ŷ = arg max y k r k .</p>
<p>Decoding Inference-Time Computation of LLM Reasoning</p>
<p>In this section, we decode the inference-time computation of LLM reasoning.First, we introduce the experimental setup, followed by the main bag of tricks for improving inference-time computation of LLM reasoning.Finally, we benchmark various inference-time computation methods.The overall framework is illustrated in Figure 1.ity and strong benchmark performance.2) Qwen 2.5 (Yang et al., 2024): Developed by Alibaba Cloud, this model offers 7B and 72B parameter configurations, showcasing diverse LLM architectures and training methods.3) Mistral 7B Instruct v0.3 (Jiang et al., 2023): A 7B parameter model from Mistral AI, recognized for its efficiency and performance rivaling larger models.These models exhibit diverse reasoning strengths, providing insights into the impact of different architectures and training approaches.Reward Model.</p>
<p>We employ four types of reward models: (1) Process Reward (Zheng et al., 2024): Evaluates each reasoning step step-by-step.</p>
<p>(2) Result Reward: Assesses only the final answer's correctness.</p>
<p>(3) RLHF Reward (Cai et al., 2024): Derived from preference samples (both human-annotated and AI-generated).( 4) Proof-Critical Reward: Applied in formal mathematical proof across multiple benchmarks.</p>
<p>The details of the setting can be seen in Appendix A.</p>
<p>Tasks.Our research focuses on the following reasoning tasks: 1) Arithmetic Reasoning: Evaluating models on GSM8K (Cobbe et al., 2021b) and GSM-Hard (Gao et al., 2022) datasets to test their arithmetic calculation skills.2) Complex mathematical reasoning: Using the MATH (Hendrycks et al., 2021) to assess proficiency in solving advanced mathematical problems.3) Logical Reasoning: measuring logical deduction and inference abilities with the ProntoQA (Saparov &amp; He, 2023) dataset.4) Code Generation: Testing code generation skills on the Hu-manEval (Chen et al., 2021) dataset.5) Question Answering: Evaluating performance in answering diverse questions using the Bamboogle (Press et al., 2022).6) Fact Verification: Assessing factual verification using the FEVER (Thorne et al., 2018) dataset.7) Common Sense Reasoning: Testing understanding of common sense knowledge and reasoning with the HotpotQA (Yang et al., 2018) dataset.</p>
<p>Inference-Time Computation Methods.This study examines common inference-time computation methods: 1) Best-of-N (Cobbe et al., 2021a): Generates multiple outputs (N samples) for a given input and selects the optimal one based on reward model.2) Step-Level Best-of-N Sampling (Cobbe et al., 2021a): Applies Best-of-N sampling at each generation step, selecting the most promising thoughts.</p>
<p>3) Self-Consistency (Wang et al., 2022): Produces multiple reasoning paths or answers and selects the most consistent one.4) Beam Search (Ott et al., 2018): Explores outputs level by level, expanding all nodes at the current depth before proceeding to the next.5) Monte Carlo Tree Search (MCTS) (Feng et al., 2023b): Uses random sampling to build a search tree and identify the most promising outputs.6) Self-Refine (Madaan et al., 2024): Allows LLMs to iteratively refine outputs during inference.</p>
<p>Evaluation Metrics.For most reasoning tasks, accuracy serves as the primary evaluation metric.For code generation, we use the pass@k metric (Chen et al., 2021), which considers a problem solved if at least one of k generated code samples passes all test cases.In our evaluation, we focus on pass@1 by generating multiple samples and selecting the best one using the reward model.</p>
<p>Bag of Tricks</p>
<p>Our goal is to investigate how previously overlooked tricks can critically affect the performance of inference-time computation methods, which typically consist of two main steps: generating candidate solutions (e.g., prompt type, temperature, top-p, etc.) and selecting the optimal solution based on specific reward signals (e.g., self-evaluation, reward type, reward process).In our default setup, we primarily adopt the Best-of-N inference-time computation with the number of candidates N = 32, the temperature τ = 0.7, and top-p set to 0.9.Additionally, the instruction prompt type is set to Chain-of-Thought (CoT).Without further modifications, we conduct ablation studies, varying only the specific tricks under investigation.We focus primarily on complex reasoning tasks, including math problems, and code generation tasks, etc, while additional tasks are detailed in the Appendix B. All additional details regarding the experimental implementation settings can be found in the Appendix A.</p>
<p>Note that our empirical observations and conclusions may not generalize to all datasets and models.However, we emphasize the necessity of using consistent implementation details to ensure fair comparisons among different inferencetime computation methods.</p>
<p>GENERATING CANDIDATE SOLUTIONS</p>
<p>Generating candidate solutions is a critical step in inferencetime computation for LLM reasoning, but the inherent randomness in this process significantly influences diversity.Hyperparameters such as temperature and top-p, along with strategies like instruction prompts, play a vital role in shaping and guiding the solution trajectory.For example, temperature, as a sampling strategy in token generation, increases diversity at higher values.2, the results illustrate that different prompts have a significant effect on LLM reasoning performance.Specifically, we observe that while CoT prompts generally improve reasoning accuracy, Reflection-based CoT prompts show more mixed results across the datasets.These findings are consistent with the observations in (Huang et al., 2023), where self-correction mechanisms failed to show a consistent improvement, with the outcomes varying across different tasks.</p>
<p>Temperature.Temperature (Hinton, 2015) τ regulates the diversity of candidate solutions in LLMs.A higher τ decreases prediction confidence but increases output variability.We revisit the previous inference-time computation settings in Table 1, where the temperature is set differently for each case.Figure 3 illustrates its effect.In most reasoning scenarios, the LLM's performance is optimized at τ = 0.8, yielding an improvement of approximately 2.32% to 4.83% across four datasets.In most cases, both larger and smaller values of τ result in decreased performance.Temperature varies from 0.6 to 1.0, highlighting its impact on LLM reasoning, with performance stabilizing around the recommended default of 0.8.</p>
<p>Top</p>
<p>by modifying the effective vocabulary size M , considering only those tokens whose cumulative probability exceeds a predefined threshold.In general, top-p strikes a balance between diversity and quality in the model's generated output.</p>
<p>As p decreases, the model becomes increasingly constrained to a smaller set of high-probability tokens, leading to more focused and deterministic outputs.Conversely, higher values of p allow for a broader selection of tokens, which increases diversity but may also result in less coherent outputs.Figure 4 demonstrates that the impact of top-p on inference-time computation in LLM reasoning is significant, with an optimal value of top-p = 0.9 , resulting in an overall improvement of 2.32%-5.88%.</p>
<p>SELECTING OPTIMAL SOLUTIONS</p>
<p>Selecting optimal solutions is a critical step in the inferencetime computation of LLM reasoning.This process typically involves either selection by the inference model itself (e.g., voting or prompt-based selection) or the use of external reward models (e.g., RLHF, Proof-Critical, or process reward models).A key question is whether LLMs can effectively evaluate their own solutions.However, self-evaluation methods often fall short, as LLMs struggle to correct errors without external guidance.Moreover, reward models frequently fail to distinguish truly correct answers from superficially correct ones, leading to inflated performance evaluations.This challenge underscores the need for more reliable evaluation mechanisms.To address these gaps, we study the selection process, focusing on self-evaluation, reward types, and investigating generalization of improved reward models.</p>
<p>Self-Evaluation.Previous studies (Zhang et al., 2024b;Yao et al., 2024) have explored self-evaluation methods for se-  Top-p varies from 0.6 to 1.0, highlighting its impact on LLM reasoning, with performance stabilizing around the recommended default of 0.9.</p>
<p>lecting the optimal candidate solution in the inference phase, where the model assesses its own generated solutions using fuzzy judgments (e.g., categorizing solutions as "impossible," "likely," or "sure" to solve a problem).These fuzzy evaluations are then translated into probabilities, such as mapping "impossible" to 0.01 and "sure" to 1.We examine the effectiveness of self-evaluation approaches, including self-process evaluation and self-result evaluation, in comparison to random selection and majority voting.Figure 5 shows that self-evaluation does not consistently improve performance; in fact, in some worst-case scenarios, it performs worse than random selection.</p>
<p>Reward Type.Recently developed reward models (e.g., reward models, critic models, process reward models) have become key tools in enhancing the reasoning capabilities of LLMs during the inference-time phase (Zheng et al., 2024;Zhang et al., 2025).We investigate various reward types, including RLHF Reward (Cai et al., 2024), Proof-Critical Reward, LLM-as-Judge, and majority voting.Figure 6 illustrates the significant impact of different reward models on inference-time computation performance.Specifically, for knowledge-based reasoning, certain reward models can substantially improve performance.In contrast, for more complex reasoning tasks such as mathematics and code generation, the LLM-as-Judge process reward seems to provide a greater performance boost.</p>
<p>Performance Fluctuation with Reward Model.Scaling test-time computation with reward models presents complexities.While one might expect LLM reasoning performance to improve steadily as the reward model is optimized, this is not always the case.Figure 7 reports that the reward model does not perform consistently across all cases dur-  and reward model can be effective.For example, as shown in Table 3, using Reflect COT with a temperature of 0.8 and top-p of 0.9 does not result in an improvement for the Bamboogle and MATH tasks.These results suggest that careful selection and combination of techniques can enhance performance, though the impact may vary across different models.</p>
<p>Takeaways.</p>
<p>(1) Instruction prompts significantly influence LLM reasoning, with self-correction yielding mixed results compared to CoT prompting; (2) A recommended temperature of τ = 0.8 balances diversity and confidence, optimizing reasoning performance, with Top-p performing best at 0.9; (3) LLMs are not effective at self-evaluation; (4) Reward models enhance inference-time computation, with process-based rewards excelling in complex tasks like mathematics and code generation;</p>
<p>(5) Reward models can inflate performance evaluations due to generalization issues, leading to inconsistent task effectiveness.</p>
<p>Proof-Critical</p>
<p>Benchmarking of Inference-time computation of LLMs reasoning under Computation Budgets</p>
<p>We further examine various inference-time computation methods, including Best-of-N, Step-level Best-of-N, Beam Search, MCTS, Self-Consistency, and Self-Refine, under a fixed computation budget.</p>
<p>Setup.We evaluate six inference-time computation methods under a fixed computation budget (equal-token results to standardize computational consumption across tasks) on Qwen-2.5-7B and LLaMA-3.3-8B.To ensure fairness, consistent settings are applied to all methods.For knowledgebased reasoning tasks (e.g., Bamboogle, HotpotQA, Fever), we use the RLHF reward model.For complex reasoning tasks (e.g., MATH, code generation), the QwQ-32B model provides the process reward.Implementation details are available in Appendix A. like MATH and GSM-HARD, Qwen-2.5-7Boutperforms Llama-3.1-8B.</p>
<p>(2) Adding more completion tokens does not always improve accuracy.While Beam Search may leverage higher token counts, the additional tokens often fail to yield proportional accuracy gains.</p>
<p>(3) The Self-Refine method generally underperforms compared to Consistency, suggesting that overly restrictive processes may hinder optimal outputs for complex reasoning tasks.</p>
<p>Figure 15 illustrates how performance varies with increased computational consumption.As token usage rises, Self-Consistency and Self-Refine achieve higher accuracy than other methods, while Step-level Best-of-N, and MCTS show slower improvements.In contrast, Beam Search exhibits minimal gains, indicating lower token efficiency.Notably, the GSM8K task demonstrates a sharper accuracy increase with higher token consumption compared to MATH500, underscoring the task-specific nature of each method's performance.Additionally, we observe some performance inflation with increased computational consumption, attributed to the generalization limitations of the reward model, which does not perform well on this task.</p>
<p>Conclusion</p>
<p>In this paper, we investigate the role of inference-time computation in enhancing the reasoning capabilities of LLMs.Our extensive experimental evaluation reveals that seemingly simple yet overlooked tricks-such as sampling strategies and reward mechanisms-can yield substantial improvements in reasoning performance.The results demonstrate that careful tuning of inference parameters, such as temperature settings, top-k sampling, reward models, plays a crucial role in optimizing the performance of LLMs during inference time.</p>
<p>Impact Statement</p>
<p>This study explores the optimization of inference-time computation strategies to enhance the reasoning abilities of LLMs.By addressing the limitations of existing methods, our findings contribute to improving LLM performance across diverse reasoning tasks, including logical reasoning, code generation, and fact verification.Our work establishes a comprehensive benchmark and demonstrates that previously overlooked techniques-such as temperature and top-p sampling adjustments-can significantly enhance reasoning accuracy.These advancements provide a critical foundation for future research and practical applications, particularly in resource-constrained settings.Optimizing LLMs at inference time without extensive retraining opens new possibilities for improving AI systems' efficiency and reliability in real-world use cases.</p>
<p>A. Experiments Setup</p>
<p>In this section, we provide a more detailed explanation of the experimental setup.Our objective is to evaluate the impact of previously overlooked techniques on the performance of inference-time computation methods.These methods typically consist of two primary steps: (1) generating candidate solutions using specified parameters (e.g., prompt type, temperature, and top-p), and (2) selecting the optimal solution based on predefined reward signals (e.g., self-evaluation strategies, reward types, and processes).</p>
<p>Default Configuration.In our default setup, we employ the Best-of-N inference-time computation, where: The number of candidate solutions N is set to 32.The temperature (τ ) is set to 0.7, which introduces moderate stochasticity during candidate generation.The nucleus sampling parameter, top-p, is configured at 0.9, ensuring diversity by sampling from the top 90% cumulative probability distribution of the predicted tokens.We utilize a Chain-of-Thought (CoT) instruction prompt type to facilitate step-by-step reasoning for more complex tasks.Unless explicitly modified, all experiments adhere to this baseline configuration.</p>
<p>Reward Model.Specifically, for process and result rewards, we employ a prompt-driven approach to guide QwQ-32B, a reasoning model, in evaluating candidate solutions.The prompts used are the process evaluation prompt and the result evaluation prompt, which refer to the evaluation of step-level solutions and the final results, respectively.For RLHF and proof-critical rewards, which are numerical reward models, scores are directly assigned to candidate solutions.RLHF Reward: Based on InternLM2-Chat-1.8B-SFT(Cai et al., 2024), trained on over 2.4 million preference samples.It balances performance, helpfulness, and alignment.Proof-Critical Reward Derived from the InternLM2-5-Step-Prover-Critic model (Wu et al., 2024b), which excels in multiple benchmarks.</p>
<p>Process-Evaluation.</p>
<p>Evaluate whether the language model can decompose the question into relevant sub-questions and assess whether this decomposition aids in answering the original question, either partially or directly.The evaluation result will be classified as "Sure," "Likely," or "Impossible" based on the effectiveness of the decomposition.Evaluation Process: 1. Relevance of Sub-Questions: Determine if the sub-questions are directly related to solving the original question.2. Effectiveness of Decomposition: Assess whether the sub-questions, when answered, lead to a comprehensive response to the original question.Evaluation Outcomes: Sure: The model successfully decomposes the question into relevant sub-questions, each structured to contribute to an accurate final answer.Likely: The model decomposes the question into relevant subquestions, but minor improvements in structure or relevance may enhance the response.Impossible: The model fails to decompose the question effectively or the sub-questions are not relevant to the original question.</p>
<p>Result-Evaluation.</p>
<p>The Result-Evaluation prompt evaluates the final outcome of the language model's reasoning process based on the accuracy, clarity, and completeness of its answer.Each evaluation is categorized as "Sure," "Likely," or "Impossible."The result is judged by verifying whether the language model's final answer is both directly relevant and valid in response to the question posed.The process involves reviewing whether the model's conclusion is definitive and supported by logical reasoning.If the answer is clear and unambiguous, it is marked as "Sure."If there are minor ambiguities, it is categorized as "Likely."If the answer is incorrect or irrelevant, it is deemed "Impossible."</p>
<p>Tasks.Our research focuses on the following reasoning tasks: (1) Arithmetic Reasoning: GSM8K: A dataset with grade school math word problems requiring 2 to 8 calculation steps using basic arithmetic operations.GSM-Hard: A harder variant of GSM8K with larger, less common numbers, increasing the challenge of arithmetic reasoning.(2) Complex Mathematical Reasoning: MATH Dataset: A dataset covering advanced topics like algebra, geometry, and calculus.Each problem includes detailed solutions to evaluate both final answers and problem-solving processes.We use the MATH 500 to test the complex mathematical reasoning ability of LLM.(3) Logical Reasoning: ProntoQA: Measures logical deduction and inference, requiring the application of logical principles to reach correct conclusions.(4) Code Generation: HumanEval: A benchmark for generating functional code snippets, with programming problems that include prompts and test cases to verify correctness.(5) Question Answering: Bamboogle: Evaluates diverse question-answering performance across various topics, testing comprehension and accurate responses.(6) Fact Verification: FEVER: Assesses the ability to verify factual claims using a document corpus, promoting fact-checking system development.( 7  Reasoning: HotpotQA: Features multi-hop questions requiring reasoning across multiple facts, testing common sense knowledge and the ability to link disparate information.</p>
<p>Instruction Prompt Type.Different instruction prompts can guide an LLM to generate distinct reasoning paths.Specifically, Input-Output (IO) prompts directly provide the answer, whereas Chain-of-Thought (CoT) prompts encourage the LLM to reason step by step.Recent research (Huang et al., 2023) suggests that self-correction or self-reflection mechanisms are often ineffective when LLMs operate without external feedback under certain prompt types.We further explores the impact of various prompt types, including IO prompts, standard Chain-of-Thought (CoT) prompts, and reflection-based CoT.</p>
<p>Input-Output (IO) Prompt.</p>
<p>The Input-Output (IO) Prompt directly answers a given question without intermediate reasoning steps.It is designed to generate concise and accurate responses, ending with the phrase "so the final answer is:".This prompt structure is particularly suitable for straightforward queries where minimal context or explanation is required.</p>
<p>Chain-of-Thought (CoT) Prompt.</p>
<p>The Chain-of-Thought (CoT) Prompt guides the model to answer questions step-by-step, breaking down the reasoning process into intermediate steps before concluding with the final answer.For instance, when comparing the lifespans of Theodor Haecker and Harry Vaughan Watkins, the CoT prompt explicitly calculates their ages step-by-step before determining the longer-lived individual.This structured approach enhances reasoning transparency and aligns the response with logical steps.</p>
<p>Reflect CoT.</p>
<p>The Reflect Chain-of-Thought (Reflect CoT) prompt introduces a structured reasoning approach where each step in answering a question is followed by a reflection to verify its accuracy and reliability.For example, when comparing the lifespans of Theodor Haecker and Harry Vaughan Watkins, the process involves step-by-step reasoning to establish each individual's age at death.After each step, a "Reflection" line is used to ensure the validity of the information, such as verifying directly provided ages or confirming the consistency of the comparison.The final conclusion, supported by reflections, ensures a reliable and transparent reasoning process.</p>
<p>B. Other Experiments</p>
<p>Instruction Prompt Type.The experimental results on additional datasets reveal a consistent finding: the type of instruction prompt significantly influences the inference-time computation of LLM reasoning, as illustrated in Figure 9.</p>
<p>Tempature.The experimental results on additional datasets reveal a consistent finding: the type of instruction prompt significantly influences the inference-time computation of LLM reasoning, as illustrated in Figure 10.</p>
<p>Top-p.We further present the ablation study on top-p applied to other reasoning tasks.The experimental results are shown in Figure 11.The impact of top-p is significant; generally, as top-p increases, the LLM's reasoning performance also improves, with optimal performance observed at 0.9 for most reasoning tasks.</p>
<p>Mistral-7B</p>
<p>Qwen-7B Llama-8B Llama-70B Qwen-72B Self-Evaluation.We further present the self-evaluation conducted on additional datasets.Figure 12 illustrates the experimental results, which reveal that the LLM still cannot effectively distinguish the correct solution.</p>
<p>Reward Type. Figure 13 further reports the impact of reward types on other datasets, confirming the same findings.Specifically, for knowledge-based reasoning, certain reward models can substantially improve performance.In contrast, for more complex reasoning tasks, such as PrOntoQA, the LLM-as-Judge process reward provides a more significant performance boost.</p>
<p>Performance Inflation with Reward Model.Scaling test-time computation with reward models presents complexities.Figure 14 reports that the reward model does not perform consistently across all cases during scaling on other datasets.For example, in the challenging MATH task, the proof-critical reward model can lead to a decrease in performance rather than a progressive improvement.This inflation of LLM reasoning performance can be attributed to the generalization issues of the reward model, as reported in (Zheng et al., 2024;Zhang et al., 2025).Currently, the reward model does not perform well across all tasks.</p>
<p>Combination of Tricks.Table 4 further investigates the combination of selected useful techniques, including prompt type, temperature, top-p, and the reward model on Qwen2.5-7B.As demonstrated in Table 4, the improvements are not always additive when combining different techniques, although methods such as prompt type, temperature, and reward models can be individually effective.For example, as shown in Tables 4 using Reflect CoT with a temperature of 0.8 and a top-p of 0.9 does not lead to improvements for the Bamboogle and MATH tasks.These results suggest that the careful selection and combination of techniques can enhance performance, though the impact may vary across different models and tasks.</p>
<p>Table 4: Combination of Inference-Time Tricks on Qwen2.5-7B.We highlight optimal trick combinations.Main experiments of Inference-time computation methods.Figure 15 illustrates how performance varies with increased computational consumption.As token usage rises, Self-Consistency and Self-Refine achieve higher accuracy than other methods, while Step-level Best-of-N, and MCTS show slower improvements.In contrast, Beam Search exhibits minimal gains, indicating lower token efficiency.Notably, the GSM8K task demonstrates a sharper accuracy increase with higher token consumption compared to MATH500, underscoring the task-specific nature of each method's performance.Additionally, we observe some performance inflation with increased computational consumption, attributed to the generalization limitations of the reward model, which does not perform well on this task.</p>
<p>Majority</p>
<p>Proof-Critical</p>
<p>Best-of-N Self-Consistency</p>
<p>Step-level Best-of-</p>
<p>Figure 1 :
1
Figure 1: Overview of Decoding Inference-Time Computation for LLM Reasoning.(A) Instruction Prompt: Includes IO, Chain-of-Thought (CoT), and reflection-based CoT prompts.(B) Reasoning Task: Evaluates models on eight datasets: Arithmetic (GSM8K, GSM-Hard), Complex Math (MATH), Logical (PrOntoQA), Code Generation (HumanEval), Question Answering (Bamboogle), Fact Verification (FEVER), and Common Sense (HotpotQA).(C) Inference Model: Analyzes LLMs (LLaMA, Qwen, Mistral) of varying sizes and architectures, with performance assessed via temperature and top-p hyperparameters.(D) Reward Model: Explores reward types like RLHF, critic, and process-based models to enhance inference performance.(E) Inference-Time Computation: Investigates methods like Best-of-N Sampling, Step-Level Best-of-N, Self-Consistency, Monte Carlo Tree Search (MCTS), and Self-Refinement to optimize reasoning.</p>
<p>Figure 3 :
3
Figure 3: Accuracy (%) versus temperature during inference.Temperature varies from 0.6 to 1.0, highlighting its impact on LLM reasoning, with performance stabilizing around the recommended default of 0.8.</p>
<p>Figure 4 :
4
Figure4: Accuracy (%) versus Top-p values during inference.Top-p varies from 0.6 to 1.0, highlighting its impact on LLM reasoning, with performance stabilizing around the recommended default of 0.9.</p>
<p>Figure 6 :
6
Figure6: Comparison of different reward models across benchmarks showcasing their impact on accuracy and pass@1 for multiple LLMs.</p>
<p>Figure 7 :
7
Figure 7: Scaling test-time performance with reward models (Proof-Critical, RLHF, LLM-as-Judge) across benchmarks.</p>
<p>Figure 9 :
9
Figure 9: Comparative accuracy of different models(Mistral7B, Qwen7B, Llama88, Llama70B, and Qwen72B) across four datasets (Ferver, GSM-Hard, HotpotQA, and PrOntoQA) using three instruction prompts: Input-Output (IO), Chain-of-Thought (CoT), and Reflect Chain-of-Thought (Reflect CoT).</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Accuracy vs. temperature for different models(Mistral7B, Qwen7B, Llama88, Llama70B, and Qwen72B) across four datasets (Ferver, GSM-Hard, HotpotQA, and PrOntoQA), with temperature settings ranging from 0.6 to 1.0.</p>
<p>Figure 12 :Figure 13 :
1213
Figure12: Accuracy (%) across four benchmark tasks with different evaluation strategies The results shows that Selfevaluation often fails to assess solution quality effectively.MajorityProof-Critical Reward RLHF Reward LLM-as-Judge</p>
<p>Figure 14 :
14
Figure14: Scaling test-time performance with reward models (Proof-Critical, RLHF, LLM-as-Judge) across benchmarks, highlighting inconsistent trends and generalization issues, with notable declines in challenging tasks like MATH500 On Qwen-2.5-7B.</p>
<p>Figure 15 :
15
Figure15: Performance versus token consumption across benchmarks for decoding strategies (Self-Consistency, Self-Refine, Best-of-N, Greedy, Beam Search, MCTS), highlighting task-specific trends and token efficiency disparities, with GSM8K showing sharp accuracy gains and MCTS lagging in improvement</p>
<p>Table 2 :
2
LLM Reasoning Performance under Inference-Time Computation with Fixed Token Budget.The table shows accuracy and token consumption across various reasoning tasks for Llama-3.1-8B and Qwen-2.5-7Bmodels.We further investigate the combination of selected useful techniques, including prompt type, temperature, top-p, and the reward model.As demonstrated in Table4, the improvements are not always additive when combining different techniques, although methods such as prompt, temperature,
Knowledge-based ReasoningComplex ReasoningMethodAccuracy (%) / #TokensPass@1 / #TokensAccuracy (%) / #TokensBamboogle HotPotQAFeverHumanEvalGSM8KGSM-HARDMATHPrOntoQALlama-3.1-8BBest-of-N48.4 / 107748.6 / 87661.1 / 119160.4 / 124483.1 / 97633.6 / 117444.6 / 141294.0 / 969Step-Level Best-of-N52.4 / 111945.7 / 63260.3 / 58155.5 / 92183.0 / 84532.9 / 101339.8 / 70893.8 / 881Beam Search33.1 / 1801 31.8 / 209157.3 / 91750.6 / 101978.7 / 106625.1 / 132843.2 / 1014 89.6 / 1241MCTS49.2 / 89641.3 / 95659.9 / 84348.7 / 111481.3 / 58231.6 / 62731.0 / 1898 93.0 / 1511Self-Consistency54.8 / 107748.3 / 87661.9 / 119160.3 / 126184.3 / 97633.2 / 117445.0 / 141293.4 / 969Self-Refine41.9 / 28740.5 / 28019.1 / 26541.4 / 49564.5 / 41025.1 / 56437.8 / 74362.2 / 525Qwen-2.5-7BBest-of-N49.2 / 88144.6 / 118955.6 / 83078.0 / 110290.5 / 146250.1 / 117461.2 / 1827 96.2 / 1271Step-Level Best-of-N51.6 / 86343.8 / 105550.2 / 99573.1 / 69189.7 / 70950.7 / 87763.0 / 116292.8 / 625Beam Search29.0 / 1552 44.3 / 128055.7 / 99775.6 / 101988.3 / 134150.2 / 141749.2 / 83690.2 / 1089MCTS47.6 / 1467 42.6 / 2585 53.9 / 144269.8 / 72466.2 / 141648.7 / 111426.2/ 180570.6 / 2582Self-Consistency49.2 / 88144.5 / 118956.7 / 83079.9 / 127190.9 / 146250.0 / 117470.2 / 1878 94.8 / 1271Self-Refine48.4 / 23242.3 / 31616.8 / 63941.4 / 49686.1 / 60143.4 / 98255.8 / 94758.4 / 1105MajorityRandomSelf-Process-EvaluationSelf-Result-EvaluationMajorityProof-Critical RewardRLHF RewardLLM-as-JudgeM is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B GSM8K M is t r a l7 B 40% 60% 80% 100% Accuracy (N=32) Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B 40% 60% 80% Pass@1 (N=32) HumanEval0% 20% 40% 60% 80% Accuracy (N=32) 40% 50% 60% 70% Accuracy (N=32)M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B MATH500 M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B BamboogleM is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B GSM8K M is t r a l7 B 30% 40% 50% 60% 70% 80% 90% 100% Accuracy (N=32) Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B 30% 40% 50% 60% 70% 80% 90% Pass@1 (N=32) HumanEval0% 20% 40% 60% 80% Accuracy (N=32) 40% 50% 60% 70% Accuracy (N=32)M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B MATH500 Q w e n 7 2 B M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B BamboogleFigure 5: Accuracy (%) across four benchmark tasks withdifferent evaluation strategies. The results shows that Self-evaluation often fails to assess solution quality effectively.ing scaling. For example, in the challenging MATH task,the Proof-Critical Reward model can lead to a decrease inperformance rather than a progressive improvement. Thisfluctuation of LLM reasoning performance can be attributedto the generalization issues of the reward model, as reportedin (Zheng et al., 2024; Zhang et al., 2025). Currently, thereward model does not perform well across all tasks.4.2.3. COMBINATION OF INFERENCE-TIMECOMPUTATION TRICKS.</p>
<p>Table 3 :
3
Combination of Inference-Time Tricks onLlama3.1-8b.We highlight optimal trick combinations.
PromptRewardTemp Top-p Bamboogle MATHIOMajority0.70.942.711.8CoTMajority0.70.952.458.4CoTMajority0.80.954.857.0CoTMajority0.60.953.258.0CoTMajority0.71.054.055.0CoTMajority0.70.856.457.8CoTRandom0.70.944.440.6CoTSelf-Process-Evaluation0.70.953.240.6CoTSelf-Result-Evaluation0.70.945.139.2Reflect CoTMajority0.70.956.458.6Reflect CoTMajority0.80.955.657.0</p>
<p>Table 2
2Best-of-N Self-ConsistencyStep-level Best-of-N Self-RefineBeam Search MCTS60% 70% 80% 90% 100% Accuracy 30% 40% 50% 60% 70% Accuracy 80% 90%1000 # Tokens 2000 GSM8K 10 3 # Tokens HumanEval3000 10 440% 50% 60% 80% 70% Accuracy 30% 20% 10 2 20% 30% 40% 50% Accuracy 60%2000 MATH500 4000 6000 # Tokens 10 3 # Tokens BamboogleFigure 8: Performance versus token consumption acrossbenchmarks for inference-time computation strategies.
summarizes the results of six inference-time computation methods across eight reasoning tasks.Key observations include: (1) Performance varies significantly across tasks.Best-of-N and Self-Consistency perform well in knowledge-based reasoning tasks, but for complex tasks</p>
<p>)</p>
<p>Common Sense
IOCoTReflect CoT50% 55% 60% 65% 70% Accuracy (N=32)M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B Ferver20% 30% 40% 50% 60% Accuracy (N=32)M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B GSM-Hard20% 30% 40% 50% 60% 70% Accuracy (N=32)M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B HotpotQA60% 80% 100% Accuracy (N=32)M is t r a l7 B Q w e n 7 B L la m a 8 B L la m a 7 0 B Q w e n 7 2 B PrOntoQA</p>
<p>N Self-Refine Beam Search MCTS
60% 65% 70% 75% 80% 85% 90% Accuracy500 1000 1500 2000 2500 # Tokens GSM8K10% 20% 30% 40% 50% 60% Accuracy1000 2000 3000 4000 5000 6000 # Tokens MATH50010% 20% 30% 40% 50% 60% 70% Accuracy10 3 # Tokens HumanEval10 430% 35% 40% 45% 50% Accuracy 55% 60%10 3 # Tokens Bamboogle10 4</p>
<p>A survey of monte carlo tree search methods. C B Browne, E Powley, D Whitehouse, S M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in games. 20124</p>
<p>. Z Cai, M Cao, H Chen, K Chen, K Chen, X Chen, X Chen, Z Chen, Z Chen, P Chu, X Dong, H Duan, Q Fan, Z Fei, Y Gao, J Ge, C Gu, Y Gu, T Gui, A Guo, Q Guo, C He, Y Hu, T Huang, T Jiang, P Jiao, Z Jin, Z Lei, J Li, J Li, L Li, S Li, W Li, Y Li, H Liu, J Liu, J Hong, K Liu, K Liu, X Liu, C Lv, H Lv, K Lv, L Ma, R Ma, Z Ma, W Ning, L Ouyang, J Qiu, Y Qu, F Shang, Y Shao, D Song, Z Song, Z Sui, P Sun, Y Sun, H Tang, B Wang, G Wang, J Wang, J Wang, R Wang, Y Wang, Z Wang, X Wei, Q Weng, F Wu, Y Xiong, C Xu, R Xu, H Yan, Y Yan, X Yang, H Ye, H Ying, J Yu, J Yu, Y Zang, C Zhang, L Zhang, P Zhang, P Zhang, R Zhang, S Zhang, S Zhang, W Zhang, W Zhang, X Zhang, X Zhang, H Zhao, Q Zhao, X Zhao, F Zhou, Z Zhou, J Zhuo, Y Zou, X Qiu, Y Qiao, 2024and Lin, D. Internlm2 technical report</p>
<p>C Chen, Z Liu, C Du, T Pang, Q Liu, A Sinha, P Varakantham, M Lin, arXiv:2406.09760Bootstrapping language models with dpo implicit rewards. 2024aarXiv preprint</p>
<p>Alphamath almost zero: process supervision without process. G Chen, M Liao, C Li, Fan , K , arXiv:2405.035532024barXiv preprint</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, Zaremba , 2021Evaluating large language models trained on code</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021aarXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021barXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>X Feng, Z Wan, M Wen, S M Mcaleer, Y Wen, W Zhang, J Wang, arXiv:2309.17179Alphazero-like tree-search can guide large language model decoding and training. 2023aarXiv preprint</p>
<p>X Feng, Z Wan, M Wen, S M Mcaleer, Y Wen, W Zhang, J Wang, arXiv:2309.17179Alphazero-like tree-search can guide large language model decoding and training. 2023barXiv preprint</p>
<p>L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, arXiv:2211.10435Program-aided language models. 2022arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Distilling the knowledge in a neural network. G Hinton, arXiv:1503.025312015arXiv preprint</p>
<p>A Holtzman, J Buys, L Du, M Forbes, Y Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, H S Zheng, A W Yu, X Song, D Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>A Jiang, A Sablayrolles, A Mensch, C Bamford, D Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023. 2023arXiv preprint</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>A comprehensive jailbreak judge benchmark with multi-agent enhanced explanation evaluation framework. F Liu, Y Feng, Z Xu, L Su, X Ma, D Yin, H Liu, Jailjudge, 2024a</p>
<p>Adversarial tuning: Defending against jailbreak attacks for llms. F Liu, Z Xu, H Liu, arXiv:2406.066222024barXiv preprint</p>
<p>Making ppo even better: Valueguided monte-carlo tree search decoding. J Liu, A Cohen, R Pasunuru, Y Choi, H Hajishirzi, A Celikyilmaz, arXiv:2309.150282023arXiv preprint</p>
<p>Q Liu, X Zheng, N Muennighoff, G Zeng, L Dou, T Pang, J Jiang, M Lin, Regmix, arXiv:2407.01492Data mixture as regression for language model pre-training. 2024carXiv preprint</p>
<p>Self-refine: Iterative refinement with selffeedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>S R Motwani, C Smith, R J Das, M Rybchuk, P H Torr, I Laptev, F Pizzati, R Clark, C S Witt, Malt, arXiv:2412.01928Improving reasoning with multi-agent llm training. 2024arXiv preprint</p>
<p>Analyzing uncertainty in neural machine translation. M Ott, M Auli, D Grangier, M Ranzato, International Conference on Machine Learning. PMLR2018</p>
<p>O Press, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, arXiv:2210.03350Measuring and narrowing the compositionality gap in language models. 2022arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Scaling llm testtime compute optimally can be more effective than scaling model parameters. C Snell, J Lee, K Xu, A Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. J Thorne, A Vlachos, C Christodoulopoulos, A Mittal, NAACL-HLT. 2018</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Y Tian, B Peng, L Song, L Jin, D Yu, H Mi, D Yu, arXiv:2404.122532024arXiv preprint</p>
<p>C Wang, Y Deng, Z Lyu, L Zeng, J He, S Yan, An , B , arXiv:2406.14283Improving multi-step reasoning for llms with deliberative planning. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts. J Wu, M Feng, S Zhang, F Che, Z Wen, J Tao, arXiv:2411.184782024aarXiv preprint</p>
<p>5-stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems. Z Wu, S Huang, Z Zhou, H Ying, J Wang, D Lin, K Chen, Internlm2, 2024b</p>
<p>Enhancing llm reasoning via critique models with test-time and trainingtime supervision. Z Xi, D Yang, J Huang, J Tang, G Li, Y Ding, W He, B Hong, S Do, W Zhan, arXiv:2411.165792024arXiv preprint</p>
<p>. A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, arXiv:2412.151152024arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W W Cohen, R Salakhutdinov, C D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>L Zhang, A Hosseini, H Bansal, M Kazemi, A Kumar, R Agarwal, arXiv:2408.15240Generative verifiers: Reward modeling as next-token prediction. 2024aarXiv preprint</p>
<p>Chain of preference optimization: Improving chain-of-thought reasoning in llms. X Zhang, C Du, T Pang, Q Liu, W Gao, M Lin, arXiv:2406.091362024barXiv preprint</p>
<p>The lessons of developing process reward models in mathematical reasoning. Z Zhang, C Zheng, Y Wu, B Zhang, R Lin, B Yu, D Liu, J Zhou, J Lin, arXiv:2501.073012025arXiv preprint</p>
<p>C Zheng, Z Zhang, B Zhang, R Lin, K Lu, B Yu, D Liu, J Zhou, J Lin, arXiv:2412.06559ProcessBench: identifying process errors in mathematical reasoning. essBench: identifying process errors in mathematical reasoning2024arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
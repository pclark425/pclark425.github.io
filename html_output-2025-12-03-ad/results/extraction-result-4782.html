<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4782 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4782</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4782</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-cd73c6870a28d13f553356c61c877b6ee684b5b4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cd73c6870a28d13f553356c61c877b6ee684b5b4" target="_blank">AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work suggests that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems.</p>
                <p><strong>Paper Abstract:</strong> With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4782.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4782.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentSims Memory System</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentSims Generative Agent Memory System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based episodic memory subsystem in the AgentSims sandbox that encodes agents' daily experiences into embeddings stored in a vector database and retrieves relevant memories to improve behavior consistency and long-term coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AgentSims Generative Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven generative agents implemented in the AgentSims platform; agents are backed by an abstracted LLM caller and augmented with pluggable support systems (Planning System, Memory System, Tool-Use System) to produce coherent, long-term behavior in simulated social-economic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>vector-database episodic memory / retrieval-augmented memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Agents' daily memories are encoded into embeddings and stored in a vector database. At decision points (e.g., conversing with familiar agents or checking task progress), the system retrieves relevant memories to inform prompts and planning; completed planning steps are recorded back into memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulated social and organizational tasks (social adaptation, long-term planning as mayor, interactive sandbox tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Task-based evaluations inside an artificial town sandbox where agents must accomplish goals that require long-term coherence, theory-of-mind, planning and social adaptation (examples: subject LLM as participant in hostile social environment; subject LLM as mayor coordinating town resources and policies).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper argues that memory (implemented as a vector DB of embeddings) is necessary for human-like long-term coherence and consistency and that support systems (memory, planning, tool-use) materially affect agent behavior; no quantitative or ablation results comparing with/without memory are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors note practical challenges: full memory streams exceed LLM context windows and are expensive to include verbatim; storing and retrieving memories requires external infrastructure (vector DB); the sandbox's fidelity is limited by LLM accuracy and environment diversity; task pass rates do not explain causes of success/failure.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Use a retrieval-augmented memory (embeddings + vector DB) to give agents access to long-term episodic context; integrate memory with a planning system to decompose goals and record progress; provide memory as a pluggable module so researchers can swap implementations and study its impact in task-based evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentSims: An Open-Source Sandbox for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4782.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4782.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents (Park et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) demonstrating that LLM-driven agents augmented with external memory and support mechanisms can generate believable, long-term coherent behaviors in interactive simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced generative-agent architecture that uses external memory to achieve long-term coherence and believable human-like behavior in simulations; served as inspiration / prior art for AgentSims' support systems.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external episodic memory (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Referenced as employing auxiliary memory beyond prompt context to store and retrieve agent experiences to maintain long-term behavior consistency (details are cited from the original Park et al. work rather than reimplemented here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Social simulacra / interactive behavior simulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Populated prototype simulations to test believable multi-day behaviors and social interactions, requiring memory of past events and relationships for coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as evidence that vanilla LLM prompting is insufficient for long-term coherence and that a memory subsystem improves simulated agent realism; AgentSims adopts a similar memory+planning+tool architecture but reports no direct quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No experimental details provided in this paper beyond citation; limitations are those attributed generally (need for external memory to overcome context-window limits).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Serves as a motivating prior example that external memory is important for long-range coherence in agent simulations and justifies AgentSims' inclusion of a vector-store memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentSims: An Open-Source Sandbox for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4782.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4782.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager (Wang et al. 2023a) - referenced</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that uses LLMs in embodied agent settings and is cited in AgentSims as part of the literature motivating external support mechanisms (e.g., memory, tool-use) for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager (Wang et al., 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An embodied agent system employing LLMs and auxiliary mechanisms (referenced) to support open-ended task learning and interaction; cited as related work motivating modular support systems in AgentSims.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>referenced memory/tool-use mechanisms (external / episodic) - not reimplemented here</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Mentioned as prior art that highlights the need for support mechanisms (including memory) when LLMs act in extended interactive environments; AgentSims adopts similar modular ideas (memory + planning + tool-use).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-ended embodied agent tasks (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Embodied open-ended tasks where an agent interacts with an environment and must learn/remember skills and world state over time; cited for motivating AgentSims' tool-use and memory components.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited to support the claim that auxiliary systems (like memory) are necessary for extended, coherent agent behavior; AgentSims does not report empirical comparisons with Voyager within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No quantitative data in AgentSims about Voyager-style memory effects; challenges discussed generally include context-window limits and infrastructure needs for memory storage/retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Reinforces the paper's recommendation to modularize memory and tool-use so researchers can study their effects in task-based evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentSims: An Open-Source Sandbox for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Training socially aligned language models in simulated human society <em>(Rating: 1)</em></li>
                <li>Dera: Enhancing large language model completions with dialog-enabled resolving agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4782",
    "paper_id": "paper-cd73c6870a28d13f553356c61c877b6ee684b5b4",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "AgentSims Memory System",
            "name_full": "AgentSims Generative Agent Memory System",
            "brief_description": "A retrieval-based episodic memory subsystem in the AgentSims sandbox that encodes agents' daily experiences into embeddings stored in a vector database and retrieves relevant memories to improve behavior consistency and long-term coherence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AgentSims Generative Agent",
            "agent_description": "LLM-driven generative agents implemented in the AgentSims platform; agents are backed by an abstracted LLM caller and augmented with pluggable support systems (Planning System, Memory System, Tool-Use System) to produce coherent, long-term behavior in simulated social-economic tasks.",
            "memory_type": "vector-database episodic memory / retrieval-augmented memory",
            "memory_description": "Agents' daily memories are encoded into embeddings and stored in a vector database. At decision points (e.g., conversing with familiar agents or checking task progress), the system retrieves relevant memories to inform prompts and planning; completed planning steps are recorded back into memory.",
            "task_name": "Simulated social and organizational tasks (social adaptation, long-term planning as mayor, interactive sandbox tasks)",
            "task_description": "Task-based evaluations inside an artificial town sandbox where agents must accomplish goals that require long-term coherence, theory-of-mind, planning and social adaptation (examples: subject LLM as participant in hostile social environment; subject LLM as mayor coordinating town resources and policies).",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The paper argues that memory (implemented as a vector DB of embeddings) is necessary for human-like long-term coherence and consistency and that support systems (memory, planning, tool-use) materially affect agent behavior; no quantitative or ablation results comparing with/without memory are reported.",
            "limitations_or_challenges": "Authors note practical challenges: full memory streams exceed LLM context windows and are expensive to include verbatim; storing and retrieving memories requires external infrastructure (vector DB); the sandbox's fidelity is limited by LLM accuracy and environment diversity; task pass rates do not explain causes of success/failure.",
            "key_insights": "Use a retrieval-augmented memory (embeddings + vector DB) to give agents access to long-term episodic context; integrate memory with a planning system to decompose goals and record progress; provide memory as a pluggable module so researchers can swap implementations and study its impact in task-based evaluations.",
            "uuid": "e4782.0",
            "source_info": {
                "paper_title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Generative Agents (Park et al. 2023)",
            "name_full": "Generative Agents: Interactive Simulacra of Human Behavior",
            "brief_description": "Prior work (cited) demonstrating that LLM-driven agents augmented with external memory and support mechanisms can generate believable, long-term coherent behaviors in interactive simulations.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al., 2023)",
            "agent_description": "Referenced generative-agent architecture that uses external memory to achieve long-term coherence and believable human-like behavior in simulations; served as inspiration / prior art for AgentSims' support systems.",
            "memory_type": "external episodic memory (as referenced)",
            "memory_description": "Referenced as employing auxiliary memory beyond prompt context to store and retrieve agent experiences to maintain long-term behavior consistency (details are cited from the original Park et al. work rather than reimplemented here).",
            "task_name": "Social simulacra / interactive behavior simulation",
            "task_description": "Populated prototype simulations to test believable multi-day behaviors and social interactions, requiring memory of past events and relationships for coherence.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited as evidence that vanilla LLM prompting is insufficient for long-term coherence and that a memory subsystem improves simulated agent realism; AgentSims adopts a similar memory+planning+tool architecture but reports no direct quantitative comparison.",
            "limitations_or_challenges": "No experimental details provided in this paper beyond citation; limitations are those attributed generally (need for external memory to overcome context-window limits).",
            "key_insights": "Serves as a motivating prior example that external memory is important for long-range coherence in agent simulations and justifies AgentSims' inclusion of a vector-store memory module.",
            "uuid": "e4782.1",
            "source_info": {
                "paper_title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Voyager (Wang et al. 2023a) - referenced",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "Referenced work that uses LLMs in embodied agent settings and is cited in AgentSims as part of the literature motivating external support mechanisms (e.g., memory, tool-use) for agents.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager (Wang et al., 2023a)",
            "agent_description": "An embodied agent system employing LLMs and auxiliary mechanisms (referenced) to support open-ended task learning and interaction; cited as related work motivating modular support systems in AgentSims.",
            "memory_type": "referenced memory/tool-use mechanisms (external / episodic) - not reimplemented here",
            "memory_description": "Mentioned as prior art that highlights the need for support mechanisms (including memory) when LLMs act in extended interactive environments; AgentSims adopts similar modular ideas (memory + planning + tool-use).",
            "task_name": "Open-ended embodied agent tasks (referenced)",
            "task_description": "Embodied open-ended tasks where an agent interacts with an environment and must learn/remember skills and world state over time; cited for motivating AgentSims' tool-use and memory components.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited to support the claim that auxiliary systems (like memory) are necessary for extended, coherent agent behavior; AgentSims does not report empirical comparisons with Voyager within this paper.",
            "limitations_or_challenges": "No quantitative data in AgentSims about Voyager-style memory effects; challenges discussed generally include context-window limits and infrastructure needs for memory storage/retrieval.",
            "key_insights": "Reinforces the paper's recommendation to modularize memory and tool-use so researchers can study their effects in task-based evaluations.",
            "uuid": "e4782.2",
            "source_info": {
                "paper_title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Training socially aligned language models in simulated human society",
            "rating": 1
        },
        {
            "paper_title": "Dera: Enhancing large language model completions with dialog-enabled resolving agents",
            "rating": 1
        }
    ],
    "cost": 0.0089555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</h1>
<p>Jiaju Lin ${ }^{1,2}$, Haoran Zhao ${ }^{1,3}$ ï¼ŒAochi Zhang ${ }^{1}$, Yiting Wu ${ }^{1,4}$, Huqiuyue Ping ${ }^{1,5}$, Qin Chen ${ }^{6}$<br>${ }^{1}$ PTA Studio<br>${ }^{2}$ Pennsylvania State University, ${ }^{3}$ Beihang University,<br>${ }^{4}$ Sun Yat-sen University, ${ }^{5}$ Zhejiang University, ${ }^{6}$ East China Normal University<br>${ }^{3}$ zhaohaoran@buaa.edu.cn<br>${ }^{2}$ jjlin.unfake@gmail.com and ${ }^{6}$ qchen@cs.ecnu.edu.cn</p>
<h4>Abstract</h4>
<p>With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that taskbased evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .</p>
<h2>1 Introduction</h2>
<p>LLMs have revolutionized Natural Language Processing (NLP) and beyond. They demonstrate great potential in few-shot learning(Brown et al., 2020), code generation(Nijkamp et al., 2023), reasoning(Yao et al., 2023) and other tasks. Furthermore, LLM powered autonomous agents(Weng, 2023) are widely applied in solving complex problems, like multimodal generation(Shen et al., 2023), software developing(Qian et al., 2023) and social simulating (Park et al., 2023).</p>
<p>Although LLMs have reformed the paradigm of NLP, the problem of evaluation keeps haunting this field. Old benchmarks become out-ofdate. Since LLMs achieve human-level Natural Language Understanding (NLU) and Natural Language Generation (NLG) abilities(OpenAI, 2023). To address the pressing need for novel benchmarks, the NLP community has introduced an array of fresh evaluation tasks and datasets, encompassing a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>diverse spectrum of abilities, including close-book question-answering (QA) based knowledge testing(Hendrycks et al., 2020; Huang et al., 2023), human-centric standardized exams(Zhong et al., 2023), multi-turn dialogue(Lin and Chen, 2023), reasoning(Liu et al., 2023a; bench authors, 2023) and safety assessment(Sun et al., 2023).</p>
<p>However, there are still many problems with these new benchmarks. 1) Evaluated abilities are limited by the task formats. Since a majority of these tasks adopt a single-turn QA format, they are insufficient to comprehensively evaluate various aspects of LLMs' capabilities. For instance, they fail to assess the models' proficiency in adhering to instructions in dialogue or mimicking human-like social interactions. 2) Benchmarks can be easily hacked. Avoiding the leakage of test set is of paramount importance when evaluate a model's ability. Nonetheless, considering the amount of pretrained knowledge of LLM, it has become more and more inevitable to inadvertently mix test cases into the training set.(Gunasekar et al., 2023). 3) For open-ended QA, existing metrics are not objective. Previous metrics for open-ended QA involve automatic metrics, and human-rating as subjective metrics(Zhou et al., 2023). In the LLM era, text segment matching based metrics become out-of-date. To mitigate the high-costly issue of human-rating, today's researchers employ well-aligned LLMs like GPT4 as automatic raters. Nevertheless, the most significant problem of this approach is that it can not evaluate super GPT4-level models, and LLMs are biased toward specific features (Wang et al., 2023b).</p>
<p>Based on these observations, we suggest taskbased evaluation for LLM benchmarks. Specifically, given an artificial social-economic environment, LLM-driven agents should achieve the predefined task goals to prove their abilities, just like humans accomplishing goals in real world or games to show their capacities. Task-based evaluation is</p>
<p>a one-for-all solution for current issues: 1) Taskbased evaluation can test an LLM's overall ability. The complexity of social simulation and adaptation far exceeds simple QA and can formulate more challenging tasks for LLMs. LLM agents need to be equipped with the ability from NLU to Theory of Mind (ToM) (Premack and Woodruff, 1978). 2) Task solving processes are less likely to be hacked. Different from unchanged test datasets whose formats can be easily mimicked and added to training data. Task settings are diversified and the emergent social behaviors and groups are less likely to be described and included in training corpus. 3) Task passing rate is an objective metric. Compared with popular rating methods by ChatGPT, the passing rate does not rely on any black-box rating process, i.e. deep neural networks or human brains, thus it is an objective and fair metric for the comparison between LLMs.</p>
<p>To all-around estimate LLMs' capacities, we hope researchers from all fields take part in the development of evaluation tasks. However, a key obstacle to fostering a collaborative research community is the absence of a standard paradigm, an easy-to-use and extensible research platform. Previous works pursue the most efficient way to implement a sandbox while ignoring the need of non-specialist users. Besides, the poor readability further results in poor extensiblity and user churn. Moreover, the agents' performance varies with different support systems, i.e. memory, planning and tool-use system. We need a standard implementation to ensure the reproducibility of experimental results.</p>
<p>To this end, we introduce AgentSims, an interactive, visualized, and program-based infrastructure for curating evaluation tasks for LLMs. It creates an artificial town with various buildings and residents. The core objective of AgentSims is to streamline the task design process, eliminating hurdles that researchers from various backgrounds and programming proficiencies might encounter.</p>
<ul>
<li>For researchers focusing on LLM, AgentSims is extendable and combinable to allow users to combine different plan, memory and learning systems to study the impacts and effectiveness of various system design.</li>
<li>For experts from other fields like behavioral economics or social psychology, AgentSims provides an interactive UI for map design and agent creation and lower the entry threshold. Such a user-friendly architecture further facilitates the
cooperation between different fields and the future prosperity of the LLM community.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Benchmarks for Large Language Models</h3>
<p>The emergency of ChatGPT and other LLMs requires new benchmarks for effective evaluation. bench authors (2023) is the most accepted benchmark to evaluate LLM's general abilities. It contains more than 200 tasks, covering from childhood development, to social bias. Zhong et al. (2023) collect test tasks from human-centric standardized exams like GRE and SAT. (Hendrycks et al., 2020; Huang et al., 2023) are benchmarks focusing on measuring knowledge acquired in pretraining. They covers subjects across STEM, the humanities, the social sciences. Lin and Chen (2023) build a benchmark for LLMs' multiturn dialogue abilities. Every dialogue is limited to two turns for simplicity. Sun et al. (2023) focus on measure the safety of LLMs. They curate a adversarial attack dataset containing insulting instructions and test whether LLMs can be jailbroke. However, as mentioned above, existing datasets have issues that can not fully demonstrate abilities of LLMs. AgentSims overcomes these difficulties and renders a chance for overall evaluation of LLMs.</p>
<h3>2.2 Multi Agent Cooperation</h3>
<p>With LLMs demonstrate their overwhelming abilities, researchers find that multi LLM agents can generate better results than a single one. Nair et al. (2023) is one of the earliest attempts of multi-agent cooperation. It builds a forum for agents to communicate feedback and iteratively improve their healthcare suggestions. Li et al. (2023) expand the application field of agent cooperation method by role-playing. From programming to domainspecific QA, it surpass single agent baselines. Qian et al. (2023) build a software development company, by meticulously dividing the development process into four distinct stages, leading to efficient resolution of specific subtasks. Liu et al. (2023b) first apply multi-agent simulated society for alignment, where agents in a sandbox learn from social interaction to understand moral rules. (Park et al., 2023) is the most sophisticated application of multi agent sandbox. Authors build support mechanisms to enable agents to produce believable individual and emergent social behaviors. However, none existing methods provide a user-friendly interface</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Front end of AgentSims, showing in a pixel game style. Users can create agents and buildings in the left-side panel and observe agents' behaviors in the main screen. Besides setting-then-observing, users can also play as the mayor and talk with agents to intervene in the experiment.</p>
<p>For unprofessional researchers or build a standard paradigm for agent support system. Nonetheless, current multi-agent systems are task-oriented rather than evaluation-oriented. AgentSims works as a platform for easy benchmark construction.</p>
<h2>3 Key Components</h2>
<p>As shown in Figure 2, key components of AgentSims can be divided into two parts: 1) generative agents driven by LLM support mechanisms. 2) buildings and equipment that consist of the sandbox environment.</p>
<h3>3.1 Generative Agents</h3>
<p>If prompted properly, LLMs can generate believable behaviors (Park et al., 2022). However, to achieve human-like memory performance and long-term coherence, LLM is not enough. We need auxiliary systems to enable agents to perform more naturally. Referring to recent work (Park et al., 2023; Wang et al., 2023a), we abstract these supportive mechanisms into three parts: Planning System, Memory System, and Tool-Use System.</p>
<p><strong>Planning System</strong> LLMs have shown some planning and reasoning capacities. However, faced with complex tasks, vanilla LLMs always fail for lacking long-term arrangement abilities. Hence, we introduce a Planning System to ensure agents' behaviors are coherent and believable. The Planning System reorganizes a goal by decomposing the target, summarizing current condition and generating subtasks. Specifically, it is assembled by a series of pluggable prompt modules, which assess current achievement of ultimate goals by checking the memory system and making decisions for next steps. Once a new step is completed, it would be recorded in the memory system.</p>
<p><strong>Memory System.</strong> Agents capable of emulating human behavior necessitate comprehending a vast array of experiences, beyond what a prompt can contain. The complete memory stream is too expensive to be accommodated in the limited context window, and attempting to do so can overwhelm the model. Thus, we add a memory system for agents' experience retention and retrieval. The system is built upon a vector database for efficient storing and retrieving. Specifically, every agent's daily memory is encoded into embeddings and stored in the database. Every time when agents face some new situation that needs the previous memory, such as chatting with familiar people, the memory system can retrieve the information about their relationship to improve agent behavior consistency.</p>
<p><strong>Tool-Use System.</strong> Ideally, agents continuously explore the simulated world would learn from previous failures and successes, then acquire diverse skills. In our framework, to realize this feature, we present a tool-use system, which endows agents</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of AgentSims architecture</p>
<p>with the ability to accomplish real-world tasks. Particularly, the tool use system stores equipment-operation pairs learning from feedback of using equipment. Once agents select equipment to interact with by planning and memory system, they need to infer an initial operation by the description of the equipment. And the equipment will return an operation result as feedback. If the agent believes the result meets their operation purpose, a new skill would be stored in the Tool-Use System.</p>
<h3>3.2 Buildings and Equipment</h3>
<p>Interactive buildings and equipment are necessities for the diversity of an LLM sandbox. They compose the physical environments of the simulated world. In our framework, a building or location contains equipment like stoves or office desks. Thus, buildings are defined by the equipment they contain and equipment is the basic element composing the interactive environment. More specifically, the equipment can be defined by some definition texts describing its features and support function, which can be either hard-coded by the developer or a language model that supports self-adaptive agent-equipment interaction. When an agent interacts with equipment, as shown in Figure 2, its operation text will be sent to the background support model. The support function then returns the operation outcome based on the predefined rules or model-generated texts. For example, if an agent wants to get a cup of tea from a stove, the operation is 'Get a cup of tea' and the support function may return 'Meaningless operation' according to the hard code or 'You can not get tea from a stove' generated by the model. Then the agent would learn from the feedback and refine its operations.</p>
<h3>4 Interaction scenarios</h3>
<p>Regarding the researchers' backgrounds and purposes, we design two interaction modes: User Mode and Developer Mode. In the User Mode, researchers who consider little about background support systems are target users. For researchers chasing better LLMs performance, Developer Mode provides flexible protocols for their development of different support mechanisms.</p>
<h4>4.1 User Mode</h4>
<p>In the User Mode, AgentSims provides an interactive interface in a pixel game style, as shown in Figure 1. Researchers can create agents, construct buildings and equipment in a graphical interface, focusing on the rationality of experiment design, free from complex background driving mechanisms.</p>
<p><strong>Agent Creation.</strong> Users can define agents within the system through an easy-to-use front end, as shown in Figure 3. AgentSims provides various protocols for users to create functional agents. Not only basic information like goals and biography, but also options of Memory and Planning Systems. We pre-design a list of memory and planning systems and users can choose their preference from a drop-down menu.</p>
<p><strong>Building Creation.</strong> Users can also customize the physical environment by constructing buildings. As shown in Figure 4, users define a building by choosing a pre-configured building with equipment inside. To be noticed, the equipment in buildings are predefined but can be modified in the Developer</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Agent Creation
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Building Creation</p>
<h2>Mode.</h2>
<p>Experiment Intervene. Besides observing, users can play as the major agent to participate in the experiment. By talking with other agents, users can intervene the experiment naturally rather than modify agents' memory or goals roughly.</p>
<h3>4.2 Developer Mode</h3>
<p>Developer Mode is designed for professional developers who are familiar with the properties of LLMs and pursue better performance of LLMs on a welldefined complex task. The highly-modularized feature of AgentSims enables developers to add new functions within a few lines of code.</p>
<p>Agent Design. Developers have the flexibility to create agents tailored for various objectives and assemble diverse agents within a single sandbox for observation. To streamline the process of agent customization, we've abstracted the LLM backbone and distinct support systems into separate classes and function calls, as illustrated below. This empowers developers to personalize an agent by making adjustments to these abstract functions.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">LLMCaller:</span>
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">model:</span> <span class="n">str</span>) -&gt; <span class="n">None:</span>
        <span class="nb">self</span>.<span class="n">model</span> = <span class="n">get_model</span>(<span class="n">model</span>)
    <span class="n">def</span> <span class="n">ask</span>(<span class="nb">self</span>, <span class="n">prompt:</span> <span class="n">str</span>) :
        <span class="nb">result</span> = <span class="nb">self</span>.<span class="n">model</span>.<span class="n">generate</span>(<span class="nb">prompt</span>)
        <span class="k">return</span> <span class="nb">result</span>
<span class="k">class</span> <span class="n">Agent:</span>
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="nb">name</span>, <span class="n">bio</span>, <span class="n">goal</span>, <span class="n">model</span>,
        <span class="n">memorySystem</span>, <span class="n">planSystem</span>, <span class="n">buildings</span>,
        <span class="n">cash</span>):
        <span class="nb">self</span>.<span class="k">state</span> = <span class="n">State</span>()
        <span class="nb">self</span>.<span class="k">state</span>.<span class="n">buildings</span> = <span class="n">buildings</span>
        <span class="nb">self</span>.<span class="k">state</span>.<span class="n">cash</span> = <span class="n">cash</span>
        <span class="nb">self</span>.<span class="n">caller</span> = <span class="n">Caller</span>(<span class="n">model</span>)
    <span class="n">def</span> <span class="nb">plan</span>(<span class="nb">self</span>) -&gt; <span class="n">None:</span>
        <span class="nb">self</span>.<span class="k">state</span>.<span class="n">plan_prompt</span> = ...
        <span class="nb">self</span>.<span class="k">state</span>.<span class="nb">plan</span> =
            <span class="nb">self</span>.<span class="n">caller</span>.<span class="n">ask</span>(<span class="nb">self</span>.<span class="k">state</span>.<span class="n">pl_prompt</span>)
    <span class="n">def</span> <span class="n">memory_store</span>(<span class="nb">self</span>) -&gt; <span class="n">None:</span>
        <span class="nb">self</span>.<span class="k">state</span>.<span class="n">memory_prompt</span> = ...
        <span class="nb">self</span>.<span class="k">state</span>.<span class="n">memory</span> =
        <span class="nb">self</span>.<span class="n">caller</span>.<span class="n">ask</span>(<span class="nb">self</span>.<span class="k">state</span>.<span class="n">mem_prompt</span>)
    <span class="n">def</span> <span class="k">use</span>(<span class="nb">self</span>, <span class="n">facility:</span> <span class="n">str</span>, <span class="n">operation:</span> <span class="n">str</span>,
        <span class="n">description:</span> <span class="n">str</span>) -&gt; <span class="n">None:</span>
        <span class="nb">self</span>.<span class="k">state</span>.<span class="n">use_prompt</span> = ...
        <span class="nb">self</span>.<span class="k">state</span>.<span class="k">use</span> =
            <span class="nb">self</span>.<span class="n">caller</span>.<span class="n">ask</span>(<span class="nb">self</span>.<span class="k">state</span>.<span class="n">use_prompt</span>)
</code></pre></div>

<p>Building and Equipment Design. To customize the physical environment, developers can design new buildings and equipment by configuring corresponding json files.
A new equipment can be defined by its type, description and a support function.</p>
<div class="codehilite"><pre><span></span><code><span class="p">[(</span><span class="s">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;counter&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;function&quot;</span><span class="p">:</span><span class="o">...</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;description&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;This is the counter ...&quot;</span><span class="p">,)]</span>
</code></pre></div>

<p>In some cases, agents can purchase commodities or earn salaries at the equipment. We use another configure file to annotate these economic features.</p>
<div class="codehilite"><pre><span></span><code>[[ &quot;id&quot;: 1,
    &quot;menu&quot;: [
        &quot;chicken&quot;: 20,},
    &quot;salary&quot;:0,)],
</code></pre></div>

<p>We define buildings by a type and the equipment it contains. Hence we use a two-dimensional array to mark the facility ids in the building blocks.</p>
<div class="codehilite"><pre><span></span><code><span class="p">[(</span><span class="s">&quot;assets&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;store_v1.2_0719&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;price&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;store&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s">&quot;blocks&quot;</span><span class="p">:[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="o">...</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span>
<span class="w">    </span><span class="s">&quot;equipment&quot;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">..]])]</span>
</code></pre></div>

<h2>5 Implementation</h2>
<p>AgentSims is run using Python $3.9^{1}$ and requires installing the requirements.txt file provided in the codebase using Python's package manager PyPI ${ }^{2}$.</p>
<h3>5.1 Backend</h3>
<p>The web server is built using Tornado ${ }^{3}$, a lightweight Python web framework. It also uses the websockets library for API calls and push notifications, and mysql-connector-python to interact with the MySQL ${ }^{4}$ database.</p>
<h3>5.2 Frontend</h3>
<p>Frontend The web client is built with Unity ${ }^{5}$. The client built by WebGL ${ }^{6}$ is embedded in the project code and can be accessed through a browser after proxying with nginx ${ }^{7}$.</p>
<h2>6 Example Application Tasks</h2>
<h3>6.1 Subject LLM as participants</h3>
<p>When subject LLM agents are participants of an artificial scenario, researchers can evaluate LLM's social abilities, like ToM . In this case, the formulation of specific social scenes is realized by other baseline agents driven by stronger LLMs. For example, to study a new model's social adaptation abilities in a hostile environment, we can embed colleague agents driven by GPT4 with a strong desire of bullying newcomers. Then we place subject agents into this adversarial milieu and test whether the new model can understand other's emotion and improve how colleagues perceive it.</p>
<h3>6.2 Subject LLM as mayor</h3>
<p>To assess LLM's long-term planning and organization abilities, researchers can appoint the subject LLM as the mayor of a town or the president of a company, where residents or employees are driven by baseline agents like GPT4. To overcome the difficulties set ahead deliberately or emerging during the experiments, then achieve the final goal of the task, the subject LLM needs to recruit new residents to handle new problems, issue sound policies</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and modify the out-of-date ones, found new functional buildings to satisfy emerging requirements, and so on. By analyzing the success rate of LLM mayor under different difficulties, researchers can gain valuable insights into the diverse capabilities of the LLM.</p>
<h3>6.3 Applications besides Evaluation</h3>
<p>Besides evaluating LLMs, AgentSims can be used as a data generation platform. Due to the fantastic NLG abilities of LLMs, researchers have applied them in data annotation and augmentation. However, some data involving social judgement and participation necessitate a more intricate approach than a single prompt can provide. Thus, we can simulate a specific social background and let LLMs generate data more precisely. Liu et al. (2023b) have applied simulated society in alignment data generation. With AgentSims tailored for more intricate social simulations, its potential for enhancing data generation across various disciplines is undeniable.
Moreover, our program can also benefit social science researchers, by conducting more controllable preliminary experiments. Given that sota LLMs can understand human instructions and simulate human behaviours, social science researchers can design social environments as they wish for preliminary studies. Once researchers have a hypothesis, pilot experiments can be conducted in our virtual sandbox as a feasibility check.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we present AgentSims, avisualized and program-based infrastructure for LLM test sandbox construction. AgentSims aims to facilitate researchers in effectively building LLM evaluation tasks. It not only intends to make all its code openly available but also commits to continuously updating its documentation with comprehensive tutorials.</p>
<h2>Limitations</h2>
<p>As a sandbox system, AgentSims' simulation ability is limited by the accuracy of LLMs and the diversity of buildings and equipment. It can never fully reflect real world cases. Besides, although task-based evaluation is a sound approach to measure the general ability of LLMs, it can hardly reflect fine-grained abilities like math reasoning. The pass rate of tasks can not provide insights on why LLMs success or fail.</p>
<h2>References</h2>
<p>BIG bench authors. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CÃ©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint arXiv:2306.11644.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. Ceval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for "mind" exploration of large scale language model society.</p>
<p>Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023a. Evaluating the logical reasoning ability of chatgpt and gpt-4.</p>
<p>Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. 2023b. Training socially aligned language models in simulated human society.</p>
<p>Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan. 2023. Dera: Enhancing large language model completions with dialog-enabled resolving agents.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Joon Sung Park, Joseph C. Oâ€™Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior.</p>
<p>Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2022. Social simulacra: Creating populated prototypes for social computing systems.</p>
<p>David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515-526.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. Safety assessment of chinese large language models.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023b. Is chatgpt a good nlg evaluator? a preliminary study.</p>
<p>Lilian Weng. 2023. Llm-powered autonomous agents. lilianweng.github.io.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://www.python.org/downloads/release/ python-390
${ }^{2}$ https://pypi.org/
${ }^{3}$ https://www.tornadoweb.org/en/stable/
${ }^{4}$ https://www.mysql.com/
${ }^{5}$ https://unity3d.com
${ }^{6}$ https://get.webgl.org
${ }^{7}$ https://nginx.org/en/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
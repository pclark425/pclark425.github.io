<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7769 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7769</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7769</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-f2209eb5ac6747319a29b87dedabb97770be3243</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f2209eb5ac6747319a29b87dedabb97770be3243" target="_blank">Can large language models provide useful feedback on research papers? A large-scale empirical analysis</a></p>
                <p><strong>Paper Venue:</strong> NEJM AI</p>
                <p><strong>Paper TL;DR:</strong> An automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers shows that LLM-generated feedback can help researchers, but also identifies several limitations.</p>
                <p><strong>Paper Abstract:</strong> Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7769.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7769.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 feedback pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based scientific feedback generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end system that parses a paper PDF, constructs a paper-specific prompt (title, abstract, captions, main text up to token limit), and invokes GPT-4 to produce structured reviewer-style feedback in four sections (significance & novelty, reasons for acceptance, reasons for rejection, suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific peer review / interdisciplinary (AI, computational biology, biology, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feedback / critique generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Retrospective overlap analysis and prospective user study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Retrospective: generate GPT-4 feedback for papers and compare extracted GPT-4 comment points to human reviewer comments via an extractive-summarization + semantic-matching pipeline and set overlap metrics; Prospective: deploy system to authors who receive GPT-4 reviews and complete a structured survey about helpfulness/alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit rate (pairwise overlap) and additional set-overlap coefficients; user-study helpfulness/ alignment percentages</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hit rate = |A ∩ B| / |A| where A is the set of comments from GPT-4 and B is the set from an individual human reviewer; other metrics: Szymkiewicz-Simpson = |A ∩ B|/min(|A|,|B|), Jaccard = |A ∩ B|/|A ∪ B|, Sørensen-Dice = 2|A ∩ B|/(|A|+|B|). User-study metrics are percentages of respondents endorsing helpfulness/alignment categories.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Nature family journals dataset (3,096 accepted papers, 8,745 reviews) and ICLR dataset (1,709 papers, ~6.5k reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Prospective user study: 308 researchers from 110 US institutions completed a survey assessing helpfulness, specificity, and alignment; retrospective human verification: two co-authors validated extractive summarization on 639 feedback pieces; three co-authors independently labeled 760 feedback pairs for semantic matching; inter-annotator agreement on sampled pairs: 89.8% pairwise agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Retrospective overlap: GPT-4 vs individual human reviewer hit rates: 30.85% (Nature) and 39.23% (ICLR); comparable human-human: 28.58% (Nature) and 35.25% (ICLR). Prospective: 57.4% of users found feedback helpful/very helpful; 82.4% thought it more beneficial than at least some human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>GPT-4 feedback overlap with human reviewers is comparable to overlap between two human reviewers; GPT-4 found more of the issues that multiple reviewers raised and greater overlap on rejected ICLR papers (47.09% GPT-4 vs humans; human-human 43.80%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Model limited by token input/output constraint (8,192 tokens); pipeline uses only first ~6,500 tokens; GPT-4 lacks visual (figure/table) understanding in this work; GPT-4 tends to emphasize certain feedback aspects (e.g., call for more datasets) and less on method-design depth; user study subject to self-selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7769.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrospective Comment Matching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage retrospective comment matching pipeline (extractive summarization + semantic matching)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that first extracts discrete comment points from free-text reviews (human and GPT-4) via extractive summarization, then performs semantic matching between comment pairs to identify overlapping points and compute overlap metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational text analysis / peer review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Extractive summarization followed by semantic text matching with similarity thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Stage 1: use GPT-4 to perform extractive summarization producing JSON lists of comment points. Stage 2: use GPT-4 to propose matched pairs across two comment lists and a self-assessed similarity rating (scale 5–10); retain matches rated ≥7 for analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pipeline validation metrics: extraction F1, matching F1, precision, recall; overlap metrics (hit rate, Jaccard, etc.) computed on matched comment pairs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Extraction stage: precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean; Matching stage: same definitions applied to matched comment pairs against human annotations; overlap metrics as defined above.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Validation samples drawn from the Nature and ICLR feedback corpora (e.g., 639 feedbacks for summarization validation; 760 feedback pairs for matching validation).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Summarization validation: two co-authors annotated 639 feedbacks producing extraction F1=0.968 (precision 0.977, recall 0.960). Matching validation: three co-authors labeled 12,035 comment pairs yielding matching F1=0.824 (precision 0.777, recall 0.878). Inter-annotator sampling: 800 comment pairs showed 89.8% pairwise agreement and an F1 of 88.7% vs majority.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Extraction F1=0.968 (P=0.977, R=0.960); Matching F1=0.824 (P=0.777, R=0.878); inter-annotator agreement 89.8%. Matches rated 7 or above retained.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Matching relies on GPT-4's lenient matching and required thresholding (kept ≥7) to align with human judgments; potential dependence on quality of extractive summarization; some variability in 'somewhat related' (5–6) matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7769.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExtractiveSumm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extractive text summarization for comment point extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach using GPT-4 to convert reviewer free-text (human or LLM) into a structured list of discrete comment points (JSON), focusing on criticisms to facilitate matching and overlap analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language processing / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method (text processing)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human-verified extractive summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Feed full review text to GPT-4 with a prompt to extract numbered comment points in JSON; validate outputs against human annotations to compute precision/recall/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 for extracted comment points</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision = correctly extracted points / extracted points; Recall = correctly extracted / ground-truth points; F1 = harmonic mean.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Sample of 639 feedback texts from Nature and ICLR datasets used for validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Two co-authors annotated results; reported extraction F1=0.968 (precision 0.977, recall 0.960).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Extraction F1 = 0.968 (P 0.977, R 0.960).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Summarization focuses on criticisms only; may omit nuanced praises or context; performance validated on sampled subset only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7769.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemanticMatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic text matching with graded similarity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic matching approach using GPT-4 to determine whether two extracted comment points are referring to the same issue, with a similarity rating on a 5–10 scale; matches rated ≥7 are considered true matches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>semantic similarity / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method (semantic matching / alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GPT-4-based pairwise semantic matching with human validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Input pairs of comment-point texts into GPT-4; GPT-4 outputs whether they match plus a similarity rating (5 to 10). Retain matches with rating ≥7. Validate against human annotations for precision/recall/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matching precision, recall, F1; retention threshold (≥7)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Matching F1 computed over retained matches compared to human-labeled matches; similarity scale 5 (Somewhat Related) to 10 (Strongly Related).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>12,035 comment pairs generated from 760 feedback pair samples for validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three co-authors annotated whether comment pairs match; matching pipeline achieved F1=0.824 (P=0.777, R=0.878).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Matching F1=0.824 (precision 0.777, recall 0.878); matches with scores 5–6 were found noisy so excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subjectivity around similarity thresholds; GPT-4's leniency required discarding matches <7; possible borderline semantic matches lost by thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7769.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OverlapMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Set-overlap metrics (Hit Rate / Jaccard / Szymkiewicz-Simpson / Sørensen-Dice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of set-based overlap measures used to quantify correspondence between GPT-4-extracted comment sets and human reviewer comment sets, including hit rate, Jaccard, Szymkiewicz-Simpson, and Sørensen-Dice coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical comparison / evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hit rate and alternative set overlap coefficients</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute proportion of comments in set A (e.g., GPT-4) that match set B (human) and also compute symmetric/normalized similarity indices to ensure robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit rate; Szymkiewicz-Simpson overlap coefficient; Jaccard index; Sørensen-Dice coefficient</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hit rate = |A∩B|/|A|. Szymkiewicz-Simpson = |A∩B|/min(|A|,|B|). Jaccard = |A∩B|/|A∪B|. Sørensen-Dice = 2|A∩B|/(|A|+|B|). Scales range 0–1 (or 0%–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to Nature and ICLR comment sets produced by extractive summarization + matching</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Results reported with confidence intervals and stratified analyses (by journal, by ICLR decision outcome, by comment position and reviewer count).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Hit rate GPT-4 vs individual human: 30.85% (Nature), 39.23% (ICLR). Human-human: 28.58% (Nature), 35.25% (ICLR). Shuffled controls dropped hit rate to ~0.43% (Nature) and ~3.91% (ICLR) depending on report.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Overlap metrics for GPT-4 vs human were comparable to human-human overlaps across datasets and stratifications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Hit rate is asymmetric (depends on ordering and number of comments); authors controlled for number of comments for human-human comparisons to mitigate bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7769.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShuffleNullTest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shuffling (null model) experiment for specificity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled null-model experiment that reassigns GPT-4 reviews randomly to other papers within the same journal/category (Nature) or conference year (ICLR) to test whether GPT-4 feedback is paper-specific versus generic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>experimental control / statistical test</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>specificity test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Shuffled-pair overlap comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare hit rates between true GPT-4→paper pairings and shuffled GPT-4→other-paper pairings; a large drop in overlap indicates paper-specific feedback rather than generic comments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Change in hit rate (pairwise overlap) pre- vs post-shuffle</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent hit rate as above; report absolute drop (e.g., from X% to Y%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Nature family journals (same journal/category) and ICLR (same conference year)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None beyond automated overlap computations; used as statistical control.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Nature: hit rate dropped from 30.85% to 0.43% (or 57.55% to 1.13% depending on metric variant reported); ICLR: drop from 39.23% to 3.91% (and majority overlap numbers showed similar drops).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Shuffling constrained within journal/category or year to preserve broad topical similarity; residual nonzero overlaps could reflect generic concerns shared across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7769.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NatureDataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nature family journals reviewer dataset (2022-2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Curated dataset of accepted papers from 15 Nature family journals (Jan 1, 2022 to Jun 17, 2023) along with published peer reviewer comments (3,096 papers, 8,745 reviews) used for retrospective analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine, multidisciplinary sciences</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset for evaluation of LLM feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Retrospective overlap with LLM-generated feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply GPT-4 pipeline to each paper to generate feedback, extract comment points and compute overlap with human reviewer comments from Nature's published peer review materials.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit rate and alternative overlap coefficients as above; per-journal stratified analyses</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hit rate = |A∩B|/|A| where A=GPT-4 comments for paper, B=human reviewer comments for that paper; metrics reported as percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>3,096 accepted papers, 8,745 reviews sourced from nature.com (transparent peer review)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used as ground truth human comments for matching; some human validation performed on pipeline outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Across Nature journals, 57.55% of GPT-4 comments were raised by at least one human reviewer; per-paper GPT-4 vs individual reviewer hit rate averaged 30.85%; per-journal rates varied (e.g., 15.58% to 39.16%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>GPT-4-to-human overlap mirrored human-human overlap after controlling for comment counts; similar patterns across journals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset comprises only accepted papers that opted into transparent peer review; excludes rejected manuscripts and non-English material.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7769.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICLRDataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ICLR open review dataset (2022-2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Stratified sample of ICLR 2022 and 2023 papers and all available open reviews, including accepted (oral, spotlight, poster), rejected, and withdrawn papers totaling 1,709 papers and ~6,506 reviews used for retrospective analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset for evaluation of LLM feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Retrospective overlap analysis across different acceptance outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate GPT-4 feedback for sampled ICLR papers and compute overlap with human reviews, stratifying by paper decision category (oral/spotlight/poster/rejected/withdrawn).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit rate and alternative overlap coefficients, stratified by decision outcome</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hit rate = |A∩B|/|A| reported as percentage; comparisons across acceptance categories.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>1,709 sampled ICLR papers from 2022-2023 and their reviews retrieved via OpenReview API</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used human reviews as ground truth; stratified analysis showed rejected papers had higher overlap with GPT-4 feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4 vs individual reviewer hit rate 39.23% overall; overlap higher for rejected papers (GPT-4 47.09% vs humans 43.80%) and lower for top accepted papers (oral ~30.63%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Overlap comparable to human-human across decision categories; GPT-4 more aligned on obvious/major issues (rejected papers).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset limited to English-language top-venue submissions and stratified sampling; withdrawn papers included but may have different review dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7769.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnnotationSchema11</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>11-aspect comment annotation schema for ICLR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated annotation schema comprising 11 distinct aspects (e.g., novelty, research implications, requests for additional experiments, ablations) used to categorize and compare the thematic focus of LLM and human comments on ICLR papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>peer review analysis / ML research norms</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>annotation framework / evaluation criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual annotation of extracted comments against 11 aspects</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Random sample of 500 ICLR papers; use extractive pipeline to get comment points; two ML-experienced annotators labeled each comment for presence of any of the 11 aspects; compare LLM vs human prevalence per aspect.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Relative frequencies and prevalence ratios between LLM and human comments per aspect</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Aspect prevalence measured as proportion of comments labeled with that aspect; reported prevalence ratios (e.g., LLM comments on implications 7.27× more frequent than humans).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Random sample of 500 ICLR papers; extracted comment lists for both LLM and human reviews</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Two annotators with ML background performed annotations; aspects selection informed by ML peer-review literature.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM comments on implications 7.27× more frequent than humans; LLM 10.69× less likely to comment on novelty; humans 6.71× more likely to request ablation experiments; LLM 2.19× more likely to request experiments on more datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Marked differences in aspect emphasis suggest complementary strengths: LLM highlights implications and dataset diversity; humans emphasize novelty and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Annotation limited to ICLR domain; only two annotators—possible subjectivity; schema and prevalence derived from sampled subset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7769.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7769.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProspectiveUserStudy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prospective user study and survey of authors receiving LLM feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured user study where 308 researchers uploaded their own papers (post-cutoff) to receive GPT-4 reviews and completed a 6-page survey assessing helpfulness, specificity, alignment with human feedback, and reuse intent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>human-subject evaluation / human-AI interaction in scholarly practice</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human evaluation / survey</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Author survey with quantitative and qualitative items</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After receiving GPT-4 generated review via a Gradio demo, participants completed a 15–20 minute compensated survey covering demographics, impressions of LLM review specificity/helpfulness, comparisons to human reviews, and willingness to reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentages across categorical responses (helpful/very helpful, alignment levels, specificity comparisons), reuse intent rates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Likert-like and categorical survey responses reported as percentages of respondents (e.g., 57.4% found feedback helpful/very helpful).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>308 participating researchers from 110 US institutions focused on AI and computational biology; papers submitted had to be post-9/2021.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>308 participants; compensation $20; survey included multiple items and free-text; results stratified by education/experience with consistent patterns reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>57.4% of users found GPT-4 feedback helpful/very helpful; 82.4% found it more beneficial than at least some human reviewers; 50.5% willing to reuse system; 35% saw considerable/substantial alignment with expected human points.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Users judged GPT-4 feedback somewhat less specific than many humans but often comparable or better than some reviewers; many reported novel overlooked perspectives by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Self-selection bias among participants; participant pool concentrated in ML and computational biology; authors instructed to submit only post-9/2021 papers to avoid training-set leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models provide useful feedback on research papers? A large-scale empirical analysis', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>The automatic creation of literature abstracts <em>(Rating: 2)</em></li>
                <li>TextRank: Bringing order into text <em>(Rating: 2)</em></li>
                <li>LexRank: Graph-based lexical centrality as salience in text summarization <em>(Rating: 2)</em></li>
                <li>Indexing by latent semantic analysis <em>(Rating: 2)</em></li>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7769",
    "paper_id": "paper-f2209eb5ac6747319a29b87dedabb97770be3243",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "GPT-4 feedback pipeline",
            "name_full": "GPT-4-based scientific feedback generation pipeline",
            "brief_description": "An end-to-end system that parses a paper PDF, constructs a paper-specific prompt (title, abstract, captions, main text up to token limit), and invokes GPT-4 to produce structured reviewer-style feedback in four sections (significance & novelty, reasons for acceptance, reasons for rejection, suggestions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "scientific peer review / interdisciplinary (AI, computational biology, biology, etc.)",
            "theory_type": "feedback / critique generation",
            "evaluation_method_name": "Retrospective overlap analysis and prospective user study",
            "evaluation_method_description": "Retrospective: generate GPT-4 feedback for papers and compare extracted GPT-4 comment points to human reviewer comments via an extractive-summarization + semantic-matching pipeline and set overlap metrics; Prospective: deploy system to authors who receive GPT-4 reviews and complete a structured survey about helpfulness/alignment.",
            "evaluation_metric": "Hit rate (pairwise overlap) and additional set-overlap coefficients; user-study helpfulness/ alignment percentages",
            "metric_definition": "Hit rate = |A ∩ B| / |A| where A is the set of comments from GPT-4 and B is the set from an individual human reviewer; other metrics: Szymkiewicz-Simpson = |A ∩ B|/min(|A|,|B|), Jaccard = |A ∩ B|/|A ∪ B|, Sørensen-Dice = 2|A ∩ B|/(|A|+|B|). User-study metrics are percentages of respondents endorsing helpfulness/alignment categories.",
            "dataset_or_benchmark": "Nature family journals dataset (3,096 accepted papers, 8,745 reviews) and ICLR dataset (1,709 papers, ~6.5k reviews)",
            "human_evaluation_details": "Prospective user study: 308 researchers from 110 US institutions completed a survey assessing helpfulness, specificity, and alignment; retrospective human verification: two co-authors validated extractive summarization on 639 feedback pieces; three co-authors independently labeled 760 feedback pairs for semantic matching; inter-annotator agreement on sampled pairs: 89.8% pairwise agreement.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Retrospective overlap: GPT-4 vs individual human reviewer hit rates: 30.85% (Nature) and 39.23% (ICLR); comparable human-human: 28.58% (Nature) and 35.25% (ICLR). Prospective: 57.4% of users found feedback helpful/very helpful; 82.4% thought it more beneficial than at least some human reviewers.",
            "comparison_to_human_generated": true,
            "comparison_results": "GPT-4 feedback overlap with human reviewers is comparable to overlap between two human reviewers; GPT-4 found more of the issues that multiple reviewers raised and greater overlap on rejected ICLR papers (47.09% GPT-4 vs humans; human-human 43.80%).",
            "limitations_noted": "Model limited by token input/output constraint (8,192 tokens); pipeline uses only first ~6,500 tokens; GPT-4 lacks visual (figure/table) understanding in this work; GPT-4 tends to emphasize certain feedback aspects (e.g., call for more datasets) and less on method-design depth; user study subject to self-selection bias.",
            "uuid": "e7769.0",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Retrospective Comment Matching",
            "name_full": "Two-stage retrospective comment matching pipeline (extractive summarization + semantic matching)",
            "brief_description": "A framework that first extracts discrete comment points from free-text reviews (human and GPT-4) via extractive summarization, then performs semantic matching between comment pairs to identify overlapping points and compute overlap metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "computational text analysis / peer review evaluation",
            "theory_type": "evaluation framework",
            "evaluation_method_name": "Extractive summarization followed by semantic text matching with similarity thresholding",
            "evaluation_method_description": "Stage 1: use GPT-4 to perform extractive summarization producing JSON lists of comment points. Stage 2: use GPT-4 to propose matched pairs across two comment lists and a self-assessed similarity rating (scale 5–10); retain matches rated ≥7 for analyses.",
            "evaluation_metric": "Pipeline validation metrics: extraction F1, matching F1, precision, recall; overlap metrics (hit rate, Jaccard, etc.) computed on matched comment pairs",
            "metric_definition": "Extraction stage: precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean; Matching stage: same definitions applied to matched comment pairs against human annotations; overlap metrics as defined above.",
            "dataset_or_benchmark": "Validation samples drawn from the Nature and ICLR feedback corpora (e.g., 639 feedbacks for summarization validation; 760 feedback pairs for matching validation).",
            "human_evaluation_details": "Summarization validation: two co-authors annotated 639 feedbacks producing extraction F1=0.968 (precision 0.977, recall 0.960). Matching validation: three co-authors labeled 12,035 comment pairs yielding matching F1=0.824 (precision 0.777, recall 0.878). Inter-annotator sampling: 800 comment pairs showed 89.8% pairwise agreement and an F1 of 88.7% vs majority.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Extraction F1=0.968 (P=0.977, R=0.960); Matching F1=0.824 (P=0.777, R=0.878); inter-annotator agreement 89.8%. Matches rated 7 or above retained.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Matching relies on GPT-4's lenient matching and required thresholding (kept ≥7) to align with human judgments; potential dependence on quality of extractive summarization; some variability in 'somewhat related' (5–6) matches.",
            "uuid": "e7769.1",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ExtractiveSumm",
            "name_full": "Extractive text summarization for comment point extraction",
            "brief_description": "An approach using GPT-4 to convert reviewer free-text (human or LLM) into a structured list of discrete comment points (JSON), focusing on criticisms to facilitate matching and overlap analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "natural language processing / summarization",
            "theory_type": "method (text processing)",
            "evaluation_method_name": "Human-verified extractive summarization",
            "evaluation_method_description": "Feed full review text to GPT-4 with a prompt to extract numbered comment points in JSON; validate outputs against human annotations to compute precision/recall/F1.",
            "evaluation_metric": "Precision, Recall, F1 for extracted comment points",
            "metric_definition": "Precision = correctly extracted points / extracted points; Recall = correctly extracted / ground-truth points; F1 = harmonic mean.",
            "dataset_or_benchmark": "Sample of 639 feedback texts from Nature and ICLR datasets used for validation",
            "human_evaluation_details": "Two co-authors annotated results; reported extraction F1=0.968 (precision 0.977, recall 0.960).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Extraction F1 = 0.968 (P 0.977, R 0.960).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Summarization focuses on criticisms only; may omit nuanced praises or context; performance validated on sampled subset only.",
            "uuid": "e7769.2",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SemanticMatch",
            "name_full": "Semantic text matching with graded similarity",
            "brief_description": "A semantic matching approach using GPT-4 to determine whether two extracted comment points are referring to the same issue, with a similarity rating on a 5–10 scale; matches rated ≥7 are considered true matches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "semantic similarity / NLP",
            "theory_type": "method (semantic matching / alignment)",
            "evaluation_method_name": "GPT-4-based pairwise semantic matching with human validation",
            "evaluation_method_description": "Input pairs of comment-point texts into GPT-4; GPT-4 outputs whether they match plus a similarity rating (5 to 10). Retain matches with rating ≥7. Validate against human annotations for precision/recall/F1.",
            "evaluation_metric": "Matching precision, recall, F1; retention threshold (≥7)",
            "metric_definition": "Matching F1 computed over retained matches compared to human-labeled matches; similarity scale 5 (Somewhat Related) to 10 (Strongly Related).",
            "dataset_or_benchmark": "12,035 comment pairs generated from 760 feedback pair samples for validation",
            "human_evaluation_details": "Three co-authors annotated whether comment pairs match; matching pipeline achieved F1=0.824 (P=0.777, R=0.878).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Matching F1=0.824 (precision 0.777, recall 0.878); matches with scores 5–6 were found noisy so excluded.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Subjectivity around similarity thresholds; GPT-4's leniency required discarding matches &lt;7; possible borderline semantic matches lost by thresholding.",
            "uuid": "e7769.3",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "OverlapMetrics",
            "name_full": "Set-overlap metrics (Hit Rate / Jaccard / Szymkiewicz-Simpson / Sørensen-Dice)",
            "brief_description": "A suite of set-based overlap measures used to quantify correspondence between GPT-4-extracted comment sets and human reviewer comment sets, including hit rate, Jaccard, Szymkiewicz-Simpson, and Sørensen-Dice coefficients.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "statistical comparison / evaluation metrics",
            "theory_type": "evaluation metrics",
            "evaluation_method_name": "Hit rate and alternative set overlap coefficients",
            "evaluation_method_description": "Compute proportion of comments in set A (e.g., GPT-4) that match set B (human) and also compute symmetric/normalized similarity indices to ensure robustness.",
            "evaluation_metric": "Hit rate; Szymkiewicz-Simpson overlap coefficient; Jaccard index; Sørensen-Dice coefficient",
            "metric_definition": "Hit rate = |A∩B|/|A|. Szymkiewicz-Simpson = |A∩B|/min(|A|,|B|). Jaccard = |A∩B|/|A∪B|. Sørensen-Dice = 2|A∩B|/(|A|+|B|). Scales range 0–1 (or 0%–100%).",
            "dataset_or_benchmark": "Applied to Nature and ICLR comment sets produced by extractive summarization + matching",
            "human_evaluation_details": "Results reported with confidence intervals and stratified analyses (by journal, by ICLR decision outcome, by comment position and reviewer count).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Hit rate GPT-4 vs individual human: 30.85% (Nature), 39.23% (ICLR). Human-human: 28.58% (Nature), 35.25% (ICLR). Shuffled controls dropped hit rate to ~0.43% (Nature) and ~3.91% (ICLR) depending on report.",
            "comparison_to_human_generated": true,
            "comparison_results": "Overlap metrics for GPT-4 vs human were comparable to human-human overlaps across datasets and stratifications.",
            "limitations_noted": "Hit rate is asymmetric (depends on ordering and number of comments); authors controlled for number of comments for human-human comparisons to mitigate bias.",
            "uuid": "e7769.4",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ShuffleNullTest",
            "name_full": "Shuffling (null model) experiment for specificity",
            "brief_description": "A controlled null-model experiment that reassigns GPT-4 reviews randomly to other papers within the same journal/category (Nature) or conference year (ICLR) to test whether GPT-4 feedback is paper-specific versus generic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "experimental control / statistical test",
            "theory_type": "specificity test",
            "evaluation_method_name": "Shuffled-pair overlap comparison",
            "evaluation_method_description": "Compare hit rates between true GPT-4→paper pairings and shuffled GPT-4→other-paper pairings; a large drop in overlap indicates paper-specific feedback rather than generic comments.",
            "evaluation_metric": "Change in hit rate (pairwise overlap) pre- vs post-shuffle",
            "metric_definition": "Percent hit rate as above; report absolute drop (e.g., from X% to Y%).",
            "dataset_or_benchmark": "Nature family journals (same journal/category) and ICLR (same conference year)",
            "human_evaluation_details": "None beyond automated overlap computations; used as statistical control.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Nature: hit rate dropped from 30.85% to 0.43% (or 57.55% to 1.13% depending on metric variant reported); ICLR: drop from 39.23% to 3.91% (and majority overlap numbers showed similar drops).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Shuffling constrained within journal/category or year to preserve broad topical similarity; residual nonzero overlaps could reflect generic concerns shared across papers.",
            "uuid": "e7769.5",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "NatureDataset",
            "name_full": "Nature family journals reviewer dataset (2022-2023)",
            "brief_description": "Curated dataset of accepted papers from 15 Nature family journals (Jan 1, 2022 to Jun 17, 2023) along with published peer reviewer comments (3,096 papers, 8,745 reviews) used for retrospective analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "biomedicine, multidisciplinary sciences",
            "theory_type": "dataset for evaluation of LLM feedback",
            "evaluation_method_name": "Retrospective overlap with LLM-generated feedback",
            "evaluation_method_description": "Apply GPT-4 pipeline to each paper to generate feedback, extract comment points and compute overlap with human reviewer comments from Nature's published peer review materials.",
            "evaluation_metric": "Hit rate and alternative overlap coefficients as above; per-journal stratified analyses",
            "metric_definition": "Hit rate = |A∩B|/|A| where A=GPT-4 comments for paper, B=human reviewer comments for that paper; metrics reported as percentages.",
            "dataset_or_benchmark": "3,096 accepted papers, 8,745 reviews sourced from nature.com (transparent peer review)",
            "human_evaluation_details": "Used as ground truth human comments for matching; some human validation performed on pipeline outputs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Across Nature journals, 57.55% of GPT-4 comments were raised by at least one human reviewer; per-paper GPT-4 vs individual reviewer hit rate averaged 30.85%; per-journal rates varied (e.g., 15.58% to 39.16%).",
            "comparison_to_human_generated": true,
            "comparison_results": "GPT-4-to-human overlap mirrored human-human overlap after controlling for comment counts; similar patterns across journals.",
            "limitations_noted": "Dataset comprises only accepted papers that opted into transparent peer review; excludes rejected manuscripts and non-English material.",
            "uuid": "e7769.6",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ICLRDataset",
            "name_full": "ICLR open review dataset (2022-2023)",
            "brief_description": "Stratified sample of ICLR 2022 and 2023 papers and all available open reviews, including accepted (oral, spotlight, poster), rejected, and withdrawn papers totaling 1,709 papers and ~6,506 reviews used for retrospective analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "machine learning / computer science",
            "theory_type": "dataset for evaluation of LLM feedback",
            "evaluation_method_name": "Retrospective overlap analysis across different acceptance outcomes",
            "evaluation_method_description": "Generate GPT-4 feedback for sampled ICLR papers and compute overlap with human reviews, stratifying by paper decision category (oral/spotlight/poster/rejected/withdrawn).",
            "evaluation_metric": "Hit rate and alternative overlap coefficients, stratified by decision outcome",
            "metric_definition": "Hit rate = |A∩B|/|A| reported as percentage; comparisons across acceptance categories.",
            "dataset_or_benchmark": "1,709 sampled ICLR papers from 2022-2023 and their reviews retrieved via OpenReview API",
            "human_evaluation_details": "Used human reviews as ground truth; stratified analysis showed rejected papers had higher overlap with GPT-4 feedback.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "GPT-4 vs individual reviewer hit rate 39.23% overall; overlap higher for rejected papers (GPT-4 47.09% vs humans 43.80%) and lower for top accepted papers (oral ~30.63%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Overlap comparable to human-human across decision categories; GPT-4 more aligned on obvious/major issues (rejected papers).",
            "limitations_noted": "Dataset limited to English-language top-venue submissions and stratified sampling; withdrawn papers included but may have different review dynamics.",
            "uuid": "e7769.7",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AnnotationSchema11",
            "name_full": "11-aspect comment annotation schema for ICLR",
            "brief_description": "A curated annotation schema comprising 11 distinct aspects (e.g., novelty, research implications, requests for additional experiments, ablations) used to categorize and compare the thematic focus of LLM and human comments on ICLR papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "peer review analysis / ML research norms",
            "theory_type": "annotation framework / evaluation criteria",
            "evaluation_method_name": "Manual annotation of extracted comments against 11 aspects",
            "evaluation_method_description": "Random sample of 500 ICLR papers; use extractive pipeline to get comment points; two ML-experienced annotators labeled each comment for presence of any of the 11 aspects; compare LLM vs human prevalence per aspect.",
            "evaluation_metric": "Relative frequencies and prevalence ratios between LLM and human comments per aspect",
            "metric_definition": "Aspect prevalence measured as proportion of comments labeled with that aspect; reported prevalence ratios (e.g., LLM comments on implications 7.27× more frequent than humans).",
            "dataset_or_benchmark": "Random sample of 500 ICLR papers; extracted comment lists for both LLM and human reviews",
            "human_evaluation_details": "Two annotators with ML background performed annotations; aspects selection informed by ML peer-review literature.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "LLM comments on implications 7.27× more frequent than humans; LLM 10.69× less likely to comment on novelty; humans 6.71× more likely to request ablation experiments; LLM 2.19× more likely to request experiments on more datasets.",
            "comparison_to_human_generated": true,
            "comparison_results": "Marked differences in aspect emphasis suggest complementary strengths: LLM highlights implications and dataset diversity; humans emphasize novelty and ablations.",
            "limitations_noted": "Annotation limited to ICLR domain; only two annotators—possible subjectivity; schema and prevalence derived from sampled subset.",
            "uuid": "e7769.8",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ProspectiveUserStudy",
            "name_full": "Prospective user study and survey of authors receiving LLM feedback",
            "brief_description": "A structured user study where 308 researchers uploaded their own papers (post-cutoff) to receive GPT-4 reviews and completed a 6-page survey assessing helpfulness, specificity, alignment with human feedback, and reuse intent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "human-subject evaluation / human-AI interaction in scholarly practice",
            "theory_type": "human evaluation / survey",
            "evaluation_method_name": "Author survey with quantitative and qualitative items",
            "evaluation_method_description": "After receiving GPT-4 generated review via a Gradio demo, participants completed a 15–20 minute compensated survey covering demographics, impressions of LLM review specificity/helpfulness, comparisons to human reviews, and willingness to reuse.",
            "evaluation_metric": "Percentages across categorical responses (helpful/very helpful, alignment levels, specificity comparisons), reuse intent rates",
            "metric_definition": "Likert-like and categorical survey responses reported as percentages of respondents (e.g., 57.4% found feedback helpful/very helpful).",
            "dataset_or_benchmark": "308 participating researchers from 110 US institutions focused on AI and computational biology; papers submitted had to be post-9/2021.",
            "human_evaluation_details": "308 participants; compensation $20; survey included multiple items and free-text; results stratified by education/experience with consistent patterns reported.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "57.4% of users found GPT-4 feedback helpful/very helpful; 82.4% found it more beneficial than at least some human reviewers; 50.5% willing to reuse system; 35% saw considerable/substantial alignment with expected human points.",
            "comparison_to_human_generated": true,
            "comparison_results": "Users judged GPT-4 feedback somewhat less specific than many humans but often comparable or better than some reviewers; many reported novel overlooked perspectives by GPT-4.",
            "limitations_noted": "Self-selection bias among participants; participant pool concentrated in ML and computational biology; authors instructed to submit only post-9/2021 papers to avoid training-set leakage.",
            "uuid": "e7769.9",
            "source_info": {
                "paper_title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "The automatic creation of literature abstracts",
            "rating": 2,
            "sanitized_title": "the_automatic_creation_of_literature_abstracts"
        },
        {
            "paper_title": "TextRank: Bringing order into text",
            "rating": 2,
            "sanitized_title": "textrank_bringing_order_into_text"
        },
        {
            "paper_title": "LexRank: Graph-based lexical centrality as salience in text summarization",
            "rating": 2,
            "sanitized_title": "lexrank_graphbased_lexical_centrality_as_salience_in_text_summarization"
        },
        {
            "paper_title": "Indexing by latent semantic analysis",
            "rating": 2,
            "sanitized_title": "indexing_by_latent_semantic_analysis"
        },
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing",
            "rating": 1,
            "sanitized_title": "reviewergpt_an_exploratory_study_on_using_large_language_models_for_paper_reviewing"
        }
    ],
    "cost": 0.018748749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can large language models provide useful feedback on research papers? A large-scale empirical analysis.</h1>
<p>Weixin Liang ${ }^{1 <em>}$, Yuhui Zhang ${ }^{1 </em>}$, Hancheng Cao ${ }^{1 *}$, Binglu Wang ${ }^{2}$, Daisy Yi Ding ${ }^{3}$, Xinyu Yang ${ }^{4}$, Kailas Vodrahalli ${ }^{3}$, Siyu He ${ }^{3}$, Daniel Scott Smith ${ }^{6}$, Yian Yin ${ }^{4}$, Daniel A. McFarland ${ }^{6}$, and James Zou ${ }^{1,3,5+}$<br>${ }^{1}$ Department of Computer Science, Stanford University, Stanford, CA 94305, USA<br>${ }^{2}$ Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA<br>${ }^{3}$ Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA<br>${ }^{4}$ Department of Information Science, Cornell University, Ithaca, NY 14850, USA<br>${ }^{5}$ Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA<br>${ }^{6}$ Graduate School of Education, Stanford University, Stanford, CA 94305, USA<br>${ }^{+}$Correspondence should be addressed to: jamesz@stanford.edu<br>${ }^{6}$ these authors contributed equally to this work</p>
<h4>Abstract</h4>
<p>Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals ( 3,096 papers in total) and the ICLR machine learning conference ( 1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap $30.85 \%$ for Nature journals, $39.23 \%$ for ICLR) is comparable to the overlap between two human reviewers (average overlap $28.58 \%$ for Nature journals, $35.25 \%$ for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers (i.e., rejected ICLR papers; average overlap 43.80\%). We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of Al and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half ( $57.4 \%$ ) of the users found GPT-4 generated feedback helpful/very helpful and $82.4 \%$ found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations. For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., 'add experiments on more datasets'), and often struggles to provide in-depth critique of method design. Together our results suggest that LLM and human feedback can complement each other. While human expert review is and should continue to be the foundation of rigorous scientific process, LLM feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review.</p>
<h2>Introduction</h2>
<p>In the 1940s, Claude Shannon, while at Bell Laboratories, embarked on developing a mathematical framework of information and communication ${ }^{1}$. Throughout this pursuit, he was faced with the challenge of naming his novel measure and considered terms such as 'information' and 'uncertainty'. Shannon shared his work with John von Neumann, who quickly recognized the profound links between Shannon's work and statistical mechanics, and proposed what later anchored modern information theory: 'Information Entropy'2. Scientific progress often rests</p>
<p>on feedback and critique. Effective feedback among peer scientists not only elucidates and promotes the way new discoveries are made, interpreted, and communicated, but also catalyzes the emergence of new scientific paradigms by connecting individual insights, coordinating concurrent lines of thoughts, and stimulating constructive debates and disagreement ${ }^{3}$.</p>
<p>However, the process of providing timely, comprehensive, and insightful feedback on scientific research is often laborious, resource-intensive, and complex ${ }^{4}$. This complexity is exacerbated by the exponential growth in scholarly publications and the deepening specialization of scientific knowledge ${ }^{5,6}$. Traditional avenues, such as peer review and conference discussions, exhibit constraints in scalability, expertise accessibility, and promptness. For instance, it has been estimated that peer review - one of the most major channels of scientific feedback - costs over 100M researcher hours and $\$ 2.5 \mathrm{~B}$ US dollars in a single year ${ }^{7}$. Yet at the same time, it has been increasingly challenging to secure enough qualified reviewers who can provide high-quality feedback given the rapid growth in the number of submissions ${ }^{8-12}$. For example, the number of submissions to the $I C L R$ machine learning conference increased from 960 in 2018 to 4,966 in 2023.</p>
<p>While shortage of high-quality feedback presents a fundamental constraint on the sustainable growth of science overall, it also becomes a source of deepening scientific inequalities. Marginalized researchers, especially those from non-elite institutions or resource-limited regions, often face disproportionate challenges in accessing valuable feedback, perpetuating a cycle of systemic scientific inequality ${ }^{13,14}$.</p>
<p>Given these challenges, there is an urgent need for crafting scalable and efficient feedback mechanisms that can enrich and streamline the scientific feedback process. Adopting such advancements holds the promise of not just elevating the quality and scope of scientific research, given the concerning deceleration in scientific advancements ${ }^{15,16}$, but also of democratizing its access across the scientific community.</p>
<p>Large language models (LLMs) ${ }^{17-19}$, especially those powered by Transformer-based architectures and pretrained at immense scales, have opened up great potential in various applications ${ }^{20-23}$. While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown. Despite recent attempts that explore the potential uses of such tools in areas such as automating paper screening ${ }^{24}$, error identification ${ }^{25}$, and checklist verification ${ }^{26}$ 1, we lack large-scale empirical evidence on whether and how LLMs may be used to facilitate scientific feedback and augment current academic practices.</p>
<p>In this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback. Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a). The system is designed to generate constructive feedback across various key aspects, mirroring the review structure of leading interdisciplinary journals ${ }^{27,28}$ and conferences ${ }^{29-33}$, including: 1) Significance and novelty, 2) Potential reasons for acceptance, 3) Potential reasons for rejection, and 4) Suggestions for improvement.</p>
<p>To characterize the informativeness of GPT-4 generated feedback, we conducted both a retrospective analysis and a prospective user study. In the retrospective analysis, we applied our pipeline on papers that had previously been assessed by human reviewers. We then compared the LLM feedback with the human feedback. We assessed the degree of overlap between key points raised by both sources to gauge the effectiveness and reliability of LLM feedback. Furthermore, we compared the topic distributions of LLM feedback and human feedback. To enable such analysis, we curated two complementary datasets containing full-text of papers, their meta information, and associated peer reviews after $2022^{2}$. The first dataset was sourced from Nature family journals, which are leading scientific journals covering multidisciplinary fields including biomedicine and basic sciences. Our second dataset was sourced from $I C L R$ (International Conference on Learning Representations), a leading computer science venue on artificial intelligence. This dataset, although narrower in scope, includes complete reviews for both accepted and rejected papers. These two datasets allowed us to evaluate the performance of LLM in generating scientific feedback across different types of scientific writing (e.g. across fields).</p>
<p>For the prospective user study, we developed a survey in which researchers were invited to evaluate the quality</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of the feedback produced by our GPT-4 system on their authored papers. By analyzing researchers' perspectives on the helpfulness, reliability, and potential limitations of LLM feedback, we can gauge the acceptability and utility of the proposed approach in the manuscript improvement process, and understand stakeholder's subjective perceptions of the framework. Through recruitment over institute mailing lists, and contacting paper authors who put preprints on arXiv, we were able to collect survey responses from 308 researchers from 110 US institutions in the field of AI and computational biology that come from diverse education status, experience, and institutes.</p>
<h1>Results</h1>
<h2>Generating Scientific Feedback using LLM</h2>
<p>We developed an automated pipeline that utilizes OpenAI's GPT-4 ${ }^{19}$ to generate feedback on the full PDF of scientific papers. The pipeline first parses the entire paper from the PDF, then constructs a paper-specific prompt for GPT-4. This prompt is created by concatenating our designed instructions with the paper's title, abstract, figure and table captions, and other main text (Fig. 1a, Methods). The prompt is then fed into GPT-4, which generates the scientific feedback in a single pass. Further details and validations of the pipeline can be found in the Supplementary Information.</p>
<h2>Retrospective Evaluation</h2>
<p>To evaluate the quality of LLM feedback retrospectively, we systematically assess the content overplay between human feedback given to submitted manuscripts and the LLM feedback using two large-scale datasets. The first dataset, sourced from Nature family journals, includes 8,745 comments from human reviewers for 3,096 accepted papers across 15 Nature family journals, including Nature, Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications (Supp. Table 1, Methods). The second dataset comprises 6,505 comments from human reviewers for 1,709 papers from the International Conference on Learning Representations (ICLR), a leading venue for artificial intelligence research in computer science (Supp. Table 2, Methods). These two datasets complement each other: the first dataset (Nature portfolio journals) spans a broad range of prominent journals across various scientific disciplines and impact levels, thereby capturing both the universality and variations in human-based scientific feedback. The second dataset (ICLR) provides an in-depth perspective of scientific feedback within leading venues of a rapidly evolving field - machine learning. Importantly, this second dataset includes expert feedback on both accepted and rejected papers.</p>
<p>We developed a retrospective comment matching pipeline to evaluate the overlap between feedback from LLM and human reviewers (Fig. 1b, Methods). The pipeline first performs extractive text summarization ${ }^{34-37}$ to extract the comments from both LLM and human-written feedback. It then applies semantic text matching ${ }^{38-40}$ to identify shared comments between the two feedback sources. We validated the pipeline's accuracy through human verification, yielding an F1 score of $96.8 \%$ for extraction (Supp. Table 3a, Methods) and $82.4 \%$ for matching (Supp. Table 3b, Methods).</p>
<h2>LLM feedback significantly overlaps with human-generated feedback</h2>
<p>We began by examining the overlap between LLM feedback and human feedback on Nature family journal data (Supp. Table 1). More than half ( $57.55 \%$ ) of the comments raised by GPT-4 were raised by at least one human reviewer (Supp. Fig. 1a). This suggests a considerable overlap between LLM feedback and human feedback, indicating potential accuracy and usefulness of the system. When comparing LLM feedback with comments from each individual reviewer, approximately one third ( $30.85 \%$ ) of GPT-4 raised comments overlapped with comments from an individual reviewer (Fig. 2a). The degree of overlap between two human reviewers was similar (28.58\%), after controlling for the number of comments (Methods). Results were consistent across other overlapping metrics including Szymkiewicz-Simpson overlap coefficient, Jaccard index, Sørensen-Dice coefficient (Supp. Fig. 2). This indicates that the overlap between LLM feedback and human feedback is comparable to the overlap observed between two human reviewers. We further stratified these overlap results by academic journals (Fig. 2c). While the degree of overlap between LLM feedback and human comments varied across different academic journals within the Nature family — from $15.58 \%$ in Nature Communications Materials to $39.16 \%$ in Nature - the overlap</p>
<p>between LLM feedback and human feedback comments largely mirrored the overlap found between two human reviewers. The robustness of the finding further indicates that scientific feedback generated from LLM is similar to what researchers could get from peer reviewers.</p>
<p>In parallel experiments, we investigated the comment overlap between LLM feedback and human feedback on $I C L R$ papers data (Supp. Table 2), and the results were largely similar. A majority ( $77.18 \%$ ) of the comments raised by GPT-4 were also raised by at least one human reviewer (Supp. Fig. 1b), indicating considerable overlap between LLM feedback and human feedback. When comparing LLM feedback with comments from each individual reviewer, more than one third ( $39.23 \%$ ) of GPT-4 raised comments overlapped with comments from an individual reviewer (Fig. 2b). The overlap between two human reviewers was similar ( $35.25 \%$ ), after controlling for the number of comments (Methods, Supp. Fig. 2). We further stratified these overlap results by the decision outcomes of the papers (Fig. 2d). Similar to results over Nature family journals, we found that the overlap between LLM feedback and human feedback comments largely mirrored the overlap found between two human reviewers.</p>
<p>In addition, as $I C L R$ dataset includes both accepted and rejected papers, we conducted stratification analysis and found a correlation between worse acceptance decisions and larger overlap in $I C L R$ papers. Specifically, papers accepted with oral presentations (representing the top $5 \%$ of accepted papers) have an average overlap of $30.63 \%$ between LLM feedback and human feedback comments. The average overlap increases to $32.12 \%$ for papers accepted with a spotlight presentation (the top $25 \%$ of accepted papers), while rejected papers bear the highest average overlap at $47.09 \%$. A similar trend was observed in the overlap between two human reviewers: $23.54 \%$ for papers accepted with oral presentations (top 5\% accepted papers), $24.52 \%$ for papers accepted with spotlight presentations (top $25 \%$ accepted papers), and $43.80 \%$ for rejected papers. This suggests that rejected papers may have more apparent issues or flaws that both human reviewers and LLMs can consistently identify. Additionally, the increased overlap between LLM feedback and actual human reviewer feedback for rejected papers indicates that LLM feedback could be particularly constructive and formative for papers that require more substantial revisions to be accepted. Indeed, by raising these concerns earlier in the scientific process before review, these papers and the science they report may be improved.</p>
<h1>LLM could generate non-generic feedbacks.</h1>
<p>Is it possible that LLM merely generates generic feedback applicable to multiple papers? A potential null model is that LLM mostly produces generic feedback applicable to many papers. To test this hypothesis, we performed a shuffling experiment aimed at verifying the specificity and relevance of LLM generated feedback. For each paper in the Nature family journal data, the LLM feedback was shuffled for papers from the same journal and within the same Nature category (Methods). If the LLM were producing only generic feedback, we would observe no decrease in the pairwise overlap between shuffled LLM feedback and human feedback. In contrast, the pairwise overlap significantly decreased from $30.85 \%$ to $0.43 \%$ after shuffling (Fig. 2a). A similar drop from $39.23 \%$ to $3.91 \%$ was observed on $I C L R$ (Fig. 2b). These results suggest that LLM feedback is paper-specific.</p>
<h2>LLM is consistent with humans on major comments</h2>
<p>What characteristics do LLMs' comments exhibit? What are the distinctive features of the human comments that align with LLMs'? Here we evaluate the unique characteristics of comments generated by LLMs. Our analysis revealed that comments identified by multiple human reviewers are more likely to be echoed by LLMs. For instance, in the Nature family journal data (Fig. 2e), a comment raised by a single human reviewer had an $11.39 \%$ chance of being identified by LLMs. This probability increased to $20.67 \%$ for comments raised by two reviewers, and further to $31.67 \%$ for comments raised by three or more reviewers. A similar trend was observed in the $I C L R$ data (Fig. 2f), where the likelihood of LLMs identifying a comment increased from $15.39 \%$ for a single reviewer to $26.21 \%$ for two reviewers, and $39.33 \%$ for three or more reviewers. These findings suggest that LLMs are more likely to identify common issues or flaws that are consistently recognized by multiple human reviewers, compared to specific comments raised by a single reviewer. This alignment of LLM with human perspectives indicates its ability to identify what is generally considered as major or significant issues.</p>
<p>We further examined the likelihood of LLM comments overlapping with human feedback based on their position in the sequence, as earlier comments in human feedback (e.g. "concern 1") may represent more significant issues.</p>
<p>To this end, we divided each human reviewer's comment sequence into four quarters within the Nature journal data (Fig. 2g). Our findings suggest that comments raised in the first quarter of the review text are most likely (21.23\%) to overlap with LLM comments, with subsequent quarters revealing decreasing likelihoods ( $16.74 \%$ for the second quarter). Similar trends were observed in the ICLR papers data, where earlier comments in the sequence showed a higher probability of overlap with LLM comments (Fig. 2h). These findings further support that LLM tends to align with human perspectives on what is generally considered as major or significant issues.</p>
<h1>LLM feedback emphasizes certain aspects more than humans</h1>
<p>We next analyzed whether certain aspects of feedback are more/less likely to be raised by the LLM and human reviewers. We focus on ICLR for this analysis, as it's more homogeneous than Nature family journals, making it easier to categorize the main aspects of review. Drawing on existing research in peer review literature within the machine learning domain ${ }^{41-44}$, we developed a schema comprising 11 distinct aspects of comments. We then performed human annotation on a randomly sampled subset (Methods).</p>
<p>Fig. 3 presents the relative frequency of each of the 11 aspects of comments raised by humans and LLM. LLM comments on the implications of research 7.27 times more frequently than humans do. Conversely, LLM is 10.69 times less likely to comment on novelty than humans are. While both LLM and humans often suggest additional experiments, their focuses differ: humans are 6.71 times more likely than LLM to request more ablation experiments, whereas LLM is 2.19 times more likely than humans to request experiments on more datasets. These findings suggest that the emphasis put on certain aspects of comments varies between LLMs and human reviewers. This variation highlights the potential advantages that a human-AI collaboration could provide. Rather than having LLM fully automate the scientific feedback process, humans can raise important points that LLM may overlook. Similarly, LLM could supplement human feedback by providing more comprehensive comments.</p>
<h2>Prospective User Study and Survey</h2>
<p>Taken together, our retrospective evaluations above suggest that LLMs can generate scientific feedback that focuses on similar aspects as human reviewers. Yet, consistency in concerns is only one of the many factors contributing to the utility of scientific feedback. Recent studies in human-AI interaction have also identified additional factors that individuals may consider when evaluating and adopting AI-based tools ${ }^{21}$, prompting us to ask: how do scientific researchers respond to feedback generated by LLMs? Thus, we launched a survey study on 308 researchers from 110 US institutions who opted in to receive LLM-generated scientific feedback on their own papers, and were asked to evaluate its utility and performance. While our sampling approach is subject to biases of self-selection, the data can provide valuable insights and subjective perspectives from researchers complementing our retrospective analysis ${ }^{45,46}$. The results from the user study are illustrated in Fig. 4.</p>
<p>Our user study provides additional evidence that is largely consistent with retrospective evaluations. First, the user study survey results corroborate the findings from the retrospective evaluation on significant overlaps between LLM feedback and human feedback: more than $70 \%$ of participants think there is at least "partial alignment" between LLM feedback and what they think/would expect on the significant points and issues with their paper, and $35 \%$ of participants think the alignment is considerable or substantial (Fig. 4b). Second, the survey study further corroborates the findings from the automated evaluation on the ability of the language model to generate non-generic feedback: $32.9 \%$ of participants think our system-generated feedback is "less specific than many, but more specific than some peer reviewers", while $17.3 \%$ and $14 \%$ think it is "about as specific as peer reviewers", or "more specific than many peer reviewers", further corroborating that LLMs can generate non-generic reviews (Fig. 4d).</p>
<h2>Researchers find LLM feedback helpful</h2>
<p>Participants were surveyed about the extent to which they found the LLM feedback helpful in improving their work or understanding of a subject. The majority responded positively, with over $50.3 \%$ considering the feedback to be helpful, and $7.1 \%$ considering it to be very helpful (Fig. 4a). When compared with human feedback, while $17.5 \%$ of participants considered it to be inferior to human feedback, $41.9 \%$ considered it to be less helpful than many, but more helpful than some human feedback. Additionally, $20.1 \%$ considered it to be about the same level of helpfulness as human feedback, and $20.4 \%$ considered it to be even more helpful than human feedback (Fig. 4c). Our evaluation</p>
<p>also revealed that the perceptions of alignment and helpfulness were consistent across various demographic groups. Individuals from different educational backgrounds, ranging from undergraduate to postgraduate levels, found the feedback equally helpful and aligned with human feedback. Similarly, whether an experienced or a novice researcher, participants across the spectrum of publishing and reviewing experience reported similar levels of satisfaction and utility from the LLM based feedback, indicating that LLM based feedback tools could potentially be helpful to a diverse range of population (Supp. Fig. 3,4).</p>
<p>In line with the helpfulness of the system, $50.5 \%$ of survey participants further expressed their willingness to reuse the system (Fig. $4 g$ ). The participants expressed optimism about the potential improvements that continued use of the system could bring to the traditional human feedback process (Fig. $4 e, f$ ). They believe that the LLM technology can further refine the quality of reviews and possibly introduce new capabilities. Interestingly, the evaluation also revealed that participants believe authors are more likely to benefit from LLM based feedback than other stakeholders such as reviewers, and area chairs (Fig. 4h). Many participants envisioned a timely feedback tool for authors to receive comments on their papers in a timely manner, e.g. one participant wrote, "The review took five minutes and was of a reasonably high quality. This can tremendously help authors to receive a fast turnaround feedback and help in polishing their submissions." Another participant wrote, "After writing a paper or a review, GPT could help me gain another perspective to re-check the paper."</p>
<h1>LLM could generate novel feedback not mentioned by humans.</h1>
<p>Beyond generating feedback that aligns with humans, our results also suggest that LLM could potentially generate useful feedback that has not been mentioned by humans, e.g., $65.3 \%$ of participants think at least to some extent LLM feedback offers perspectives that have been overlooked or underemphasized by humans. Several participants mentioned that:</p>
<ul>
<li>"It consists more points, covering aspects which human may forget to think about."</li>
<li>"It actually highlighted a few limitations which human reviewers didn't point out to, but as authors we were aware of it and were expecting it. But this GPT figured out some of them, so that's interesting."</li>
<li>"The GPT-generated review suggested me to do visualization to make a more concrete case for interpretability. It also asked to address data privacy issues. Both are important, and human reviewers missed this point."</li>
</ul>
<h2>Limitations of LLM feedback</h2>
<p>Study participants also discussed limitations of the current system. The most important limitation is its ability to generate specific and actionable feedback, e.g.</p>
<ul>
<li>"Potential Reasons are too vague and not domain specific."</li>
<li>"GPT cannot provide specific technical areas for improvement, making it potentially difficult to improve the paper."</li>
<li>"The reviews crucially lacked much in-depth critique of model architecture and design, something actual reviewers would be able to comment on given their likely considerable experience in fields closely related to the focus of the paper."
As such, one future direction to improve the LLM based scientific feedback system is to nudge the system towards generating more concrete and actionable feedback, e.g. through pointing to specific missing work, experiments to add. As one participant nicely summarized:</li>
<li>"(large language model generated) reviews were less about the content and more about the testing regime as well as less ML details-focused, but this is okay as it still gave relevant and actionable advice on areas of improvement in terms of paper layout and presenting results. GPT-generated reviews are especially useful here when less-experience authors may leave out details on implementation and construction or forget to thoroughly explain testing regime by providing pointers on areas to polish the paper in, potentially decreasing the number of review cycles before publication."</li>
</ul>
<h1>Discussion</h1>
<p>In this study, we characterized the usefulness and reliability of LLM in scientific evaluation by building and evaluating an LLM-based scientific feedback generation framework. Through a combination of retrospective (comparing LLM feedback with human feedback from the peer review process) and prospective evaluation design (user study with researchers), we have seen a substantial level of overlap and positive user perceptions regarding the usefulness of LLM feedback. Furthermore, in evaluating user perceptions, we found that a majority of participants regarded LLM feedback as useful in the manuscript improvement process, and sometimes LLM could bring up novel points not covered by humans. The positive feedback from users highlights the potential value and utility of leveraging LLM feedback as a valuable resource for authors seeking constructive feedback and suggestions for enhancing their manuscripts. This could be especially helpful for researchers who lack access to timely quality feedback mechanisms, e.g., researchers from traditionally underprivileged regions who may not have resources to access conferences, or even peer review (their works are much more likely than those of "mainstream" researchers to get desk rejected by journals and thus seldom go through the peer review process ${ }^{14}$ ). For others, the framework could be used as a mechanism for authors to self-check and improve their work in a timely manner, especially in an age of exponentially growing scientific papers and increasing challenges to secure timely and quality peer reviewer feedback. Our analysis suggests that people from diverse educational backgrounds and publishing experience can find the LLM scientific feedback generation framework useful (Supp. Fig. 3,4).</p>
<p>Despite the potential of LLMs in providing timely and helpful scientific feedback, it is important to note that expert human feedback will still be the cornerstone of rigorous scientific evaluation. As demonstrated in our findings, our analysis reveals limitations of the framework, e.g., LLM is biased towards certain aspects of scientific feedback (e.g., "add experiments on more datasets"), and sometimes feels "generic" to the authors (while participants also indicate that quite often human reviewers are "generic"). While comparable and even better than some reviewers, the current LLM feedback cannot substitute specific and thoughtful human feedback by domain experts.</p>
<p>It is also important to note the potential misuse of LLM for scientific feedback. We argue that LLM feedback should be primarily used by researchers identify areas of improvements in their manuscripts prior to official submission. It is important that expert human reviewers should deeply engage with the manuscripts and provide independent assessment without relying on LLM feedback. Automatically generating reviews without thoroughly reading the manuscript would undermine the rigorous evaluation process that forms the bedrock of scientific progress.</p>
<p>More broadly, our study contributes to the recent discussions on the impacts of LLM and generative AI on existing work practices. Researchers have discussed the potential of LLM to improve productivity ${ }^{47,48}$, creativity ${ }^{49}$, and facilitate scientific discovery ${ }^{50}$. We envision that LLM and generative AI, if deployed responsibly, could also potentially bring a paradigm change to how researchers conduct research, collaborate, and provide evaluations, influencing the way science and technology advance. Our work brings a preliminary investigation into such potentials through a concrete prototype for scientific feedback.</p>
<p>There are several limitations to our study that are important to highlight. First, our results are based on one specific instantiation of scientific feedback from LLM, i.e., our framework is based on the GPT-4 model, enabled by a specific prompt. While we have spent significant efforts in improving the performance of our GPT-4 feedback pipeline (and achieved reasonable utility), the results should be interpreted as a lower bound, rather than an upper bound, on the potential of leveraging LLMs for scientific feedback. Moreover, our system only leverages zero-shot learning of GPT-4 without fine-tuning on additional datasets. Further, the architecture and prompt used in our study only represent one of the many possible forms of LLM-based scientific feedback. Aside from exploring other LLMs and conducting more sophisticated prompt engineering, future work could incorporate labeled datasets of "high quality scientific feedback" to further fine-tune the LLM, or prompt LLM to leverage tools (e.g., fact check API, algorithm analysis API) so that the feedback could be more detailed and method-specific. Nevertheless, our proposed framework proves to be helpful and aligns well with comments brought up by human reviewers, demonstrating the potential of incorporating LLM in the scientific evaluation process, echoing prior works arguing for AI in the scientific process ${ }^{50}$. Our retrospective evaluation used Nature family data and ICLR data. While these data cover a range of scientific fields, including biology, computer science, etc., and the ICLR dataset includes both accepted and</p>
<p>rejected papers, all studied papers are targeted at top venues in English. Future work should further evaluate the framework with more coverage. Our user study is limited in coverage of participant population and suffer from a self-selection issue. Current results are based on responses from researchers in machine learning and computational biology, and while we aim to ensure representativeness of researchers by reaching out to randomly sampled authors who upload preprints to arXiv in recent months, participants who opt into the study are likely to be interested and familiar with LLM or AI in general. Finally, the current version of the GPT-4 model we utilized does not possess the capability to understand or interpret visual data such as tables, graphs, and figures, which are integral components of scientific literature. Future iterations could explore integrating visual LLMs or specialized modules that can comprehend and critique visual elements, thereby offering a more comprehensive form of scientific feedback.</p>
<p>One direction for future work is to explore the extent to which the proposed approach can help identify and correct errors in scientific papers. This would involve artificially introducing various types of errors, including typos, mistakes in data analyses, and errors in mathematical equations. By evaluating whether LLM-generated feedback can effectively detect and rectify these errors, we can gain further insights into the system's ability to improve the overall accuracy and quality of scientific manuscripts. Furthermore, investigating the limitations and challenges associated with error detection and correction by LLM is crucial. This includes understanding the types of errors that may be more challenging for the model to detect and correct, as well as evaluating the potential impact of false positives or false negatives in the generated feedback. Such insights can inform the development of more robust and accurate AI-assisted review systems. Additionally, we intend to broaden the scope of evaluated scientific papers to include manuscripts written in languages other than English, or by authors for whom English is not their first language, in order to understand whether LLMs can provide useful feedback for such papers.</p>
<h1>Methods</h1>
<h2>Overview of the Nature Family Journals Dataset</h2>
<p>Several journals within the Nature group have adopted a transparent peer review policy, enabling authors to publish reviewers' comments alongside the accepted papers ${ }^{51}$. For instance, by 2021, approximately $46 \%$ of Nature authors opted to make their reviewer discussions public ${ }^{52}$. Our dataset comprises papers from 15 Nature family journals, published between January 1, 2022, and June 17, 2023. We sourced papers from 15 Nature family journals, focusing on those published between January 1, 2022, and June 17, 2023. Within this period, our dataset includes 773 accepted papers from Nature with 2,324 reviews, 810 sampled accepted papers from Nature Communications with 2,250 reviews, and many others. In total, our dataset includes 3,096 accepted papers and 8,745 reviews (Supp. Table 1). The data were sourced directly from the Nature website (https://nature.com/).</p>
<h2>Overview of the ICLR Dataset</h2>
<p>The International Conference on Learning Representations (ICLR) is a leading publication venue in the machine learning field. ICLR implements an open review policy, making reviews for all papers accessible, including those for rejected papers. Accepted papers at ICLR are categorized into Oral presentations (top 5\% of papers), Spotlight (top 25\%), and Poster presentations. In 2022, ICLR received 3,407 submissions, which increased to 4,966 by 2023. Using a stratified sampling method, we included 55 Oral (with 200 reviews), 173 Spotlight (664 reviews), 197 Poster ( 752 reviews), 213 rejected ( 842 reviews), and 182 withdrawn ( 710 reviews) papers from 2022. For 2023, we included 90 Oral ( 317 reviews), 200 Spotlight ( 758 reviews), 200 Poster ( 760 reviews), 212 rejected ( 799 reviews), and 187 withdrawn ( 703 reviews) papers. The dataset comprises 1709 papers and 6,506 reviews in total (Supp. Table 2). The paper PDFs and corresponding reviews were retrieved using the OpenReview API (https://docs.openreview.net/).</p>
<h2>Generating Scientific Feedbacks using LLM</h2>
<p>We prototyped a pipeline to generate scientific feedback using OpenAI's GPT-4 ${ }^{19}$ (Fig. 1a). The system's input was the academic paper in PDF format, which was then parsed with the machine-learning-based ScienceBeam PDF parser ${ }^{53}$. Given the token constraint of GPT-4, which allows 8,192 tokens for combined input and output, the initial 6,500 tokens of the extracted title, abstract, figure and table captions, and main text were utilized to construct</p>
<p>the prompt for GPT-4 (Supp. Fig. 5). This token limit exceeds the 5,841.46-token average of $I C L R$ papers and covers over half of the 12,444.06-token average for Nature family journal papers (Supp. Table 4). For clarity and simplicity, we instructed GPT-4 to generate a structured outline of scientific feedback. Following the reviewer report instructions from machine learning conferences ${ }^{29-33}$ and Nature family journals ${ }^{27,28}$, we provided specific instructions to generate four feedback sections: significance and novelty, potential reasons for acceptance, potential reasons for rejection, suggestions for improvement (Supp. Fig. 12). The feedback for each paper was generated by GPT-4 in a single pass.</p>
<h1>Retrospective Extraction and Matching of Comments from Scientific Feedback</h1>
<p>To evaluate the overlap between LLM feedback and human feedback, we developed a two-stage comment matching pipeline (Supp. Fig. 6). In the first stage, we employed an extractive text summarization approach ${ }^{34-37}$. Each feedback text, either from the LLM or a human, was processed by GPT-4 to extract a list of the points of comments raised in the text (see prompt in Supp. Fig. 13). The output was structured in a JSON (JavaScript Object Notation) format. Within this format, each JSON key assigns an ID to a specific point, while the corresponding value details the content of the point (Supp. Fig. 13). We focused on criticisms in the feedback, as they provide direct feedback to help authors improve their papers ${ }^{34}$. The second stage focused on semantic text matching ${ }^{38-40}$. Here, we input both the JSON-formatted feedback from the LLM and the human into GPT-4. The LLM then generated another JSON output where each key identified a pair of matching point IDs and the associated value provided the explanation for the match. Given that our preliminary experiments showed GPT-4's matching to be lenient, we introduced a similarity rating mechanism. In addition to identifying corresponding pairs of matched comments, GPT-4 was also tasked with self-assessing match similarities on a scale from 5 to 10 (Supp. Fig. 14). We observed that matches graded as " 5 . Somewhat Related" or " 6 . Moderately Related" introduced variability that did not always align with human evaluations. Therefore, we only retained matches ranked " 7 . Strongly Related" or above for subsequent analyses.</p>
<p>We validated our retrospective comment matching pipeline using human verification. In the extractive text summarization stage, we randomly selected 639 pieces of scientific feedback, including 150 from the LLM and 489 from human contributors. Two co-authors assessed each feedback and its corresponding list of extracted comments, identifying true positives (correctly extracted comments), false negatives (missed relevant comments), and false positives (incorrectly extracted or split comments). This process resulted in an F1 score of 0.968 , with a precision of 0.977 and a recall of 0.960 (Supp. Table 3a), demonstrating the accuracy of the extractive summarization stage. For the semantic text matching stage, we sampled 760 pairs of scientific feedbacks: 332 comparing GPT to Human feedback and 428 comparing Human feedbacks. Each feedback pair was processed to enumerate all potential pairings of their extracted comment lists, resulting in 12,035 comment pairs. Three co-authors independently determined whether the comment pairs matched, without referencing the pipeline's predictions. Comparing these annotations with pipeline outputs yielded an F1 score of 0.824 , a recall of 0.878 , and a precision of 0.777 (Supp. Table 3b). To assess inter-annotator agreement, we collected three annotations for 800 randomly selected comment pairs. Given the prevalence of non-matches, we employed stratified sampling, drawing 400 pairs identified as matches by the pipeline and 400 as non-matches. We then calculated pairwise agreement between annotations and the F1 score for each annotation against the majority consensus. The data showed $89.8 \%$ pairwise agreement and an F1 score of $88.7 \%$, indicating the reliability of the semantic text matching stage.</p>
<h2>Evaluating Specificity of LLM Feedback through Review Shuffling</h2>
<p>To evaluate the specificity of the feedback generated by the LLM, we compared the overlap between human-authored feedback and shuffled LLM feedback. For papers published in the Nature journal family, the LLM-generated feedback for a given paper was randomly paired with human feedback for a different paper from the same journal and Nature root category. These categories included physical sciences, earth and environmental sciences, biological sciences, health sciences, and scientific community and society. If a paper was classified under multiple categories, the shuffle algorithm paired it with another paper that spanned the same categories. For the $I C L R$ dataset, we compared human feedback for a paper with LLM feedback for a different paper, randomly selected from the same</p>
<p>conference year, either $I C L R 2022$ or $I C L R 2023$. This shuffling procedure was designed to test the null hypothesis: if LLM mostly produces generic feedback applicable to many papers, then there would be little drop in the pairwise overlap between LLM feedback and the comments from each individual reviewer after the shuffling.</p>
<h1>Overlap Metrics for Retrospective Evaluations and Control</h1>
<p>In the retrospective evaluation, we assessed the pairwise overlap of both GPT-4 vs. Human and Human vs. Human in terms of hit rate (Fig. 2). The hit rate, defined as the proportion of comments in set $A$ that match those in set $B$, was calculated as follows:</p>
<p>$$
\text { Hit Rate }=\frac{|A \cap B|}{|A|}
$$</p>
<p>To facilitate a direct comparison between the hit rates of GPT-4 vs. Human and Human vs. Human, we controlled for the number of comments when measuring the hit rate for Human vs. Human. Specifically, we considered only the first $N$ comments made by the first human (i.e., the human comments used as set $A$ ) for matching, where $N$ is the number of comments made by GPT-4 for the same paper. The results, with and without this control, were largely similar across both the $I C L R$ dataset for different decision outcomes (Supp. Fig. 7,10) and the Nature family journals dataset across different journals (Supp. Fig. 8,9,10). To examine the robustness of the results across different set overlap metrics, we also evaluated three additional metrics: the Szymkiewicz-Simpson overlap coefficient, the Jaccard index, and the Sørensen-Dice coefficient. These were calculated as follows:</p>
<p>$$
\begin{aligned}
\text { Szymkiewicz-Simpson Overlap Coefficient } &amp; =\frac{|A \cap B|}{\min (|A|,|B|)} \
\text { Jaccard Index } &amp; =\frac{|A \cap B|}{|A \cup B|} \
\text { Sørensen-Dice Coefficient } &amp; =\frac{2|A \cap B|}{|A|+|B|}
\end{aligned}
$$</p>
<p>Results on these additional metrics suggest that our findings are robust on different set overlap metrics: the overlap GPT-4 vs. Human appears comparable to those of Human vs. Human both with and without control for the number of comments (Supp. Fig. 2).</p>
<h2>Characterizing the comment aspects in human and LLM feedback</h2>
<p>We curated an annotation schema of 11 key aspects to identify and measure the prevalence of these aspects in human and LLM feedback. This schema was developed with a focus on the $I C L R$ dataset, due to its specialized emphasis on Machine Learning. Each aspect was defined by its underlying emphasis, such as novelty, research implications, suggestions for additional experiments, and more. The selection of these 11 key aspects was based on a combination of the common schemes identified in the literature within the machine learning domain ${ }^{41-44}$, comments from machine learning researchers, and initial exploration by the annotators. From the $I C L R$ dataset, a random sample of 500 papers was selected to ensure a broad yet manageable representation. Using our extractive text summarization pipeline, we extracted lists of comments from both the LLM and human feedback for each paper. Each comment was then annotated according to our predefined schema, identifying any of the 11 aspects it represented (Supp. Table 5,6,7). To ensure annotation reliability, two researchers with a background in machine learning performed the annotations.</p>
<h2>Prospective User Study and Survey</h2>
<p>We conduct a prospective user study to further validate the effectiveness of leveraging LLMs to generate scientific feedback. To facilitate our user study, we launched an online Gradio demo ${ }^{55}$ of the aforementioned generation pipeline, accessible at a public URL (Supp. Fig. 11). Users are prompted to upload a research paper in its original PDF format, after which the system delivers the review to user's email. We ask users to only upload papers published after 9/2021 to ensure the papers are never seen by GPT-4 during training (the cutdown date of GPT-4 training corpora is $9 / 2021$ ). We have also incorporated an ethics statement to discourage the direct use of LLM content for any</p>
<p>review-related tasks. After the review is generated and sent to users, users are asked to fill a 6-page survey (Figure 4), which includes 1) author background information, 2) review situation in author's area, 3) general impression of LLM review, 4) detailed evaluation of LLM review, 5) comparison with human review, and 6) additional questions and feedback, which systematically investigates human evaluations of different aspects of LLM reviews. The survey takes around 15-20 minutes and users will be compensated with $\$ 20$. We recruit the participants through 1) relevant institute mailing lists, and 2) reaching out to all authors who have published at least one preprint on arXiv in the field of computer science and computational biology during January to March, 2023, provided their email contact information is available in the first three pages of the PDF. The study has been approved by Stanford University's Institutional Review Board.</p>
<h1>Acknowledgements</h1>
<p>We thank S. Eyuboglu, D. Jurafsky, and M. Bernstein for their guidance and helpful discussions. J.Z. is supported by the National Science Foundation (CCF 1763191 and CAREER 1942926), the US National Institutes of Health (P30AG059307 and U01MH098953) and grants from the Silicon Valley Foundation and the Chan-Zuckerberg Initiative. H.C. is supported by the Stanford Interdisciplinary Graduate Fellowship.</p>
<h2>Code Availability</h2>
<p>The codes can be accessed at https://github.com/Weixin-Liang/LLM-scientific-feedback.</p>
<h2>References</h2>
<ol>
<li>Shannon, C. E. A mathematical theory of communication. The Bell system technical journal 27, 379-423 (1948).</li>
<li>Tribus, M. \&amp; McIrvine, E. C. Energy and information. Sci. Am. 225, 179-190 (1971).</li>
<li>Kuhn, T. S. The structure of scientifi revolutions. The Un Chic. Press. 2, 90 (1962).</li>
<li>Horbach, S. P. \&amp; Halffman, W. The changing forms and expectations of peer review. Res. integrity peer review 3, 1-15 (2018).</li>
<li>Price, D. J. D. S. Little science, big science (Columbia University Press, 1963).</li>
<li>Jones, B. F. The burden of knowledge and the "death of the renaissance man": Is innovation getting harder? The Rev. Econ. Stud. 76, 283-317 (2009).</li>
<li>Aczel, B., Szaszi, B. \&amp; Holcombe, A. O. A billion-dollar donation: estimating the cost of researchers' time spent on peer review. Res. Integr. Peer Rev. 6, 1-8 (2021).</li>
<li>Alberts, B., Hanson, B. \&amp; Kelner, K. L. Reviewing peer review (2008).</li>
<li>Björk, B.-C. \&amp; Solomon, D. The publishing delay in scholarly peer-reviewed journals. J. informetrics 7, 914-923 (2013).</li>
<li>Lee, C. J., Sugimoto, C. R., Zhang, G. \&amp; Cronin, B. Bias in peer review. J. Am. Soc. for information Sci. Technol. 64, 2-17 (2013).</li>
<li>Kovanis, M., Porcher, R., Ravaud, P. \&amp; Trinquart, L. The global burden of journal peer review in the biomedical literature: Strong imbalance in the collective enterprise. PloS one 11, e0166387 (2016).</li>
<li>Shah, N. B. Challenges, experiments, and computational solutions in peer review. Commun. ACM 65, 76-87 (2022).</li>
<li>Bourdieu, P. Cultural reproduction and social reproduction. In Knowledge, education, and cultural change, 71-112 (Routledge, 2018).</li>
<li>
<p>Merton, R. K. The matthew effect in science: The reward and communication systems of science are considered. Science 159, 56-63 (1968).</p>
</li>
<li>
<p>Chu, J. S. \&amp; Evans, J. A. Slowed canonical progress in large fields of science. Proc. Natl. Acad. Sci. 118, e2021636118 (2021).</p>
</li>
<li>Bloom, N., Jones, C. I., Van Reenen, J. \&amp; Webb, M. Are ideas getting harder to find? Am. Econ. Rev. 110, $1104-1144$ (2020).</li>
<li>Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, 1877-1901 (2020).</li>
<li>Ouyang, L. et al. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst. 35, 27730-27744 (2022).</li>
<li>OpenAI. GPT-4 Technical Report. ArXiv abs/2303.08774 (2023).</li>
<li>Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA internal medicine (2023).</li>
<li>Lee, M. et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746 (2022).</li>
<li>Kung, T. H. et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health 2, e0000198 (2023).</li>
<li>Terwiesch, C. Would chat gpt3 get a wharton mba? a prediction based on its performance in the operations management course. Mack Inst. for Innov. Manag. at Whart. Sch. Univ. Pennsylvania (2023).</li>
<li>Schulz, R. et al. Is the future of peer review automated? BMC Res. Notes 15, 1-5 (2022).</li>
<li>Liu, R. \&amp; Shah, N. B. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622 (2023).</li>
<li>Robertson, Z. Gpt4 is slightly helpful for peer-review assistance: A pilot study. arXiv preprint arXiv:2307.05492 (2023).</li>
<li>Nature. How to Write a Report. https://www.nature.com/nature/for-referees/how-to-write-a-report. Accessed: 21 September 2023.</li>
<li>Nature Communications. Writing Your Report. https://www.nature.com/ncomms/for-reviewers/ writing-your-report. Accessed: 21 September 2023.</li>
<li>Rogers, A. \&amp; Augenstein, I. How to review for ACL Rolling Review. https://aclrollingreview.org/reviewertutorial (2021).</li>
<li>Association for Computational Linguistics. ACL'23 Peer Review Policies. https://2023.aclweb.org/blog/ review-acl23/ (2023).</li>
<li>Association for Computational Linguistics. ACL-IJCNLP 2021 Instructions for Reviewers. https://2021.aclweb. org/blog/instructions-for-reviewers/ (2021).</li>
<li>International Conference on Machine Learning. ICML 2023 Reviewer Tutorial. https://icml.cc/Conferences/ 2023/ReviewerTutorial (2023).</li>
<li>Nicholas, K. A. \&amp; Gordon, W. S. A quick guide to writing a solid peer review. Eos, Transactions Am. Geophys. Union 92, 233-234 (2011).</li>
<li>Luhn, H. P. The automatic creation of literature abstracts. IBM J. research development 2, 159-165 (1958).</li>
<li>Edmundson, H. P. New methods in automatic extracting. J. ACM (JACM) 16, 264-285 (1969).</li>
<li>Mihalcea, R. \&amp; Tarau, P. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, 404-411 (2004).</li>
<li>Erkan, G. \&amp; Radev, D. R. Lexrank: Graph-based lexical centrality as salience in text summarization. J. artificial intelligence research 22, 457-479 (2004).</li>
<li>
<p>Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K. \&amp; Harshman, R. Indexing by latent semantic analysis. J. Am. society for information science 41, 391-407 (1990).</p>
</li>
<li>
<p>Socher, R., Huang, E., Pennin, J., Manning, C. D. \&amp; Ng, A. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Adv. neural information processing systems 24 (2011).</p>
</li>
<li>Bowman, S. R., Angeli, G., Potts, C. \&amp; Manning, C. D. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326 (2015).</li>
<li>Birhane, A. et al. The values encoded in machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, 173-184, DOI: 10.1145/3531146. 3533083 (Association for Computing Machinery, New York, NY, USA, 2022).</li>
<li>Smith, J. J., Amershi, S., Barocas, S., Wallach, H. \&amp; Wortman Vaughan, J. Real ml: Recognizing, exploring, and articulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 587-597 (2022).</li>
<li>Koch, B., Denton, E., Hanna, A. \&amp; Foster, J. G. Reduced, reused and recycled: The life of a dataset in machine learning research. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021).</li>
<li>Scheuerman, M. K., Hanna, A. \&amp; Denton, E. Do datasets have politics? disciplinary values in computer vision dataset development. Proc. ACM on Human-Computer Interact. 5, 1-37 (2021).</li>
<li>Meyer, B. D., Mok, W. K. \&amp; Sullivan, J. X. Household surveys in crisis. J. Econ. Perspectives 29, 199-226 (2015).</li>
<li>Ross, M. B. et al. Women are credited less in science than men. Nature 608, 135-145 (2022).</li>
<li>Noy, S. \&amp; Zhang, W. Experimental evidence on the productivity effects of generative artificial intelligence. Available at SSRN 4375283 (2023).</li>
<li>Peng, S., Kalliamvakou, E., Cihon, P. \&amp; Demirer, M. The impact of ai on developer productivity: Evidence from github copilot. arXiv preprint arXiv:2302.06590 (2023).</li>
<li>Epstein, Z. et al. Art and the science of generative ai. Science 380, 1110-1111 (2023).</li>
<li>Wang, H. et al. Scientific discovery in the age of artificial intelligence. Nature 620, 47-60 (2023).</li>
<li>Nature will publish peer review reports as a trial. Nature 578, 8, DOI: 10.1038/d41586-020-00309-9 (2020).</li>
<li>Nature is trialling transparent peer review - the early results are encouraging. Nature 603, 8, DOI: 10.1038/ d41586-022-00493-w (2022).</li>
<li>Ecer, D. \&amp; Maciocci, G. Sciencebeam—using computer vision to extract pdf data. Elife Blog Post (2017). [Online; accessed 2023-Sep-8].</li>
<li>Goodman, S. N., Berlin, J., Fletcher, S. W. \&amp; Fletcher, R. H. Manuscript quality before and after peer review and editing at annals of internal medicine. Annals internal medicine 121, 11-21 (1994).</li>
<li>Abid, A. et al. An online platform for interactive feedback in biomedical machine learning. Nat. Mach. Intell. 2, 86-88 (2020).</li>
<li>Collins, E., Augenstein, I. \&amp; Riedel, S. A supervised approach to extractive summarisation of scientific papers. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), 195-205 (2017).</li>
<li>Nuijten, M. B., Hartgerink, C. H., Van Assen, M. A., Epskamp, S. \&amp; Wicherts, J. M. The prevalence of statistical reporting errors in psychology (1985-2013). Behav. research methods 48, 1205-1226 (2016).</li>
<li>Zhou, Y., Beltagy, I., Bethard, S., Cotterell, R. \&amp; Chakraborty, T. ACL pubcheck. https://github.com/acl-org/ aclpubcheck.</li>
<li>
<p>Zhang, J., Zhang, H., Deng, Z. \&amp; Roth, D. Investigating fairness disparities in peer review: A language model enhanced approach. arXiv preprint arXiv:2211.06398 (2022).</p>
</li>
<li>
<p>Hosseini, M. \&amp; Horbach, S. P. Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. Res. Integr. Peer Rev. 8, 4 (2023).</p>
</li>
<li>Verharen, J. P. Chatgpt identifies gender disparities in scientific peer review. bioRxiv 2023-07 (2023).</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Characterizing the capability of LLM in providing helpful feedback to researchers. a, Pipeline for generating LLM scientific feedback using GPT-4. Given a PDF, we parse and extract the paper's title, abstract, figure and table captions, and main text to construct the prompt. We then prompt GPT-4 to provide structured comments with four sections, following the feedback structure of leading interdisciplinary journals and conferences: significance and novelty, potential reasons for acceptance, potential reasons for rejection, and suggestions for improvement. b, Retrospective analysis of LLM feedback on 3,096 Nature family papers and 1,709 ICLR papers. We systematically compare LLM feedback with human feedback using a two-stage comment matching pipeline. The pipeline first performs extractive text summarization to extract the points of comments raised in LLM and human-written feedback respectively, and then performs semantic text matching to match the points of shared comments between LLM and human feedback. c, Prospective user study survey with 308 researchers from 110 US institutions in the field of AI and computational biology. Each researcher uploaded a paper they authored, and filled out a survey on the LLM feedback generated for them.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Retrospective analysis of LLM and human scientific feedback. a, Retrospective overlap analysis between feedback from the LLM versus individual human reviewers on papers submitted to Nature Family Journals. Approximately one third ( $30.85 \%$ ) of GPT-4 raised comments overlap with the comments from an individual reviewer (hit rate). "GPT-4 (shuffle)" indicates feedback from GPT-4 for another randomly chosen paper from the same journal and category. As a null model, if LLM mostly produces generic feedback applicable to many papers, then there would be little drop in the pairwise overlap between LLM feedback and the comments from each individual reviewer after the shuffling. In contrast, the hit rate drops substantially from $57.55 \%$ to $1.13 \%$ after shuffling, indicating that the LLM feedback is paper-specific. b, In the International Conference on Learning Representations (ICLR), more than one third (39.23\%) of GPT-4 raised comments overlap with the comments from an individual reviewer. The shuffling experiment shows a similar result, indicating that the LLM feedback is paper-specific. c-d, The overlap between LLM feedback and human feedback appears comparable to the overlap observed between two human reviewers across Nature family journals (c) $(r=0.80, P=3.69 \times 10^{-4}$ ) and across $I C L R$ decision outcomes (d) $\left(r=0.98, P=3.28 \times 10^{-3}\right)$. e-f, Comments raised by multiple human reviewers are disproportionately more likely to be hit by GPT-4 on Nature Family Journals (e) and ICLR (f). The X-axis indicates the number of reviewers raising the comment. The Y-axis indicates the likelihood that a human reviewer comment matches a GPT-4 comment (GPT-4 recall rate). g-h, Comments presented at the beginning of a reviewer's feedback are more likely to be identified by GPT-4 on Nature Family Journals (g) and ICLR (h). The X-axis indicates a comment's position in the sequence of comments raised by the human reviewer. Error bars represent $95 \%$ confidence intervals. ${ }^{<em>} \mathrm{P}&lt;0.05,{ }^{</em> <em>} \mathrm{P}&lt;0.01,{ }^{</em> * <em>} \mathrm{P}&lt;0.001$, and ${ }^{</em> * * *} \mathrm{P}&lt;0.0001$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. LLM based feedback emphasizes certain aspects more than humans. LLM comments on the implications of research 7.27 times more frequently than human reviewers. Conversely, LLM is 10.69 times less likely to comment on novelty compared to human reviewers. While both LLM and humans often suggest additional experiments, their focuses differ: human reviewers are 6.71 times more likely than LLM to request additional ablation experiments, whereas LLM is 2.19 times more likely than humans to request experiments on more datasets. Circle size indicates the prevalence of each aspect in human feedback.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Human study of LLM and human review feedback ( $n=308$ ). a-b, LLM generated feedback is generally helpful and has substantial overlaps with actual feedback from human reviewers. c-d, Compared to human feedback, LLM feedback is slightly less helpful and less specific. e-f, Users generally believe that the LLM feedback system can improve the accuracy and thoroughness of reviews, and reduce the workload of reviewers. g, Most users intend to use or potentially use the LLM feedback system again. h, Users believe that the LLM feedback system mostly helps authors, followed by reviewers and editors / area chairs. Numbers are percentages (\%).</p>
<h1>Supplementary Information</h1>
<h2>Additional Related Work</h2>
<p>The use of AI tools in aiding the scientific publication process has garnered considerable attention. Algorithms have been developed to summarize paper contents ${ }^{56}$, detect inaccurately reported p -values ${ }^{57}$, rectify citation errors ${ }^{58}$, and identify fairness disparities ${ }^{59}$. Recent advances in LLMs like ChatGPT and GPT-4 have intensified interest in leveraging these technologies for scientific feedback. There are some exploratory and unpublished studies. Hosseini et al. conducted a small-scale qualitative investigation to gauge ChatGPT's effectiveness in the peer review process ${ }^{60}$. Similarly, Robertson et al. involved 10 participants to assess GPT-4's benefits in aiding peer review ${ }^{26}$. Liu et al. demonstrated that GPT-4 could identify paper errors, verify checklists, and compare paper quality through analysis of 10-20 computer science papers ${ }^{25}$. Verharen et al. utilized ChatGPT to analyze 200 published neuroscience papers and uncovered gender disparities in scientific peer review ${ }^{61}$.</p>
<p>Our study differs from the existing literature in two key aspects. First, we provide a large-scale empirical analysis, in contrast to the small-scale exploratory analysis conducted in previous efforts. Our dataset includes 3,096 papers from the Nature family of journals and 1,709 papers from the ICLR conference, spanning a wide range of scientific disciplines. We also incorporate 308 human responses from 110 US institutions obtained through a prospective user study. Secondly, while most previous works only present qualitative results, our work provides a systematic quantitative assessment. This involves both overlap analyses and human evaluation metrics, providing a comprehensive analysis of the promise and limitations of LLMs in providing scientific feedback.</p>
<p>Supplementary Table 1. Summary of papers and their associated reviews sampled from 15 Nature family journals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Journal</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Papers</td>
<td style="text-align: center;">Reviews</td>
</tr>
<tr>
<td style="text-align: left;">Nature</td>
<td style="text-align: center;">773</td>
<td style="text-align: center;">2324</td>
</tr>
<tr>
<td style="text-align: left;">Nature Communications</td>
<td style="text-align: center;">810</td>
<td style="text-align: center;">2250</td>
</tr>
<tr>
<td style="text-align: left;">Communications Earth \&amp; Environment</td>
<td style="text-align: center;">299</td>
<td style="text-align: center;">807</td>
</tr>
<tr>
<td style="text-align: left;">Communications Biology</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">526</td>
</tr>
<tr>
<td style="text-align: left;">Communications Physics</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">464</td>
</tr>
<tr>
<td style="text-align: left;">Communications Medicine</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">343</td>
</tr>
<tr>
<td style="text-align: left;">Nature Ecology \&amp; Evolution</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">332</td>
</tr>
<tr>
<td style="text-align: left;">Nature Structural \&amp; Molecular Biology</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">290</td>
</tr>
<tr>
<td style="text-align: left;">Communications Chemistry</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">279</td>
</tr>
<tr>
<td style="text-align: left;">Nature Cell Biology</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">233</td>
</tr>
<tr>
<td style="text-align: left;">Nature Human Behaviour</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">211</td>
</tr>
<tr>
<td style="text-align: left;">Communications Materials</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">181</td>
</tr>
<tr>
<td style="text-align: left;">Nature Immunology</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">165</td>
</tr>
<tr>
<td style="text-align: left;">Nature Microbiology</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">174</td>
</tr>
<tr>
<td style="text-align: left;">Nature Biomedical Engineering</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">166</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">3096</td>
<td style="text-align: center;">8745</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 2. Summary of ICLR papers and their associated reviews sampled from the years 2022 and 2023, grouped by decision.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Decision Category</th>
<th style="text-align: center;">ICLR 2022</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ICLR 2023</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"># of Papers</td>
<td style="text-align: center;"># of Reviews</td>
<td style="text-align: center;"># of Papers</td>
<td style="text-align: center;"># of Reviews</td>
</tr>
<tr>
<td style="text-align: left;">Accept (Oral) - notable-top-5\%</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">317</td>
</tr>
<tr>
<td style="text-align: left;">Accept (Spotlight) - notable-top-25\%</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">664</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">758</td>
</tr>
<tr>
<td style="text-align: left;">Accept (Poster)</td>
<td style="text-align: center;">197</td>
<td style="text-align: center;">752</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">760</td>
</tr>
<tr>
<td style="text-align: left;">Reject after author rebuttal</td>
<td style="text-align: center;">213</td>
<td style="text-align: center;">842</td>
<td style="text-align: center;">212</td>
<td style="text-align: center;">799</td>
</tr>
<tr>
<td style="text-align: left;">Withdrawn after reviews</td>
<td style="text-align: center;">182</td>
<td style="text-align: center;">710</td>
<td style="text-align: center;">187</td>
<td style="text-align: center;">703</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">820</td>
<td style="text-align: center;">3168</td>
<td style="text-align: center;">889</td>
<td style="text-align: center;">3337</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For a more detailed literature review, see supplementary information.
${ }^{2}$ Focusing data after 2022 avoids bias introduced by 'testing on the training set', since GPT-4, the LLM we used, is trained on data up to Sep $2021^{19}$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
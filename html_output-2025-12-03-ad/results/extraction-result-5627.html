<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5627 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5627</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5627</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-20398b63519548ebe04b73918da99d491fa034aa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/20398b63519548ebe04b73918da99d491fa034aa" target="_blank">Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trained on human responses, and generative LLMs have high potential in generating training data for neural retrieval models.</p>
                <p><strong>Paper Abstract:</strong> We investigate the usefulness of generative Large Language Models (LLMs) in generating training data for cross-encoder re-rankers in a novel direction: generating synthetic documents instead of synthetic queries. We introduce a new dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models fine-tuned on LLM-generated and human-generated data. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public question collections with human responses and answers from ChatGPT. We fine-tune a range of cross-encoder re-rankers on either human-generated or ChatGPT-generated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trained on human responses. In a supervised setting, the human-trained re-rankers outperform the LLM-trained re-rankers. Our novel findings suggest that generative LLMs have high potential in generating training data for neural retrieval models. Further work is needed to determine the effect of factually wrong information in the generated responses and test our findings' generalizability with open-source LLMs. We release our data, code, and cross-encoders checkpoints for future work.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5627.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5627.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (as document generator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (used to generate synthetic documents/responses for IR re-rankers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses ChatGPT to generate synthetic documents/responses given queries to create a training corpus (ChatGPT-RetrievalQA) for fine-tuning cross-encoder re-rankers; the generated data is evaluated both in supervised and zero-shot retrieval settings across multiple domains (medicine, finance, Reddit, Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5 family, referred to as ChatGPT in HC3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative conversational large language model from OpenAI (GPT-3.5 / ChatGPT family) used via prompting to produce full textual responses to user queries; described in paper as a generative LLM producing lengthier and more lexically overlapping responses than humans. Exact training corpus and size not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Information retrieval / Question answering across subdomains (Medicine, Finance, Reddit long-form Q&A, Wikipedia/OpenQA)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate synthetic documents / full-text responses given natural language queries to serve as relevant passages/documents for training cross-encoder re-rankers (data augmentation for retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Downstream retrieval metrics assessing re-ranker effectiveness: MAP@1000, NDCG@10, MRR@10; additional diagnostics: BM25 MAP/NDCG/Recall on generated vs human responses; lexical-overlap percentage between query and response.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>When used to generate training data and then fine-tuning cross-encoders: zero-shot MiniLM trained on ChatGPT-generated data achieved (examples) on TREC DL'20: MAP=0.344, NDCG=0.539, MRR=0.978; on MS MARCO DEV: MAP=0.226, NDCG=0.267, MRR=0.218. BM25 retrieval is more effective on ChatGPT-generated responses than on human responses (Test: ChatGPT MAP=0.370, NDCG=0.396, Recall@1K=0.898 vs human MAP=0.143, NDCG=0.184, Recall@1K=0.520). Lexical overlap: average percent of query words appearing in ChatGPT responses = 34.6% vs human = 25.5%. (All numbers as reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Domain specificity (medical domain shows human-trained advantage), lexical overlap between query and generated response, length/repetition in generated responses (ChatGPT tends to repeat query and produce longer answers), train/test document overlap in supervised setting, architecture/size of downstream re-ranker (MiniLM vs TinyBERT), and potential factual errors/hallucinations in generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Domain effect: Table 3 shows MiniLM_human outperforms MiniLM_ChatGPT in most domains (e.g., Medicine MAP 0.397 vs 0.379). Lexical overlap: measured averages (34.6% vs 25.5%) and quartile increases reported. BM25 diagnostics: Table 4 shows BM25 performs much better on ChatGPT-generated responses than on human responses, correlating with higher lexical overlap and length. Model architecture: comparisons between MiniLM and TinyBERT fine-tuned on ChatGPT vs human data (Table 2 & 3) show differing gains. Data overlap: authors discuss that shared document collections between train/test may benefit human-trained models in supervised setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated retrieval evaluation: cross-encoder re-rankers fine-tuned on ChatGPT-generated vs human-generated responses were evaluated on (a) supervised setting using the ChatGPT-RetrievalQA test set and (b) zero-shot evaluation on MS MARCO Dev and TREC Deep Learning tracks (DL'19, DL'20). Metrics: MAP@1000, NDCG@10, MRR@10. Additional analyses: BM25 retrieval scores on generated vs human responses, lexical overlap statistics, evaluation on seen queries with unseen human-document collection (Table 5). Ground truth relevance in supervised experiments: human responses considered as relevant answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper notes ChatGPT can hallucinate and produce factually incorrect information (potential risk for using generated text directly in sensitive domains); in supervised/domain-specific settings (notably Medicine) models fine-tuned on human data outperform ChatGPT-trained models; BM25 and lexical analyses suggest ChatGPT output is easier to match lexically (not necessarily more factually accurate); generalizability to open-source LLMs is untested; shared train/test collections may have inflated supervised performance for human-trained re-rankers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons in paper: CE_ChatGPT (cross-encoders fine-tuned on ChatGPT-generated responses) vs CE_human (fine-tuned on human responses), across two re-ranker architectures (MiniLM 12-layer and TinyBERT 2-layer) and baseline BM25. CE_ChatGPT outperforms CE_human in zero-shot evaluations (e.g., MiniLM_ChatGPT higher on TREC DL'20 and MS MARCO Dev), whereas CE_human is slightly better in supervised/domain-specific scenarios (e.g., Medicine). Paper also contrasts the ChatGPT-document generation approach (this work) with prior LLM-based synthetic-query approaches (InPars, InPars-v2, Promptagator).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Authors recommend using generative LLMs like ChatGPT to augment training data for re-rankers, especially for zero-shot transfer and domains with limited labeled data, but caution: (1) evaluate and mitigate factual errors/hallucinations before using generated content in sensitive domains, (2) test generalizability with open-source LLMs, (3) consider separating train/test document collections to avoid overlap that may bias supervised evaluation, and (4) be aware that lexical overlap and response length will affect retrieval behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5627.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5627.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B (InPars-v2 mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B (used in InPars-v2 as an open-source LLM for synthetic data generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work (InPars-v2) used an open-source LLM, GPT-J-6B, to generate synthetic query-document pairs for data augmentation in retrieval; the current paper cites this as related work contrasting query-generation (prior) vs document-generation (this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-source 6-billion-parameter autoregressive transformer model (referenced as the open-source LM used by InPars-v2 to generate synthetic queries/documents).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Information retrieval (data augmentation for retrieval models / BEIR benchmark tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate synthetic queries given documents to create synthetic query-document training pairs (synthetic query generation for IR data augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Reported in cited InPars-v2 as state-of-the-art on BEIR (paper here only references the outcome; specific BEIR metrics not reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>The paper states InPars-v2 using GPT-J-6B achieves state-of-the-art results on the BEIR dataset (numerical values not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Use of an external high-quality re-ranker (MonoT5-MSMARCO) to filter synthetic pairs, size/capacity of the LLM (6B), and dataset filtering strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Cited description: InPars-v2 filters top-10k high-quality synthetic query-document pairs using a powerful external re-ranker (MonoT5-MSMARCO) which is credited for improving downstream performance; specific ablation details are in the referenced InPars-v2 work rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>As reported by the cited prior work: performance measured on BEIR benchmark; synthetic pairs filtered by an external re-ranker (MonoT5) before augmentation (details referenced, not performed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This paper does not report limitations specific to GPT-J-6B; it only references InPars-v2's approach and outcomes. No numerical failure cases provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Mentioned as contrast to this paper's reverse approach (document generation from queries): InPars and InPars-v2 focus on generating synthetic queries given documents, whereas this paper generates documents given queries.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>From the cited description: combining an open-source LLM for generation with a strong external re-ranker to filter and select high-quality synthetic pairs is an effective strategy for IR data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5627.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5627.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Promptagator / InPars (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promptagator and InPars (LLM-based synthetic-query generation methods referenced as prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior LLM-based data augmentation methods that generate synthetic queries from documents (Promptagator: few-shot dense retrieval prompting; InPars: synthetic queries using large LMs). They are cited to position the current paper's reverse direction (document generation) relative to existing work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (general mention; specific models not detailed here except GPT-J-6B for InPars-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based approaches that produce synthetic queries given documents for retrieval data augmentation; described in related work section without in-paper experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Information retrieval / Data augmentation for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Synthetic query generation conditioned on documents to produce query-document pairs for training retrieval models.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Not specified in this paper (referenced works evaluate on retrieval benchmarks such as BEIR and MS MARCO).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Mentioned factors in cited literature include prompt design (Promptagator few-shot prompting), model capacity, and filtering strategies, but specifics are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>This paper references those works to motivate its different approach; it does not reproduce their experiments or provide additional evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not applicable within this paper; cited works used standard retrieval benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail here (only referenced as different prior lines of work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Used to contrast prior query-generation approaches with the paper's novel document-generation direction.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>No direct recommendations from this paper for these methods beyond acknowledging their value for IR augmentation and motivating exploration of the reverse direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inpars: Data augmentation for information retrieval using large language models <em>(Rating: 2)</em></li>
                <li>InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval <em>(Rating: 2)</em></li>
                <li>Promptagator: Few-shot dense retrieval from 8 examples <em>(Rating: 2)</em></li>
                <li>How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection <em>(Rating: 2)</em></li>
                <li>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent <em>(Rating: 1)</em></li>
                <li>Perspectives on Large Language Models for Relevance Judgment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5627",
    "paper_id": "paper-20398b63519548ebe04b73918da99d491fa034aa",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "ChatGPT (as document generator)",
            "name_full": "ChatGPT (used to generate synthetic documents/responses for IR re-rankers)",
            "brief_description": "This paper uses ChatGPT to generate synthetic documents/responses given queries to create a training corpus (ChatGPT-RetrievalQA) for fine-tuning cross-encoder re-rankers; the generated data is evaluated both in supervised and zero-shot retrieval settings across multiple domains (medicine, finance, Reddit, Wikipedia).",
            "citation_title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5 family, referred to as ChatGPT in HC3)",
            "model_description": "A generative conversational large language model from OpenAI (GPT-3.5 / ChatGPT family) used via prompting to produce full textual responses to user queries; described in paper as a generative LLM producing lengthier and more lexically overlapping responses than humans. Exact training corpus and size not specified in this paper.",
            "model_size": null,
            "scientific_subdomain": "Information retrieval / Question answering across subdomains (Medicine, Finance, Reddit long-form Q&A, Wikipedia/OpenQA)",
            "simulation_task": "Generate synthetic documents / full-text responses given natural language queries to serve as relevant passages/documents for training cross-encoder re-rankers (data augmentation for retrieval).",
            "accuracy_metric": "Downstream retrieval metrics assessing re-ranker effectiveness: MAP@1000, NDCG@10, MRR@10; additional diagnostics: BM25 MAP/NDCG/Recall on generated vs human responses; lexical-overlap percentage between query and response.",
            "reported_accuracy": "When used to generate training data and then fine-tuning cross-encoders: zero-shot MiniLM trained on ChatGPT-generated data achieved (examples) on TREC DL'20: MAP=0.344, NDCG=0.539, MRR=0.978; on MS MARCO DEV: MAP=0.226, NDCG=0.267, MRR=0.218. BM25 retrieval is more effective on ChatGPT-generated responses than on human responses (Test: ChatGPT MAP=0.370, NDCG=0.396, Recall@1K=0.898 vs human MAP=0.143, NDCG=0.184, Recall@1K=0.520). Lexical overlap: average percent of query words appearing in ChatGPT responses = 34.6% vs human = 25.5%. (All numbers as reported in paper tables.)",
            "factors_affecting_accuracy": "Domain specificity (medical domain shows human-trained advantage), lexical overlap between query and generated response, length/repetition in generated responses (ChatGPT tends to repeat query and produce longer answers), train/test document overlap in supervised setting, architecture/size of downstream re-ranker (MiniLM vs TinyBERT), and potential factual errors/hallucinations in generated text.",
            "evidence_for_factors": "Domain effect: Table 3 shows MiniLM_human outperforms MiniLM_ChatGPT in most domains (e.g., Medicine MAP 0.397 vs 0.379). Lexical overlap: measured averages (34.6% vs 25.5%) and quartile increases reported. BM25 diagnostics: Table 4 shows BM25 performs much better on ChatGPT-generated responses than on human responses, correlating with higher lexical overlap and length. Model architecture: comparisons between MiniLM and TinyBERT fine-tuned on ChatGPT vs human data (Table 2 & 3) show differing gains. Data overlap: authors discuss that shared document collections between train/test may benefit human-trained models in supervised setting.",
            "evaluation_method": "Automated retrieval evaluation: cross-encoder re-rankers fine-tuned on ChatGPT-generated vs human-generated responses were evaluated on (a) supervised setting using the ChatGPT-RetrievalQA test set and (b) zero-shot evaluation on MS MARCO Dev and TREC Deep Learning tracks (DL'19, DL'20). Metrics: MAP@1000, NDCG@10, MRR@10. Additional analyses: BM25 retrieval scores on generated vs human responses, lexical overlap statistics, evaluation on seen queries with unseen human-document collection (Table 5). Ground truth relevance in supervised experiments: human responses considered as relevant answers.",
            "limitations_or_failure_cases": "The paper notes ChatGPT can hallucinate and produce factually incorrect information (potential risk for using generated text directly in sensitive domains); in supervised/domain-specific settings (notably Medicine) models fine-tuned on human data outperform ChatGPT-trained models; BM25 and lexical analyses suggest ChatGPT output is easier to match lexically (not necessarily more factually accurate); generalizability to open-source LLMs is untested; shared train/test collections may have inflated supervised performance for human-trained re-rankers.",
            "comparisons": "Direct comparisons in paper: CE_ChatGPT (cross-encoders fine-tuned on ChatGPT-generated responses) vs CE_human (fine-tuned on human responses), across two re-ranker architectures (MiniLM 12-layer and TinyBERT 2-layer) and baseline BM25. CE_ChatGPT outperforms CE_human in zero-shot evaluations (e.g., MiniLM_ChatGPT higher on TREC DL'20 and MS MARCO Dev), whereas CE_human is slightly better in supervised/domain-specific scenarios (e.g., Medicine). Paper also contrasts the ChatGPT-document generation approach (this work) with prior LLM-based synthetic-query approaches (InPars, InPars-v2, Promptagator).",
            "recommendations_or_best_practices": "Authors recommend using generative LLMs like ChatGPT to augment training data for re-rankers, especially for zero-shot transfer and domains with limited labeled data, but caution: (1) evaluate and mitigate factual errors/hallucinations before using generated content in sensitive domains, (2) test generalizability with open-source LLMs, (3) consider separating train/test document collections to avoid overlap that may bias supervised evaluation, and (4) be aware that lexical overlap and response length will affect retrieval behavior.",
            "uuid": "e5627.0",
            "source_info": {
                "paper_title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-J-6B (InPars-v2 mention)",
            "name_full": "GPT-J-6B (used in InPars-v2 as an open-source LLM for synthetic data generation)",
            "brief_description": "Referenced prior work (InPars-v2) used an open-source LLM, GPT-J-6B, to generate synthetic query-document pairs for data augmentation in retrieval; the current paper cites this as related work contrasting query-generation (prior) vs document-generation (this paper).",
            "citation_title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts",
            "mention_or_use": "mention",
            "model_name": "GPT-J-6B",
            "model_description": "An open-source 6-billion-parameter autoregressive transformer model (referenced as the open-source LM used by InPars-v2 to generate synthetic queries/documents).",
            "model_size": "6B",
            "scientific_subdomain": "Information retrieval (data augmentation for retrieval models / BEIR benchmark tasks)",
            "simulation_task": "Generate synthetic queries given documents to create synthetic query-document training pairs (synthetic query generation for IR data augmentation).",
            "accuracy_metric": "Reported in cited InPars-v2 as state-of-the-art on BEIR (paper here only references the outcome; specific BEIR metrics not reported in this paper).",
            "reported_accuracy": "The paper states InPars-v2 using GPT-J-6B achieves state-of-the-art results on the BEIR dataset (numerical values not provided in this paper).",
            "factors_affecting_accuracy": "Use of an external high-quality re-ranker (MonoT5-MSMARCO) to filter synthetic pairs, size/capacity of the LLM (6B), and dataset filtering strategy.",
            "evidence_for_factors": "Cited description: InPars-v2 filters top-10k high-quality synthetic query-document pairs using a powerful external re-ranker (MonoT5-MSMARCO) which is credited for improving downstream performance; specific ablation details are in the referenced InPars-v2 work rather than in this paper.",
            "evaluation_method": "As reported by the cited prior work: performance measured on BEIR benchmark; synthetic pairs filtered by an external re-ranker (MonoT5) before augmentation (details referenced, not performed in this paper).",
            "limitations_or_failure_cases": "This paper does not report limitations specific to GPT-J-6B; it only references InPars-v2's approach and outcomes. No numerical failure cases provided here.",
            "comparisons": "Mentioned as contrast to this paper's reverse approach (document generation from queries): InPars and InPars-v2 focus on generating synthetic queries given documents, whereas this paper generates documents given queries.",
            "recommendations_or_best_practices": "From the cited description: combining an open-source LLM for generation with a strong external re-ranker to filter and select high-quality synthetic pairs is an effective strategy for IR data augmentation.",
            "uuid": "e5627.1",
            "source_info": {
                "paper_title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Promptagator / InPars (mentions)",
            "name_full": "Promptagator and InPars (LLM-based synthetic-query generation methods referenced as prior work)",
            "brief_description": "Prior LLM-based data augmentation methods that generate synthetic queries from documents (Promptagator: few-shot dense retrieval prompting; InPars: synthetic queries using large LMs). They are cited to position the current paper's reverse direction (document generation) relative to existing work.",
            "citation_title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts",
            "mention_or_use": "mention",
            "model_name": "various LLMs (general mention; specific models not detailed here except GPT-J-6B for InPars-v2)",
            "model_description": "LLM-based approaches that produce synthetic queries given documents for retrieval data augmentation; described in related work section without in-paper experiments.",
            "model_size": null,
            "scientific_subdomain": "Information retrieval / Data augmentation for retrieval",
            "simulation_task": "Synthetic query generation conditioned on documents to produce query-document pairs for training retrieval models.",
            "accuracy_metric": "Not specified in this paper (referenced works evaluate on retrieval benchmarks such as BEIR and MS MARCO).",
            "reported_accuracy": null,
            "factors_affecting_accuracy": "Mentioned factors in cited literature include prompt design (Promptagator few-shot prompting), model capacity, and filtering strategies, but specifics are not detailed in this paper.",
            "evidence_for_factors": "This paper references those works to motivate its different approach; it does not reproduce their experiments or provide additional evidence.",
            "evaluation_method": "Not applicable within this paper; cited works used standard retrieval benchmarks.",
            "limitations_or_failure_cases": "Not discussed in detail here (only referenced as different prior lines of work).",
            "comparisons": "Used to contrast prior query-generation approaches with the paper's novel document-generation direction.",
            "recommendations_or_best_practices": "No direct recommendations from this paper for these methods beyond acknowledging their value for IR augmentation and motivating exploration of the reverse direction.",
            "uuid": "e5627.2",
            "source_info": {
                "paper_title": "Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inpars: Data augmentation for information retrieval using large language models",
            "rating": 2
        },
        {
            "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
            "rating": 2
        },
        {
            "paper_title": "Promptagator: Few-shot dense retrieval from 8 examples",
            "rating": 2
        },
        {
            "paper_title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
            "rating": 2
        },
        {
            "paper_title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
            "rating": 1
        },
        {
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "rating": 1
        }
    ],
    "cost": 0.0131675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generating Synthetic Documents for Cross-Encoder Re-Rankers: A Comparative Study of ChatGPT and Human Experts</h1>
<p>Arian Askari<br>a.askari@liacs.leidenuniv.nl<br>Leiden University<br>The Netherlands<br>Evangelos Kanoulas<br>e.kanoulas@uva.nl<br>University of Amsterdam<br>The Netherlands</p>
<h4>Abstract</h4>
<p>We investigate the usefulness of generative Large Language Models (LLMs) in generating training data for cross-encoder re-rankers in a novel direction: generating synthetic documents instead of synthetic queries. We introduce a new dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models fine-tuned on LLMgenerated and human-generated data. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public question collections with human responses and answers from ChatGPT. We fine-tune a range of cross-encoder re-rankers on either human-generated or ChatGPTgenerated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trained on human responses. In a supervised setting, the human-trained re-rankers outperform the LLM-trained re-rankers. Our novel findings suggest that generative LLMs have high potential in generating training data for neural retrieval models. Further work is needed to determine the effect of factually wrong information in the generated responses and test our findings' generalizability with open-source LLMs. We release our data, code, and cross-encoders checkpoints for future work. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Generative large language models (LLMs) such as GPT-3 [3] and GPT-3.5 (including ChatGPT) have shown remarkable performance in generating realistic text outputs for a variety of tasks such as summarization [40], machine translation [26], sentiment analysis [32, 36], retrieval interpretability [20], and stance detection [39]. Although ChatGPT can produce impressive answers, it is not immune to errors or hallucinations [12]. Furthermore, the lack of transparency in the source of information generated by ChatGPT can be a bigger concern in domains such as law, medicine, and science, where accountability and trustworthiness are critical [4, 28, 31].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Mohammad Aliannejadi <br> m.aliannejadi,e.kanoulas <br> University of Amsterdam <br> The Netherlands</h2>
<p>Suzan Verberne<br>s.verberne@liacs.leidenuniv.nl<br>Leiden University<br>The Netherlands</p>
<p>Retrieval models, as opposed to generative models, retrieve the actual (true) information from sources and search engines provide the source of each retrieved item [29]. This is why information retrieval (IR) - even when generative LLMs are available - remains an important application, especially in situations where reliability is vital. One potential purpose of generative LLMs in IR is to generate training data for retrieval models. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. In this paper, we present the ChatGPT-RetrievalQA dataset to address two research questions: RQ1: How does the effectiveness of cross-encoder re-rankers finetuned on ChatGPT-generated responses compare to those finetuned on human-generated responses in both supervised and zeroshot settings?
RQ2: How does the effectiveness of using ChatGPT for generating relevant documents differ between specific and general domains?</p>
<p>By answering these questions, we aim to shed light on the potential of using LLMs for data augmentation in cross-encoder re-rankers and the domain dependency of their effectiveness. As shown in Figure 1, our primary experimental setup involves using $\mathrm{CE}_{\text {ChatGPT }}{ }^{2}$ for inference (i.e., re-ranking task) on human-generated responses.</p>
<p>Our dataset and analysis provide insights into the benefits and limitations of using generative LLMs for augmenting training data for retrieval models.</p>
<p>Our main contributions in this work are three-fold: (i) We release the ChatGPT-RetrievalQA dataset, which is based on the HC3 dataset [13] but is designed specifically for information retrieval tasks in both full-ranking and re-ranking setups. This dataset contains 24,322 queries, 26,882 responses generated by ChatGPT, and 58, 546 human-generated responses. (ii) We fine-tune cross-encoder re-rankers on both the human- and ChatGPT-generated responses, evaluating their performance on our dataset in a supervised setting. We also show the effectiveness of the ChatGPT-trained models in a zero-shot evaluation on the MS MARCO-passage collection and the TREC Deep Learning tracks. (iii) We conduct an analysis of the effectiveness of ChatGPT-trained cross-encoders on different domains and show that human-trained models are slightly more effective in domain-specific tasks, e.g., in the medicine domain. Our novel findings highlight the potential of using generative LLMs</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: center;">Train set</th>
<th style="text-align: center;"># of queries <br> Validation set</th>
<th style="text-align: center;">Test set</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">16788</td>
<td style="text-align: center;">606</td>
<td style="text-align: center;">6928</td>
</tr>
<tr>
<td style="text-align: left;">Medicine: Meddialog [5]</td>
<td style="text-align: center;">862</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">355</td>
</tr>
<tr>
<td style="text-align: left;">Finance: FiQA [22]</td>
<td style="text-align: center;">2715</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">1120</td>
</tr>
<tr>
<td style="text-align: left;">Reddit: ELI5 [11]</td>
<td style="text-align: center;">11809</td>
<td style="text-align: center;">427</td>
<td style="text-align: center;">4876</td>
</tr>
<tr>
<td style="text-align: left;">Wikipedia: openQA [38]</td>
<td style="text-align: center;">820</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">338</td>
</tr>
<tr>
<td style="text-align: left;">Wikipedia: csai [13]</td>
<td style="text-align: center;">582</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">239</td>
</tr>
</tbody>
</table>
<p>like ChatGPT for generating high-quality responses in information retrieval tasks in order to create training datasets.</p>
<h2>2 RELATED WORK</h2>
<p>InPars [2], Promptagator [8], and InPars-v2 [15] have utilized LLMs to generate synthetic queries given documents. Particularly, InParsv2 [15] achieves state-of-the-art results on the BEIR dataset in a zeroshot setting by using an open-source language model, GPT-J-6B [33] and a powerful external re-ranker, MonoT5-MSMARCO [24] to filter the top-10k high-quality pairs of synthetic query-document pairs for data augmentation. In contrast, we use documents (passages) generated by ChatGPT given a query - the reverse from InParsv2. Document generation for given queries as a source for data augmentation has not been explored in prior work.</p>
<p>We believe that exploring this reverse direction is important as it allows us to augment training data with a focus on user behavior and query logs rather than the (static) document collection itself. This can improve the effectiveness of re-rankers by augmenting the training data with synthetic documents according to the queries that actual search engine users are searching for, increasing the diversity of the training data, while allowing the rankers to better generalize to new queries.</p>
<p>Guo et al. [13] use public question-answering datasets (see below) and prompt the questions to the ChatGPT user interface for generating answers. The goal of the HC3 dataset is to linguistically compare human and ChatGPT responses and explore the possibility of distinguishing between responses generated by ChatGPT and those written by humans. The HC3 dataset contains questions (i.e., queries) from four different domains: medicine (Medical Dialog [5]), finance (FiQA [22]), Wikipedia (WikiQA [38] and Wiki_csai [13]), and Reddit (ELI5 [11]). While there is no study on generating documents to augment training data, a more recent study, QuerytoDoc [34], generates documents for query expansion, which is out of the scope of augmenting data for information retrieval. Furthermore, there are various recent studies on ChatGPT with a focus on ranking and retrieval but to the best of our knowledge, none of them focus on data augmentation by generating relevant documents. Examples of recent studies are the one by Faggioli et al [10], who study if ChatGPT can be used for generating relevance labels, and Sun et al. [30], who assess whether ChatGPT is good at searching by giving it a query and set of candidate documents and asking for re-ranking.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our main experimental setup involves above steps.</p>
<h2>3 DATASET</h2>
<p>Our ChatGPT-RetrievalQA dataset is based on the HC3 dataset produced by Guo et al. [13], which contains 24,322 queries and 26, 882 ChatGPT-generated responses, as well as 58, 546 humangenerated responses. There is on average one ChatGPT-generated and 2.4 human-generated response per query. To build the ChatGPTRetrievalQA dataset for retrieval and ranking experiments (an experimental setup different from [13]), we convert the dataset files to a format similar to the MSMarco dataset [23], in both full-ranking and re-ranking setups. ${ }^{3}$ We divide the data into training, validation, and test sets.</p>
<p>To facilitate training, we provide training triples files in TSV format, including both textual and ID-based representations, where the structure of each line is composed of 'query, positive passage, negative passage' and 'qid, positive pid, negative pid'. We consider the actual response by ChatGPT or human as the relevant answer and we randomly sample 1000 negative answers for each query similar to MS MARCO. In addition, we provide the top 1000 documents, ranked by BM25, per query to enable re-ranking studies. Table 1 shows the size of the train, validation, and test sets for each domain.</p>
<h2>4 METHODS</h2>
<h3>4.1 First-stage ranker: BM25</h3>
<p>Lexical retrievers use word overlap to produce the relevance score between a document and a query. Several lexical approaches have been developed in the past, such as vector space models, Okapi BM25 [27], and query likelihood. We use BM25 as the first-stage ranker because of its popularity and effectiveness. BM25 calculates a score for a query-document pair based on the statistics of the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Comparing the effectiveness of cross-encoder re-rankers fine-tuned on human and ChatGPT responses in supervised and zero-shot settings. $\dagger$ indicates that a CE achieves statistically significant improvement for that dataset among all of the cross-encoder re-rankers and BM25 on that dataset. Statistical significance was measured with a paired t-test ( $p&lt;0.05$ ) with Bonferroni correction for multiple testing. The cutoff for MAP, NDCG, and MRR are 1000, 10, and 10.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Zero-shot setting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Supervised setting</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">TREC DL'19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TREC DL'20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MS MARCO DEV</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT-RetrievalQA (Ours)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAP</td>
<td style="text-align: center;">NDCG</td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">MAP</td>
<td style="text-align: center;">NDCG</td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">MAP</td>
<td style="text-align: center;">NDCG</td>
<td style="text-align: center;">MRR</td>
<td style="text-align: center;">MAP</td>
<td style="text-align: center;">NDCG</td>
</tr>
<tr>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">.377</td>
<td style="text-align: center;">.506</td>
<td style="text-align: center;">.858</td>
<td style="text-align: center;">.286</td>
<td style="text-align: center;">.480</td>
<td style="text-align: center;">.819</td>
<td style="text-align: center;">.195</td>
<td style="text-align: center;">.234</td>
<td style="text-align: center;">.187</td>
<td style="text-align: center;">.143</td>
<td style="text-align: center;">.184</td>
</tr>
<tr>
<td style="text-align: center;">MiniLM ${ }_{\text {human }}$</td>
<td style="text-align: center;">.326</td>
<td style="text-align: center;">.451</td>
<td style="text-align: center;">.833</td>
<td style="text-align: center;">.269</td>
<td style="text-align: center;">.376</td>
<td style="text-align: center;">.913</td>
<td style="text-align: center;">.130</td>
<td style="text-align: center;">.155</td>
<td style="text-align: center;">.118</td>
<td style="text-align: center;">.310 $\dagger$</td>
<td style="text-align: center;">.384 $\dagger$</td>
</tr>
<tr>
<td style="text-align: center;">MiniLM ${ }_{\text {ChatGPT }}$</td>
<td style="text-align: center;">.342 $\dagger$</td>
<td style="text-align: center;">.510 $\dagger$</td>
<td style="text-align: center;">.903</td>
<td style="text-align: center;">.344 $\dagger$</td>
<td style="text-align: center;">.539 $\dagger$</td>
<td style="text-align: center;">.978 $\dagger$</td>
<td style="text-align: center;">.226 $\dagger$</td>
<td style="text-align: center;">.267 $\dagger$</td>
<td style="text-align: center;">.218 $\dagger$</td>
<td style="text-align: center;">.294</td>
<td style="text-align: center;">.362</td>
</tr>
<tr>
<td style="text-align: center;">TinyBERT ${ }_{\text {human }}$</td>
<td style="text-align: center;">.294</td>
<td style="text-align: center;">.360</td>
<td style="text-align: center;">.741</td>
<td style="text-align: center;">.277</td>
<td style="text-align: center;">.364</td>
<td style="text-align: center;">.791</td>
<td style="text-align: center;">.128</td>
<td style="text-align: center;">.154</td>
<td style="text-align: center;">.116</td>
<td style="text-align: center;">.244</td>
<td style="text-align: center;">.310</td>
</tr>
<tr>
<td style="text-align: center;">TinyBERT ${ }_{\text {ChatGPT }}$</td>
<td style="text-align: center;">.328</td>
<td style="text-align: center;">.488</td>
<td style="text-align: center;">.942 $\dagger$</td>
<td style="text-align: center;">.303</td>
<td style="text-align: center;">.460</td>
<td style="text-align: center;">.972</td>
<td style="text-align: center;">.194</td>
<td style="text-align: center;">.231</td>
<td style="text-align: center;">.185</td>
<td style="text-align: center;">.231</td>
<td style="text-align: center;">.291</td>
</tr>
</tbody>
</table>
<p>words that overlap between them:</p>
<p>$$
s_{l e x}(q, d)=B M 25(q, d)=\sum_{t \in q \cap d} r s j_{t} \cdot \frac{t f_{t, d}}{t f_{t, d}+k_{1}\left{(1-b)+b \frac{t f_{t}}{l}\right}}
$$</p>
<p>where $t$ is a term, $t f_{t, d}$ is the frequency of $t$ in document $d, r s j_{t}$ is the Robertson-Sprck Jones weight [27] of $t$, and $l$ is the average document length. $k_{1}$ and $b$ are parameters.</p>
<h3>4.2 Cross-encoder re-rankers</h3>
<p>The common approach to employ pre-trained Transformer models with a cross-encoder architecture in a re-ranking setup is by concatenating the input sequences of query and passage. This method, known as MonoBERT or $\mathrm{CE}<em _mathrm_CAT="\mathrm{CAT">{\mathrm{CAT}}$, is illustrated in Figure 1 and has been utilized in several studies. In $\mathrm{CE}</em>}}$, the sequences of query words $q_{1}: q_{m}$ and passage words $p_{1}: p_{n}$ are joined with the [SEP] token, and the ranking model of $\mathrm{CE<em _mathrm_s="\mathrm{s">{\mathrm{CAT}}$ calculates the score for the representation of the [CLS] token obtained by cross-encoder (CE) using a single linear layer $\mathrm{W}</em>$ :}</p>
<p>$$
C E_{C A T}\left(q_{1: m}, p_{1: n}\right)=C E([C L S] q[S E P] p[S E P]) * W_{s}
$$</p>
<p>We use $\mathrm{CE}<em _ChatGPT="{ChatGPT" _text="\text">{\mathrm{CAT}}$ as our cross-encoder re-ranker with a re-ranking depth of 1000 . In our experiments, both $\mathrm{CE}</em>$ follow the above design.}}$ and $\mathrm{CE}_{\text {human }</p>
<h2>5 EXPERIMENTAL DESIGN</h2>
<p>Evaluation setup. We conduct our zero-shot evaluation experiments on the MS MARCO-passage collection [23] and the two TREC Deep Learning tracks (TREC-DL'19 and DL'20) [6, 7]. The evaluation metrics used are MAP@1000, NDCG@10, and MRR@10, which are standard for these datasets, to make our results comparable to previously published and upcoming research [6, 7]. The MS MARCO-passage dataset contains about 8.8 million passages (average length: 73.1 words) and about 1 million natural language queries (average length: 7.5 words) and has been extensively used to train deep language models for ranking. Following prior work on MS MARCO [17, 19, 21, 42, 43], we only use the dev set ( $\sim 7 k$ queries) for our empirical evaluation. The TREC DL'19 and DL'20 collections share the passage corpus of MS MARCO and have 43 and 54 queries respectively with much more relevant documents per query. We measure the same metrics in the supervised setting on the test set of ChatGPT-RetrievalQA. The average length of</p>
<p>Table 3: Comparing the effectiveness of $\mathrm{CE}<em _mathrm_H="\mathrm{H">{\mathrm{C}}$ and $\mathrm{CE}</em>$ in supervised setting across different domains where CE, C, and H refer to the MiniLM, human, and ChatGPT. The OpenQA and Wiki_csai datasets are in Wikipedia domain.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MAP@1K</th>
<th style="text-align: center;">NDCG@10</th>
<th style="text-align: center;">MRR@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{H}}$</td>
<td style="text-align: center;">.310</td>
<td style="text-align: center;">.384</td>
<td style="text-align: center;">.460</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{C}}$</td>
<td style="text-align: center;">.294</td>
<td style="text-align: center;">.362</td>
<td style="text-align: center;">.444</td>
</tr>
<tr>
<td style="text-align: center;">Medicine [5]</td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{H}}$</td>
<td style="text-align: center;">.397</td>
<td style="text-align: center;">.419</td>
<td style="text-align: center;">.395</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{C}}$</td>
<td style="text-align: center;">.379</td>
<td style="text-align: center;">.400</td>
<td style="text-align: center;">.377</td>
</tr>
<tr>
<td style="text-align: center;">Finance [22]</td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{H}}$</td>
<td style="text-align: center;">.257</td>
<td style="text-align: center;">.399</td>
<td style="text-align: center;">.251</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{C}}$</td>
<td style="text-align: center;">.250</td>
<td style="text-align: center;">.368</td>
<td style="text-align: center;">.245</td>
</tr>
<tr>
<td style="text-align: center;">Reddit [11]</td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{H}}$</td>
<td style="text-align: center;">.323</td>
<td style="text-align: center;">.418</td>
<td style="text-align: center;">.543</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{C}}$</td>
<td style="text-align: center;">.302</td>
<td style="text-align: center;">.391</td>
<td style="text-align: center;">.522</td>
</tr>
<tr>
<td style="text-align: center;">OpenQA [38]</td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{H}}$</td>
<td style="text-align: center;">.322</td>
<td style="text-align: center;">.345</td>
<td style="text-align: center;">.320</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{C}}$</td>
<td style="text-align: center;">.331</td>
<td style="text-align: center;">.341</td>
<td style="text-align: center;">.328</td>
</tr>
<tr>
<td style="text-align: center;">Wiki_csai [13]</td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{H}}$</td>
<td style="text-align: center;">.149</td>
<td style="text-align: center;">.152</td>
<td style="text-align: center;">.135</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CE}_{\mathrm{C}}$</td>
<td style="text-align: center;">.163</td>
<td style="text-align: center;">.159</td>
<td style="text-align: center;">.144</td>
</tr>
</tbody>
</table>
<p>human responses is 142.5 words, and 198.1 words for ChatGPT in the ChatGPT-RetrievalQA dataset.</p>
<p>Training configuration. We use the Huggingface library [37], and PyTorch [25] for the cross-encoder re-ranking training and inference. Following prior work [14], we use the Adam [18] optimizer with a learning rate of $7 * 10^{-6}$ for all cross-encoder layers, regardless of the number of layers trained. We use a training batch size of 32. For all cross-encoder re-rankers, we use the cross-entropy loss [41]. For the lexical retrieval with BM25, we use the similarity function of Elasticsearch [9]. We cap the query length at 30 tokens and the passage length at 200 tokens following prior work [1, 14].</p>
<h2>6 RESULTS</h2>
<h3>6.1 Main results (RQ1)</h3>
<p>Table 2 shows a comparison of the effectiveness of $\mathrm{CE}<em _ChatGPT="{ChatGPT" _text="\text">{\text {human }}$ and $\mathrm{CE}</em>$. Please note that for both models, during inference, we evaluate their effectiveness in retrieving human responses in the supervised or zero-shot settings. We choose MiniLM (12 layers version) [35] for the experiments due to its competitive results in}</p>
<p>comparison to BERT re-ranker [1] while being three times smaller and six times faster. In addition, we conduct experiments with TinyBERT (2 layers version) [16] to assess the generalizability of our results.</p>
<p>In the supervised setting where we evaluate the on test set queries with human documents of our ChatGPT-RetrievalQA dataset, $\mathrm{MiniLM}<em _ChatGPT="{ChatGPT" _text="\text">{\text {human }}$ significantly outperforms all of the other cross-encoders re-rankers. Although lower than the human-trained models, $\operatorname{MiniLM}</em>$ still outperform the strong baseline [2], BM25 [27], statistically significantly by a large margin in this setting.}}$ and TinyBERT $_{\text {ChatGPT }</p>
<p>In the zero-shot setting, the $\operatorname{MiniLM}<em _human="{human" _text="\text">{\text {ChatGPT }}$ consistently outperforms the other cross-encoder re-rankers including $\operatorname{MiniLM}</em>$ and BM25 significantly across the TREC DL'20 and MS MARCO DEV. However, on TREC DL'19, BM25 achieves the highest effectiveness for MAP@1000, MiniLM ChatGPT for NDCG@10, and TinyBERT ChatGPT for MRR@10. Overall, we can see the models fine-tuned on ChatGPT-generated responses are significantly more effective in the zero-shot setting compared to those fine-tuned on human-generated responses.}</p>
<h3>6.2 Domain-level re-ranker effectiveness (RQ2)</h3>
<p>Table 3 shows the effectiveness of $\operatorname{MiniLM}<em _ChatGPT="{ChatGPT" _text="\text">{\text {human }}$ and $\operatorname{MiniLM}</em>}}$ re-rankers in the supervised settings - on the test set of our dataset - across all of the domains including Medicine, Finance, Reddit, and Wikipedia. Overall, the results show that $\operatorname{MiniLM<em _ChatGPT="{ChatGPT" _text="\text">{\text {human }}$ achieves higher effectiveness than $\operatorname{MiniLM}</em>}}$ for all domains except Wikipedia. However, the difference in effectiveness is small, and $\operatorname{MiniLM<em _human="{human" _text="\text">{\text {ChatGPT }}$ still achieves a reasonable level of effectiveness. In the Finance domain, both $\operatorname{MiniLM}</em>}}$ and $\operatorname{MiniLM<em _human="{human" _text="\text">{\text {ChatGPT }}$ achieve relatively low effectiveness compared to other domains. In the Wikipedia domain, $\operatorname{MiniLM}</em>}}$ and $\operatorname{MiniLM<em _human="{human" _text="\text">{\text {ChatGPT }}$ achieve relatively similar levels of effectiveness. In the Medicine domain, the $\mathrm{CE}</em>$ performs more effective in supervised settings, particularly in specific domains such as Medicine, even though the difference in performance is small.}}$ shows the highest effectiveness. Overall, these results suggest that $\operatorname{MiniLM}_{\text {human }</p>
<h2>7 DISCUSSION</h2>
<p>Data overlap. It is worth noting that in the supervised setting, the collection of documents used for training and testing is shared for $\mathrm{CE}<em _human="{human" _text="\text">{\text {human }}$ re-rankers. Therefore, some documents may be seen during both training and evaluation. This setup is very common when working with human-assessed data, and similar to the MS MARCO dataset [23]. The shared collection could be a potential benefit for $\mathrm{CE}</em>$ re-rankers in the supervised setting, as the models may have already seen some of the documents during training. To further investigate this hypothesis, it would be worth exploring a different setup in future work in which the collection of documents is completely separated between the training and test sets.}</p>
<p>Effectiveness of BM25. Table 4 shows an analysis of the effectiveness of BM25 on human and ChatGPT-generated responses in the train, and test sets. BM25 is less effective for human-generated responses than for ChatGPT-generated responses on the train and test sets, as evidenced by lower scores for all metrics. We observed the same pattern for the validation set. These results suggest that</p>
<p>Table 4: Analyzing the effectiveness of BM25 on human/ChatGPT responses in train, validation, and test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Split</th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">MAP@1K</th>
<th style="text-align: center;">NDCG@10</th>
<th style="text-align: center;">Recall@1K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">.143</td>
<td style="text-align: center;">.184</td>
<td style="text-align: center;">.520</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.370</td>
<td style="text-align: center;">.396</td>
<td style="text-align: center;">.898</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">.158</td>
<td style="text-align: center;">.202</td>
<td style="text-align: center;">.560</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.413</td>
<td style="text-align: center;">.443</td>
<td style="text-align: center;">.903</td>
</tr>
</tbody>
</table>
<p>Table 5: Analyzing the effectiveness of $\mathrm{CEs}_{\text {ChatGPT }}$ on the seen queries of the train set and unseen documents of human-generated documents collection.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MAP@1K</th>
<th style="text-align: center;">NDCG@10</th>
<th style="text-align: center;">MRR@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\operatorname{MiniLM}_{\text {ChatGPT }}$</td>
<td style="text-align: center;">.318</td>
<td style="text-align: center;">.388</td>
<td style="text-align: center;">.510</td>
</tr>
<tr>
<td style="text-align: left;">TinyBERT $_{\text {ChatGPT }}$</td>
<td style="text-align: center;">.254</td>
<td style="text-align: center;">.318</td>
<td style="text-align: center;">.420</td>
</tr>
</tbody>
</table>
<p>the task of retrieving human-generated responses is more challenging for BM25 than for ChatGPT-generated responses. This is probably related to the lexical overlap discussed below.</p>
<p>Queries without label. In Table 5, we investigate a common scenario in real-world search engines where query logs and a collection of human-generated documents are available, and there are not any judged documents for part or all of the query logs. To simulate and analyze this situation, we evaluate $\mathrm{CE}<em _ChatGPT="{ChatGPT" _text="\text">{\text {ChatGPT }}$ on the seen queries of the train set and unseen documents of the humangenerated documents collection. Table 5 shows that $\mathrm{CE}</em>$ is 0.388 and 0.202 for BM25 (see the third row of Table 4). This suggests the potential of augmenting training data with generative LLMs for fine-tuning models to effectively re-rank sourced and reliable human-generated documents from the corpus given the query logs where there are no judged documents for the queries.}}$ rankers are fairly effective in this scenario. Especially, they are more effective than BM25 in the same setup, in that the NDCG@10 for $\operatorname{MiniLM}_{\text {ChatGPT }</p>
<p>Lexical overlap. Our data analysis reveals that ChatGPT-generated responses have a slightly higher lexical overlap than human-generated responses with the queries. The average percentage of query words that occur in ChatGPT-generated responses is $34.6 \%$, compared to $25.5 \%$ for humans. The Q1, median, and Q3 are also on average $7 \%$ points higher for ChatGPT compared to human responses. We suspect that this higher lexical overlap compared to the human response happens because ChatGPT often repeats the question or query in the response, and it tends to generate lengthier responses compared to human which increase the chance of having mutual words with the query. It is noteworthy to mention that lexical overlap is not the best indicator of response quality for fine-tuning effective cross-encoders, as there may be cases where responses with low lexical overlap are still relevant and informative, especially in question-answering tasks.</p>
<h2>8 CONCLUSION</h2>
<p>In this paper, we have presented an analysis of the effectiveness of fine-tuning cross-encoders on human-generated responses versus ChatGPT-generated responses. Our results show that the cross$\operatorname{encoder}<em _human="{human" _text="\text">{\text {ChatGPT }}$ is more effective than cross-encoder ${ }</em>$ in the}</p>
<p>zero-shot setting while MinILM human is slightly more effective in the supervised setting and this is consistent across different domains. Furthermore, we show that BM25 is less effective on human-generated responses than on ChatGPT-generated responses, indicating that human-generated responses are more challenging to match with queries than ChatGPT-generated responses. Overall, our findings suggest that ChatGPT-generated responses are more useful than human-generated responses for training effective zero-shot reranker, at least based on our dataset and experiments, and highlight the potential of using generative LLMs for generating effective and useful responses for creating training datasets in natural language processing tasks. Our study can be particularly advantageous for domain-specific tasks where relying on LLM-generated output as a direct response to a user query can be risky. Our results confirm that it is possible to train effective cross-encoder re-rankers by training them on ChatGPT-generated responses even for domainspecific queries. Further work is needed to determine the effect of factually wrong information in the generated responses and to test the generalizability of our findings on open-source LLMs.</p>
<h2>REFERENCES</h2>
<p>[1] Arias Askari, Amin Abolghasemi, Gabriella Pasi, Wessel Kraaij, and Suzan Verberne. 2023. Injecting the BM25 Score as Text Improves BERT-Based Re-rankers. In Advances in Information Retrieval. Springer Nature Switzerland, Cham, 66-83.
[2] Luiz Bonifacio, Hugo Abonzioz, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144 (2022).
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[4] Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. 2023. Evaluating the feasibility of ChatGPT in healthcare: an analysis of multiple clinical and research scenarios. Journal of Medical Systems 47, 1 (2023), 1-5.
[5] Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng, Ruxi Zhang, Ruoyu Zhang, Meng Zhou, et al. 2020. Meddialog: a large-scale medical dialogue dataset. arXiv preprint arXiv:2004.03329 (2020).
[6] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning track. arXiv preprint arXiv:2102.07662 (2021).
[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020. Overview of the TREC 2019 deep learning track. arXiv preprint arXiv:2003.07820 (2020).
[8] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[9] BV Elasticsearch. 2023. Elasticsearch. software], version 6, 1 (2023).
[10] Guglielmo Faggioli, Laura Dietz, Charles Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, et al. 2023. Perspectives on Large Language Models for Relevance Judgment. arXiv preprint arXiv:2304.09161 (2023).
[11] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELIS: Long form question answering. arXiv preprint arXiv:1907.09190 (2019).
[12] Josh A Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. 2023. Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations. arXiv preprint arXiv:2301.04246 (2023).
[13] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. arXiv preprint arXiv:2301.07597 (2023).
[14] Sebastian Hofsttter, Sophia Althammer, Michael Schrder, Mete Sertkan, and Allan Hanbury. 2020. Improving efficient neural ranking models with crossarchitecture knowledge distillation. arXiv preprint arXiv:2010.02666 (2020).
[15] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonzioz, Marzieh Fadaee, Roberto Lotufo, Jakub Zavre1, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. arXiv preprint arXiv:2301.01820 (2023).
[16] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language</p>
<p>Understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 4163-4174. https://doi.org/10.18653/v1/2020.findings-emnlp. 372
[17] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 39-48.
[18] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[19] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021. Pretrained transformers for text ranking: Bert and beyond. Synthesis Lectures on Human Language Technologies 14, 4 (2021), 1-325.
[20] Michael Llordes, Debasis Ganguly, Sumit Bhatia, and Chirag Agarwal. 2023. Explain like I am BM25: Interpreting a Dense Model's Ranked-List with a Sparse Approximation. arXiv preprint arXiv:2304.12631 (2023).
[21] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder. 2020. Expansion via prediction of importance with contextualization. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, 1573-1576.
[22] Macedo Maia, Siegfried Handschuh, Andr Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahue. 2018. Www'18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018. 1941-1942.
[23] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MABCO: A human generated machine reading comprehension dataset. In CoCo@ NIPs.
[24] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020).
[25] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Liu, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).
[26] Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanzin Ouyang, and Dacheng Tao. 2023. Towards Making the Most of ChatGPT for Machine Translation. arXiv preprint arXiv:2303.13780 (2023).
[27] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR'94. Springer, 232-241.
[28] Malik Sallam, Nesreen Salim, Muna Barakat, and Alaa Al-Tammemi. 2023. ChatGPT applications in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitations. Narra J 3, 1 (2023), e103-e103.
[29] Mark Sanderson and W Bruce Croft. 2012. The history of information retrieval research. Proc. IEEE 100, Special Centennial Issue (2012), 1444-1451.
[30] Weinex Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).
[31] Zhongxiang Sun. 2023. A short survey of viewing large language models in legal aspect. arXiv preprint arXiv:2303.09136 (2023).
[32] Teo Susnjak. 2023. Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in Scientific Literature. arXiv preprint arXiv:2302.06474 (2023).
[33] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingofloil/mesh-transformer-jax.
[34] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. arXiv:2303.07678 [cs.IR]
[35] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilim: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems 33 (2020), 3776-3788.
[36] Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023. Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. arXiv preprint arXiv:2304.04339 (2023).
[37] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).
[38] Yi Yang, Wen-tan Yih, and Christopher Meek. 2015. Wikiqu: A challenge dataset for open-domain question answering. In Proceedings of the 2015 conference on empirical methods in natural language processing. 2013-2018.
[39] Bowen Zhang, Daijun Ding, and Liwen Jing. 2022. How would Stance Detection Techniques Evolve after the Launch of ChatGPT? arXiv preprint arXiv:2312.14548 (2022).
[40] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023. Extractive Summarization via ChatGPT for Faithful Summary Generation. arXiv preprint arXiv:2304.04193 (2023).
[41] Zhilu Zhang and Mert Sabuncu. 2018. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems 31 (2018).</p>
<p>[42] Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. Deep query likelihood model for information retrieval. In European Conference on Information Retrieval. Springer, 463-470.
[43] Shengyao Zhuang and Guido Zuccon. 2021. TILDE: Term independent likelihood moDEI for passage re-ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1483-1492.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ This allows for easy reuse of available scripts on MS MARCO.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ We refer to the cross-encoders fine-tuned on ChatGPT-generated and humangenerated responses as $\mathrm{CE}<em _human="{human" _text="\text">{\text {ChatGPT }}$ and $\mathrm{CE}</em>$, respectively.&#160;}<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1142 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1142</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1142</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-252090421</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.02644v2.pdf" target="_blank">Modeling and Active Learning for Experiments with Quantitative-Sequence Factors</a></p>
                <p><strong>Paper Abstract:</strong> A new type of experiment that aims to determine the optimal quantities of a sequence of factors is eliciting considerable attention in medical science, bioengineering, and many other disciplines. Such studies require the simultaneous optimization of both quantities and the sequence orders of several components which are called quantitative-sequence (QS) factors. Given the large and semi-discrete solution spaces in such experiments, efficiently identifying optimal or near-optimal solutions by using a small number of experimental trials is a nontrivial task. To address this challenge, we propose a novel active learning approach, called QS-learning, to enable effective modeling and efficient optimization for experiments with QS factors. QS-learning consists of three parts: a novel mapping-based additive Gaussian process (MaGP) model, an efficient global optimization scheme (QS-EGO), and a new class of optimal designs (QS-design). The theoretical properties of the proposed method are investigated, and optimization techniques using analytical gradients are developed. The performance of the proposed method is demonstrated via a real drug experiment on lymphoma treatment and several simulation studies.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1142.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1142.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QS-learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantitative-Sequence learning (QS-learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning / Bayesian-optimization style agent for experiments whose inputs combine continuous quantities and sequence (permutation) factors; it couples a mapping-based additive Gaussian process surrogate (MaGP), expected-improvement acquisition, and specialized optimization/design tools to adaptively select experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>QS-learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A sequential experimental-design agent that (1) fits a mapping-based additive Gaussian process (MaGP) surrogate to data over mixed continuous+sequence inputs, (2) chooses new trials by maximizing Expected Improvement (EI) under the MaGP predictive distribution, and (3) uses tailored optimizers (QS-EGO / SFTA) and QS-design initializations. It also has a 'fast' variant that freezes hyperparameters for batches and uses O(n^2) updates between refits.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / Bayesian optimization (Expected Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>QS-learning fits a probabilistic surrogate (MaGP) to collected runs, computes EI for candidate inputs (x, o), and selects the next experiment by maximizing EI. For sequence inputs it alternates optimizing continuous quantities given a sequence and optimizing sequences given quantities (when enumeration is infeasible), using QS-EGO (GA+BFGS) and SFTA for sequence search. It updates the surrogate after each new observation (or in batches for the fast variant) and stops when EI fails to improve sufficiently or budget/time is exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black‑box QS experiments (case studies: lymphoma drug combination experiment; traveling salesman problem (TSP) simulator; single-machine scheduling; arranging four operations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box objective functions; semi-discrete input space combining continuous quantities and permutation/sequence inputs; environments can be deterministic (computer simulators) or noisy (physical experiments); large combinatorial sequence space (k! permutations), possibly non-separable interactions between quantity and order; limited budget (small-sample regime).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by benchmark: lymphoma case k=3 (6 sequences, 24 total runs in original experiment); TSP example k=8 (8! = 40,320 sequences) with 8 continuous stay durations (x_i ∈ [1,4]); QS-design initial sizes recommended ~O(k^2) (e.g., 46-run QS-design for TSP), sequential budgets tens to low hundreds of runs; surrogate estimation cost O(n^3) per refit (fast variant reduces refit frequency and uses O(n^2) updates).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Case study (lymphoma): QS-learning found the true maximum response 47.18 using 15 total runs (8-run initial QS-design + 7 sequential), saving 37.5% of the original 24-run budget. TSP simulation: starting from a 46-run QS-design and selecting 42 sequential runs (88 runs total) under 2d-MaGP, QS-learning found maximum profit 336 (found at 41st sequential run). Fast QS-learning (time-limited) with 96 total runs found max 313. Compared to baselines: large random sample (4,032,000) found 325 (worse than QS-learning's 336), sequential generalized-PWO and CP baselines (BM2/BM3) had average results 254 and 264 (best replications 321 and 324) in the TSP study, clearly inferior on average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random one-shot or baseline sequential methods: lymphoma random sampling (BM1) selecting 15 of 24 runs has only a 62.5% chance to include the optimal treatment; BM2/BM3 in lymphoma starting from QS-design found only 43.93 (vs QS-learning 47.18). In TSP, BM1 (very large random sample) found max 325; BM2 and BM3 averaged 254 and 264, best observed 321 and 324. Fast QS-learning (adaptive) outperformed random or classical sequential baselines in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High in small-sample regime: lymphoma optimum recovered with 15 runs (initial n0=8 then 7 sequential); TSP optimum near-found with 88 runs total versus millions of random samples required to approach similar results; fast variant reduced wall-clock time allowing more runs under time budget (example: sequential run time ~1 min for fast QS-learning vs ~4 min for general QS-learning per run in TSP example).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via EI acquisition: EI has an exploitation term (favoring low posterior-mean) and an exploration term (favoring large posterior variance). For sequences, SFTA Phase I encourages space-filling (exploration) and Phase II uses threshold-accepting (gradually reduced thresholds) to refine promising sequences (exploitation). A suggested ε-greedy enumerative trick is mentioned to avoid local optima by occasionally sampling random sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random sampling (BM1); sequential generalized PWO (pairwise ordering) model (BM2); sequential generalized CP (component-position) model (BM3); large random search/enumeration; random-search sequence optimizer (baseline to SFTA).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Integrating the MaGP surrogate (which maps permutation orders into learned latent coordinates) with EI and specialized sequence optimization (SFTA) and QS-design initializations yields strong small-sample performance on mixed continuous+sequence black-box problems: it recovered the true optimum in a 24-run real drug experiment using only 15 runs, and outperformed baseline methods across simulations (TSP, scheduling, arranging operations). SFTA (space-filling + threshold-accepting) is effective for large permutation spaces when combined with EI-driven quantitative optimization. The fast QS-learning variant trades off refitting frequency for wall-clock efficiency while preserving good solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Theoretical convergence of EI is not guaranteed in all settings; convergence rates rely on kernel smoothness and assumptions (remarks cite Bull (2011) and stabilized EI results). Performance depends on a good initial QS-design—poor initial designs degrade results; enumerating all sequences is infeasible for large k, requiring heuristics (SFTA) whose quality depends on tuning and sampling budget. Computational scaling: standard GP refits cost O(n^3); mapping-dimension t choice is a trade-off between flexibility and number of parameters (full-MaGP can be very high-dimensional). Case-study limitations: the real drug experiment had very few dose levels, limiting curvature estimation. Reported failures/weaknesses: BM2/BM3 often performed worse and sometimes failed to include the optimum; random-search comparators performed poorly versus SFTA.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>All algorithmic components (MaGP, QS-EGO, SFTA, QS-design, fast QS-learning) are proposed and empirically used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling and Active Learning for Experiments with Quantitative-Sequence Factors', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1142.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1142.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QS-EGO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QS Efficient Global Optimization (QS-EGO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The EI maximization / inner optimization engine inside QS-learning that alternates continuous-variable optimization (GA + BFGS using analytic gradients) with sequence optimization (SFTA) to maximize Expected Improvement over mixed continuous+sequence inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>QS-EGO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An optimizer that maximizes the EI acquisition function under the MaGP surrogate by iteratively: (1) optimizing quantitative variables x given a fixed sequence o via a hybrid genetic algorithm with BFGS (analytical gradients provided), and (2) optimizing sequence o given x via the SFTA algorithm; repeated multi-start rounds and stopping criteria are used.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Acquisition-function optimization for Bayesian optimization (Expected Improvement) tailored to mixed continuous+permutation inputs</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Alternating coordinate optimization: with current sequence o_c, run GA+BFGS to optimize continuous x; with optimized x_c, run SFTA to optimize sequence o; accept the best EI found over rounds and use chosen (x,o) as next experiment. Uses analytic EI gradients (derived for MaGP) to accelerate continuous optimization and multi-start/space-filling initialization for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mixed continuous+sequence black-box optimization problems used throughout the paper (lymphoma, TSP, scheduling, operation ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same semi-discrete black-box environments: continuous quantities × permutation sequences, possibly noisy measurements (physical experiments) or deterministic simulators; high-dimensional sequence combinatorics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Requires optimizing over continuous dimension (sum of k continuous variables) and discrete permutation space of size k!; for TSP k=8 (40,320 permutations) enumeration is infeasible, so QS-EGO uses SFTA.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>QS-EGO used within QS-learning enabled efficient EI maximization leading to the reported experimental successes: e.g., in the TSP experiment QS-EGO + MaGP led to identification of the best found profit 336 after 88 runs; in small problems enumeration of all sequences with QS-EGO is used when k small.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>By enabling effective EI maximization over mixed domains, QS-EGO contributes to the small-sample successes reported (e.g., 15 total runs to find optimum in lymphoma); the paper reports that EI optimization rounds N_round typically set 10–100.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed by EI itself; QS-EGO's alternating routine helps both exploitation (improving predicted best means) and exploration (via EI's variance term and SFTA's space-filling Phase I).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Enumeration (when feasible), random search over sequences, naive sequence optimization baselines (random-search sequence optimizer).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>QS-EGO's hybrid strategy with analytic gradients makes EI optimization tractable over continuous parts and amenable to heuristic but effective searches over permutations, enabling successful adaptive experiments in semi-discrete domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When k is large, the sequence optimization remains heuristic (SFTA) and may require many function evaluations; QS-EGO performance depends on quality of surrogate and on hyperparameter estimation; no absolute optimality guarantee for the mixed-domain EI maximization is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling and Active Learning for Experiments with Quantitative-Sequence Factors', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1142.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1142.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Space-Filling Threshold-Accepting (SFTA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase algorithm for optimizing sequence (permutation) inputs under EI: Phase I produces space-filling diverse candidate sequences; Phase II uses threshold-accepting (a deterministic analog of simulated annealing) with empirically determined thresholds to refine solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SFTA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Sequence optimizer combining (Phase I) space-filling candidate generation using Hamming-distance-based acceptance probabilities to initialize diverse starting points, and (Phase II) a Threshold-Accepting local/global search with decreasing thresholds derived from empirical increments to move from exploration toward exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Heuristic global optimization tailored for discrete permutation spaces used inside an EI-driven active learning loop</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Phase I selects sequences that are far (in Hamming distance) from already-observed sequences to encourage exploration; Phase II starts from the best Phase I sequence and iteratively accepts neighbor permutations if objective increment δ is below a current threshold τ_r (thresholds decreasing over rounds), enabling escape from local optima early and fine-tuning later.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Permutation (order-of-addition) optimization in QS experiments (TSP sequences, drug-order sequences, scheduling sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Large discrete permutation spaces (k! permutations), neighbor moves defined by swapping two elements (Hamming distance 2), objective is EI (computed via MaGP given fixed continuous x), search budget constrained (600 sequences vs 1000 in random baseline in example).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Example: TSP with k=8 (40,320 permutations). In SFTA example, Phase I used n(1)=100 space-filling candidates, Phase II used n_seq=500 neighbors to build threshold distribution, and 100 rounds/steps as tuning (typical practical settings given).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In an empirical comparison (TSP first-EI selection), SFTA with 600 sequences (100 Phase I + 500 Phase II) found the true maximum EI value (5.34) in most of 100 replications (results were 5.28 or 5.34, with 5.34 the true maximum in most trials). Random search comparing 1000 random sequences (similar CPU time) produced much worse and less consistent EI maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random-search baseline (1000 random sequences) typically failed to find the true EI maximum and produced inferior EI values in the presented replication study.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SFTA found accurate EI-maximizing sequences with only 600 sequence evaluations (under a second on a laptop in the reported example), whereas random search needed more evaluations and still underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly structured: Phase I emphasizes exploration via space-filling Hamming-distance-based sampling; Phase II uses threshold-accepting with decreasing thresholds to trade toward exploitation over rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random search over sequences (baseline), enumeration (when feasible for small k).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SFTA provides an efficient and robust sequence optimizer inside EI loops: space-filling initialization plus threshold-accepting refinement outperformed random search in the paper's experiments and enabled effective EI maximization for large permutation domains within modest evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on SFTA tuning parameters (n(1)_steps, n_seq, n_rounds, n(2)_steps) and on available evaluation budget; no theoretical optimality guarantees; may require more evaluations for very large k or extremely rugged EI landscapes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling and Active Learning for Experiments with Quantitative-Sequence Factors', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1142.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1142.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast QS-learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast QS-learning (batch-refit variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally efficient variant of QS-learning that fixes MaGP hyperparameters for batches of sequential selections and uses fast O(n^2) Sherman–Morrison–style updates to the covariance inverse between refits, trading off some adaptivity for wall-clock efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Fast QS-learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Instead of reestimating GP hyperparameters after every new observation, this variant records the time cost t of fitting and, for the next floor(N * t / T) iterations, keeps parameters fixed and uses O(n^2) updates to Φ^{-1} and predictive quantities when adding points; hyperparameters are reestimated periodically to respect a total compute-time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / EI with infrequent surrogate hyperparameter reestimation and fast covariance updates</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts selection frequency of hyperparameter reestimation to remaining time budget; within fixed-parameter epochs it still selects points adaptively by maximizing EI but using the current surrogate; periodically refits to incorporate new data and update parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Large-budget or time-limited QS experiments (e.g., TSP with large number of sequential selections and limited wall-clock time)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same semi-discrete black-box problems; emphasis on scenarios where time for model refitting dominates experimental or simulation cost and more sequential runs are desired under a time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Example: TSP fast-QS-learning run used a 1-hour time budget, produced 96 total runs with only 12 MaGP refits; sequential runtime per run ~1 minute (fast) vs ~4 minutes for full QS-learning per run in the reported example.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In TSP example the fast QS-learning variant (96 runs, 12 refits) achieved maximum response 313, which was better than averages from BM2/BM3 baselines (254/264) and competitive given the time constraint (though slightly worse than full QS-learning which found 336 using 88 runs but with longer per-run compute).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves wall-clock sample throughput by reducing refit frequency and using O(n^2) incremental updates; the example shows ~4× faster per-run time versus full QS-learning in one scenario, enabling more adaptive trials under fixed time budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Still driven by EI; reduced re-estimation frequency may slightly reduce adaptivity (hyperparameter updates) which could modestly affect the balance but empirical results remain good.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Full QS-learning (per-iteration refit), random sampling, BM2/BM3 baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Fast QS-learning trades frequent surrogate refitting for significantly faster per-run time while retaining good adaptive performance under time constraints; useful when experiments/simulators are fast and many sequential runs are desired.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May degrade performance if hyperparameters drift significantly and are not refitted often enough; requires careful allocation of compute budget (t, T) and choice of reestimate frequency parameter α.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling and Active Learning for Experiments with Quantitative-Sequence Factors', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient global optimization of expensive black-box functions <em>(Rating: 2)</em></li>
                <li>Information-theoretic regret bounds for gaussian process optimization in the bandit setting <em>(Rating: 2)</em></li>
                <li>Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Adaptive design of experiments based on gaussian processes <em>(Rating: 2)</em></li>
                <li>Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness <em>(Rating: 1)</em></li>
                <li>A latent variable approach to Gaussian process modeling with qualitative and quantitative factors <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1142",
    "paper_id": "paper-252090421",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "QS-learning",
            "name_full": "Quantitative-Sequence learning (QS-learning)",
            "brief_description": "An active learning / Bayesian-optimization style agent for experiments whose inputs combine continuous quantities and sequence (permutation) factors; it couples a mapping-based additive Gaussian process surrogate (MaGP), expected-improvement acquisition, and specialized optimization/design tools to adaptively select experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "QS-learning",
            "agent_description": "A sequential experimental-design agent that (1) fits a mapping-based additive Gaussian process (MaGP) surrogate to data over mixed continuous+sequence inputs, (2) chooses new trials by maximizing Expected Improvement (EI) under the MaGP predictive distribution, and (3) uses tailored optimizers (QS-EGO / SFTA) and QS-design initializations. It also has a 'fast' variant that freezes hyperparameters for batches and uses O(n^2) updates between refits.",
            "adaptive_design_method": "Active learning / Bayesian optimization (Expected Improvement)",
            "adaptation_strategy_description": "QS-learning fits a probabilistic surrogate (MaGP) to collected runs, computes EI for candidate inputs (x, o), and selects the next experiment by maximizing EI. For sequence inputs it alternates optimizing continuous quantities given a sequence and optimizing sequences given quantities (when enumeration is infeasible), using QS-EGO (GA+BFGS) and SFTA for sequence search. It updates the surrogate after each new observation (or in batches for the fast variant) and stops when EI fails to improve sufficiently or budget/time is exhausted.",
            "environment_name": "Black‑box QS experiments (case studies: lymphoma drug combination experiment; traveling salesman problem (TSP) simulator; single-machine scheduling; arranging four operations)",
            "environment_characteristics": "Black-box objective functions; semi-discrete input space combining continuous quantities and permutation/sequence inputs; environments can be deterministic (computer simulators) or noisy (physical experiments); large combinatorial sequence space (k! permutations), possibly non-separable interactions between quantity and order; limited budget (small-sample regime).",
            "environment_complexity": "Varies by benchmark: lymphoma case k=3 (6 sequences, 24 total runs in original experiment); TSP example k=8 (8! = 40,320 sequences) with 8 continuous stay durations (x_i ∈ [1,4]); QS-design initial sizes recommended ~O(k^2) (e.g., 46-run QS-design for TSP), sequential budgets tens to low hundreds of runs; surrogate estimation cost O(n^3) per refit (fast variant reduces refit frequency and uses O(n^2) updates).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Case study (lymphoma): QS-learning found the true maximum response 47.18 using 15 total runs (8-run initial QS-design + 7 sequential), saving 37.5% of the original 24-run budget. TSP simulation: starting from a 46-run QS-design and selecting 42 sequential runs (88 runs total) under 2d-MaGP, QS-learning found maximum profit 336 (found at 41st sequential run). Fast QS-learning (time-limited) with 96 total runs found max 313. Compared to baselines: large random sample (4,032,000) found 325 (worse than QS-learning's 336), sequential generalized-PWO and CP baselines (BM2/BM3) had average results 254 and 264 (best replications 321 and 324) in the TSP study, clearly inferior on average.",
            "performance_without_adaptation": "Random one-shot or baseline sequential methods: lymphoma random sampling (BM1) selecting 15 of 24 runs has only a 62.5% chance to include the optimal treatment; BM2/BM3 in lymphoma starting from QS-design found only 43.93 (vs QS-learning 47.18). In TSP, BM1 (very large random sample) found max 325; BM2 and BM3 averaged 254 and 264, best observed 321 and 324. Fast QS-learning (adaptive) outperformed random or classical sequential baselines in reported experiments.",
            "sample_efficiency": "High in small-sample regime: lymphoma optimum recovered with 15 runs (initial n0=8 then 7 sequential); TSP optimum near-found with 88 runs total versus millions of random samples required to approach similar results; fast variant reduced wall-clock time allowing more runs under time budget (example: sequential run time ~1 min for fast QS-learning vs ~4 min for general QS-learning per run in TSP example).",
            "exploration_exploitation_tradeoff": "Balanced via EI acquisition: EI has an exploitation term (favoring low posterior-mean) and an exploration term (favoring large posterior variance). For sequences, SFTA Phase I encourages space-filling (exploration) and Phase II uses threshold-accepting (gradually reduced thresholds) to refine promising sequences (exploitation). A suggested ε-greedy enumerative trick is mentioned to avoid local optima by occasionally sampling random sequences.",
            "comparison_methods": "Random sampling (BM1); sequential generalized PWO (pairwise ordering) model (BM2); sequential generalized CP (component-position) model (BM3); large random search/enumeration; random-search sequence optimizer (baseline to SFTA).",
            "key_results": "Integrating the MaGP surrogate (which maps permutation orders into learned latent coordinates) with EI and specialized sequence optimization (SFTA) and QS-design initializations yields strong small-sample performance on mixed continuous+sequence black-box problems: it recovered the true optimum in a 24-run real drug experiment using only 15 runs, and outperformed baseline methods across simulations (TSP, scheduling, arranging operations). SFTA (space-filling + threshold-accepting) is effective for large permutation spaces when combined with EI-driven quantitative optimization. The fast QS-learning variant trades off refitting frequency for wall-clock efficiency while preserving good solutions.",
            "limitations_or_failures": "Theoretical convergence of EI is not guaranteed in all settings; convergence rates rely on kernel smoothness and assumptions (remarks cite Bull (2011) and stabilized EI results). Performance depends on a good initial QS-design—poor initial designs degrade results; enumerating all sequences is infeasible for large k, requiring heuristics (SFTA) whose quality depends on tuning and sampling budget. Computational scaling: standard GP refits cost O(n^3); mapping-dimension t choice is a trade-off between flexibility and number of parameters (full-MaGP can be very high-dimensional). Case-study limitations: the real drug experiment had very few dose levels, limiting curvature estimation. Reported failures/weaknesses: BM2/BM3 often performed worse and sometimes failed to include the optimum; random-search comparators performed poorly versus SFTA.",
            "notes": "All algorithmic components (MaGP, QS-EGO, SFTA, QS-design, fast QS-learning) are proposed and empirically used in this paper.",
            "uuid": "e1142.0",
            "source_info": {
                "paper_title": "Modeling and Active Learning for Experiments with Quantitative-Sequence Factors",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "QS-EGO",
            "name_full": "QS Efficient Global Optimization (QS-EGO)",
            "brief_description": "The EI maximization / inner optimization engine inside QS-learning that alternates continuous-variable optimization (GA + BFGS using analytic gradients) with sequence optimization (SFTA) to maximize Expected Improvement over mixed continuous+sequence inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "QS-EGO",
            "agent_description": "An optimizer that maximizes the EI acquisition function under the MaGP surrogate by iteratively: (1) optimizing quantitative variables x given a fixed sequence o via a hybrid genetic algorithm with BFGS (analytical gradients provided), and (2) optimizing sequence o given x via the SFTA algorithm; repeated multi-start rounds and stopping criteria are used.",
            "adaptive_design_method": "Acquisition-function optimization for Bayesian optimization (Expected Improvement) tailored to mixed continuous+permutation inputs",
            "adaptation_strategy_description": "Alternating coordinate optimization: with current sequence o_c, run GA+BFGS to optimize continuous x; with optimized x_c, run SFTA to optimize sequence o; accept the best EI found over rounds and use chosen (x,o) as next experiment. Uses analytic EI gradients (derived for MaGP) to accelerate continuous optimization and multi-start/space-filling initialization for robustness.",
            "environment_name": "Mixed continuous+sequence black-box optimization problems used throughout the paper (lymphoma, TSP, scheduling, operation ordering)",
            "environment_characteristics": "Same semi-discrete black-box environments: continuous quantities × permutation sequences, possibly noisy measurements (physical experiments) or deterministic simulators; high-dimensional sequence combinatorics.",
            "environment_complexity": "Requires optimizing over continuous dimension (sum of k continuous variables) and discrete permutation space of size k!; for TSP k=8 (40,320 permutations) enumeration is infeasible, so QS-EGO uses SFTA.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "QS-EGO used within QS-learning enabled efficient EI maximization leading to the reported experimental successes: e.g., in the TSP experiment QS-EGO + MaGP led to identification of the best found profit 336 after 88 runs; in small problems enumeration of all sequences with QS-EGO is used when k small.",
            "performance_without_adaptation": null,
            "sample_efficiency": "By enabling effective EI maximization over mixed domains, QS-EGO contributes to the small-sample successes reported (e.g., 15 total runs to find optimum in lymphoma); the paper reports that EI optimization rounds N_round typically set 10–100.",
            "exploration_exploitation_tradeoff": "Managed by EI itself; QS-EGO's alternating routine helps both exploitation (improving predicted best means) and exploration (via EI's variance term and SFTA's space-filling Phase I).",
            "comparison_methods": "Enumeration (when feasible), random search over sequences, naive sequence optimization baselines (random-search sequence optimizer).",
            "key_results": "QS-EGO's hybrid strategy with analytic gradients makes EI optimization tractable over continuous parts and amenable to heuristic but effective searches over permutations, enabling successful adaptive experiments in semi-discrete domains.",
            "limitations_or_failures": "When k is large, the sequence optimization remains heuristic (SFTA) and may require many function evaluations; QS-EGO performance depends on quality of surrogate and on hyperparameter estimation; no absolute optimality guarantee for the mixed-domain EI maximization is provided.",
            "uuid": "e1142.1",
            "source_info": {
                "paper_title": "Modeling and Active Learning for Experiments with Quantitative-Sequence Factors",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "SFTA",
            "name_full": "Space-Filling Threshold-Accepting (SFTA)",
            "brief_description": "A two-phase algorithm for optimizing sequence (permutation) inputs under EI: Phase I produces space-filling diverse candidate sequences; Phase II uses threshold-accepting (a deterministic analog of simulated annealing) with empirically determined thresholds to refine solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SFTA",
            "agent_description": "Sequence optimizer combining (Phase I) space-filling candidate generation using Hamming-distance-based acceptance probabilities to initialize diverse starting points, and (Phase II) a Threshold-Accepting local/global search with decreasing thresholds derived from empirical increments to move from exploration toward exploitation.",
            "adaptive_design_method": "Heuristic global optimization tailored for discrete permutation spaces used inside an EI-driven active learning loop",
            "adaptation_strategy_description": "Phase I selects sequences that are far (in Hamming distance) from already-observed sequences to encourage exploration; Phase II starts from the best Phase I sequence and iteratively accepts neighbor permutations if objective increment δ is below a current threshold τ_r (thresholds decreasing over rounds), enabling escape from local optima early and fine-tuning later.",
            "environment_name": "Permutation (order-of-addition) optimization in QS experiments (TSP sequences, drug-order sequences, scheduling sequences)",
            "environment_characteristics": "Large discrete permutation spaces (k! permutations), neighbor moves defined by swapping two elements (Hamming distance 2), objective is EI (computed via MaGP given fixed continuous x), search budget constrained (600 sequences vs 1000 in random baseline in example).",
            "environment_complexity": "Example: TSP with k=8 (40,320 permutations). In SFTA example, Phase I used n(1)=100 space-filling candidates, Phase II used n_seq=500 neighbors to build threshold distribution, and 100 rounds/steps as tuning (typical practical settings given).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In an empirical comparison (TSP first-EI selection), SFTA with 600 sequences (100 Phase I + 500 Phase II) found the true maximum EI value (5.34) in most of 100 replications (results were 5.28 or 5.34, with 5.34 the true maximum in most trials). Random search comparing 1000 random sequences (similar CPU time) produced much worse and less consistent EI maxima.",
            "performance_without_adaptation": "Random-search baseline (1000 random sequences) typically failed to find the true EI maximum and produced inferior EI values in the presented replication study.",
            "sample_efficiency": "SFTA found accurate EI-maximizing sequences with only 600 sequence evaluations (under a second on a laptop in the reported example), whereas random search needed more evaluations and still underperformed.",
            "exploration_exploitation_tradeoff": "Explicitly structured: Phase I emphasizes exploration via space-filling Hamming-distance-based sampling; Phase II uses threshold-accepting with decreasing thresholds to trade toward exploitation over rounds.",
            "comparison_methods": "Random search over sequences (baseline), enumeration (when feasible for small k).",
            "key_results": "SFTA provides an efficient and robust sequence optimizer inside EI loops: space-filling initialization plus threshold-accepting refinement outperformed random search in the paper's experiments and enabled effective EI maximization for large permutation domains within modest evaluation budgets.",
            "limitations_or_failures": "Performance depends on SFTA tuning parameters (n(1)_steps, n_seq, n_rounds, n(2)_steps) and on available evaluation budget; no theoretical optimality guarantees; may require more evaluations for very large k or extremely rugged EI landscapes.",
            "uuid": "e1142.2",
            "source_info": {
                "paper_title": "Modeling and Active Learning for Experiments with Quantitative-Sequence Factors",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Fast QS-learning",
            "name_full": "Fast QS-learning (batch-refit variant)",
            "brief_description": "A computationally efficient variant of QS-learning that fixes MaGP hyperparameters for batches of sequential selections and uses fast O(n^2) Sherman–Morrison–style updates to the covariance inverse between refits, trading off some adaptivity for wall-clock efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Fast QS-learning",
            "agent_description": "Instead of reestimating GP hyperparameters after every new observation, this variant records the time cost t of fitting and, for the next floor(N * t / T) iterations, keeps parameters fixed and uses O(n^2) updates to Φ^{-1} and predictive quantities when adding points; hyperparameters are reestimated periodically to respect a total compute-time budget.",
            "adaptive_design_method": "Active learning / EI with infrequent surrogate hyperparameter reestimation and fast covariance updates",
            "adaptation_strategy_description": "Adapts selection frequency of hyperparameter reestimation to remaining time budget; within fixed-parameter epochs it still selects points adaptively by maximizing EI but using the current surrogate; periodically refits to incorporate new data and update parameters.",
            "environment_name": "Large-budget or time-limited QS experiments (e.g., TSP with large number of sequential selections and limited wall-clock time)",
            "environment_characteristics": "Same semi-discrete black-box problems; emphasis on scenarios where time for model refitting dominates experimental or simulation cost and more sequential runs are desired under a time budget.",
            "environment_complexity": "Example: TSP fast-QS-learning run used a 1-hour time budget, produced 96 total runs with only 12 MaGP refits; sequential runtime per run ~1 minute (fast) vs ~4 minutes for full QS-learning per run in the reported example.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In TSP example the fast QS-learning variant (96 runs, 12 refits) achieved maximum response 313, which was better than averages from BM2/BM3 baselines (254/264) and competitive given the time constraint (though slightly worse than full QS-learning which found 336 using 88 runs but with longer per-run compute).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Improves wall-clock sample throughput by reducing refit frequency and using O(n^2) incremental updates; the example shows ~4× faster per-run time versus full QS-learning in one scenario, enabling more adaptive trials under fixed time budgets.",
            "exploration_exploitation_tradeoff": "Still driven by EI; reduced re-estimation frequency may slightly reduce adaptivity (hyperparameter updates) which could modestly affect the balance but empirical results remain good.",
            "comparison_methods": "Full QS-learning (per-iteration refit), random sampling, BM2/BM3 baselines.",
            "key_results": "Fast QS-learning trades frequent surrogate refitting for significantly faster per-run time while retaining good adaptive performance under time constraints; useful when experiments/simulators are fast and many sequential runs are desired.",
            "limitations_or_failures": "May degrade performance if hyperparameters drift significantly and are not refitted often enough; requires careful allocation of compute budget (t, T) and choice of reestimate frequency parameter α.",
            "uuid": "e1142.3",
            "source_info": {
                "paper_title": "Modeling and Active Learning for Experiments with Quantitative-Sequence Factors",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient global optimization of expensive black-box functions",
            "rating": 2,
            "sanitized_title": "efficient_global_optimization_of_expensive_blackbox_functions"
        },
        {
            "paper_title": "Information-theoretic regret bounds for gaussian process optimization in the bandit setting",
            "rating": 2,
            "sanitized_title": "informationtheoretic_regret_bounds_for_gaussian_process_optimization_in_the_bandit_setting"
        },
        {
            "paper_title": "Bayesian optimization",
            "rating": 2,
            "sanitized_title": "bayesian_optimization"
        },
        {
            "paper_title": "Adaptive design of experiments based on gaussian processes",
            "rating": 2,
            "sanitized_title": "adaptive_design_of_experiments_based_on_gaussian_processes"
        },
        {
            "paper_title": "Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness",
            "rating": 1,
            "sanitized_title": "convergence_guarantees_for_gaussian_process_means_with_misspecified_likelihoods_and_smoothness"
        },
        {
            "paper_title": "A latent variable approach to Gaussian process modeling with qualitative and quantitative factors",
            "rating": 1,
            "sanitized_title": "a_latent_variable_approach_to_gaussian_process_modeling_with_qualitative_and_quantitative_factors"
        }
    ],
    "cost": 0.02258975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Modeling and Active Learning for Experiments with Quantitative-Sequence Factors</p>
<p>Qian Xiao 
Department of Statistics
University of Georgia
AthensGA</p>
<p>Yaping Wang 
School of Statistics
East China Normal University
ShanghaiChina</p>
<p>Abhyuday Mandal 
Department of Statistics
University of Georgia
AthensGA</p>
<p>Xinwei Deng 
Department of Statistics
Virginia Tech
BlacksburgVA</p>
<p>Modeling and Active Learning for Experiments with Quantitative-Sequence Factors
Adaptive designGaussian process modelglobal optimizationorder- of-addition experimentsequential experiment
A new type of experiment that aims to determine the optimal quantities of a sequence of factors is eliciting considerable attention in medical science, bioengineering, and many other disciplines. Such studies require the simultaneous optimization of both quantities and the sequence orders of several components which are called quantitative-sequence (QS) factors. Given the large and semi-discrete solution spaces in such experiments, efficiently identifying optimal or near-optimal solutions by using a small number of experimental trials is a nontrivial task. To address this challenge, we propose a novel active learning approach, called QS-learning, to enable effective modeling and efficient optimization for experiments with QS factors. QS-learning consists of three parts: a novel mapping-based additive Gaussian process (MaGP) model, an efficient global optimization scheme (QS-EGO), and a new class of optimal designs (QS-design). The theoretical properties of the proposed method are investigated, and optimization techniques using analytical gradients are developed. The performance of the proposed method is demonstrated via a real drug experiment on lymphoma treatment and several simulation studies.</p>
<p>Introduction</p>
<p>In modern scientific areas, nontraditional experiments that consider the quantities and sequences for arranging components, called quantitative sequence (QS) factors, are being conducted. For example, both the doses and order-of-addition for multiple drug components as a combination showed significant impacts on the efficacy of cancer treatments (Ding et al., 2015;Wang et al., 2020a). In nanocellulose (NC) gel production, a pre-treatment process involved swelling agents, different acids and enzymes to release hemicellulose. The sequence in which the pretreatment components were added, along with their quantities, was to be optimized for the NC size (Bharimalla et al., 2015). In the bio-plastics industry, the order in which the compatibilizer and scavenger were mixed with resin, along with their quantities, can cause a significant difference between catfish algae plastic and Solix microalgae plastic.</p>
<p>Such QS factors are also used in physical or simulation experiments (a.k.a. computer experiments) in biochemistry (Shinohara and Ogawa, 1998), food science (Jourdain et al., 2009) and management science (Panwalkar et al., 1973).</p>
<p>To illustrate the characteristics of experiments with QS factors, Table 1 presents three runs from an in vitro drug combination experiment (Wang et al., 2020a). Three antitumor drugs (A, B, and C) were added every 6 hours in a sequence at different doses. The percentage of tumor inhibition was measured as the response 6 hours after administering the last drug. As indicated in Table 1, different drug doses (comparing Runs 1 and 2) and sequences of adding drugs (comparing Runs 1 and 3) lead to varying responses. Thus, to identify the best drug combination, the doses and sequence for administering drugs should be optimized simultaneously. Notably, this experiment is different from crossover trials (Jones and Kenward, 2014). A crossover trial measures all responses after the addition of every drug, and each drug exerts a fixed effect that may be carried over to the next period but does not depend on its order-of-addition. By contrast, only the end point efficacy after adding all the drugs will be measured as the response in a QS experiment, and drug effects are assumed to be dependent on the order-of-addition. For experiments with QS factors, one of the key tasks is finding the optimal settings of quantities and sequences for arranging components to optimize experimental outcomes.</p>
<p>In the current literature, researchers frequently enumerate all possible sequences and apply factorial designs to determine the quantities for each sequence (Wang et al., 2020a). However, when the number of components is moderate or large, such a strategy may require a prohibitively large number of runs. It may also miss the optimal setting unless a wide range of levels is adopted. To the best of our knowledge, very few studies have been conducted on how to optimize the settings of QS factors via efficient modeling and experimental design.</p>
<p>This problem is new and challenging, because QS factors are neither purely continuous nor categorical. To fill this gap, we propose a novel active learning approach, called QS-learning, which can identify good solutions by using only a few sequential experimental trials.</p>
<p>Active learning has attracted considerable attention in recent years (Cohn et al., 1996;Settles, 2009;Deng et al., 2009). It sequentially queries the next data point on the basis of what it has learned from the current ones. Different methods for formulating query strategies have been proposed in the literature, including uncertainty sampling (Lewis and Gale, 1994), query-by-committee (Seung et al., 1992), expected model change (Settles et al., 2007) and variance reductions (Cohn et al., 1996). Refer to Settles (2009) for a survey.</p>
<p>From the experimentation perspective, active learning overlaps with optimal design (Burnaev and Panov, 2015) and Bayesian optimization (Frazier, 2018). It allocates runs in an adaptive manner, efficiently improving the decision for designing the next experimental trial as more information is acquired over time. Active learning frequently outperforms one-shot experimental designs for optimization when the solution space is large and complex (Kapoor et al., 2007;Burnaev and Panov, 2015;Frazier, 2018). It involves three major parts: (1) a method for statistical modeling and inference, (2) optimization of some acquisition functions for choosing the next design point, and (3) an initial design for exploring input space. Here, the acquisition function is typically a function that measures the "utility" of the run that will be evaluated next. It often considers "exploration/exploitation" trade-off, such that balance is achieved between focusing on alternatives that appear to be good and experimenting with little known alternatives. Common choices of acquisition functions include expected improvement (Jones et al., 1998), knowledge gradient (Frazier et al., 2009), and entropy search (Hennig and Schuler, 2012).</p>
<p>In this work, we propose an active learning approach (QS-learning) for experiments with QS factors. It includes a novel mapping-based additive Gaussian process (MaGP) model for prediction and uncertainty quantification, a sequential scheme that uses efficient global optimization algorithms (QS-EGO), and a new class of optimal experimental designs (QSdesign) for collecting initial data points. A flowchart of the proposed QS-learning method is presented in Figure 1. The proposed method targets experiments with budget constraints in which practitioners prefer fewer runs. For cases with large data, we develop a variant of QS-learning for computational efficiency.</p>
<p>The key contributions of this work are summarized as follows. First, our proposed MaGP model enables the use of the Gaussian process (GP) in analyzing quantitative and sequence factors, providing desirable predictions and uncertainty quantification. Notably, the classic GP method has been widely used for modeling data with only quantitative factors (Rasmussen and Williams, 2006;Kleijnen, 2009). Some recent developments have enabled it for both quantitative and qualitative factors (Qian et al., 2008;Deng et al., 2017;Zhang et al., 2019;Xiao et al., 2021). However, this method cannot be easily adapted for modeling data with QS factors due to the semi-discrete nature of sequence input. Second, we develop a new algorithm (QS-EGO) for efficiently optimizing the expected improvement (EI) acquisition function (Jones et al., 1998), which is nontrivial for QS factors. To address a complex solution space with both continuous and semi-discrete characteristics, the proposed QS-EGO combines a genetic algorithm and a new space-filling threshold-accepting (SFTA) algorithm.</p>
<p>We derive analytical gradients for model estimation and acquisition function optimization to facilitate computation. Third, we develop a new class of optimal designs (QS-designs)</p>
<p>for collecting initial data in the proposed active learning. QS-designs can reduce the number of required sequential runs while simultaneously improving performance. New design criteria are established to search for QS-designs with flexible sizes. We also develop an algebraic construction for QS-designs with certain sizes and prove their desirable properties.</p>
<p>In the current experimental design literature, researchers have focused on either quantitative (Wu and Hamada, 2021;Joseph, 2016) or sequence (Mee, 2020;Voelkel, 2019;Lin and Peng, 2019;Yang et al., 2021) factors, while the proposed QS-designs consider both factors simultaneously.</p>
<p>The remainder of this article is organized as follows. In Section 2, we review several related methods in the literature. In Section 3, we describe the formulation and estimation of the proposed MaGP model in detail. In In Section 4, we discuss the proposed sequential scheme along with QS-EGO. In Section 5, we illustrate the construction of a new class of optimal designs (i.e., QS-designs). A case study of a drug combination experiment on lymphoma is reported in Section 6, and a simulation study on the traveling salesman problem (TSP) is presented in Section 7. We conclude this work with discussions in in Section 8. All proofs, technical details, convergence results, and additional numerical studies are included in the Supplementary Materials.</p>
<p>Brief Literature Review</p>
<p>QS factors are commonly observed in drug combination studies (Wang et al., 2020a). However, conventional methods often consider only the effects of drug doses (quantitative input), e.g., the Hill model (Chou, 2006), polynomial model (Jaynes et al., 2013), Hill-based model (Ning et al., 2014), and Kriging or GP model (Xiao et al., 2019). Some recent studies have shown that if several drugs with fixed doses are added in desirable sequences, then such drug combinations will have enhanced efficacy (Ding et al., 2015). To model drug sequences with fixed doses, two types of linear models are proposed: the pairwise ordering (PWO) model (Van Nostrand, 1995;Voelkel, 2019;Mee, 2020) and component-position (CP) model (Yang et al., 2021). We first review the two models and then generalize them for QS factors.</p>
<p>Let us consider a drug combination experiment with n runs and k drugs. For its i th run, let x i = (x i,1 , . . . , x i,k ) T be a vector containing the doses of k drugs and α i = (α i,1 , . . . , α i,k ) T be a vector containing the sequence of applying them. For example, if we add Drug B first, then C and finally A (k = 3), then the sequence of adding drugs (B, C, A) is represented by the vector α i = (2, 3, 1) T . In the PWO model, the features of α i are represented by the precedence patterns between all k 2 pairs of drugs. Explicitly, let S be the set of all pairs (p, q) for 1 ≤ p &lt; q ≤ k and define the PWO indicator between p and q for any (p, q) ∈ S as consider the PWO model, which often suffices in practice. The triplet PWO model includes as many as 1 + k + k(k − 1)/2 + k(k − 1)(k − 2)/3 parameters, which often exceed the total number of runs in sequential experiment considered in this work.</p>
<p>Another class of linear models is CP model (Yang et al., 2021), which is defined as
f (x T i , α T i ) = β 0 + k−1 j=1 k−1 c=1 x (j) i,c β j,c + i ,(2)
where x (j) i,c equals 1 if α i,j = c and 0 otherwise. That is, x</p>
<p>i,c is an indicator of whether Drug c is used in the ith run at the jth position. Simply put, CP is a multivariate linear regression model treating each position as a factor with k levels.</p>
<p>Both the PWO and CP models in the current literature work only for sequence factors.</p>
<p>In order to establish some benchmark models, we generalize the PWO and CP models via adding covariates for quantitative factors (e.g., doses), such that they can work for QS factors. Both the generalized PWO and CP models can be represented as
g(x T i , α T i ) = k j=1 β j x i,j + f (x T i , α T i ),(3)
where β j denotes the coefficients for quantitative factors, and f (x T i , α T i ) can be either the PWO model in (1) or the CP model in (2).</p>
<p>Linear models may work well under one-shot experimental designs for prediction purposes, but they are less popular in active learning for optimization. Compared with GP models, linear models often perform worse in uncertainty quantification (Smith, 2013;Burnaev and Panov, 2015). The GP model, where responses are represented by random variables whose probability distributions characterize the beliefs of experimenters about the unknown values, provides a good probabilistic framework for active learning (Rasmussen and Williams, 2006;Kapoor et al., 2007;Frazier, 2018). It enables the predictive distribution of the outcome of the next experiment and the selection of the best one by maximizing an acquisition function.</p>
<p>3 Mapping based Additive GP Model</p>
<p>Model Formulation</p>
<p>Let us consider a QS experiment with n runs and k components (i.e., c 1 , c 2 , . . . , c k ), where the i th input is denoted as w i = (x T i , o T i ) T and the corresponding output is denoted as that o is a permutation of the integers 1 to k. As an illustration, the third run in Table 1,
y i . Here, x i = (x i,1 , . . . , x i,k ) T isw T 3 = (x T 3 , o T 3 )
, has x 3 = (3.75, 95, 0.16) T and o 3 = (3, 1, 2) T . The vector o 3 represents that Drug A is added in the third place, B is added in the first place, and C is added in the second place, i.e., B → C → A. Notably, the o defined here contains the index orders of the corresponding elements in the vector α defined in Section 2, and they have the same practical meaning.</p>
<p>The order sequence is semi-discrete in nature; hence, the relationship between output and QS input can be complicated. To model such data, we consider the adoption of the GP model because of its flexibility and promising prediction and uncertainty quantification. For an experiment with n runs and k components, we model the output at w = (
x T , o T ) T as Y (w) = µ + k h=1 G h (w) + ,(4)
where G 1 , . . . , G k are independent zero-mean GPs with stationary covariance functions, and ∼ N (0, τ 2 ) is a random error. The GP component G h (h = 1, . . . , k) corresponds to the effect of the h th component c h on the output. For physical experiments, we assume homogeneous error variances τ 2 &gt; 0 which may come from measurement errors or some environmental factors. For computer experiments, we take τ 2 = 0, because computer codes provide deterministic output (Fang et al., 2005).</p>
<p>In GP models, distances between pairs of input are used to measure their similarities </p>
<p>i,h ). Given that the sequence input o is an assignment of k components to k "fixed" order positions, we should use the same mapping for all components c 1 , . . . , c k (corresponding to GP components G 1 , . . . , G k , respectively) to quantify the effects of fixed order positions via latent variables. In particular, the t-dimensional mapping (t = 1, . . . , k − 1) for the order of any component is defined as
c 1 , . . . , c k             1 2 . . . k →      õ (1)õ(2) ...õ (t) δ (1) 1 δ (2) 1 . . . δ (t) 1 δ (1) 2 δ (2) 2 . . . δ (t) 2 . . . . . . . . . . . . δ (1) k δ (2) k . . . δ (t) k       k×t ,(5)
where δ (j) l = 0 for all j ≥ l to avoid over-parametrization. The interactions among different levels (i.e., orders) can be reflected by the mapping parameters in (5), which are estimated from the data. As all components use the same mapping, the total number of mapping parameters is t(t + 1)/2 + (k − t − 1)t. Specifically, when t = k − 1, we call it full mapping with a total of k(k − 1)/2 mapping parameters. When t = 2, we call it 2d-mapping, which has (2k − 3) mapping parameters.</p>
<p>Example 1 For illustration, consider a QS experiment to find the optimal sequence and quantity to add for k = 4 operations in a single production line with four fixed locations to be assigned with different operations. We use the same mapping for all four operations (c 1 , c 2 , c 3 , c 4 ), which quantifies the effects due to locations (i.e., position orders) to be assigned with operations:</p>
<p>full mapping
c 1 , c 2 , c 3 , c 4           order 1 order 2 order 3 order 4 →      õ (1)õ(2)õ(3) 0 0 0 δ (1) 2 0 0 δ (1) 3 δ (2) 3 0 δ (1) 4 δ (2) 4 δ (3) 4       , 2d-mapping c 1 , c 2 , c 3 , c 4           order 1 order 2 order 3 order 4 →      õ (1)õ(2) 0 0 δ (1) 2 0 δ (1) 3 δ (2) 3 δ (1) 4 δ (2) 4       , where δ (j)
l (j &lt; l) denotes parameters to be estimated via maximum likelihood estimation (MLE).</p>
<p>The prespecified tuning parameter t (t ∈ {1, . . . , k − 1}) controls the flexibility of defining similarities between pairs of order positions. Under full mapping (t = k − 1), all pairwise distances between order positions can be independently determined. Then, all possible patterns in defining similarities between sequence input can be captured. By contrast, under 1d-mapping (t = 1), the mapping in (5) is simplified as order 1 → 0, order 2 → δ 1 , . . . , order k → δ k−1 , or equivalently order 1 → 0, order 2 → δ 1 , order 3 → δ 1 + δ 2 , . . . , order k → k−1 i=1 δ i . Evidently, only the distances between adjacent order positions are independently determined here. For example, when t = 1, the distance between orders 1 and 3 (determined via δ 1 + δ 2 ) is dependent on the distance between orders 1 and 2 (determined via δ 1 ) and the distance between orders 2 and 3 (determined via δ 2 ). Such restrictive mapping works for cases where only adjacent orders interact with one another. In this work, we consider t ≥ 2 to allow a more general pattern of interactions.</p>
<p>An appropriate choice of t provides a trade-off between model flexibility and computational cost. When many components are involved (i.e., large k), low-dimensional mapping (e.g., 2d-mapping) is often a good choice. It will considerably reduce the number of parameters in the MaGP model, facilitating model estimation in practice. In 2d-mapping, any pairwise distance between order positions can be partially (not fully) determined by other pairwise distances. Thus, this type of mapping can provide certain flexibility to capturing possible patterns for defining similarities between sequence input.
In the i th run w i = (x T i , o T i ) T , the elements that correspond to the h th component c h are (x i,h , o i,h ), where i = 1, .
. . , n and h = 1, . . . , k. From the mapping in (5), we define the distance between the i th and j th runs that correspond to the h th component c h under the L 2 norm as
d (h) i,j = ||(x i,h , o i,h ) − (x j,h , o j,h )|| = θ h (x i,h − x j,h ) 2 + t l=1 (õ (l) i,h −õ (l) j,h ) 2 ,(6)
where θ h is the correlation parameter that scales the quantitative input of c h . Here, the
t-dimensional latent vectors (õ (1) i,h , . . . ,õ (t) i,h ) and (õ (1) j,h , . . . ,õ (t)
j,h ) correspond to the orders o i,h in w i and o j,h in w j , respectively, and their values are determined by the mapping parameters in (5) (denoted as δ). Notably, there is no need to include any correlation parameters to scale latent vectors, because the mapping parameters (δ) are estimated from the data.</p>
<p>Subsequently, we can describe the proposed covariance function for the h th GP component
G h (h = 1, . . . , k) in (4) as φ h (w i , w j |σ 2 h , θ h , δ) = φ h ((x i,h , o i,h ), (x j,h , o j,h )|σ 2 h , θ h , δ) = σ 2 h K(d (h) i,j ),(7)
where σ 2 h is the variance parameter corresponding to the h th component c h , and K(·) is any valid kernel function. Popular kernels include the Matern class with a smoothness parameter ν ∈ (0, ∞),
K(d (h) i,j ) = 2 1−ν Γ(ν) ( √ 2νd (h) i,j ) ν k ν ( √ 2νd (h) i,j ),(8)
where k ν is a modified Bessel function of the second kind. Specifically, we focus on the case of ν → ∞, i.e. the Gaussian kernel, in this work:
K(d (h) i,j ) = exp(−(d (h) i,j ) 2 ).(9)
In (9), we remove a constant multiplier of 1/2 in the exponent for re-parameterization.</p>
<p>By (4), (5), (6), (7), and (9), for any two input w i and w j , the covariance function for the MaGP model in (4) can be specified by:
φ(w i , w j ) = Cov(Y (w i ), Y (w j )) = k h=1 φ h (w i , w j |σ 2 h , θ (h) , δ) + τ 2 1(w i = w j ) = k h=1 σ 2 h exp − θ h (x i,h − x j,h ) 2 − t l=1 (õ (l) i,h −õ (l) j,h ) 2 + τ 2 1(w i = w j ),(10)
where τ 2 0, and 1(·) is an indicator function. Here, the variance parameter σ 2 h corresponds to the effect of the h th component, and τ 2 is the variance of the error term ∼ N (0, τ 2 ) in (4). This covariance function combines different dimensions via addition, and the quantity part and sequence part in each dimension via multiplication. It cannot be decomposed into the sum or product of a covariance for purely quantitative factors and a covariance for purely sequence factors. Thus, it is not the separable covariance function as defined in Gneiting (2002).</p>
<p>Given the noise variance τ 2 , the MaGP model with the covariance function in (10) includes n par = 1 + 2k + kt − t(t + 1)/2 parameters. Specifically, the full-MaGP (t = k − 1) and the 2d-MaGP (t = 2) include 1 + k(k + 3)/2 and 4k − 2 parameters, respectively.
Theorem 1 Given n input w i = (x T i , o T i ) T (i = 1, .
. . , n), the covariance matrix of outputs y = (Y (w 1 ), . . . , Y (w n )) T induced by the covariance function in (10) is positive semi-definite.</p>
<p>Theorem 1 holds for any w 1 , . . . , w n , including duplicated input, and τ 2 can be 0. For appropriate model inference, Cov(y) must be positive definite, and the following two corollaries shed some light on this aspect.</p>
<p>Corollary 1 Given n input and the noise variance τ 2 &gt; 0, the covariance matrix Cov(y) induced by the covariance function in (10) is positive definite.</p>
<p>Corollary 2 When the noise variance τ 2 = 0, if no two runs have the same quantitative input (i.e., x i = x j for i = j), then the covariance matrix Cov(y) induced by the covariance function in (10) is positive definite.</p>
<p>Corollary 1 guarantees the validity of the covariance matrix for modeling physical experiments. For modeling computer experiments, if Latin hypercube designs (Lin and Tang, 2015), orthogonal arrays (Hedayat et al., 1999), or space-filling designs (Wang et al., 2018) are used as the quantitative parts of design matrices (where all x i = x j for i = j), then the covariance matrices in the proposed model are positive definite by Corollary 2. In Section 5, we propose a new class of optimal designs for QS factors that satisfies the requirements in Corollary 2 and has more attractive properties. Notably, if two runs have the same quantitative part but different sequence parts, then we need to set δ (l−1) l = 0 for l = 2, . . . , k in the model estimation to guarantee that the covariance matrix is positive definite.</p>
<p>Notably, the warping technique in the literature (Snelson et al., 2004;Xiao and Xu, 2021) can be used for ordinal factors, where an ordinal input o i,h is mapped to a quantitative input f (o i,h ) via a certain transformation function f h (·). Evidently, such a technique is a special case of, and thus, more restrictive than the 1d-mapping used in the current work. The warping technique frequently considers the case of independent ordinal factors. However, the sequence factors in QS experiments are not independent, because they are required to form sequence input (i.e. permutations of 1, . . . , k). Zhang et al. (2019) considered a latent approach for mapping qualitative input to some quantitative vectors in GP. Their method differs from the proposed MaGP in at least two aspects. First, they considered a single GP with a multiplicative covariance structure wherein a single variance parameter is adopted.</p>
<p>Such a model structure may not distinguish the specific effects of different qualitative factors.</p>
<p>By contrast, the MaGP model considers additive GPs, wherein each GP component has a specific variance parameter that measures the effect of each component. Second, Zhang et al. (2019) set different mapping matrices for various qualitative factors. While, the MaGP model adopts the same mapping for all components, because the order sequence is an assignment of k components to k "fixed" order positions.</p>
<p>Model Estimation</p>
<p>For parameter estimation, the proposed MaGP model in (4) with the covariance function in (10) contains parameters µ, σ 2 = (σ 2 1 , . . . , σ 2 k ) T , θ = (θ 1 , . . . , θ k ) T , δ = (δ (1) 2 , . . . , δ (t) k ) T , and τ 2 . These parameters can be estimated via the likelihood function. The covariance matrix is denoted by Φ = Φ(σ 2 , θ, δ, τ 2 ) = (φ(w i , w j )) n×n , which follows the covariance function in (7). With some simple algebra, the negative log-likelihood function can be expressed as (up to a constant)
log|Φ| + (y − µ1) T Φ −1 (y − µ1),(11)
where the response vector y = (Y (w 1 ), . . . , Y (w n )) T , and 1 is an n × 1 column vector of all 1s. For given σ 2 , θ, δ, and τ 2 , the MLE of µ can be obtained explicitly as
µ = (1 T Φ −1 1) −1 1 T Φ −1 y.(12)
By substituting (12) into (11), the estimation of σ 2 , θ, δ, and τ 2 can be obtained by
[σ 2 , θ, δ, τ 2 ] = argmin log |Φ| + (y T Φ −1 y) − (1 T Φ −1 1) −1 (1 T Φ −1 y) 2 .(13)
This minimization problem can be solved using some standard nonlinear optimization algorithms in Matlab or R. Different algorithms or the same algorithm with different initializations may lead to different parameter estimates (Erickson et al., 2018). In the current work, we adopt the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm with random initialization (Liu and Nocedal, 1989). It is a popular method for estimating GP models.</p>
<p>It determines descent direction by preconditioning the gradient with curvature information.</p>
<p>Numerical gradients can be used, but they are approximate and are expensive to compute.</p>
<p>In the current study, we derive analytical gradients to facilitate fast and exact computation.</p>
<p>We report all analytical gradients and the implementation of the optimization algorithm in Supplementary Materials Sections S1.2 and S3.1, respectively.</p>
<p>Given all the estimated parameters, the prediction mean and variance of the response at the target input w * are given by
Y (w * ) = µ + γ T Φ −1 (y − µ1),(14)s 2 (w * ) = φ(w * , w * ) − γ T Φ −1 γ + (1 − 1 T Φ −1 γ) 2 (1 T Φ −1 1) .(15)
Here, γ is the covariance vector (φ (w * ,
w i )) n×1 , where φ (w * , w i ) = k h=1 σ 2 h exp −θ h (x i,h − x * ,h ) 2 − t l=1 (õ (l) i,h −õ (l) * ,h ) 2 .
Notably, the estimators in (14) and (15) are commonly used in the literature (Rasmussen and Williams, 2006;Kleijnen, 2009;Gramacy, 2020). For an unbiased small-sample estimator of s 2 (w * ), refer to Mehdad and Kleijnen (2015).</p>
<p>For computer experiments with τ 2 = 0, when w * is the i th observed input w i , γ T is the i th row in Φ, and γ T Φ −1 is a row vector with the i th entry being 1 and the others being 0. Evidently, Y (w * ) = Y (w i ) = y i , and thus (15) yields s 2 (w * ) = 0. Therefore, the interpolation property holds. Note that if Φ is ill-conditioned, then a nugget (or noise) effect may be added, and the interpolation property may not hold; refer to Gramacy and Lee (2012) for details. For physical experiments, the interpolation property does not hold due to the presence of random errors in (4). In practice, when no replicates are included in physical experiments, the homogeneous noise variance τ 2 of random errors is often prespecified according to known background information, and different small τ 2 values may not lead to a significant difference in prediction (Xiao et al., 2019). When replicates are included, τ 2 should be estimated via MLE as shown in (13). For some basic derivations of GP model estimation, refer to Rasmussen and Williams (2006) and Roustant et al. (2012) for a survey.</p>
<p>Active Learning for Experiments with QS Factors</p>
<p>In this section, we first introduce a general active learning scheme for experiments with QS factors and then discuss its variant for computational scalability. Given the large and semi-discrete input spaces in such experiments, adapting existing methods for optimization is nontrivial. This issue motivates us to develop a tailored new optimization algorithm.</p>
<p>EI Optimization with QS Factors</p>
<p>In experimentation, active learning has received considerable attention since the expected improvement (EI) framework, which works for quantitative factors, was proposed by Jones et al. (1998). In this section, we adapt the EI acquisition function to work with QS factors and develop an efficient global optimization algorithm (QS-EGO). This new algorithm adopts the proposed MaGP as the probabilistic model for the input-output relationship, under which we derive the analytical gradients for optimizing EI.</p>
<p>Without loss of generality, we focus on finding the optimal solution w to minimize the "black-box" objective function y(w) in either physical or computer experiments. Notably, any maximization problem can be viewed as a minimization problem to the negative objective function −y(w). Let the improvement function be I(w) = (y (n) min − y(w)) + for an input w, where a + = max(a, 0) indicates the nonnegative part of a, and y (n) min is the minimum response of the n current observations. EI is defined as E[I(w)] = I(w)f n (y|w)dy, where f n (y|w) is the probability density function of the predictive distribution given by the MaGP model based on the n current observations. EI at input w = (x T , o T ) T can be expressed in closed form as
EI = E[I(w)] = (y (n) min − Y (w))Φ y (n) min − Y (w) s(w) + s(w)ϕ y (n) min − Y (w) s(w) ,(16)
where Φ(·) and ϕ(·) denote the cumulative distribution function and probability density function of the standard normal distribution, respectively. The prediction mean Y (w) and</p>
<p>its standard error s(w) = s 2 (w) are provided in (14) and (15), respectively. EI inherits a trade-off between exploitation and exploration (Jones et al., 1998). The first term in (16) is maximized by the experimental point having the smallest mean value, and thus, it can be interpreted as the exploitation part. Meanwhile, the second term is maximized by the unexplored point having the largest uncertainty, and thus, it can be interpreted as the exploration part.</p>
<p>As shown in Figure 1, the workflow of QS-learning includes the following four steps.</p>
<ol>
<li>
<p>Construct an optimal initial design for QS factors with n 0 runs w 1 , . . . , w n 0 . Compute (or simulate) their responses as y 1 , . . . , y n 0 . Then, fit the MaGP model based on these observations. Set n = n 0 .</p>
</li>
<li>
<p>Select the next design point w n+1 that maximizes the EI acquisition function in (16) by using the QS-EGO (shown as Algorithm 1), and then compute (or simulate) its response as y n+1 .</p>
</li>
<li>
<p>Re-fit the MaGP model based on observations (w 1 , y 1 ), . . . , (w n+1 , y n+1 ). Set n = n+1.</p>
</li>
</ol>
<p>Repeat</p>
<p>Steps 2 and 3 until the stopping criterion is satisfied.</p>
<p>In</p>
<p>Step 2, when the number of components k is small, we can enumerate k! sequences (possibly with parallel computing) and identify the optimal x given each sequence o that can maximize the EI function. For a large k, such an enumeration may become prohibitively time-consuming. To address this challenge, we propose to iteratively optimize quantitative input x and sequence input o given the other, as summarized in Algorithm 1. In both the four-step QS-learning and Algorithm 1, we adopt the stopping criterion used in Jones et al. (1998), i.e., that the algorithm stops when three consecutive EIs do not produce more than α (α ∈ [0.1%, 1%]) improvement over the current best output.</p>
<p>In Algorithm 1, to optimize the quantitative input x given the sequence input o c , we adopt a BFGS method in a hybrid genetic optimization algorithm (Mebane, Jr. and Sekhon, 2011).</p>
<p>Numerical gradients suffice for small cases, but they can be slow for large ones. In this study, we derive the analytical gradients for EI maximization under the proposed MaGP model.</p>
<p>These gradients are exact and fast to compute. See Supplementary Materials Section S1.3 for details.</p>
<p>In Algorithm 1, optimizing the sequence input o given the quantitative input x c is nontrivial, because its solution space is semi-discrete and can be extremely large. To address this issue, we propose the so-called space-filling threshold accepting (SFTA) algorithm for a large k. As detailed in Algorithm 2, the proposed SFTA algorithm includes two phases.</p>
<p>The first phase seeks space-filling points that are far from one another to achieve robustness Algorithm 1 An efficient optimization of EI for QS factors (QS-EGO) Initialize the maximum number of rounds N round .</p>
<p>Initialize the current input w c = (x c , o c ). Set the current optimal input w opt = w c .</p>
<p>Initialize an empty vector ei and a scalar ei opt = 0.</p>
<p>for i = 1 to N round do 1. Given o c , find the quantitative input x that maximizes EI via a genetic optimization algorithm. Set x c = x.</p>
<ol>
<li>Given x c , find the sequence input o that maximizes EI via the SFTA method in (16).
Algorithm 2. Set o c = o. 3. Set w c = (x c , o c ) and evaluate E[I(w c )] defined inif E[I(w c )] &gt; ei opt , then set ei opt = E[I(w c )] and w opt = w c . Set ei[i] = ei opt , where ei[i] denotes the i th element of ei.
if the stopping criterion is satisfied, then break.</li>
</ol>
<p>end for</p>
<p>Return w opt . (Johnson et al., 1990). The second phase inherits from the classic threshold-accepting (TA)</p>
<p>algorithm (Dueck and Scheuer, 1990), which balances exploration and exploitation. 
O obs = [o T 0 ] T . while i n (1) step do Generate a random sequence o i . if H min (o i , O obs )/k &gt; ( is drawn from Unif(0,1)), then let i = i + 1, O obs = [O obs , o T i ] T , and δ = f (o i ) − f (o opt ), if δ &lt; 0, then let o opt = o i . end while Set the current solution o c = o opt . for j = 1 to n seq do Generate a neighbor solution N (o c ), and let ∆ j = |f (o c ) − f (N (o c ))|.</p>
<p>end for</p>
<p>Compute the empirical distribution of ∆ j , j = 1, 2, . . . , n seq , denoted as F .</p>
<p>for r = 1 to n rounds do Generate threshold τ r = F −1 (0.5(1 − r/n rounds ))
for i = 1 to n (2) steps do Generate a neighbor solution N (o c ), and let δ = f (N (o c )) − f (o c ). if δ &lt; τ r , then let o c = N (o c ). if f (o c ) &lt; f (o opt ), then let o opt = o c . end for end for Return o opt .
because threshold values decrease. The SFTA algorithm can avoid being trapped at local optima and focus more on exploration in the beginning.</p>
<p>When the number of allowed evaluations is considerably fewer than the total number of possible sequences, random initialization (and generation) of neighbor solutions may not consistently and efficiently explore space. To address this issue, we adopt space-filling samples in Phase I, which provide a good initialization for TA global optimization in Phase II. When parallel computing is available, more than one solution o opt in Phase I can be selected as multi-starting points. Refer to Supplementary Materials Section S3.2 for additional details on Algorithms 1 and 2.</p>
<p>Fast QS-learning for Large Experiments</p>
<p>In most literature on experimentation, the costs for estimating surrogate models and assessing acquisition functions are negligible compared with the costs of conducting experiments (Frazier, 2018;Gramacy, 2020). In other cases, experiments may be executed rapidly, and researchers will need a fast sequential scheme for a large number of runs (Gramacy, 2020). 1. Construct an optimal initial design for QS factors with n 0 runs w 1 , . . . , w n 0 , evaluate their responses y 1 , . . . , y n 0 , and fit the MaGP model based on these observations. Set n = n 0 . Record the time used for fitting the MaGP model as t. Record the time left from the budget T as T and the number of runs left from the budget N as N .</p>
<ol>
<li>For the next N t/T iterations, fix the estimated parameters of the MaGP model and sequentially select runs based on EI by using the fast updating technique illustrated below. Set n = n + N t/T ( a is the largest integer not exceeding a).</li>
</ol>
<p>Refit the MaGP model (including reestimating all parameters) based on observations</p>
<p>(w 1 , y 1 ), . . . , (w n , y n ). Update the time used for fitting the model as t. Record the time left from the budget as T and the number of runs left from the budget as N .</p>
<p>Repeat</p>
<p>Steps 2 and 3 until the stopping criterion is satisfied.</p>
<p>In</p>
<p>Step 2, we adopt fast updating of model fit in O(n 2 ) computing time given all parameters in the MaGP model. Let Φ n be the covariance of the n current input. The key is to update the model when the (n + 1) th data point arrives via fast calculation of the covariance Φ n+1 and its inverse Φ −1 n+1 . Similar to a rank one Sherman-Morrison update (Sherman and Morrison, 1950), we have
Φ n+1 = Φ n γ γ T φ(w n+1 , w n+1 ) , Φ −1 n+1 = Φ −1 n + gg T v g g T v −1 ,
where the covariance function φ is defined in (10), the covariance vector
γ = (φ (w n+1 , w i )) n×1 with φ (w n+1 , w i ) = k h=1 σ 2 h exp − θ h (x i,h − x n+1,h ) 2 − t l=1 (õ (l) i,h −õ (l) n+1,h ) 2 for i = 1, . . . , n, v = φ(w n+1 , w n+1 ) − γ T Φ −1 γ and g = −v −1 Φ −1 n γ.
Here, the update on the covariance inverse requires O(n 2 ) time, and thus, the updates to all relevant quantities for each model fit are of O(n 2 ) (Gramacy, 2020). The total cost of updating with sequential runs from n = n 0 , . . . , N demands flops in O(N 3 ). Compared with the general QS-learning that reestimates parameters for every sequential run, the fast QS-learning method reestimates them much less frequently, and thus, saves computations for large experiments. Refer to Supplementary Materials Section S3.3 for details on the stopping criterion and parameter tuning.</p>
<p>Notably, small-sample performance is often more important and relevant than the convergence rate in experimentation, because only a small number of trials are frequently allowed in practice (Fang et al., 2005). Asymptotic guarantees may provide minimal information about the practical effectiveness of the method (Sutton and Barto, 2018). The discussion of convergence for learning QS experiments is included in Section S2 of the Supplementary Materials.</p>
<p>Optimal Initial Designs for QS-learning</p>
<p>Desirable initial designs are important in active learning. They may save the total number of runs and lead to better solutions. In this section, we propose a new class of optimal designs for QS factors, called QS-design, which exhibits space-filling and pair-balanced properties. We first propose a general approach for constructing QS-designs with flexible sizes in Section 5.1, and then provide a deterministic algebraic construction for QS-designs with certain sizes in Section 5.2.</p>
<p>General Construction</p>
<p>The design for QS factors is denoted as D = (X, O) where X is the quantitative part, and O is the sequence part. Both parts use components as columns. To construct a desirable design D, we will first construct a good sequence design O, and then construct a good quantitative design X in combination with O. For illustration, the following two designs have the same practical meaning:
O =    A B C 1 2 3 2 1 3 2 3 1    ⇔ O =    1 2 3 A B C B A C C A B    .
To identify the optimal sequence design O, we first find the optimal O by minimizing the following ν p criterion:
ν p =     ρ 1 k i=1 k j=1 i =j 1 (t i,j + 1) p + ρ 2 n i=2 i−1 j=1 1 (h i,j + 1) p     1 p ,(17)
where t i,j is the number of appearances of the subsequence "(i j)" in rows of O ; h i,j is the Hamming distance between the i th and j th rows in O ; and ρ 1 , ρ 2 , and p are tuning parameters. A design is called pair-balanced if it has the same t i,j value for all subsequences (i.e., pairs) of (i, j), where we use {1, 2, . . . , } to denote the levels {A, B, . . .}. A pairbalanced design assigns equal importance to all pairwise interactions among components. It also accounts for different precedence patterns where pairs (i, j) and (j, i) are different. It is similar to the "balance" idea in crossover designs (Dean et al., 2015). To find (near) pairbalanced designs, we propose to maximize designs' minimum t i,j values, which is equivalent to minimizing the term 1/(t i,j + 1) p in (17) for a sufficiently large tuning parameter p.</p>
<p>In practice, p = 15 often suffices. In the denominators, we add 1 to t i,j (and h i,j ) to avoid numerical problems when they are equal to 0.</p>
<p>The term 1/(h i,j + 1) p in (17) considers designs' space-filling properties. Here, we adopt the popular maximin distance criterion (Johnson et al., 1990), which seeks to scatter design points over the experimental domain such that the minimum pairwise distance between points is maximized. The Hamming distance is used here because the elements in O are categorical. Analogous to the scalar criterion in Morris and Mitchell (1995), minimizing the term 1/(h i,j + 1) p is asymptotically equivalent to the maximin Hamming distance criterion as p goes toward infinity, where p = 15 often suffices. A space-filling O benefits the exploration of the response surface and is a robust choice for initial points (Frazier, 2018).</p>
<p>In this study, we set weights ρ 1 = 0.2 and ρ 2 = 0.8 in (17) 
O A =       1 2 3 4 A B C D B C D A C D A B D A B C       , O B =       1 2 3 4 A B C D B D A C C A D B D C B A       ⇔ O B =       A B C D 1 2 3 4 3 1 4 2 2 4 1 3 4 3 2 1       .O A 3 0 0 0 3 0 0 0 3 3 0 0 O B 1 1 1 1 1 1 1 1 1 1 1 1
To search for optimal designs O , we adopt a standard TA algorithm (Dueck and Scheuer, 1990;Xiao and Xu, 2018) by using the criterion ν p in (17) as the objective function. The algorithm starts with a random design O and defines its neighbor design N (O ) by exchanging two random levels in a random row. It can be implemented with the R package "NMOF" (Gilli et al., 2019).</p>
<p>After obtaining the optimal O , we propose to construct the optimal D = (X, O )</p>
<p>minimizing the following C p criterion, which measures a design's space-filling property.
C p = n i=2 i−1 j=1 1 (ρ 1 d i,j + ρ 2 h i,j + 1) p 1 p ,(18)where d i,j = k l=1 (x il − x jl ) 2
is the L 2 -distance between the rows x i , and x j in X and h i,j is the Hamming distance between the rows o i and o j in O . Here, we adopt weights ρ 1 = ρ 2 = 0.5 and the tuning parameter p = 15.</p>
<p>To search for the optimal D given O , we adopt the same TA algorithm. It starts from
a design D c = (X c , O ), where X c is the maximin distance Latin hypercube design (LHD)
found by the R package "SLHD"  or "LHD" (Wang et al., 2020b). Notably, the maximin distance LHD is a popular type of space-filling design, which has been proved to be robust for model misspecification and can minimize the theoretical prediction variance of fitted GP models (Gramacy, 2020). Here, the C p criterion in (18) is used as the objective function, and neighbor designs N (D c ) are defined by exchanging two randomly chosen rows of X c . Finally, we convert the optimal D to its equivalent form, i.e., the QS-design D. In practice, we may need to normalize quantitative designs X to [0, 1] range.</p>
<p>Algebraic Construction Method</p>
<p>We develop an algebraic construction for QS-designs whose component sizes k and run sizes n are p r − 1, where p r is any odd prime number. Denote the n × n good lattice point design (Zhou and Xu, 2015) as D glp , whose i th row is h × i mod p r , where vector h = (1, . . . , n) and i = 1, . . . , n. Design D glp is a Latin square whose rows and columns are both permutations of 1, . . . , n. To construct D = (X, O ), we propose to use O = D glp and X as any column permutation of D glp . For illustration, design O b in Example 2 is a 4 × 4 D glp , where we treat A as 1, B as 2, and so on.</p>
<p>Theorem 2 Let n = k = p r − 1, where p r is any odd prime number. Then, the n-run sequence design O = D glp has the following properties (for any 1 ≤ i = j ≤ n):</p>
<p>(i) O is the maximin Hamming distance design, where all h i,j = n;</p>
<p>(ii) O is the pair-balanced design, where all t i,j = 1;</p>
<p>(iii) O is optimal under the ν p criterion defined in (17) for any positive weights ρ 1 and ρ 2 , and it has a ν p value of:
ν p (O ) = n(n − 1) ρ 1 2(n + 1) p + ρ 2 2 p 1 p .(19)
Theorem 3 Let n = k = p r − 1 (p r is any odd prime number), and D = (X, O ), where O = D glp and X is any column permutation of D glp . D has the following properties:</p>
<p>(i) the minimum row-pairwise L 2 -distance in X is n(n + 1)(n + 2)/12;</p>
<p>(ii) it has a upper bound of C p as defined in (18)
, i.e., C p (D ) ≤ n 1 p C(ρ 1 , ρ 2 , p), where C(ρ 1 , ρ 2 , p) =   n/2 − 1 ρ 1 1 12 n(n + 1)(n + 2) 1 2 + nρ 2 + 1 p + 1 2 ρ 1 1 3 n (n 2 − 1) 1 2 + nρ 2 + 1 p   1 p
is a constant that only depends on adopted weights ρ 1 and ρ 2 in the C p criterion.</p>
<p>Corollary 3 Let n = k = p r − 1 (p r is any odd prime number), and X be any column
permutation of D glp . Then, the minimum row-pairwise L 2 -distance in X, denoted as d(X), satisfies d(X) d upper = n + 2 2n &gt; √ 2 2 ,
where d upper = n (n + 1)/6 is the upper bound of d(X).</p>
<p>Theorem 2 shows that the proposed sequence design O is optimal under both the spacefilling and pair-balanced criteria. Theorem 3 shows that the constructed D = (X, O ), or equivalently the QS-design D = (X, O), has the best space-filling property, i.e., the minimized C p value. Corollary 3 shows that the proposed quantitative design X also exhibits good space-filing property, because it has a large minimum pairwise L 2 -distance. Corollary 3</p>
<p>can easily be obtained on the basis of Theorem 3 in Zhou and Xu (2015) along with the proofs for Theorems 2 and 3 in this work. Notably, the upper bound d upper = n (n + 1)/6 may not be achievable for all design sizes.</p>
<p>Although the minimum run size of an initial design in active learning can be as small as 2, many researchers have recommended using initial designs with moderate sizes (Jones et al., 1998;Loeppky et al., 2009;Frazier, 2018). We would remark that the run size of QSdesign can be flexibly determined. One recommended run size is the number of parameters in the GP part, e.g. 4k − 3 for 2d-MaGP and k(k + 3)/2 for full-MaGP. When a larger number of runs is allowed, we recommend a rule-of-thumb run size of 2 + k(k + 3)/2 for any t-dimensional MaGP for simplicity, which is one more than the total number of parameters in full-MaGP. For the special case, when k = p r − 1 and p r is any odd prime, we find that the k-run QS-design proposed in this subsection performs very well, as illustrated in Sections S5</p>
<p>and S6 of Supplementary Materials.</p>
<p>Case Study</p>
<p>Lymphoma is cancer that begins in infection-fighting cells of the immune system, called lymphocytes. When a patient has lymphoma, lymphocytes change and grow out of control. In a recent pioneering work (Wang et al., 2020a), Here, we run the proposed QS-learning to determine if we can use fewer runs (compared with the original 24 runs) to identify the optimal treatment in this experiment. Notably, 2d-MaGP and full-MaGP are the same for k = 3 components. Given that the GP part of the model has eight parameters, we construct an eight-run QS-design to collect the initial data.</p>
<p>The proposed QS-learning selects seven sequential runs until the stopping rule is satisfied,</p>
<p>i.e., the last three EIs are all less than 1% of the current best output. Figure 2 shows the plots for EIs and the cumulative maximum responses of the seven sequential runs.  To further evaluate the proposed QS-learning compared with other approaches, we consider three benchmark methods: the random sampling approach (BM 1 ), sequential generalized PWO approach (BM 2 ), and sequential generalized CP approach (BM 3 ). Here, the BM 1 method considers a random sampling of 15 runs out of the original 24 runs, which is the same total run size used above under QS-learning. Proving that the probability of including the optimal treatment in such a random sample is only 62.5% will be straightforward. The BM 2 and BM 3 methods consider sequential experiments based on the generalized PWO and CP approaches, respectively, introduced in Section 2. Both methods will start from 8 initial runs, and then choose the setting with the optimal prediction to be the next experiment trial until no further improvement is achieved. Specifically, when the BM 2 and BM 3 methods start from the QS-design, the largest response identified is only 43.93. In addition, when they start from random designs, we present the plots of their largest responses found from (47.18) for BM 2 and BM 3 is less than 50%, and many results are not good in Figure 3.</p>
<p>This 24-run real experiment (Wang et al., 2020a) on doses and sequences is a pioneering work in the literature. It serves as a good example to demonstrate the importance of such experiments. Nevertheless, it also has limitations. First, only two dose levels for Drugs A and B are considered, which does not support the estimation of any curvature effect. In addition, only one dose level for Drug C is used, and we cannot estimate its effect. When doses are not restricted to only a few levels and more drugs are included, QS-learning is expected to perform better.</p>
<p>Simulation Study</p>
<p>In this section, we evaluate the performance of the proposed QS-learning and its fast variant through a traveling salesman problem (TSP, Applegate et al. (2007)). In Sections S5 and S6 of the Supplementary Materials, we provide two additional simulations of arranging the four mathematical operations problem (Mee, 2020;Yang et al., 2021) and the single machine scheduling problem (Allahverdi et al., 1999;Wan and Yuan, 2013). These additional results illustrate the advantages of QS-designs (particularly for those from algebraic construction), the superior predictive power of the MaGP model, the difference between 2d-MaGP and full-MaGP, and the general applicability of QS-learning.</p>
<p>TSP is a well-known nonpolynomial-hard problem in combinatorial optimization (Tan et al., 2000;Applegate et al., 2007). Here, we consider a modified TSP that involves the optimization of quantitative input and sequence input. We regard it as a computer experiment wherein the simulator is assumed to be black-box and expensive to evaluate (Fang et al., 2005). The cost for evaluating runs is assumed to be considerably higher than that for estimating surrogate models or assessing acquisition functions.</p>
<p>Suppose a salesman needs to travel to k cities to sell products, indexed as Cities 1, . . . , k.</p>
<p>All cities are available for visiting at time zero, and the salesman must visit all cities one by one. The time to travel from City i to City j (i = j) is s i,j days, and s i,j can be different from s j,i . The salesman will stay in City i for x i days to sell products. He has a due date to complete the business in City i, denoted as d i . If he misses the due date, then a penalty rate of f dollars per day will be charged. After completing his business in each city, he will earn a fixed income of a dollars and a variable income of e dollars per day when staying in the city. During his entire trip, the expense is b dollars per day. In this problem, the target is to identify the optimal traveling schedule that can maximize the profit.</p>
<p>Let us define α = (α 1 , . . . , α k ) as the sequence of cities visited, and the corresponding order sequence is o = (o 1 , . . . , o k ), where City α i is visited at order o α i = i (i = 1, . . . , k). α 0 = 0 is defined to be the starting point at time 0. The completion time of the business in City α i is C(x, α i ) = i l=1 (s α l−1 ,α l + x α l ), and the tardiness (days passed the due time) for City α i is T (x, α i ) = max(0, C(x, α i ) − d α i ). Thus, the profit function that involves the days-staying-in-cities x and the sequence-of-cities-visited α (or equivalently o) is
F (x, α) = ka + e k i=1 x i − bC(x, α k ) − f k j=1 T (x, α j ).
Example 3 Consider the above TSP with k = 8 cities. Here, we set a = 20, e = 10, b = 2, f = 15, due dates (d 1 , . . . , d 8 ) = (26,10,42,23,25,12,44,10), and staying days x i ∈ [1, 4] for i = 1, . . . , 8. The traveling time s i,j (i &lt; j) is sampled from a uniform distribution U (0.5, 3), and set s j,i = (1 + 0.1 × ji )s i,j , where ji is sampled from the standard normal distribution.</p>
<p>Refer to Section S4.2 of Supplementary Materials for additional details of this simulation.</p>
<p>Such a TSP does not have a known analytical solution. We consider the proposed QSlearning to identify the (nearly) optimal setting that will maximize the profit function via a few experimental trials. It starts from the 46-run (the rule-of-thumb run size illustrated in Section 5.2) QS-design and selects 42 sequential runs under the 2d-MaGP model before the stopping criterion is satisfied. The maximum response identified is 336, which is found at the 41 st sequential run. The optimal setting includes x * = (1.14, 3.44, 2.48, 2.86, 3.78, 4.00, 3.11, 4.00) and o * = (8, 6, 2, 1, 4, 5, 3, 7). Figure 4 displays EIs and cumulative maximum responses of sequential runs. After the 10 th sequential run, the ordinal parts in all the runs are either o = (8, 6, 2, 1, 4, 5, 3, 7) or o = (8, 6, 2, 1, 4, 5, 7, 3), both of which are good candidates. Such an observation indicates the stability of QS-EGO (i.e., Algorithm 1 in Section 4.1). In practice, 2d-MaGP is preferred over full-MaGP when many components are involved, because it is more computationally efficient.</p>
<p>To make comparisons, we first consider the random sampling approach (BM 1 ), which uses a large random sample of 4,032,000 observations, where a random Latin hypercube design is used for the quantitative part and a hundred replicates of all possible sequences are used for the sequence part. The maximum response found is 325, which is clearly worse than that identified by the QS-learning by using only 88 runs. Next, we consider the sequential generalized PWO (BM 2 ) and CP (BM 3 ) approaches starting from random initial designs with the required sizes (i.e., 37 and 38 runs, respectively). We replicate BM 2 and BM 3 1000 times. Their average results are 254 and 264, and their best results are 321 and 324, respectively. Their performances are clearly inferior. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q where the average value is 208 and the largest value is 309. Evidently, the fast QS-learning approach appears to exhibit reasonably good performance. </p>
<p>Discussion</p>
<p>In this work, we propose an active learning approach to identify ( In this work, we focus on the widely used EI framework, which works well empirically.</p>
<p>In the current literature, the upper confidence bound (UCB) is another popular framework for working with GP models in active learning, particularly for purely discrete input spaces (Srinivas et al., 2012;Djolonga et al., 2013;Berkenkamp et al., 2019;Vakili et al., 2020).</p>
<p>Results on convergence rates have been established for GP-UCB and its variants. Embedding some of the currently popular non-separable covariance functions (Gneiting, 2002) into the MaGP model is also interesting. For example, one may consider
Cov(Y (x i , o i ), Y (w j , o j )) = σ 2 ψ(||o i − o j || 2 ) 1/d ϕ ||x i − x j || 2 ψ(||o i − o j || 2 ) ,
where || · || is the L 2 norm (or other norms), ϕ(·) is a completely monotonic function (e.g., ϕ(t) = exp(−ct γ )), and ψ(t) is a positive function with a completely monotonic derivative (e.g. ψ(t) = (at α + 1) β ). Such a structure may have an interpretation for certain choices of ϕ and ψ (Haslett and Raftery, 1989). A planned future study can investigate the appropriate choices for ϕ and ψ and their parameters to explain QS experiments.</p>
<p>In active learning and other non-sequential learning methods, optimal designs for experiments with QS factors are important but not well addressed. In this work, we propose criteria for QS-designs that consider designs' space-filling and pair-balanced properties. The current literature presents various types of space-filling designs, including maximin distance designs (Johnson et al., 1990;Xiao and Xu, 2017), minimax distance designs (Chen et al., 2015), uniform designs (Fang and Lin, 2003), MaxPro designs (Joseph et al., 2015) and uniform projection designs (Sun et al., 2019), which can all be used as the quantitative part in the QS-design. For the sequence part, desirable properties beyond the pair balance can be studied analogous to component orthogonal arrays (Yang et al., 2021), order-of-addition orthogonal arrays (Voelkel, 2019) and optimal fractional PWO designs Chen et al., 2020). Moreover, some desirable structures that connect the quantitative and sequence parts of QS-designs can be investigated (Deng et al., 2015). 
i,j = σ 2 exp (−θ(x i − x j ) 2 )
is a function of x i − x j and is isotropic. According to the Bochner's theorem (Theorem 4.1 in Rasmussen and Williams (2006)) and Wiener-Khintchine theorem (Chatfield, 2003), showing the positive semidefiniteness of Gaussian kernel can reduce to finding its spectral density S(s) such that K i,j = S(s) exp(2πis(x 1 −x 2 ))ds. As the Fourier transform of a Gaussian is another Gaussian, we have S(s) = σ 2 (π/θ) 1/2 exp(−π 2 s 2 /θ). Such a conclusion on positive semidefiniteness can be easily generalized to any vector x ∈ R D where S(s) = σ 2 (π/θ) D/2 exp(−π 2 s 2 /θ). If we view the inputs w in the covariance function φ h in (7) as a new input (x,õ (1) , . . . ,õ (t) ) with correlation parameters forõ being 1, the corresponding covariance matrix Φ h = (φ h (w i , w j |σ 2 h , θ h , δ)) n×n is positive semi-definite for h = 1, . . . , k. As the diagonal matrix τ 2 1(w i = w j ) is positive semi-definite and the sum of positive semi-definite matrices are still positive semi-definite, the covariance matrix defined by (10) is positive semi-definite, which completes the proof.</p>
<p>If τ 2 = 0, the diagonal matrix τ 2 1(w i = w j ) is positive definite. Since the sum of a positive definite matrix and some positive semi-definite matrices is positive definite, the covariance matrix in Corollary 1 is positive definite.</p>
<p>When τ 2 = 0, if there are no two runs having the same quantitative inputs (that is x i = x j for i = j), all matrices Φ h (h = 1, . . . , k) are positive definite. Thus, the covariance matrix in Corollary 2 is positive definite.</p>
<p>Proof of Theorem 2 (i) Because p r is an odd prime number, for any 1 ≤ x ≤ n, the set {i × x mod p r : i = 1, . . . , n} equals {1, . . . , n}. Therefore, each column of O is a permutation of 1 to n, and thus the Hamming distance between any two distinct rows of O is n, which is clearly the optimal.</p>
<p>(ii) For any 1 ≤ i = j ≤ k = p r − 1, let α = (j − i) mod p r . Consider the α th row in O . By the construction of D glp , the α th row is α × h mod p r which is a permutation of 1 to n. The first and last elements in the α th row are α and p r − α, respectively. Here, i cannot equal p r − α; otherwise, it will lead to j = 0 mod p r , which is a contradiction. Thus, the sub-sequence "(i j)" must occur in the α th row of O . Next, we show that the sub-sequence "(i j)" can only occur in the α th row of O . If it does not, suppose that "(i j)" also occurs in the α th row with 1 ≤ α = α ≤ n. Since j is adjacent to i in the α th row, j = i + α = i + α, which leads to the contradiction (α = α). Therefore, t i,j = 1 for any 1 ≤ i = j ≤ k, and O is pair-balanced.</p>
<p>(iii) By (i) and (ii), it is straightforward to give the formula for ν p (O ) in (19). To show that ν p (O ) achieves the lower bound among all possible designs, we only need to prove that for any other designÕ ,
n i=2 i−1 j=1 1 (h i,j + 1) p ≥ n(n − 1) 2(n + 1) p and k i=1 k j=1 i =j 1 (t i,j + 1) p ≥ k(k − 1) 2 p = n(n − 1) 2 p .
The first inequality is obvious as h i,j ≤ n for any two rows. The second inequality follows
from the fact that k i=1 k j=1 i =j (t i,j + 1) = 2k(k − 1) and k i=1 k j=1 i =j 1 (t i,j + 1) p ≥ k i=1 k j=1 i =j 1 2k(k−1) k(k−1) p = k i=1 k j=1 i =j 1 2 p .
Proof of Theorem 3 (i) We first consider the pairwise L 2 -distances between rows in X.</p>
<p>Since column permutations do not change the L 2 -distances, WLOG, we take X = D glp .</p>
<p>Denote the minimum L 2 -distance of D as d(D) = min{d i,j : 1 ≤ i &lt; j ≤ n}. Based on the Theorems 1 and 4 in Zhou and Xu (2015), we can prove that
d(X) = k i=1 min{i, p r − i} 2 1 2 =
1 12 k(k + 1)(k + 2) 1 2 = 1 12 n(n + 1)(n + 2) 1 2 , which completes the proof.</p>
<p>(ii) By Theorem 2, O is a Hamming equidistant design with h i,j = k for any 1 ≤ i = j ≤ n. To get the upper bound of C p (D ), the key is the distribution of all pairwise L 2 -distances of the design X. For any 1 ≤ i = j ≤ n = p r − 1, consider the i th and j th rows of X where</p>
<p>x i = i × h mod p r and x j = j × h mod p r . As p r is an odd prime, there exists a unique number l (1 ≤ l ≤ p r − 1) such that j = i × l mod p r . This means that the j th row can also be written as l × (i × h) mod p r . Since each row is a permutation of (1, . . . , n), (x T i , x T j ) T is the same as (x T 1 , x T l ) T = (h T , x T l ) T up to some column permutation. Thus, d i,j equals d 1,l . For all n(n − 1)/2 possible distances d i,j , it suffices to only consider the (n − 1) different d 1,l with l = 2, . . . , n. In addition, for 1 ≤ l ≤ n = p r − 1 and l = p r − 1, there exists a unique number 1 ≤ l ≤ n (l = l) such that l × l = 1 mod p r . Thus, (x T 1 , x T l ) T is the same as (x T l , x T 1 ) T = l (x T 1 , x T l ) T mod p r . Then, all possible d i,j take at most n/2 different values, which are in the set {d 1,i 1 , . . . , d 1,i n/2−1 , d 1,n } where i 1 , . . . , i n/2−1 are in {2, . . . , n − 1} such that the product of any two of them does not equal 1 mod p r . Among all d i,j , there are n of them take the value d 1,i j for j = 1, . . . , n/2 − 1 and n/2 of them take the value of d 1,n . WLOG, let i 1 = 2, we have d 1,2 = 1 12 n(n + 1)(n + 2) 1 2 , which equals the minimum L 2 -distance d(X). Additionally, we can show that
d 1,n = k i=1 [i − (p r − i)] 2 1 2 = 1 3 n n 2 − 1 1 2 .
Denote d 1,i 2 , . . . , d 1,i n/2−1 , d 1,n as d 2 , . . . , d n/2−1 , respectively. We can write the distributions of all pairwise L 2 -distances of X as </p>
<p>Analytical Gradients for Parameter Estimation in MaGP</p>
<p>Following the notations in Section 3 and based on (11), maximizing the likelihood is equivalent to minimizing the function:
f = log |Φ| + (y − µ) T Φ −1 (y − µ),
where µ = (1 T Φ −1 1) −1 1 T Φ −1 y. For any parameter inside Φ, the expression of the analytical gradient given µ is:
∂f ∂• = tr(Φ −1 ∂Φ ∂• ) − (y − µ) T Φ −1 ∂Φ ∂• Φ −1 (y − µ).
In the MaGP model with covariance function in (10), for any i, j = 1, . . . , n and h = 1, . . . , k,</p>
<p>we have:
∂Φ ∂σ 2 h = ∂φ(w i , w j ) ∂σ 2 h n×n , ∂Φ ∂θ h = ∂φ(w i , w j ) ∂θ h n×n , ∂φ(w i , w j ) ∂σ 2 h = exp − θ h (x i,h − x j,h ) 2 exp − t l=1 (õ (l) i,h −õ (l) j,h ) 2 , ∂φ(w i , w j ) ∂θ h = −σ 2 h (x i,h − x j,h ) 2 exp − θ h (x i,h − x j,h ) 2 exp − t l=1 (õ (l) i,h −õ (l) j,h ) 2 .
First, we consider the analytical gradients of mapping parameters δ 1 , . . . , δ 2k−3 in the 2d-MaGP (t = 2) model, where we rearrange the 2k − 3 mapping parameters in (5) by rows.</p>
<p>For any 1 s 2k − 3, define two indicator functions (h = 1, . . . , k): It will be straightforward to prove that δ s corresponds to the order-level s−1 2 + 2 where x is the least integer that is no less than x. The indicator functions I h are used to judge where parameters δ s are used. Thus, we have ∂Φ/∂δ s = (∂φ(w i , w j )/∂δ s ) n×n , and
I (1) h (w i , w j ) = 1 when o i,h = o j,∂φ(w i , w j ) ∂δ 1 = −2 k h=1 σ 2 h (δ 1 −õ (1) j,h )exp − θ h (x i,h − x j,h ) 2 exp − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 I (1) h (w i , w j ) −2 k h=1 σ 2 h (δ 1 −õ (1) i,h )exp − θ h (x i,h − x j,h ) 2 exp − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 I (2) h (w i , w j ) = −2 k h=1 σ 2 h exp − θ h (x i,h − x j,h ) 2 − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 (δ 1 −õ (1) j,h )I (1) h (w i , w j ) +(δ 1 −õ (1) i,h )I (2) h (w i , w j ) , ∂φ(w i , w j ) ∂δ s = −2 k h=1 σ 2 h (δ s −õ (t(s)) j,h )exp − θ h (x i,h − x j,h ) 2 exp − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 I (1) h (w i , w j ) −2 k h=1 σ 2 h (δ s −õ (t(s)) i,h )exp − θ h (x i,h − x j,h ) 2 exp − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 I (2) h (w i , w j ) = −2 k h=1 σ 2 h exp − θ h (x i,h − x j,h ) 2 − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 (δ s −õ (t(s)) j,h )I (1) h (w i , w j ) +(δ s −õ (t(s)) i,h )I (2) h (w i , w j ) ,
where s = 2, . . . 2k − 3, and t(s) = 1 for even s and t(s) = 2 for odd s.</p>
<p>Next, we consider the analytical gradients of mapping parameters δ 1 , . . . , δ k(k−1)/2 in the full-MaGP model (t = k − 1), where we rearrange the k(k − 1)/2 mapping parameters in (5) by rows. For any 1 s k(k − 1)/2, define two indicator functions (h = 1, . . . , k): where δ s corresponds to the order-level l s such that (l s − 1)(l s − 2)/2 &lt; s l s (l s − 1)/2. Similar to the above, we can show that
I (1) h (w i , w j ) = 1 when o i,h = o j,∂φ(w i , w j ) ∂δ s = −2 k h=1 σ 2 h (δ s −õ (t(s)) j,h )exp − θ h (x i,h − x j,h ) 2 exp − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 I (1) h (w i , w j ) −2 k h=1 σ 2 h (δ s −õ (t(s)) i,h )exp − θ h (x i,h − x j,h ) 2 exp − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 I (2) h (w i , w j ) = −2 k h=1 σ 2 h exp − θ h (x i,h − x j,h ) 2 − 2 l=1 (õ (l) i,h −õ (l) j,h ) 2 (δ s −õ (t(s)) j,h )I (1) h (w i , w j ) +(δ s −õ (t(s)) i,h )I (2) h (w i , w j ) ,
where s = 1, . . . k(k − 1)/2 and t(s) = s − (l s − 1)(l s − 2)/2. Finally, it will be straightforward to generalize the above results on analytical gradients to other t-dimensional MaGP model (t = 3, . . . , k − 2).</p>
<p>Analytical Gradients for Optimizing Expected Improvements</p>
<p>Following the notations and analytical results in Sections 3 and 4, the expected improvement in (16) at the input w * = (x * , o * ) can be expressed in the closed form as
E[I(w * )] = (y (n) min − Y (w * ))Φ y (n) min − Y (w * ) s(w * ) + s(w * )ϕ y (n) min − Y (w * ) s(w * )
for the minimization scenario. Using the relations Φ = φ and φ(t) = −tφ(t), the gradient of E[I(w * )] can be derived as:
∇E[I(w * )] = −∇ Y (w * ) × Φ y (n) min − Y (w * ) s(w * ) + ∇s(w * ) × ϕ y (n) min − Y (w * ) s(w * ) .
Based on (14) and (15) and using the fact that ∇s 2 (w * ) = 2s(w * )∇s(w * ), we have:
∇ Y (w * ) = ∇γ T Φ −1 (y − µ1), and ∇s(w * ) = − 1 s(w * ) ∇γ T Φ −1 γ + (1 − 1 T Φ −1 γ)∇γ T Φ −1 1 1 T Φ −1 1 .
Given the sequence part o * in w * and all estimated parameters in the MaGP model, we can derive that (for h = 1, . . . , k and i = 1, . . . , n)
∇φ(w * , w i ) ∇x * ,h = −2θ h σ 2 h (x * ,h − x i,h )exp − θ h (x * ,h − x i,h ) 2 − t l=1 (õ (l) * ,h −õ (l) i,h ) 2 . Thus, we have ∇γ T ∇x * ,h = ∇φ(w * , w 1 ) ∇x * ,h , . . . , ∇φ(w * , w n ) ∇x * ,h , and ∇γ T = ( ∇γ T ∇x * ,1 ) T , . . . , ( ∇γ T ∇x * ,k ) T T .
It will be straightforward to adapt this proof for the maximization scenario.</p>
<p>On the Convergence for Learning QS Experiments</p>
<p>Here, we look at the worst case scenario via studying the convergence rate. We start from considering the setting for computer experiment, i.e. τ 2 = 0 in (10), and the QS-learning with naive EI, where all possible sequences can be enumerated and the EI strategy is applied to the quantitative factors for each sequence. It is known that the EI strategy does not converge for all cases; see counter-examples in Locatelli (1997) and Bull (2011). Bull (2011) showed that the EI strategy converges under Matern kernels when all parameters are fixed. The following Remark 1 directly follows Theorem 2 in Bull (2011) under the same assumptions, since there is a finite number (k!) of sequences and the convergence rate (in terms of n) for only quantitative factors will keep the same as that in the QS-learning with naive EI.</p>
<p>Remark 1 For QS experiments with k components, the aforementioned QS-learning with naive EI under given model parameters converges at least at rate of n −min(ν,1)/k up to log factors, where ν is the smoothness parameter of Matern kernels in (8).</p>
<p>Specifically, by Theorem 2 in Bull (2011) Thus, Remark 2 hold under the same assumptions in Wynne et al. (2021).</p>
<p>Several techniques can be used to enumerate sequences in the aforementioned QS-learning methods, e.g. the -greedy approach (Sutton and Barto, 2018). With a probability of ∈ (0, 1), uniform sampling is performed to draw a random sequence o and then apply the EI to find the best x. With a probability of 1 − , we apply a simplified Algorithm 1 in Section 4.1 where we keep its Step 1 but drop its Steps 2 and 3. That is, we select the sequence o c from the current best observations, and then apply the EI to find the best x c .</p>
<p>Clearly, such an -greedy approach can enumerate all possible sequences and will converge to the global optimum, since there exist converged sub-sequences as stated in Remarks 1 and 2. Here, a good choice of should be small enough to not interfere with the EI algorithm, but large enough to prevent it from getting stuck in a local optimum.</p>
<p>It is worth to remarking that the small-sample performance is often more important and relevant than the rate of convergence in experimentation, since only a few number of trials are often allowed in practice (Fang et al., 2005). The asymptotic guarantees may say little about the practical effectiveness of the methods (Sutton and Barto, 2018). Though the general EI does not converge for all cases, it outperforms many other convergence-guaranteed methods in real applications (Osborne, 2010). In Sections 6 and 7 in the main manuscript and the following Sections S5 and S6 in this Supplementary Materials, results from real and simulation experiments clearly illustrate the superior small-sample performances of our proposed QS-learning, though it may not converge without further restrictions as discussed above.</p>
<p>Additional Implementation Details</p>
<p>Details on Model Estimation</p>
<p>The minimization problem for the model estimation in (13) can be solved via some standard non-linear optimization algorithms in Matlab or R. We recommend to use a BFGS method in a hybrid algorithm from the R package "rgenoud" (Mebane, Jr. and Sekhon, 2011) which is a widely used package for GP estimation. Two key tuning parameters in "rgenoud" package that control the computing time are "pop.size" and "max.generations".</p>
<p>According to the available computing resources, we set "pop.size" from 100 to 1000 and "max.generations" from 10 to 1000, respectively. For large experiments, to further speed up the model estimation, we also recommend to use a low storage BFGS method implemented by the R package "nloptr" (developed by Steven G. Johnson). We set its tuning parameter "maxeval" (which controls the computing time) from 50 to 500 according to the practical needs. In both R packages, analytical gradients developed in Section S1.2 should be used to facilitate the computation.</p>
<p>Details on the Algorithms 1 and 2</p>
<p>We first discuss the parameter tuning in the proposed algorithms. In Algorithm 1, a recommended value for N round is from 10 to 100 based on the time constraint. To optimize the quantitative input x given the sequence input o c , we adopt a BFGS method implemented by the R package "rgenoud" (Mebane, Jr. and Sekhon, 2011), where we set "pop.size" from 100 to 1000 and "max.generations" from 10 to 1000. When the number of components k is small or parallel computing is available, enumeration of all sequences o given x c can be used in Algorithm 1; otherwise, Algorithm 2 (SFTA) should be adopted. In Algorithm 2, we typically set n (1) steps from 100 to 5000, n seq from 500 to 2000, n rounds from 10 to 100 and n (2) steps from 500 to 5000 according to the practical needs.</p>
<p>To illustrate the performance of Algorithm 2 (SFTA), we compare it to a random search method. Here, we consider the same traveling salesman problem (Example 3 in Section 7), and compare the SFTA to a random search for optimizing the sequence input o given the quantitative input x that will lead to the largest EI for selecting the first sequential run, where the estimated model parameters are (σ 2 , θ, δ) = (8522. 10, 4217.90, 6253.08, 5734.86, 4163.44, 3241.29, 4353.13, 1678.87, 15.83, 67.25, 20.31, 54.61, 81.90, 62.45, 0.99, 0.26 In SFTA, 600 sequences (100 sequences in Phase I and 500 sequences in Phase II) are compared, which costs less than a second on a regular laptop computer. We replicate it 100 times and show the maximum EIs found in Figure 6(a). Its results are either 5.28 or 5.34.</p>
<p>It finds the true maximum 5.34 in most replications. As a comparison, we apply a random search method which randomly selects and compares 1000 sequences to find the maximum.</p>
<p>The computing time is similar to that used in SFTA. We also replicate it 100 times and show its results in Figure 6(b). Clearly, most of the results in Figure 6(b) are not good.</p>
<p>Stopping Rules for the Fast QS-learning</p>
<p>In Section 4.2, we set multiple stopping criteria for the fast QS-learning based on the obtained </p>
<p>Additional Details on Numerical Studies</p>
<p>Details on the Case Study</p>
<p>Here we provide some additional details on the case study in Section 6 of the main paper.</p>
<p>We first provide a summary of drugs' names, levels and their corresponding doses in Table 3, and then show the complete data in Table 4.  <br />
QS-design                 X A O A X B O B O C Y                Sequential runs              X A O A X B O B O C Y</p>
<p>Details on the Traveling Salesman Problem</p>
<p>In Example 3 of Section 7, the sampled matrix for s i,j (rounding to 1 decimal place) is 
s i,j i=0,...,k j=1,...,k =                                    .
In the following Figure 7, we show the histogram for the responses of 4,032,000 randomly sampled observations . Then, we show the 46-run QS-design along with its sequential runs of the QS-learning approach based on the 2d-MaGP model.</p>
<p>QS-design
                                                                                                             x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 x 5 o 5 x 6 o 6 x 7 o 7 x 8 o 8 Y
1.10 8 3.51 1 3.38 7 2.73 4 2.40 2 3.58 3 1.95 5 1.49 6 −97.33 3.32 8 2.47 3 2.34 2 2.08 1 1.82 5 3.84 6 3.71 4 3.77 7 −367.75 3.25 2 3.18 3 3.18 1 3.18 7 2.14 5 2.14 8 1.03 6 1.10 4 −341.80 1. 29 6 1.95 3 2.99 7 1.29 4 1.42 8 2.01 1 2.53 2 3.64 5 70.21 3.38 1 2.79 2 2.21 4 3.64 3 3.77 8 3.18 5 3.90 7 2.92 6 −220.43 1.55 7 2.21 2 3.97 3 3.05 5 2.60 4 3.64 1 3.25 6 3.32 8 −152.48 2.47 4 3.25 8 2.86 1 3.58 7 1.10 2 2.99 6 3.77 3 2.27 5 −568.18 2.79 1 1.55 6 3.58 8 3.90 5 3.64 2 2.08 4 2.14 3 2.21 7 −426.28 2.08 1 2.86 4 1.29 5 1.49 6 2.73 2 3.90 3 1.82 7 3.38 8 −58.55 1.36 3 1.42 5 1.68 8 2.66 4 3.12 7 3.77 6 3.12 1 2.34 2 −19.<br />
                                                                                                             Sequential runs under the 2d-MaGP                                                                                                    x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 x 5 o 5 x 6 o 6 x 7 o 7 x 8 o                                                                                                  
Below, we show the sequential runs of the fast QS-learning approach based on the 2d-MaGP model.  </p>
<p>Sequential runs under the 2d-MaGP
                                                                                                        x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 x 5 o 5 x 6 o 6 x 7 o 7 x 8 o 8 Y 2.                                                                                                        9.5y max = (Y 0 /c 4 + c 1 ) × c 3 − c 2 = (Y 0 /3 + 11) × 4 − 2 = 68.67.
Here we consider the QS-learning approach to identify the optimal setting for maximizing the response via experiment trials. We start from the QS-design with the rule-of-thumb run size of 2 + k(k + 3)/2) = 16 (as explained in Section 5), and then the QS-learning is to sequentially select the next design point. When using the 2d-MaGP as the surrogate model, 15 sequential runs are selected until the stopping criterion is satisfied. The optimal settings of (x 1 , . . . x 4 ) = (0.25, 0.40, 1.00, 1.00) (rounding to 2 decimal places) and (o 1 , . . . , o 4 ) =</p>
<p>(2, 4, 3, 1) are successfully identified in the 14 th sequential run. The maximum response found is 68.66, where the minor difference from the truth (68.67) is due to rounding-off errors.      1, 4, 2). Here, the identified settings for the quantitative inputs (x 1 , . . . x 4 ) are optimal. Given these quantities, the identified setting for the sequence inputs (3, 1, 4, 2) is the second best, which leads to the second largest response 68.00 in this study. When using the full-MaGP model, 22 sequential runs are selected and the identified maximum response is 67.99 with (x 1 , . . . x 4 ) = (0.25, 0.42, 1.00, 1.00) and</p>
<p>QS-design
                                   x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 Y 0                                   Sequential runs under 2d-MaGP                                  x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 Y 0.                                 Sequential runs under full-MaGP                                  x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 Y 0.                                
(o 1 , . . . , o 4 ) = (3, 1, 4, 2). As the QS-design from the proposed algebraic construction is small, the 2d-MaGP is often recommended for practical use. Figure 11 displays its expected improvements and cumulative maximum responses.    </p>
<p>QS-design
     x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 Y                                                       x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 Y 0                                                       Sequential runs under full-MaGP                                                                                             
To compare the proposed QS-learning with some benchmark methods, we first consider a random sampling approach (BM 1 ) which is equivalent to using a large random one-shot experimental design. In BM 1 , we draw a very large random sample consisting of 24,000 observations using a random design D r = (X r , O r ) whose quantitative part X r is a random</p>
<p>Latin hypercube design and sequence part O r includes a thousand replicates of all possible sequences. The maximum response found by BM 1 is 66.85, which is clearly inferior compared to that by QS-learning. The histogram of largest responses identified by BM 1 is shown in We also consider the sequential generalized PWO (BM 2 ) and CP (BM 3 ) approaches.</p>
<p>Both methods start from random initial designs of 16 runs, where random Latin hypercube designs and random subsets of all possible sequences are used for the quantitative and sequence parts, respectively. We replicate both methods 1000 times. Figure 13 shows the histograms of their maximum responses found. Even the best results in these 1000 replications (65.52 for BM 1 and 66.51 for BM 2 ) are worse than that by QS-learning. Moreover, most results in Figure 13 are not good. Finally, we evaluate the prediction accuracy of the proposed MaGP model compared to the generalized PWO and CP models. In Table 5, we report the prediction root mean square errors (RMSEs) of different models under various one-shot designs. Since the rule-ofthumb run size for the QS-design is 16 and the QS-learning discussed above includes 28 and 31 runs in total, here we consider random one-shot designs having run sizes of 16, 28 and 31 to form the training sets. In random designs, the quantitative parts are random Latin hypercube designs and the sequence parts include randomly selected sequences. The testing set is formed under a large 2400-run random design whose quantitative part is a random</p>
<p>Latin hypercube design and sequence part includes 100 replicates of all possible sequences.</p>
<p>We replicate the analysis 100 times and report both the median and mean of RMSEs in Table 5. In addition, we also list the results from all models using the training data formed by the proposed 16-run QS-design.</p>
<p>From Table 5, it is seen that both the 2d-MaGP and full-MaGP models have smaller RMSEs (thus stronger prediction power) compared to the generalized PWO and CP models for all cases. Specifically, the proposed MaGP models perform very well under the proposed QS-design. The results from this 16-run design are even better than the median (or average) results from random 31-run designs. This justifies the necessity of using QS-designs to collect initial data in QS-learning. </p>
<p>Additional Simulations on Single Machine Scheduling</p>
<p>Single machine scheduling (SMS) is an NP-hard optimization problem in literature (Emmons, 1969;Allahverdi et al., 1999;Wan and Yuan, 2013), where k jobs are to be sequenced on a single machine. Here we consider a complex SMS problem whose object function is assumed to be black-box and expensive-to-evaluate. In the following Example 5, we consider a classic SMS where only jobs' arrangement sequences are to be optimized. We show that the proposed QS-learning can also work well for only sequence inputs. In Example 6, we further discuss a general SMS where both jobs' processing time and their arrangement sequences are to be optimized.</p>
<p>Example 5 Consider an SMS problem consisting of k = 6 jobs, indexed by Jobs 1, . . . , 6.</p>
<p>These jobs need to be processed on a single machine one after another, where Job i takes a fixed processing time x i for i = 1, . . . , 6. In this example, we randomly assign standardized 
i is T (α i ) = i j=1 x α j .
Consider the quadratic cost of completion time by Townsend (1978), which is defined as
C(α) = k i=1 w i T 2 (α i ),
where (w 1 , . . . , w k ) = (0.3, 0.6, 0.1, 0.9, 0.8, 0.5) are randomly assigned weights. There are 6! = 720 possible sequences in total and the cost function C(α) has a unique global minimum of 22.43 at α * = (4, 5, 6, 2, 3, 1) or equivalently o * = (6, 4, 5, 1, 2, 3).</p>
<p>We consider the proposed QS-learning to minimize the cost via experimental trials. As all processing time (quantitative inputs) are fixed, there are 15 and 21 parameters in the GP parts of the 2d-MaGP and full-MaGP models, respectively. Thus, we start from the QS-designs consisting of 15 and 21 runs (with fixed X part), respectively. When using the 2d-MaGP model as the surrogate, 6 sequential runs are selected before the stopping criterion is satisfied. When using the full-MaGP model, 5 sequential runs are selected. Figures 14 and 15 display the expected improvements and cumulative maximum responses of the selected sequential runs, respectively. It is seen that the QS-learning using both models can successfully identify the true minimum of 22.43 and the optimal sequence o * = (6, 4, 5, 1, 2, 3). In addition, we also try the QS-learning starting from the 6-run QS-design constructed via the algebraic construction (for part O only). When using 2d-MaGP, 9 sequential runs are selected and the optimal sequence o * = (6, 4, 5, 1, 2, 3) is found. Below, we first the 15-run QS-design along with its sequential runs from the QS-learning under 2d-MaGP, and then show the 21-run QS-design along with its sequential runs from the QS-learning under full-MaGP.    Next, we consider using the 6-run QS-design from the algebraic construction for the QS-learning under 2d-MaGP. Figure 16 displays the expected improvements and cumulative maximum responses of sequential runs. It is seen that only 15 runs in total (initial and sequential runs) are needed to identify the optimum. Below, we list the QS-design along with its sequential runs. To make a comparison, we consider the sequential generalized PWO (BM 2 ) and CP (BM 3 ) approaches. The BM 2 and BM 3 start from random initial designs of 16 and 26 runs (random subsets of all possible sequences), respectively. We replicate both methods 1000 times and show the histograms of their smallest response values identified in Figure 17. It is seen that most results from BM 2 and BM 3 are not satisfactory. To evaluate the prediction accuracy of the proposed MaGP model compared to the original PWO model (Van Nostrand, 1995;Voelkel, 2019) and CP model (Yang et al., 2021), here we consider the cases where only sequence inputs are involved. Random one-shot designs with run sizes of 30, 40, 50 and 60 are used to form the training data, and the testing data includes all 720 possible sequences. We replicate the analysis 100 times and report both the median and mean of prediction RMSEs in Table 6. It is seen that both 2d-MaGP and full-MaGP have much smaller RMSEs, thus stronger prediction power, compared to PWO and CP for all cases. Note that both PWO and CP are good at capturing the interactions between the sequence orders (Van Nostrand, 1995;Mee, 2020;Yang et al., 2021). Considering that this SMS example contains significant interactions, it is clear that the MaGP model can better capture the interactions among sequence orders compared to PWO and CP. Example 6 Following the same way to define the cost function C(α) in Example 5, here we consider a general SMS problem where both the processing time x = (x 1 , . . . , x 6 ) (x i ∈ (0, 1)) and job-sequence are to be optimized. Suppose the products manufactured by this single machine can sell for revenue of R(x) = w 0 k i=1 x i . That is, products can sell for higher revenue if more time is spent on each job. The total profit made by this machine with processing time x and job-sequence α (or equivalently order-sequence o) is</p>
<p>QS-design
                                 o 1 o 2 o 3 o 4 o 5 o                                 Sequential runs under 2d-MaGP           o 1 o 2 o 3 o 4 o 5 o          QS-design                                                o 1 o 2 o 3 o 4 o 5 o                                              </p>
<p>QS-design
          o 1 o 2 o 3 o 4 o 5 o 6 YF (x, α) = R(x) − C(α, x) = w 0 k i=1 x i − k h=1 w h T 2 (α h ),
where weights w 1 , . . . , w k are the same as those in Example 5, and we arbitrarily set w 0 = 10.</p>
<p>The target of this study is to identify the optimal setting for maximizing the profit.</p>
<p>It is known that there is no analytical solution for this SMS problem. Here, we consider the proposed QS-learning to identify the optimal setting via experimental trials. It starts from the 29-run (rule-of-thumb run size) QS-design, and then selects 43 and 42 sequential runs under the 2d-MaGP and full-MaGP models, respectively. When using 2d-MaGP, the maximum response found is 21.60 with x = (0.73, 0.46, 0.12, 0.97, 0.99, 0.14) and o = (3, 4, 2, 5, 6, 1) (or equivalently α = (6, 3, 1, 2, 4, 5)). When using full-MaGP, the maximum response found is 21.68 with x = (0.52, 0.28, 0.03, 1.00, 1.00, 0.46) and o = (4, 2, 1, 5, 6, 3). Figures 18 and   19 show the plots for the expected improvements and cumulative maximum responses of sequential runs. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Below, we show the 29-run QS-design along with the sequential runs in the QS-learning under 2d-MaGP and full-MaGP, respectively. </p>
<p>QS-design
                                                                   x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 x 5 o 5 x 6o                                                                  
Sequential runs under the 2d-MaGP 
                                                                                                      x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 x 5 o 5 x 6 o 6 Y 0.                                                                                                     
Sequential runs under the full-MaGP
                                                                                                   x 1 o 1 x 2 o 2 x 3 o 3 x 4 o 4 x 5 o 5 x 6 o 6 Y 0.26 4 0.97 5 21.57                                                                                                   
To make a comparison, we first consider a random sampling approach (BM 1 ). Here, we draw a large random sample consisting of 72,000 observations using a random design whose quantitative part is a random Latin hypercube and sequence part includes a hundred replicates of all possible sequences. We show the histogram of these 72,000 observations in Figure 20, and the maximum response found is 21.53 which is worse than that by the QS-learning using only around 70 runs. In addition, we consider the sequential generalized PWO (BM 2 ) and CP (BM 3 ) approaches starting from randomly selected 22 and 32 runs, respectively. We replicate the BM 2 and BM 3 methods 1000 times. In Figure 21, </p>
<p>Figure 1 :
1A flow chart of QS-learning for optimization.</p>
<p>Specifically, Phase I of SFTA starts from the current optimal sequence input o 0 with the smallest f (o) value in the observed data, where the objective function f (o) is the negative EI value for o given x. Then, the algorithm iteratively accepts a random sequence o i (i = 1, . . . , n (1) ) for evaluation with probability P (o i ) = H min (o i )/k, where H min (o i ) is the minimum pairwise Hamming distance between the sequence vector o i and all observed sequence vectors o 1 , . . . , o i−1 . The Hamming distance is the number of positions at which corresponding symbols are different in two vectors. If a candidate o i is far from the observed o 1 , . . . , o i−1 under the Hamming distance, then it has a high probability of being included for evaluation. Phase II of SFTA starts from the optimal solution o c = o opt with the smallest f (o) value in Phase I. We define the neighbor solution as N (o c ) by randomly exchanging two elements of the sequence vector o c (current solution). Evidently, all possible N (o c )'s have a Hamming distance of 2 with o c . The threshold values for accepting neighbor solutions are generated by empirical distributions of increments (denoted as F ) for the objective function f ; refer to Dueck and Scheuer (1990) for details on the calculation of threshold values. A neighbor solution is more likely to be accepted early in the search than later in the search, Algorithm 2 SFTA algorithm for optimizing o given x c Initialize n (1) step (number of steps) in SFTA Phase I. Initialize n seq (number of iterations to compute the threshold sequence), n rounds (number of rounds), and n (2) steps (number of steps) in SFTA Phase II. Initialize a starting solution o 0 , set current optimal o opt = o 0 , and let</p>
<p>Classic GP-based active learning approaches have a computational complexity of O(N 4 ), where N is the total number of runs. The estimation of GP models in each iteration requires O(n 3 ) computation, where n is the number of data points used. Here, we propose a fast variant of QS-learning with O(N 3 ) computation. Suppose that the total budgets for the number of runs and computing time are N and T , respectively. The proposed fast QSlearning approach includes the following four steps.</p>
<p>Sequence designs have two equivalent representations: one with components as columns (denoted as O), and the other with order positions as columns (denoted as O ). Designs O consist of runs o defined in Section 3, and designs O consist of runs α defined in Section 2.</p>
<p>Figure 2 :
2the researchers conducted a series of drug experiments on lymphoma treatment. Among them, a 24-run in vitro experiment of three Food and Drug Administration (FDA) approved chemotherapeutics, namely, paclitaxel, doxorubicin, and mitoxantrone (denoted as Drugs A, B, and C, respectively), was included. This experiment considered the doses and sequences of drugs. In this experiment, all six sequences of the three drugs were enumerated. For each sequence, two dose levels for A (Level 0: 2.8 µM; Level 1: 3.75 µM) and B (Level 0: 70 nM; Level 1: 95 nM), and a fixed dose level for C (0.16 µM) were considered. The experiment was performed on Raji cells, a human lymphoma cell line. In any treatment (run), each drug was added every 6 hours in a sequence into the Raji cell culture, and the inhibition percentages (the larger, the better the response) were measured 6 hours after the addition of the last drug. The four largest responses in this experiment are 47.18, 44.87, 44.38 and 44.33. Plots for (a) EIs and (b) cumulative maximum responses from the QS-learning approach in the case study.</p>
<p>Figure 3 :
3Histograms of the largest response values identified by (a) sequential generalized PWO approach and (b) sequential generalized CP approach in the case study.</p>
<p>Figure 4 :
4Plots for (a) EIs and (b) cumulative maximum responses from the QS-learning approach under 2d-MaGP in Example 3.Finally, we evaluate the performance of the fast QS-learning approach introduced in Section 4.2. It starts from the same QS-design as above, and we set a total time budget of 1 hour. It includes 96 runs in total, where the adopted 2d-MaGP model is estimated for only 12 times. Its cumulative maximum responses are reported inFigure 5(a). On average, each sequential run takes about 1 minute, while the general QS-learning takes about 4 minutes in this example. The maximum response found here is 313, which is still much better than the average results of BM 2 and BM 3 (i.e., 254 and 264, respectively). In addition, we show the histogram of maximum responses found from 1000 random samples of 96 trials inFigure 5(b),</p>
<p>Figure 5 :
5Plots for (a) cumulative maximum responses from the fast QS-learning approach and (b) maximum responses in random samples of 96 trials in Example 3.</p>
<p>nearly) optimal solutions for experiments with QS factors. Analyzing such experiments is challenging due to their semi-discrete and possibly extremely large solution spaces and complex input-output relationships. The proposed QS-learning includes a novel MaGP surrogate model, an efficient sequential scheme (QS-EGO), and a new class of optimal experimental designs (QS-designs), providing a systematic solution for analyzing QS experiments. The theoretical properties of the proposed method are investigated, and techniques for optimization by using analytical gradients are developed. A case study on lymphoma treatment and several simulations are presented to illustrate the advantages of the proposed method.</p>
<p>For example,Srinivas et al. (2012) proved a cumulative regret bound of n (ν+d(d+1))/(2ν+d(d+1)) by using a Matern kernel of smoothness ν on a d-dimensional space.Vakili et al. (2020) further improved the bound to O(n (d−ν)/d ) for d &gt; ν, O(log(n)) for d = ν, and some constants for d &lt; ν. All these results require finite or general compact input spaces, but the space in the QS-experiment (a joint one of continuous and sequence spaces) does not satisfy this requirement. Considering QS-learning under the UCB framework and studying its convergence rate will be interesting topics for future research.</p>
<p>h
(w i , w j ) = 1 when o i,h = o j,h and o j,h =</p>
<p>h
(w i , w j ) = 1 when o i,h = o j,h and o j,h = l s 0 otherwise .</p>
<p>, the rate is O(n −1/d ) for ν &gt; 1 or O(n −ν/d (log n) α ) for ν ≤ 1 where α is some constant. In the fast QS learning procedure, if the estimated parameters can reach their true values and all possible sequences can be enumerated, it will converge to the global optimum by Remark 1 .In practice, model parameters are estimated from the data and the smoothness parameter (ν) specified in the Matern kernel may be different from the underlying smoothness (ν f ) in the simulation model(Wynne et al., 2021). InWenzel et al. (2021) and Wynne et al.(2021), a γ-stabilized algorithm is developed for quantitative factors with domain X and any acquisition function (including EI) F : X → R. When choosing the (n + 1) th run based on current n observations, it searches over X n,γ = {x ∈ X : P n (x) γ||P n (x)|| L∞ }, where γ ∈ (0, 1] and P n (x) is the square root of the posterior variance. That is, such a strategy only allows points to be selected from areas of non-trivial prediction variances; seeWenzel et al. (2021) orWynne et al. (2021) for detailed discussions. Here, we denote the QS learning with stabilized EI as the method where all possible sequences are enumerated and this γstabilized algorithm using EI acquisition function is applied to the quantitative factors for each sequence. The posterior variance used in P n (x) is given by (15). By Theorem 11 inWynne et al. (2021), it is straightforward to show the following Remark 2.Remark 2 For QS experiments with k components, the aforementioned QS learning with stabilized EI using Matern kernels of smoothness ν &gt; 1 converges at least at rate of n −min(ν,ν f )/k , where ν f is the underlying smoothness.In this QS learning for w = (x, o), the domain of x (denoted as [a, b] k ) is a Lipschitz domain and the domain of o is a finite set of points. For each sequence o, (a, b) k × o is a Lipschitz domain. The union of all (a, b) k × o is a Lipschitz domain, since different o's are disjoint.</p>
<p>Figure 6 :
6EI values, the budget constraint N and the time constraint T . If the last three EIs are all less than α (α ∈ [0.1%, 1%]) of the current best output, the algorithm will stop. In addition, if the maximum number of runs N or the time constraint T is reached, the algorithm will also stop. Here, as the computing time for model estimation is updated every batch of runs, we allow some flexibility in meeting with the time constraint. The algorithm will stop updating the parameters in MaGP when the latest estimation exceeds the time α T (α ∈ (0, 1] is a Histograms of maximum EI values identified by (a) SFTA and (b) a random search method.tuning parameter). Then, it will only update sequential runs until the time budget is used up. Considering that updating parameters in the MaGP model (O(n 3 )) requires much more computing time than updating the model having fixed parameters with a new run (O(n 2 )), setting α = 0.95 often suffices in practice.</p>
<p>Figure 7 :Figure 8 :
78Histogram of the responses in a large random sample of 4,032,000 observations. In Figures 8(a) and (b), we show the histograms of largest responses identified by the sequential generalized PWO approach (BM 2 ) and sequential generalized CP approach (BM 3 Histograms of largest responses identified by (a) BM 2 and (b) BM 3 .</p>
<p>2.01 8 2.73 1 3.38 2 1.49 5 3.45 7 1.36 4 3.58 6 −560.99</p>
<p>2020) described a problem on arranging four mathematical operations: addition, subtraction, multiplication and division, where only the sequence factors are considered. Here, we generalize this problem by considering both the quantities and sequences for arranging the four mathematical operations. By this simulation study, We first show the efficiency of QS-learning for optimization, and then evaluate the accuracy of MaGP for prediction. Example 4 Suppose that the four mathematical operations (i.e. components c 1 , c 2 , c 3 and c 4 ) have the following effects starting with Y 0 = 20 before any components: c 1 : add 1 + 10sin(2πx 1 ); c 2 : subtract 2 + 10(x 2 − 0.4) 2 ; c 3 : multiply by 3 + x 3 ; c 4 : divide by 4 − x 4 , where x i ∈ [0, 1] for i = 1, . . . , 4. In this example, the analytical solution exists for maximizing the response Y after the four operations, where the optimal settings are (x 1 , . . . x 4 ) = (0.25, 0.4, 1, 1) and (o 1 , . . . , o 4 ) = (2, 4, 3, 1) and the maximum response is</p>
<p>Figure 9
9displays its expected improvements and cumulative maximum responses. When using the full-MaGP, the QS-learning also stops after 15 sequential runs. The maximum response found is 68.65 and the best settings identified are (x 1 , . . . , x 4 ) = (0.25, 0.39, 1, 1) and (o 1 , . . . , o 4 ) = (2, 4, 3, 1), which are still very close to the truth. Figure 10 displays its expected improvements and cumulative maximum responses. Below, We show the QS-design with the rule-of-thumb run size of 16 along with the sequential runs in the QS-learning under the 2d-MaGP and the full-MaGP models, respectively.</p>
<p>Figure 9 :Figure 10 :
910Plots for (a) expected improvements and (b) cumulative maximum responses from the QS-learning approach under the 2d-MaGP with 16-run QS-design. Plots for (a) expected improvements and (b) cumulative maximum responses from the QS-learning approach under the full-MaGP with 16-run QS-design. Next, we consider the QS-learning starting from the 4-run QS-design constructed by the algebraic method in Section 5.2. When using the 2d-MaGP model, 24 sequential runs are selected until the stopping criterion is satisfied. The best settings found are (x 1 , . . . x 4 ) = (0.25, 0.40, 1.00, 1.00) and (o 1 , . . . , o 4 ) = (3,</p>
<p>Figure 11 :
11Plots for (a) expected improvements and (b) cumulative maximum responses from the QS-learning under the 2d-MaGP using the 4-run QS-design. Below, we show the 4-run QS-design from the algebraic construction, along with the sequential runs in QS-learning under the 2d-MaGP and full-MaGP model, respectively.</p>
<p>Figure 12 Figure 12 :
1212Histogram of largest responses identified by BM 1 .</p>
<p>Figure 13 :
13Histograms of largest responses identified by (a) BM 2 and (b) BM 3 approaches.</p>
<p>x
= (x 1 , . . . , x 6 ) = (0.96, 0.74, 0.87, 0.43, 0.51, 0.64). Denote the sequence for arranging jobs as α = (α 1 , . . . , α k ) and its corresponding order-sequence as o = (o 1 , . . . , o k ), where Job α i has the processing order o α i = i. The completion time of Job α</p>
<p>Figure 14 :Figure 15 :
1415Plots for expected improvements of sequential runs in QS-learning under (a) 2d-MaGP and (b) full-MaGP in Example 5. Plots for cumulative maximum responses of sequential runs in QS-learning under (a) the 2d-MaGP and (b) the full-MaGP in Example 5.</p>
<p>Figure 16 :
16Plots for (a) expected improvements and (b) cumulative maximum responses from QS-learning under 2d-MaGP using the 6-run QS-design in Example 5.</p>
<p>Figure 17 :
17Histograms of smallest response values identified by (a) BM 2 and (b) BM 3 methods in Example 5.</p>
<p>Figure 18 :
18Plots for (a) expected improvements and (b) cumulative maximum responses from the QS-learning approach under 2d-MaGP in Example 6.</p>
<p>Figure 19 :
19Plots for (a) expected improvements and (b) cumulative maximum responses from the QS-learning approach under full-MaGP in Example 6.</p>
<p>Figure 20 :Figure 21 :
2021we show the histograms of largest response values identified by BM 2 and BM 3 . Their average results are 20.57 and 20.66, and their best results are 21.36 and 21.40, respectively. Histogram of the responses in a large random sample of 72,Histograms of largest responses identified by (a) BM 2 and (b) BM 3 in Example 6.</p>
<p>Table 1 :
1Illustration of drug data involving QS factors.Run 
Drug A 
Drug B 
Drug C 
Response </p>
<p>dosage order dosage order dosage order </p>
<p>1 
3.75 µM 
1 
95 nM 
2 
0.16 µM 
3 
39.91 </p>
<p>2 
2.80 µM 
1 
70 nM 
2 
0.16 µM 
3 
30.00 </p>
<p>3 
3.75 µM 
3 
95 nM 
1 
0.16 µM 
2 
34.68 </p>
<p>a vector of quantitative values for k components, and o i = (o i,1 , . . . , o i,k ) T is a vector that contains the orders of components in the arrangement sequence. Notably, x i,h and o i,h (h = 1, . . . , k and i = 1, . . . , n) are the quantitative and sequence parts of the h th component c h , respectively. Without loss of generality, we assume</p>
<p>when formulating covariance functions. For the h th (h = 1, . . . , k) component, its sequence input o i,h (for any i = 1, . . . , n) is ordinal. Thus, a distance measure should be specified for sequence input to form the covariance function in the GP component G h . To address this challenge, we consider mapping the order o i,h (o i,h ∈ {1, . . . , k}) to a t-dimensional latentvector (õ </p>
<p>(1) </p>
<p>i,h , . . . ,õ </p>
<p>Two designs O A and O B shown below are compared, and they have the same Hamming distance structure. Their t i,j pairs are listed inTable 2. Given the possibility of synergistic or antagonistic interactions between two drugs, the order in which they are administered isimportant. For example, if Drugs A and B exhibit a strong synergistic effect, then they should be administered in adjacent order. By contrast, if they exhibit a strong antagonistic effect, then their order of administration should be well separated in time. Furthermore, drugs may have different (i.e., immediate, delayed, or cumulative) time course effects (Al-Sallami et al., 2009), and thus, their precedence patterns matter. For example, consider that Drugs A and B have a synergistic interaction, where the effect of A is immediate, whereas the effect of B appears delayed with respect to the concentration-time profile. In such case, B should be added before A to maximize the synergistic effect, because B requires more time to fully exert its work with A. Evidently, subsequences (A, B) and (B, A) may lead to different outcomes in this study. Considering all of the above, it is clear that the pair-balanced design O B is better than O A , as all possible adjacent pairs in O B appear the same number of times in the experiment (i.e. all t i,j values are equal). Finally, we should use O B , the equivalent form of O B , to be the sequence part of the QS-design D.emphasizing more on the design's </p>
<p>space-filling property. </p>
<p>Example 2 Consider a drug combination experiment consisting of four drug components. </p>
<p>Table 2 :
2Comparison of designs' t i,j pairs. t i,j pairs AB AC AD BA BC BD CA CB CD DA DB DC</p>
<p>The true maximum response, i.e., 47.18, is found, along with the third and fourth largest responses, 44.38 and 44.33, respectively. Notably, the initial QS-design does not include settings that lead to the largest four responses. The proposed QS-learning requires 15 runs (8 initial runs plus 7 follow-up runs) to identify the optimal treatment of the original 24-run experiment, saving 37.5% of the budget. Refer to Supplementary Materials Section S4.1 for the complete data and more details regarding the analysis. 35.04 37.19 37.37 38.18 38.91 39.91 42.3 43.93 44.33 44.38 44.87 47.18 35.04 37.19 37.37 38.18 38.91 39.91 42.3 43.93 44.33 44.38 44.87 47.18Largest response value identified 
Relative Frequency </p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 </p>
<p>(a) </p>
<p>Table 3 :
3Drug levels and doses.Below, we show the QS-design along with the sequential runs in QS-learning. We use bold font to highlight the optimal setting throughout this Supplementary Materials.Drug 
A: paclitaxel 
B: doxorubicin C: mitoxantrone </p>
<p>Levels High: 1 Low: 0 High: 1 Low: 0 
Fixed </p>
<p>Doses 3.75 µM 2.8 µM 95 nM 70 nM 
0.16 µM </p>
<p>Table 4 :
4A Real Data on Lymphoma Treatment.Drug A 
Drug B 
Drug C 
Drug A 
Drug B 
Drug C </p>
<p>Dose Order Dose Order 
Order 
y 
Dose Order Dose Order 
Order 
y </p>
<p>1 
1 
1 
2 
3 
39.91 
1 
1 
0 
2 
3 
44.33 </p>
<p>1 
1 
1 
3 
2 
44.38 
1 
1 
0 
3 
2 
38.18 </p>
<p>1 
2 
1 
1 
3 
17.08 
1 
2 
0 
1 
3 
22.26 </p>
<p>1 
2 
1 
3 
1 
20.88 
1 
2 
0 
3 
1 
31.40 </p>
<p>1 
3 
1 
1 
2 
34.68 
1 
3 
0 
1 
2 
38.91 </p>
<p>1 
3 
1 
2 
1 
37.37 
1 
3 
0 
2 
1 
42.30 </p>
<p>0 
1 
0 
2 
3 
30.00 
0 
1 
1 
2 
3 
44.87 </p>
<p>0 
1 
0 
3 
2 
47.18 
0 
1 
1 
3 
2 
43.93 </p>
<p>0 
2 
0 
1 
3 
25.10 
0 
2 
1 
1 
3 
26.02 </p>
<p>0 
2 
0 
3 
1 
33.60 
0 
2 
1 
3 
1 
22.56 
0 
3 
0 
1 
2 
35.04 
0 
3 
1 
1 
2 
31.15 </p>
<p>0 
3 
0 
2 
1 
35.04 
0 
3 
1 
2 
1 
37.19 </p>
<p>Table 5 :
5The "medians(means)" of the RMSEs from different models under various designs.PWO 
CP 
2d-MaGP 
full-MaGP </p>
<p>Random 16-run 
14.81(15.44) 20.57(22.30) 11.86(12.11) 11.59(11.99) </p>
<p>Random 28-run 
11.76(12.12) 13.19(13.60) 10.86(11.47) 11.01(11.29) </p>
<p>Random 31-run 
11.08(11.29) 12.19(12.61) 10.70(10.84) 10.69(10.90) </p>
<p>QS-design 16-run 
15.25 
15.63 
9.41 
10.01 </p>
<p>Sequential runs under full-MaGP </p>
<p> 
 
 
 
 
 
 </p>
<p>o 1 
o 2 
o 3 
o 4 
o 5 
o 6 
Y </p>
<p>6 
4 
5 
2 
1 
3 
22.45 
6 
4 
5 
1 
3 
2 
22.59 
6 
4 
5 
1 
2 
3 22.43 
6 
3 
5 
1 
2 
4 
22.46 
6 
3 
5 
2 
1 
4 
22.49 </p>
<p> </p>
<p> 
 
 
 
 
 
 </p>
<p>Table 6 :
6The "medians(means)" of RMSEs from different models under various designs. PWO CP 2d-MaGP full-MaGP Random 30-run 2.33(2.35) 2.94(2.95) 0.18(0.21) 0.18(0.20) Random 40-run 2.04(2.07) 2.94(2.94) 0.12(0.13) 0.12(0.13) Random 50-run 1.95(1.96) 2.94(2.94) 0.11(0.11) 0.11(0.11)Random 60-run 1.89(1.90) 2.94(2.94) 0.10(0.10) 0.10(0.10) </p>
<p>The time course of drug effects. H S Al-Sallami, V V Kumar, C B Landersdorfer, J B Bulitta, S B Duffull, Pharmaceutical Statistics: The Journal of Applied Statistics in the Pharmaceutical Industry. 8Al-Sallami, H. S., Pavan Kumar, V. V., Landersdorfer, C. B., Bulitta, J. B., and Duffull, S. B. (2009), "The time course of drug effects," Pharmaceutical Statistics: The Journal of Applied Statistics in the Pharmaceutical Industry, 8, 176-185.</p>
<p>A review of scheduling research involving setup considerations. A Allahverdi, J N Gupta, Aldowaisan , T , Omega. 27Allahverdi, A., Gupta, J. N., and Aldowaisan, T. (1999), "A review of scheduling research involving setup considerations," Omega, 27, 219-239.</p>
<p>The traveling salesman problem: a computational study. D L Applegate, R E Bixby, V Chvatal, W J Cook, Princeton university pressApplegate, D. L., Bixby, R. E., Chvatal, V., and Cook, W. J. (2007), The traveling salesman problem: a computational study, Princeton university press.</p>
<p>Optimal sliced Latin hypercube designs. S Ba, W R Myers, W A Brenneman, Technometrics. 57Ba, S., Myers, W. R., and Brenneman, W. A. (2015), "Optimal sliced Latin hypercube designs," Technometrics, 57, 479-487.</p>
<p>No-Regret Bayesian Optimization with Unknown Hyperparameters. F Berkenkamp, A P Schoellig, A Krause, Journal of Machine Learning Research. 20Berkenkamp, F., Schoellig, A. P., and Krause, A. (2019), "No-Regret Bayesian Optimization with Unknown Hyperparameters," Journal of Machine Learning Research, 20, 1-24.</p>
<p>Energy efficient manufacturing of nanocellulose by chemo-and bio-mechanical processes: a review. A K Bharimalla, S P Deshmukh, P G Patil, N Vigneshwaran, World Journal of Nano Science and Engineering. 5204Bharimalla, A. K., Deshmukh, S. P., Patil, P. G., Vigneshwaran, N., et al. (2015), "En- ergy efficient manufacturing of nanocellulose by chemo-and bio-mechanical processes: a review," World Journal of Nano Science and Engineering, 5, 204.</p>
<p>Convergence rates of efficient global optimization algorithms. A D Bull, Journal of Machine Learning Research. 12Bull, A. D. (2011), "Convergence rates of efficient global optimization algorithms." Journal of Machine Learning Research, 12.</p>
<p>Adaptive design of experiments based on gaussian processes. E Burnaev, M Panov, International Symposium on Statistical Learning and Data Sciences. SpringerBurnaev, E. and Panov, M. (2015), "Adaptive design of experiments based on gaussian pro- cesses," in International Symposium on Statistical Learning and Data Sciences, Springer, pp. 116-125.</p>
<p>The analysis of time series: an introduction. C Chatfield, Chapman and Hall/CRCChatfield, C. (2003), The analysis of time series: an introduction, Chapman and Hall/CRC.</p>
<p>Construction of optimal fractional Orderof-Addition designs via block designs. J Chen, R Mukerjee, D K J Lin, Statistics &amp; Probability Letters. 161108728Chen, J., Mukerjee, R., and Lin, D. K. J. (2020), "Construction of optimal fractional Order- of-Addition designs via block designs," Statistics &amp; Probability Letters, 161, 108728.</p>
<p>Minimax optimal designs via particle swarm optimization methods. R.-B Chen, S.-P Chang, W Wang, H.-C Tung, W K Wong, Statistics and Computing. 25Chen, R.-B., Chang, S.-P., Wang, W., Tung, H.-C., and Wong, W. K. (2015), "Minimax optimal designs via particle swarm optimization methods," Statistics and Computing, 25, 975-988.</p>
<p>Theoretical basis, experimental design, and computerized simulation of synergism and antagonism in drug combination studies. T.-C Chou, Pharmacological reviews. 58Chou, T.-C. (2006), "Theoretical basis, experimental design, and computerized simulation of synergism and antagonism in drug combination studies," Pharmacological reviews, 58, 621-681.</p>
<p>Active learning with statistical models. D A Cohn, Z Ghahramani, Jordan , M I , Journal of artificial intelligence research. 4Cohn, D. A., Ghahramani, Z., and Jordan, M. I. (1996), "Active learning with statistical models," Journal of artificial intelligence research, 4, 129-145.</p>
<p>A Dean, M Morris, J Stufken, D Bingham, Handbook of design and analysis of experiments. CRC Press7Dean, A., Morris, M., Stufken, J., and Bingham, D. (2015), Handbook of design and analysis of experiments, vol. 7, CRC Press.</p>
<p>Design for computer experiments with qualitative and quantitative Factors. X Deng, Y Hung, C D Lin, Statistica Sinica. 25Deng, X., Hung, Y., and Lin, C. D. (2015), "Design for computer experiments with qualita- tive and quantitative Factors," Statistica Sinica, 25, 1567-1581.</p>
<p>Active learning through sequential design, with applications to detection of money laundering. X Deng, V R Joseph, A Sudjianto, C J Wu, Journal of the American Statistical Association. 104Deng, X., Joseph, V. R., Sudjianto, A., and Wu, C. J. (2009), "Active learning through sequential design, with applications to detection of money laundering," Journal of the American Statistical Association, 104, 969-981.</p>
<p>Additive Gaussian process for computer models with qualitative and quantitative factors. X Deng, C D Lin, K.-W Liu, Rowe , R , Technometrics. 59Deng, X., Lin, C. D., Liu, K.-W., and Rowe, R. (2017), "Additive Gaussian process for computer models with qualitative and quantitative factors," Technometrics, 59, 283-292.</p>
<p>Optimized combinations of bortezomib, camptothecin, and doxorubicin show increased efficacy and reduced toxicity in treating oral cancer. X Ding, K Matsuo, L Xu, J Yang, L Zheng, Anti-cancer drugs. 26Ding, X., Matsuo, K., Xu, L., Yang, J., and Zheng, L. (2015), "Optimized combinations of bortezomib, camptothecin, and doxorubicin show increased efficacy and reduced toxicity in treating oral cancer," Anti-cancer drugs, 26, 547-554.</p>
<p>High-Dimensional Gaussian Process Bandits. J Djolonga, A Krause, V Cevher, C Burges, L Bottou, M Welling, Z Ghahramani, K Weinberger, Advances in Neural Information Processing Systems. Curran Associates, Inc26Djolonga, J., Krause, A., and Cevher, V. (2013), "High-Dimensional Gaussian Process Ban- dits," in Advances in Neural Information Processing Systems, eds. Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K., Curran Associates, Inc., vol. 26.</p>
<p>Threshold accepting: a general purpose optimization algorithm appearing superior to simulated annealing. G Dueck, T Scheuer, Journal of Computational Physics. 90Dueck, G. and Scheuer, T. (1990), "Threshold accepting: a general purpose optimization algorithm appearing superior to simulated annealing," Journal of Computational Physics, 90, 161-175.</p>
<p>One-machine sequencing to minimize certain functions of job tardiness. H Emmons, Operations Research. 17Emmons, H. (1969), "One-machine sequencing to minimize certain functions of job tardi- ness," Operations Research, 17, 701-715.</p>
<p>Comparison of Gaussian process modeling software. C B Erickson, B E Ankenman, S M Sanchez, European Journal of Operational Research. 266Erickson, C. B., Ankenman, B. E., and Sanchez, S. M. (2018), "Comparison of Gaussian process modeling software," European Journal of Operational Research, 266, 179-192.</p>
<p>Design and modeling for computer experiments. K.-T Fang, R Li, A Sudjianto, Chapman and Hall/CRCFang, K.-T., Li, R., and Sudjianto, A. (2005), Design and modeling for computer experiments, Chapman and Hall/CRC.</p>
<p>Uniform experimental designs and their applications in industry. K.-T Fang, D K J Lin, Handbook of statistics. 22Fang, K.-T. and Lin, D. K. J. (2003), "Uniform experimental designs and their applications in industry," Handbook of statistics, 22, 131-170.</p>
<p>The knowledge-gradient policy for correlated normal beliefs. P Frazier, W Powell, S Dayanik, INFORMS journal on Computing. 21Frazier, P., Powell, W., and Dayanik, S. (2009), "The knowledge-gradient policy for corre- lated normal beliefs," INFORMS journal on Computing, 21, 599-613.</p>
<p>Bayesian optimization," in Recent advances in optimization and modeling of contemporary problems, Informs. P I Frazier, Frazier, P. I. (2018), "Bayesian optimization," in Recent advances in optimization and mod- eling of contemporary problems, Informs, pp. 255-278.</p>
<p>Numerical methods and optimization in finance. M Gilli, D Maringer, E Schumann, Academic PressGilli, M., Maringer, D., and Schumann, E. (2019), Numerical methods and optimization in finance, Academic Press.</p>
<p>Nonseparable, stationary covariance functions for space-time data. T Gneiting, Journal of the American Statistical Association. 97Gneiting, T. (2002), "Nonseparable, stationary covariance functions for space-time data," Journal of the American Statistical Association, 97, 590-600.</p>
<p>Surrogates: Gaussian process modeling, design, and optimization for the applied sciences. R B Gramacy, Chapman and Hall/CRCGramacy, R. B. (2020), Surrogates: Gaussian process modeling, design, and optimization for the applied sciences, Chapman and Hall/CRC.</p>
<p>Cases for the nugget in modeling computer experiments. R B Gramacy, H K Lee, Statistics and Computing. 22Gramacy, R. B. and Lee, H. K. (2012), "Cases for the nugget in modeling computer experi- ments," Statistics and Computing, 22, 713-722.</p>
<p>Space-time modelling with long-memory dependence: Assessing Ireland's wind power resource. J Haslett, A E Raftery, Journal of the Royal Statistical Society: Series C (Applied Statistics). 38Haslett, J. and Raftery, A. E. (1989), "Space-time modelling with long-memory dependence: Assessing Ireland's wind power resource," Journal of the Royal Statistical Society: Series C (Applied Statistics), 38, 1-21.</p>
<p>A S Hedayat, N J A Sloane, J Stufken, Orthogonal arrays: theory and applications. Springer Science &amp; Business MediaHedayat, A. S., Sloane, N. J. A., and Stufken, J. (1999), Orthogonal arrays: theory and applications, Springer Science &amp; Business Media.</p>
<p>Entropy Search for Information-Efficient Global Optimization. P Hennig, C J Schuler, Journal of Machine Learning Research. 13Hennig, P. and Schuler, C. J. (2012), "Entropy Search for Information-Efficient Global Op- timization." Journal of Machine Learning Research, 13.</p>
<p>Application of fractional factorial designs to study drug combinations. J Jaynes, X Ding, H Xu, W K Wong, C.-M Ho, Statistics in medicine. 32Jaynes, J., Ding, X., Xu, H., Wong, W. K., and Ho, C.-M. (2013), "Application of fractional factorial designs to study drug combinations," Statistics in medicine, 32, 307-318.</p>
<p>Minimax and maximin distance designs. M E Johnson, L M Moore, D Ylvisaker, Journal of Statistical Planning and Inference. 26Johnson, M. E., Moore, L. M., and Ylvisaker, D. (1990), "Minimax and maximin distance designs," Journal of Statistical Planning and Inference, 26, 131-148.</p>
<p>Design and analysis of cross-over trials. B Jones, M G Kenward, CRC PressBoca Raton, FL3rd edJones, B. and Kenward, M. G. (2014), Design and analysis of cross-over trials, 3rd ed, Boca Raton, FL: CRC Press.</p>
<p>Efficient global optimization of expensive black-box functions. D R Jones, M Schonlau, W J Welch, Journal of Global optimization. 13Jones, D. R., Schonlau, M., and Welch, W. J. (1998), "Efficient global optimization of expensive black-box functions," Journal of Global optimization, 13, 455-492.</p>
<p>Space-filling designs for computer experiments: A review. V R Joseph, Quality Engineering. 28Joseph, V. R. (2016), "Space-filling designs for computer experiments: A review," Quality Engineering, 28, 28-35.</p>
<p>Maximum projection designs for computer experiments. V R Joseph, E Gul, S Ba, Biometrika. 102Joseph, V. R., Gul, E., and Ba, S. (2015), "Maximum projection designs for computer experiments," Biometrika, 102, 371-380.</p>
<p>Mixed layers of sodium caseinate+ dextran sulfate: influence of order of addition to oil-water interface. L S Jourdain, C Schmitt, M E Leser, B S Murray, E Dickinson, Langmuir. 25Jourdain, L. S., Schmitt, C., Leser, M. E., Murray, B. S., and Dickinson, E. (2009), "Mixed layers of sodium caseinate+ dextran sulfate: influence of order of addition to oil-water interface," Langmuir, 25, 10026-10037.</p>
<p>Active learning with gaussian processes for object categorization. A Kapoor, K Grauman, R Urtasun, Darrell , T , 2007 IEEE 11th International Conference on Computer Vision. IEEEKapoor, A., Grauman, K., Urtasun, R., and Darrell, T. (2007), "Active learning with gaus- sian processes for object categorization," in 2007 IEEE 11th International Conference on Computer Vision, IEEE, pp. 1-8.</p>
<p>Kriging metamodeling in simulation: a review. J P Kleijnen, European Journal of Operational Research. 192Kleijnen, J. P. (2009), "Kriging metamodeling in simulation: a review," European Journal of Operational Research, 192, 707-716.</p>
<p>A sequential algorithm for training text classifiers. D D Lewis, W A Gale, SIGIR'94. SpringerLewis, D. D. and Gale, W. A. (1994), "A sequential algorithm for training text classifiers," in SIGIR'94, Springer, pp. 3-12.</p>
<p>Handbook of design and analysis of experiments. C D Lin, B Tang, Latin hypercubes and space-filling designsLin, C. D. and Tang, B. (2015), "Latin hypercubes and space-filling designs," Handbook of design and analysis of experiments, 593-625.</p>
<p>Order-of-addition experiments: A review and some new thoughts. D K J Lin, J Peng, Quality Engineering. 31Lin, D. K. J. and Peng, J. (2019), "Order-of-addition experiments: A review and some new thoughts," Quality Engineering, 31, 49-59.</p>
<p>On the limited memory BFGS method for large scale optimization. D C Liu, J Nocedal, Mathematical programming. 45Liu, D. C. and Nocedal, J. (1989), "On the limited memory BFGS method for large scale optimization," Mathematical programming, 45, 503-528.</p>
<p>Bayesian algorithms for one-dimensional global optimization. M Locatelli, Journal of Global Optimization. 10Locatelli, M. (1997), "Bayesian algorithms for one-dimensional global optimization," Journal of Global Optimization, 10, 57-76.</p>
<p>Choosing the sample size of a computer experiment: A practical guide. J L Loeppky, J Sacks, W J Welch, Technometrics. 51Loeppky, J. L., Sacks, J., and Welch, W. J. (2009), "Choosing the sample size of a computer experiment: A practical guide," Technometrics, 51, 366-376.</p>
<p>Genetic optimization using derivatives: The rgenoud package for R. Jr Mebane, W R Sekhon, J S , Journal of Statistical Software. 42Mebane, Jr., W. R. and Sekhon, J. S. (2011), "Genetic optimization using derivatives: The rgenoud package for R," Journal of Statistical Software, 42, 1-26.</p>
<p>Order-of-addition modeling. R W Mee, Statistica Sinica. 30Mee, R. W. (2020), "Order-of-addition modeling," Statistica Sinica, 30, 1543-1559.</p>
<p>Classic Kriging versus Kriging with bootstrapping or conditional simulation: classic Kriging's robust confidence intervals and optimization. E Mehdad, J P Kleijnen, Journal of the Operational Research Society. 66Mehdad, E. and Kleijnen, J. P. (2015), "Classic Kriging versus Kriging with bootstrapping or conditional simulation: classic Kriging's robust confidence intervals and optimization," Journal of the Operational Research Society, 66, 1804-1814.</p>
<p>Exploratory designs for computational experiments. M D Morris, T J Mitchell, Journal of Statistical Planning and Inference. 43Morris, M. D. and Mitchell, T. J. (1995), "Exploratory designs for computational experi- ments," Journal of Statistical Planning and Inference, 43, 381-402.</p>
<p>An application of a Hill-based response surface model for a drug combination experiment on lung cancer. S Ning, H Xu, I Al-Shyoukh, J Feng, R Sun, Statistics in medicine. 33Ning, S., Xu, H., Al-Shyoukh, I., Feng, J., and Sun, R. (2014), "An application of a Hill-based response surface model for a drug combination experiment on lung cancer," Statistics in medicine, 33, 4227-4236.</p>
<p>Bayesian Gaussian processes for sequential prediction, optimisation and quadrature. M A Osborne, UKOxford UniversityPh.D. thesisOsborne, M. A. (2010), "Bayesian Gaussian processes for sequential prediction, optimisation and quadrature," Ph.D. thesis, Oxford University, UK.</p>
<p>Sequencing research and the industrial scheduling problem. S Panwalkar, R Dudek, M Smith, Symposium on the Theory of Scheduling and its Applications. SpringerPanwalkar, S., Dudek, R., and Smith, M. (1973), "Sequencing research and the industrial scheduling problem," in Symposium on the Theory of Scheduling and its Applications, Springer, pp. 29-38.</p>
<p>Design of order-of-addition experiments. J Peng, R Mukerjee, D K J Lin, Biometrika. 106Peng, J., Mukerjee, R., and Lin, D. K. J. (2019), "Design of order-of-addition experiments," Biometrika, 106, 683-694.</p>
<p>Gaussian process models for computer experiments with qualitative and quantitative factors. P Z G Qian, H Wu, C F J Wu, Technometrics. 50Qian, P. Z. G., Wu, H., and Wu, C. F. J. (2008), "Gaussian process models for computer experiments with qualitative and quantitative factors," Technometrics, 50, 383-396.</p>
<p>Gaussian processes for machine learning. C E Rasmussen, C K Williams, MIT pressCambridge, MARasmussen, C. E. and Williams, C. K. (2006), Gaussian processes for machine learning, MIT press Cambridge, MA.</p>
<p>Dicekriging, Diceoptim: Two R packages for the analysis of computer experiments by kriging-based metamodelling and optimization. O Roustant, D Ginsbourger, Y Deville, Journal of Statistical Software. 5154Roustant, O., Ginsbourger, D., and Deville, Y. (2012), "Dicekriging, Diceoptim: Two R packages for the analysis of computer experiments by kriging-based metamodelling and optimization," Journal of Statistical Software, 51, 54p.</p>
<p>Active Learning Literature Survey. B Settles, 1648University of Wisconsin-MadisonComputer Sciences Technical ReportSettles, B. (2009), "Active Learning Literature Survey," Computer Sciences Technical Report 1648, University of Wisconsin-Madison.</p>
<p>Multiple-instance active learning. B Settles, M Craven, S Ray, Proceedings of the 20th International Conference on Neural Information Processing Systems. the 20th International Conference on Neural Information Processing SystemsSettles, B., Craven, M., and Ray, S. (2007), "Multiple-instance active learning," in Proceed- ings of the 20th International Conference on Neural Information Processing Systems, pp. 1289-1296.</p>
<p>Query by committee. H S Seung, M Opper, H Sompolinsky, Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theorySeung, H. S., Opper, M., and Sompolinsky, H. (1992), "Query by committee," in Proceedings of the fifth annual workshop on Computational learning theory, pp. 287-294.</p>
<p>Adjustment of an inverse matrix corresponding to a change in one element of a given matrix. J Sherman, W J Morrison, The Annals of Mathematical Statistics. 21Sherman, J. and Morrison, W. J. (1950), "Adjustment of an inverse matrix corresponding to a change in one element of a given matrix," The Annals of Mathematical Statistics, 21, 124-127.</p>
<p>Stimulation by Rad52 of yeast Rad51-mediated recombination. A Shinohara, T Ogawa, Nature. 404Shinohara, A. and Ogawa, T. (1998), "Stimulation by Rad52 of yeast Rad51-mediated re- combination," Nature, 391, 404.</p>
<p>Uncertainty quantification: theory, implementation, and applications. R C Smith, 12SiamSmith, R. C. (2013), Uncertainty quantification: theory, implementation, and applications, vol. 12, Siam.</p>
<p>Warped gaussian processes. E Snelson, Z Ghahramani, C E Rasmussen, Advances in neural information processing systems. Snelson, E., Ghahramani, Z., and Rasmussen, C. E. (2004), "Warped gaussian processes," in Advances in neural information processing systems, pp. 337-344.</p>
<p>Information-theoretic regret bounds for gaussian process optimization in the bandit setting. N Srinivas, A Krause, S M Kakade, M W Seeger, IEEE Transactions on Information Theory. 58Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W. (2012), "Information-theoretic regret bounds for gaussian process optimization in the bandit setting," IEEE Transactions on Information Theory, 58, 3250-3265.</p>
<p>Uniform projection designs. F Sun, Y Wang, H Xu, Annals of Statistics. 47Sun, F., Wang, Y., and Xu, H. (2019), "Uniform projection designs," Annals of Statistics, 47, 641-661.</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressSutton, R. S. and Barto, A. G. (2018), Reinforcement learning: An introduction, MIT press.</p>
<p>A comparison of four methods for minimizing total tardiness on a single processor with sequence dependent setup times. K.-C Tan, R Narasimhan, P A Rubin, G L Ragatz, Omega. 28Tan, K.-C., Narasimhan, R., Rubin, P. A., and Ragatz, G. L. (2000), "A comparison of four methods for minimizing total tardiness on a single processor with sequence dependent setup times," Omega, 28, 313-326.</p>
<p>The single machine problem with quadratic penalty function of completion times: a branch-and-bound solution. W Townsend, Management Science. 24Townsend, W. (1978), "The single machine problem with quadratic penalty function of completion times: a branch-and-bound solution," Management Science, 24, 530-534.</p>
<p>Regret bounds for noise-free Bayesian optimization. S Vakili, V Picheny, N Durrande, arXiv:2002.05096arXiv preprintVakili, S., Picheny, V., and Durrande, N. (2020), "Regret bounds for noise-free Bayesian optimization," arXiv preprint arXiv:2002.05096.</p>
<p>Design of experiments where the order of addition is important. R Van Nostrand, ASA Proceedings of the Section on Physical and Engineering Sciences. Van Nostrand, R. (1995), "Design of experiments where the order of addition is important," in ASA Proceedings of the Section on Physical and Engineering Sciences, pp. 155-160.</p>
<p>The design of order-of-addition experiments. J G Voelkel, Journal of Quality Technology. Voelkel, J. G. (2019), "The design of order-of-addition experiments," Journal of Quality Technology, 1-12.</p>
<p>Single-machine scheduling to minimize the total earliness and tardiness is strongly NP-hard. L Wan, J Yuan, Operations Research Letters. 41Wan, L. and Yuan, J. (2013), "Single-machine scheduling to minimize the total earliness and tardiness is strongly NP-hard," Operations Research Letters, 41, 363-365.</p>
<p>Simultaneous Optimization of Drug Combination Dose-Ratio Sequence with Innovative Design and Active Learning. A Wang, H Xu, X Ding, Advanced Therapeutics. 3Wang, A., Xu, H., and Ding, X. (2020a), "Simultaneous Optimization of Drug Combination Dose-Ratio Sequence with Innovative Design and Active Learning," Advanced Therapeu- tics, 3, 1900135.</p>
<p>H Wang, Q Xiao, A Mandal, LHD: Latin Hypercube Designs (LHDs) Algorithms. R package version 1.3.3Wang, H., Xiao, Q., and Mandal, A. (2020b), LHD: Latin Hypercube Designs (LHDs) Algo- rithms, R package version 1.3.3.</p>
<p>Optimal maximin L 1 -distance Latin hypercube designs based on good lattice point designs. L Wang, Q Xiao, H Xu, The Annals of Statistics. 46Wang, L., Xiao, Q., and Xu, H. (2018), "Optimal maximin L 1 -distance Latin hypercube designs based on good lattice point designs," The Annals of Statistics, 46, 3741-3766.</p>
<p>A novel class of stabilized greedy kernel approximation algorithms: Convergence, stability and uniform point distribution. T Wenzel, G Santin, B Haasdonk, Journal of Approximation Theory. 262105508Wenzel, T., Santin, G., and Haasdonk, B. (2021), "A novel class of stabilized greedy ker- nel approximation algorithms: Convergence, stability and uniform point distribution," Journal of Approximation Theory, 262, 105508.</p>
<p>Experiments: planning, analysis, and optimization. C J Wu, M S Hamada, John Wiley &amp; Sons3rd edWu, C. J. and Hamada, M. S. (2021), Experiments: planning, analysis, and optimization, John Wiley &amp; Sons, 3rd ed.</p>
<p>Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness. G Wynne, F.-X Briol, M Girolami, Journal of Machine Learning Research. 22Wynne, G., Briol, F.-X., and Girolami, M. (2021), "Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness," Journal of Machine Learn- ing Research, 22, 1-40.</p>
<p>EzGP: Easy-to-Interpret Gaussian Process Models for Computer Experiments with Both Quantitative and Qualitative Factors. Q Xiao, A Mandal, C D Lin, X Deng, SIAM/ASA Journal on Uncertainty Quantification. 9Xiao, Q., Mandal, A., Lin, C. D., and Deng, X. (2021), "EzGP: Easy-to-Interpret Gaus- sian Process Models for Computer Experiments with Both Quantitative and Qualitative Factors," SIAM/ASA Journal on Uncertainty Quantification, 9, 333-353.</p>
<p>Application of kriging models for a drug combination experiment on lung cancer. Q Xiao, L Wang, H Xu, Statistics in Medicine. 38Xiao, Q., Wang, L., and Xu, H. (2019), "Application of kriging models for a drug combination experiment on lung cancer," Statistics in Medicine, 38, 236-246.</p>
<p>Construction of maximin distance Latin squares and related Latin hypercube designs. Q Xiao, H Xu, Biometrika. 104Xiao, Q. and Xu, H. (2017), "Construction of maximin distance Latin squares and related Latin hypercube designs," Biometrika, 104, 455-464.</p>
<p>Construction of maximin distance designs via level permutation and expansion. Statistica Sinica. 28-(2018), "Construction of maximin distance designs via level permutation and expansion," Statistica Sinica, 28, 1395-1414.</p>
<p>A mapping-based universal Kriging model for order-of-addition experiments in drug combination studies. Computational Statistics &amp; Data Analysis. 157107155-(2021), "A mapping-based universal Kriging model for order-of-addition experiments in drug combination studies," Computational Statistics &amp; Data Analysis, 157, 107155.</p>
<p>A component-position model, analysis and design for order-of-addition experiments. J.-F Yang, F Sun, H Xu, Technometrics. 63Yang, J.-F., Sun, F., and Xu, H. (2021), "A component-position model, analysis and design for order-of-addition experiments," Technometrics, 63, 212-224.</p>
<p>A latent variable approach to Gaussian process modeling with qualitative and quantitative factors. Y Zhang, S Tao, W Chen, D W Apley, TechnometricsZhang, Y., Tao, S., Chen, W., and Apley, D. W. (2019), "A latent variable approach to Gaussian process modeling with qualitative and quantitative factors," Technometrics, 1- 12.</p>
<p>Space-filling properties of good lattice point sets. Y Zhou, H Xu, Biometrika. 102Zhou, Y. and Xu, H. (2015), "Space-filling properties of good lattice point sets," Biometrika, 102, 959-966.</p>            </div>
        </div>

    </div>
</body>
</html>
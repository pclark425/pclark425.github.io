<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2178 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2178</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2178</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278959434</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.21935v2.pdf" target="_blank">From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors''into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2178.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2178.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained transformer language models that generate natural-language hypotheses, explanations, rules, and predictions by leveraging large text corpora and in-context learning; used across abduction, deduction, and induction tasks in the surveyed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning / multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>natural-language scientific hypotheses, explanatory rules, predictions, deductive consequences, and code-based formal hypotheses when prompted</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>varies: (1) human expert evaluation; (2) LLM-based self-evaluation (verifiers or critics); (3) comparison to ground-truth hypotheses or answers when benchmarks provide them; (4) translation to formal representations and use of symbolic solvers; (5) retrieval-grounding (RAG) to external documents; (6) automated test-case execution when hypotheses are expressed as code</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>expert ratings / human judgments, held-out (post-date) splits (seen vs unseen) in curated benchmarks, automated novelty/originality scoring in RAG loops, and distance from training/publication timestamps (e.g., pre- vs post-2023 splits)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>strong on in-distribution deductive and instruction-following tasks; can generate plausible and diverse natural-language hypotheses via prompting and in-context examples but tends to produce less novel outputs as demonstrated-example counts increase (Qi et al. 2024); performance drops on counterfactual or unfamiliar rule-following tasks (reported qualitatively across multiple works).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>high when hypotheses are formalized and checked by symbolic solvers or when ground-truth answers are available; substantially weaker and more subjective for open-ended natural-language validation relying on human judges or LLM self-assessment; LLM-based validation can be brittle and reflect parametric biases.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically in the survey; qualitatively noted to be nontrivial for natural-language validation (models may accept or assert hypotheses without sufficient evidence), and likely higher on novel/out-of-distribution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically; qualitatively can occur when models discount valid but unfamiliar hypotheses—no reliable numbers provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation reliability degrades as novelty increases: natural-language validations become more subjective and inconsistent across annotators, while formal validation remains stable provided the hypothesis can be translated accurately into formal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>yes — LLMs can generate many plausible hypotheses (creative output) but validation is often weaker, especially for novel outputs; the survey repeatedly highlights a fabrication/validation gap where generation outpaces reliable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>poor to mixed: strong on familiar, in-distribution examples; performance degrades on counterfactual, adversarial, or otherwise out-of-distribution scenarios (evidence cited from counterfactual rule-following and inductive/refinement benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not well calibrated for novel outputs — model confidence and belief scores from LLM internal mechanisms are reported to be unreliable for high-novelty cases (survey notes need for better calibration and belief-scoring methods).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>human evaluation is expensive and slow; formal solver validation can be computationally cheap/deterministic but requires accurate translation; LLM-based iterative validation (RAG + critique loops) increases compute due to multiple retrieval/generation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>retrieval-augmentation (RAG), human-in-the-loop, translation to formal languages / neurosymbolic pipelines, verifier-critic models (automated verifiers), ensemble and multi-agent critique loops, and test-case execution for code hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Survey evidence shows LLMs readily generate hypotheses and rules but validating those outputs—especially when novel or out-of-distribution—remains a major weakness; formalization, retrieval grounding, and human-in-the-loop methods mitigate but do not eliminate this gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2178.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pipelines that retrieve task- or domain-specific documents and then use an LLM to generate or refine hypotheses with iterative retrieval / critique steps, often used to ground hypotheses in external evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid system (retrieval + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific hypothesis generation (domain-specific: e.g., biomedical, chemistry, literature review)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>domain-grounded hypotheses, refinements of candidate hypotheses, literature-inspired research ideas</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>iterative retrieval of related documents, LLM self-critique loops, novelty/originality scoring, grounding claims against retrieved evidence; in some pipelines, experts inspect final hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>automated novelty/originality scoring, editorial/expert judgment; comparison against existing literature retrieved during loop (e.g., whether claim is supported or novel relative to retrieved corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>improves relevance and factual consistency compared to unconstrained generation; pipelines report improved coherence and domain specificity (Hu et al. 2024; Yang et al. 2025), but may still trade off some novelty when many examples are provided in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>better fact-consistency due to grounding, but dependent on retrieval coverage and quality; when retrieval misses relevant literature, validation fails silently.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically; qualitatively reduced relative to unguided LLM generation due to grounding, but remains nonzero.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically; may increase when retrieval is conservative or missing documents.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>RAG improves validation of familiar/known facts but is less helpful for truly novel hypotheses not present in the retrieval corpus; novelty that lies outside the retrieved evidence remains hard to validate automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>reduced relative to naive LLMs but not eliminated — grounding helps, yet generative creativity can still outpace retrieval-backed validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>limited by the retrieval index; if novel examples are out-of-distribution relative to the corpus, performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>improved when retrieval provides supporting evidence; LLM confidence still not fully trustworthy without explicit provenance checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>higher than pure generation due to retrieval steps and repeated generation/critique loops; cost grows with iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>grounding to knowledge graphs, novelty-guided re-generation, iterative critique loops, human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG pipelines increase factual grounding and reduce some validation failures, but rely on retrieval coverage and still struggle to validate genuinely novel discoveries not present in the corpus.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2178.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE-Chem (LLM-based multi-agent chemistry hypothesis system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM pipeline applied to chemistry papers that retrieves snippets, drafts hypotheses, and scores them for originality to rediscover or propose chemistry hypotheses from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>chemistry (scientific hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>chemistry hypotheses derived from paper background/context and retrieved literature</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>retrieval of related snippets, scoring for originality and relevance, multi-agent critique and iterative refinement; expert-segmented paper fields used as seed context</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>originality scoring by the system and comparisons to existing literature; evaluated on rediscovering 'unseen' hypotheses from 51 chemistry papers</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>reported to produce inventive candidates and to rediscover hypotheses across held-out chemistry papers in the study; outperforms baseline transformer approaches in the reported evaluations (qualitative/relative claims in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation is based on retrieval-grounding and scoring; effectiveness depends on retrieval quality and the segmentation of paper fields; not described with absolute numeric metrics in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically in the survey; dependent on scoring thresholds and retrieval errors.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>system explicitly targets 'unseen' chemistry hypotheses, but validation is more reliable when related literature exists to ground claims; purely novel chemistry claims without literature support remain difficult to validate.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>present — generation can propose novel chemistry hypotheses, while automated validation relies on existing literature and scoring heuristics that may miss true novelties.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>tested specifically on rediscovery of hypotheses that were not present in the immediate seed; performance reported as positive in the study context but specifics not given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; scoring provides a proxy for confidence but calibration details absent.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>higher than single-pass generation due to retrieval, multi-agent scoring, and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>multi-agent critique, retrieval grounding, expert segmentation of paper components, iterative novelty-guided loops.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MOOSE-Chem demonstrates that multi-agent retrieval-grounded pipelines can rediscover and propose domain-specific hypotheses, but automated validation still hinges on retrieval coverage and scoring heuristics, leaving truly novel hypotheses hard to confirm without human or experimental follow-up.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2178.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APEx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APEx (Automatic Programmatic Experimentation / benchmarking framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal automated benchmarking framework that iteratively programs experiments (e.g., tailored test inputs and augmentations) to evaluate hypotheses about model capabilities in a fully automated loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic benchmarking of large multimodal models via iterative experiment programming.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>APEx</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automated benchmarking / experiment programming system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>multimodal model evaluation / hypothesis testing about model capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>automatically generates test cases (e.g., images) and experiment protocols to probe model hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>text-to-image retrieval/generation, image augmentations, automated execution of experiments on a library of models, automated analysis and iterative refinement of experiments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>robustness to augmentations and transformations; capacity to find counterexamples or failure modes via iterative experiment generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>framework successfully automates the generation of tailored tests to probe hypotheses; reported to support iterative discovery of model weaknesses in multimodal settings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>automated and repeatable benchmarking reduces human evaluation cost; validation quality depends on the experiment generation heuristics and chosen augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically; automated tests can still generate spurious cases if generation tools introduce artefacts.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically; may miss subtle failure modes if augmentations are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>APEx helps surface robustness failures for novel transformations but cannot fully validate scientific novelty of model-generated hypotheses outside the realm of its experiment primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>reduced for probing model capabilities because APEx automates both generation of tests and validation; however, for open-ended scientific claims APEx is not a complete validation solution.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>APEx is designed to construct out-of-distribution tests via augmentations; performance is dependent on the richness of its generation tools.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; framework focuses on automated detection of failures rather than calibrated confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>nontrivial due to generation of many test artifacts and repeated model evaluations; higher cost than single-shot generation but substantially cheaper than large-scale human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>automated iterative experiment programming, augmentation strategies to probe robustness, automated analysis loops.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>APEx shows automated, iterative experiment programming can systematically validate hypotheses about model capabilities and find robustness failures without heavy human involvement, but it is specialized to model-behavior hypotheses and not general scientific discovery validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2178.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CauseJudger</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CauseJudger (LLM-based cause identification / abductive reasoning framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline using LLMs to translate natural-language inputs into first-order-logic representations, filter premises, and perform forward reasoning to decide which hypothesis explains observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causejudger: Identifying the cause with llms for abductive logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CauseJudger</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neurosymbolic pipeline (LLM + FOL representation + reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>abductive logical reasoning / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>not primarily generative; translates and evaluates candidate hypotheses in formal logic</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>LLM-based translation of NL to FOL, premise filtering, forward (formal) reasoning; evaluated on the CauseLogics dataset with programmatic ground-truth labels</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>dataset-difficulty levels and binary ground-truth labels; novelty not the primary focus</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (system focused on validation); translation quality from NL to FOL is the key performance factor.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>when translation is accurate, formal forward reasoning yields deterministic, verifiable validation results; overall performance depends on LLM translation quality and filtering accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically; potential for false positives arises from translation/filtering errors that introduce or omit premises.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically; translation omissions can cause false negatives by failing to capture necessary premises.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>formal method preserves validation correctness for problem classes within CauseLogics, but novel natural-language formulations that are hard to formalize reduce validation reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>system narrows the asymmetry by translating validation into deterministic formal checks, but relies on non-deterministic LLM translation which can reintroduce asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>performance falls when NL formulations deviate substantially from patterns seen in translator training or when required logical constructs are rare.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not explicitly reported; confidence hinges on translator fidelity rather than calibrated belief scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>formal reasoning is efficient once representation is produced; translation and filtering add compute overhead but are cheaper than repeated human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>neurosymbolic translation to formal logic, premise filtering, and deterministic solvers for final validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Translating natural-language hypotheses into formal logic and then applying deterministic solvers can yield rigorous validation, but translation errors remain a major bottleneck that limits reliability on unconstrained natural-language inputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2178.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HtT / HypoGeniC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypotheses-to-Theories (HtT) and HypoGeniC frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark-driven iterative frameworks where LLMs propose hypotheses, apply them to benchmark problems, and keep rules that consistently produce correct predictions; HypoGeniC similarly generates candidate hypotheses and updates confidence based on accuracy across examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypotheses-to-Theories (HtT) / HypoGeniC</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven rule-learning pipelines (benchmarked)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>synthetic / benchmark hypothesis discovery (general reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates candidate rules/hypotheses from benchmark examples</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>external algorithm compares model-produced predictions to benchmark ground-truth answers; correct rules are retained in a rule library; hypothesis confidence updated from accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not designed to measure domain novelty — relies on benchmark correctness / prediction accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>able to build effective rule libraries for benchmark tasks when ground truth answers are available; success dependent on benchmark design and ground-truth availability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>deterministic and reliable only because benchmarks provide ground-truth answers; not applicable to real-world discovery without ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically in survey; can be low within benchmarks due to deterministic check against ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>frameworks perform well on benchmark/familiar tasks with ground truth but are not designed to validate novel, real-world hypotheses lacking immediate ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>apparent: these frameworks rely on external ground-truth to validate and therefore do not address validation of genuinely novel hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not evaluated meaningfully because tasks and observations are drawn from benchmarks and are not truly out-of-distribution by design.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; confidence is derived from empirical prediction accuracy on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>low to moderate — validation is a matter of comparing predicted answers to ground truth across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>use of ground-truth-labeled benchmarks and iterative rule-library maintenance</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>HtT and HypoGeniC show that iterative rule-keeping from benchmark feedback can produce effective rule libraries for known tasks, but they do not generalize to real-world discovery where ground truth is unavailable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2178.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wason-Task / Seals & Shalin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wason-card-style hypothesis testing (evaluative benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Minimal proactive hypothesis-testing benchmark inspired by the Wason selection task: models must choose which observations (cards) to reveal to validate an 'if p then q' hypothesis expressed in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the deductive competence of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Wason-style hypothesis testing benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / probing task</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>deductive rule-following / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>not generative — probes models' ability to select informative actions to validate a rule</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>minimal action selection (flip appropriate 'cards') to obtain sufficient evidence for or against the rule; model must identify which observations are informative for validation (modus ponens and modus tollens)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>tests ability to apply rules in novel natural-language formulations and minimal action planning rather than measuring novelty of discoveries</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>used to reveal whether models can identify required tests to validate clauses; survey reports benchmark exposes limitations in natural-language rule-following.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>models often struggle to identify the minimal sufficient checks in natural-language formulations, revealing failures in proactive validation and rule-following under natural-language variation.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not given numerically; qualitatively nontrivial as models sometimes choose irrelevant cards.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not given numerically; models sometimes fail to choose necessary cards, causing false negatives in validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>task explicitly designed to probe validation under natural-language variability; validation fails more often as language framing becomes unfamiliar.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>shows asymmetry: even when models can restate rules, they may not perform the correct minimal validation actions in novel framings.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>degrades with unfamiliar linguistic expressions of p and q.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>low — evaluation is action-selection and can be programmatically scored.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>explicit training and fine-tuning on formal rule-following, or more structured prompting to map NL to formal constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The Wason-style benchmarks show LLMs struggle to proactively select minimal evidence for validating simple 'if p then q' rules when expressed in varied natural-language forms, exposing gaps in hypothesis-testing ability on novel formulations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2178.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TURTLEBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TURTLEBENCH (Turtle Soup-inspired benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark of real-world yes/no puzzles and question-answer pairs designed to test whether models can follow a story, maintain consistent reasoning over narrative premises, and answer True/False/Not Relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Turtlebench: Evaluating top language models via real-world yes/no puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TURTLEBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>narrative deductive reasoning / hypothesis application</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>n/a (benchmark for evaluating deduction and story-based rule-following)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>prediction-based checks comparing model answers (True/False/Not Relevant) to human-annotated ground-truth across 1,532 curated question-answer pairs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not a novelty metric; measures story-consistent deduction and application of given hypotheses/rules</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>provides a rigorous setting to evaluate whether LLMs can apply story-derived hypotheses; survey notes usage of TURTLEBENCH to reveal limits in following and applying story rules.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>ground-truth comparison yields clear correctness metrics for deduction over narratives; LLMs show variable performance, particularly when reasoning requires deep contextual tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>benchmark focuses on narrative complexity rather than novelty; performance degrades with more complex or less typical narrative patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>benchmark exposes when models' verbalized rules/hypotheses do not translate into correct applied answers to questions.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>performance falls on puzzles with uncommon narrative structures or uncommon commonsense knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>moderate — automated scoring against annotated dataset is inexpensive; dataset creation expensive due to human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>programmatic test cases, annotated QA pairs, and chain-of-thought supervision to align reasoning steps with final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TURTLEBENCH and similar narrative QA benchmarks reveal that LLMs can fail to consistently apply hypothesized rules in story settings, especially when narratives require nuanced commonsense or multi-step inference.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2178.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>He-puzzle-envs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>He et al. puzzle environments (interactive numeric/letter probing puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Interactive puzzle environments where agents input arbitrary integers or letters and receive tailored feedback based on a hidden quantitative rule; used to evaluate iterative probing, quantitative rule discovery, and clarity of deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Idea: Enhancing the rule learning ability of large language model agent through induction, deduction, and abduction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>He et al. puzzle environments</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>interactive simulation / probe environment</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>quantitative rule learning / controlled discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>agents generate probe actions (numeric/letter queries) and candidate quantitative rules/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>execute probes, collect feedback, evaluate whether the inferred rule predicts withheld test cases correctly; human judgments used to assess clarity and rigor of chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>measured by ability to discover hidden rules not trivially present in pretraining; tasks are intentionally designed to reduce hypothesis leakage</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>LLMs can discover simple quantitative rules via iterative probing in these controlled settings, but discovered rules often remain toy-like and limited in realism</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation via test-case prediction is reliable for the synthetic puzzles; however, human judgments are required to assess reasoning clarity and scientific rigor</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically; synthetic test-case matching reduces false positives but chain-of-thought quality issues can hide failures.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>puzzles reduce pretraining leakage but still produce simplistic hypotheses; validation is robust for synthetic held-outs but does not translate trivially to real-world novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>present: models may generate candidate rules but require iterative probing and human evaluation to achieve robust validation and scientific rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>not applicable beyond the synthetic environment; generalization to realistic scientific tasks is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>moderate — many probe iterations and repeated model runs required; human scoring adds cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>interactive probing with tailored feedback, synthetic test-case evaluation, human judgment of reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactive puzzle environments allow rigorous test-case-based validation of quantitative rule discovery but currently yield simplistic hypotheses and still require human assessment for reasoning quality and real-world applicability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2178.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoveryWorld / ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryWorld / ScienceWorld virtual lab environments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated interactive environments (DiscoveryWorld, ScienceWorld) that allow agents to plan experiments, perform actions, collect observations, and attempt hypothesis discovery across scientific tasks; used to evaluate end-to-end discovery behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DiscoveryWorld / ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>interactive simulated environment / virtual lab</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific discovery (education-style experiments, multi-step scientific tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>agents generate experimental plans, probe actions, and candidate hypotheses based on collected observations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>task completion metrics, execution of key experimental steps, comparison of discovered hypotheses to ground-truth rules built into the environment; some environments use RL policies to collect evidence</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not primarily focused on open scientific novelty — evaluates ability to rediscover or apply predefined rules and conduct experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>agents can carry out multi-step experiments and solve many curriculum-style tasks, but coarse action spaces and prebuilt rules limit the complexity of hypotheses discovered</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>programmatic ground-truth comparisons enable objective validation in-simulation; however, RL-driven evidence collection can obscure evaluation of the agent's proactive hypothesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically; environment-level ground truth reduces false positives for rediscovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation is reliable inside the environment but does not reflect validation of genuinely novel scientific claims outside the simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>mitigated inside simulations because ground truth exists, but real-world discovery remains more asymmetric.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>agents struggle when tasks require fine-grained experimental interventions or when action spaces lack expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>moderate to high due to simulation runs and RL training; programmatic scoring reduces human cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>interactive simulation with ground-truth rules, expanded action spaces, and finer-grained experimental primitives to better approximate real-world discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Virtual lab environments enable end-to-end evaluation of hypothesis discovery with objective validation, but many current sims use limited action spaces and known rules, limiting assessment of genuinely novel, real-world discovery and validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2178.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2178.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neurosymbolic LLM+Solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neurosymbolic translation to formal solvers (LLM + FOL/code + symbolic prover)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use LLMs to parse natural-language hypotheses into formal languages (FOL or code) and then apply symbolic provers or execute code for deterministic validation of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>neurosymbolic LLM-to-formal-solver pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neurosymbolic / hybrid (LLM + symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>logical reasoning, formal hypothesis validation, program synthesis for hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>formal hypotheses (FOL rules, executable functions/code) synthesized from natural-language observations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>formal verification using symbolic provers or execution of generated code on held-out test cases; deterministic correctness checks when translation is accurate</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>measured via held-out example generalization and predictions on unseen test inputs (e.g., programmatic testcases, ARC-like tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>when NL-to-formal translation is successful, deductive correctness is high; however, models often fail to produce correct formalizations in complex or ambiguous natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>very strong and deterministic once correct formalization is obtained; validator (solver) performance is essentially perfect for the formal domain but depends critically on translation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>low for solver-side validation (deterministic), but effective false positives arise when translation introduces spurious formal rules — numerical rates not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>low for solver-side validation; translation omissions can cause false negatives — not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>novel natural-language hypotheses that are hard to formalize reduce validation reliability; the formal solver itself handles novelty in the logical sense but not linguistic novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>reduced when formalization is possible, but LLM translation errors reintroduce asymmetry between creative generation and provable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>solver side remains deterministic; NL-to-formal mapping degrades on OOD linguistic constructions or very novel conceptual mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; systems often lack calibrated uncertainty about translation correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>formal verification typically cheaper than human evaluation; translation step can add cost depending on model and iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>LLM fine-tuning for semantic parsing, verifier models, chain-of-thought supervision, and hybrid architectures (Logic-LM, LINC) that combine LLMs with provers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Neurosymbolic pipelines provide deterministic validation when natural-language hypotheses can be translated accurately, but NL-to-formal translation remains the bottleneck limiting reliable validation of open-ended, novel hypotheses.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Automatic benchmarking of large multimodal models via iterative experiment programming. <em>(Rating: 2)</em></li>
                <li>Causejudger: Identifying the cause with llms for abductive logical reasoning. <em>(Rating: 2)</em></li>
                <li>Turtlebench: Evaluating top language models via real-world yes/no puzzles. <em>(Rating: 2)</em></li>
                <li>Evaluating the deductive competence of large language models. <em>(Rating: 2)</em></li>
                <li>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. <em>(Rating: 2)</em></li>
                <li>Hypothesis generation with large language models. <em>(Rating: 1)</em></li>
                <li>Can llms follow simple rules? <em>(Rating: 1)</em></li>
                <li>Towards monosemanticity: Decomposing language models with dictionary learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2178",
    "paper_id": "paper-278959434",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "LLMs (general)",
            "name_full": "Large Language Models",
            "brief_description": "Pretrained transformer language models that generate natural-language hypotheses, explanations, rules, and predictions by leveraging large text corpora and in-context learning; used across abduction, deduction, and induction tasks in the surveyed literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "large language models (LLMs)",
            "system_type": "large language model",
            "domain": "general scientific reasoning / multi-domain",
            "generation_capability": "natural-language scientific hypotheses, explanatory rules, predictions, deductive consequences, and code-based formal hypotheses when prompted",
            "validation_method": "varies: (1) human expert evaluation; (2) LLM-based self-evaluation (verifiers or critics); (3) comparison to ground-truth hypotheses or answers when benchmarks provide them; (4) translation to formal representations and use of symbolic solvers; (5) retrieval-grounding (RAG) to external documents; (6) automated test-case execution when hypotheses are expressed as code",
            "novelty_measure": "expert ratings / human judgments, held-out (post-date) splits (seen vs unseen) in curated benchmarks, automated novelty/originality scoring in RAG loops, and distance from training/publication timestamps (e.g., pre- vs post-2023 splits)",
            "generation_performance": "strong on in-distribution deductive and instruction-following tasks; can generate plausible and diverse natural-language hypotheses via prompting and in-context examples but tends to produce less novel outputs as demonstrated-example counts increase (Qi et al. 2024); performance drops on counterfactual or unfamiliar rule-following tasks (reported qualitatively across multiple works).",
            "validation_performance": "high when hypotheses are formalized and checked by symbolic solvers or when ground-truth answers are available; substantially weaker and more subjective for open-ended natural-language validation relying on human judges or LLM self-assessment; LLM-based validation can be brittle and reflect parametric biases.",
            "false_positive_rate": "not reported numerically in the survey; qualitatively noted to be nontrivial for natural-language validation (models may accept or assert hypotheses without sufficient evidence), and likely higher on novel/out-of-distribution tasks.",
            "false_negative_rate": "not reported numerically; qualitatively can occur when models discount valid but unfamiliar hypotheses—no reliable numbers provided in survey.",
            "novelty_effect_on_validation": "validation reliability degrades as novelty increases: natural-language validations become more subjective and inconsistent across annotators, while formal validation remains stable provided the hypothesis can be translated accurately into formal representations.",
            "generation_validation_asymmetry": "yes — LLMs can generate many plausible hypotheses (creative output) but validation is often weaker, especially for novel outputs; the survey repeatedly highlights a fabrication/validation gap where generation outpaces reliable validation.",
            "out_of_distribution_performance": "poor to mixed: strong on familiar, in-distribution examples; performance degrades on counterfactual, adversarial, or otherwise out-of-distribution scenarios (evidence cited from counterfactual rule-following and inductive/refinement benchmarks).",
            "calibration_quality": "not well calibrated for novel outputs — model confidence and belief scores from LLM internal mechanisms are reported to be unreliable for high-novelty cases (survey notes need for better calibration and belief-scoring methods).",
            "validation_computational_cost": "human evaluation is expensive and slow; formal solver validation can be computationally cheap/deterministic but requires accurate translation; LLM-based iterative validation (RAG + critique loops) increases compute due to multiple retrieval/generation steps.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "retrieval-augmentation (RAG), human-in-the-loop, translation to formal languages / neurosymbolic pipelines, verifier-critic models (automated verifiers), ensemble and multi-agent critique loops, and test-case execution for code hypotheses.",
            "evidence_type": "supports",
            "key_findings": "Survey evidence shows LLMs readily generate hypotheses and rules but validating those outputs—especially when novel or out-of-distribution—remains a major weakness; formalization, retrieval grounding, and human-in-the-loop methods mitigate but do not eliminate this gap.",
            "uuid": "e2178.0"
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG) pipelines",
            "brief_description": "Pipelines that retrieve task- or domain-specific documents and then use an LLM to generate or refine hypotheses with iterative retrieval / critique steps, often used to ground hypotheses in external evidence.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_type": "hybrid system (retrieval + LLM)",
            "domain": "scientific hypothesis generation (domain-specific: e.g., biomedical, chemistry, literature review)",
            "generation_capability": "domain-grounded hypotheses, refinements of candidate hypotheses, literature-inspired research ideas",
            "validation_method": "iterative retrieval of related documents, LLM self-critique loops, novelty/originality scoring, grounding claims against retrieved evidence; in some pipelines, experts inspect final hypotheses",
            "novelty_measure": "automated novelty/originality scoring, editorial/expert judgment; comparison against existing literature retrieved during loop (e.g., whether claim is supported or novel relative to retrieved corpus)",
            "generation_performance": "improves relevance and factual consistency compared to unconstrained generation; pipelines report improved coherence and domain specificity (Hu et al. 2024; Yang et al. 2025), but may still trade off some novelty when many examples are provided in prompts.",
            "validation_performance": "better fact-consistency due to grounding, but dependent on retrieval coverage and quality; when retrieval misses relevant literature, validation fails silently.",
            "false_positive_rate": "not reported numerically; qualitatively reduced relative to unguided LLM generation due to grounding, but remains nonzero.",
            "false_negative_rate": "not reported numerically; may increase when retrieval is conservative or missing documents.",
            "novelty_effect_on_validation": "RAG improves validation of familiar/known facts but is less helpful for truly novel hypotheses not present in the retrieval corpus; novelty that lies outside the retrieved evidence remains hard to validate automatically.",
            "generation_validation_asymmetry": "reduced relative to naive LLMs but not eliminated — grounding helps, yet generative creativity can still outpace retrieval-backed validation.",
            "out_of_distribution_performance": "limited by the retrieval index; if novel examples are out-of-distribution relative to the corpus, performance drops.",
            "calibration_quality": "improved when retrieval provides supporting evidence; LLM confidence still not fully trustworthy without explicit provenance checks.",
            "validation_computational_cost": "higher than pure generation due to retrieval steps and repeated generation/critique loops; cost grows with iterative refinement.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "grounding to knowledge graphs, novelty-guided re-generation, iterative critique loops, human expert review.",
            "evidence_type": "mixed",
            "key_findings": "RAG pipelines increase factual grounding and reduce some validation failures, but rely on retrieval coverage and still struggle to validate genuinely novel discoveries not present in the corpus.",
            "uuid": "e2178.1"
        },
        {
            "name_short": "MOOSE-Chem",
            "name_full": "MOOSE-Chem (LLM-based multi-agent chemistry hypothesis system)",
            "brief_description": "A multi-agent LLM pipeline applied to chemistry papers that retrieves snippets, drafts hypotheses, and scores them for originality to rediscover or propose chemistry hypotheses from literature.",
            "citation_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "mention_or_use": "mention",
            "system_name": "MOOSE-Chem",
            "system_type": "multi-agent LLM pipeline",
            "domain": "chemistry (scientific hypothesis generation)",
            "generation_capability": "chemistry hypotheses derived from paper background/context and retrieved literature",
            "validation_method": "retrieval of related snippets, scoring for originality and relevance, multi-agent critique and iterative refinement; expert-segmented paper fields used as seed context",
            "novelty_measure": "originality scoring by the system and comparisons to existing literature; evaluated on rediscovering 'unseen' hypotheses from 51 chemistry papers",
            "generation_performance": "reported to produce inventive candidates and to rediscover hypotheses across held-out chemistry papers in the study; outperforms baseline transformer approaches in the reported evaluations (qualitative/relative claims in the survey).",
            "validation_performance": "validation is based on retrieval-grounding and scoring; effectiveness depends on retrieval quality and the segmentation of paper fields; not described with absolute numeric metrics in the survey.",
            "false_positive_rate": "not reported numerically in the survey; dependent on scoring thresholds and retrieval errors.",
            "false_negative_rate": "not reported numerically.",
            "novelty_effect_on_validation": "system explicitly targets 'unseen' chemistry hypotheses, but validation is more reliable when related literature exists to ground claims; purely novel chemistry claims without literature support remain difficult to validate.",
            "generation_validation_asymmetry": "present — generation can propose novel chemistry hypotheses, while automated validation relies on existing literature and scoring heuristics that may miss true novelties.",
            "out_of_distribution_performance": "tested specifically on rediscovery of hypotheses that were not present in the immediate seed; performance reported as positive in the study context but specifics not given in survey.",
            "calibration_quality": "not reported; scoring provides a proxy for confidence but calibration details absent.",
            "validation_computational_cost": "higher than single-pass generation due to retrieval, multi-agent scoring, and iterative refinement.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "multi-agent critique, retrieval grounding, expert segmentation of paper components, iterative novelty-guided loops.",
            "evidence_type": "supports",
            "key_findings": "MOOSE-Chem demonstrates that multi-agent retrieval-grounded pipelines can rediscover and propose domain-specific hypotheses, but automated validation still hinges on retrieval coverage and scoring heuristics, leaving truly novel hypotheses hard to confirm without human or experimental follow-up.",
            "uuid": "e2178.2"
        },
        {
            "name_short": "APEx",
            "name_full": "APEx (Automatic Programmatic Experimentation / benchmarking framework)",
            "brief_description": "A multimodal automated benchmarking framework that iteratively programs experiments (e.g., tailored test inputs and augmentations) to evaluate hypotheses about model capabilities in a fully automated loop.",
            "citation_title": "Automatic benchmarking of large multimodal models via iterative experiment programming.",
            "mention_or_use": "mention",
            "system_name": "APEx",
            "system_type": "automated benchmarking / experiment programming system",
            "domain": "multimodal model evaluation / hypothesis testing about model capabilities",
            "generation_capability": "automatically generates test cases (e.g., images) and experiment protocols to probe model hypotheses",
            "validation_method": "text-to-image retrieval/generation, image augmentations, automated execution of experiments on a library of models, automated analysis and iterative refinement of experiments",
            "novelty_measure": "robustness to augmentations and transformations; capacity to find counterexamples or failure modes via iterative experiment generation",
            "generation_performance": "framework successfully automates the generation of tailored tests to probe hypotheses; reported to support iterative discovery of model weaknesses in multimodal settings.",
            "validation_performance": "automated and repeatable benchmarking reduces human evaluation cost; validation quality depends on the experiment generation heuristics and chosen augmentations.",
            "false_positive_rate": "not reported numerically; automated tests can still generate spurious cases if generation tools introduce artefacts.",
            "false_negative_rate": "not reported numerically; may miss subtle failure modes if augmentations are limited.",
            "novelty_effect_on_validation": "APEx helps surface robustness failures for novel transformations but cannot fully validate scientific novelty of model-generated hypotheses outside the realm of its experiment primitives.",
            "generation_validation_asymmetry": "reduced for probing model capabilities because APEx automates both generation of tests and validation; however, for open-ended scientific claims APEx is not a complete validation solution.",
            "out_of_distribution_performance": "APEx is designed to construct out-of-distribution tests via augmentations; performance is dependent on the richness of its generation tools.",
            "calibration_quality": "not reported; framework focuses on automated detection of failures rather than calibrated confidence scores.",
            "validation_computational_cost": "nontrivial due to generation of many test artifacts and repeated model evaluations; higher cost than single-shot generation but substantially cheaper than large-scale human evaluation.",
            "human_validation_required": false,
            "gap_closing_mechanisms": "automated iterative experiment programming, augmentation strategies to probe robustness, automated analysis loops.",
            "evidence_type": "supports",
            "key_findings": "APEx shows automated, iterative experiment programming can systematically validate hypotheses about model capabilities and find robustness failures without heavy human involvement, but it is specialized to model-behavior hypotheses and not general scientific discovery validation.",
            "uuid": "e2178.3"
        },
        {
            "name_short": "CauseJudger",
            "name_full": "CauseJudger (LLM-based cause identification / abductive reasoning framework)",
            "brief_description": "A pipeline using LLMs to translate natural-language inputs into first-order-logic representations, filter premises, and perform forward reasoning to decide which hypothesis explains observations.",
            "citation_title": "Causejudger: Identifying the cause with llms for abductive logical reasoning.",
            "mention_or_use": "use",
            "system_name": "CauseJudger",
            "system_type": "neurosymbolic pipeline (LLM + FOL representation + reasoning)",
            "domain": "abductive logical reasoning / hypothesis validation",
            "generation_capability": "not primarily generative; translates and evaluates candidate hypotheses in formal logic",
            "validation_method": "LLM-based translation of NL to FOL, premise filtering, forward (formal) reasoning; evaluated on the CauseLogics dataset with programmatic ground-truth labels",
            "novelty_measure": "dataset-difficulty levels and binary ground-truth labels; novelty not the primary focus",
            "generation_performance": "N/A (system focused on validation); translation quality from NL to FOL is the key performance factor.",
            "validation_performance": "when translation is accurate, formal forward reasoning yields deterministic, verifiable validation results; overall performance depends on LLM translation quality and filtering accuracy.",
            "false_positive_rate": "not reported numerically; potential for false positives arises from translation/filtering errors that introduce or omit premises.",
            "false_negative_rate": "not reported numerically; translation omissions can cause false negatives by failing to capture necessary premises.",
            "novelty_effect_on_validation": "formal method preserves validation correctness for problem classes within CauseLogics, but novel natural-language formulations that are hard to formalize reduce validation reliability.",
            "generation_validation_asymmetry": "system narrows the asymmetry by translating validation into deterministic formal checks, but relies on non-deterministic LLM translation which can reintroduce asymmetry.",
            "out_of_distribution_performance": "performance falls when NL formulations deviate substantially from patterns seen in translator training or when required logical constructs are rare.",
            "calibration_quality": "not explicitly reported; confidence hinges on translator fidelity rather than calibrated belief scores.",
            "validation_computational_cost": "formal reasoning is efficient once representation is produced; translation and filtering add compute overhead but are cheaper than repeated human evaluation.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "neurosymbolic translation to formal logic, premise filtering, and deterministic solvers for final validation.",
            "evidence_type": "mixed",
            "key_findings": "Translating natural-language hypotheses into formal logic and then applying deterministic solvers can yield rigorous validation, but translation errors remain a major bottleneck that limits reliability on unconstrained natural-language inputs.",
            "uuid": "e2178.4"
        },
        {
            "name_short": "HtT / HypoGeniC",
            "name_full": "Hypotheses-to-Theories (HtT) and HypoGeniC frameworks",
            "brief_description": "Benchmark-driven iterative frameworks where LLMs propose hypotheses, apply them to benchmark problems, and keep rules that consistently produce correct predictions; HypoGeniC similarly generates candidate hypotheses and updates confidence based on accuracy across examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Hypotheses-to-Theories (HtT) / HypoGeniC",
            "system_type": "LLM-driven rule-learning pipelines (benchmarked)",
            "domain": "synthetic / benchmark hypothesis discovery (general reasoning)",
            "generation_capability": "generates candidate rules/hypotheses from benchmark examples",
            "validation_method": "external algorithm compares model-produced predictions to benchmark ground-truth answers; correct rules are retained in a rule library; hypothesis confidence updated from accuracy",
            "novelty_measure": "not designed to measure domain novelty — relies on benchmark correctness / prediction accuracy",
            "generation_performance": "able to build effective rule libraries for benchmark tasks when ground truth answers are available; success dependent on benchmark design and ground-truth availability.",
            "validation_performance": "deterministic and reliable only because benchmarks provide ground-truth answers; not applicable to real-world discovery without ground truth.",
            "false_positive_rate": "not reported numerically in survey; can be low within benchmarks due to deterministic check against ground truth.",
            "false_negative_rate": "not reported numerically.",
            "novelty_effect_on_validation": "frameworks perform well on benchmark/familiar tasks with ground truth but are not designed to validate novel, real-world hypotheses lacking immediate ground-truth answers.",
            "generation_validation_asymmetry": "apparent: these frameworks rely on external ground-truth to validate and therefore do not address validation of genuinely novel hypotheses.",
            "out_of_distribution_performance": "not evaluated meaningfully because tasks and observations are drawn from benchmarks and are not truly out-of-distribution by design.",
            "calibration_quality": "not reported; confidence is derived from empirical prediction accuracy on benchmarks.",
            "validation_computational_cost": "low to moderate — validation is a matter of comparing predicted answers to ground truth across datasets.",
            "human_validation_required": false,
            "gap_closing_mechanisms": "use of ground-truth-labeled benchmarks and iterative rule-library maintenance",
            "evidence_type": "supports",
            "key_findings": "HtT and HypoGeniC show that iterative rule-keeping from benchmark feedback can produce effective rule libraries for known tasks, but they do not generalize to real-world discovery where ground truth is unavailable.",
            "uuid": "e2178.5"
        },
        {
            "name_short": "Wason-Task / Seals & Shalin",
            "name_full": "Wason-card-style hypothesis testing (evaluative benchmark)",
            "brief_description": "Minimal proactive hypothesis-testing benchmark inspired by the Wason selection task: models must choose which observations (cards) to reveal to validate an 'if p then q' hypothesis expressed in natural language.",
            "citation_title": "Evaluating the deductive competence of large language models.",
            "mention_or_use": "use",
            "system_name": "Wason-style hypothesis testing benchmark",
            "system_type": "benchmark / probing task",
            "domain": "deductive rule-following / hypothesis validation",
            "generation_capability": "not generative — probes models' ability to select informative actions to validate a rule",
            "validation_method": "minimal action selection (flip appropriate 'cards') to obtain sufficient evidence for or against the rule; model must identify which observations are informative for validation (modus ponens and modus tollens)",
            "novelty_measure": "tests ability to apply rules in novel natural-language formulations and minimal action planning rather than measuring novelty of discoveries",
            "generation_performance": "used to reveal whether models can identify required tests to validate clauses; survey reports benchmark exposes limitations in natural-language rule-following.",
            "validation_performance": "models often struggle to identify the minimal sufficient checks in natural-language formulations, revealing failures in proactive validation and rule-following under natural-language variation.",
            "false_positive_rate": "not given numerically; qualitatively nontrivial as models sometimes choose irrelevant cards.",
            "false_negative_rate": "not given numerically; models sometimes fail to choose necessary cards, causing false negatives in validation.",
            "novelty_effect_on_validation": "task explicitly designed to probe validation under natural-language variability; validation fails more often as language framing becomes unfamiliar.",
            "generation_validation_asymmetry": "shows asymmetry: even when models can restate rules, they may not perform the correct minimal validation actions in novel framings.",
            "out_of_distribution_performance": "degrades with unfamiliar linguistic expressions of p and q.",
            "calibration_quality": "not reported.",
            "validation_computational_cost": "low — evaluation is action-selection and can be programmatically scored.",
            "human_validation_required": false,
            "gap_closing_mechanisms": "explicit training and fine-tuning on formal rule-following, or more structured prompting to map NL to formal constructs.",
            "evidence_type": "supports",
            "key_findings": "The Wason-style benchmarks show LLMs struggle to proactively select minimal evidence for validating simple 'if p then q' rules when expressed in varied natural-language forms, exposing gaps in hypothesis-testing ability on novel formulations.",
            "uuid": "e2178.6"
        },
        {
            "name_short": "TURTLEBENCH",
            "name_full": "TURTLEBENCH (Turtle Soup-inspired benchmark)",
            "brief_description": "A benchmark of real-world yes/no puzzles and question-answer pairs designed to test whether models can follow a story, maintain consistent reasoning over narrative premises, and answer True/False/Not Relevant.",
            "citation_title": "Turtlebench: Evaluating top language models via real-world yes/no puzzles.",
            "mention_or_use": "use",
            "system_name": "TURTLEBENCH",
            "system_type": "benchmark",
            "domain": "narrative deductive reasoning / hypothesis application",
            "generation_capability": "n/a (benchmark for evaluating deduction and story-based rule-following)",
            "validation_method": "prediction-based checks comparing model answers (True/False/Not Relevant) to human-annotated ground-truth across 1,532 curated question-answer pairs",
            "novelty_measure": "not a novelty metric; measures story-consistent deduction and application of given hypotheses/rules",
            "generation_performance": "provides a rigorous setting to evaluate whether LLMs can apply story-derived hypotheses; survey notes usage of TURTLEBENCH to reveal limits in following and applying story rules.",
            "validation_performance": "ground-truth comparison yields clear correctness metrics for deduction over narratives; LLMs show variable performance, particularly when reasoning requires deep contextual tracking.",
            "false_positive_rate": "not reported numerically in survey.",
            "false_negative_rate": "not reported numerically in survey.",
            "novelty_effect_on_validation": "benchmark focuses on narrative complexity rather than novelty; performance degrades with more complex or less typical narrative patterns.",
            "generation_validation_asymmetry": "benchmark exposes when models' verbalized rules/hypotheses do not translate into correct applied answers to questions.",
            "out_of_distribution_performance": "performance falls on puzzles with uncommon narrative structures or uncommon commonsense knowledge.",
            "calibration_quality": "not reported.",
            "validation_computational_cost": "moderate — automated scoring against annotated dataset is inexpensive; dataset creation expensive due to human annotation.",
            "human_validation_required": false,
            "gap_closing_mechanisms": "programmatic test cases, annotated QA pairs, and chain-of-thought supervision to align reasoning steps with final answers.",
            "evidence_type": "supports",
            "key_findings": "TURTLEBENCH and similar narrative QA benchmarks reveal that LLMs can fail to consistently apply hypothesized rules in story settings, especially when narratives require nuanced commonsense or multi-step inference.",
            "uuid": "e2178.7"
        },
        {
            "name_short": "He-puzzle-envs",
            "name_full": "He et al. puzzle environments (interactive numeric/letter probing puzzles)",
            "brief_description": "Interactive puzzle environments where agents input arbitrary integers or letters and receive tailored feedback based on a hidden quantitative rule; used to evaluate iterative probing, quantitative rule discovery, and clarity of deductive reasoning.",
            "citation_title": "Idea: Enhancing the rule learning ability of large language model agent through induction, deduction, and abduction.",
            "mention_or_use": "use",
            "system_name": "He et al. puzzle environments",
            "system_type": "interactive simulation / probe environment",
            "domain": "quantitative rule learning / controlled discovery",
            "generation_capability": "agents generate probe actions (numeric/letter queries) and candidate quantitative rules/hypotheses",
            "validation_method": "execute probes, collect feedback, evaluate whether the inferred rule predicts withheld test cases correctly; human judgments used to assess clarity and rigor of chain-of-thought",
            "novelty_measure": "measured by ability to discover hidden rules not trivially present in pretraining; tasks are intentionally designed to reduce hypothesis leakage",
            "generation_performance": "LLMs can discover simple quantitative rules via iterative probing in these controlled settings, but discovered rules often remain toy-like and limited in realism",
            "validation_performance": "validation via test-case prediction is reliable for the synthetic puzzles; however, human judgments are required to assess reasoning clarity and scientific rigor",
            "false_positive_rate": "not reported numerically; synthetic test-case matching reduces false positives but chain-of-thought quality issues can hide failures.",
            "false_negative_rate": "not reported numerically.",
            "novelty_effect_on_validation": "puzzles reduce pretraining leakage but still produce simplistic hypotheses; validation is robust for synthetic held-outs but does not translate trivially to real-world novelty.",
            "generation_validation_asymmetry": "present: models may generate candidate rules but require iterative probing and human evaluation to achieve robust validation and scientific rigor.",
            "out_of_distribution_performance": "not applicable beyond the synthetic environment; generalization to realistic scientific tasks is limited.",
            "calibration_quality": "not reported.",
            "validation_computational_cost": "moderate — many probe iterations and repeated model runs required; human scoring adds cost.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "interactive probing with tailored feedback, synthetic test-case evaluation, human judgment of reasoning steps.",
            "evidence_type": "mixed",
            "key_findings": "Interactive puzzle environments allow rigorous test-case-based validation of quantitative rule discovery but currently yield simplistic hypotheses and still require human assessment for reasoning quality and real-world applicability.",
            "uuid": "e2178.8"
        },
        {
            "name_short": "DiscoveryWorld / ScienceWorld",
            "name_full": "DiscoveryWorld / ScienceWorld virtual lab environments",
            "brief_description": "Simulated interactive environments (DiscoveryWorld, ScienceWorld) that allow agents to plan experiments, perform actions, collect observations, and attempt hypothesis discovery across scientific tasks; used to evaluate end-to-end discovery behavior.",
            "citation_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents.",
            "mention_or_use": "use",
            "system_name": "DiscoveryWorld / ScienceWorld",
            "system_type": "interactive simulated environment / virtual lab",
            "domain": "automated scientific discovery (education-style experiments, multi-step scientific tasks)",
            "generation_capability": "agents generate experimental plans, probe actions, and candidate hypotheses based on collected observations",
            "validation_method": "task completion metrics, execution of key experimental steps, comparison of discovered hypotheses to ground-truth rules built into the environment; some environments use RL policies to collect evidence",
            "novelty_measure": "not primarily focused on open scientific novelty — evaluates ability to rediscover or apply predefined rules and conduct experiments",
            "generation_performance": "agents can carry out multi-step experiments and solve many curriculum-style tasks, but coarse action spaces and prebuilt rules limit the complexity of hypotheses discovered",
            "validation_performance": "programmatic ground-truth comparisons enable objective validation in-simulation; however, RL-driven evidence collection can obscure evaluation of the agent's proactive hypothesis planning.",
            "false_positive_rate": "not reported numerically; environment-level ground truth reduces false positives for rediscovery tasks.",
            "false_negative_rate": "not reported numerically.",
            "novelty_effect_on_validation": "validation is reliable inside the environment but does not reflect validation of genuinely novel scientific claims outside the simulation.",
            "generation_validation_asymmetry": "mitigated inside simulations because ground truth exists, but real-world discovery remains more asymmetric.",
            "out_of_distribution_performance": "agents struggle when tasks require fine-grained experimental interventions or when action spaces lack expressivity.",
            "calibration_quality": "not reported.",
            "validation_computational_cost": "moderate to high due to simulation runs and RL training; programmatic scoring reduces human cost.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "interactive simulation with ground-truth rules, expanded action spaces, and finer-grained experimental primitives to better approximate real-world discovery.",
            "evidence_type": "mixed",
            "key_findings": "Virtual lab environments enable end-to-end evaluation of hypothesis discovery with objective validation, but many current sims use limited action spaces and known rules, limiting assessment of genuinely novel, real-world discovery and validation.",
            "uuid": "e2178.9"
        },
        {
            "name_short": "Neurosymbolic LLM+Solver",
            "name_full": "Neurosymbolic translation to formal solvers (LLM + FOL/code + symbolic prover)",
            "brief_description": "Approaches that use LLMs to parse natural-language hypotheses into formal languages (FOL or code) and then apply symbolic provers or execute code for deterministic validation of generated hypotheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "neurosymbolic LLM-to-formal-solver pipelines",
            "system_type": "neurosymbolic / hybrid (LLM + symbolic solver)",
            "domain": "logical reasoning, formal hypothesis validation, program synthesis for hypothesis testing",
            "generation_capability": "formal hypotheses (FOL rules, executable functions/code) synthesized from natural-language observations",
            "validation_method": "formal verification using symbolic provers or execution of generated code on held-out test cases; deterministic correctness checks when translation is accurate",
            "novelty_measure": "measured via held-out example generalization and predictions on unseen test inputs (e.g., programmatic testcases, ARC-like tasks)",
            "generation_performance": "when NL-to-formal translation is successful, deductive correctness is high; however, models often fail to produce correct formalizations in complex or ambiguous natural language.",
            "validation_performance": "very strong and deterministic once correct formalization is obtained; validator (solver) performance is essentially perfect for the formal domain but depends critically on translation fidelity.",
            "false_positive_rate": "low for solver-side validation (deterministic), but effective false positives arise when translation introduces spurious formal rules — numerical rates not reported.",
            "false_negative_rate": "low for solver-side validation; translation omissions can cause false negatives — not reported numerically.",
            "novelty_effect_on_validation": "novel natural-language hypotheses that are hard to formalize reduce validation reliability; the formal solver itself handles novelty in the logical sense but not linguistic novelty.",
            "generation_validation_asymmetry": "reduced when formalization is possible, but LLM translation errors reintroduce asymmetry between creative generation and provable validation.",
            "out_of_distribution_performance": "solver side remains deterministic; NL-to-formal mapping degrades on OOD linguistic constructions or very novel conceptual mappings.",
            "calibration_quality": "not reported; systems often lack calibrated uncertainty about translation correctness.",
            "validation_computational_cost": "formal verification typically cheaper than human evaluation; translation step can add cost depending on model and iterations.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "LLM fine-tuning for semantic parsing, verifier models, chain-of-thought supervision, and hybrid architectures (Logic-LM, LINC) that combine LLMs with provers.",
            "evidence_type": "mixed",
            "key_findings": "Neurosymbolic pipelines provide deterministic validation when natural-language hypotheses can be translated accurately, but NL-to-formal translation remains the bottleneck limiting reliable validation of open-ended, novel hypotheses.",
            "uuid": "e2178.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "rating": 2
        },
        {
            "paper_title": "Automatic benchmarking of large multimodal models via iterative experiment programming.",
            "rating": 2
        },
        {
            "paper_title": "Causejudger: Identifying the cause with llms for abductive logical reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Turtlebench: Evaluating top language models via real-world yes/no puzzles.",
            "rating": 2
        },
        {
            "paper_title": "Evaluating the deductive competence of large language models.",
            "rating": 2
        },
        {
            "paper_title": "Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents.",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis generation with large language models.",
            "rating": 1
        },
        {
            "paper_title": "Can llms follow simple rules?",
            "rating": 1
        },
        {
            "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning.",
            "rating": 1
        }
    ],
    "cost": 0.026584999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models
24 Aug 2025</p>
<p>Kaiyu He kaiyu.he@utdallas.edu 
Department of Computer Science
University of Texas at Dallas</p>
<p>Zhiyu Chen zhiyu.chen2@utdallas.edu 
Department of Computer Science
University of Texas at Dallas</p>
<p>From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models
24 Aug 20258F37A326FD3062241B8C2D95CF9FF566arXiv:2505.21935v2[cs.AI]https:openreview. netforum? id= d7W38UzUg0
Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge.In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world.Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery.We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps.By unifying these threads, we illuminate how LLMs might evolve from mere "information executors" into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.</p>
<p>Introduction</p>
<p>One major pillar of human intelligence is the capacity to discover hypotheses and learning rules.We call this capability hypothesis discovery (or rule learning).Earlier AI systems struggled with it because formal symbolic methods lacked the commonsense background needed for inventive rule formation (Yu et al., 2024a).Recent advances in natural language processing (NLP) have produced LLMs pretrained on extensive text corpora that embed substantial commonsense knowledge.These models now enable tasks that demand rich background knowledge, such as formulating new hypotheses and deriving novel conclusions.</p>
<p>Hypothesis discovery inherently relies on a blend of reasoning that includes abduction, induction, and deduction, each defined differently by various scholars.For instance, Gilbert H. Harman considers induction to be a special case of abduction, describing it as "inference to the best explanation" (IBE) (Harman, 1965;Douven, 2021).However, while this definition is easy to understand, it oversimplifies key aspects of hypothesis discovery.In particular, the notion of the "best" explanation is ambiguous and often requires additional assumptions that vary by context.Moreover, this framework does not fully capture real-world scenarios, where a "best" explanation is rarely reached immediately; rather, we continually experiment, gather new observations, and refine our hypotheses.Based on these considerations, we adopt Charles Peirce's definition of hypothesis discovery and reasoning, which posits that hypothesis discovery begins with forming an explanatory hypothesis to explain observations through abduction, proceeds with iteratively apply hypothesis to solve problem or derive new knowledge with deduction, and validate hypothesis through induction (Frankfurt, 1958;Peirce, 1974;Burks, 1946;Minnameier, 2004) (See explanation in Figure 2).Figure 1: Taxonomy for Hypothesis Discovery with LLMs.Our survey categorizes work into four topics based on Peirce's definition of hypothesis discovery: Generation (creating hypotheses that explain given observations with abduction), Application (deducing new observations from established hypotheses with deduction), Validation (verifying and refining hypotheses against new evidence with induction), and Integrated Hypothesis Discovery (examining the dynamic interdependencies among these components in a continuous, iterative process).</p>
<p>The rest of the survey is organized as follows.Section 2 presents background knowledge on hypothesis discovery using LLMs, including different forms of reasoning and representations involved in the process.Section 3 examines prior surveys on LLM reasoning and hypothesis discovery, highlighting their narrow emphasis on deductive tasks or application-specific methods.Section 4 reviews methods for forming hypotheses (Abduction).Section 5 then covers approaches for applying these hypotheses (Deduction), and Section 6 focuses on techniques for validating given hypotheses with new observations (Induction).Finally, Section 7 explores the entire hypothesis-discovery cycle by examining the interdependencies among these reasoning steps and showing how abduction, deduction, and induction can be iteratively used to refine more robust hypotheses.For each stage, we discuss methods, benchmarks, evaluations, and identify limitations and future directions.A high-level taxonomy guiding this survey is shown in Figure 1.</p>
<p>Figure 2: On the left-hand side, a) illustrates Peirce's framework for hypothesis discovery through abduction, deduction, and induction.The process begins with abduction, which generates explanatory hypotheses based on an initial set of observations.Deduction is then used to apply these hypotheses and derive predictions.Induction evaluates how well the predicted observations align with actual outcomes, updating the confidence of the hypotheses or rejecting those that are no longer valid.This process is iterative: validated hypotheses may be refined through further rounds of abduction using updated observations, gradually leading to more robust theories.On the right-hand side, b) provides a simple example that illustrates this process.</p>
<p>Background</p>
<p>Before LLMs, most AI systems stored knowledge as handcrafted symbols and rules.That format works well for deduction, because most of the problems we need to solve with symbolic AI systems work with limited premises and countable task-specific knowledge; for example, questions in the ProofWriter (Tafjord et al., 2021) and FOLIO (Han et al., 2024) benchmarks are limited to fewer than a hundred premises.However, abduction and induction are different: they call for generating and validating many tentative explanations inspired by vast commonsense or expert domain knowledge (such as weather patterns, social norms, or physics) and for updating beliefs as new observations arrive.Handling these reasoning tasks with symbolic AI meant writing and maintaining vast, interlocking rule bases, an effort so costly that few projects moved beyond toy domains (Yang et al., 2024c).Consequently, the research landscape remained dominated by deductive tasks (Yu et al., 2024a;Liu et al., 2025;Huang &amp; Chang, 2023).</p>
<p>LLMs have transformed this landscape.Trained on vast corpora, they implicitly absorb broad commonsense and domain knowledge, exhibiting strong reasoning abilities on complex, natural-language tasks (Yang et al., 2024c).With a simple text prompt, we can now ask them to carry out abduction or induction and even inspect their intermediate reasoning steps (Li et al., 2024;Jung et al., 2022), exposing the latent information they rely on.This advancement has made it practical to study and deploy defeasible reasoning (Yang et al., 2024c).Defeasible reasoning refers to forms of reasoning, such as abduction and induction, that yield probable conclusions that remain open to revision as new evidence emerges.This shift has fueled a wave of NLP research that places such flexible reasoning at the heart of AI progress (Liu et al., 2025;Huang &amp; Chang, 2023).</p>
<p>Hypothesis Discovery</p>
<p>Hypothesis discovery or Rule learning, the cyclical process of formulating hypotheses, gathering evidence, validating or refuting them, and ultimately establishing robust theories, lies at the heart of scientific progress (Eger et al., 2025).Early humans, for example, hypothesized that the Earth was flat based on everyday observations.Later, Eratosthenes measured shadow angles at different locations, obtaining evidence that suggested the Earth's surface was curved.This evidence challenged the flat Earth hypothesis, and subsequent findings, notably Magellan's circumnavigation, conclusively confirmed the Earth's roundness.Even with to-day's sophisticated instruments, researchers continue to iterate this loop in new domains, validating and refining theories as new data emerges.Today, there is growing interest in whether LLMs can autonomously generate, apply, and validate hypotheses from natural language represented observations, mirroring this iterative process to achieve interpretable and adaptive hypothesis discovery.Although many studies have explored individual steps of hypothesis discovery, their efforts tend to be scattered across abduction, deduction, and induction, with insufficient attention to how these forms of reasoning interconnect to drive genuinely iterative, hypothesis-driven discovery.</p>
<p>Reasoning</p>
<p>Reasoning is central to hypothesis discovery.Researchers have historically debated how best to categorize reasoning into clear, operational types.Different frameworks each have strengths and limitations (Harman, 1965;Douven, 2021;Bacon, 1878;Laudan, 1971;Mill, 2024;Stadler, 2011;Popper, 2005;Okoli, 2023).In this survey, we adopt Charles Peirce's definition of reasoning (Peirce, 1974;Burks, 1946;Minnameier, 2004), emphasizing abduction, deduction, and induction as separate but interrelated processes.We choose Peirce's framework for three main reasons.First, clarity: Unlike many other approaches, Peirce explicitly differentiates among the three reasoning types, preventing confusion, such as the common conflation of abduction and induction.Second, practicality: Peirce's model aligns each form of reasoning directly with a distinct phase in the hypothesis discovery cycle-abduction for generating hypotheses, deduction for applying these hypotheses, and induction for validating them.This clear mapping makes his framework particularly suitable for systematically studying the entire process of hypothesis discovery, rather than isolated reasoning components.Finally, granularity: Peirce's framework breaks down the scientific discovery process into welldefined, finer-grained steps, facilitating detailed analysis and enabling more structured evaluation.</p>
<p>Abductive Reasoning is the process of forming explanatory hypotheses to make sense of observed phenomena.It is the only form of reasoning that generates entirely new ideas or explanations (Peirce, 1974;Frankfurt, 1958).Given a set of observations, one uses creative thinking and recalls necessary knowledge to come up with hypotheses that plausibly explain these observations.Importantly, a single set of observations can lead to multiple possible explanations.For instance, if you come home and find the floor wet, you might form several possible explanations: perhaps a pipe leaked, or someone spilled water accidentally.Without additional evidence or testing, you can't know for sure which explanation is correct.This illustrates how abduction helps generate potential explanations, which then must be tested further.</p>
<p>Inductive Reasoning is the process of testing whether the hypothesis and deduced consequences really obtain and evaluating to what extent they obtain (Minnameier, 2004;Peirce, 1974).In practice, induction updates a hypothesis's confidence based on new observations, including rejecting it outright, or selects the most convincing candidate from a set of competing hypotheses.Consider the claim "Swans are (100%) white," or linguistically, "All swans are white," formed after observing 99 white swans in Texas.Encountering a black swan in New York contradicts that hypothesis.Through induction, we recognize this contradiction and lower our confidence in the original claim, adjusting it to "Swans are (99%) white," linguistically expressed as "Almost all swans are white."In this example, although the hypothesis appears "revised," the change is limited to its confidence level; no new explanatory perspective is introduced, and we do not actually form a new hypothesis.By contrast, abduction can lead us to a fresh explanatory hypothesis with new observations, e.g., "Swans' color depends on their habitat," or "All swans in Texas are white," which introduces new ideas and is not a case of induction.Thus, inductive reasoning verifies or refines existing hypotheses (in terms of confidence) based on accumulating evidence.</p>
<p>Deductive Reasoning is the process of logically deriving specific conclusions from general hypotheses or rules.If the initial hypotheses are true, deduction guarantees that the derived conclusions must also be true.For instance, from the general rule "All swans are white" and the observation "This bird is a swan," we logically conclude "This bird must be white".While traditional deductive reasoning tasks, such as instruction-following and standard problem-solving, have been extensively studied with LLMs (Pan et al., 2023;Wei et al., 2022;Liu et al., 2025;Huang &amp; Chang, 2023), deductive reasoning in the context of hypothesis discovery poses unique challenges.Specifically, it emphasizes inferential rule-following, requiring models to consistently apply hypotheses or rules to derive new and potentially unfamiliar conclusions, even when these hypotheses are counterfactual, unfamiliar, or incorrect.For example, when a flawed hypothesis is introduced in an unfamiliar domain, inferential rule-following requires us to strictly derive its predicted consequence, even if that consequence itself is incorrect.By comparing this consequence with experimental data, we can directly assess the hypothesis's validity and guide its revision.Conversely, if the deductive process is unreliable, we may overlook real contradictions and thus retain invalid hypotheses or discard valid ones.Indeed, recent work shows that although LLMs can demonstrate strong deductive performance on indistribution tasks, they rely heavily on surface-level pattern matching and fail to generalize their inferential rule-following to novel or counterfactual scenarios (Pu et al., 2025;Mirzadeh et al., 2024;Kang et al., 2024;Yan et al., 2025).</p>
<p>There are also other types of reasoning, such as analogical reasoning (Yuan et al., 2023;Jiayang et al., 2023).However, their function in hypothesis discovery is generally covered by abduction and induction.We will include these additional forms when we encounter a relevant case in the following section.</p>
<p>Rule Representation: Formal Language vs Natural Language</p>
<p>Table 1: Comparison of Natural vs. Formal Language Representations for the Hypothesis "Sam is a dragon".In natural language, commonsense knowledge is implicitly embedded, and derived knowledge relies on extensive commonsense, potentially resulting in different interpretations depending on background knowledge and context.In formal languages (e.g., FOL or code), the knowledge base must be defined explicitly and cannot fully capture all commonsense knowledge, however, the derived conclusions are deterministic and precise. . . .</p>
<p>Representation</p>
<p>Sam.fly()</p>
<p>There are many ways to represent hypotheses and rules, which we broadly divide into two categories: formal languages (FL) and natural languages (NL).Formal languages, such as first-order logic and programming languages, are systematic and rule-bound.After real-world entities are encoded as explicit literals, precise inference rules yield provably correct and sound conclusions, making these systems well suited to deductive reasoning.Yet the encoding process strips away many subtle semantic relationships and commonsense knowledge, limiting the system's ability to handle the creative, defeasible reasoning required for abduction and induction (McCarthy &amp; Hayes, 1981;Reiter, 1980;Hanks &amp; McDermott, 1987;Liu et al., 2025;Yu et al., 2024a;Huang &amp; Chang, 2023).Natural language preserves those nuances and aligns more closely with human cognition, so it is better suited to abductive and inductive tasks.However, its meanings are implicit and context-dependent, making it difficult to define a deterministic reasoning pipeline and reducing the reliability of the resulting inferences (See in Table 1).Accordingly, the following sections treat formal-language and natural-language approaches separately, emphasizing how their reasoning methods and evaluation protocols differ.</p>
<p>Related Surveys</p>
<p>Most existing work assessing LLM reasoning, both survey syntheses and popular benchmarks such as GSM8K (Cobbe et al., 2021), centres almost exclusively on multi-step deductive tasks, leaving abduction and induction, the engines of hypothesis discovery, largely unexplored.Surveys of the field Yu et al. (2024a); Liu et al. (2025); Huang &amp; Chang (2023) highlight the absence of systematic study and clear analytical frameworks for these modes, while benchmark analyses likewise show that abductive and inductive inference receive limited attention (Plaat et al., 2024;Li et al., 2025b).This imbalance obscures our understand-ing of whether, and to what extent, LLMs can perform the creative, evidence-based reasoning required for hypothesis-driven discovery.</p>
<p>On the other hand, research in the AI for Science domain takes a distinctly horizontal, application-driven approach.This body of work emphasizes practical tasks such as generating research ideas, conducting experiments, and synthesizing reports, often employing domain-specific pipelines tailored to individual scientific fields.However, these studies usually lack a generalizable reasoning framework applicable across different scientific contexts.Furthermore, their evaluation metrics, typically novelty, creativity, or consistency, tend to be subjective, human-centric, and thus difficult to generalize, offering limited theoretical insight into the underlying reasoning mechanisms involved in scientific discovery (Movva et al., 2025;Alkan et al., 2025;Reddy &amp; Shojaee, 2025;Gridach et al., 2025;Bazgir et al., 2025).</p>
<p>Our survey adopts a vertical, reasoning-centered perspective grounded in Peirce's classical framework.</p>
<p>It integrates three modes of reasoning into a unified view of hypothesis discovery: abduction for hypothesis generation, deduction for hypothesis application, and induction for hypothesis validation.Unlike prior surveys that emphasize primarily deductive tasks, we concentrate on the entire reasoning process involved in hypothesis discovery, explicitly covering both defeasible reasoning (abduction and induction) and deductive reasoning.By clearly defining each reasoning mode and explaining its role within each stage of the discovery process, we provide a structured basis for designing principled, model-agnostic benchmarks and evaluation tasks.Compared to existing application-oriented surveys, our framework thus offers a more abstract, systematic, and theoretically informed approach to understanding and enhancing the role of LLMs in automated scientific discovery.</p>
<p>Hypothesis Generation</p>
<p>Every scientific discovery begins with a set of observations, denoted as O = {o 1 , o 2 , . . ., o n }, that we aim to explain.Let h represent the generated explanation or hypothesis.The hypothesis generation task can be defined as generating an h such that:
h |= (o 1 ∧ o 2 ∧ • • • ∧ o n )
This notation means that h logically entails the observations.In other words, assuming h holds, it guarantees that all observations
o 1 ∧ o 2 ∧ • • • ∧ o n follow.
In this survey, we follow Peirce's definitions for reasoning.Accordingly, the primary process used in hypothesis generation is abduction, the method of formulating explanatory hypotheses to account for observed phenomena.</p>
<p>Method</p>
<p>Despite LLMs' demonstrated prowess in tasks like summarization or code generation, devising robust methods to guide them in hypothesis generation remains an active area of research.Recent work has sought to leverage LLMs' in-context learning and natural language understanding to produce novel or domain-specific hypotheses, spurring the development of new techniques aimed at improving both the quality and applicability of generated hypotheses (Yang et al., 2024c).In this section, we review these methods, spanning approaches that rely solely on prompting, those that integrate external knowledge sources, and those that incorporate human expertise in the loop.</p>
<p>Natural Language Hypothesis Generation with LLMs</p>
<p>Prompt-Based Methods: Due to the lack of large-scale, domain-specific data for hypothesis generation, most abduction approaches rely on prompt-based methods that are easy to deploy and don't require extensive additional data.For instance, when provided with observations expressed in natural language and asked to generate a plausible hypothesis that explains them, both Wiegreffe et al. (2022) and Qi et al. (2024) employ few-shot prompting to guide LLMs in generating hypotheses.Specifically, Wiegreffe et al. (2022) constructs few-shot examples using a triplet format (question, answer, explanation).In solving a task of generating biomedical hypotheses with given observations, Qi et al. (2024) embeds a small set of independent observation-to-hypothesis pairs in the prompt.By showing how each block of biomedical background observations maps to its corresponding hypothesis, the model learns to extract relevant domain cues and generate novel biomedical hypotheses.Their findings indicate that including more examples in the prompt tends to reduce the novelty of the generated hypotheses while increasing their correctness.Furthermore, Yang et al. (2024a) propose a pipeline for hypothesis generation that involves five prompt-based modules: one to generate hypotheses, one to test deductive consistency, one to verify that the hypothesis is not merely a copy of the given context, one to assess its generalizability, and one to determine whether the hypothesis is trivial.</p>
<p>RAG-Based Methods:</p>
<p>Labeling massive corpora for pre-training is costly, but assembling a small or medium dataset for Retrieval-Augmented Generation (RAG) is practical, and several studies follow a similar iterative three-step pattern: (i) retrieve task-specific documents, (ii) let an LLM generate or refine hypotheses, and (iii) iterate with LLM feedback.For instance, after a user supplies a seed paper and asks the LLM to generate a worthwhile hypothesis to pursue in research, Hu et al. (2024) query the Scholar API for related work, then repeatedly generate and critique hypotheses, gradually expanding a web of novel ideas.Yang et al. (2025) apply the same loop to 51 top-tier chemistry papers from 2024: experts first segment each paper into background, inspiration, and hypothesis; an LLM-based multi-agent system (MOOSE-Chem) then retrieves relevant snippets, drafts hypotheses, and scores them for originality.A similar pipeline appears in Yang et al. (2024b), where 50 conference papers are annotated in the same three fields, augmented with thematically similar web documents and 14 survey papers so that the LLM can judge both relevance and novelty.</p>
<p>Two variants enrich the retrieval step with structured or fine-tuned knowledge.Xiong et al. (2024) ground each hypothesis in a domain knowledge graph: entities mentioned during generation are checked against graph relations, ensuring the final claims remain fact-consistent.In contrast, Chai et al. (2024) fine-tune a T5 model (Raffel et al., 2020) on curated scientific abstracts and, during inference, retrieve citation contexts and related data; a novelty-guided loop then re-generates until the candidate is both coherent and inventive, outperforming standard transformer baselines.</p>
<p>Human-in-the-loop Hypothesis Generation with LLM:</p>
<p>Recent studies show that combining humans with LLM support yields higher-quality, more novel hypotheses than either party working alone.The quality of natural-language hypothesis generation largely depends on the inherent capabilities of LLMs.Because these models excel at in-context learning, prompt strategies such as Chain-of-Thought (CoT) and Reflexion (Wei et al., 2022;Shinn et al., 2023) can be applied directly to this task.However, unlike computervision research, which gained rapid momentum from the ImageNet benchmark, hypothesis generation lacks a comparable, widely recognized task set.The main challenge is therefore the absence of a reliable evaluation task and benchmark for natural-language hypotheses, an issue examined further in Section 4.2.</p>
<p>Formal Language Hypothesis Generation with LLM</p>
<p>One major advantage of formal hypotheses is that once a formal language hypothesis is obtained, we can directly perform inference on it with guarantees of soundness and correctness.Depending on whether observations are represented in formal or natural language, methods for proposing a formal language hypothesis need to be discussed separately.</p>
<p>Formal Language Observations:</p>
<p>When observations are encoded in a formal language, dedicated formal language solvers typically yield clear, white-box solutions that outperform language models.Consequently, using an LLM for these tasks is generally not preferred.Nevertheless, a few early studies in the LLM era have explored this approach.For example, Young et al. (2022) 2025) first ask the model for a single-word "main concept," then use that concept to steer subsequent code generation, avoiding the similarity of low-temperature outputs and the degeneration of high-temperature sampling while still producing coherent hypotheses.</p>
<p>A complementary line of work probes the model's internal representations.Using sparse autoencoders (SAE) (Bricken et al., 2023), Movva et al. (2025) isolate neurons activated when the LLM predicts the click rate of Twitter posts and discover that neurons associated with "surprise" or "shock" positively influence the score, supporting the hypothesis that surprising or shocking content tends to receive more clicks.</p>
<p>Evaluation for Hypothesis Generation</p>
<p>Due to LLMs' strong reasoning abilities and natural language interface, many methods have been proposed for hypothesis generation, and numerous ideas based on everyday human reasoning can be adapted for this purpose (Niu et al., 2024).However, a major challenge remains in establishing a grounded and convincing way to evaluate the quality of the generated hypotheses.</p>
<p>Natural Language Hypothesis Evaluation</p>
<p>Although prompting LLMs to generate natural language hypotheses is straightforward, evaluating the quality of these hypotheses is challenging due to the ambiguity inherent in natural language representations.Consequently, a common evaluation method involves either human evaluation or using an LLM to assess the generated hypotheses' validity (Zhao et al., 2024;Yang et al., 2024b;Hu et al., 2024;Qi et al., 2024;Yang et al., 2025).While human evaluation can provide valuable insights without relying on predefined answers, it is inherently subjective, less reproducible, expensive, and sometimes not entirely convincing.Therefore, alternative evaluation strategies are needed.</p>
<p>Implicit Prediction-based Evaluation:</p>
<p>Early benchmarks often relied on question-answering (QA) tasks that required the model to implicitly form a hypothesis to answer a question (Sinha et al., 2019;Weston et al., 2015).For example, consider the observation: "Lily is a swan, Lily is white, Bernhard is green, Gerg is a swan.What color is Greg?"To answer correctly, one must infer an implicit hypothesis, such as "All swans are white" or "Most swans are white," based on the fact that Lily is both a swan and white.Thus, the correct answer is "white."By verifying whether the model's answer is "white," one can indirectly assess its ability to form an appropriate hypothesis and perform reasoning.Similarly, recent work shows that prompting LLMs to generate an intermediate hypothesis and then using that hypothesis for inference yields higher performance on complex tasks (Balepur et al., 2024;Shi et al., 2023;Wang et al., 2025).However, this approach is problematic: the hypothesis may be formed incorrectly, the subsequent inference could be flawed, and the model might arrive at the correct answer through memorization or random guessing rather than proper abductive reasoning.Therefore, success in these tasks does not directly imply that the model possesses superior abductive capabilities, making them unsuitable for reliably evaluating hypothesis generation.</p>
<p>Ground Truth-based Evaluation: Some studies build benchmarks with labeled hypotheses so that outputs of LLM can be matched directly against references.DEER (Yang et al., 2024a) supplies 1,200 fact-rule pairs, all written in natural language by experts across six topics-zoology, botany, geology, astronomy, history, and physics.Generated hypotheses are compared with the gold rules using token-level mapping metrics like METEOR (Banerjee &amp; Lavie, 2005).In biomedicine, Qi et al. ( 2024) curate a benchmark with both seen and unseen samples: the seen split contains 2,700 background-hypothesis pairs collected before January 2023, whereas the unseen split has 200 pairs collected after that date.Outputs are evaluated against the ground truth with BLEU and ROUGE (Papineni et al., 2002;Lin, 2004)</p>
<p>Formal Language Hypothesis Evaluation</p>
<p>Unlike natural language hypotheses, formal hypotheses evaluations are more grounded due to their clarity and unambiguous semantics.</p>
<p>Ground Truth-based Evaluation: Generated formal hypotheses can be evaluated against pre-defined ground truth hypotheses.Unlike natural language evaluation, where ground truth is often written by domain experts and evaluated using token-level metrics like BLEU or ROUGE, formal hypotheses can be evaluated procedurally using solvers.This allows us to verify correctness deterministically.For example, Bowen et al. (2024) designed formal representations for synthetic grouping tasks to evaluate formal language hypothesis generation.Hua et al. (2025)</p>
<p>Discussion and Future Directions in Hypothesis Generation</p>
<p>There exists a significant gap between formal and natural language approaches to hypothesis generation.</p>
<p>In natural language hypothesis generation, observations typically originate from recent research papers, and generated hypotheses can potentially inspire novel research ideas with tangible real-world impacts (Eger et al., 2025).However, rigorous and reliable evaluation methods for such hypotheses remain underdeveloped.Token-based metrics, such as BLEU or ROUGE, do not effectively capture the qualitative aspects of openended hypothesis generation (Yang et al., 2024b).Meanwhile, alternative approaches involving human or LLM-based evaluations are costly, subjective, and prone to inconsistencies.</p>
<p>Conversely, formal language hypothesis generation benefits from grounded, objective evaluation methods.Nevertheless, existing formal tasks often involve simplified or artificial scenarios that fail to reflect the complexity and nuance inherent in real-world applications.Consequently, the field faces a trade-off: formal representations facilitate robust evaluation but risk omitting critical real-world nuances, while natural language representations capture real-world complexity yet lack rigorous evaluation mechanisms.</p>
<p>To address this challenge, future research in hypothesis generation could focus on two key directions.Firstly, there is an urgent need to develop novel evaluation methodologies tailored specifically for natural language hypothesis generation.Current implicit prediction-based evaluations suffer from inherent limitations, and ground truth-based evaluations remain inadequate due to reliance on token-level similarity metrics.Alternative evaluation strategies, potentially involving multi-dimensional human assessments, structured feedback mechanisms, or hybrid evaluation frameworks integrating automated and expert evaluations, merit exploration.Secondly, bridging the gap between formal and natural language hypothesis generation is crucial.</p>
<p>Leveraging code as an intermediate representation offers a promising path forward, combining evaluative rigor with expressive capability.However, existing code-based hypothesis generation benchmarks tend to focus on oversimplified problems that lack relevance to practical scenarios.Thus, developing realistic, code-based hypothesis-generation tasks grounded in established research papers, real-world datasets, and open-source repositories presents a compelling and valuable direction for future research (Chen et al., 2024).</p>
<p>Hypothesis Application</p>
<p>Given a hypothesis h, hypothesis application is defined as the derivation of a new observation o new such that:
h |= o new
In some cases, the hypothesis may depend on a context c, so that h can be viewed as a function of c.In this context-dependent formulation, hypothesis application is defined as deriving a new observation o new such that:
h(c) = o new
In our work, we follow Peirce's definitions for reasoning.Accordingly, the primary process used in hypothesis application is deduction, the method of deriving necessary consequences from a given hypothesis.</p>
<p>Notably, when a hypothesis is expressed in a formal language, directly applying it with a deterministic solver yields a correct and sound prediction.Therefore, there is little motivation to leverage LLMs for deductive reasoning on formal hypotheses.This section, consequently, focuses on the natural language hypothesis application and evaluation.2024) treat LLMs as formal language parsers, using them to translate natural language hypotheses into formal representations like FOL and code before applying a formal inference procedure.This translation significantly improves deductive correctness.However, these methods have primarily been evaluated on benchmarks such as ProofWriter (Tafjord et al., 2021) and FOLIO (Han et al., 2024), where the questions are already closely aligned with formal language.For example, given the input "Fact1: Eric is young, Fact2: Dave is white, Rule 10: if someone is young and not kind then they are big", translating this into FOL is relatively straightforward.It remains unclear whether LLMs can reliably parse more complex, everyday natural language into formal representations.</p>
<p>Method</p>
<p>LLM as Formal</p>
<p>Fine-Tuning-Based Method: Fine-tuning is a common approach to improve model performance when corresponding training data is available.Sun et al. (2024) proposed a synthetic "StringGame" task in which ground truth hypotheses and answers are provided.Leveraging the CoT approach, a LLM is prompted to generate multiple candidate hypothesis application trajectories along with their results.By comparing these results with the ground truth, the trajectories that produce correct outcomes are identified as correct and stored for fine-tuning.The resulting fine-tuned model then demonstrates improved performance in both hypothesis application and instruction following.</p>
<p>Prompt-Based Method: Although CoT prompting has improved performance on multi-hop question answering tasks, Sun et al. (2024) found that it does not directly enhance performance in hypothesis application.Therefore, new prompting methods have been designed specifically for this purpose.Inspired by mathematical induction, Cai et al. (2025) propose quantifying the difficulty of a question so that the LLM can solve it incrementally, from simpler versions to more complex ones, ultimately arriving at the correct answer.In another approach, Ling et al. ( 2023) design a pipeline that supervise the correctness of each reasoning step during hypothesis application.First, the LLM indexes all premises; then it is asked to label the minimal set of premises required to derive new facts.This pipeline generates multiple candidate hypothesis application trajectories, and by having the LLM vote on each step, the most convincing deductive trajectory is selected.</p>
<p>Evaluation for Hypothesis Application</p>
<p>Although many benchmarks and evaluation methods exist for general deductive reasoning, such as question answering and mathematical tasks like GSM-8k (Cobbe et al., 2021), these question types do not explicitly test the formation of new facts based on given hypotheses or rules.Evaluating the correctness of a naturallanguage deductive trajectory is challenging because annotated reasoning paths for hypothesis application are scarce, and the same result can follow from different reasoning paths.As a result, most evaluations use prediction-based checks.We assume that, given a correct hypothesis and a known ground-truth result, a valid deduction will reproduce that result.By comparing the model's deduced outcome with the ground truth, we can judge whether its deduction is correct.For example, take the hypothesis "Coin flips are independent and identically distributed (i.i.d.) with a 50 percent chance of heads."When asked, "After three consecutive heads, what is the probability of a tail on the fourth flip?," a flawed model might claim the chance of a tail has increased.In fact, under the i.i.d.assumption, the probability remains 50 percent.Supplying the correct hypothesis and comparing the model's answer to the true result lets us evaluate whether its deductive reasoning is valid.</p>
<p>Building on this idea, Yu et al. (2024b) create the TURTLEBENCH benchmark, inspired by the "Turtle Soup" game in which players deduce a story's hidden explanation by asking yes/no questions; in TURTLEBENCH, the LLM instead answers human-annotated questions with "True," "False," or "Not Relevant," across 1,532 high-quality question-answer pairs sourced from an online platform to test whether it can fully follow a story and provide accurate answers.Similarly, Mu et al. (2024) introduced the RULES benchmark, comprising 14 rule-following scenarios, each paired with concise test cases and programmatic evaluation functions that objectively assess adherence to specified rules.In addition, Cai et al. (2025) presented the Holiday Puzzle benchmark, which features multiple holiday schedule scenarios ranging from simple single-week planning to multi-phase arrangements and complex date arithmetic tasks, again using test cases and evaluation functions to verify correct computation of extra holiday rest days under provided rules.Moreover, Sun et al. (2024) constructed RuleBench to evaluate not only whether models can produce correct answers based on factual rules but also their ability to apply counterfactual rules, designed to yield incorrect outcomes, and experiments show that while LLMs achieve near-perfect accuracy on factual rules, their performance drops dramatically under counterfactual rules, revealing a significant gap in counterfactual rule-following capability.</p>
<p>Discussion and Future Directions in Hypothesis Application</p>
<p>While traditional deductive reasoning tasks (e.g., question answering, problem solving) in LLMs have been widely studied, the capability for hypothesis application remains significantly underexplored.According to Sun et al. (2024), hypothesis application involves inferential rule-following, requiring models to consistently apply given hypotheses to derive novel knowledge in unfamiliar domains.Robust hypothesis application is critical to hypothesis discovery, as hypotheses must generalize to scenarios with unseen observations.However, existing LLMs frequently struggle to extend hypotheses beyond familiar contexts, thus limiting the evaluation of hypothesis generation.</p>
<p>Future research could therefore focus on rigorously evaluating LLMs' hypothesis application, both factual and counterfactual, in novel scenarios.Developing benchmarks explicitly designed for hypothesis-driven inference in unfamiliar domains could reveal important insights into model adaptability and generalization.</p>
<p>Additionally, current evaluations of hypothesis application mainly rely on outcome-based correctness, comparing predicted results to ground truth given correct hypotheses.However, incorrect reasoning may still lead to correct predictions in natural-language contexts.Although Ling et al. (2023) propose improving hypothesis application by intervening in reasoning trajectories, a large-scale benchmark specifically designed to evaluate trajectory-based hypothesis application remains absent.</p>
<p>Hypothesis Validation</p>
<p>According to Peirce, induction validates a hypothesis by updating its confidence when new evidence appears.However, in studies that focus exclusively on induction, tasks are typically one-off: a hypothesis (or set of hypotheses) and a collection of observations are provided, and there is no iterative updating of confidence.</p>
<p>A simplified framework for hypothesis validation treats it as a multiple-choice problem: given observations O = {o 1 , o 2 , . . ., o n } and a set of hypothesis H = {h 1 , h 2 , . . ., h m }, the model selects the most possible hypothesis.In simpler scenarios, where only one hypothesis is provided, the model determines whether the hypothesis correctly explains the observations.In the next section, when combined with deduction and abduction, induction can subsequently be used to iteratively update the confidence in the hypothesis.</p>
<p>Natural language representations add significant complexity to induction.In formal language settings, all necessary information is explicitly provided, and reasoning follows rigorous, well-defined steps.In contrast, validating a natural language hypothesis often requires commonsense knowledge and interpretation of nuanced language.For example, consider the observations "Neil wanted to see the mountains of Asia" and "Neil loved being so close to the mountains in Nepal," with candidate hypotheses "Neil booked a trip online" and "Neil took a trip to see the Rocky Mountains instead."Here, the nuanced meaning of the term "instead" and the geographic relationships require careful analysis and may lead to different conclusions.Indeed, Zhao et al. (2023) reports that, when verifying their dataset where five annotators judged the plausibility of handwritten hypotheses, disagreements occurred in 62.34% of 1,365 explanations, underscoring the challenge of natural language hypothesis validation.</p>
<p>Method</p>
<p>Formal Language Hypothesis Validation</p>
<p>He &amp; Lu (2024) introduce the CauseJudger framework, which leverages LLMs at every stage to validate candidate hypotheses.First, an LLM transforms the natural language inputs into an FOL-based representation by integrating each candidate hypothesis into the premises.Next, an LLM filters out irrelevant premises and rules.Finally, another LLM performs forward reasoning to decide which hypothesis explains the observations.</p>
<p>Natural Language Hypothesis Validation</p>
<p>Prompt-Based Method: Lampinen et al. (2022); Sun et al. (2024) employ a few-shot prompting approach for hypothesis validation.In this method, case triplets, consisting of an observation, a hypothesis, and its corresponding validity, are provided to the model, which then answers a hypothesis validation question.</p>
<p>Although this approach improves performance, Sun et al. (2024) reports that the performance boost is limited.Their experiments further indicate that fine-tuning outperforms few-shot prompting.</p>
<p>Fine-Tuning-Based Method: Since hypothesis validation essentially constitutes a classification problem, many Natural Language Inference (NLI) datasets can be adapted into hypothesis validation tasks.Consequently, fine-tuning is a popular method in this context.For example, Zhao et al. (2023); Chan et al. (2023); Sun et al. (2024) fine-tune models to select the correct hypothesis from a set of hypotheses based on new observations.</p>
<p>Evaluation for Hypothesis Validation</p>
<p>Formal Language Evaluation</p>
<p>Along with the CauseJudger framework, He &amp; Lu (2024) also proposed the CauseLogics dataset.Based on the required formal reasoning depth, the dataset is divided into four difficulty levels for hypothesis validation tasks, with 50,000 samples per level.Each hypothesis is assigned a binary ground-truth label indicating whether it correctly explains the observations.</p>
<p>Natural language Evaluation</p>
<p>Binary-Classification-Based Evaluation: Lampinen et al. ( 2022) chose a subset of 40 tasks from the crowd-sourced benchmark BIG-bench (bench authors, 2023) and constructed their own benchmark specifically for hypothesis validation.Each data sample consists of an observation, its corresponding hypothesis, and a ground truth label indicating whether the hypothesis truly explains the observation.</p>
<p>Hypothesis validation using natural language is inherently challenging because the implicit information and required common-sense background are not explicitly stated.This often leads different individuals to draw different conclusions when validating a hypothesis based solely on recalled information.Rudinger et al. (2020) mitigate this issue by adopting a different strategy.Instead of asking annotators to directly judge whether an observation explains a hypothesis, they ask the model to determine if a given observation weakens or strengthens the hypothesis.Specifically, they sample observation-hypothesis pairs from existing datasets and then manually craft two types of sentences: one that acts as a "strengthener" (increasing the likelihood of the hypothesis) and one that acts as a "weakener" (decreasing the likelihood of the hypothesis).</p>
<p>Their validation process showed that the strengthening and weakening effects are consistent across different annotators.During evaluation, the model is required to decide whether a new observation strengthens or weakens the hypothesis.This approach aligns with the paper's goal of modeling defeasible inference by leveraging explicit contextual updates rather than relying on potentially variable human interpretations of implicit information.Furthermore, Zhang et al. (2025) extended this task to include visual observations.In their extension, given a visual observation and a natural language hypothesis, an LLM is tasked to determine whether the provided sentence serves as a strengthener or a weakener.</p>
<p>Multiple-Choice-Based Evaluation</p>
<p>Bhagavatula et al. ( 2020) introduce the ART benchmark, comprising roughly 20k narrative contexts where each sample includes two time-ordered observations, one depicting a story's start (o 1 ) and the other its outcome (o 2 ), alongside two hypotheses: a plausible explanation (h + ) and a less plausible one (h − ), challenging models to choose the best explanatory hypothesis and enabling adaptation to hypothesis-generation tasks evaluated against ground-truth explanations.Similarly, Jiang et al. (2023) present the BRAINTEASER benchmark of about 1.1k lateral-thinking puzzles, each offering a question with multiple-choice answers, one that defies commonsense and several conventional distractors, in both sentence (narrative) and word (meaning-alteration) formats to test creative reasoning, with additional semantic and context reconstruction variants assessing reasoning consistency and robustness across formulations.Moreover, Del &amp; Fishel (2023) introduced the True Detective benchmark for deep hypothesis validation, featuring 191 long-form detective puzzles (≈ 1200 words each) from the "5 Minute Mystery" platform, where models (and humans) select the correct explanation from 4-5 options, human accuracy averages 47%, top solvers exceed 80%, and each puzzle includes golden chain-of-thought explanations detailing the reasoning steps that lead to the correct answer.</p>
<p>Discussion and Future Directions in Hypothesis Validation</p>
<p>Previous literature often conflates hypothesis generation and hypothesis validation, primarily due to ambiguity inherent in the IBE paradigm.Within IBE-based approaches, hypothesis validation typically appears as an implicit intermediate step, where selecting the "best" hypothesis is frequently based on unclear or subjective criteria without dedicated, independent evaluation.However, adopting Peirce's explicit distinction between abduction, deduction, and induction clearly separates validation from generation, underscoring the need for dedicated research on validating hypotheses against newly observed evidence.</p>
<p>Current validation methodologies predominantly adopt end-to-end metrics that only assess final correctness, neglecting the reasoning processes and commonsense knowledge required to validate hypotheses in realistic settings.The subjective nature of natural language, coupled with different interpretations of observations, highlights the necessity for richer evaluative frameworks.Future benchmarks should incorporate detailed intermediate Chain-of-Thought data, capturing explicit reasoning steps humans take when validating hypotheses, such as recalling relevant commonsense knowledge and performing nuanced inference.Evaluations should then emphasize consistency between the reasoning process and available commonsense context rather than relying solely on superficial similarity to reference answers.Such benchmarks would greatly enhance our understanding of hypothesis validation and better reflect the complexities of human-like reasoning.</p>
<p>Hypothesis Discovery</p>
<p>Although many works introduced in the previous sections propose methods and evaluation metrics, they mainly focus on individual phases of Hypothesis Discovery-Hypothesis generation (Abduction 4), Hypothesis application (Deduction 5), and Hypothesis validation (Induction 6).However, in real-life Hypothesis Discovery, these reasoning stages are not independent and must be treated holistically.Initially, we form hypotheses based on limited observations using abduction, which subsequently informs the application of these hypotheses through deduction, enabling the collection of further evidence.Concurrently, induction continuously evaluates and resolves inconsistencies arising between newly obtained observations and earlier hypotheses.This iterative interplay means that each hypothesis formulated, action taken, observation gathered, and inconsistency identified dynamically shapes and reshapes our evolving understanding, influencing subsequent reasoning steps and contributing to diverse interpretations of the world.Treating any single reasoning phase in isolation oversimplifies hypothesis discovery.For example, although Bowen et al. (2024) evaluated every reasoning, they handled each step separately and thus failed to assess the true rule-learning capability of LLMs.Consequently, integrating abduction, deduction, and induction into a unified learning loop remains both challenging and largely understudied, yet it is the ultimate goal for constructing end-to-end agents capable of scientific discovery.</p>
<p>Despite a few studies that acknowledge the interdependence among reasoning types and allow models to refine hypotheses iteratively, they still overlook two decisive aspects of real-world hypothesis discovery.First, most benchmarks remain static and passive: they hand agents a fixed set of observations deemed sufficient to reach the correct hypothesis, whereas real-life hypothesis discovery requires actively seeking additional evidence.Second, even in settings that allow proactive information gathering, the granularity of the action space is still too coarse: agents fetch observations via one-shot "recall" or "web-search" commands, whereas real scientists must strategically plan and carry out precisely staged experiments-often designing specialized equipment at each step.Recognizing these limitations, we categorize existing hypothesis-discovery research into three classes (see Fig. 3).</p>
<p>Passive Hypothesis Discovery</p>
<p>In this type of study, LLMs generate, apply, and validate hypotheses iteratively.However, the observations are provided by a fixed dataset.The LLM does not need to worry about which observations it will receive.Instead, it simply reasons based on the given data, passively receiving and processing the information provided.2024) proposed the Hypotheses-to-Theories (HtT) Framework to generate formal hypotheses (e.g., "if A then B") by leveraging existing benchmarks (Sinha et al., 2019;Wang et al., 2022b;Rule, 2020).</p>
<p>In HtT, LLMs generate a hypothesis and propose learned rules to each question.When a new question is received, the model first formulates a preliminary hypothesis based on the context.It then proposes candidate rules that might lead to the correct answer.These candidate rules are applied to the problem and verified against the ground truth.Rules that consistently yield correct predictions are retained and added to the rule library, while ineffective ones are discarded.Iteratively, after processing all questions in the benchmark, the LLM builds a rule library containing effective rules for solving the questions.et al., 2021), and Tweet Popularity (Tan et al., 2014).Due to the complexity of real-world data, the generated hypotheses are more nuanced and expressed in natural language.Similar to HtT, HypoGeniC begins by generating a set of candidate hypotheses from a small number of examples.As new observations are processed, each hypothesis is used to make predictions and is assigned a reward based on its accuracy.The system dynamically updates the confidence of each hypothesis; those that consistently perform poorly are removed from the hypothesis bank.New hypotheses are generated from examples that existing hypotheses fail to explain, allowing the model to refine and expand its understanding over time.</p>
<p>Both HypoGeniC and HtT simplify hypothesis discovery by relying on benchmark questions that include ground-truth answers.This configuration allows an external algorithm, not the LLMs themselves, to validate generated hypotheses and update their confidence based on the correctness of predictions.In real-world scenarios, where no ground-truth answers are available, these frameworks become inapplicable and would require substantial adaptation.</p>
<p>Proactive Hypothesis Discovery</p>
<p>In real-life hypothesis discovery, we do not start with a predefined set of observations that continuously propose new insights.Instead, once an initial hypothesis is formed, we proactively recall our memories or explore further to gather new observations that either strengthen or weaken the hypothesis, allowing us to verify and refine our ideas.</p>
<p>Given a hypothesis, Li et al. (2024) and Jung et al. (2022) propose two proactive methods for hypothesis discovery that both leverage the LLM's parametric memory to generate evidence that either strengthens or weakens the hypothesis.In Hypothesis Testing Prompting, the model directly uses its internal reasoning to evaluate the generated evidence, determining which pieces are more convincing, and then decides whether the hypothesis is correct based on the balance of evidence that strengthens or weakens it.In contrast, Maieutic Prompting iteratively constructs a tree of evidence by generating both strengthening and weakening explanations.It then employs the LLM to assign a belief score (reflecting the model's confidence in the evidence) and a consistency score (measuring how well the evidence aligns with the hypothesis).Finally, a MAX-SAT solver is applied to select the subset of evidence that maximizes the overall scores, thereby determining whether to accept or reject the hypothesis.</p>
<p>Different from relying solely on an LLM's parametric memory to generate new evidence, Seals &amp; Shalin (2024) propose a minimal setting for proactive hypothesis discovery.Inspired by the Wason Task from cognitive science, this task challenges LLMs to prove a formal language hypothesis of the form "if p then q." Here, both p and q are objects described in natural language, for example, "if a person is a man, then he drinks alcohol."The task provides four cards, each with two sides representing different attributes.Initially, one side of each card is shown, displaying p, q, ¬p, and ¬q, while the other side reveals the state of another attribute.To rigorously validate the hypothesis "if p then q," one must flip the p card to confirm that its hidden side is q (modus ponens) and flip the ¬q card to check that its hidden side is ¬p (modus tollens).</p>
<p>Flipping only these two cards provides sufficient evidence for the hypothesis, while the other two cards do not offer the necessary information.Thus, in this benchmark, by proactively flipping two cards, we can determine whether the LLM can correctly identify natural language expressions of p and q and validate the hypothesis using a minimal action space.</p>
<p>Moreover, Conti et al. (2024) propose APEx, a multimodal automatic benchmarking framework that evaluates hypotheses about large multimodal models in a fully automated and iterative fashion.For example, to test a hypothesis such as "a model is able to identify graffiti-styled images," APEx first leverages textto-image retrieval and generation tools to create a tailored set of test images.It then employs a range of transformation tools to perform image augmentation, introducing variations that challenge the models' robustness.In an iterative experimental loop, the framework executes these experiments on a library of models, analyzes the results, and refines the testing protocol accordingly.</p>
<p>Complete Loop: Real-World Discovery Simulation</p>
<p>Other works equip LLM agents with interactive environments that more closely mirror the complexity of real-world hypothesis discovery by combining planning, acting, and evidence collection.For example, Xu et al. ( 2023) construct a Minecraft-like world in which a "vandal" agent performs up to 26 types of actions (e.g., moving, eating, crafting) to achieve a hidden goal (such as collecting lava or crafting a particular item) and leaves behind tracks as evidence.A detective agent-driven by reinforcement learning to maximize information gain-then gathers those tracks and presents them to an LLM, which must answer a multiplechoice question about the vandal's original objective.Because evidence collection relies on an RL policy rather than LLM planning, however, this setup evaluates only the model's capacity to interpret evidence, not its ability to proactively generate and test hypotheses in a dynamic setting.</p>
<p>Building on this approach, Wang et al. (2022a) introduce 30 scientific tasks drawn from five topics in fifthgrade curricula, ranging from measuring the friction coefficient of an inclined plane to testing electrical conductivity.Here, agents must execute long action sequences and apply deductive reasoning grounded in established theories and definitions to complete each task.Likewise, Jansen et al. (2024) design 120 experiments across eight subjects (e.g., Chemistry, Archaeology), each with three difficulty levels, and allow 14 coarse-grained actions (such as "take," "put," and "move").Agents are evaluated on (1) task completion, (2) execution of key experimental steps, and (3) accurate hypothesis discovery compared to a ground truth.While these virtual labs simulate multi-step procedures and test hypothesis application, their restricted action spaces support only qualitative inference and preclude the fine-grained interventions needed for quantitative rule-learning.</p>
<p>To address these limitations, He et al. (2024) propose puzzle environments in which agents can input arbitrary integers or letters and receive tailored feedback based on a hidden rule.In this framework, an LLM must iteratively probe the environment, uncover the underlying quantitative rule, and solve the puzzle.Performance is assessed not only by whether the agent solves the puzzle but also by human judgments of the clarity and rigor of its reasoning steps, thereby offering a finer-grained evaluation of both quantitative hypothesis generation and the quality of the model's deductive process.</p>
<p>Discussion and Future Directions in Hypothesis Discovery</p>
<p>Hypothesis discovery fundamentally differs from isolated reasoning tasks by requiring iterative learning and continuous refinement of hypotheses within dynamic, evolving contexts.Particularly in Real-World simulation scenarios, the decisions and actions taken by an LLM may lead to entirely different trajectories of observation collection, varied learning efficiencies, and alternative hypotheses.</p>
<p>Building effective benchmarks for hypothesis discovery requires constructing rich, realistic environments capable of simulating real-world complexities.These environments should contain diverse, comprehensive action spaces and varied observational feedback mechanisms.Compared to traditional static, label-based datasets, creating such benchmarks is significantly more labor-intensive, demanding at least two key components: 1, A set of rules unknown to the LLM that can be learned within the environment.2, A sufficiently expressive action space that allows the LLM to interact with the environment, receive feedback, and gather new information.</p>
<p>Given that current LLMs are trained on vast quantities of data, there is a risk of hypothesis leakage, where underlying rules might already be implicitly embedded in their parametric memory.For instance, benchmarks such as those introduced by Wang et al. (2022a) often rely on relatively straightforward tasks that do not genuinely necessitate novel hypothesis formation.Conversely, tasks proposed by He et al. (2024), despite aiming to encourage creative hypothesis formation, often yield simplistic, toy-like hypotheses with limited applicability to realistic scenarios.Therefore, future research should aim to develop environments with greater complexity and realism, fostering diverse and genuinely novel hypotheses.Benchmarks should be explicitly designed to push LLMs beyond their pretrained knowledge boundaries, and must provide practical tools for validating newly generated hypotheses.Such realistic simulation environments would address critical challenges such as hypothesis leakage and task oversimplification, ultimately fostering more robust and practical hypothesis discovery capabilities within LLMs.</p>
<p>Summary</p>
<p>In this survey, we have presented a comprehensive and structured framework for hypothesis discovery using LLMs, guided by Peirce's reasoning paradigm of abduction, deduction, and induction.Specifically, we systematically explored current methods and benchmarks across the three core components: hypothesis generation, hypothesis application, and hypothesis validation.</p>
<p>Our analysis identifies a significant gap between formal and natural language representations.While formal representations enable rigorous and objective evaluations, they often remain restricted to simplified, artificial scenarios lacking real-world complexity.Conversely, natural language representations effectively capture the nuanced complexities inherent in real-world reasoning tasks, yet suffer from a lack of reliable, rigorous evaluation metrics due to their inherently open-ended nature.</p>
<p>Existing methods, including prompt-based and fine-tuning approaches, demonstrate considerable potential but frequently isolate individual reasoning components.To move forward, we advocate for the development of integrated benchmarks and realistic, dynamic environments that more closely mimic real-world scientific inquiry and hypothesis discovery processes.Such benchmarks should provide rich intermediate Chain-of-Thought data, detailed commonsense reasoning steps, and comprehensive action spaces, thereby bridging the current divide between formal and informal reasoning representations.</p>
<p>Ultimately, establishing environments that demand proactive hypothesis generation, robust application to novel contexts, and rigorous validation against evolving evidence will be crucial.By addressing these challenges, future research will significantly advance the ability of LLMs to not merely execute instructions but to autonomously generate, refine, and validate hypotheses, thus realizing their potential as true engines of discovery and innovation.</p>
<p>Language Parser: Since formal symbolic solvers yield sound and correct predictions, Pan et al. (2023); Olausson et al. (2023); Kalyanpur et al. (</p>
<p>Figure 3 :
3
Figure 3: Differences and similarities among different types of hypothesis discovery tasks</p>
<p>Hypothesis Knowledge Base Derived Knowledge
Natural LanguageEnglish: 'Sam is a dragon"English commonsense of 'dragon"Sam is dangerousChinese: 'Sam is a dragon"Chinese commonsense of 'dragon"Sam brings good fortune and a bountiful harvestFormal LanguageFOL: Dragon(Sam)∀x(Dragon(x) → Fly(x)) . . .Fly(Sam)Class Dragon:Code: Sam = Dragon()def fly(self):</p>
<p>Qiu et al. (2024))))r model on FOL abduction tasks, demonstrating that the model can generate FOL hypotheses from formal observations.Similarly,Nguyen et al. (2023)fine-tuned state-of-the-art legal transformers on FOL abduction tasks and found that models pre-trained on natural language legal abduction tasks do not show any performance improvements on FOL hypothesis generation problems.When observations are represented in natural language, traditional symbolic solvers struggle to extract the key information needed for hypothesis generation.With LLMs, however, we can directly generate formal hypotheses.A popular formal language for this purpose is code, as it offers greater flexibility than other symbolic representations like FOL, and LLMs excel at coding.The simplest variant prompts an LLM with an observation set and asks it to produce executable functions as hypotheses that match the input-output pairs;Cheng et al. (2024)follow this pattern, treating each observation as an (x, y) example and evaluating the generated function by execution.Extending this idea,Wang et al. (2024);Qiu et al. (2024)have the LLM create multiple executable hypotheses, run them on the observations, feed the results back to the model, and iterate, discarding weak candidates and refining promising ones until one covers all examples.To encourage diversity, il Lee et al. (
Natural Language Observations:</p>
<p>Yang et al. (2024b)ora such asWIKIZhong et al., 2024) al., 2024;Zhong et al., 2024),Movva et al. (2025)treat hypothesis generation as identifying the key features that drive a prediction.The model proposes feature sets, which are judged by how well they match and cover the ground-truth features, thereby quantifying the LLM's ability to isolate causal signals.Despite these efforts,Yang et al. (2024b)note that reference-based metrics such as BLEU, ROUGE, and ME-TEOR assume a single correct answer and therefore struggle to capture the open-ended nature of hypothesis generation; developing fair, reliable metrics remains an open challenge.</p>
<p>Li et al. (2025a)2)enchmark based on deterministic regular functions, providing a procedural framework for evaluating formal hypotheses.Similarly,Young et al. (2022)used first-order logic (FOL) representations, where LLMs were tasked with generating FOL hypotheses to explain given facts, and the outputs were evaluated by comparing them against ground truth hypotheses verified by solvers.Since inference on formal hypotheses is deterministic, a common evaluation method is to test whether the generated hypothesis produces correct outcomes on held-out examples.For instance, Rule (2020) propose the list function task, where LLMs generate a hypothesis function from observed (x, y) pairs, and evaluation is based on how well the hypothesis predicts hidden pairs.Similarly,Chollet (2019)introduces the Abstract Reasoning Corpus (ARC), where tasks involve transforming input grids of colored cells into output grids.The generated function is executed on test inputs, and correctness is determined by exact matches with the target output grids, including grid dimensions.Liu et al. (2024)further propose a benchmark consisting of arithmetic calculations, color token mapping, and Kalamang vocabulary tasks, all evaluated in the same way.Additionally,Li et al. (2025a)construct diverse application
Prediction-based Evaluation:
Chen et al. (2024)ng list transformations, real-world problems, code generation, and string transformations, where the generated hypothesis is executed on both seen and test observations and the final score aggregates performance across both sets.In a more realistic setting,Chen et al. (2024)extract 102 tasks from 44 peer-reviewed publications, unifying the target output for every task into a self-contained Python program file, accompanied by a set of test cases validated by human experts.LLMs are then asked to read the paper and reproduce the tasks in code, and the generated code is directly evaluated on the prepared test cases.</p>
<p>Published in Transactions on Machine LearningResearch (04/2025)   </p>
<p>Kevin Schawinski, and Ioana Ciucă. A survey on hypothesis generation for scientific discovery in the era of large language models. Shashwat Atilla Kaan Alkan, Maja Sourav, Simone Jablonska, Rishabh Astarita, Nikhil Chakrabarty, Pranav Garuda, Maciej Khetarpal, Dimitrios Pióro, Tanoglidis, G Kartheik, Mugdha S Iyer, Michael J Polimera, Tirthankar Smith, Marc Ghosal, Sandor Huertas-Company, Kruk, 2025</p>
<p>Francis Bacon, Novum organum. Clarendon press1878</p>
<p>Artifacts or abduction: How do LLMs answer multiple-choice questions without the question?. Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger, 10.18653/v1/2024.acl-long.555Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAugust 20241Association for Computational Linguistics</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Jade Goldstein, Alon Lavie, Chin-Yew Lin, Clare Voss, the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, MichiganAssociation for Computational LinguisticsJune 2005</p>
<p>BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Adib Bazgir, Rama Chandra Praneeth, Yuwen Madugula, Zhang, Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025. 2023Agentichypothesis: A survey on hypothesis generation using LLM systems</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Yejin Wen Tau Yih, Choi, 2020</p>
<p>A comprehensive evaluation of inductive reasoning capabilities and problem solving in large language models. Chen Bowen, Rune Saetre, Yusuke Miyao, Findings of the Association for Computational Linguistics: EACL 2024. Yvette Graham, Matthew Purver, St. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden Mclean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Christopher Olah, 2023</p>
<p>Peirce's theory of abduction. Arthur W Burks, Philosophy of Science. 003182481341946</p>
<p>Chengkun Cai, Xu Zhao, Haoliang Liu, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Serge Belongie, Lei Li, The role of deductive and inductive reasoning in large language models. 2025</p>
<p>Exploring scientific hypothesis generation with mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, 10.18653/v1/2024.nlp4science-1.17Proceedings of the 1st Workshop on NLP for Science (NLP4Science). Lotem Peled-Cohen, Nitay Calderon, Shir Lissak, Roi Reichart, the 1st Workshop on NLP for Science (NLP4Science)Miami, FL, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Selfconsistent narrative prompts on abductive natural language inference. Chunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang Cheng, Yangqiu Song, Ginny Wong, Simon See, 10.18653/v1/2023.ijcnlp-main.67Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. Jong C Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, Adila Alfa Krisnadhi, the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational LinguisticsNovember 20231</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, 2024</p>
<p>. Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, 2024Bing Yin, and Yizhou SunInductive or deductive? rethinking the fundamental reasoning abilities of llms</p>
<p>On the measure of intelligence. François Chollet, 2019</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>Automatic benchmarking of large multimodal models via iterative experiment programming. Alessandro Conti, Enrico Fini, Paolo Rota, Yiming Wang, Massimiliano Mancini, Elisa Ricci, 2024</p>
<p>True detective: A deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4. Maksym Del, Mark Fishel, 10.18653/v1/2023.starsem-1.28Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (<em>SEM 2023). Alexis Palmer, Jose Camacho-Collados, the 12th Joint Conference on Lexical and Computational Semantics (</em>SEM 2023)Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Igor Douven, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2021Metaphysics Research Lab, Stanford UniversitySummer 2021 edition</p>
<p>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. Steffen Eger, Yong Cao, D' Jennifer, Andreas Souza, Christian Geiger, Stephanie Greisinger, Yufang Gross, Brigitte Hou, Anne Krenn, Yizhi Lauscher, Chenghua Li, Nafise Sadat Lin, Wei Moosavi, Tristan Zhao, Miller, 2025</p>
<p>Peirce's notion of abduction. G Harry, Frankfurt, The Journal of Philosophy. 0022362X55141958</p>
<p>Pär Anders Granhag and Aldert Vrij. Deception detection. Psychology and law: An empirical perspective. 2005</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, 2025</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alex Wardle-Solano, Hannah Szabo, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R Fabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, and Dragomir Radev. Folio: Natural language reasoning with first-order logic. 2024</p>
<p>Nonmonotonic logic and temporal projection. Steve Hanks, Drew Mcdermott, Artificial intelligence. 3331987</p>
<p>The inference to the best explanation. Gilbert H Harman, The Philosophical Review. 00318108, 155814707411965</p>
<p>Causejudger: Identifying the cause with llms for abductive logical reasoning. Jinwei He, Feng Lu, 2024</p>
<p>Idea: Enhancing the rule learning ability of large language model agent through induction, deduction, and abduction. Kaiyu He, Mian Zhang, Shuo Yan, Peilin Wu, Zhiyu Zoey, Chen , 2024</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, 2024</p>
<p>Inductionbench: Llms fail in the simplest complexity class. Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang, Wang , 2025</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaJuly 2023Association for Computational Linguistics</p>
<p>Generating diverse hypotheses for inductive reasoning. Hyukhun Kang Il Lee, Dongryeol Koh, Seunghyun Lee, Minsung Yoon, Kyomin Kim, Jung, 2025</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, Advances in Neural Information Processing Systems. 202437</p>
<p>Brainteaser: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati, 2023</p>
<p>StoryAnalogy: Deriving story-level analogies from large language models to unlock analogical understanding. Cheng Jiayang, Lin Qiu, Tsz Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang, 10.18653/v1/2023.emnlp-main.706Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, 10.18653/v1/2022.emnlp-main.82Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Llm-arc: Enhancing llms with an automated reasoning critic. Aditya Kalyanpur, Karthik Kailash, Victor Saravanakumar, Jennifer Barres, David Chu-Carroll, David Melville, Ferrucci, 2024</p>
<p>How far is video generation from world model: A physical law perspective. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng, arXiv:2411.023852024arXiv preprint</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, 10.18653/v1/2022.findings-emnlp.38Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Mirage: Evaluating and explaining inductive reasoning process in language models. Larry Laudan, ; Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao, 1971. 2025aWilliam whewell on the consilience of inductions. The Monist</p>
<p>Hypothesis testing prompting improves deductive reasoning in large language models. Yitian Li, Jidong Tian, Hao He, Yaohui Jin, 2024</p>
<p>Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu, From system 1 to system 2: A survey of reasoning large language models. 2025b</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>An incomplete loop: Instruction inference, instruction following, and in-context learning in language models. Emmy Liu, Graham Neubig, Jacob Andreas, 2024</p>
<p>Logical reasoning in large language models: A survey. Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, Yue Zhang, 2025</p>
<p>The upworthy research archive. Jorge Nathan, Matias , Kevin Munger, Marianne Aubin Le Quere, Charles R Ebersole, 487 experiments in U.S. media. Scientific Data. 322021</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. John Mccarthy, Patrick J Hayes, Readings in artificial intelligence. Elsevier1981</p>
<p>John Stuart, Mill , A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Principles of Evidence, and Methods of Scientific Investigation. BoD-Books on Demand2024I</p>
<p>Peirce-suit of truth -why inference to the best explanation and abduction ought not to be confused. Gerhard Minnameier, 10.1023/B:ERKE.0000005162.52052.7fErkenntnis. 6012004</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024arXiv preprint</p>
<p>Sparse autoencoders for hypothesis generation. Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson, 2025</p>
<p>Can llms follow simple rules?. Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner, 2024</p>
<p>How well do sota legal reasoning models support abductive reasoning?. Ha-Thanh Nguyen, Randy Goebel, Francesca Toni, Kostas Stathis, Ken Satoh, 2023</p>
<p>Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges. Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li, Yichao Lawrence Kq Yan, Caitlyn Zhang, Cheng Heqi Yin, Tianyang Fei, Yunze Wang, Silin Wang, Ming Chen, Liu, 2024</p>
<p>Inductive, abductive and deductive theorising. Chitu Okoli, International Journal of Management Concepts and Philosophy. 1632023</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>URL. </p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Pierre Isabelle, Eugene Charniak, Dekang Lin, the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsJuly 2002</p>
<p>Collected papers of charles sanders peirce. Charles Sanders, Peirce , 1974Harvard University Press5</p>
<p>TopicGPT: A promptbased topic modeling framework. Minh Chau, Alexander Pham, Simeng Hoyle, Philip Sun, Mohit Resnik, Iyyer, 10.18653/v1/2024.naacl-long.164Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Reasoning with large language models, a survey. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, 2024</p>
<p>The logic of scientific discovery. Karl Popper, 2005Routledge</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, Kevin Kj, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, 2024</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. K Chandan, Parshin Reddy, Shojaee, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>A logic for default reasoning. Raymond Reiter, Artificial intelligence. 131-21980</p>
<p>Thinking like a skeptic: Defeasible inference in natural language. Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Le Ronan, Noah A Bras, Yejin Smith, Choi, 10.18653/v1/2020.findings-emnlp.418Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational LinguisticsNovember 2020</p>
<p>The child as hacker: building more human-like models of learning. Joshua Stewart, Rule , 2020Massachusetts Institute of TechnologyPhD thesis</p>
<p>Evaluating the deductive competence of large language models. S Seals, Valerie Shalin, 10.18653/v1/2024.naacl-long.476Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Language models can improve event prediction by few-shot abductive reasoning. Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Zhang, Jun Zhou, Chenhao Tan, Hongyuan Mei, Advances in Neural Information Processing Systems. 202336</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>The road to experience and prediction from within: Hans reichenbach's scientific correspondence from berlin to istanbul. Friedrich Stadler, Synthese. 1812011</p>
<p>Beyond instruction following: Evaluating inferential rule following of large language models. Wangtao Sun, Chenxiang Zhang, Xueyou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu, 2024</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, Association for Computational LinguisticsAugust 2021</p>
<p>The effect of wording on message propagation: Topic-and authorcontrolled natural experiments on twitter. Chenhao Tan, Lillian Lee, Bo Pang, Proceedings of ACL. ACL2014</p>
<p>Llm assists hypothesis generation and testing for deliberative questions. Fuchun Wang, Xian Zhou, Wenpeng Hu, Zhunchen Luo, Wei Luo, Xiaoying Bai, Natural Language Processing and Chinese Computing. Derek F Wong, Zhongyu Wei, Muyun Yang, Singapore; SingaporeSpringer Nature2025</p>
<p>Hypothesis search: Inductive reasoning with language models. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, 2024</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022a</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, 2015</p>
<p>Reframing human-AI collaboration for generating free-text explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, 10.18653/v1/2022.naacl-main.47Proceedings of the 2022 Conference of the North American Chapter. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, 2024</p>
<p>Active reasoning in an open-world environment. Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu, Advances in Neural Information Processing Systems. 202336</p>
<p>Do phd-level llms truly grasp elementary addition? probing rule learning vs. memorization in large language models. Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan, arXiv:2504.052622025arXiv preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024a1</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, 10.18653/v1/2024.findings-acl.804Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024b</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, 2024c</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 2025</p>
<p>AbductionRules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: ACL 2022. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 10.1145/3664194ACM Comput. Surv. 0360-03005612October 2024a</p>
<p>Turtlebench: Evaluating top language models via real-world yes/no puzzles. Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, Zhiyu Li, 2024b</p>
<p>Beneath surface similarity: Large language models make reasonable scientific analogies after structure abduction. Siyu Yuan, Jiangjie Chen, Xuyang Ge, Yanghua Xiao, Deqing Yang, 10.18653/v1/2023.findings-emnlp.160Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Defeasible visual entailment: Benchmark, evaluator, and reward-driven optimization. Yue Zhang, Liqiang Jing, Vibhav Gogate, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Abductive commonsense reasoning exploiting mutually exclusive explanations. Wenting Zhao, Justin T Chiu, Claire Cardie, Alexander M Rush, 2023</p>
<p>UNcommonsense reasoning: Abductive reasoning about uncommon situations. Wenting Zhao, Justin Chiu, Jena Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang Li, Alane Suhr, 10.18653/v1/2024.naacl-long.469Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Explaining datasets in words: Statistical models with natural language parameters. Ruiqi Zhong, Heng Wang, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202437</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024</p>
<p>Large language models can learn rules. Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
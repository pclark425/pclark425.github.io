<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4635 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4635</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4635</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-266999418</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.07764v2.pdf" target="_blank">When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment</a></p>
                <p><strong>Paper Abstract:</strong> AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction, and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks, leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4635.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4635.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge Large Language Model Agent (global agent / "Accident Report Assistant")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A global LLM deployed on edge servers (implemented in the paper with GPT-4 as the global agent named "Accident Report Assistant") that performs long-horizon reasoning, retrieval-augmented generation, chain-of-thought planning, and reflection using historical interactions and external memory to ground decisions for mobile agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Edge LLM Agent (Accident Report Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A large global LLM running on edge servers (paper implementation: GPT-4) that aggregates local perceptions from multiple mobile agents, uses retrieval-augmented generation (RAG), chain-of-thought (CoT) and self-reflection to produce high-level plans and grounded outputs for downstream mobile agents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term memory / retrieval-augmented (external vector database)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Historical interactions from mobile agents are embedded via memory embedding layers and stored in external vector databases; the edge agent retrieves relevant items (RAG) during generation and uses them together with CoT reasoning and reflection to improve consistency and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Collaborative accident report generation / multistep reasoning and planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate multimodal local observations from multiple mobile agents about car crashes, perform multistep CoT reasoning and planning to generate comprehensive accident reports, and provide high-level plans for mobile agents to act (text responses, API calls, embodied actions).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Car Crash dataset (https://github.com/Cogito2012/CarCrashDataset) used for validating collaborative accident-reporting; perception validated on IN1K, K400, LLVIP, NYU-D, ESC-50, Ego4D datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper reports that edge agents using long-term memory (vector DB + RAG) and CoT improve consistency and grounding for complex, long-horizon tasks; caching popular models and leveraging accumulated 'thoughts' reduces edge inference loss. No numeric with/without-memory ablation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited context windows (models have fixed token windows), edge memory and storage capacity constraints (cannot load all models simultaneously), network costs for transmitting context between mobile and edge, compute costs of CoT and reflection, and privacy concerns when storing user interactions on edge servers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Use external vector databases and RAG at the edge to ground LLM outputs and ensure consistency across interactions; combine CoT and self-reflection for accurate reasoning; preserve and index historical interactions for retrieval; freshness of intermediate 'thoughts' matters (motivates AoT metric); tradeoffs exist between grounding performance and compute/network costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4635.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4635.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mobile LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mobile Local Large Language Model Agent (local LLaMA-7B based agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight LLM deployed on mobile devices (implemented with LLaMA-7B variants) that performs real-time perception, short-term in-context memory, immediate alignment, and can offload intermediate results to edge agents for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mobile LLM Agent (LLaMA-7B based)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tiny local LLMs (0–10B parameters; paper uses LLaMA-7B variants) running on mobile devices to handle real-time multimodal perception, short-term in-context reasoning and alignment, generation of local descriptions, and execution of localized actions or API calls; they offload complex planning to edge agents when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term memory / in-context memory (internal state); convertible to long-term via external storage</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains short-term in-context memory (recent dialogue, scene descriptions, internal states, and generated task plans). Short-term memory items can be uploaded/embedded and stored in edge vector databases to become long-term memory for the global agent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-time perception and local grounding for accident reporting; local multimodal perception</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Perceive multimodal sensory inputs (vision, audio, depth, IMU, infrared, video), generate local environmental descriptions, interact with users in real time, and execute local decisions or offload to edge for complex reasoning; perception modules evaluated on standard modality datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Perception evaluation datasets: IN1K (images), K400 (video), LLVIP (infrared), NYU-D (depth), ESC-50 (audio), Ego4D (IMU); downstream collaborative evaluation: Car Crash dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mobile agents rely on short-term in-context memory for immediate responsiveness; complex, long-horizon tasks require offloading to edge where long-term memory and retrieval improve outcomes. No quantitative ablation directly comparing with/without memory in mobile agents reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited compute, RAM, and context window on mobile devices constrain model size and the length of in-context memory; cannot perform extended CoT or maintain large histories locally; reliance on offloading increases bandwidth and latency demands.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Keep lightweight short-term memory for low-latency, on-device responsiveness and offload history to edge vector stores for long-term grounding; design for efficient serialization/embedding of short-term memory to reduce communication overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4635.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4635.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAoT caching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least Age-of-Thought (LAoT) Model Caching Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model caching algorithm that uses an Age-of-Thought (AoT) metric to score and evict cached global models whose generated intermediate 'thoughts' are least fresh/valuable, aiming to maximize in-context learning capabilities while reducing edge execution costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LAoT model caching (Least Age-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cache management policy for edge LLM servers that ranks loaded models by the freshness/value of their accumulated intermediate reasoning 'thoughts' (AoT) and evicts models with the least valuable/oldest thoughts to reduce switching and inference costs while preserving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>cache of model states and accumulated in-context 'thoughts' (memory-aware model caching)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores loaded global LLMs and their accumulated CoT intermediate outputs ('thoughts') in edge memory; AoT assigns lower value to older thoughts so the caching policy evicts models whose stored thoughts contribute least to future reasoning/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Model scheduling / caching for multi-service edge LLM deployment; improve reasoning/planning for collaborative agent services</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a set of services and request patterns (30 service types, 10 edge LLM agents in experiments), schedule which global models to keep loaded in limited edge memory to minimize total cost (accuracy loss, model switching cost, inference cost/latency, cloud fallback cost) while maximizing performance of edge reasoning/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Simulated edge deployment experiment described in paper (30 service types, 10 edge LLM agents, edge server with 64 GPUs); compared caching policies: LAoT vs cloud-only, FIFO, LFU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>LAoT outperformed baseline caching/policy strategies (cloud-only inference, FIFO, LFU) in the paper's experiments: it reduced total execution cost and improved edge inference accuracy by keeping popular/fresh models and thoughts loaded, though no numeric accuracy numbers or percentages were provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Edge memory constraints and context-window limits still cause evictions; CoT thought accumulation raises storage and compute needs; determining thought value/AoT requires bookkeeping and may add overhead; no numeric tradeoff or robust ablation of AoT hyperparameters reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Scoring cached models by the freshness/value of their intermediate reasoning ('thoughts') yields better caching decisions for multi-request edge LLMs than naive frequency or FIFO policies; thought freshness (AoT) is a useful signal to prioritize models for in-memory loading when serving long-horizon CoT workloads.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-based language models and applications <em>(Rating: 2)</em></li>
                <li>A survey on large language model based autonomous agents. <em>(Rating: 2)</em></li>
                <li>The rise and potential of large language model based agents: A survey. <em>(Rating: 2)</em></li>
                <li>A survey of chain of thought reasoning: Advances, frontiers and future. <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating large language models to master 16000+ real-world apis. <em>(Rating: 1)</em></li>
                <li>ImageBind: One embedding space to bind them all. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4635",
    "paper_id": "paper-266999418",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Edge LLM Agent",
            "name_full": "Edge Large Language Model Agent (global agent / \"Accident Report Assistant\")",
            "brief_description": "A global LLM deployed on edge servers (implemented in the paper with GPT-4 as the global agent named \"Accident Report Assistant\") that performs long-horizon reasoning, retrieval-augmented generation, chain-of-thought planning, and reflection using historical interactions and external memory to ground decisions for mobile agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Edge LLM Agent (Accident Report Assistant)",
            "agent_description": "A large global LLM running on edge servers (paper implementation: GPT-4) that aggregates local perceptions from multiple mobile agents, uses retrieval-augmented generation (RAG), chain-of-thought (CoT) and self-reflection to produce high-level plans and grounded outputs for downstream mobile agents.",
            "memory_type": "long-term memory / retrieval-augmented (external vector database)",
            "memory_description": "Historical interactions from mobile agents are embedded via memory embedding layers and stored in external vector databases; the edge agent retrieves relevant items (RAG) during generation and uses them together with CoT reasoning and reflection to improve consistency and grounding.",
            "task_name": "Collaborative accident report generation / multistep reasoning and planning",
            "task_description": "Aggregate multimodal local observations from multiple mobile agents about car crashes, perform multistep CoT reasoning and planning to generate comprehensive accident reports, and provide high-level plans for mobile agents to act (text responses, API calls, embodied actions).",
            "benchmark_name": "Car Crash dataset (https://github.com/Cogito2012/CarCrashDataset) used for validating collaborative accident-reporting; perception validated on IN1K, K400, LLVIP, NYU-D, ESC-50, Ego4D datasets",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper reports that edge agents using long-term memory (vector DB + RAG) and CoT improve consistency and grounding for complex, long-horizon tasks; caching popular models and leveraging accumulated 'thoughts' reduces edge inference loss. No numeric with/without-memory ablation reported.",
            "limitations_or_challenges": "Limited context windows (models have fixed token windows), edge memory and storage capacity constraints (cannot load all models simultaneously), network costs for transmitting context between mobile and edge, compute costs of CoT and reflection, and privacy concerns when storing user interactions on edge servers.",
            "key_insights": "Use external vector databases and RAG at the edge to ground LLM outputs and ensure consistency across interactions; combine CoT and self-reflection for accurate reasoning; preserve and index historical interactions for retrieval; freshness of intermediate 'thoughts' matters (motivates AoT metric); tradeoffs exist between grounding performance and compute/network costs.",
            "uuid": "e4635.0",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Mobile LLM Agent",
            "name_full": "Mobile Local Large Language Model Agent (local LLaMA-7B based agent)",
            "brief_description": "A lightweight LLM deployed on mobile devices (implemented with LLaMA-7B variants) that performs real-time perception, short-term in-context memory, immediate alignment, and can offload intermediate results to edge agents for complex tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Mobile LLM Agent (LLaMA-7B based)",
            "agent_description": "Tiny local LLMs (0–10B parameters; paper uses LLaMA-7B variants) running on mobile devices to handle real-time multimodal perception, short-term in-context reasoning and alignment, generation of local descriptions, and execution of localized actions or API calls; they offload complex planning to edge agents when needed.",
            "memory_type": "short-term memory / in-context memory (internal state); convertible to long-term via external storage",
            "memory_description": "Maintains short-term in-context memory (recent dialogue, scene descriptions, internal states, and generated task plans). Short-term memory items can be uploaded/embedded and stored in edge vector databases to become long-term memory for the global agent.",
            "task_name": "Real-time perception and local grounding for accident reporting; local multimodal perception",
            "task_description": "Perceive multimodal sensory inputs (vision, audio, depth, IMU, infrared, video), generate local environmental descriptions, interact with users in real time, and execute local decisions or offload to edge for complex reasoning; perception modules evaluated on standard modality datasets.",
            "benchmark_name": "Perception evaluation datasets: IN1K (images), K400 (video), LLVIP (infrared), NYU-D (depth), ESC-50 (audio), Ego4D (IMU); downstream collaborative evaluation: Car Crash dataset",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mobile agents rely on short-term in-context memory for immediate responsiveness; complex, long-horizon tasks require offloading to edge where long-term memory and retrieval improve outcomes. No quantitative ablation directly comparing with/without memory in mobile agents reported.",
            "limitations_or_challenges": "Limited compute, RAM, and context window on mobile devices constrain model size and the length of in-context memory; cannot perform extended CoT or maintain large histories locally; reliance on offloading increases bandwidth and latency demands.",
            "key_insights": "Keep lightweight short-term memory for low-latency, on-device responsiveness and offload history to edge vector stores for long-term grounding; design for efficient serialization/embedding of short-term memory to reduce communication overhead.",
            "uuid": "e4635.1",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LAoT caching",
            "name_full": "Least Age-of-Thought (LAoT) Model Caching Algorithm",
            "brief_description": "A model caching algorithm that uses an Age-of-Thought (AoT) metric to score and evict cached global models whose generated intermediate 'thoughts' are least fresh/valuable, aiming to maximize in-context learning capabilities while reducing edge execution costs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LAoT model caching (Least Age-of-Thought)",
            "agent_description": "Cache management policy for edge LLM servers that ranks loaded models by the freshness/value of their accumulated intermediate reasoning 'thoughts' (AoT) and evicts models with the least valuable/oldest thoughts to reduce switching and inference costs while preserving accuracy.",
            "memory_type": "cache of model states and accumulated in-context 'thoughts' (memory-aware model caching)",
            "memory_description": "Stores loaded global LLMs and their accumulated CoT intermediate outputs ('thoughts') in edge memory; AoT assigns lower value to older thoughts so the caching policy evicts models whose stored thoughts contribute least to future reasoning/planning.",
            "task_name": "Model scheduling / caching for multi-service edge LLM deployment; improve reasoning/planning for collaborative agent services",
            "task_description": "Given a set of services and request patterns (30 service types, 10 edge LLM agents in experiments), schedule which global models to keep loaded in limited edge memory to minimize total cost (accuracy loss, model switching cost, inference cost/latency, cloud fallback cost) while maximizing performance of edge reasoning/planning.",
            "benchmark_name": "Simulated edge deployment experiment described in paper (30 service types, 10 edge LLM agents, edge server with 64 GPUs); compared caching policies: LAoT vs cloud-only, FIFO, LFU",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "LAoT outperformed baseline caching/policy strategies (cloud-only inference, FIFO, LFU) in the paper's experiments: it reduced total execution cost and improved edge inference accuracy by keeping popular/fresh models and thoughts loaded, though no numeric accuracy numbers or percentages were provided in text.",
            "limitations_or_challenges": "Edge memory constraints and context-window limits still cause evictions; CoT thought accumulation raises storage and compute needs; determining thought value/AoT requires bookkeeping and may add overhead; no numeric tradeoff or robust ablation of AoT hyperparameters reported.",
            "key_insights": "Scoring cached models by the freshness/value of their intermediate reasoning ('thoughts') yields better caching decisions for multi-request edge LLMs than naive frequency or FIFO policies; thought freshness (AoT) is a useful signal to prioritize models for in-memory loading when serving long-horizon CoT workloads.",
            "uuid": "e4635.2",
            "source_info": {
                "paper_title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-based language models and applications",
            "rating": 2,
            "sanitized_title": "retrievalbased_language_models_and_applications"
        },
        {
            "paper_title": "A survey on large language model based autonomous agents.",
            "rating": 2,
            "sanitized_title": "a_survey_on_large_language_model_based_autonomous_agents"
        },
        {
            "paper_title": "The rise and potential of large language model based agents: A survey.",
            "rating": 2,
            "sanitized_title": "the_rise_and_potential_of_large_language_model_based_agents_a_survey"
        },
        {
            "paper_title": "A survey of chain of thought reasoning: Advances, frontiers and future.",
            "rating": 2,
            "sanitized_title": "a_survey_of_chain_of_thought_reasoning_advances_frontiers_and_future"
        },
        {
            "paper_title": "ToolLLM: Facilitating large language models to master 16000+ real-world apis.",
            "rating": 1,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "ImageBind: One embedding space to bind them all.",
            "rating": 1,
            "sanitized_title": "imagebind_one_embedding_space_to_bind_them_all"
        }
    ],
    "cost": 0.00960925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment
16 Feb 2024</p>
<p>Minrui Xu minrui001@e.ntu.edu.sg 
Fellow, IEEEDusit Niyato dniyato@ntu.edu.sg 
Jiawen Kang 
Zehui Xiong zehuixiong@sutd.edu.sg 
Fellow, IEEEShiwen Mao smao@ieee.org 
Fellow, IEEEZhu Han zhuhan22@gmail.com 
Fellow, IEEEDong In Kim dikim@skku.ac.kr 
Fellow, IEEEKhaled B Letaief 
In Dong 
Kim </p>
<p>School of Computer Science and Engineering
Nanyang Technological University
639798SingaporeSingapore</p>
<p>School of Automation
Guangdong University of Technology
China</p>
<p>Pillar of Information Systems Technology and Design
University of Technology
Singapore</p>
<p>Department of Electrical and Computer Engineering
Auburn University
36849-5201AuburnALUSA</p>
<p>Department of Electrical and Computer Engineering
University of Houston
77004HoustonTXUSA</p>
<p>Department of Computer Science and Engineering
Kyung Hee University
446-701SeoulSouth Korea</p>
<p>Department of Electrical and Computer Engineering
Sungkyunkwan University
16419SuwonSouth Korea</p>
<p>Department of Electrical and Computer Engineering
Hong Kong University of Science and Technology (HKUST)
Hong Kong</p>
<p>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment
16 Feb 2024A9BA9EA5F22F30FF5A354B94F137402FarXiv:2401.07764v2[cs.AI]6G networksAI agentsintegrated sensing and communicationdigital twinstask-oriented communications
AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment.Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions.In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively.In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating intermodule communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications.Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.</p>
<p>I. INTRODUCTION</p>
<p>AI agents, designed to integrate AI models into everyday services as personal assistants to humans, have become a pivotal element in advancing towards artificial general intelligence (AGI) [1], [2].AI agents powered by large language models (LLMs), i.e., LLM agents, possess the capability to follow user instructions, observe environments, make decisions, and execute actions at a human-equivalent level.Therefore, LLM agents can proactively provide users with recommendations for final decisions by understanding and remembering cross-application user intentions and behaviors.Particularly, AI agents observe surrounding environments by processing information in various modalities from sensors, leveraging the versatility of multimodal LLMs [3].In addition, LLM agents can solve complex tasks by grounding the plan of action to achieve their missions through reasoning, memory, and verification.After the alignment between LLM agents and humans, agents can attain human-like intelligence to provide recommendations to users with text, tools, and embodied actions that are consistent with human values.</p>
<p>Although the deployment of LLM agents on mobile devices in 6G networks allows democratizing access to services currently considered prohibitively expensive at cloud data centers, several issues remain in implementing the LLM agents for complex, multi-round interaction agent services [4], [5].For mobile devices with limited capacities, running AI models of agents, which is both computation-and memory-intensive, is challenging for supporting the long-term execution of LLMs.In addition, these limitations are further exacerbated by the restricted context windows of LLMs, hindering LLM agents from performing long-term and complex interactions, such as perception, reasoning, and coding, which consume considerable available context resources [6].To address these challenges, a split learning system based on collaborative endedge-cloud computing, which aims at partitioning LLM agents into mobile and edge agents, emerges as a viable solution.In this system, mobile LLM agents, operating local LLMs (0-10B parameters, e.g., LLAMA-7B) on mobile devices, can handle real-time, direct perception and alignment tasks.Meanwhile, edge LLM agents, hosting global LLMs, (&gt;10B parameters, e.g., GPT-3) on edge servers, can utilize global information and historical memory to help mobile LLM agents perform complex tasks.</p>
<p>There are several advantages to partitioning LLM agents into mobile and edge agents in 6G networks.First, flexible deployment of LLM agents can be supported by heterogeneous devices with different locations, capabilities, and contextual adaptability.Specifically, mobile LLM agents with proper local LLMs can operate effectively with their computing capabilities regardless of their locations and user scenarios.Second, long-horizon collaboration can be enabled across multiple mobile devices by bridging the integration between lowlevel operational plans of local LLMs and high-level strategic plans of global LLMs.Third, mobile LLM agents exhibit enhanced adaptability in dynamic open-ended environments.</p>
<p>For instance, mobile LLM agents can understand instructions using local LLMs and then adjust their actions based on immediate environmental feedback for real-time responsiveness and relevance during their interactions with physical environments.</p>
<p>In this article, we propose a split learning system of LLM agents consisting of mobile LLM agents and edge LLM agents in 6G networks, which is democratic, flexible, and longhorizon for running sustainable AI agents in open-ended environments.First, we introduce the basic concept of AI agents and introduce the processes of constructing LLM agents via collaborative end-edge-cloud computing.Secondly, we discuss three main issues in developing LLM agents in 6G networks, including multimodal perception, interactive grounding, and alignment with humans.Thirdly, we investigate a real-world application that leverages mobile and edge LLM agents to generate accident reports collaboratively.At an accident site, vehicles can employ mobile LLM agents to observe the surrounding scene of a car accident and generate their local environmental descriptions.By sending these descriptions to edge servers, edge LLM agents can use global observations to deduce and offer more detailed and precise plans for vehicles.Finally, the mobile LLM agents can generate text responses, functional call requests, and embodied actions based on the global plan.In addition, we propose a metric called age of thought (AoT) to assess the significance of thoughts, i.e., the intermediate steps generated by LLMs, during the reasoning and planning processes of edge LLM agents.This metric emphasizes that older thoughts hold less importance and thus can ensure the high performance of cached models.Based on this metric, we introduce the Least Age-of-Thought (LAoT) model caching algorithm, which evicts global models that have the least impactful and relevant thoughts, and thus reduces the grounding cost in terms of latency, resource consumption, and performance loss for serving edge LLM agents in 6G networks.Overall, our main contribution can be summarized as follows.</p>
<p>• We propose a split learning system for LLM agents in 6G networks, which aims to provide democratic AI assistant services via the collaboration of mobile and edge LLM agents over end-edge-cloud computing.• During the integration of 6G networks and LLM agents, we discuss several major issues, including integrated sensing and communication for multimodal perception, digital twins for grounding decisions, and task-oriented communications for the alignment of agents.• We propose a new optimization framework in the system, i.e., model caching for AI agents, which aims at maximizing the in-context learning capabilities of LLM agents while reducing the network costs of serving mobile and edge LLM agents.</p>
<p>II. COLLABORATIVE END-EDGE-CLOUD COMPUTING FOR LLM AGENTS IN 6G NETWORKS</p>
<p>As a pivotal stride towards achieving AGI, AI agents are the key computational entities that can proactively perceive user instructions, observe the environment, ground decisions, and perform human-like actions [2].In 6G networks, AI agents are developed to execute intricate tasks collaboratively, from managing networks to acting as personal assistants for humans.According to the difference in fundamental working mechanisms, there are two major categories of AI agents, i.e., reinforcement learning (RL) agents and LLM agents, which are discussed below.</p>
<p>A. Categories of AI Agents 1) RL Agents: Utilizing RL algorithms to observe states, make decisions, and take actions in an environment, RL agents learn through trial and error, by receiving feedback as rewards or penalties as a result of their actions.They aim to maximize their cumulative reward over time by learning optimal policies.For example, in communications and networking, RL agents can make decisions for dynamic network access, transmit power control, wireless caching, and data offloading locally to maximize network performance under uncertain network environments.Specifically, RL agents formulate the communication and networking environment into a Markov decision process (MDP) consisting of states, actions, transition probabilities, and rewards.However, although RL agents learn to make decisions for network access and management [2], they cannot interact with humans and other agents using texts in open-ended environments which limits their potential to offer more diverse services that require understanding and responding to human instructions.</p>
<p>2) LLM Agents: To achieve human-level intelligence, LLM agents build upon versatile and powerful LLMs that have demonstrated remarkable capabilities in few-shot and zeroshot environment perception and instruction understanding [1], [2].In addition to the decision-making capabilities of RL agents, LLM agents can interact with the environment through texts, API tools, and embodied actions continuously while gradually improving their performance during the interaction.Meanwhile, pre-training on large-scale datasets elicits emerging abilities of LLMs, allowing them to tackle various downstream tasks related to data management, question answering, route planning, and scientific inquiries.Furthermore, equipped with memory, reasoning, planning, and tool capabilities, LLM agents can not only make decisions for network environments but also leverage language understanding and employ tools such as the Internet and databases for tackling complex control tasks.Compared with the generalization of RL agents, the role-playing capability of LLM agents allows them to serve specific roles while handling different tasks.For example, LLM agents can act as experiment assistants, automating the design, planning, and execution of scientific experiments based on human-crafted instructions.However, textual instructions are usually not sufficient for LLM agents to perceive the entire environment in a realistic setting.</p>
<p>To enhance LLMs with multi-sensory capabilities, such as visual and audio understanding, multimodal LLMs [3], like GPT-4V(ision), are introduced for agents to perceive and process inputs from multiple modalities, including tactile feedback, gestures, Inertial Measurement Units (IMUs) motion sensor data, and 3D maps.For visual input, multimodal LLMs can be leveraged to generate a description for the current environment, where they can produce multimodal descriptions, such as text, audio, and images, which enable better accessibility for visually impaired individuals and improve positioning capabilities.Specifically, multimodal LLM agents can use a pre-trained encoder to convert signals from different modalities into a common textual representation, allowing for reasoning across modalities [7].</p>
<p>B. Construction of Mobile Edge-empowered Agents</p>
<p>As illustrated in Fig. 1, the construction of LLM agents in collaborative end-edge-cloud computing consists of three main processes, namely, mobile LLM agent execution, edge LLM agent execution, and inter-agent communication between mobile agents and edge agents to update information and assign tasks.</p>
<p>1) Mobile LLM Agent Execution: Initially, each user downloads tiny local LLMs (0-10B), e.g., LLAMA-7B, to its mobile device from edge servers via radio access networks (RANs) for personalized initialization.During initialization, users can configure mobile LLM agents with personal profiles such as age, gender, and career, which agents use to tailor their interactions and responses with specific roles.In addition, mobile LLM agents can leverage contextual initialization based on the current situation by processing and analyzing historical interactions.There are two major methods for LLM agents to perceive environments, i.e., human instruction and sensing.On the one hand, human instructions are given through interactive dialogues between humans and LLM agents.On the other hand, LLM agents can perceive the physical environment, which provides multimodal sensory inputs from interacting objects, including visual, auditory, and spatial data.</p>
<p>To process the received instruction and multimodal sensing data, mobile LLM agents can utilize pre-trained components, such as modality encoders, word embedding layers, and projection layers, to combine multi-sensory inputs.Each modality encoder is specific to one modality, such as CLIP for images, CLAP for audio signals, IMU2CLIP for IMU motion sensor, and Intervideo for videos [8].In mobile devices, multiple encoders process and combine the multimodal input data and then project the output into the text token embedding space of local LLMs.To process human instructions, the word embedding layer is a crucial component that maps words or tokens into a continuous vector space, capturing semantic relationships between them, and helping in understanding userspecific instructions.In mobile AI agents, due to limited capacities in mobile devices, tiny local LLMs with a limited amount of parameters can generate real-time responses based on local perception but cannot tackle complex tasks that require comprehensive consideration and generalization.</p>
<p>2) Global Agent Execution: In edge servers, edge LLM agents with huge global LLMs (&gt;10B), e.g., GPT3, can leverage long-term memory, reasoning, and planning modules to enhance the quality of responses with global information and understanding of environments.Historical interactions of mobile LLM agents can be stored as long-term memory in vector databases through memory embedding layers.Based on the long-term memory from mobile LLM agents, edge LLM agents can use retrieval-augmented generation (RAG) to output responses with better performance and consistency [9].In addition, edge LLM agents can utilize chain-of-thought (CoT) reasoning to improve the performance in complex tasks [10].When tackling complex tasks, edge LLM agents using CoT start by employing various reasoning paths to deduce potential answers, considering that each complex problem has multiple ways of thinking.This way, edge LLM agents can adapt to unfamiliar scenarios through knowledge generalization and transfer abilities inherent in global LLMs.Furthermore, edge LLM agents can leverage self-reflection to verify reasoning paths, gaining more accurate results of their actions and making better decisions for future behaviors.</p>
<p>C. Inter-agent Communication between Local and Edge Agents</p>
<p>When mobile LLM agents are incapable of accomplishing complex tasks, they can offload the intermediate results, including local perceptions and user intentions, to edge LLM agents equipped with huge global LLMs and global information for remote execution.Mobile LLM agents can transmit intermediate results, such as text or other embeddings, through inter-agent communication over RANs.Due to limited bandwidth and uncertain wireless channels, mobile LLM agents need to optimize the size of the transmitted content, i.e., intermediate inference results of local LLMs, and configure communication parameters for successful offloading, e.g., the transmit power and the chosen channel.Based on the responses and decision results generated by edge LLM agents, mobile LLM agents adapt global general plans to the local specific plans to interact with users and the environments.After understanding the locally specific plans using local tiny LLMs, mobile LLM agents generate responses, use API tools, and perform embodied actions locally using their actuation modules.</p>
<p>III. ISAC FOR WIRELESS PERCEPTION: UBIQUITOUS AND ADAPTABILITY</p>
<p>To run LLM agents efficiently in 6G networks with ubiquitous low-end devices, mobile LLM agents can perceive user instructions and sense environments for modeling and understanding the current situation.In addition, to improve adaptability and generalization, mobile LLM agents need to offload computation-intensive and intractable tasks to edge LLM agents for remote execution.Therefore, mobile LLM agents need to collect and extract information from noisy observations and communicate with edge servers to transfer information, which requires the implementation of integrated sensing and communication (ISAC) by utilizing the wireless communication infrastructure.</p>
<p>1) Environmental Perception: In multi-functional 6G networks, mobile LLM agents can autonomously perceive the surrounding environment using equipped sensors [7], which consume network resources for supporting the sensing functionality.By integrating basic perceptual abilities such as vision, text, and light sensitivity, LLM agents can develop various user-friendly perception modules [11].For example, LLM agents in mobile devices can perceive more complex user inputs, such as eye-tracking, body motion capture, and even brainwave signals in brain-computer interaction.Furthermore, LLM agents in vehicular networks can be equipped with Lidar, GPS, and IMUs, allowing them to perceive location-based data for vehicles and mobile users.</p>
<p>2) Human-language Instruction: During the interaction between users and agents, text instructions can be given to mobile LLM agents by providing them with explicit requests as well as implied values and intentions.Mobile LLM agents can understand implicit meanings within textual input based on contextual interaction with users, thanks to their short-term memory.After processing through local LLMs, mobile LLM agents can respond with answers in human language.Additionally, users can also provide instructions via audio, which contains environmental information compared to text [7].Handling audio input involves leveraging existing models, cascading paradigms, and integrating audio with other modalities to enhance agents' perception and understanding of the environment.</p>
<p>3) Inter-agent Interactions: In the proposed system, ubiquitous interaction between mobile and edge LLM agents is crucial for offloading intermediate results, receiving feedback, interactive reasoning, and self-reflection over RANs [6].During collaboration between mobile and edge agents, they need to continuously communicate with each other with messages in text or other embedded formats in a noisy environment.Therefore, this communication process usually consumes a large amount of bandwidth and network resources for longterm and multimodal interactions.</p>
<p>For the wireless perception of LLM agents in multifunctional 6G networks, ISAC is promising to improve spectral and energy efficiencies for mobile LLM agents to collect information from environments and transmit intermediate results to edge LLM agents simultaneously.For example, mobile LLM agents in vehicles need to perform radar sensing and transmit the perception results to edge LLM agents simultaneously.By utilizing network resources more efficiently for sensing and communication, ubiquitous LLM agents can be deployed in wireless environments and become more adaptable to a dynamic and open-ended world.</p>
<p>IV. DIGITAL TWINS FOR WIRELESS GROUNDING: RELIABILITY AND CONSISTENCY</p>
<p>For grounding the responses and actions, mobile LLM agents maintain digital twins (DTs) at the edge servers to interactively perform retrieval-augmented generation (RAG), reasoning and planning, and reflection with edge LLM agents in 6G networks with hyper reliable and low-latency communication.DTs of mobile LLM agents are created as digital replicas of physical entities with perceived data and can help to perform global grounding with internal memory and external knowledge.Through continuously updating external observation and internal reasoning results, DTs of mobile LLM agents can be created for real-time monitoring, analysis, and optimization of the decisions of mobile agents while performing complex tasks.</p>
<p>1) Memory and Retrieval-Augmented Generation: In the proposed system, mobile LLM agents maintain a short-term memory while global agents maintain a long-term memory for grounding agents' responses and actions.In mobile LLM agents, the short-term memory is collected through various mechanisms, such as in-context learning, maintaining internal states, utilizing scene descriptions or environment feedback, and generating task plans.Additionally, short-term memory in mobile LLM agents can be converted to long-term memory by leveraging external storage resources in edge servers, such as vector databases, that allow rapid querying and retrieval of information as needed.Based on the long-term memory, edge LLM agents can perform RAG [9] to improve consistency in generation by using a retrieval model to retrieve relevant information from a knowledge base or a set of reference documents and then incorporating this retrieved information into the generation process.In this way, by processing retrieved content using global LLMs, RAG can be leveraged in complex and long-horizon tasks using specialized knowledge, up-to-date information, and customizable definitions for better performance and consistency.Specifically, edge LLM agents can access past responses of mobile LLM agents to ensure consistent collaboration.</p>
<p>2) Reasoning and Planning: Edge LLM agents can tackle complex tasks by decomposing them into sequential steps and sub-tasks to output accurate responses.To perform intricate reasoning, CoT [10] involving a step-by-step reasoning process along a single path can improve the reliability and interpretability of LLMs decisions.Specifically, edge LLM agents have the ability to use CoT to break down complex tasks and offer step-by-step instructions for mobile LLM agents to complete individual tasks.In addition, self-consistent CoT (CoT-SC) is proposed to improve performance of reasoning tasks by aggregating multiple language model outputs and selecting the most consistent answer through a majority vote.To extend CoT, tree-of-thoughts (ToT), a proposed extension of CoT that formulates thought units into a tree structure, allows LLMs to explore coherent thought units as intermediate steps, enabling better problem-solving and planning capabilities.Moreover, graph-of-thoughts (GoT) is a static structure that specifies the graph decomposition of a given task in the CoT paradigm.It prescribes the transformations to be applied to language model thoughts, along with their order and dependencies.Although these step-by-step reasoning and planning mechanisms allow for multiple choices at each step and mimic human thinking, they might request more computing resources from edge servers to generate the intermediate results compared with outputting the results directly.</p>
<p>3) Verification and Reflection: To ensure the correctness of the reasoning process before final response generation, LLM agents can leverage verification reflection to validate the correctness of each step in the CoT process.For example, SelfCheck [2] is a zero-shot checking scheme for LLMs that aims to improve question-answering accuracy by identifying errors in the LLM's reasoning process.It works as a step-bystep checker, individually checking each step in the LLM's reasoning process based on the available context.Using confidence scores as weights, SelfCheck allows for improved question-answering accuracy by focusing on the most accurate answer.Therefore, LLM agents can independently summarize and infer more abstract, complex, and high-level information.</p>
<p>Therefore, there should be a tradeoff between the performance of grounding modules and the computing resources consumed during the grounding processes.For inter-agent grounding, mobile and edge LLM agents can leverage their own self-correction capabilities to improve the accuracy of final decisions leveraging computing resources in mobile devices and edge servers.Moreover, inter-agent grounding requires additional networking resources to transmit correction results between mobile and edge LLM agents for crossverification for grounding the actions of LLM agents.</p>
<p>V. TASK-ORIENTED COMMUNICATIONS FOR WIRELESS ALIGNMENT: TRUSTWORTHY AND GENERALIZABILITY</p>
<p>In 6G networks with limited bandwidth resources, taskoriented communications [12] refer to a communication approach where the performance is measured based on the success level of achieving a sequence of application-related tasks, rather than traditional metrics such as data rate or wireless link reliability.The alignment of LLM agents in 6G networks offering ubiquitous connectivity can be regarded as a type of task-oriented communication where LLM agents can leverage the massive resources of mobile devices and edge servers to achieve their alignment goals.For instance, mobile LLM agents are the data destinations of global general plans from edge LLM agents to generate texts, call API functions, and perform embodied actions.In addition, feedback from humans and mobile LLM agents can be collected as datasets for supervised fine-tuning, reinforcement learning from human feedback (RLHF), and direct preference optimization (DPO) to regularize global LLMs.Beyond data-oriented communications, the accomplishment of alignment between mobile and edge LLM agents and humans in view of semantics is directly linked to task-oriented communication performance and strategies that wireless users and mobile and edge LLM agents can provide real-time evaluation and feedback.</p>
<p>1) Text Responses: Since LLMs are pre-trained on largescale datasets with biased data, a mismatch or distribution shift between the training and test data can cause LLMs to generate incorrect information, known as hallucination [13].However, to ensure that mobile and edge LLM agents align with human intentions and preferences, the system needs to reduce the likelihood of generating harmful outputs and improve usability by better following human instructions.For instance, Ope-nAI (https://openai.com/blog/introducing-superalignment), the creator of ChatGPT, has announced that they are going to leverage 20% of computing resources to fine-tune strong pretrained LLMs for regularizing LLMs to faithfully follow instructions or generate safe outputs.Fortunately, wireless alignment enables massive users to contribute their efforts and computing resources in alignment activities and contribute their efforts towards unlocking the full potential of LLMs while following human value and intentions, positively impacting various domains and enriching human experiences.</p>
<p>2) Tool Usage and Generation: By instruction fine-tuning on API datasets, mobile LLM agents should become proficient in leveraging tools and APIs to accomplish intricate tasks and interact with different virtual applications effectively based on general plans from edge LLM agents [14].Therefore, depending on specific environments and agent types, mobile LLM agents can be customized using local instruction Response: In the video, we see a car driving down the road next to a tall building in the city.The road is wide, and there are many trees and tall buildings visible in the background.The car appears to be white and it's driving slowly.There are no other vehicles on the road, and there are no pedestrians or bicycles in sight.The view of the building changes as the car moves down the road ...</p>
<p>Prompt :</p>
<p>Based on [description of the environment], how to generate an accident report for this car crash?Let's think step by step.Response: As a safety professional, I understand that you want me to help you generate an accident report based on the scenario provided.</p>
<p>Here's a step-by-step guide on how to create an accident report for the hypothetical car crash:</p>
<p>Step 1: Gather Information: ...</p>
<p>Step 2: Identify Parties Involved: ...</p>
<p>Step 3: Description of Accident: ...</p>
<p>Step 4: Investigation Findings: ...</p>
<p>Step 5: Conclusion &amp; Recommendations: ...</p>
<p>User: I want to get traffic incidents news in real-time reported by department of transportation.</p>
<p>Assistant: I will call the function 'Traffic_Incident_Report' next step."function_call": {"name": "Traffic_Incident_Report", "arguments": "{country, region}", Observation: {"report": {"type": "FeatureCollection", "features": [{"properties": {"location_description": "I-95N at MM 118.2", "mrm": "118.2","route": "I-95N", "priority": "Major", "icon": "incident"}, "geometry": {"coordinates": ["-77.513872","38.126552"], "type": "Point" }, "type": "Feature", "id": "INNO7328665401-11272023"}]} Prompt: Based on the guideline and [description of the environment], how to generate an accident report for this car crash?Let's think step by step.Response: Based on the information from the video and following the guideline for generating a car accident report, we can proceed step by step.However, it's important to note that the description provided does not mention a specific car crash.We'll create a hypothetical scenario based on the video description:</p>
<p>Step 1: Details of the Accident: ...</p>
<p>Step 2: Time, Date, and Location: ...</p>
<p>Step 3: Weather and Roadway Conditions: ...</p>
<p>Step 4: Driver's and Witness Statements: ...</p>
<p>Step 5: Accident Diagram: ...</p>
<p>Step 6: Violations and Citations: ...</p>
<p>Step 7: Officer's Findings and Conclusions: ... Since the description does not include a specific accident, the report is based on assumptions and hypothetical details.In a real situation, all the specific and factual details available at the scene would need to be accurately and thoroughly documented.</p>
<p>Global reasoning &amp; planing</p>
<p>Memory: Guideline_Car_Accident_Police_Report.pdf Prompt: Based on the knowledge, what is the guideline to generate an accident report for a car crash?Response: To generate an accident report for a car crash based on the guidelines provided in the document, you should include the following key elements: Details of the Accident: ..... Time, Date, and Location: ... Statements: ... Accident Diagram: ... Violations and Citations: ... Officer's Findings: ... Remember, police reports can contain both facts (like vehicle make and model, accident location and time, weather conditions) and opinions (such as a determination of fault, which is the opinion of the police officer) ...</p>
<p>Offload local reasoning results</p>
<p>Return global reasoning and planning results</p>
<p>Perception results</p>
<p>Reasoning and planning results</p>
<p>Mobile LLM agents</p>
<p>Edge LLM agents</p>
<p>Mobile LLM agents</p>
<p>Mobile</p>
<p>LLM agents</p>
<p>Fig. 2: An example of mobile and edge LLM agents used in the generation of accident reports for car crashes.fine-tuning datasets, which encompass real-world APIs and practical scenarios, to accomplish both single-tool and multitool tasks.Furthermore, mobile LLM agents can showcase exceptional adaptability when faced with unfamiliar APIs and tool-use datasets that are outside their usual field, particularly when users are inexperienced in the given environments.</p>
<p>3) Embodied Actions: To effectively interact with humans and physical environments, mobile LLM agents can perform embodied actions, including movements, gestures, or other physical behaviors [2] to interact with the physical world directly under the high-level plans from edge LLM agents.Embodied actions of mobile LLM agents are physically performed according to their design, mechanical features, and technology.For instance, vehicles can perform basic mechanical movements, such as driving forward and backward, tuning, braking, and accelerating.By performing these embodied actions, vehicles can easily adapt to road conditions, e.g., bumpy roads, slippery surfaces, and bad weather by adjusting internal temperature and air quality.To navigate through unfamiliar environments, robots with mobile LLM agents need to gather information, carry out tasks, and interact with other agents like humans.By performing embodied actions, mobile and edge LLM agents can extend their capabilities beyond digital boundaries, and interact and manipulate their physical surroundings directly.</p>
<p>VI. CASE STUDY OF MODEL CACHING FOR COLLABORATIVE MOBILE AND EDGE LLM AGENTS</p>
<p>In the split learning system of LLM agents over collaborative end-edge-cloud computing, each mobile AI agent is composed of a perception module, a local reasoning module, and an alignment module while each edge LLM agent consists of a global reasoning and planning module.In addition to allocating traditional computing, communication, and storage resources for executing LLM agents, the LLMs running in these agents are new resources to be allocated for performing contextual tasks of AI agents.Specifically, mobile LLM agents can leverage local LLMs for zero-shot environmental perception and auction, which is more comprehensive.Meanwhile, edge LLM agents with global LLMs can perform more intricate step-by-step reasoning and planning with global information for reliable and interpretative decision-making.</p>
<p>To construct the perception module of mobile LLM agents that can collect multimodal information from the environment, we leverage the ImageBind [8] and the LanguageBind [15] for sensing the environment.The ImageBind, based on ViT-Huge, and the LanguageBind, based on ViT-Large, use image embeddings as a central anchor to align embeddings from other modalities like text, audio, depth, thermal, and IMU data.The LanguageBind employs contrastive learning to align and bind different modalities including video, infrared, depth, and audio from the environment with the language modality.In this study, we evaluate the perception module using the IN1K dataset for image data, the K400 dataset for video data, the LLVIP dataset for infrared data, the NYU-D dataset for depth data, the ESC-50 dataset for audio data, and the Ego4D dataset for IMU data.</p>
<p>In this use case, we leverage mobile and edge LLM agents to generate accident reports collaboratively for car crashes, where multiple mobile agents perceive the environment and report their local observations to edge LLM agents.Meanwhile, edge LLM agents generate a general report by leveraging global information and high-level plans for mobile agents.We validate the performance of LLM agents based on the Car Crash dataset (https://github.com/Cogito2012/CarCrashDataset).The mobile LLM agents on vehicles are implemented based on After the perception of environments, each mobile LLM agent for perception describes the local situation and reports the current situation to the edge LLM agent.Based on the collected local perception, the edge LLM agent aggregates them into a comprehensive picture for the following multistep reasoning and planning processing, e.g., CoT.Finally, the edge LLM agent provides general plans to mobile LLM agents for actuation and lets them interact with users and environments with local plans translated by their local LLMs, including text responses, APIs, and embodied actions.Due to the limited context window of mobile and edge LLM agents, we consider the inference of perception and actuation to be zero-shot and their performance is determined by perception fidelity of multimodal information and successful ratio during interaction with users and environments.Furthermore, edge LLM agents can provide suggestions for multiple local agents and their performance is affected by their historical thoughts.As the CoT is a step-by-step inference process that generates multiple intermediate thoughts during the grounding of final decisions, the thought that is closer to the final decisions should contribute more value to making the final decisions.In this regard, we propose a metric of age-of-thought (AoT) to evaluate the value of thoughts based on their freshness.</p>
<p>With the limited memory of edge servers and the massive amount of parameters of LLMs, edge servers cannot load all the models into the main memory at the same time.To provide AI services to satisfy user requirements, edge servers need to schedule the global AI models for reasoning and planning for the requested services.To minimize the cost in terms of edge accuracy loss, model switching cost, edge inference cost, edge inference latency, and cloud inference cost, effective model caching algorithms should be designed to manage loaded models for edge LLM agents.Especially, the cached models not only can be evicted proactively according to the caching policies, but also can be evicted due to the used context exceeding the context window.Therefore, we propose an LAoT model caching algorithm based on the concept of AoT where the model with the least valuable thoughts is evicted.</p>
<p>The maximum token consumption for each CoT step is set to 200 in the experiment.The context window of LLaMA is 4K tokens, the context window of GPT-3.5-turbo is 16K tokens, and the context window of GPT-4 is 32K tokens.We consider an edge server with 64 GPUs with 80 GB memory, 312 TFLOPS, and 300W max thermal design power.We consider 30 types of services and 10 edge LLM agents.The experimental results are illustrated in Fig. 3.As the number of time slots increases, the cost of edge inference of LLM agents decreases due to less switching cost and higher model performance.The reason is that the popular models are loaded into the memory of edge servers.In addition, during the inference of edge LLM agents, the thoughts are accumulated to improve the reasoning and planning results and thus the edge accuracy loss is lower.Overall, we can observe that the LC algorithm can improve the accuracy of edge LLM agents while reducing total execution costs compared to the existing baselines, including cloud-only inference, the first-in-first-out (FIFO) policy, and the least frequently used (LFU) policy.</p>
<p>VII. CONCLUSIONS AND FUTURE DIRECTIONS</p>
<p>In this article, we have proposed a split learning system for LLM agents over collaborative end-edge-cloud computing in 6G networks for multimodal perception, interactive grounding, and alignment.We have introduced the evolution of LLM agents and the construction of LLM agents over end-edgecloud computing with collaborative mobile and edge LLM agents.Furthermore, we have investigated the communication and networking issues in developing mobile edge-empowered agents including perception, grounding, and alignment.Finally, we have developed a use case for the application of mobile and edge LLM agents in vehicular networks and propose a model caching algorithm to optimize the performance of AI agent services while reducing execution costs.</p>
<p>In future research, it is important to explore further integration of 6G networks and AI agents.This could involve incorporating next-generation multiple access, metasurface, and over-the-air computation to support LLM agents in dynamic wireless environments.Additionally, it is crucial to address the model privacy concerns that may arise during collaboration between mobile and edge LLM agents.This will help prevent any potential information breach, especially in cases where malicious edge servers may attempt to access users' private information from running models.</p>
<p>Fig. 1 :
1
Fig. 1: The split learning system of mobile and edge LLM agents over collaborative end-edge-cloud computing.</p>
<p>Please generate an accident report for this car crash.</p>
<p>Prompt:</p>
<p>Based on the description of the car crash, please generate a accident diagram and a piece of audio for the accident report.Response: Candidate APIs: Vehicle Communication, Traffic Management, Telematics, Mobile Notification, Geolocation, Healthcare and Hospital, Insurance Claim, Legal and Police Report Offload perception results</p>
<p>Fig. 3 :
3
Fig. 3: The execution cost of various model caching algorithms in different time slots.</p>
<p>TABLE I :
I
Evaluation of mobile LLM agents with different perception modules for different modalities.
ImageBindLanguageBindModelViT-Huge (632 M) ViT-Large/14 (307M)Image -IN1K77.7-Video -K40050.064.0Infrared -LLVIP63.487.2Depth -NYU-D54.065.1Audio -ESC-5066.989.8IMU -Ego4D25.0-LLaMA. The perception module is developed based on Video-LLaMA, the local grounding module is developed basedon LLaMA-7B-Chat, and the alignment module is devel-oped based on ToolLLM. The edge LLM agent on theedge server is developed based on GPT4 and we implementa GPT named "Accident Report Assistant" as the globalAI agent (https://chat.openai.com/g/g-7sWJT5dSD-accident-report-assistant). The actuation module is implemented withToolLLaMA, which is a fine-tuned LLaMA-7B model usingthe instruction-solution pairs.</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Z Yang, L Li, K Lin, J Wang, C.-C Lin, Z Liu, L Wang, arXiv:2309.174212023arXiv preprint</p>
<p>Large language models empowered autonomous edge ai for connected intelligence. Y Shen, J Shao, X Zhang, Z Lin, H Pan, D Li, J Zhang, K B Letaief, IEEE Communications Magazine. 2024</p>
<p>Pushing large language models to the 6G edge: Vision, challenges, and opportunities. Z Lin, G Qu, Q Chen, X Chen, Z Chen, K Huang, arXiv:2309.167392023arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Q Wu, G Bansal, J Zhang, Y Wu, S Zhang, E Zhu, B Li, L Jiang, X Zhang, C Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Anymal: An efficient and scalable any-modality augmented language model. S Moon, A Madotto, Z Lin, T Nagarajan, M Smith, S Jain, C.-F Yeh, P Murugesan, P Heidari, Y Liu, arXiv:2309.160582023arXiv preprint</p>
<p>Imagebind: One embedding space to bind them all. R Girdhar, A El-Nouby, Z Liu, M Singh, K V Alwala, A Joulin, I Misra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Retrieval-based language models and applications. A Asai, S Min, Z Zhong, D Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20236Tutorial Abstracts)</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, arXiv:2309.154022023arXiv preprint</p>
<p>Integrated sensing and communications: Toward dual-functional wireless networks for 6G and beyond. F Liu, Y Cui, C Masouros, J Xu, T X Han, Y C Eldar, S Buzzi, IEEE Journal on Selected Areas in Communications. 4062022</p>
<p>Taskoriented communications for 6G: Vision, principles, and technologies. Y Shi, Y Zhou, D Wen, Y Wu, C Jiang, K B Letaief, IEEE Wireless Communications. 3032023</p>
<p>Trustworthy llms: a survey and guideline for evaluating large language models' alignment. Y Liu, Y Yao, J.-F Ton, X Zhang, R G H Cheng, Y Klochkov, M F Taufiq, H Li, arXiv:2308.053742023arXiv preprint</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. B Zhu, B Lin, M Ning, Y Yan, J Cui, H Wang, Y Pang, W Jiang, J Zhang, Z Li, arXiv:2310.018522023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
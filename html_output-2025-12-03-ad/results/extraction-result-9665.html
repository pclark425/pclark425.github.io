<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9665 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9665</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9665</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-267783118</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.10410v4.pdf" target="_blank">Evaluating Large Language Models on Wikipedia-Style Survey Generation</a></p>
                <p><strong>Paper Abstract:</strong> Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors like GPT-3.5, PaLM2, and LLaMa2 in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. No-tably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9665.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9665.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combined Automatic and Human Evaluation Framework for LLM-generated Surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates LLM-generated Wikipedia-style survey articles using a combination of automatic metrics (ROUGE, BERTScore, MoverScore, UniEval, BARTScore), human expert scoring (six criteria with 1-5 scale and detailed guidance), and LLM-based evaluators (GPT-4, G-Eval); it also analyses error types, entity novelty, and effects of retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4, GPT-3.5, PaLM2 (textbison), LLaMa2-13B, LLaMa2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Evaluated models include OpenAI GPT-4 and GPT-3.5 (proprietary GPT family), Google PaLM2 (textbison), and Meta LLaMa2 (13B and 70B); the paper uses these off-the-shelf LLMs in multiple prompt settings (zero-shot, one-shot, description prompt, combined OSP, and retrieval-augmented OS+IR).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing education / automated survey generation (computer science - NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Multi-pronged: (1) automatic reference-based metrics (ROUGE, BERTScore, MoverScore, UniEval, BARTScore) comparing generated surveys to Surfer100 ground-truth; (2) human expert scoring on six aspects using a 1–5 rubric with written guidance; (3) LLM-based evaluation (GPT-4 and G-Eval) scored on same rubric; (4) error taxonomy analysis and entity-novelty analysis via Stanza; (5) blind side-by-side likeability comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Readability, Relevancy, Redundancy, Hallucination, Completeness/Accuracy, Factuality (each scored 1–5); additionally automatic metrics ROUGE (R-1/R-2/R-L), BERTScore (P/R/F1), MoverScore, UniEval, BARTScore; inter-annotator agreement metrics (Krippendorff's α, Kendall's τ, Cohen's Kappa for pairwise comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Surfer100 — a dataset of 100 manually written Wikipedia-style survey articles on NLP concepts, each with five sections (Introduction, History, Key Ideas, Uses/Applications, Variations), 50–150 words per section; used as ground truth reference.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 consistently outperforms GPT-3.5, PaLM2, and LLaMa2 across automatic metrics and human judgments; enriching prompts (OSP) improves GPT-4 by around 2%–20% on automatic metrics, with GPT-4 OSP topping most metrics. Retrieval/external-knowledge (GPT-4 OS+IR or OS+Wiki) further increases ROUGE, MoverScore, and UniEval. Human expert ratings indicate high overall quality but lowest scores on completeness; GPT-based evaluators show higher variability and systematic bias favoring machine-generated texts. Error analysis shows 'Missing Information' is the most frequent error, followed by verbosity and factual errors; History and Introduction sections contain the most errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ground-truth surveys are from 2021 and may be temporally mismatched with LLM outputs (contemporary content), causing imperfect ROUGE comparisons; LLMs sometimes hallucinate or omit key historical details; GPT-based evaluators exhibit bias (prefer its own outputs) and higher randomness across sessions; subjectivity in criteria like Redundancy yields lower correlation with humans; human evaluation requires pre-selection to achieve higher agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human experts show higher inter-annotator agreement and consistent judgments; GPT-4 and G-Eval approximate human judgments on objective criteria (Factuality correlates best), but deviate on subjective criteria (Redundancy) and systematically prefer machine-generated content. Cohen's Kappa for human likeability pairwise comparison was 0.68 (substantial agreement). Krippendorff's α and Kendall's τ were used to quantify agreement and correlation, showing humans more consistent than LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a mixed evaluation protocol combining multiple automatic metrics, human expert scoring with explicit rubric (the provided six criteria), and targeted error analyses; pre-select and calibrate human judges to reduce variance; include external knowledge retrieval (RAG or Wikipedia links) to improve generation quality; beware of LLM-evaluator bias — do not rely solely on LLM-based evaluation for human-authored text; perform section-wise error analyses (focus on History and Introduction) and entity-novelty checks (via NER) when assessing 'completeness' and 'novelty'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models on Wikipedia-Style Survey Generation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9665.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9665.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based Automatic Metrics (ROUGE, BERTScore, MoverScore, UniEval, BARTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of established automatic metrics used to compare generated surveys to ground truth: lexical overlap (ROUGE), contextual embedding similarity (BERTScore, MoverScore), learned quality estimators (UniEval), and generation-focused scoring (BARTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>applied to outputs of GPT-4, GPT-3.5, PaLM2, LLaMa2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Metrics computed on generated text vs Surfer100 ground truth using official implementations and repos (rouge, bert_score, moverScore, UniEval, BARTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / text generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute ROUGE (R-1/R-2/R-L), BERTScore (P/R/F1), MoverScore, UniEval, and BARTScore for each generated survey against its corresponding Surfer100 ground-truth survey; compare across models and prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lexical overlap (ROUGE), contextual semantic similarity (BERTScore, MoverScore), learned quality estimation (UniEval), and generative model scoring (BARTScore); used as quantitative proxies for quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Surfer100 ground-truth surveys (100 topics; each survey 5 sections); metrics computed per-survey and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Automatic metrics rank GPT-4 highest overall, with GPT-4 OSP producing the highest scores on many metrics; adding external knowledge (OS+IR / OS+Wiki) yields further gains (e.g., slight increases in ROUGE and MoverScore). The paper reports GPT-4 OSP R-1 ≈ 31.47 and GPT-4 OS+IR R-1 ≈ 31.96 (values quoted from Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ROUGE and similar reference-based metrics penalize contemporary or more complete LLM outputs when ground truth is older/less complete; they can fail to capture factuality/hallucination and may reward lexical overlap rather than true understanding; they do not detect bias in LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automatic metrics broadly align with human judgments in ranking models (GPT-4 top), but do not fully capture completeness or factual correctness; human assessments remain necessary for nuanced evaluation (e.g., hallucinations, missing domain-specific facts).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine multiple automatic metrics with human evaluation; treat high automatic scores as indicative but not sufficient; supplement with factuality checks and retrieval-based comparison when ground truth is out-of-date.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models on Wikipedia-Style Survey Generation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9665.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9665.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Six-Criterion Human Evaluation Rubric (Readability, Relevancy, Redundancy, Hallucination, Completeness/Accuracy, Factuality)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed 1–5 scoring guidance provided to human experts for assessing generated surveys along six dimensions, with explicit descriptions of what constitutes 1 vs 5 for each criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>used to evaluate outputs of GPT-4 (OSP best) and other LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Human scoring applied to outputs from the evaluated LLMs; human judges pre-selected for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / educational content quality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Two human experts (after pre-selection) scored the 99 generated surveys produced by the best GPT-4 OSP setting on the six criteria, using provided written guidance; Krippendorff's α and percentage identical scores used to quantify IAA; Kendall's τ used to compare human and GPT-4 evaluator correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Explicit definitions: Readability (grammar/coherence), Relevancy (on-topic coverage), Redundancy (conciseness, non-repetitiveness), Hallucination (presence of false/misleading info), Completeness/Accuracy (coverage of key info), Factuality (objective correctness, especially in History/Main Idea).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the 99-topic subset of Surfer100 surveys generated under GPT-4 OSP.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Human experts rated generated surveys highly on most aspects; completeness received the lowest mean scores. Inter-annotator agreement among humans was high after pre-selection; Kendall's τ shows Factuality has the highest human–GPT correlation whereas Redundancy shows the lowest.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity in criteria like Redundancy leads to lower human–LLM agreement; human evaluation is labor-intensive and requires pre-selection to reduce variance; blind evaluation cannot assess Factuality and Completeness effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human experts are more consistent and better at manual fact-checking; GPT-4 evaluations approximate human scores on objective aspects but show bias and higher variability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide explicit rubric and examples, pre-select/calibrate judges, use blind comparisons for likeability, and keep Factuality/Completeness as separate non-blind checks requiring external knowledge verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models on Wikipedia-Style Survey Generation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9665.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9665.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt/Setting Ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Types and Retrieval-Augmented Settings (ZS, OS, DP, OSP, OS+IR, OS+Wiki)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study compares zero-shot (ZS), one-shot (OS), description-prompt (DP), combined one-shot+description (OSP), and retrieval-augmented settings (OS+IR: web search; OS+Wiki: provided Wikipedia links) to measure the impact of prompting and external knowledge on survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>primarily GPT-4 (with comparisons across GPT-3.5, PaLM2, LLaMa2 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Prompt strategies applied to pre-trained LLMs; OS uses a single ground-truth Word2Vec example; DP supplies section-level descriptions; OSP combines both; OS+IR links to web search or Wikipedia to provide external context.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP generation conditioning / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate 99 surveys per setting per model; compare automatic metrics and human evaluations across settings; run special GPT-4 OS+IR and OS+Wiki settings to measure retrieval benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same automatic and human criteria; comparison focuses on metric improvements and error reductions across prompt types.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Surfer100 topics (99 used per prompt setting), plus crawled Wikipedia links (87 effective links) for OS+Wiki.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Prompt enrichment generally improves quality: GPT-4 OSP achieves top results across many metrics. However, prompt enrichment does not uniformly help all models (e.g., LLaMa2 sometimes performed better with OS or DP than OSP). Retrieval augmentation (OS+IR, OS+Wiki) yields measurable improvements in ROUGE, MoverScore, and UniEval, indicating external knowledge aids completeness and factuality (GPT-4 OS+IR had slightly higher R-1 than GPT-4 OSP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt engineering gains are model-dependent; combining multiple prompt enrichments can sometimes hurt for certain architectures. Retrieval depends on quality of external sources and correct integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Retrieval-augmented outputs are closer to human ground-truth in automatic metrics and can reduce missing-information errors, but human scrutiny still required for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use combined one-shot plus explicit section descriptions (OSP) for best results with strong LLMs (GPT-4); use retrieval augmentation when factual completeness is critical; evaluate prompt effects per-model rather than assuming uniform gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models on Wikipedia-Style Survey Generation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9665.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9665.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error & Novelty Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error Taxonomy and Novel Entity Mention Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper defines an error taxonomy (Verbose, Wrong Fact, Missing Information, No Error) and applies NER-based novelty analysis via Stanza to quantify unique entities mentioned by LLM outputs relative to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>analysis applied primarily to GPT-4 OSP outputs (comparisons with other models also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Error labelling performed by two human experts on GPT-4 OSP outputs; entity extraction via Stanza to count unique novel entities.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP error analysis / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual error annotation of GPT-4 OSP outputs into four categories; Stanza-based NER to compare entity sets between generated text and ground truth; quantify distributions by section and error type.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Error counts and proportions by type; counts of unique novel entities per model/setting.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the set of generated surveys (99 topics), compared to Surfer100 ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Missing Information is the most frequent error type, followed by verbosity and factual inaccuracies; History and Introduction sections have highest error rates while Applications has the fewest. Entity-novelty analysis shows models (e.g., LLaMa2-13B) vary in introducing new entities; GPT-4 did not show marked novelty despite high quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>NER-based novelty measures depend on extraction quality; novel entity mention does not necessarily correlate with better performance and may reflect hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human annotators provide error labels and confirm missing details that automatic metrics miss; manual inspection remains necessary to interpret novel entities and factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include section-wise error annotation to target common weak spots (History/Introduction); pair NER novelty analysis with fact-checking to distinguish useful novelty from hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models on Wikipedia-Style Survey Generation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9665.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9665.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias in LLM-based Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Systematic Bias of GPT-4 and LLM Evaluators Toward Machine-generated Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study finds that GPT-4 tends to assign higher ratings to texts generated by itself and may prefer machine-generated texts over human-written ground truth in some comparisons, indicating evaluator bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 and G-Eval (GPT-4-based evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLM-based evaluation tools (GPT-4, G-Eval) used to rate outputs on the same six-criterion rubric; multiple independent sessions reveal variability.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare human expert scores vs GPT-4 and G-Eval scores on generated surveys; perform blind side-by-side likeability tests and compute agreement metrics (Kendall's τ, p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Discrepancies in Readability, Relevancy, Redundancy, Hallucination, Completeness, Factuality; bias measured by preference patterns (e.g., GPT-4 preferring machine-generated survey groups).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>99-topic GPT-4 OSP outputs and corresponding Surfer100 ground-truth surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 exhibits a measurable bias favoring machine-generated texts (including its own), often assigning higher ratings and showing greater variability across sessions. Factuality correlates best between human and GPT scores; Redundancy correlates least. The paper advises caution in replacing humans with LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM evaluators can be overconfident and biased; session-to-session randomness reduces reproducibility; blind tests cannot evaluate some criteria (Factuality, Completeness) reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human evaluators remain gold-standard for nuanced judgments and fact-checking; LLM evaluators can approximate humans on objective aspects but are not reliable standalone assessors.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not use LLM-only evaluation to replace human judgment; use LLM evaluators as complementary tools, calibrate them, and cross-check with human annotations and external factual sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models on Wikipedia-Style Survey Generation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>BARTScore: Evaluating generated text as text generation <em>(Rating: 2)</em></li>
                <li>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating text generation with BERT <em>(Rating: 2)</em></li>
                <li>Surfer100 (paper introducing the Surfer100 dataset) <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9665",
    "paper_id": "paper-267783118",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Evaluation Framework",
            "name_full": "Combined Automatic and Human Evaluation Framework for LLM-generated Surveys",
            "brief_description": "The paper evaluates LLM-generated Wikipedia-style survey articles using a combination of automatic metrics (ROUGE, BERTScore, MoverScore, UniEval, BARTScore), human expert scoring (six criteria with 1-5 scale and detailed guidance), and LLM-based evaluators (GPT-4, G-Eval); it also analyses error types, entity novelty, and effects of retrieval augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4, GPT-3.5, PaLM2 (textbison), LLaMa2-13B, LLaMa2-70B",
            "llm_description": "Evaluated models include OpenAI GPT-4 and GPT-3.5 (proprietary GPT family), Google PaLM2 (textbison), and Meta LLaMa2 (13B and 70B); the paper uses these off-the-shelf LLMs in multiple prompt settings (zero-shot, one-shot, description prompt, combined OSP, and retrieval-augmented OS+IR).",
            "scientific_domain": "Natural Language Processing education / automated survey generation (computer science - NLP)",
            "evaluation_method": "Multi-pronged: (1) automatic reference-based metrics (ROUGE, BERTScore, MoverScore, UniEval, BARTScore) comparing generated surveys to Surfer100 ground-truth; (2) human expert scoring on six aspects using a 1–5 rubric with written guidance; (3) LLM-based evaluation (GPT-4 and G-Eval) scored on same rubric; (4) error taxonomy analysis and entity-novelty analysis via Stanza; (5) blind side-by-side likeability comparisons.",
            "evaluation_criteria": "Readability, Relevancy, Redundancy, Hallucination, Completeness/Accuracy, Factuality (each scored 1–5); additionally automatic metrics ROUGE (R-1/R-2/R-L), BERTScore (P/R/F1), MoverScore, UniEval, BARTScore; inter-annotator agreement metrics (Krippendorff's α, Kendall's τ, Cohen's Kappa for pairwise comparisons).",
            "benchmark_or_dataset": "Surfer100 — a dataset of 100 manually written Wikipedia-style survey articles on NLP concepts, each with five sections (Introduction, History, Key Ideas, Uses/Applications, Variations), 50–150 words per section; used as ground truth reference.",
            "results_summary": "GPT-4 consistently outperforms GPT-3.5, PaLM2, and LLaMa2 across automatic metrics and human judgments; enriching prompts (OSP) improves GPT-4 by around 2%–20% on automatic metrics, with GPT-4 OSP topping most metrics. Retrieval/external-knowledge (GPT-4 OS+IR or OS+Wiki) further increases ROUGE, MoverScore, and UniEval. Human expert ratings indicate high overall quality but lowest scores on completeness; GPT-based evaluators show higher variability and systematic bias favoring machine-generated texts. Error analysis shows 'Missing Information' is the most frequent error, followed by verbosity and factual errors; History and Introduction sections contain the most errors.",
            "limitations_or_challenges": "Ground-truth surveys are from 2021 and may be temporally mismatched with LLM outputs (contemporary content), causing imperfect ROUGE comparisons; LLMs sometimes hallucinate or omit key historical details; GPT-based evaluators exhibit bias (prefer its own outputs) and higher randomness across sessions; subjectivity in criteria like Redundancy yields lower correlation with humans; human evaluation requires pre-selection to achieve higher agreement.",
            "comparison_to_human_or_traditional": "Human experts show higher inter-annotator agreement and consistent judgments; GPT-4 and G-Eval approximate human judgments on objective criteria (Factuality correlates best), but deviate on subjective criteria (Redundancy) and systematically prefer machine-generated content. Cohen's Kappa for human likeability pairwise comparison was 0.68 (substantial agreement). Krippendorff's α and Kendall's τ were used to quantify agreement and correlation, showing humans more consistent than LLM evaluators.",
            "recommendations_or_best_practices": "Use a mixed evaluation protocol combining multiple automatic metrics, human expert scoring with explicit rubric (the provided six criteria), and targeted error analyses; pre-select and calibrate human judges to reduce variance; include external knowledge retrieval (RAG or Wikipedia links) to improve generation quality; beware of LLM-evaluator bias — do not rely solely on LLM-based evaluation for human-authored text; perform section-wise error analyses (focus on History and Introduction) and entity-novelty checks (via NER) when assessing 'completeness' and 'novelty'.",
            "uuid": "e9665.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Automatic Metrics",
            "name_full": "Reference-based Automatic Metrics (ROUGE, BERTScore, MoverScore, UniEval, BARTScore)",
            "brief_description": "A set of established automatic metrics used to compare generated surveys to ground truth: lexical overlap (ROUGE), contextual embedding similarity (BERTScore, MoverScore), learned quality estimators (UniEval), and generation-focused scoring (BARTScore).",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "applied to outputs of GPT-4, GPT-3.5, PaLM2, LLaMa2",
            "llm_description": "Metrics computed on generated text vs Surfer100 ground truth using official implementations and repos (rouge, bert_score, moverScore, UniEval, BARTScore).",
            "scientific_domain": "NLP evaluation / text generation",
            "evaluation_method": "Compute ROUGE (R-1/R-2/R-L), BERTScore (P/R/F1), MoverScore, UniEval, and BARTScore for each generated survey against its corresponding Surfer100 ground-truth survey; compare across models and prompt settings.",
            "evaluation_criteria": "Lexical overlap (ROUGE), contextual semantic similarity (BERTScore, MoverScore), learned quality estimation (UniEval), and generative model scoring (BARTScore); used as quantitative proxies for quality.",
            "benchmark_or_dataset": "Surfer100 ground-truth surveys (100 topics; each survey 5 sections); metrics computed per-survey and averaged.",
            "results_summary": "Automatic metrics rank GPT-4 highest overall, with GPT-4 OSP producing the highest scores on many metrics; adding external knowledge (OS+IR / OS+Wiki) yields further gains (e.g., slight increases in ROUGE and MoverScore). The paper reports GPT-4 OSP R-1 ≈ 31.47 and GPT-4 OS+IR R-1 ≈ 31.96 (values quoted from Table 1).",
            "limitations_or_challenges": "ROUGE and similar reference-based metrics penalize contemporary or more complete LLM outputs when ground truth is older/less complete; they can fail to capture factuality/hallucination and may reward lexical overlap rather than true understanding; they do not detect bias in LLM evaluations.",
            "comparison_to_human_or_traditional": "Automatic metrics broadly align with human judgments in ranking models (GPT-4 top), but do not fully capture completeness or factual correctness; human assessments remain necessary for nuanced evaluation (e.g., hallucinations, missing domain-specific facts).",
            "recommendations_or_best_practices": "Combine multiple automatic metrics with human evaluation; treat high automatic scores as indicative but not sufficient; supplement with factuality checks and retrieval-based comparison when ground truth is out-of-date.",
            "uuid": "e9665.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Human Rubric",
            "name_full": "Six-Criterion Human Evaluation Rubric (Readability, Relevancy, Redundancy, Hallucination, Completeness/Accuracy, Factuality)",
            "brief_description": "Detailed 1–5 scoring guidance provided to human experts for assessing generated surveys along six dimensions, with explicit descriptions of what constitutes 1 vs 5 for each criterion.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "used to evaluate outputs of GPT-4 (OSP best) and other LLMs",
            "llm_description": "Human scoring applied to outputs from the evaluated LLMs; human judges pre-selected for alignment.",
            "scientific_domain": "NLP / educational content quality evaluation",
            "evaluation_method": "Two human experts (after pre-selection) scored the 99 generated surveys produced by the best GPT-4 OSP setting on the six criteria, using provided written guidance; Krippendorff's α and percentage identical scores used to quantify IAA; Kendall's τ used to compare human and GPT-4 evaluator correlations.",
            "evaluation_criteria": "Explicit definitions: Readability (grammar/coherence), Relevancy (on-topic coverage), Redundancy (conciseness, non-repetitiveness), Hallucination (presence of false/misleading info), Completeness/Accuracy (coverage of key info), Factuality (objective correctness, especially in History/Main Idea).",
            "benchmark_or_dataset": "Applied to the 99-topic subset of Surfer100 surveys generated under GPT-4 OSP.",
            "results_summary": "Human experts rated generated surveys highly on most aspects; completeness received the lowest mean scores. Inter-annotator agreement among humans was high after pre-selection; Kendall's τ shows Factuality has the highest human–GPT correlation whereas Redundancy shows the lowest.",
            "limitations_or_challenges": "Subjectivity in criteria like Redundancy leads to lower human–LLM agreement; human evaluation is labor-intensive and requires pre-selection to reduce variance; blind evaluation cannot assess Factuality and Completeness effectively.",
            "comparison_to_human_or_traditional": "Human experts are more consistent and better at manual fact-checking; GPT-4 evaluations approximate human scores on objective aspects but show bias and higher variability.",
            "recommendations_or_best_practices": "Provide explicit rubric and examples, pre-select/calibrate judges, use blind comparisons for likeability, and keep Factuality/Completeness as separate non-blind checks requiring external knowledge verification.",
            "uuid": "e9665.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Prompt/Setting Ablations",
            "name_full": "Prompt Types and Retrieval-Augmented Settings (ZS, OS, DP, OSP, OS+IR, OS+Wiki)",
            "brief_description": "The study compares zero-shot (ZS), one-shot (OS), description-prompt (DP), combined one-shot+description (OSP), and retrieval-augmented settings (OS+IR: web search; OS+Wiki: provided Wikipedia links) to measure the impact of prompting and external knowledge on survey generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "primarily GPT-4 (with comparisons across GPT-3.5, PaLM2, LLaMa2 variants)",
            "llm_description": "Prompt strategies applied to pre-trained LLMs; OS uses a single ground-truth Word2Vec example; DP supplies section-level descriptions; OSP combines both; OS+IR links to web search or Wikipedia to provide external context.",
            "scientific_domain": "NLP generation conditioning / evaluation",
            "evaluation_method": "Generate 99 surveys per setting per model; compare automatic metrics and human evaluations across settings; run special GPT-4 OS+IR and OS+Wiki settings to measure retrieval benefits.",
            "evaluation_criteria": "Same automatic and human criteria; comparison focuses on metric improvements and error reductions across prompt types.",
            "benchmark_or_dataset": "Surfer100 topics (99 used per prompt setting), plus crawled Wikipedia links (87 effective links) for OS+Wiki.",
            "results_summary": "Prompt enrichment generally improves quality: GPT-4 OSP achieves top results across many metrics. However, prompt enrichment does not uniformly help all models (e.g., LLaMa2 sometimes performed better with OS or DP than OSP). Retrieval augmentation (OS+IR, OS+Wiki) yields measurable improvements in ROUGE, MoverScore, and UniEval, indicating external knowledge aids completeness and factuality (GPT-4 OS+IR had slightly higher R-1 than GPT-4 OSP).",
            "limitations_or_challenges": "Prompt engineering gains are model-dependent; combining multiple prompt enrichments can sometimes hurt for certain architectures. Retrieval depends on quality of external sources and correct integration.",
            "comparison_to_human_or_traditional": "Retrieval-augmented outputs are closer to human ground-truth in automatic metrics and can reduce missing-information errors, but human scrutiny still required for verification.",
            "recommendations_or_best_practices": "Use combined one-shot plus explicit section descriptions (OSP) for best results with strong LLMs (GPT-4); use retrieval augmentation when factual completeness is critical; evaluate prompt effects per-model rather than assuming uniform gains.",
            "uuid": "e9665.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Error & Novelty Analysis",
            "name_full": "Error Taxonomy and Novel Entity Mention Analysis",
            "brief_description": "The paper defines an error taxonomy (Verbose, Wrong Fact, Missing Information, No Error) and applies NER-based novelty analysis via Stanza to quantify unique entities mentioned by LLM outputs relative to ground truth.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "analysis applied primarily to GPT-4 OSP outputs (comparisons with other models also reported)",
            "llm_description": "Error labelling performed by two human experts on GPT-4 OSP outputs; entity extraction via Stanza to count unique novel entities.",
            "scientific_domain": "NLP error analysis / evaluation",
            "evaluation_method": "Manual error annotation of GPT-4 OSP outputs into four categories; Stanza-based NER to compare entity sets between generated text and ground truth; quantify distributions by section and error type.",
            "evaluation_criteria": "Error counts and proportions by type; counts of unique novel entities per model/setting.",
            "benchmark_or_dataset": "Applied to the set of generated surveys (99 topics), compared to Surfer100 ground truth.",
            "results_summary": "Missing Information is the most frequent error type, followed by verbosity and factual inaccuracies; History and Introduction sections have highest error rates while Applications has the fewest. Entity-novelty analysis shows models (e.g., LLaMa2-13B) vary in introducing new entities; GPT-4 did not show marked novelty despite high quality.",
            "limitations_or_challenges": "NER-based novelty measures depend on extraction quality; novel entity mention does not necessarily correlate with better performance and may reflect hallucination.",
            "comparison_to_human_or_traditional": "Human annotators provide error labels and confirm missing details that automatic metrics miss; manual inspection remains necessary to interpret novel entities and factuality.",
            "recommendations_or_best_practices": "Include section-wise error annotation to target common weak spots (History/Introduction); pair NER novelty analysis with fact-checking to distinguish useful novelty from hallucination.",
            "uuid": "e9665.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Bias in LLM-based Evaluation",
            "name_full": "Systematic Bias of GPT-4 and LLM Evaluators Toward Machine-generated Text",
            "brief_description": "The study finds that GPT-4 tends to assign higher ratings to texts generated by itself and may prefer machine-generated texts over human-written ground truth in some comparisons, indicating evaluator bias.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4 and G-Eval (GPT-4-based evaluator)",
            "llm_description": "LLM-based evaluation tools (GPT-4, G-Eval) used to rate outputs on the same six-criterion rubric; multiple independent sessions reveal variability.",
            "scientific_domain": "NLP evaluation methodology",
            "evaluation_method": "Compare human expert scores vs GPT-4 and G-Eval scores on generated surveys; perform blind side-by-side likeability tests and compute agreement metrics (Kendall's τ, p-values).",
            "evaluation_criteria": "Discrepancies in Readability, Relevancy, Redundancy, Hallucination, Completeness, Factuality; bias measured by preference patterns (e.g., GPT-4 preferring machine-generated survey groups).",
            "benchmark_or_dataset": "99-topic GPT-4 OSP outputs and corresponding Surfer100 ground-truth surveys.",
            "results_summary": "GPT-4 exhibits a measurable bias favoring machine-generated texts (including its own), often assigning higher ratings and showing greater variability across sessions. Factuality correlates best between human and GPT scores; Redundancy correlates least. The paper advises caution in replacing humans with LLM evaluators.",
            "limitations_or_challenges": "LLM evaluators can be overconfident and biased; session-to-session randomness reduces reproducibility; blind tests cannot evaluate some criteria (Factuality, Completeness) reliably.",
            "comparison_to_human_or_traditional": "Human evaluators remain gold-standard for nuanced judgments and fact-checking; LLM evaluators can approximate humans on objective aspects but are not reliable standalone assessors.",
            "recommendations_or_best_practices": "Do not use LLM-only evaluation to replace human judgment; use LLM evaluators as complementary tools, calibrate them, and cross-check with human annotations and external factual sources.",
            "uuid": "e9665.5",
            "source_info": {
                "paper_title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "BARTScore: Evaluating generated text as text generation",
            "rating": 2,
            "sanitized_title": "bartscore_evaluating_generated_text_as_text_generation"
        },
        {
            "paper_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
            "rating": 2,
            "sanitized_title": "moverscore_text_generation_evaluating_with_contextualized_embeddings_and_earth_mover_distance"
        },
        {
            "paper_title": "BERTScore: Evaluating text generation with BERT",
            "rating": 2,
            "sanitized_title": "bertscore_evaluating_text_generation_with_bert"
        },
        {
            "paper_title": "Surfer100 (paper introducing the Surfer100 dataset)",
            "rating": 2,
            "sanitized_title": "surfer100_paper_introducing_the_surfer100_dataset"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        }
    ],
    "cost": 0.01420075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models on Wikipedia-Style Survey Generation</p>
<p>Fan Gao 
Hang Jiang 
MIT Center for Constructive Communication</p>
<p>Rui Yang 
National University of Singapore</p>
<p>Qingcheng Zeng 
Northwestern University</p>
<p>Jinghui Lu 
Moritz Blum 
Bielefeld University
6 Smartor.me</p>
<p>Dairui Liu 
University College Dublin
8 Moveworks</p>
<p>Tianwei She 
Yuang Jiang 
Irene Li ireneli@ds.itc.u-tokyo.ac.jp 
University of Tokyo</p>
<p>Chantal Shaib 
Millicent L Li 
Sebastian Joseph 
JunyiIain J Marshall 
Jessy Li 
Zejiang Shen 
Tal August 
Pao Siangliulue 
Kyle Lo 
Jonathan Bragg 
Jeff Hammerbacher 
Doug Downey 
Shubo Tian 
Qiao Jin 
Lana Yeganova 
Po-Ting Lai 
Qingqing Zhu 
Xiuying Chen 
Yifan Yang 
Qingyu Chen 
Won Kim 
Donald C Comeau 
Rezarta Isla- Maj 
University of Tokyo</p>
<p>Aadit Kapoor 
Xin Gao 
Zhiyong Lu 
Hugo Touvron 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
Naman Goyal 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Marie-Anne Lachaux 
Thibaut Lavril 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi 
Xiaoxuan Wang 
Ziniu Hu 
Pan Lu 
Yanqiao Zhu 
Jieyu Zhang 
Satyen Subramaniam 
Arjun R Loomba 
Kaiyu Yang 
Aidan M Swope 
Alex Gu 
Rahul Chala- Mala 
Tokyo Institute of Technology</p>
<p>Peiyang Song 
Shixing Yu 
Saad Godil 
Ryan Prenger 
Anima 2023 Anandkumar 
Leandojo 
Haoran Liu 
Edison Marrese-Taylor 
Yu He Ke 
Wanxin Li 
Lechao Cheng </p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Melanie Kambadur, Sharan NarangYuchen Zhang</p>
<p>Aurelien Ro-driguez
Sergey EdunovRobert Stojnic</p>
<p>Thomas Scialom
2023Llama</p>
<p>Shichang Zhang
Yizhou Sun, and Wei Wang</p>
<p>Evaluating Large Language Models on Wikipedia-Style Survey Generation
E100F1B2C715E6E4365B0E9A32870DD2SSRN Electronic Journal 2023Scibench: Evaluating college-level scientific problem-solving abilities of large language modelsArXiv, abs/230710635
Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update.Recently, Large Language Models (LLMs) have achieved significant success across various general tasks.However, their effectiveness and limitations in the education domain are yet to be fully explored.In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics.Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth.We compare both human and GPT-based evaluation scores and provide in-depth analysis.While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed.Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors.At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs) have attracted significant attention due to their strong performance on general natural language processing (NLP) tasks (Shaib et al., 2023;Feng et al., 2022).Especially, the GPT family (Brown et al., 2020) shows great ability in various applications.While it has been demonstrated that they perform well in many general tasks, their effectiveness in domainspecific tasks continues to be under scrutiny (Tian et al., 2023).Specifically, the text produced by LLMs can sometimes exhibit issues like creating false information and hallucination (Zhao et al., 2023;Yang et al., 2024a).</p>
<p>In the context of scientific education, automatic survey generation aims to employ machine learning or NLP techniques to create a structured overview of a specific concept (Sun and Zhuge, 2022;Li et al., 2022;Yang et al., 2024b).Automating this process not only alleviates the manual effort but also ensures timely updates at a reduced cost.A common approach involves an initial information retrieval phase to select pertinent documents or sentences based on the query topic.This is followed by a summarization or simplification phase to produce the final survey (Jha et al., 2013;Li et al., 2022).While LLMs have the potential to be an alternative method for writing scientific surveys, their effectiveness and limitations are not yet thoroughly investigated.</p>
<p>Existing work focuses on applying LLMs to similar scenarios, including aiding scientific writing (Shen et al., 2023;Altmäe et al., 2023), questionanswering with scientific papers (Tahri et al., 2022), writing paper reviews (Liang et al., 2023), and answering quiz or exam questions (Song et al., 2023;Wang et al., 2023).This study pushes the boundary of this research area as the first to evaluate the capability of LLMs in generating education surveys within the scientific domain of NLP (Li et al., 2022).Our primary objective is to understand whether LLMs can be used to explain concepts in a more structured manner.To this end, we aim to answer the following research questions (RQs):</p>
<p>• RQ1: How proficient are LLMs in generating survey articles on NLP concepts?</p>
<p>• RQ2: Can LLMs emulate human judgment when provided with specific criteria?</p>
<p>• RQ3: Do LLMs introduce a noticeable bias in evaluating machine-generated texts compared to human-written texts?We empirically conduct experiments on LLaMa2 (Touvron et al., 2023), PaLM2 (Anil et al., 2023), arXiv:2308.10410v4[cs.CL] 23 May 2024</p>
<p>Zero-Shot</p>
<p>Generate a survey about <Topic>.There should be five sub-sections: Introduction, History, Key Ideas, Variations and Applications.Each sub-section should contain 50-150 words.</p>
<p>One-Shot</p>
<p>Example survey: <INTRODUCTION> Word2Vec is one of the most popular tools to learn word embeddings using shallow neural networks.It first constructs a vocabulary from the training text data and then learns word embeddings.... <HISTORY> Word2vec was developed by a group of researchers headed by Tomas Mikolov at Google.Machine learning models take vectors as input, ... <KEY IDEAS> Word2Vec converts words into vector forms such that similar meaning words appear together and dissimilar words are located far away... <USES/APPLICATIONS> Gensim provides the Word2Vec class for working with a Word2Vec model.Training your own word vectors can take a long time and uses lots of memory... <VARIATIONS> Word embeddings is an active research area trying to figure out better word representations than the existing ones... Generate a survey about <Topic>.There should be five sub-sections: Introduction, History, Key Ideas, Variations and Applications.Each sub-section should contain 50-150 words.</p>
<p>With Prompt</p>
<p>Generate a survey about <Topic>.There should be five sub-sections: Introduction, History, Key Ideas, Variations and Applications.Each sub-section should contain 50-150 words.The following is the guideline for each section: SECTION 1: INTRODUCTION Describe what the topic is (a method, a model, a task, a dataset), which field/subfield it is part of, quick overview of applications and motivation behind concept and related ideas) SECTION 2: HISTORY Describe when or by who the topic was introduced, in what context, what problems it addresses.SECTION 3: KEY IDEAS Describe in greater depth (could provide some mathematical context or explain core concepts).SECTION 4: USES/APPLICATIONS Describe for what tasks this model/data is used.SECTION 5: VARIATIONS What variations or similar models, datasets, tasks exist and how does this topic fit into a bigger picture.</p>
<p>Figure 1: The three main prompt types we compared.We eliminated some text in the one-shot setting, which is the ground truth from the survey of Word2Vec.</p>
<p>GPT-3.5 (Brown et al., 2020) and GPT-4 (OpenAI, 2023) across four different settings.Furthermore, we engage human experts to provide a qualitative dimension, ensuring that our results not only reflect the technical performance but also incorporate subjective human perspectives.We release the LLMs-generated surveys of all these works 1 .</p>
<p>Method</p>
<p>Dataset</p>
<p>We adopt the Surfer100 dataset (Li et al., 2022), which contains 100 manually written Wikipediastyle articles on NLP concepts.Each survey is structured into five sections: Introduction, History, Key Ideas, Uses/Applications, and Variations, with each section containing between 50 and 150 words.</p>
<p>Experiment Setup</p>
<p>We compare three settings: zero-shot (ZS), oneshot (OS) and description prompt (DP).For zeroshot, we directly ask the model to generate the arti-1 https://github.com/astridesa/EDULLM/tree/master cle by providing the following prompt: Generate a survey about <Topic>.There should be five subsections: Introduction, History, Key Ideas, Variations and Applications.Each subsection should contain 50-150 words.For the one-shot setting, we add a ground-truth article of Word2Vec as the sample survey; for the description prompt setting, we add a detailed description to each section explaining what should be included.For example, SECTION 1: INTRODUCTION Describe what the topic is (a method, a model, a task, a dataset), which field/subfield it is part of, quick overview of applications and motivation behind the concept and related ideas).To further enrich the provided information, we also introduce a combination of one-shot and description prompt (OSP).The full prompt is shown in Figure 1.By employing a single ground truth for one-shot learning, we generate 99 surveys per setting.Moreover, we evaluate a special retrieval augmented generation (RAG) (Gao et al., 2023;Ram et al., 2023;Yang et al., 2023;Aksitov et al., 2023) setting (denoted as OS+IR) which enables GPT-4 link to Wikipedia pages and access to web data.We elaborate more experimental details in Appendix B</p>
<p>Evaluation Metrics</p>
<p>Automatic Evaluation We evaluate the generated surveys using a range of automatic metrics including ROUGE, BERTScore (Zhang* et al., 2020), MoverScore (Zhao et al., 2019), UniEval (Zhong et al., 2022) and BARTScore (Yuan et al., 2021).</p>
<p>Tab. 1 provides an overview of results for the following LLMs: LLaMa2 (13B, 70B), PaLM2 (textbison), GPT-3.5 (Turbo-0613) as well as GPT-4 (0613) across different prompt settings.We first notice that GPT-4 consistently outperforms other baselines, obtaining a significant improvement of around 2% to 20% when enhancing prompts.Specifically, GPT-4 OSP achieves the top spot under most situations.However, it is not to say that prompt enrichment always yields positive results.For instance, in the case of LLaMa2, oneshot and description prompts perform better than OSP.As for PaLM2, four types of prompts obtain similar results.When we add external knowledge (GPT-4 OS+IR), there is some improvement compared to GPT-4 OS.As our primary goal is to study the extent of knowledge LLMs possess in this task, we mainly focus on analyses in settings without external data.However, additional analysis about GPT-4 OS+IR setting can be found in Appendix B.</p>
<p>Human and GPTs Evaluation We employ two NLP experts, GPT-4 and G-Eval (Liu et al., 2023a) to evaluate surveys generated by the best GPT-4 OSP setting, focusing on 6 perspectives: Readability, Relevancy, Hallucination, Completeness, Factuality.Both GPT models and humans are required to score each aspect on a scale from 1 to 5, following the same guidelines.The detailed guidance can be found in Appendix A. It's important to note that we implement a pre-selection stage in the choice of human experts (Appendix A).Tab. 2 shows that both human experts and GPTs agree that the generated surveys perform well across most aspects, though the completeness exhibits marginally lowest scores.According to IAA, we can observe that human experts demonstrate a high consistent quality of the generated surveys whlie GPT-4 and G-Eval have more randomness.To better understand the degree of agreement between human experts and GPT-4 on ratings, we also calculate Kendall's τ and p-value as shown in Tab. 3. We can observe that the Factuality possesses the highest degree of correlation.In contrast, Redundancy displays the lowest correlation while the other aspects exhibit relatively lower correlation levels.This difference is largely because Factuality is based on objective ground truth, while Redundancy is more dependent on subjective judgment.Notably, we can conclude that in most scenarios, GPT-4 showcases similar evaluative opinions as humans, despite showing a higher degree of variability across different independent sessions.Regarding RQ1 and RQ2, we find that 1) LLMs can produce highquality survey articles, and 2) with specific guidance, there's a strong consistency between GPT outputs and human judgment.</p>
<p>Analysis</p>
<p>In this section, we provide an in-depth analysis of the LLMs' internal knowledge on survey writing ability, and compare the evaluation scores of human and LLM assessments.</p>
<p>Error Types We have shown that both automated and manual evaluations demonstrated that LLMs excel in crafting survey articles on scientific concepts.We analyze the best setting, GPT-4 OSP, assessing errors identified by two experts, and summarize error types and distributions in Fig. 2. We classify these errors into four categories: Verbose, Wrong Fact, Missing Information, and No Error (indicating flawless content).It shows that most errors are missing information, followed by verbosity and factual inaccuracies.Furthermore, the History and Introduction sections of the generated articles contained the highest number of errors, while the Application section exhibited the best.</p>
<p>Novel Entity Mention To further investigate how interesting the generated content is, we look at the mentions of novel entities following (Lee et al., 2022).Specifically, we examine the survey content by comparing the entities it contains with those in the ground truth.We employ Stanza (Qi et al., 2020) to identify all entities in both the LLMgenerated text and the ground truth.Subsequently, we quantify the number of unique entities found in the LLM-generated content.For a fair comparison, we analyze the one-shot with prompt settings of LLaMa2-13b, PaLM2, and GPT-4, in addition to the ZS setting of GPT-3.5, as depicted in Fig. 3. Our findings reveal that PaLM2 exhibited the least variation in entity mentions, while LLaMa2-13b showcased the most.Despite GPT-4's outstanding performance in both automated and human evalu- Table 2: Human and GPTs Evaluation Results.We report the mean and standard deviation.We also quantify the IAA (inter-annotator agreement) (Karpinska et al., 2021) between human experts and the GPT results, respectively, using Krippendorff's α coefficient and calculating the percentage (%) of scores that are identical.ations, we didn't discern a marked novelty in its entity mentions.We speculate that this might be an inherent compromise when generating high-fidelity content in relation to the ground truth.So far, regarding RQ1, although LLMs register commendable results based on predefined criteria, certain shortcomings are evident.Specifically, we observe some omitted details, particularly within the Introduction and History sections.While LLMs often introduce new entities, we don't find a significant correlation between this tendency and their performance.More case studies are in Appendix C.  2023b).To test the veracity of this assertion within the context of survey generation tasks, we took the opportunity to investigate whether a similar observation holds in the context of survey generation tasks.Hence, we recruited two human experts in a blind side-by-side comparison of both the ground truth survey articles and articles generated using the best GPT-4 settings, and they assessed the content based on 'Likeability'(Chiang and Lee, 2023).Subsequently, we categorized the survey articles into three groups: a) (human experts) Liked, b) (human experts) Disliked, and c) Equal (equally good).The experts reached a significant agreement, reflected in a Cohen's Kappa score of 0.68 (Cohen, 1960).In instances of disagreement, we randomly selected a score to reach a final consensus.We then apply the GPT-4 evaluation scores on the first four criteria except for Factuality and Completeness because both are impossible to do a blind test.We show the average ratings on all 99 concepts in Fig. 4. One main observation is the bias of GPT-4 towards texts generated by itself and consistently conferring high ratings -an observation consistent with other studies (Liu et al., 2023b).
Method ROUGE BERTScore MoverScore UniEval BARTScore R-1 R-2 R-L P R F1 LLaMa2-</p>
<p>LLM and Human Preference</p>
<p>When evaluating the ground truth, GPT-4 consistently assigns marginally lower ratings across all three categories.Intriguingly, GPT-4 shows a preference for the Disliked group over the Liked group when considering the ground truth, a tendency that diverges from human inclinations.This suggests that when assessing human-composed text, such as ground truth survey articles, GPT-4 might not yet be an impeccable substitute for human discernment.Thus, in response to RQ3, we found that GPT-4 exhibits a notable preference for machine-generated texts with specific biases.Furthermore, we contend that the complete replacement of human experts by GPT-4 is a challenging prospect.For instance, human expertise remains indispensable for manual content fact checking.</p>
<p>Discussion and Conclusion</p>
<p>In this work, we evaluate the ability of LLMs to write surveys on NLP concepts.We find that LLMs, particularly GPT-4, can author surveys following specific guidelines that rival the quality of human experts, even though there are shortcomings such as incomplete information.Our findings also indicate that GPT-4 may not be a perfect replacement for human judgment when evaluating human-composed texts, and certain biases exist when asking it to rate machine-generated texts.Nevertheless, the results imply that these advanced generative LLMs could play a transformative role in the realm of education.They hold the promise of effectively structuring domain-specific knowledge tailored to general learners.This adaptability could potentially lead to a more interactive and personalized learning experience, enabling students to engage in query-driven studies that cater directly to their unique curiosities and learning objectives.</p>
<p>Limitations and Ethical Considerations</p>
<p>GPT-4 can generate contemporary, accessible content, but sometimes compromises depth and detail, leading to potential information gaps and occasional factual inaccuracies.This requires extra verification.Comparing ratings between human experts and GPT-4 revealed a systematic bias in GPT evaluations, which can skew outcomes and mislead quality perception.The primary objective of this work is to explore the potential applicability of LLMs in the field of education, with a specific emphasis on enhancing the understanding of LLMs' generative capabilities within computer science.Our focus is on assessing the efficacy and identifying the boundaries of the generated texts.It's important to note that the generated texts are devoid of any harmful content, and all data used and produced in this study contains no private information.</p>
<p>A Human Evaluation Guidance</p>
<p>The detailed human evaluation guidance is listed in the following:</p>
<ol>
<li>Readability:</li>
</ol>
<p>• 1 (bad): The text is highly difficult to read, full of grammatical errors, and lacks coherence and clarity.• 5 (good): The text is easy to read, well-structured, and flows naturally.</p>
<p>Relevancy:</p>
<p>• 1 (bad): The generated text is completely irrelevant to the given context or prompt.</p>
<p>• 5 (good): The generated text is highly relevant and directly addresses the given context or prompt.</p>
<ol>
<li>Redundancy:</li>
</ol>
<p>• 1 (bad): The text is excessively repetitive, containing unnecessary repetitions of the same information.For example, each section should have 50-150 tokens.If it is too long, we should give a low rating.• 5 (good): The text is concise and free from redundancy, providing only essential information.</p>
<p>Hallucination:</p>
<p>• 1 (bad): The generated text includes false or misleading information that does not align with the context or is factually incorrect.• 5 (good): The generated text is free from hallucinations and provides accurate and contextually appropriate information.</p>
<p>Completeness/Accuracy:</p>
<p>• 1 (bad): The generated text is incomplete (missing key information), leaving out crucial details or providing inaccurate information.• 5 (good): The generated text is comprehensive, accurate, and includes all relevant information.</p>
<p>Factuality:</p>
<p>• 1 (bad): The text contains a significant number of factual inaccuracies or false statements, especially in History and Main Idea.For example, Year or people are wrong.• 5 (good): The text is factually accurate, supported by evidence, and free from misinformation.</p>
<p>Pre-selection</p>
<p>We initially engaged four NLP specialists to assess the surveys produced by GPT on 20 handpicked topics, as listed in Tab. 4. The evaluation scores across four model configurations are showcased in Tab. 5. Noting the considerable standard deviations among the evaluations of the four judges, we subsequently opted for two judges with a higher alignment in their scores to assess the entirety of the concepts.</p>
<p>B Comparisons with External Knowledge</p>
<p>We conduct further evaluations with the inclusion of links to Wikipedia articles (GPT-4 OS+Wiki) and information retrieval (GPT OS+IR).In the GPT-4 OS+Wiki set up, we apply Embedchain (Taranjeet Singh, 2023) which supports embedding open sources for LLM querying.We crawl Wikipedia articles to concepts in Surfer 100 datasets, yielding 87 effective links.We then prompt GPT-4 to generate survey articles for the respective 87 topics, providing corresponding Wikipedia links and a sample survey as references.As for the setting GPT-4 OS+IR, we ask GPT-4 to 'Search on the web for helpful information' in the prompt and utilitze the web search APIs.Table 6 shows the comparison results between the GPT-4 with and without external knowledge.It's clear to see that both Wikipedia links and the information retrieval component significantly improve the Rouge scores.Notably, searching for web sources efficiently improve both the MoverScore and UniEval.In summary, external knowledge aids GPT-4 in generating higher-quality survey articles than using internal knowledge only.This suggests that LLMs possess limited proficiency when functioning as an academic search engine.</p>
<p>C More Case Study and Observations</p>
<p>C.1 Understanding of "Survey"</p>
<p>When we give the prompt to GPT models by asking them to write a "survey", they sometimes generate survey articles as desired, but they will write other types of content.For example, as indicated in Fig. 5, it appears that GPT would understand the term "Survey" as the questionnaire.Moreover, even if they are able to generate a survey article in the format, there is still the situation that the generated content is not a typical survey.As shown in Fig. 6, there are inconsequential sentences in an attempt to extend and explain the provided text.For example, it repeats saying this section and participants.But this is mostly observed in the GPT-3.5 zero-shot setting.</p>
<p>C.2 Incomplete Information</p>
<p>In the "History" section, GPT models occasionally produce incomplete evolutionary history, and thus, potentially result in misleading information.For instance, in Fig. 7, when discussing the Knowledge Graph topic, GPT-4 model simply asserts that the term was invented by Google, while the reality is that the concept of Knowledge Graph has a long history, and it is Google that popularized the term.Similarly, in the case of the topic on Decision Trees, although the GPT model yields accurate context, it ignores landmark events and consequently causes misunderstandings.</p>
<p>C.3 Nebulous Sentence Structure</p>
<p>We observe that GPT models frequently construct sentences, especially within the "Application" Section, that employ a rather vague sentence structure, which lacks specificity and can be used in different NLP topics.As shown in Fig. 8, it is evident that GPT models tend to generate similar sentences, such as "The Topic has a wide spectrum of applications" and "The Topic plays a vital role in Natural Language Processing and Natural Language Understanding"; These statements hold significant meaning when "The Topic" is substituted with any NLP topics.</p>
<p>Introduction History</p>
<p>This section will delve into the fundamental concepts and principles behind Hidden Markov Models.</p>
<p>Participants will learn about the core elements of HMMs, such as states, transitions, and observations.They will also be introduced to the notion of hidden states and the probabilistic framework that characterizes HMMs.</p>
<p>Key Ideas</p>
<p>The final section will focus on the diverse range of applications of Hidden Markov Models.Participants will discover how HMMs have been successfully applied in various fields, including speech recognition, bioinformatics, natural language processing, and finance.Real-world examples will be provided, showcasing the practical significance of HMMs and their potential for solving complex problems.</p>
<p>Uses/Applications</p>
<p>Variations Figure 6: An example of an invalid generated survey.</p>
<p>E Experimental Details</p>
<p>In our work, we mainly utilize the paid GPT-4 model to generate Wiki-style survey articles and further explore its capabilities to score the generations.The total cost of these experiments is around 230 USD.</p>
<p>During the automatic evaluation stage, we compute the ROUGE score and BERTScore using the officially provided APIs: rouge3 and bert_score4 .For calculations involving MoverScore, UniEval, and BARTScore, we directly download their source codes.All experiments were performed using the highperformance machine with 4 A100 40GB NVIDIA cards.As the experiments do not involve fine-tuning, for each setting, we were able to finish in a few hours.As for the human evaluation stage, we calculate the Krippendorff's and Kendall's scores with the authorized APIs krippendorff5 and scipy6 .</p>
<p>Ground Truth</p>
<p>The</p>
<p>GPT-4 zero-shot with prompt</p>
<p>Debuted by Google in 2012, Knowledge Graph is a synthesis of graph theory and semantic searching that targets more precise search results by understanding intent and meaning of search queries.Google introduced it to refine their search engine, understanding user queries' context and delivering relevant results.The Knowledge Graph evolution saw ontology and machine learning playing significant roles, with schema verification, information extraction and data alignment offering more precise data interpretations and integration.</p>
<p>Ground Truth</p>
<p>In 1960, Semantic Networks were invented as a knowledge representation framework.The main idea of Semantic Networks was to capture a wide range of entities including real-world objects, events, situations or abstract concepts.In 2001, Tim Berners-Lee coined the term Semantic Web which is an application of Semantic Network combined with the Web.In 2012, Google popularized the term knowledge graph with the announcement of Knowledge Graph.there are very few technical details about its organization, coverage and size, but it is said that it contained 70 billion connected facts by 2016.Today, knowledge graphs are used by companies from a wide range of industries.</p>
<p>GPT-4 one-shot with prompt</p>
<p>Decision Trees were first introduced by Ross Quinlan, a researcher in machine learning, who developed an algorithm known as ID3 (Iterative Dichotomiser 3) dating back to the 1980s.The algorithm was developed in order to create a Decision Tree that predicts the value of a target variable based on several input variables.This was a significant development in the field of Machine Learning as it addressed the problem of handling categorical data and missing values while predicting multi-output concepts.</p>
<p>Topic: Knowledge Graph Section: History Topic: Decision Trees Section: History Model: GPT-4 zero-shot; Topic: Decision Trees; Section: Application Knowledge Graphs have a broad spectrum of applications.They significantly improve the efficiency of search engines, personalized recommendations, innovations in AI, and semantic searches.They also play a vital role in Natural Language Processing tasks such as machine reading and language understanding.Other potential applications include healthcare for an accurate analysis of patient data, e-commerce for efficient product recommendation, social media analytics, and cultural heritage preservation.With tech giants continually enhancing their proprietary knowledge graphs, the use of Knowledge Graphs in various sectors is anticipated to grow exponentially in the future.</p>
<p>Model: GPT-4 one-shot with prompt; Topic: Meta-learning; Section: Application Meta-learning has a wide spectrum of applications.Aside from its notable use in deep learning where neural networks are used, it's also used in reinforcement learning.These tasks include but aren't limited to, robotic control, game playing, and autonomous navigation.Additionally, meta-learning can be used for rapid adaptation in natural language processing and, in particular, natural language understanding.Several companies use meta-learning algorithms for designing personalized recommendations and adaptive chatbots.</p>
<p>Introduction</p>
<p>LSTM found a myriad of applications in fields where sequence or time series data are crucial.(demonstrate domain-specificity) In natural language processing, LSTM has proven effective in machine translation, sentiment analysis, named entity recognition, and text generation due to its ability to capture context dependencies.In computer vision, LSTM has been utilized for video processing and in action recognition tasks.Also, in time series analysis and forecasting including stock market prediction or weather forecasting, LSTM exhibits prime performance.</p>
<p>Uses/Applications</p>
<p>Ground Truth</p>
<p>Hidden Markov models (HMMs) are a way of relating a sequence of observations to a sequence of hidden classes or hidden states that explain the observations.They are a class of a full probabilistic model-the model parameters and the overall sequence 'scores' are all probabilities.They form the foundation for creating probabilistic models of linear sequence 'labeling' problems.From just drawing an intuitive picture, HMMs offer fundamental concepts for building a complex model and are the core of many algorithms in computational sequence analysis, including genefinding, profile searches, multiple sequence alignment, and regulatory site identification.</p>
<p>GPT-4 one-shot with prompt</p>
<p>This specific type of training allows it to create shared representations for syntactically or semantically similar words across different languages.One core concept behind mBERT is that it does not require distinct models for each language; it works on the idea of shared multilingual embeddings, leading to representation where different languages' embeddings should be close to each other if they are translations.</p>
<p>Ground Truth</p>
<p>One important point to note is that while training on many languages, we keep a shared vocabulary for all languages rather than keeping distinct vocab for every language.This saves space and most importantly our model learns the root structure of language and learn the underlying structure rather than just learning of the vocab.</p>
<p>GPT-4 one-shot with prompt</p>
<p>Hidden Markov Models (HMMs) are a statistical model used primarily in the field of machine learning and data analysis.Part of the broader category of Markov chain models (present the category), HMMs offer a way to describe the evolution of observable events that are driven by an internal state which is not directly accessible (present the features).The model's effectiveness has found it a important role in numerous applications, ranging from image recognition, speech recognition, genomics to finance (present the applications).The primary motivation behind HMMs is to provide a framework to infer the missing information from observable data, where every state from the hidden sequence generates an output according to a certain probability distribution related to that state (present the motivation).</p>
<p>Topic: Multilingual BERT Section: Main Idea Topic: Hidden Markov Models Section: Introduction</p>
<p>Previous studies have indicated that LLM-based evaluation methods tend to favor content generated by LLMs (Liu et al., (a) Error Type Distribution.(b) Section Error Distribution.</p>
<p>Figure 2 :Figure 3 :Figure 4 :
234
Figure 2: Error Analysis by types and sections.</p>
<p>Figure 7 :
7
Figure 7: Two example surveys with incomplete information.</p>
<p>Figure 8 :
8
Figure 8: Example surveys with nebulous sentence structure.</p>
<p>Figure 9 :
9
Figure 9: An example of the high-quality survey.</p>
<p>Figure 10 :
10
Figure 10: Two examples showing that the generated output is better than the ground truth.</p>
<p>Table 1 :
1
Automatic evaluation scores: we compare ROUGE, BERTScore, MoverScore, UniEval, and BARTScore on different settings.The superior scores among the same models are underlined, while the highest scores across all models and settings are highlighted in bold.<em> We use plugins including A&amp;B Web Search and Keymate.ai.
13B ZS27.65 7.81 25.22 85.30 84.73 85.0155.3676.03-4.78LLaMa2-13B OS26.53 7.01 24.39 84.86 84.43 84.6554.9471.98-4.81LLaMa2-13B DP28.23 7.68 25.83 85.18 85.12 85.1455.4274.57-4.65LLaMa2-13B OSP 25.84 6.66 23.67 84.51 84.55 84.5354.6569.23-4.74LLaMa2-70B ZS27.77 7.59 25.30 85.05 84.82 84.9355.3474.06-4.73LLaMa2-70B OS29.69 8.49 27.39 85.72 85.49 85.6055.6371.46-4.48LLaMa2-70B DP28.74 8.06 26.29 85.31 84.98 85.1455.4972.36-4.67LLaMa2-70B OSP 27.74 7.80 25.48 85.32 85.04 85.1855.5272.92-4.68PaLM2 ZS27.95 8.95 25.99 85.28 84.61 84.9455.2172.69-4.76PaLM2 OS28.81 9.05 26.90 85.16 84.71 84.9355.3572.73-4.68PaLM2 DP28.77 9.13 26.65 85.27 84.66 84.9655.3172.41-4.75PaLM2 OSP28.71 9.34 26.67 85.14 84.61 84.8755.2872.72-4.74GPT-3.5 ZS26.60 6.30 24.36 85.57 84.68 85.1255.4781.31-4.75GPT-4 ZS26.72 6.61 24.35 85.42 85.39 85.4055.7175.24-4.66GPT-4 OS30.09 7.98 27.71 86.01 86.15 86.0855.9874.80-4.38GPT-4 OSP31.47 8.62 29.04 86.1986.44 86.3156.0475.55-4.28</em>GPT-4 OS+IR31.96 9.43 29.60 86.27 85.88 86.0756.4478.56-4.38Evaluator Readability Relevancy Redundancy Hallucination Completeness FactualityHuman4.950.304.880.474.770.534.840.484.290.684.800.55MeanSTDGPT-44.840.324.670.504.850.344.860.333.930.424.560.51G-Eval4.770.644.630.684.270.744.940.514.260.764.760.66Human0.4196.960.4787.870.3568.680.4182.820.5566.660.5982.82IAA%GPT-40.0969.690.3564.640.00372.720.0875.750.3270.700.4563.63G-Eval0.0649.490.2538.380.0132.320.00895.950.4233.330.0258.58</p>
<p>Table 3 :
3
The Kendall's τ correlation coefficient and pvalue between human and GPT-4.</p>
<p>Table 4 :
4
The 20 selected concepts in pre-selection stage.
BERTAutoencodersClusteringDecision TreesEnsemble Learning Gaussian Mixture ModelGenerative Adversarial Network Gradient BoostingHidden Markov ModelsKnowledge GraphsLanguage Modeling Long Short-Term Memory NetworkMaximum Marginal RelevanceMeta LearningMultilingual BERTPerceptronRelation Extraction Residual Neural NetworkRMSprop OptimizerSentiment AnalysisModelReadability Relevancy Redundancy Hallucination Completeness Factuality MeanSTD MeanSTD MeanSTD MeanSTD MeanSTD MeanSTDGPT-3.5 ZS4.010.983.661.613.621.043.821.182.770.943.560.83GPT-4 ZS4.560.654.250.764.200.694.520.793.500.713.910.92GPT-4 ZPS 24.580.724.410.754.030.814.560.643.930.694.070.93GPT-4 OPS4.600.604.350.794.200.644.450.783.900.704.961.07</p>
<p>Table 5 :
5
Human evaluation scores on 20 topics of four human experts.</p>
<p>first regression tree was invented in 1963 (AID project, Morgan and Sonquist).The first publication on decision trees was in 1966 (by Hunt).The first classification tree appeared in the THAID project (by Messenger and Mandell).In 1974, Statistics professors Leo Breiman and Charles Stone from Berkeley and Jerome Friedman and Richard Olshen from Stanford started developing the classification and regression tree (CART) algorithm.In 1977, Breiman, Stone, Friedman, and Olshen invented the first CART version.</p>
<p>Ming Zhong,Yang Liu, Da Yin, Yuning Mao, Yizhu  Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and  Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2023-2038, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
ZPS means zero-shot with description prompt.
https://pypi.org/project/rouge/
https://github.com/Tiiiger/bert_score
https://pypi.org/project/krippendorff/
https://scipy.org/
AcknowledgementsThis work was partially supported by the Deutsche Forschungsgemeinschaft (DFG) through the priority program RATIO (SPP-1999), Grant No. 376059226.It was also supported by the Japan Society for the Promotion of Science (JSPS) KAK-ENHI, Grant Number 24K20832.We would like to thank these funding agencies for their generous support.Additionally, we would like to thank the anonymous reviewers for their helpful feedback.C.4 High-quality SurveyWe also present a high-quality generated survey in Fig 9.It is designed to read and understand easily, providing readers with comprehensive and detailed information.The example survey on LSTM is wellstructured, with a summary provided in the first sentence and followed by the detailed explanation in each section.Specially, when discussing applications, it demonstrates a high level of domain specificity.Most importantly, the generated information is both accurate and concise.C.5 Going Beyond the Ground TruthGPT-4 based methods maintain an overall high-quality response regarding all aspects.We show two examples by comparing the GPT-4 one-shot with prompt setting result with the ground truth in Fig.10.In the first topic, multilingual BERT (mBERT), GPT successfully points out that the key idea behind mBERT is mapping words from distinct languages into a shared embedding space.However, the ground truth only mentions shared vocabulary, which is superficial.In the second example, Hidden Markov Models (HMMs), the GPT response is more precise and more complete than the ground truth.One can find that the content flow is present as algorithm category → features → applications → motivation (highlighted in bold and italicized words).In contrast, the ground truth texts spend a lot of words to which category HMM belongs, including many terminologies which is less informative.Limitations of Ground TruthWe refer to this previous work(Li et al., 2022)on how the ground truth was generated.In general, the human writer was asked to rely on web data when writing the survey article; while these data were collected in the year 2021, it may be hard to say if it is a fair ROUGE score comparison with GPT models in Tab. 1.While the ground truth may not be a perfect reference, in this work, we focus more on human evaluation and case studies.D Potential RisksSole dependence on LLMs for educational content can lead to a homogenization of information and lack the nuanced understanding that human experts bring.If not properly vetted, the occasional factual errors made by GPT-4 could lead to the propagation of misinformation, especially detrimental in an educational context.Over time, heavy reliance on automated systems might diminish the role of human experts in content creation, leading to a potential loss of rich, experience-based insights.The observed systematic bias in GPT evaluations can lead researchers to draw incorrect conclusions about the quality of content, potentially impacting future research and educational endeavors.In this section, participants will explore the different variations and extensions of Hidden Markov Models.The section will cover topics such as continuous HMMs, time-inhomogeneous HMMs, and higher-order HMMs.Participants will gain insights into the modifications made to basic HMMs, enabling them to handle more complex real-world scenarios.In this section, participants will explore the historical development of Hidden Markov Models.Starting from its early roots in the 1960s, the section will highlight key contributions and milestones in the field.Participants will gain an understanding of how HMMs have evolved over time and their relevance in various disciplines.This section aims to introduce the concept of Hidden Markov Models (HMMs) to the participants.HMMs are powerful statistical models used to represent systems that undergo successive probabilistic transitions.This section will briefly explain the basics of HMMs and provide the necessary background information for the subsequent sections.Over the years, several variations of LSTMs have been introduced to maximize efficiency and minimize computational resources.The Gated Recurrent Unit (GRU) is a popular variation, which merges the cell state and hidden state, and uses two gates.Another important variation is the Peephole LSTM, which lets the gate layers peep into the cell state.The Convolutional LSTM replaces simple multiplication operations with convolutional operations, making it especially effective for spatial temporal data.Introduced by Sepp Hochreiter and Jürgen Schmidhuber in 1997, the LSTM addresses the vanishing gradient problem experienced by traditional RNNs.The main novelty of LSTM was the incorporation of gating units.These units permit or restrict information from flowing through the sequence chain, thereby effectively containing the exploding or vanishing gradient problem.Since its inception, LSTM has served as a prerequisite for various network designs addressing further issues and limitations.HistoryThe core concept behind LSTM networks is the cell state, a controllable information pipeline that carries the required details from early input sequences to later ones, making provision for long-term dependencies.(Accurate and easy-to-understand) LSTM adjusts the cell state through carefully designed structures called gates, which are capable of removing or adding information to the cell state.There are three main types of gates: forget gate deciding what information should be discarded, input gate deciding what new information should be stored in the cell state, and output gate deciding what information should be utilized.
Characterizing attribution and fluency tradeoffs for retrievalaugmented large language models. Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, Yunhsuan Sung, arXiv:2302.055782023arXiv preprint</p>
<p>Artificial intelligence in scientific writing: a friend or a foe?. Signe Altmäe, Alberto Sola-Leyva, Andres Salumets, BioMedicine Online. 2023</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, 2023Palm 2 technical report</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>A coefficient of agreement for nominal scales. Educational and psychological measurement. Jacob Cohen, 196020</p>
<p>Diffuser: Efficient transformers with multihop attention diffusion for long sequences. Aosong Feng, Irene Li, Yuang Jiang, Rex Ying, ArXiv, abs/2210.117942022</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>A system for summarizing scientific topics starting from keywords. Rahul Jha, Amjad Abu-Jbara, Dragomir R Radev, Annual Meeting of the Association for Computational Linguistics. 2013</p>
<p>The perils of using mechanical turk to evaluate open-ended text generation. Marzena Karpinska, Nader Akoury, Mohit Iyyer, arXiv:2109.068352021arXiv preprint</p>
<p>Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. Mina Lee, Percy Liang, Qian Yang, ; Irene Li, Alex Fabbri, Rina Kawamura, Yixin Liu, Xiangru Tang, Jaesung Tae, Chang Shen, Sally Ma, Tomoe Mizutani, Dragomir Radev, 10.1145/3491102.3502030Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22. the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22New York, NY, USA; Marseille, FranceAssociation for Computing Machinery2022. 2022Proceedings of the Thirteenth Language Resources and Evaluation Conference. European Language Resources Association</p>
<p>Can large language models provide useful feedback on research papers?. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel A Mcfarland, James Zou, 2023a large-scale empirical analysis</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023b</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Stanza: A python natural language processing toolkit for many human languages. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher D Manning, arXiv:2003.070822020arXiv preprint</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, arXiv:2302.00083Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation. Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li, ArXiv, abs/2402.14293Advances in Neural Information Processing Systems. 2024bLeveraging large language models for concept graph recovery and question answering in nlp education</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Llm calibration and automatic hallucination detection via pareto optimal selfsupervision. Theodore Zhao, Mu Wei, J Samuel Preston, Hoifung Poon, 2023</p>
<p>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
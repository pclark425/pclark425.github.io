<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2332 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2332</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2332</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-162168885</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1905.08883v3.pdf" target="_blank">Explainable Machine Learning for Scientific Insights and Discoveries</a></p>
                <p><strong>Paper Abstract:</strong> Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2332.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2332.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BlockTower-Intuition (ResNet/GoogLeNet)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning physical intuition of block towers using convolutional neural networks (ResNet-34, GoogLeNet, PhysNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised CNN models (ResNet-34, GoogLeNet) trained on simulated video/image data to predict physical stability (will a tower fall) and a mask-prediction network (PhysNet) to estimate block trajectories; used to derive intuitive physics predictions from visual data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning physical intuition of block towers by example.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Intuitive physics / physical scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict whether stacked objects (wooden block towers) will remain stable or fall and predict trajectories of blocks during collapse from images or video frames.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate — synthetic and real image/video datasets were used; synthetic datasets can be generated abundantly whereas real annotated collapse data are scarcer; training labels are supervised (stability/fall), accessibility moderate (research datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured image and video data (RGB frames), with instance segmentation masks for PhysNet; time series (video) for trajectory prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High visual and physical complexity — nonlinear dynamics, occlusions, combinatorial object arrangements; requires reasoning about object interactions and future dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging subfield: established vision architectures exist but domain-specific physics reasoning methods are developing; domain knowledge (intuition of physics) is qualitative rather than fully formalized.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — scientific insight benefits from interpretable intermediate outputs (masks/trajectories), but black-box prediction of stability can be useful; PhysNet adds more interpretable structure.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional neural networks (ResNet-34, GoogLeNet), PhysNet mask-prediction CNN</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised CNN classifiers (ResNet-34, GoogLeNet) trained on labeled images/videos to predict binary stability; PhysNet is a deeper mask-prediction network with alternating upsampling and convolution designed to capture block arrangement and motion, trained on instance segmentation masks to output per-block masks and hence trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning / vision models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable — CNNs map well from images to stability labels; PhysNet architecture adapted to capture object structure and dynamics. Limitations include domain shift between synthetic and real scenes and lack of explicit physics laws.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked well on synthetic data (outperformed humans) and produced comparable results on real data; PhysNet provided improved trajectory/motion predictions and improved interpretability via masks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables automated reasoning about physical stability from images, useful in robotics, simulation validation, and cognitive modeling; helps bridge perception and physical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to human subjects and baseline vision architectures; PhysNet designed specifically to reason about block arrangement and outperformed humans on synthetic data and matched on real data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of synthetic simulated data for training, architecture tailored to capture object composition and motion (design transparency), and use of instance-level supervision (masks) improved interpretability and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised CNNs trained on simulated visual data can predict intuitive physical outcomes; adding design choices that encode object structure (mask prediction) improves interpretability and predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2332.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperspectral-GAN Forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperspectral plant disease forecasting using cycle-consistent generative adversarial networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of cycle-consistent GANs to predict how microscopic hyperspectral images of barley plants change day-to-day, effectively forecasting disease spread in image appearance space without explicit biological parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hyperspectral plant disease forecasting using generative adversarial networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Plant pathology / remote sensing / phenotyping</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict temporal evolution of microscopic hyperspectral plant images to forecast disease progression based on observational image sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate — time-series hyperspectral imagery collected over days; labeled biological parameters may be limited, but image sequences provide self-supervised signals; data quality depends on imaging modality.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional image data (hyperspectral cubes), temporal sequences (time series of images), unstructured pixel-wise spectral signatures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — high-dimensional spectral data, complex disease appearance changes over time, non-linear mappings between days, and limited mechanistic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Developing — imaging and hyperspectral sensing are established, but machine-learning-based forecasting of disease appearance is relatively new.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — paper uses image-space forecasting without incorporating biological mechanistic models, so black-box predictions were acceptable though scientific interpretation requires domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Cycle-consistent generative adversarial networks (CycleGAN-style)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>GANs trained to map images from day t to day t+1 (and vice versa) with cycle-consistency losses to enforce plausible reversible mappings, learning appearance evolution without explicit biological parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for predicting image appearance changes where mechanistic labels are scarce; limitations include lack of explicit biological interpretability and dependence on sufficient image sequence coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Produced highly probable image-based appearances across days demonstrating temporal evolution capture; lacks integration with mechanistic biological models so scientific causal claims are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables short-term visual forecasting of disease progression useful for monitoring and planning; could be combined with mechanistic models for improved interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Not directly compared to physics/biologically-informed models in the survey text; approach trades mechanistic interpretability for image-level predictive ability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of sequential hyperspectral image data and the capacity of GANs to model complex, high-dimensional appearance changes without labeled mechanistic variables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Generative models can forecast high-dimensional hyperspectral image evolution without explicit mechanistic parameters, yielding useful visual predictions but limited scientific causal insight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2332.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FunctionalGlass-ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerating the design of functional glasses through neural-network modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks trained on several hundred silicate compositions to predict manufacturing and end-use properties (e.g., liquidus temperatures), supporting materials design by screening novel compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating the design of functional glasses through modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Materials design / inorganic chemistry (glass formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict liquidus temperatures and other properties of silicate glass compositions to identify optimized formulations for manufacturing and end-use.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited to moderate — several hundred measured composites available for training; experimental data acquisition is costly, so labeled data is relatively scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured numeric data (composition vectors with up to eight components) and scalar target properties (liquidus temperature), i.e., tabular composition-property data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high — high-dimensional compositional space with nonlinear composition-property relationships and sparse sampling across the combinatorial space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established domain knowledge (physics-driven models exist) but data-driven screening is emerging; practitioners have substantial domain expertise and alternative simulation models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — scientific plausibility is important; the ML model is used alongside physics-driven models and domain checks rather than as sole explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Feedforward neural networks (regression NNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised regression NNs trained on composition-property pairs (several hundred samples) to predict liquidus temperatures and guide selection of candidate compositions; used in a workflow that also consults physics-driven models outside the ML chain.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable — NNs can approximate complex composition-property mappings given available data; limited by training sample size and need for extrapolation to novel chemistries.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enabled efficient screening and estimation of properties, reducing costly trial-and-error; used in hybrid workflows with physics models for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Speeds materials discovery by narrowing candidate sets, reducing experimental cost and time; provides practical utility in materials formulation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Used in combination with physics-driven models rather than strictly compared; hybrid use ensures cross-validation with established scientific models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of domain knowledge (physics-based models) outside the ML chain for validation, focused property targets, and careful feature representation of compositional vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised NNs trained on modest-sized experimental datasets can accelerate materials screening when combined with physics-based validation, trading pure interpretability for practical utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2332.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiddenPhysics-NNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hidden physics models / physics-informed neural networks for learning nonlinear PDEs and dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural-network-based approaches that incorporate differential-equation structure or physics constraints to learn dynamics or PDEs from scattered spatio-temporal data, enabling forecasting and discovery of governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hidden physics models: Machine learning of nonlinear partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational physics / dynamical systems / PDE identification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Infer the form of governing PDEs or learn dynamic evolution operators from space-time scattered data to forecast future states and uncover underlying dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies — can work with scattered simulation or experimental spatio-temporal samples; some methods are demonstrated on numerical simulation data (e.g., Burgers', Schrödinger, Navier–Stokes).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal continuous data (scattered measurements in space and time), numerical simulation outputs, potentially noisy observations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — PDEs represent high-dimensional, nonlinear, and potentially chaotic dynamics; identification requires resolving differential operator structure and derivatives from finite data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging research area blending ML and numerical analysis; prior mathematical theory exists for PDEs but data-driven discovery is newer.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — goal includes recovering mechanistic/physical forms of PDEs or consistent dynamic models for scientific insight.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-informed neural networks (PINNs) / hidden physics models; structured NNs learning differential operators</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks represent solution fields or terms in PDEs; physics constraints (residuals of PDEs) are incorporated in the loss to enforce consistency, enabling recovery of dynamic operators; used to forecast future states with reported relative L2 errors ~1e-3 on benchmark PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / supervised and unsupervised hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for problems where PDE form is partly known or where enforcing physics residuals improves generalization; requires sufficiently informative data and appropriate regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported relative L2 errors on benchmark PDE forecasting tasks up to order 10^-3 in published benchmarks (Burgers', nonlinear Schrödinger, Navier–Stokes).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effectively learned dynamics and produced accurate forecasts on simulated PDE benchmarks; model's generality allows learning underlying operators but direct interpretability of NN parameters is limited without additional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables discovery and forecasting of governing dynamics from data, promising for science domains governed by PDEs (fluid dynamics, materials, geoscience); aids in reducing reliance on handcrafted simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to purely data-driven black-box models due to physics constraints improving plausibility and stability; also complementary to sparse-regression PDE identification methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Enforcing physical constraints in loss functions, using simulation benchmark datasets for validation, and careful treatment of differential operators/regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding PDE structure or physics residuals into neural-network training yields accurate, physically plausible models that can both forecast dynamics and aid discovery of governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2332.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exoplanet-Deep (Multi-source NN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific domain knowledge improves exoplanet transit classification with deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-source neural network architecture that integrates centroid time-series and stellar parameters into the model to improve exoplanet transit detection/classification from light-curve data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scientific domain knowledge improves exoplanet transit classification with deep learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Astrophysics – exoplanet detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Classify and detect exoplanet transit signals in stellar photometric time-series data, distinguishing true planets from false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relatively abundant labeled time-series data from space telescopes (e.g., Kepler); labels exist from validated planet catalogs though some classes can be imbalanced.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series (light curves), auxiliary tabular stellar parameters, and centroid motion time-series (multimodal inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate — signals can be weak and contaminated by stellar/systematic noise; non-linear detection boundaries and class imbalance complicate learning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established observational domain with extensive prior knowledge and catalogs; ML methods are now applied for classification tasks with strong expert oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — for discovery and validation, interpretability and domain-informed features help trust and reduce false positives; pure black-box less acceptable for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Multi-source neural network with concatenated hidden-layer outputs and centroid/time-series integration</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Deep NN architecture that inputs multiple aligned sources (light curve, centroid time series duplicated across inputs, and stellar parameters concatenated at hidden layer) to guide learning of transit features and context; design choices reflect domain knowledge about relevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable — domain-informed architecture improves detection and reduces false positives; must handle class imbalance and instrumentation biases.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Integration of domain knowledge (centroid duplication, stellar parameter concatenation) improved classification accuracy compared to models without such design choices; provided more scientifically plausible decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves automated vetting of exoplanet candidates, increasing throughput of discovery and reducing manual validation burden; potentially discovers novel candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to baseline architectures lacking domain-informed inputs; domain-aware design outperformed generic networks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit incorporation of astrophysical domain knowledge into model input representation and architecture, leveraging abundant labeled catalogs for supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Designing network architectures that incorporate domain-specific inputs and representations materially improves scientific classification tasks over generic deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2332.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NewtonianNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Newtonian neural networks for long-term motion prediction (Newtonian image understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stream CNN architecture mapping static images of objects to states in a set of pre-defined Newtonian scenarios (12 scenarios) derived from simulated videos, enabling interpretable long-term motion prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Newtonian image understanding: Unfolding the dynamics of objects in static images.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Physical dynamics inference / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>From a single static color image, infer the long-term motion trajectory of an object by mapping it to a discrete set of Newtonian scenario templates parameterized by physical variables.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate — image datasets augmented by game-engine-simulated videos representing the Newtonian scenarios; labels map images to scenario states.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Static RGB images and simulated video-derived convolutional filter representations (paired modalities), i.e., multimodal image and filter data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — predicting long-term dynamics from a single image is underdetermined; mapping to discrete scenario classes reduces complexity but remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging — combines classical physics abstractions with data-driven image understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for scientific insight — mapping images to interpretable Newtonian scenarios provides causal/mechanistic abstraction rather than pure black-box forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Two-stream CNN (image encoder + video-derived scenario encoder) — Newtonian NNs</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Parallel CNNs: one encodes the input image; the other derives convolutional filters from game-engine simulation videos representing 12 Newtonian scenarios; coupling yields interpretable assignment of image to scenario and thus physical parameters for trajectory prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning with interpretable scenario mapping (hybrid physics-abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when a set of representative physical scenarios can be pre-defined; reduces problem ambiguity by constraining outputs to interpretable physics templates.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides interpretable mapping from images to physical scenarios and enables plausible long-term motion predictions; success depends on coverage of the pre-defined scenario set.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful for robotics and visual reasoning tasks requiring interpretable motion forecasts; represents a pathway to combine simulated physics scenarios with perception.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Design contrasts with direct regression-based motion predictors by using scenario abstraction; reported to be more interpretable though not directly compared quantitatively in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of simulated scenario videos to create an interpretable scenario space and architecture that couples visual encoding with scenario templates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Mapping visual input to a discrete, physically interpretable scenario space enables mechanistic motion prediction from single images while improving interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2332.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-Loss Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Label-free supervision of neural networks with physics and domain knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that supervise neural networks by enforcing physics constraints (e.g., conservation laws) in loss functions rather than relying on labeled outputs, enabling training in data-limited settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Label-free supervision of neural networks with physics and domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Physics-based simulation / inverse problems / scientific modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train models when labeled outputs are scarce by using known physics (constraints or simulators) to create supervision signals or consistency losses during training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often limited labeled data; physics constraints provide alternative supervisory signals; data can be unlabeled observations or partial measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Varies: could be spatio-temporal measurement data, sensor observations, or other continuous measurements; sometimes simulation-generated.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — enforcing differential or conservation constraints inside learning introduces complex loss landscapes and requires differentiable physics representations or surrogate losses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging technique with growing adoption where strong domain physics exist; relies on substantial domain knowledge representation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — enforcing physics aligns models with mechanistic expectations and improves scientific plausibility; goal often includes mechanistic consistency rather than only predictive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-constrained neural network training (physics-informed losses / label-free supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks trained with loss terms encoding physics constraints (e.g., divergence-free conditions, conservation laws, or residuals of known equations) that penalize violation of physical laws, enabling learning without explicit labeled targets.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed ML / hybrid supervised-unsupervised</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where mechanistic equations or constraints are known; particularly useful in small-data regimes and for improving physical plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to produce plausible and physically consistent outputs in fluid simulation and other domains; effectiveness depends on fidelity of the physics constraints and their numerical implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Can drastically reduce labeling needs and improve generalization and scientific consistency of ML models across physics-driven domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over purely data-driven losses by enforcing domain constraints; compared qualitatively against generic architectures where physics-aware losses yielded more plausible results.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability and correct encoding of domain physics, differentiable formulations of constraints, and careful balancing of physics and data-driven loss components.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Using physics constraints as supervision compensates for scarce labeled data and yields scientifically consistent models, making ML viable for many scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2332.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RANS-Embedded-Invariance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reynolds-averaged turbulence modelling using deep neural networks with embedded invariance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep neural networks for turbulence closure modeling that embed invariance properties (rotational, reflectional) via specialized network layers (higher-order multiplicative layers) to improve generalization and physical plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational fluid dynamics / turbulence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict Reynolds stresses or closure terms for RANS turbulence models to improve predictive accuracy of turbulent flows while respecting physical invariances.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate — training data from high-fidelity simulations or experiments; variety of geometries limited which can challenge generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured spatial fields (flow feature fields), tensor-valued quantities (Reynolds stresses), and possibly parametric geometry descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high — turbulent flows are highly nonlinear, multi-scale, and sensitive to geometry; learning closure terms is innately complex.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established theoretical and simulation foundations exist; ML integration is an active research area seeking to augment traditional turbulence models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — physical invariances and constraints are critical for scientifically plausible turbulence predictions and for generalization beyond training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep neural networks with embedded invariance layers (higher-order multiplicative layers / tensor-invariant representations)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Design-transparent architectures incorporate invariant representations via multiplicative layers and tensor-basis models to ensure outputs respect known symmetries; trained supervisedly to predict closure quantities from flow features.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning with physics-informed architectural constraints</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and beneficial: embedding invariances improves accuracy and generalization, demonstrated on test cases with different geometries than training.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported significantly more accurate predictions compared to generic NN architectures on test cases (specific numerical metrics not provided in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Models with embedded invariance outperformed generic architectures and generalized better to unseen geometries, indicating improved physical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potential to improve RANS predictions in engineering workflows, reduce reliance on empirical closures, and enable more reliable ML-augmented CFD for novel geometries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to generic NN architectures; invariant-embedded networks produced more accurate and physically consistent predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Architectural encoding of known invariances and symmetries, use of physics-based tensor bases, and supervised training on representative high-fidelity data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Architectures that enforce known physical symmetries substantially improve ML model accuracy and generalization for complex fluid-dynamics closure problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2332.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-Calibrate-OPV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian process Bayesian calibration of computational predictions to experimental organic photovoltaic materials data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Gaussian processes with chemically informed similarity measures (Tanimoto) as a Bayesian calibration to map computational quantum-chemistry predictions to experimental observables, providing uncertainty-aware calibrated predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Materials informatics / computational chemistry (organic photovoltaics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Calibrate theoretical/computational property predictions (e.g., from quantum chemistry) to match experimental measurement distributions for screening candidate molecules for organic photovoltaics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate — computational databases are large, experimental calibration points fewer; survey reports efficient screening of >51,000 molecules with 838 high-performing identified.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured molecular descriptors and fingerprints (chemical representations), scalar target properties (computed and experimental values), tabular datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate — mapping systematic computational biases to experimental outcomes requires modeling complex, non-linear deviations and domain-specific similarity structures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established computational chemistry foundations with growing ML augmentation; domain expertise available to craft similarity measures and priors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — calibration aims to correct systematic discrepancies rather than derive new mechanisms; uncertainty quantification is important for applicability warnings.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gaussian Processes with Tanimoto-similarity-informed priors (Bayesian calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Non-parametric GP regression models the deviation between computational predictions and experimental values; chemical similarity (Tanimoto) incorporated in kernel/prior construction; outputs provide predictive mean and confidence indicating applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Bayesian supervised learning / probabilistic calibration</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited — GPs provide uncertainty estimates and leverage molecular similarity to improve calibration, indicating when model predictions are unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Enables screening of over 51,000 molecules and identification of 838 high-performing candidates (from survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provided calibrated, uncertainty-aware predictions and flagged out-of-distribution cases; design transparency via similarity-informed prior improved trust in applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves virtual screening reliability, speeds materials discovery pipelines by prioritizing experimentally promising candidates while providing confidence indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers advantages over uncalibrated computational predictions by quantifying deviations and uncertainties; compared implicitly to raw theoretical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of domain-specific similarity kernels, probabilistic modelling providing uncertainties, and integration of experimental prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Probabilistic calibration with chemically informed GPs makes computational screening more reliable by correcting systematic biases and returning applicability-confidence for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2332.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy (SparseID)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Identification of Nonlinear Dynamical Systems (SINDy) for discovering governing equations from data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sparse regression over a library of candidate nonlinear functions to identify parsimonious governing equations (ODEs/PDEs) from time-series data, yielding interpretable mechanistic equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering governing equations from data by sparse identification of nonlinear dynamical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Dynamical systems / model discovery (biology, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Infer low-dimensional dynamical system equations (ordinary differential equations) that govern observed time-series data, enabling mechanistic understanding and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies — demonstrated on synthetic and experimental time-series; needs sufficient temporal sampling but can succeed with limited dimension if dynamics are sparse in chosen library.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series (temporal sequences) of state variables; often multivariate sequential measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high — complexity depends on the true system's sparsity in the candidate dictionary; recovery from noisy or corrupted data is challenging but addressed by robust sparse methods.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing approach with strong theoretical grounding in sparse regression and system identification; practitioners provide candidate libraries using domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — primary goal is to obtain interpretable, mechanistic equations (causal/dynamical form) rather than black-box predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Sparse regression over function dictionaries (SINDy)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Construct a large library of candidate nonlinear functions of the state variables and use sparsity-promoting regression (L0/L1-like / sequential thresholding) to select a small subset that explains dynamics, yielding explicit differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Sparse model discovery / interpretable ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when dynamics are sparse in a chosen basis and time-series quality is sufficient; less applicable if true dynamics are not representable in the library or data are extremely noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Successfully recovers governing equations in many benchmark problems and biological network inference tasks; advantage is interpretability and parsimony but sensitive to library choice and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scientific discovery: yields explicit governing equations enabling mechanistic insight, reduced-order modeling, and improved scientific understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides more interpretable, parsimonious models than black-box NNs; compared favorably to other data-driven discovery approaches for sparse systems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate choice of candidate function dictionary driven by domain knowledge, sparsity enforcement, and sufficient temporal resolution and signal-to-noise in data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Sparsity-promoting identification methods can extract interpretable governing equations from time-series when suitable candidate functions and data quality are available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2332.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciNet (VAE-based concept discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciNet: a modified variational autoencoder learning minimal representations that map to physical parameters (concept discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoencoder-like architecture constrained to learn low-dimensional latent representations whose activations correspond linearly to physical parameters, enabling recovery of explanatory factors from experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Physics / experimental concept discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>From experimental observation data, learn compact latent representations that encode physical parameters and explanatory factors for subsequent prediction and interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies — demonstrated on experimental and simulated datasets; method benefits from datasets where physical parameters influence observations but may be limited by sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured observational data (could be images or numeric measurements) compressed into low-dimensional latent vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate — complexity arises in disentangling latent factors that align with known physical parameters, requiring architectures with appropriate inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging approach in interpretable representation learning for physics discovery; relies on combination of ML and domain verification.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — designed to produce mechanistically meaningful latent variables to support scientific explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Modified variational autoencoder (SciNet)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>VAE-like encoder-decoder with bottleneck constrained to small number of neurons, encouraged (via training objectives/architecture) to align latent activations linearly with physical parameters, enabling interpretability of latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / representation learning (autoencoder-based)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for tasks seeking compact, interpretable latent representations that can be mapped to physical concepts; requires careful architecture and analysis to confirm alignment with true physical parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated ability to recover physical parameters and provide simpler representations useful for downstream analysis; interpretability depends on alignment and post-hoc verification with domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Facilitates discovery of latent physical factors from experimental data, aiding hypothesis generation and simplified modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers more explicit latent-parameter interpretability than generic autoencoders; compared conceptually to architectures with designed bottlenecks representing known parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Architectural constraints enforcing low-dimensional bottleneck, alignment analysis between latent neurons and physical parameters, and domain validation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Constraining representation learning to low-dimensional, physically aligned latent spaces enables extraction of explanatory factors from data, bridging representation learning and scientific concept discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2332.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepTensorQC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep tensor neural networks for quantum-chemical prediction with interpretability (deep tensor networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep tensor neural networks that predict molecular energies up to chemical accuracy while enabling interpretability via local chemical potentials and 3D response maps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantum-chemical insights from deep tensor neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Quantum chemistry / materials</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict molecular energies and other quantum-chemical properties from molecular structure with high accuracy and provide interpretable local insights.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate to large — high-throughput quantum-chemical datasets exist for training; labeled computed energies available from quantum chemistry calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph- or geometry-based molecular representations (atomic coordinates, types), 3D spatial structure data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — molecular energy landscapes are high-dimensional and sensitive to 3D geometry; accurate prediction requires modeling complicated many-body interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established field with extensive computational methods; ML methods are well-adopted and growing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — chemists require interpretable outputs to connect ML predictions with chemical intuition (e.g., reactivity, aromaticity).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep tensor neural networks (DTNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks operating on atomic features and pairwise/interatomic tensors to predict total molecular energy; interpretability via computation of local chemical potentials (sensitivity analyses) and 3D response maps indicating reactive/stable regions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (graph/geometry-aware networks)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for property prediction tasks where abundant computed labels exist; local interpretability offers scientific insights.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported prediction accuracy up to chemical accuracy in cited work (i.e., errors within chemical accuracy thresholds), specific metrics not quoted in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides accurate predictions and localized interpretability enabling identification of chemical motifs (e.g., aromatic ring stability); balances performance with domain insight.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Accelerates computational screening and offers chemical interpretability to guide synthesis and theory; bridges ML and domain-expert analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to other ML and quantum-chemical surrogate models by achieving high accuracy with interpretable local potentials.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Model architectures that respect molecular structure and 3D geometry, and interpretability tools (local chemical potential) aligned with chemical intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Geometry-aware deep tensor networks can reach chemical-accuracy predictions while providing local interpretable measures that map to chemical concepts, enhancing scientific utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2332.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2332.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-Genomics/Healthcare</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-based neural models for genomics and healthcare (e.g., Attend-and-Predict, AttentiveChrome, RETAIN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of attention mechanisms in neural networks to highlight important sequence motifs or clinical visits/features, improving interpretability of predictions in genomics and patient outcome modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attend and predict: Understanding gene regulation by selective attention on chromatin.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Genomics and clinical predictive modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify sequence motifs or genomic regions relevant to gene regulation and predict clinical outcomes by learning which parts of sequences or patient histories the model attends to.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies — genomic datasets and chromatin data can be large (abundant), while high-quality clinical labeled histories may be more limited; labels (e.g., clinical outcomes) often available.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequential genomic data (DNA sequences), multi-visit electronic health record time-series (categorical/numeric), multimodal clinical features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — long-range dependencies in sequences, heterogeneity in patient records, class imbalance, and need to map attention weights to biologically meaningful motifs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature experimental genomics and clinical domains with growing adoption of interpretable deep models; attention mechanisms are an established ML tool.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — for biological discovery and clinical trust, interpretability (which motifs/visits drive predictions) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Attention-based neural networks (sequence-attention, hierarchical attention, RETAIN reverse-time attention)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural architectures incorporate attention modules that produce weights over positions/time steps; attention maps are visualized to identify important sequence motifs or patient visits; hierarchical attention stacks allow multi-scale interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning with attention for interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for tasks where identifying salient input regions is important; attention provides human-interpretable cues though attention weight interpretation has caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Attention models successfully highlighted biologically or clinically relevant regions and provided interpretable cues for experts; architectures aided motif discovery and clinical variable importance assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables extraction of biologically meaningful motifs and clinically actionable insights, supporting hypothesis generation and decision support with interpretable evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to vanilla NNs, attention adds interpretability and often improves performance on sequence and time-series tasks; caveats exist about equating attention weights directly to causal importance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Hierarchical and task-specific attention design, domain validation of attention-derived motifs/visits, and adequate training data to learn meaningful attention patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Attention mechanisms provide a practical route to interpretable neural predictions in genomics and healthcare, but require domain validation to ensure scientific meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Machine Learning for Scientific Insights and Discoveries', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics informed deep learning (part II): Data-driven discovery of nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>Machine learning for molecular and materials science <em>(Rating: 2)</em></li>
                <li>Machine learning and the physical sciences <em>(Rating: 2)</em></li>
                <li>Theory-guided data science: A new paradigm for scientific discovery from data <em>(Rating: 2)</em></li>
                <li>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance <em>(Rating: 2)</em></li>
                <li>Hidden physics models: Machine learning of nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials <em>(Rating: 2)</em></li>
                <li>Learning physical intuition of block towers by example <em>(Rating: 1)</em></li>
                <li>Hyperspectral plant disease forecasting using generative adversarial networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2332",
    "paper_id": "paper-162168885",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "BlockTower-Intuition (ResNet/GoogLeNet)",
            "name_full": "Learning physical intuition of block towers using convolutional neural networks (ResNet-34, GoogLeNet, PhysNet)",
            "brief_description": "Supervised CNN models (ResNet-34, GoogLeNet) trained on simulated video/image data to predict physical stability (will a tower fall) and a mask-prediction network (PhysNet) to estimate block trajectories; used to derive intuitive physics predictions from visual data.",
            "citation_title": "Learning physical intuition of block towers by example.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Intuitive physics / physical scene understanding",
            "problem_description": "Predict whether stacked objects (wooden block towers) will remain stable or fall and predict trajectories of blocks during collapse from images or video frames.",
            "data_availability": "Moderate — synthetic and real image/video datasets were used; synthetic datasets can be generated abundantly whereas real annotated collapse data are scarcer; training labels are supervised (stability/fall), accessibility moderate (research datasets).",
            "data_structure": "Unstructured image and video data (RGB frames), with instance segmentation masks for PhysNet; time series (video) for trajectory prediction.",
            "problem_complexity": "High visual and physical complexity — nonlinear dynamics, occlusions, combinatorial object arrangements; requires reasoning about object interactions and future dynamics.",
            "domain_maturity": "Emerging subfield: established vision architectures exist but domain-specific physics reasoning methods are developing; domain knowledge (intuition of physics) is qualitative rather than fully formalized.",
            "mechanistic_understanding_requirements": "Medium — scientific insight benefits from interpretable intermediate outputs (masks/trajectories), but black-box prediction of stability can be useful; PhysNet adds more interpretable structure.",
            "ai_methodology_name": "Convolutional neural networks (ResNet-34, GoogLeNet), PhysNet mask-prediction CNN",
            "ai_methodology_description": "Supervised CNN classifiers (ResNet-34, GoogLeNet) trained on labeled images/videos to predict binary stability; PhysNet is a deeper mask-prediction network with alternating upsampling and convolution designed to capture block arrangement and motion, trained on instance segmentation masks to output per-block masks and hence trajectories.",
            "ai_methodology_category": "Supervised deep learning / vision models",
            "applicability": "Applicable — CNNs map well from images to stability labels; PhysNet architecture adapted to capture object structure and dynamics. Limitations include domain shift between synthetic and real scenes and lack of explicit physics laws.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Worked well on synthetic data (outperformed humans) and produced comparable results on real data; PhysNet provided improved trajectory/motion predictions and improved interpretability via masks.",
            "impact_potential": "Enables automated reasoning about physical stability from images, useful in robotics, simulation validation, and cognitive modeling; helps bridge perception and physical reasoning.",
            "comparison_to_alternatives": "Compared to human subjects and baseline vision architectures; PhysNet designed specifically to reason about block arrangement and outperformed humans on synthetic data and matched on real data.",
            "success_factors": "Availability of synthetic simulated data for training, architecture tailored to capture object composition and motion (design transparency), and use of instance-level supervision (masks) improved interpretability and performance.",
            "key_insight": "Supervised CNNs trained on simulated visual data can predict intuitive physical outcomes; adding design choices that encode object structure (mask prediction) improves interpretability and predictive performance.",
            "uuid": "e2332.0",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Hyperspectral-GAN Forecasting",
            "name_full": "Hyperspectral plant disease forecasting using cycle-consistent generative adversarial networks",
            "brief_description": "Use of cycle-consistent GANs to predict how microscopic hyperspectral images of barley plants change day-to-day, effectively forecasting disease spread in image appearance space without explicit biological parameters.",
            "citation_title": "Hyperspectral plant disease forecasting using generative adversarial networks.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Plant pathology / remote sensing / phenotyping",
            "problem_description": "Predict temporal evolution of microscopic hyperspectral plant images to forecast disease progression based on observational image sequences.",
            "data_availability": "Moderate — time-series hyperspectral imagery collected over days; labeled biological parameters may be limited, but image sequences provide self-supervised signals; data quality depends on imaging modality.",
            "data_structure": "High-dimensional image data (hyperspectral cubes), temporal sequences (time series of images), unstructured pixel-wise spectral signatures.",
            "problem_complexity": "High — high-dimensional spectral data, complex disease appearance changes over time, non-linear mappings between days, and limited mechanistic labels.",
            "domain_maturity": "Developing — imaging and hyperspectral sensing are established, but machine-learning-based forecasting of disease appearance is relatively new.",
            "mechanistic_understanding_requirements": "Low-to-medium — paper uses image-space forecasting without incorporating biological mechanistic models, so black-box predictions were acceptable though scientific interpretation requires domain knowledge.",
            "ai_methodology_name": "Cycle-consistent generative adversarial networks (CycleGAN-style)",
            "ai_methodology_description": "GANs trained to map images from day t to day t+1 (and vice versa) with cycle-consistency losses to enforce plausible reversible mappings, learning appearance evolution without explicit biological parameters.",
            "ai_methodology_category": "Unsupervised / generative modeling",
            "applicability": "Applicable for predicting image appearance changes where mechanistic labels are scarce; limitations include lack of explicit biological interpretability and dependence on sufficient image sequence coverage.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Produced highly probable image-based appearances across days demonstrating temporal evolution capture; lacks integration with mechanistic biological models so scientific causal claims are limited.",
            "impact_potential": "Enables short-term visual forecasting of disease progression useful for monitoring and planning; could be combined with mechanistic models for improved interpretability.",
            "comparison_to_alternatives": "Not directly compared to physics/biologically-informed models in the survey text; approach trades mechanistic interpretability for image-level predictive ability.",
            "success_factors": "Availability of sequential hyperspectral image data and the capacity of GANs to model complex, high-dimensional appearance changes without labeled mechanistic variables.",
            "key_insight": "Generative models can forecast high-dimensional hyperspectral image evolution without explicit mechanistic parameters, yielding useful visual predictions but limited scientific causal insight.",
            "uuid": "e2332.1",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "FunctionalGlass-ML",
            "name_full": "Accelerating the design of functional glasses through neural-network modeling",
            "brief_description": "Neural networks trained on several hundred silicate compositions to predict manufacturing and end-use properties (e.g., liquidus temperatures), supporting materials design by screening novel compositions.",
            "citation_title": "Accelerating the design of functional glasses through modeling.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Materials design / inorganic chemistry (glass formulation)",
            "problem_description": "Predict liquidus temperatures and other properties of silicate glass compositions to identify optimized formulations for manufacturing and end-use.",
            "data_availability": "Limited to moderate — several hundred measured composites available for training; experimental data acquisition is costly, so labeled data is relatively scarce.",
            "data_structure": "Structured numeric data (composition vectors with up to eight components) and scalar target properties (liquidus temperature), i.e., tabular composition-property data.",
            "problem_complexity": "Moderate-to-high — high-dimensional compositional space with nonlinear composition-property relationships and sparse sampling across the combinatorial space.",
            "domain_maturity": "Established domain knowledge (physics-driven models exist) but data-driven screening is emerging; practitioners have substantial domain expertise and alternative simulation models.",
            "mechanistic_understanding_requirements": "Medium — scientific plausibility is important; the ML model is used alongside physics-driven models and domain checks rather than as sole explanation.",
            "ai_methodology_name": "Feedforward neural networks (regression NNs)",
            "ai_methodology_description": "Supervised regression NNs trained on composition-property pairs (several hundred samples) to predict liquidus temperatures and guide selection of candidate compositions; used in a workflow that also consults physics-driven models outside the ML chain.",
            "ai_methodology_category": "Supervised learning (regression)",
            "applicability": "Applicable — NNs can approximate complex composition-property mappings given available data; limited by training sample size and need for extrapolation to novel chemistries.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Enabled efficient screening and estimation of properties, reducing costly trial-and-error; used in hybrid workflows with physics models for validation.",
            "impact_potential": "Speeds materials discovery by narrowing candidate sets, reducing experimental cost and time; provides practical utility in materials formulation workflows.",
            "comparison_to_alternatives": "Used in combination with physics-driven models rather than strictly compared; hybrid use ensures cross-validation with established scientific models.",
            "success_factors": "Use of domain knowledge (physics-based models) outside the ML chain for validation, focused property targets, and careful feature representation of compositional vectors.",
            "key_insight": "Supervised NNs trained on modest-sized experimental datasets can accelerate materials screening when combined with physics-based validation, trading pure interpretability for practical utility.",
            "uuid": "e2332.2",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "HiddenPhysics-NNs",
            "name_full": "Hidden physics models / physics-informed neural networks for learning nonlinear PDEs and dynamics",
            "brief_description": "Neural-network-based approaches that incorporate differential-equation structure or physics constraints to learn dynamics or PDEs from scattered spatio-temporal data, enabling forecasting and discovery of governing equations.",
            "citation_title": "Hidden physics models: Machine learning of nonlinear partial differential equations.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computational physics / dynamical systems / PDE identification",
            "problem_description": "Infer the form of governing PDEs or learn dynamic evolution operators from space-time scattered data to forecast future states and uncover underlying dynamics.",
            "data_availability": "Varies — can work with scattered simulation or experimental spatio-temporal samples; some methods are demonstrated on numerical simulation data (e.g., Burgers', Schrödinger, Navier–Stokes).",
            "data_structure": "Spatio-temporal continuous data (scattered measurements in space and time), numerical simulation outputs, potentially noisy observations.",
            "problem_complexity": "High — PDEs represent high-dimensional, nonlinear, and potentially chaotic dynamics; identification requires resolving differential operator structure and derivatives from finite data.",
            "domain_maturity": "Emerging research area blending ML and numerical analysis; prior mathematical theory exists for PDEs but data-driven discovery is newer.",
            "mechanistic_understanding_requirements": "High — goal includes recovering mechanistic/physical forms of PDEs or consistent dynamic models for scientific insight.",
            "ai_methodology_name": "Physics-informed neural networks (PINNs) / hidden physics models; structured NNs learning differential operators",
            "ai_methodology_description": "Neural networks represent solution fields or terms in PDEs; physics constraints (residuals of PDEs) are incorporated in the loss to enforce consistency, enabling recovery of dynamic operators; used to forecast future states with reported relative L2 errors ~1e-3 on benchmark PDEs.",
            "ai_methodology_category": "Physics-informed ML / supervised and unsupervised hybrid",
            "applicability": "Highly applicable for problems where PDE form is partly known or where enforcing physics residuals improves generalization; requires sufficiently informative data and appropriate regularization.",
            "effectiveness_quantitative": "Reported relative L2 errors on benchmark PDE forecasting tasks up to order 10^-3 in published benchmarks (Burgers', nonlinear Schrödinger, Navier–Stokes).",
            "effectiveness_qualitative": "Effectively learned dynamics and produced accurate forecasts on simulated PDE benchmarks; model's generality allows learning underlying operators but direct interpretability of NN parameters is limited without additional structure.",
            "impact_potential": "Enables discovery and forecasting of governing dynamics from data, promising for science domains governed by PDEs (fluid dynamics, materials, geoscience); aids in reducing reliance on handcrafted simulators.",
            "comparison_to_alternatives": "Compared favorably to purely data-driven black-box models due to physics constraints improving plausibility and stability; also complementary to sparse-regression PDE identification methods.",
            "success_factors": "Enforcing physical constraints in loss functions, using simulation benchmark datasets for validation, and careful treatment of differential operators/regularization.",
            "key_insight": "Embedding PDE structure or physics residuals into neural-network training yields accurate, physically plausible models that can both forecast dynamics and aid discovery of governing equations.",
            "uuid": "e2332.3",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Exoplanet-Deep (Multi-source NN)",
            "name_full": "Scientific domain knowledge improves exoplanet transit classification with deep learning",
            "brief_description": "A multi-source neural network architecture that integrates centroid time-series and stellar parameters into the model to improve exoplanet transit detection/classification from light-curve data.",
            "citation_title": "Scientific domain knowledge improves exoplanet transit classification with deep learning.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Astrophysics – exoplanet detection",
            "problem_description": "Classify and detect exoplanet transit signals in stellar photometric time-series data, distinguishing true planets from false positives.",
            "data_availability": "Relatively abundant labeled time-series data from space telescopes (e.g., Kepler); labels exist from validated planet catalogs though some classes can be imbalanced.",
            "data_structure": "Time-series (light curves), auxiliary tabular stellar parameters, and centroid motion time-series (multimodal inputs).",
            "problem_complexity": "Moderate — signals can be weak and contaminated by stellar/systematic noise; non-linear detection boundaries and class imbalance complicate learning.",
            "domain_maturity": "Well-established observational domain with extensive prior knowledge and catalogs; ML methods are now applied for classification tasks with strong expert oversight.",
            "mechanistic_understanding_requirements": "Medium — for discovery and validation, interpretability and domain-informed features help trust and reduce false positives; pure black-box less acceptable for scientific claims.",
            "ai_methodology_name": "Multi-source neural network with concatenated hidden-layer outputs and centroid/time-series integration",
            "ai_methodology_description": "Deep NN architecture that inputs multiple aligned sources (light curve, centroid time series duplicated across inputs, and stellar parameters concatenated at hidden layer) to guide learning of transit features and context; design choices reflect domain knowledge about relevant signals.",
            "ai_methodology_category": "Supervised deep learning (multimodal)",
            "applicability": "Highly applicable — domain-informed architecture improves detection and reduces false positives; must handle class imbalance and instrumentation biases.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Integration of domain knowledge (centroid duplication, stellar parameter concatenation) improved classification accuracy compared to models without such design choices; provided more scientifically plausible decision-making.",
            "impact_potential": "Improves automated vetting of exoplanet candidates, increasing throughput of discovery and reducing manual validation burden; potentially discovers novel candidates.",
            "comparison_to_alternatives": "Compared to baseline architectures lacking domain-informed inputs; domain-aware design outperformed generic networks.",
            "success_factors": "Explicit incorporation of astrophysical domain knowledge into model input representation and architecture, leveraging abundant labeled catalogs for supervised training.",
            "key_insight": "Designing network architectures that incorporate domain-specific inputs and representations materially improves scientific classification tasks over generic deep models.",
            "uuid": "e2332.4",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "NewtonianNN",
            "name_full": "Newtonian neural networks for long-term motion prediction (Newtonian image understanding)",
            "brief_description": "A two-stream CNN architecture mapping static images of objects to states in a set of pre-defined Newtonian scenarios (12 scenarios) derived from simulated videos, enabling interpretable long-term motion prediction.",
            "citation_title": "Newtonian image understanding: Unfolding the dynamics of objects in static images.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Physical dynamics inference / computer vision",
            "problem_description": "From a single static color image, infer the long-term motion trajectory of an object by mapping it to a discrete set of Newtonian scenario templates parameterized by physical variables.",
            "data_availability": "Moderate — image datasets augmented by game-engine-simulated videos representing the Newtonian scenarios; labels map images to scenario states.",
            "data_structure": "Static RGB images and simulated video-derived convolutional filter representations (paired modalities), i.e., multimodal image and filter data.",
            "problem_complexity": "High — predicting long-term dynamics from a single image is underdetermined; mapping to discrete scenario classes reduces complexity but remains challenging.",
            "domain_maturity": "Emerging — combines classical physics abstractions with data-driven image understanding.",
            "mechanistic_understanding_requirements": "High for scientific insight — mapping images to interpretable Newtonian scenarios provides causal/mechanistic abstraction rather than pure black-box forecasting.",
            "ai_methodology_name": "Two-stream CNN (image encoder + video-derived scenario encoder) — Newtonian NNs",
            "ai_methodology_description": "Parallel CNNs: one encodes the input image; the other derives convolutional filters from game-engine simulation videos representing 12 Newtonian scenarios; coupling yields interpretable assignment of image to scenario and thus physical parameters for trajectory prediction.",
            "ai_methodology_category": "Supervised deep learning with interpretable scenario mapping (hybrid physics-abstraction)",
            "applicability": "Applicable when a set of representative physical scenarios can be pre-defined; reduces problem ambiguity by constraining outputs to interpretable physics templates.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Provides interpretable mapping from images to physical scenarios and enables plausible long-term motion predictions; success depends on coverage of the pre-defined scenario set.",
            "impact_potential": "Useful for robotics and visual reasoning tasks requiring interpretable motion forecasts; represents a pathway to combine simulated physics scenarios with perception.",
            "comparison_to_alternatives": "Design contrasts with direct regression-based motion predictors by using scenario abstraction; reported to be more interpretable though not directly compared quantitatively in the survey text.",
            "success_factors": "Use of simulated scenario videos to create an interpretable scenario space and architecture that couples visual encoding with scenario templates.",
            "key_insight": "Mapping visual input to a discrete, physically interpretable scenario space enables mechanistic motion prediction from single images while improving interpretability.",
            "uuid": "e2332.5",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Physics-Loss Supervision",
            "name_full": "Label-free supervision of neural networks with physics and domain knowledge",
            "brief_description": "Approaches that supervise neural networks by enforcing physics constraints (e.g., conservation laws) in loss functions rather than relying on labeled outputs, enabling training in data-limited settings.",
            "citation_title": "Label-free supervision of neural networks with physics and domain knowledge.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Physics-based simulation / inverse problems / scientific modeling",
            "problem_description": "Train models when labeled outputs are scarce by using known physics (constraints or simulators) to create supervision signals or consistency losses during training.",
            "data_availability": "Often limited labeled data; physics constraints provide alternative supervisory signals; data can be unlabeled observations or partial measurements.",
            "data_structure": "Varies: could be spatio-temporal measurement data, sensor observations, or other continuous measurements; sometimes simulation-generated.",
            "problem_complexity": "High — enforcing differential or conservation constraints inside learning introduces complex loss landscapes and requires differentiable physics representations or surrogate losses.",
            "domain_maturity": "Emerging technique with growing adoption where strong domain physics exist; relies on substantial domain knowledge representation.",
            "mechanistic_understanding_requirements": "High — enforcing physics aligns models with mechanistic expectations and improves scientific plausibility; goal often includes mechanistic consistency rather than only predictive accuracy.",
            "ai_methodology_name": "Physics-constrained neural network training (physics-informed losses / label-free supervision)",
            "ai_methodology_description": "Neural networks trained with loss terms encoding physics constraints (e.g., divergence-free conditions, conservation laws, or residuals of known equations) that penalize violation of physical laws, enabling learning without explicit labeled targets.",
            "ai_methodology_category": "Physics-informed ML / hybrid supervised-unsupervised",
            "applicability": "Applicable where mechanistic equations or constraints are known; particularly useful in small-data regimes and for improving physical plausibility.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Shown to produce plausible and physically consistent outputs in fluid simulation and other domains; effectiveness depends on fidelity of the physics constraints and their numerical implementation.",
            "impact_potential": "Can drastically reduce labeling needs and improve generalization and scientific consistency of ML models across physics-driven domains.",
            "comparison_to_alternatives": "Improves over purely data-driven losses by enforcing domain constraints; compared qualitatively against generic architectures where physics-aware losses yielded more plausible results.",
            "success_factors": "Availability and correct encoding of domain physics, differentiable formulations of constraints, and careful balancing of physics and data-driven loss components.",
            "key_insight": "Using physics constraints as supervision compensates for scarce labeled data and yields scientifically consistent models, making ML viable for many scientific tasks.",
            "uuid": "e2332.6",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "RANS-Embedded-Invariance",
            "name_full": "Reynolds-averaged turbulence modelling using deep neural networks with embedded invariance",
            "brief_description": "Deep neural networks for turbulence closure modeling that embed invariance properties (rotational, reflectional) via specialized network layers (higher-order multiplicative layers) to improve generalization and physical plausibility.",
            "citation_title": "Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computational fluid dynamics / turbulence modeling",
            "problem_description": "Predict Reynolds stresses or closure terms for RANS turbulence models to improve predictive accuracy of turbulent flows while respecting physical invariances.",
            "data_availability": "Moderate — training data from high-fidelity simulations or experiments; variety of geometries limited which can challenge generalization.",
            "data_structure": "Structured spatial fields (flow feature fields), tensor-valued quantities (Reynolds stresses), and possibly parametric geometry descriptors.",
            "problem_complexity": "Very high — turbulent flows are highly nonlinear, multi-scale, and sensitive to geometry; learning closure terms is innately complex.",
            "domain_maturity": "Well-established theoretical and simulation foundations exist; ML integration is an active research area seeking to augment traditional turbulence models.",
            "mechanistic_understanding_requirements": "High — physical invariances and constraints are critical for scientifically plausible turbulence predictions and for generalization beyond training regimes.",
            "ai_methodology_name": "Deep neural networks with embedded invariance layers (higher-order multiplicative layers / tensor-invariant representations)",
            "ai_methodology_description": "Design-transparent architectures incorporate invariant representations via multiplicative layers and tensor-basis models to ensure outputs respect known symmetries; trained supervisedly to predict closure quantities from flow features.",
            "ai_methodology_category": "Supervised deep learning with physics-informed architectural constraints",
            "applicability": "Applicable and beneficial: embedding invariances improves accuracy and generalization, demonstrated on test cases with different geometries than training.",
            "effectiveness_quantitative": "Reported significantly more accurate predictions compared to generic NN architectures on test cases (specific numerical metrics not provided in survey text).",
            "effectiveness_qualitative": "Models with embedded invariance outperformed generic architectures and generalized better to unseen geometries, indicating improved physical consistency.",
            "impact_potential": "Potential to improve RANS predictions in engineering workflows, reduce reliance on empirical closures, and enable more reliable ML-augmented CFD for novel geometries.",
            "comparison_to_alternatives": "Compared to generic NN architectures; invariant-embedded networks produced more accurate and physically consistent predictions.",
            "success_factors": "Architectural encoding of known invariances and symmetries, use of physics-based tensor bases, and supervised training on representative high-fidelity data.",
            "key_insight": "Architectures that enforce known physical symmetries substantially improve ML model accuracy and generalization for complex fluid-dynamics closure problems.",
            "uuid": "e2332.7",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "GP-Calibrate-OPV",
            "name_full": "Gaussian process Bayesian calibration of computational predictions to experimental organic photovoltaic materials data",
            "brief_description": "Use of Gaussian processes with chemically informed similarity measures (Tanimoto) as a Bayesian calibration to map computational quantum-chemistry predictions to experimental observables, providing uncertainty-aware calibrated predictions.",
            "citation_title": "A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Materials informatics / computational chemistry (organic photovoltaics)",
            "problem_description": "Calibrate theoretical/computational property predictions (e.g., from quantum chemistry) to match experimental measurement distributions for screening candidate molecules for organic photovoltaics.",
            "data_availability": "Moderate — computational databases are large, experimental calibration points fewer; survey reports efficient screening of &gt;51,000 molecules with 838 high-performing identified.",
            "data_structure": "Structured molecular descriptors and fingerprints (chemical representations), scalar target properties (computed and experimental values), tabular datasets.",
            "problem_complexity": "Moderate — mapping systematic computational biases to experimental outcomes requires modeling complex, non-linear deviations and domain-specific similarity structures.",
            "domain_maturity": "Established computational chemistry foundations with growing ML augmentation; domain expertise available to craft similarity measures and priors.",
            "mechanistic_understanding_requirements": "Medium — calibration aims to correct systematic discrepancies rather than derive new mechanisms; uncertainty quantification is important for applicability warnings.",
            "ai_methodology_name": "Gaussian Processes with Tanimoto-similarity-informed priors (Bayesian calibration)",
            "ai_methodology_description": "Non-parametric GP regression models the deviation between computational predictions and experimental values; chemical similarity (Tanimoto) incorporated in kernel/prior construction; outputs provide predictive mean and confidence indicating applicability.",
            "ai_methodology_category": "Bayesian supervised learning / probabilistic calibration",
            "applicability": "Well-suited — GPs provide uncertainty estimates and leverage molecular similarity to improve calibration, indicating when model predictions are unreliable.",
            "effectiveness_quantitative": "Enables screening of over 51,000 molecules and identification of 838 high-performing candidates (from survey text).",
            "effectiveness_qualitative": "Provided calibrated, uncertainty-aware predictions and flagged out-of-distribution cases; design transparency via similarity-informed prior improved trust in applicability.",
            "impact_potential": "Improves virtual screening reliability, speeds materials discovery pipelines by prioritizing experimentally promising candidates while providing confidence indicators.",
            "comparison_to_alternatives": "Offers advantages over uncalibrated computational predictions by quantifying deviations and uncertainties; compared implicitly to raw theoretical outputs.",
            "success_factors": "Use of domain-specific similarity kernels, probabilistic modelling providing uncertainties, and integration of experimental prior knowledge.",
            "key_insight": "Probabilistic calibration with chemically informed GPs makes computational screening more reliable by correcting systematic biases and returning applicability-confidence for experimental validation.",
            "uuid": "e2332.8",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "SINDy (SparseID)",
            "name_full": "Sparse Identification of Nonlinear Dynamical Systems (SINDy) for discovering governing equations from data",
            "brief_description": "Sparse regression over a library of candidate nonlinear functions to identify parsimonious governing equations (ODEs/PDEs) from time-series data, yielding interpretable mechanistic equations.",
            "citation_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Dynamical systems / model discovery (biology, physics)",
            "problem_description": "Infer low-dimensional dynamical system equations (ordinary differential equations) that govern observed time-series data, enabling mechanistic understanding and prediction.",
            "data_availability": "Varies — demonstrated on synthetic and experimental time-series; needs sufficient temporal sampling but can succeed with limited dimension if dynamics are sparse in chosen library.",
            "data_structure": "Time-series (temporal sequences) of state variables; often multivariate sequential measurements.",
            "problem_complexity": "Moderate-to-high — complexity depends on the true system's sparsity in the candidate dictionary; recovery from noisy or corrupted data is challenging but addressed by robust sparse methods.",
            "domain_maturity": "Maturing approach with strong theoretical grounding in sparse regression and system identification; practitioners provide candidate libraries using domain knowledge.",
            "mechanistic_understanding_requirements": "High — primary goal is to obtain interpretable, mechanistic equations (causal/dynamical form) rather than black-box predictions.",
            "ai_methodology_name": "Sparse regression over function dictionaries (SINDy)",
            "ai_methodology_description": "Construct a large library of candidate nonlinear functions of the state variables and use sparsity-promoting regression (L0/L1-like / sequential thresholding) to select a small subset that explains dynamics, yielding explicit differential equations.",
            "ai_methodology_category": "Sparse model discovery / interpretable ML",
            "applicability": "Highly applicable when dynamics are sparse in a chosen basis and time-series quality is sufficient; less applicable if true dynamics are not representable in the library or data are extremely noisy.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Successfully recovers governing equations in many benchmark problems and biological network inference tasks; advantage is interpretability and parsimony but sensitive to library choice and noise.",
            "impact_potential": "High for scientific discovery: yields explicit governing equations enabling mechanistic insight, reduced-order modeling, and improved scientific understanding.",
            "comparison_to_alternatives": "Provides more interpretable, parsimonious models than black-box NNs; compared favorably to other data-driven discovery approaches for sparse systems.",
            "success_factors": "Appropriate choice of candidate function dictionary driven by domain knowledge, sparsity enforcement, and sufficient temporal resolution and signal-to-noise in data.",
            "key_insight": "Sparsity-promoting identification methods can extract interpretable governing equations from time-series when suitable candidate functions and data quality are available.",
            "uuid": "e2332.9",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "SciNet (VAE-based concept discovery)",
            "name_full": "SciNet: a modified variational autoencoder learning minimal representations that map to physical parameters (concept discovery)",
            "brief_description": "An autoencoder-like architecture constrained to learn low-dimensional latent representations whose activations correspond linearly to physical parameters, enabling recovery of explanatory factors from experimental data.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Physics / experimental concept discovery",
            "problem_description": "From experimental observation data, learn compact latent representations that encode physical parameters and explanatory factors for subsequent prediction and interpretation.",
            "data_availability": "Varies — demonstrated on experimental and simulated datasets; method benefits from datasets where physical parameters influence observations but may be limited by sample size.",
            "data_structure": "Structured observational data (could be images or numeric measurements) compressed into low-dimensional latent vectors.",
            "problem_complexity": "Moderate — complexity arises in disentangling latent factors that align with known physical parameters, requiring architectures with appropriate inductive biases.",
            "domain_maturity": "Emerging approach in interpretable representation learning for physics discovery; relies on combination of ML and domain verification.",
            "mechanistic_understanding_requirements": "High — designed to produce mechanistically meaningful latent variables to support scientific explanation.",
            "ai_methodology_name": "Modified variational autoencoder (SciNet)",
            "ai_methodology_description": "VAE-like encoder-decoder with bottleneck constrained to small number of neurons, encouraged (via training objectives/architecture) to align latent activations linearly with physical parameters, enabling interpretability of latent space.",
            "ai_methodology_category": "Unsupervised / representation learning (autoencoder-based)",
            "applicability": "Applicable for tasks seeking compact, interpretable latent representations that can be mapped to physical concepts; requires careful architecture and analysis to confirm alignment with true physical parameters.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated ability to recover physical parameters and provide simpler representations useful for downstream analysis; interpretability depends on alignment and post-hoc verification with domain knowledge.",
            "impact_potential": "Facilitates discovery of latent physical factors from experimental data, aiding hypothesis generation and simplified modeling.",
            "comparison_to_alternatives": "Offers more explicit latent-parameter interpretability than generic autoencoders; compared conceptually to architectures with designed bottlenecks representing known parameters.",
            "success_factors": "Architectural constraints enforcing low-dimensional bottleneck, alignment analysis between latent neurons and physical parameters, and domain validation.",
            "key_insight": "Constraining representation learning to low-dimensional, physically aligned latent spaces enables extraction of explanatory factors from data, bridging representation learning and scientific concept discovery.",
            "uuid": "e2332.10",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "DeepTensorQC",
            "name_full": "Deep tensor neural networks for quantum-chemical prediction with interpretability (deep tensor networks)",
            "brief_description": "Deep tensor neural networks that predict molecular energies up to chemical accuracy while enabling interpretability via local chemical potentials and 3D response maps.",
            "citation_title": "Quantum-chemical insights from deep tensor neural networks.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Quantum chemistry / materials",
            "problem_description": "Predict molecular energies and other quantum-chemical properties from molecular structure with high accuracy and provide interpretable local insights.",
            "data_availability": "Moderate to large — high-throughput quantum-chemical datasets exist for training; labeled computed energies available from quantum chemistry calculations.",
            "data_structure": "Graph- or geometry-based molecular representations (atomic coordinates, types), 3D spatial structure data.",
            "problem_complexity": "High — molecular energy landscapes are high-dimensional and sensitive to 3D geometry; accurate prediction requires modeling complicated many-body interactions.",
            "domain_maturity": "Established field with extensive computational methods; ML methods are well-adopted and growing.",
            "mechanistic_understanding_requirements": "High — chemists require interpretable outputs to connect ML predictions with chemical intuition (e.g., reactivity, aromaticity).",
            "ai_methodology_name": "Deep tensor neural networks (DTNNs)",
            "ai_methodology_description": "Neural networks operating on atomic features and pairwise/interatomic tensors to predict total molecular energy; interpretability via computation of local chemical potentials (sensitivity analyses) and 3D response maps indicating reactive/stable regions.",
            "ai_methodology_category": "Supervised deep learning (graph/geometry-aware networks)",
            "applicability": "Highly applicable for property prediction tasks where abundant computed labels exist; local interpretability offers scientific insights.",
            "effectiveness_quantitative": "Reported prediction accuracy up to chemical accuracy in cited work (i.e., errors within chemical accuracy thresholds), specific metrics not quoted in survey text.",
            "effectiveness_qualitative": "Provides accurate predictions and localized interpretability enabling identification of chemical motifs (e.g., aromatic ring stability); balances performance with domain insight.",
            "impact_potential": "Accelerates computational screening and offers chemical interpretability to guide synthesis and theory; bridges ML and domain-expert analysis.",
            "comparison_to_alternatives": "Compared favorably to other ML and quantum-chemical surrogate models by achieving high accuracy with interpretable local potentials.",
            "success_factors": "Model architectures that respect molecular structure and 3D geometry, and interpretability tools (local chemical potential) aligned with chemical intuition.",
            "key_insight": "Geometry-aware deep tensor networks can reach chemical-accuracy predictions while providing local interpretable measures that map to chemical concepts, enhancing scientific utility.",
            "uuid": "e2332.11",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Attention-Genomics/Healthcare",
            "name_full": "Attention-based neural models for genomics and healthcare (e.g., Attend-and-Predict, AttentiveChrome, RETAIN)",
            "brief_description": "Use of attention mechanisms in neural networks to highlight important sequence motifs or clinical visits/features, improving interpretability of predictions in genomics and patient outcome modeling.",
            "citation_title": "Attend and predict: Understanding gene regulation by selective attention on chromatin.",
            "mention_or_use": "use",
            "scientific_problem_domain": "Genomics and clinical predictive modeling",
            "problem_description": "Identify sequence motifs or genomic regions relevant to gene regulation and predict clinical outcomes by learning which parts of sequences or patient histories the model attends to.",
            "data_availability": "Varies — genomic datasets and chromatin data can be large (abundant), while high-quality clinical labeled histories may be more limited; labels (e.g., clinical outcomes) often available.",
            "data_structure": "Sequential genomic data (DNA sequences), multi-visit electronic health record time-series (categorical/numeric), multimodal clinical features.",
            "problem_complexity": "High — long-range dependencies in sequences, heterogeneity in patient records, class imbalance, and need to map attention weights to biologically meaningful motifs.",
            "domain_maturity": "Mature experimental genomics and clinical domains with growing adoption of interpretable deep models; attention mechanisms are an established ML tool.",
            "mechanistic_understanding_requirements": "High — for biological discovery and clinical trust, interpretability (which motifs/visits drive predictions) is critical.",
            "ai_methodology_name": "Attention-based neural networks (sequence-attention, hierarchical attention, RETAIN reverse-time attention)",
            "ai_methodology_description": "Neural architectures incorporate attention modules that produce weights over positions/time steps; attention maps are visualized to identify important sequence motifs or patient visits; hierarchical attention stacks allow multi-scale interpretation.",
            "ai_methodology_category": "Supervised deep learning with attention for interpretability",
            "applicability": "Well-suited for tasks where identifying salient input regions is important; attention provides human-interpretable cues though attention weight interpretation has caveats.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Attention models successfully highlighted biologically or clinically relevant regions and provided interpretable cues for experts; architectures aided motif discovery and clinical variable importance assessment.",
            "impact_potential": "Enables extraction of biologically meaningful motifs and clinically actionable insights, supporting hypothesis generation and decision support with interpretable evidence.",
            "comparison_to_alternatives": "Compared to vanilla NNs, attention adds interpretability and often improves performance on sequence and time-series tasks; caveats exist about equating attention weights directly to causal importance.",
            "success_factors": "Hierarchical and task-specific attention design, domain validation of attention-derived motifs/visits, and adequate training data to learn meaningful attention patterns.",
            "key_insight": "Attention mechanisms provide a practical route to interpretable neural predictions in genomics and healthcare, but require domain validation to ensure scientific meaning.",
            "uuid": "e2332.12",
            "source_info": {
                "paper_title": "Explainable Machine Learning for Scientific Insights and Discoveries",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics informed deep learning (part II): Data-driven discovery of nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physics_informed_deep_learning_part_ii_datadriven_discovery_of_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "rating": 2,
            "sanitized_title": "discovering_governing_equations_from_data_by_sparse_identification_of_nonlinear_dynamical_systems"
        },
        {
            "paper_title": "Machine learning for molecular and materials science",
            "rating": 2,
            "sanitized_title": "machine_learning_for_molecular_and_materials_science"
        },
        {
            "paper_title": "Machine learning and the physical sciences",
            "rating": 2,
            "sanitized_title": "machine_learning_and_the_physical_sciences"
        },
        {
            "paper_title": "Theory-guided data science: A new paradigm for scientific discovery from data",
            "rating": 2,
            "sanitized_title": "theoryguided_data_science_a_new_paradigm_for_scientific_discovery_from_data"
        },
        {
            "paper_title": "Reynolds averaged turbulence modelling using deep neural networks with embedded invariance",
            "rating": 2,
            "sanitized_title": "reynolds_averaged_turbulence_modelling_using_deep_neural_networks_with_embedded_invariance"
        },
        {
            "paper_title": "Hidden physics models: Machine learning of nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "hidden_physics_models_machine_learning_of_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials",
            "rating": 2,
            "sanitized_title": "a_bayesian_approach_to_calibrating_highthroughput_virtual_screening_results_and_application_to_organic_photovoltaic_materials"
        },
        {
            "paper_title": "Learning physical intuition of block towers by example",
            "rating": 1,
            "sanitized_title": "learning_physical_intuition_of_block_towers_by_example"
        },
        {
            "paper_title": "Hyperspectral plant disease forecasting using generative adversarial networks",
            "rating": 1,
            "sanitized_title": "hyperspectral_plant_disease_forecasting_using_generative_adversarial_networks"
        }
    ],
    "cost": 0.0281145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explainable Machine Learning for Scientific Insights and Discoveries</p>
<p>Ribana Roscher 
Institute of Geodesy and Geoinformation
University of Bonn
53115BonnGermany</p>
<p>Institute of Computer Science
University of Osnabrueck
49074OsnabrückGermany</p>
<p>Member, IEEEBastian Bohn 
Institute for Numerical Simulation
University of Bonn
53115BonnGermany</p>
<p>Marco F Duarte 
Department of Electrical and Computer Engineering
University of Massachusetts Amherst
01003AmherstMAUSA</p>
<p>ANDJochen Garcke jochen.garcke@scai.fraunhofer.de 
Institute for Numerical Simulation
University of Bonn
53115BonnGermany</p>
<p>Fraunhofer Center for Machine Learning and Fraunhofer SCAI
53757Sankt AugustinGermany</p>
<p>Jochen Garcke 
Explainable Machine Learning for Scientific Insights and Discoveries
10.1109/ACCESS.2020.2976199Received January 21, 2020, accepted February 11, 2020, date of publication February 24, 2020, date of current version March 11, 2020.Corresponding author:
Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.INDEX TERMS Explainable machine learning, informed machine learning, interpretability, scientific consistency, transparency.42200This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020</p>
<p>I. INTRODUCTION</p>
<p>Machine learning methods, especially with the rise of neural networks (NNs), are nowadays used widely in commercial applications. This success has led to a considerable uptake of machine learning (ML) in many scientific areas. Usually these models are trained with regard to high accuracy, but there is a recent and ongoing high demand for understanding the way a specific model operates and the underlying reasons for the decisions made by the model. One motivation behind this is that scientists increasingly adopt ML for optimizing and producing scientific outcomes. Here, explainability is a prerequisite to ensure the scientific value of the outcome. In this context, research directions such as explainable artificial intelligence (AI) [1], informed ML [2], or intelligible intelligence [3] have emerged. Though related, the concepts, goals, and motivations vary, and core technical terms are defined in different ways.</p>
<p>The associate editor coordinating the review of this manuscript and approving it for publication was Massimo Cafaro .</p>
<p>In the natural sciences, the main goals for utilizing ML are scientific understanding, inferring causal relationships from observational data, or even achieving new scientific insights. With ML approaches, one can nowadays (semi-)automatically process and analyze large amounts of scientific data from experiments, observations, or other sources. The specific aim and scientific outcome representation will depend on the researchers' intentions, purposes, objectives, contextual standards of accuracy, and intended audiences. Regarding conditions for an adequate scientific representation, we defer to the philosophy of science [4].</p>
<p>This article provides a survey of recent ML approaches that are meant to derive scientific outcomes, where we specifically focus on the natural sciences. Given the scientific outcomes, novel insights can be derived to deepen understanding, or scientific discoveries can be revealed that were not known before. Gaining scientific insights and discoveries from an ML algorithm means gathering information from its output and/or its parameters regarding the scientific process or experiments underlying the data. One should note that a data-driven effort of scientific discovery is nothing new, but mimics the revolutionary work of Johannes Kepler and Sir Isaac Newton, which was based on a combination of data-driven and analytical work. As stated by [5], Data science is not replacing mathematical physics and engineering, but is instead augmenting it for the twenty-first century, resulting in more of a renaissance than a revolution. What is new, however, is the abundance of high-quality data in the combination with scalable computational and data processing infrastructure.</p>
<p>The main contribution of this survey is the discussion of commonly used ML-based chains leading to scientific outcomes that have been used in the natural sciences (see Fig. 1). The three elements transparency, interpretability, and explainability play a central role. These concepts will be defined and discussed in detail in this survey. Another essential component is domain knowledge, which is necessary to achieve explainability, but can also be used to foster scientific consistency of the model and the result. We provide diverse examples from the natural sciences of approaches that can be related to these topics. Moreover, we define several groups of ML chains based on the presence of the components from Fig. 1. Our goal is to foster a better understanding and a clearer overview of ML algorithms applied to data from the natural sciences.</p>
<p>The paper is structured as follows. Section II discusses transparency, interpretability, and explainability in the context of this article. While these terms are more methodology-driven and refer to properties of the model and the algorithm, we also describe the role of additional information and domain knowledge, as well as scientific consistency. Section III highlights several applications in the natural sciences that use these concepts to gain new scientific insights, while organizing the ML workflows into characteristic groups based on the different uptakes of interpretability and explainability. Section IV closes the paper with a discussion.</p>
<p>II. TERMINOLOGY</p>
<p>Several descriptive terms are used in the literature about explainable ML with diverse meanings, e.g., [6]- [11]. Nonetheless, distinct ideas can be identified. For the purpose of this work, we distinguish between transparency, interpretability, and explainability. Roughly speaking, transparency considers the ML approach, interpretability considers the ML model together with data, and explainability considers the model, the data, and human involvement.</p>
<p>A. TRANSPARENCY</p>
<p>An ML approach is transparent if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer. We say that the transparency of an ML approach concerns its different ingredients: the overall model structure, the individual model components, the learning algorithm, and how the specific solution is obtained by the algorithm. We propose to distinguish between model transparency, design transparency, and algorithmic transparency. Generally, to expect an ML method to be completely transparent in all aspects is rather unrealistic; usually there will be different degrees of transparency.</p>
<p>As an example, consider kernel-based ML approaches [12], [13]. The obtained model is transparent as it is given as a sum of kernel functions. The individual design VOLUME 8, 2020 component is the chosen kernel. Choosing between a linear or nonlinear kernel is typically a transparent design decision. However, using the common Gaussian kernel based on Euclidean distances can be a non-transparent design decision. In other words, it may not be clear why a given nonlinear kernel was chosen. Domain specific design choices can be made, in particular using suitable distance measures to replace the Euclidean distance, making the design of this model component (more) transparent. In the case of Gaussian process (GP) regression, the specific choice of the kernel can be built into the optimization of the hyper-parameters using the maximum likelihood framework [13]. Thereby, design transparency crosses over to algorithmic transparency. Furthermore, the obtained specific solution is, from a mathematical point of view, transparent. Namely, it is the unique solution of a convex optimization problem that can be reproducibly obtained [12], [13], resulting in algorithmic transparency. In contrast, approximations in the specific solution method such as early stopping, matrix approximations, stochastic gradient descent, and others, can result in (some) non-transparency of the algorithm.</p>
<p>As another example, consider NNs [14]. The model is transparent since its input-output relation and structure can be written down in mathematical terms. Individual model components, such as a layer of a NN, that are chosen based on domain knowledge can be considered as design transparent. Nonetheless, the layer parameters -be it their numbers, size, or nonlinearities involved -are often chosen in an ad-hoc or heuristic fashion and not motivated by domain knowledge; these decisions are therefore not design transparent. The learning algorithm is typically transparent, e.g., stochastic gradient descent can be easily written down. However, the choice of hyper-parameters such as learning rate, batch size, etc., has a more heuristic, non-transparent algorithmic nature. Due to the presence of several local minima, the solution is usually not easily reproducible; therefore, the obtained specific solution is not (fully) algorithmically transparent.</p>
<p>Our view is closely related to Lipton [9], who writes: Note that although there are many mathematical attempts to a better understanding of deep learning, at this stage ''the [mathematical] interpretation of NNs appears to mimic a type of Rorschach test,'' according to [15].
Informally,
Overall, we argue that transparency in its three forms does to a large degree not depend on the specific data, but solely on the ML method. But clearly, the obtained specific solution, in particular the ''solution path'' to it by the (iterative) algorithm, depends on the training data. The analysis task and the type of attributes usually play a role in achieving design transparency. Moreover, the choice of hyper-parameters might involve model structure, components, or the algorithm, while in an algorithmic determination of hyper-parameters the specific training data comes into play again.</p>
<p>B. INTERPRETABILITY</p>
<p>For our purposes, interpretability pertains to the capability of making sense of an obtained ML model. Generally, to interpret means ''to explain the meaning of'' or ''present in understandable terms;'' 1 see also [6]- [8]. We consider explanation distinct from interpretation, and focus here on the second aspect. Therefore, the aim of interpretability is to present some of the properties of an ML model in terms understandable to a human. Ideally, one could answer the question from [16]: ''Can we understand what the ML algorithm bases its decision on?'' Somewhat formally, [10] states:</p>
<p>An interpretation is the mapping of an abstract concept (e.g., a predicted class) into a domain that the human can make sense of. Interpretations can be obtained by way of understandable proxy models, which approximate the predictions of a more complex approach [7], [8]. Longstanding approaches involve decision trees or rule extraction [17] and linear models. In prototype selection, one or several examples similar to the inspected datum are selected, from which criteria for the outcome can be obtained. For feature importance, the weights in a linear model are employed to identify attributes that are relevant for a prediction, either globally or locally. For example, [18] introduced the model-agnostic approach LIME (Local Interpretable Model-Agnostic Explanations), which gives interpretation by creating locally a linear proxy model in the neighborhood of a datum, while the scores in layer-wise relevance propagation (LRP) are obtained by means of a first-order Taylor expansion of the nonlinear function [10]. A sensitivity analysis can be used to inspect how a model output (locally) depends upon the different input parameters [19]. Such an extraction of information from the input and the output of a learned model is also called post hoc interpretability [9] or reverse engineering [8]. Further details, types of interpretation, and specific realization can be found in recent surveys [7], [8], [20].</p>
<p>Visual approaches such as saliency masks or heatmaps show relevant patterns in the input based on feature importance, sensitivity analysis, or relevance scores to explain model decisions, and are employed in particular with deep learning approaches for image classification [10], [21], [22]. [23] introduces a formal notion for interpreting NNs where a set of input features is deemed relevant for a classi-fication decision if the expected classifier score remains nearly constant when randomising the remaining features. The authors prove that under this notion, the problem of finding small sets of relevant features is NP-hard, even when considering approximation within any non-trivial factor. This, on the one hand, shows the difficulty of algorithmically determining interpretations; on the other hand, it justifies the current use of heuristic methods in practical applications.</p>
<p>In unsupervised learning, the analysis goal can be a better understanding of the data, for example, by an interpretation of the obtained representation by linear or nonlinear dimensionality reduction [24], [25], or by inspecting the components of a low-rank tensor decomposition [26].</p>
<p>Note that, in contrast to transparency, to achieve interpretability the data is always involved. Although there are model-agnostic approaches for interpretability, transparency or retaining the model can assist in the interpretation. Furthermore, method-specific approaches depend on transparency. For example, layer-wise relevance propagation for NNs exploits the known model layout [10].</p>
<p>While the methods for interpretation allow the inspection of a single datum, [27] observes that it quickly becomes very time consuming to investigate large numbers of individual interpretations. As a step to automate the processing of the individual interpretations for a single datum, they employ clustering of heatmaps of many data to obtain an overall impression of the interpretations for the predictions of the ML algorithm.</p>
<p>Finally, note that the interpretable and human level understanding of the performance of an ML approach can result in a different choice of the ML model, algorithm, or data pre-processing later on.</p>
<p>C. EXPLAINABILITY</p>
<p>While research into explainable ML is widely recognized as important, a joint understanding of the concept of explainability still needs to evolve. Concerning explanations, it has also been argued that there is a gap of expectations between ML and so-called explanation sciences such as law, cognitive science, philosophy, and the social sciences [28].</p>
<p>While in philosophy and psychology explanations have been the focus for a long time, a concise definition is not available. For example, explanations can differ in completeness or the degree of causality. We suggest to follow a model from a recent review relating insights from the social sciences to explanations in AI [29], which places explanatory questions into three classes: (1) what-questions, such as ''What event happened?''; (2) how-questions, such as ''How did that event happen?''; and (3) why-questions, such as ''Why did that event happen?''. From the field of explainable AI we consider a definition from [10]:</p>
<p>An explanation is the collection of features of the interpretable domain, that have contributed for a given example to produce a decision (e.g., classification or regression).</p>
<p>As written in [8], ''[in explainable ML] these definitions assume implicitly that the concepts expressed in the understandable terms composing an explanation are self-contained and do not need further explanations.''</p>
<p>On the other hand, we believe that a collection of interpretations can be an explanation only with further contextual information, stemming from domain knowledge and related to the analysis goal. In other words, explainability usually cannot be achieved purely algorithmically. On its own, the interpretation of a model -in understandable terms to a human -for an individual datum might not provide an explanation to understand the decision. For example, the most relevant variables might be the same for several data; however, it is possible that the important observation for an understanding of the overall predictive behavior is that when ranking variables with respect to their interpretation, different lists of relevant variables are determined for each datum. Overall, the result will depend on the underlying analysis goal. ''Why is the decision made?'' will need a different explanation than ''Why is the decision for datum A different to (the nearby) datum B?''.</p>
<p>In other words, for explainability, the goal of the ML ''user'' is very relevant. According to [20], there are essentially four reasons to seek explanations: to justify decisions, to (enhance) control, to improve models, and to discover new knowledge. For regulatory purposes it might be fine to have an explanation by examples or (local) feature analysis, so that certain ''formal'' aspects can be checked. But, to attain scientific outcomes with ML one wants an understanding. Here, the scientist is using the data, the transparency of the method, and its interpretation to explain the output results (or the data) using domain knowledge and thereby to obtain a scientific outcome.</p>
<p>Furthermore, we suggest differentiating between scientific explanations and algorithmic explanations. For scientific explanations, [30] identifies five broad categories to classify the large majority of objects that are explained in science: data, entities, kinds, models, and theories. Furthermore, it is observed that the existence of a unifying general account of scientific explanation remains an open question. With an algorithmic explanation, one aims to reveal underlying causes to the decision of an ML method. This is what explainable ML aims to address. In recent years, a focus on applying interpretation tools to better explain the output of an ML model can be observed. This can be seen in contrast to symbolic AI techniques, e.g., expert or planning systems, which in contrast are often seen as explainable per se. Hybrid systems of both symbolic and, so-called, connectionist AI, e.g., artificial NNs, are investigated to combine advantages from both approaches. For example, [31] proposes ''object-oriented deep learning'' with the goal to convert a NN to a symbolic description to gain interpretability and explainability. They state that generally in NNs, there is inherently no explicit representation of symbolic concepts like objects or events, but rather a feature-oriented representation, which is difficult to explain. In their representation, objects could be formulated to have VOLUME 8, 2020 disentangled and interpretable properties. Although not commonly used so far, their work is one example of a promising direction towards a higher explainability of models.</p>
<p>In the broader context, other properties that can be relevant when considering explainability of ML algorithms are safety/trust, accountability, reproducibility, transferability, robustness and multi-objective trade-off or mismatched objectives, see e.g. [6], [9]. For example, in societal contexts, reasons for a decision often matter. Typical examples are (semi-)automatic loan applications, hiring decisions, or risk assessment for insurance applicants, where one wants to know why a model gives a certain prediction and how one might be affected by those decisions. In this context, and also due to regulatory reasons, one goal is that decisions based on ML models involve fair and ethical decision making. The importance to give reasons for decisions of an ML algorithm is also high for medical applications, where a motivation is the provision of trust in decisions such that patients are comfortable with the decision made. All this is supported by the General Data Protection Regulation, which contains new rules regarding the use of personal information. One component of these rules can be summed up by the phrase ''right to an explanation'' [32]. Finally, for ML models deployed for decision-support and automation, in particular in potentially changing environments, an underlying assumption is that robustness and reliability can be better understood, or more easily realized, if the model is interpretable [9].</p>
<p>One should also observe that explanations can be used to manipulate. For illustration, [33] distinguishes between the intuitive scientist, who seeks to make the most accurate or otherwise optimal decision, and the intuitive lawyer, who desires to justify a preselected conclusion. With that in mind, one often aims for human-centric explanations of black-box models. There are simple or purely algorithmic explanations, e.g., based on emphasising relevant pixels in an image. In so-called slow judgements tasks, an explanation might more easily enforce confirmation biases. For example, using human-centric explanations as evaluation baselines can be biased towards certain individuals. Further, a review of studies of experimental manipulations that require people to generate explanations or imagine scenarios indicates that people express greater confidence in a possibility, although false, when asked to generate explanations for it or imagine the possibility [34].</p>
<p>D. DOMAIN KNOWLEDGE</p>
<p>As outlined, domain knowledge is an essential part of explainability; it is also essential for treating small data scenarios or for performance reasons. A taxonomy for the explicit integration of knowledge into the ML pipeline, dubbed informed ML, is proposed by [2]. Three aspects are involved:</p>
<p>• type of knowledge, • representation and transformation of knowledge, and • integration of knowledge into the ML approach. See also the related works of [35], who use the term theoryguided data science, or physics-informed learning by [36].</p>
<p>For the purpose of this article, we follow [2], who arrange different types of knowledge along their degree of formality, from the sciences, over (engineering or production) process flow to world knowledge and finally individual (expert's) intuition. Knowledge can be assigned to several of the types in this incomplete list.</p>
<p>In the sciences, knowledge is often given in terms of mathematical equations, such as analytic expressions or differential equations, or as relations between instances and/or classes in the form of rules or constraints. Its representation can, for example, be in the form of ontologies, symmetries, or similarity measures. Knowledge can also be exploited by numerical simulations of models or through human interaction.</p>
<p>As ingredients of an ML approach, one considers the training data, the hypothesis space, the training algorithm, and the final model. In each of these, one can incorporate additional knowledge. Feature engineering is a common and longstanding way to incorporate knowledge into the training data, whereas using numerical simulations to generate (additional) training data is a modern phenomena. One common way to integrate knowledge into the hypothesis space is by choosing the structure of the model. Examples of this include defining a specific architecture of a NN or by choosing a structure of probability distributions that observes existing or non-existing links between variables. An example for the training phase is modifying the loss function according to additional knowledge, for example by adding a consistency term. Finally, the obtained model can be put in relation to existing knowledge, for example by checking known constraints for the predictions.</p>
<p>E. SCIENTIFIC CONSISTENCY</p>
<p>A fundamental prerequisite for generating reliable outcomes for scientific applications is scientific consistency. This means that the result obtained is plausible and consistent with existing scientific principles. The selection and formulation of the scientific principles to be met is based on domain knowledge, where the manner of integration is the core research question in areas such as informed ML. In the chain of Fig. 1, scientific consistency can be considered a priori at the model design stage or a posteriori by analysing the output results. As pointed out by [2], scientific consistency at the design stage can be understood as the result of a regularization effect, where various ways exist to restrict the solution space to scientifically consistent solutions. Reference [37] identifies scientific consistency besides interpretability as one of the five major challenges we need to tackle to successfully adopt deep learning approaches in the geosciences. Reference [35] underlines the importance of consistency by defining it as an essential component to measure performance:</p>
<p>One of the overarching visions of [theory-guided data science] is to include [. . . ] consistency as a critical component of model performance along with training accuracy and model complexity.</p>
<p>This can be summarized in a simple way by the TABLE 1. Group 1 includes approaches without any means of interpretability. In Group 2, a first level of interpretability is added by employing domain knowledge to design the models or explain the outcomes. Group 3 deals with specific tools included in the respective algorithms or applied to their outputs to make them interpretable. Finally, Group 4 lists approaches where scientific insights are gained by explaining the machine learning model itself.</p>
<p>following revised objective of model performance [. . . ]: Performance ∝ Accuracy + Simplicity + Consistency.</p>
<p>They discuss several ways to restrict the solution space to physically consistent solutions, by (1) designing the model family, such as specific network architectures; (2) guiding a learning algorithm using, e.g., specific initializations, constraints, or (loss) regularizations; (3) refining the model output, e.g., using closed-form equations or model simulations; (4) formulating hybrid models of theory and ML, and (5) augmenting theory-based models using real data such as data assimilation or calibration.</p>
<p>Overall, the explicit restriction of the solution space to scientifically consistent and plausible solutions is not a requirement to achieve valuable scientific outcomes. Neglecting this restriction, however, means that a consistent and plausible solution cannot be guaranteed, even if an optimal result has been achieved from a mathematical point of view.</p>
<p>III. SCIENTIFIC OUTCOMES FROM MACHINE LEARNING</p>
<p>In this section, we review examples that use ML and strive for different levels of transparency, interpretability, or explainability to produce scientific outcomes. To structure the different ML chains following Fig. 1, we define common groups and describe representative papers for each. The core ingredient is the basic ML chain, in which a model is learned from given input data and with a specific learning paradigm, yielding output results utilizing the learned model. In order to derive a scientific outcome, either the output results or the model is explained, where interpretability is the prerequisite for explainability. Moreover, transparency is required to explain a model. Generally, providing domain knowledge to an algorithm means to enhance the input data, model, optimizer, output results, or any other part of the ML algorithm by using information gained from domain insights such as laws of nature and chemical, biological, or physical models [2]. Besides the purpose of explainability, integrating domain knowledge can help with model tractability and regularization in scenarios where not enough data is available. It might also increase the performance of a model or reduce computational time.</p>
<p>In Table 1, we specify the four major groups and several subgroups in more detail. We expect that examples for additional subgroups can be found, but that will not affect our core observations. In particular, we distinguish between the following components:</p>
<p>Transparency We consider a model to be design-transparent if the model, or a part of it, was chosen for specific reasons, usually due to knowledge from the application domain. We call a model algorithmically transparent if the determination of the solution is obvious and traceable. In view of reproducible science, it is not surprising that essentially all the examples we found can be considered to be model-transparent. Interpretability We take a closer look at two types of interpretability. First, we consider model components, such as neurons in a NN or obtained latent variables, to be interpretable if they are represented in a way that can be further explained, e.g., with domain knowledge. Second, the scientific outcome, i.e., the decision of the model, can be interpreted using the input, e.g., by using heatmaps. Integration of domain knowledge We will look at several ways in which domain knowledge can be integrated.</p>
<p>On the one hand, domain knowledge is needed to explain -either to explain the scientific outcome or to derive scientific findings from the model or individual model components. On the other hand, domain knowledge can be integrated to enforce scientifically plausible and consistent results. This can be done in different ways; cf. [2]. Besides the integration of domain knowledge during the learning process of the model, it can also be used for post-hoc checks, where the scientific plausibility and consistency of the results is checked and possibly invalid results are removed.</p>
<p>The following collection of research works is a non-exhaustive selection from the literature of the last few years, where we aim to cover a broad range of usages of ML with a variety of scientific outcomes. Furthermore, we focus on examples that utilize an extensive amount of scientific domain knowledge from the natural sciences. Due to the recent uptake of NNs in the sciences, these tend to be the dominating ML approach in current literature. Nonetheless, many of the described ML workflows or the approaches to integrate domain knowledge can be performed with other ML methods as well. Note that the choice of a group for a given article is not always a clear judgement, particularly in view of how and where domain knowledge is employed, and in what form, and to what extent, an explanation is derived.</p>
<p>A. SCIENTIFIC OUTCOMES BY EXPLAINING OUTPUT RESULTS</p>
<p>Many works address the derivation of outcomes by learning an ML model and generalizing from known input-output relations to new input-output pairs. This represents the lowest degree of explainability without the need for a transparent or interpretable model. In the case that a scientifically useful outcome is to be estimated, most of these approaches so far solely explain what the outcome is from a scientific point of view (scientific explanation), but cannot answer the question of why this specific outcome was obtained from an algorithmic point of view (algorithmic explanation). Other approaches attempt to scientifically explain the output in terms of the specific corresponding input, given a learned model. Here, interpretation tools are utilized, where the model is used only as a means to an end to explain the result and it is not explicitly analyzed itself.</p>
<p>1) PREDICTION OF INTUITIVE OUTCOMES</p>
<p>The derivation of intuitive physics is a task that is often considered in papers from the following group. Intuitive physics are everyday-observed rules of nature that help us to predict the outcome of events even with a relatively untrained human perception [38].</p>
<p>Group 1a (Basic ML Chain): A black-box approach, or at most model-transparent approach, is used to derive an outcome. It is not interpretable and cannot be explained. The outcome is not or only minimally explainable from a scientific point of view.</p>
<p>For example, [39] uses video simulations to learn intuitive physics, e.g., about the stability of wooden block towers. They use ResNet-34 and GoogLeNet and formulate a binary classification task to predict whether these towers will fall. In a similar way, but with more complex scenes or differently shaped objects, [40] predicts the physical stability of stacked objects using various popular convolutional neural network (CNN) architectures. Reference [41] predicts the spread of diseases on barley plants in microscopic hyperspectral images by generating highly probable image-based appearances over the course of several days. They use cycle-consistent generative adversarial networks to learn how an image will change from one day to the next or to the previous day, albeit without any biological parameters involved. Reference [42] presents an approach for the design of new functional glasses that comprises the prediction of characteristics relevant for manufacturing as well as end-use properties of glass. They utilize NNs to estimate the liquidus temperatures for various silicate compositions consisting of up to eight different components. Generally, the identification of an optimized composition of the silicates yielding a suitable liquidus temperature is a costly task and is oftentimes based on trial-and-error. For this, they learn from several hundred composites with known output properties and apply the model to novel, unknown composites. In their workflow, they also consider, outside of the ML chain, other quantities of interest that are derived by physics-driven models. Reference [43] proposes a nonlinear regression approach employing NNs to learn closed form representations of partial differential equations (PDEs) from scattered data collected in space and time, thereby uncovering the dynamic dependencies and obtaining a model that can be subsequently used to forecast future states. In benchmark studies, using Burgers' equation, nonlinear Schrödinger equation, or Navier-Stokes equation, the underlying dynamics are learned from numerical simulation data up to a specific time. The obtained model is used to forecast future states, where relative L 2 -errors of up to the order of 10 −3 are observed. While the method inherently models the PDEs and the dynamics themselves, the rather general network model does not allow the drawing of direct scientific conclusions on the structure of the underlying process.</p>
<p>Group 1b: These models are not only model-but also design-transparent to some extent, where the design is chosen and motivated with certain intentions.</p>
<p>Besides the simple prediction network presented by [39] in Group 1a, they also propose a network called PhysNet to predict the trajectory of the wooden blocks in case the tower is collapsing. It is formulated as a mask prediction network trained for instance segmentation, where each wooden block is defined as one class. The construction of Phys-Net is made design-transparent in the sense that the network is constructed to capture the arrangement of blocks by using alternating upsampling and convolution layers, and an increased depth to reason about the block movement, as well. PhysNet outperforms human subjects on synthetic data and achieves comparable results on real data. Reference [44] designed a multi-source NN for exoplanet transit classification. They integrate additional information by adding identical information about centroid time-series data to all input sources, which is assumed to help the network learn important connections, and by concatenating the output of a hidden layer with stellar parameters, as it is assumed they are correlated with classification. Reference [45] introduces a framework that calculates physical concepts from color-depth videos that estimates tool and tool-use such as cracking a nut. In their work, they learn task-oriented representations for each tool and task combination defined over a graph with spatial, temporal, and causal relations. They distinguish between 13 physical concepts, e.g., painting a wall, and show that the framework is able to generalize from known to unseen concepts by selecting appropriate tools and tool-uses. A hybrid approach is presented by [46] to successfully model the properties of contaminant dispersion in soil. The authors extract temporal information from dynamic data using a long short-term memory network and combine it with static data in a NN. In this way, the network models the spatial correlations underlying the dispersion model, which are independent of the location of the contaminant. Reference [47] has proposed a data-centric approach for scientific design based on the combination of a generative model for the data being considered, e.g., an autoencoder trained on genomes or proteins, and a predictive model for a quantity or property of interest, e.g., disease indicators or protein fluorescence. For DNA sequence design, these two components are integrated by applying the predictive model to samples from the generative model. In this way, it is possible to generate new synthetic data samples that optimize the value of the quantity or property by leveraging an adaptive sampling technique over the generative model; see also [48].</p>
<p>Group 1c: Here, in addition to Group 1b, the design process is influenced by domain knowledge regarding the model, the cost function, or the feature generation process with the particular aim to enhance scientific consistency and plausibility.</p>
<p>For example, [49], [50] use physics-informed approaches for applications such as fluid simulations based on the incompressible Navier-Stokes equations, where physics-based losses are introduced to achieve plausible results. The idea in [49] is to use a transparent cost function design by reformulating the condition of divergence-free velocity fields into an unsupervised learning problem at each time step. The random forest model used in [50] to predict a fluid particle's velocity can be viewed as a transparent choice per se due to its simple nature. Reference [51] classifies land use and land cover and their changes based on remote sensing satellite timeseries data. They integrate domain knowledge about the transition of specific land use and land cover classes, such as forest or burnt areas, to increase the classification accuracy. They utilize a discriminative random field with transition matrices that contain the likelihoods of land cover and land use changes to enforce, for example, that a transition from burnt area to forest is unlikely.</p>
<p>2) PREDICTION OF SCIENTIFIC PARAMETERS AND PROPERTIES</p>
<p>Although the approaches just described set up prediction as a supervised learning problem, there is still a gap between common supervised tasks, e.g., classification, object detection, and prediction, and actual understanding of a scene and its reasoning. Like a layman in the corresponding scientific field, the methods presented so far do not learn a model that is able to capture and derive scientific properties and dynamics of phenomena or objects and their environment, as well as their interactions. Therefore, the model cannot inherently explain why an outcome was obtained from a scientific viewpoint. Reference [52] labels these respective approaches as bottomup, where observations are directly mapped to an estimate of some behavior or some outcome of a scenario. To tackle the challenge of achieving a higher explainability and a better scientific usability, several so-called top-down classification and regression frameworks have been formulated that infer scientific parameters. In both cases, only the scientific explanation is sought.</p>
<p>Group 2a: In these ML models, domain knowledge is incorporated, often to enforce scientific consistency. Therefore, the design process is partly transparent and tailored to the application. The outcome is explainable from a scientific point of view since scientific parameters and properties are derived, which can be used for further processing.</p>
<p>For example, [53] detects and tracks objects in videos in an unsupervised way. The authors use a regression CNN and introduce terms during training that measure the consistency of the output when compared to physical laws which specifically and thoroughly describe the dynamics in the video. In this case, the input of the regression network is a video sequence and the output is a time-series of physical parameters such as the height of a thrown object. By incorporating domain knowledge and image properties into their loss functions, part of their design process becomes transparent and explainability is gained due to comparisons to the underlying physical process. However, the model and the algorithms are not completely transparent since standard CNNs with an ADAM minimizer are employed. Although these choices of model and algorithms are common in ML, they are usually motivated by their good performance, and not because there is any application-driven reasoning behind it; thus, there is no design transparency on this aspect. Furthermore, the reason why such choice works for this highly nonconvex problem is currently not well understood from a mathematical point of view; therefore, no algorithmic transparency is present. Reference [54] introduces Physics101, a dataset of over 17,000 video clips containing 101 objects of different characteristics, which was built for the task of deriving physical parameters such as velocity and mass. In their work, they use the LeNet CNN architecture to capture visual as well as physical characteristics while explicitly integrating physical laws based on material and volume to aim for scientific consistency. Their experiments show that predictions can be made about the behavior of an object after a fall or a collision using estimated physical characteristics, which serve as input to an independent physical simulation model. Reference [55] introduces SMASH, which extracts physical collision parameters from videos of colliding objects, such as pre-and post collision velocities, to use them as inputs for existing physics engines for modifications. For this, they estimate the position and orientation of objects in videos using constrained least-squares estimation in compliance with physical laws such as momentum conservation. Based on the determined trajectories, parameters such as velocities can be derived. While their approach is based more on statistical parameter estimation than ML, their model and algorithm building process is completely transparent. Individual outcomes become explainable due to the direct relation of the computations to the underlying physical laws.</p>
<p>Reference [56] introduces Newtonian NNs in order to predict the long-term motion of objects from a single color image. Instead of predicting physical parameters from the image, they introduce 12 Newtonian scenarios serving as physical abstractions, where each scenario is defined by physical parameters defining the dynamics. The image, which contains the object of interest, is mapped to a state in one of these scenarios that best describes the current dynamics in the image. Newtonian NNs are two parallel CNNs: one encodes the images, while the other derives convolutional filters from videos acquired with a game engine simulating each of the 12 Newtonian scenarios. The specific coupling of both CNNs in the end leads to an interpretable approach, which also (partially) allows for explaining the classification results of a single input image.</p>
<p>A tensor-based approach to ML for uncertainty quantification problems can be found in [57], where the solutions to parametric convection-diffusion PDEs are learned based on a few samples. Rather than directly aiming for interpretability or explainability, this approach helps to speed up the process of gaining scientific insight by computing physically relevant quantities of interest from the solution space of the PDE. As there are convergence bounds for some cases, the design process is to some extent transparent and benefits from domain knowledge.</p>
<p>An information-based ML approach using NNs to solve an inverse problem in biomechanical applications was presented in [58]. Here, in mechanical property imaging of soft biological media under quasi-static loads, elasticity imaging parameters are computed from estimated stresses and strains. For a transparent design of the ML approach, domain knowledge is incorporated in two ways. First, NNs for a material model are pre-trained with stress-strain data, generated using linear-elastic equations, to avoid non-physical behavior. Second, finite-element analysis is used to model the data acquisition process.</p>
<p>Group 2b: These ML models are highly transparent, which means that the design process as well as the algorithmic components are fully accessible. The outcome of the model is explainable and the scientific consistency of the outcome is enforced.</p>
<p>For organic photovoltaics material, an approach utilizing quantum chemistry calculations and ML techniques to calibrate theoretical results to experimental data was presented by [59], [60]. The authors consider previously performed experiments as current knowledge, which is embedded within a probabilistic non-parametric mapping. In particular, GPs were used to learn the deviation of properties calculated by computational models from the experimental analogues. By employing the chemical Tanimoto similarity measure and building a prior based on experimental observations, design transparency is attained. Furthermore, since the prediction results involves a confidence in each calibration point being returned, the user can be informed when the scheme is being used for systems for which it is not suited [59]. In [60], 838 high-performing candidate molecules have been identified within the explored molecular space thanks to the newly possible efficient screening of over 51,000 molecules.</p>
<p>In [61], a data-driven algorithm for learning the coefficients of general parametric linear differential equations from noisy data was introduced, solving a so-called inverse problem. The approach employs GP priors that are tailored to the corresponding and known type of differential operators, resulting in design and algorithmic transparency. The combination of rather generic ML models with domain knowledge in the form of the structure of the underlying differential equations leads to an efficient method. Besides classical benchmark problems with different attributes, the approach was used on an example application in functional genomics, determining the structure and dynamics of genetic networks based on real expression data.</p>
<p>Group 2c: These ML models are similar to the models in Group 2a, but besides enforced scientific consistency and plausibility of the explainable outcome, an additional post-hoc consistency check is performed.</p>
<p>In [62], a deep learning approach for Reynolds-averaged Navier-Stokes (RANS) turbulence modelling was presented. Here, domain knowledge led to the construction of a network architecture that embedded invariance using a higher-order multiplicative layer. This was shown to result in significantly more accurate predictions compared to a generic, but less interpretable, NN architecture. Further, the improved prediction on a test case that had a different geometry than any of the training cases indicates that improved RANS predictions for more than just interpolation situations seem achievable. A related approach for RANS-modeled Reynolds stresses for high-speed flat-plate turbulent boundary layers was presented in [63], which uses a systematic approach with basis tensor invariants proposed by [64]. Additionally, a metric of prediction confidence and a nonlinear dimensionality reduction technique are employed to provide a priori assessment of the prediction confidence.</p>
<p>3) INTERPRETATION TOOLS FOR SCIENTIFIC OUTCOMES</p>
<p>Commonly used feature selection and extraction methods enhance the interpretability of the input data, and thus can lead to outcomes that can be explained by interpretable input. Other approaches use interpretation tools to extract information from learned models and to help to scientifically explain the individual output or several outputs jointly. Often, approaches are undertaken to present this information via feature importance plots, visualizations of learned representations, natural language representations, or the discussion of examples. Nonetheless, human interaction is still required to interpret this additional information, which has to be derived ante-hoc or with help of the learned model during a post-hoc analysis.</p>
<p>Group 3a: These ML approaches use interpretation tools to explain the outcome by means of an interpretable representation of the input. Such tools include feature importance plots or heatmaps.</p>
<p>While handcrafted and manually selected features are typically easier to understand, automatically determined features can reveal previously unknown scientific attributes and structures. Reference [65], for example, proposes FINE (feature importance in nonlinear embeddings) for the analysis of cancer patterns in breast cancer tissue slides. This approach relates original and automatically derived features to each other by estimating the relative contributions of the original features to the reduced-dimensionality manifold. This procedure can be combined with various, possibly intransparent, nonlinear dimensionality reduction techniques. Due to the feature contribution detection, the resulting scheme remains interpretable.</p>
<p>Arguably, visualizations are one of the most widely used interpretation tools. Reference [21] gives a survey of visual analytics in deep learning research, where such visualization systems have been developed to support model explanation, interpretation, debugging, and improvement. The main consumers of these analytics are the model developers and users as well as non-experts. Reference [66] uses interpretation tools for image-based plant stress phenotyping. The authors train a CNN model and identify the most important feature maps in various layers that isolate the visual cues for stress and disease symptoms. They produce so-called explanation maps as sums of the most important features maps indicated by their activation level. A comparison of manually marked visual cues by an expert and the automatically derived explanation maps reveals a high level of agreement between the automatic approach and human ratings. The goals of their approach are the analysis of the performance of their model, the provision of visual cues that are human-interpretable to support the prediction of the system, and a provision of important cues for the identification of plant stress. References [39] and [67] use attention heatmaps to visualize the stackability of multiple wooden blocks in images. They conduct a conclusion study by applying localized blurring to the image and collecting the resulting changes in the stability classification of the wooden blocks into a heatmap. Moreover, [67] provides a first step towards a physics-aware model by using their trained stability predictor and heatmap analysis to provide stackability scores for unseen object sets, for the estimation of an optimal placement of blocks, and to counterbalance instabilities by placing additional objects on top of unstable stacks.</p>
<p>As another example, ML has been applied to functional magnetic resonance imaging data to design biomarkers that are predictive of psychiatric disorders. However, only ''surrogate'' labels are available, e.g., behavioral scores, and so the biomarkers themselves are also ''surrogates'' of the optimal descriptors [68], [69]. The biomarker design promotes spatially compact pixel selections, producing biomarkers for disease prediction that are focused on regions of the brain. These are then assessed by expert physicians. As the analysis is based on high-dimensional linear regression approaches, transparency of the ML model is assured. Reference [70] introduces DeepTune, a visualization framework for CNNs, for applications in neuroscience. DeepTune consists of an ensemble of CNNs that learn multiple complementary representations of natural images. The features from these CNNs are fed into regression models to predict the firing rates of neurons in the visual cortex. The interpretable DeepTune images, i.e., representative images of the visual stimuli for each neuron, are generated from an optimization process and pooling over all ensemble members.</p>
<p>Classical tools such as confusion matrices are also used as interpretation tools on the way to scientific outcomes. In a bio-acoustic application for the recognition of anurans using acoustic sensors, [71] uses a hierarchical approach to jointly classify on three taxonomic levels, namely the family, the genus, and the species. Investigating the confusion matrix per level enabled for example the identification of bio-acoustic similarities between different species.</p>
<p>Group 3b: These models are design-transparent in the sense that they use specially tailored components such as attention modules to achieve increased interpretability. The output is explained by the input using the specially selected components.</p>
<p>In [72], [73] attention-based NN models are employed to classify and segment histological images, e.g., microscopic tissue images, magnetic resonance imaging (MRI), or computed tomography (CT) scans. Reference [72] found that the employed modules turned out to be very attentive to regions of pathological, cancerous tissue and non-attentive in other regions. Furthermore, [73] builds an attentive gated network that gradually fitted its attention weights with respect to targeted organ boundaries in segmenting tasks. The authors also used their attention maps to employ a weakly supervised object detection algorithm, which successfully created bounding boxes for different organs.</p>
<p>Interpretability methods have also been used for applications that utilize time-series data, often by way of highlighting features of the sequence data. For example, [74] applies attention modules in NNs trained on genomic sequences for the identification of important sequence motifs by visualizing the attention mask weights. They propose a genetic architect that finds a suitable network architecture by iteratively searching over various NN building blocks. In particular, they state that the choice of the NN architecture highly depends on the application domain, which is a challenge if no prior knowledge is available about the network design. It is cautioned that, depending on the optimized architecture, attention modules and expert knowledge may lead to different scientific insights. Reference [75] uses attention modules for genomics in their AttentiveChrome NN. The network contains a hierarchy of attention modules to gain insights about where and on what the network has focused on and, thus, gain interpretability of the results. Also [76] developed a hierarchical attention-based interpretation tool called RETAIN (REverse Time AttentIoN) in healthcare. The tool identifies influential past visits of a patient as well as important clinical variables during these visits from the patient's medical history to support medical explanations. Attention modules in recurrent NNs for multi-modal sensorbased activity recognition have been used by [77]. Depending on the activity, their approach provides the most contributing body parts, modals, and sensors for the network's decision.</p>
<p>Group 3c: As in Group 3b, these ML approaches use interpretation tools for a better understanding of the model's decision. Moreover, they integrate domain knowledge to enhance the scientific consistency and plausibility, for example, in combination with the outcome of interpretation tools.</p>
<p>For example, [78] discusses explainable ML for scientific discoveries in material sciences. In their work, they propose an ensemble of simple models to predict material properties along with a novel evaluation focusing on trust by quantifying generalization performance. Moreover, their pipeline contains a rationale generator that provides decision-level interpretations for individual predictions and model-level interpretations for the whole regression model. In detail, they produce interpretations in terms of prototypes that are analyzed and explained by an expert, as well as global interpretations by estimating feature importance for material sub-classes. Moreover, they use domain knowledge for the definition of material sub-classes and integrate it into the estimation process. Reference [79] proposes contextual decomposition explanation penalization, which constrains a classification or regression result to more accurate and more scientifically plausible results by leveraging the explained outcome of interpretation tools. They add an explanation term in the loss function, which compares the interpretation outcome (e.g., a heatmap indicating the important parts in the image) and an interpretation provided by the user. They determine a more accurate model from an International Skin Imaging Collaboration dataset whose goal is to classify cancerous and non-cancerous images, by learning that colorful patches present only in the benign data are not relevant for classification.</p>
<p>Group 3d: These approaches use the common featureoriented representation with focus on the disentanglement of the underlying factors of variation in a system, which can be explained by an expert afterwards. Domain knowledge is employed in the design of the model and in the interpretation of the outcome.</p>
<p>A broad framework leverages unsupervised learning approaches to learn low-complexity representations of physical process observations. In many cases where the underlying process features a small number of degrees of freedom, it is shown that nonlinear manifold learning algorithms are able to discern these degrees of freedoms as the component dimensions of low-dimensional nonlinear manifold embeddings, which preserve the underlying geometry of the original data space [80]- [82]. It can be seen that the embedding coordinates relate to known physical quantities. At this stage, ongoing research is focused on obtaining new scientific outcomes in new situations using this promising approach. In a similar way, [83] and [84] use principal component analysis and the derived interpretable principal components for exploration of different phases, phase-transition, and crossovers in classical spin models. Embedded feature selection schemes have been recently explored to establish or refine models in physical processes. Using a sparsity-promoting penalty, they propose groups of variables that may explain a property of interest and promote the simplest model, i.e., the model involving the fewest variables possible while achieving a target accuracy. Domain knowledge is employed during the selection of the dictionary of candidate features. The application of sparsity has also proved fruitful in the broader class of problems leveraging PDEs and dynamical system models [85]- [89].</p>
<p>The combination of parse trees with ML is investigated in [90], [91]. A so-called syntax-direct variational autoencoder is introduced in [90], where syntax and semantic constraints are used in a generative model for structured data. As an application, the drug properties of molecules are predicted. The learned latent space is visually interpreted, while the diversity of the generated molecules is interpreted using domain expertise. The work in [91] uses a NN during a Monte Carlo tree search to guide its finding of an expression for symbolic regression that conforms to a set of data points and has the desired leading polynomial powers of the data. The NN learns the relation between syntactic structure and leading powers. As a proof-of-concept application, the authors are able to learn a physical force field, where the leading powers in the short and long ranges are known by domain experts and can be used as asymptotic constraints. Reference [92] proposes a sparsity-enforcing technique to recover domain-specific meaning for the abstract embedding coordinates obtained from unsupervised nonlinear dimensionality reduction approaches in a principled fashion. The ansatz is to explain the embedding coordinates as nonlinear compositions of functions from a user-defined dictionary. As an illustrative example, the ethanol molecule is studied, where the approach identifies the bond torsions that explain the torus obtained from the embedding method, which reflects the two rotational degrees of freedom.</p>
<p>Group 3e: In addition to the works in Group 3d, domain knowledge is employed to perform a posteriori consistency checks on feature-oriented representations.</p>
<p>Feature selection schemes using embedded methods, similar to the previous group, have been used in areas such as material sciences [93], [94]. In contrast to the preceding works, additional consistency checks on the outcome of the predictive model are performed based on domain expertise, including the robustness of the model and in particular their extrapolation capability for predicting new materials.</p>
<p>B. SCIENTIFIC OUTCOMES BY EXPLAINING MODELS</p>
<p>In the following examples, either interpretation tools are used to project processes in the model into a space that is interpretable or the model is designed inherently to be interpretable. In this way, models and their components can be explained utilizing domain knowledge.</p>
<p>Group 4a: These models are designed in a transparent way and the model design enforces that model components are interpretable and scientifically explainable. Due to their design, scientific consistency and plausibility is enforced, even if not as a primary goal. The explanation of specific model components is meant to lead to novel scientific discoveries or insights.</p>
<p>Complex ML methods such as NNs, for example, can be customized to a specific scientific application so that the used architecture restricts or promotes properties that are desirable in the data modeled by the network. For example, in [95], an application of ML for epidemiology leverages a networked dynamical system model for contagion dynamics, where nodes correspond to subjects with assigned states; thus, most properties of the ML model match the properties of the scientific domain considered. A complex NN is reduced by [96] to understand processes in neuroscience. By reducing the number of units in the complex model by means of a quantified importance utilizing gradients and activation values, a simple NN with one hidden layer is derived that can be easily related to neuroscientific processes. Reference [97] constructs a NN for computing Koopman eigenfunctions from data. Motivated by domain knowledge, the authors employ an auxiliary network to parameterize the continuous frequency. Thereby, a compact autoencoder model is obtained, which, in addition, is interpretable. For the example of the nonlinear pendulum, two eigenfunctions are learned with a NN, which can be mapped into magnitude and phase coordinates. In this interpretable form, it can be observed that the magnitude traces level sets of the Hamiltonian energy, a new insight that turned out to be consistent with recent theoretical derivations previously unknown to the authors. Reference [98] introduces visible NNs, which encode the hierarchical structure of a gene ontology tree into a NN, either from literature or inferred from large-scale molecular data sets. This enables transparent biological interpretation, while successfully predicting effects of gene mutations on cell proliferation. Furthermore, it is argued that the employed deep hierarchical structure captures many different clusters of features at multiple scales and pushes interpretation from the model input to internal features representing biological subsystems. In their work, despite no information about subsystem states being provided during model training, previously undocumented learned subsystem states could be confirmed by molecular measurements.</p>
<p>Beside NNs, other ML algorithms can also be used to derive scientific outcomes from an interpretable model. Reference [99] use the ML algorithm 'Sir Isaac' to infer a dynamical model of biological time-series data to understand and predict dynamics of worm escape behavior. They model a system of differential equations, where the number of hidden variables is determined automatically from the system, and their meaning can be explained by an expert.</p>
<p>Reference [100] introduces SciNet, a modified variational autoencoder that learns a representation from experimental data and uses the learned representation to derive physical concepts from it rather than from the experimental input data. The learned representation is forced to be much simpler than the experimental data, for example by being captured in a few neurons, and it contains the explanatory factors of the system, such as the physical parameters. This is proven by the fact that physical parameters and the activations of the neurons in the hidden layers have a linear relationship. Additionally, [101] constructs the bottleneck layer in their NN to represent physical parameters to predict the outcome of a collision of objects from videos. However, the architecture of the bottleneck layer is not learned, but designed with prior knowledge about the underlying physical process.</p>
<p>Understanding structures such as groups, relations, and interactions is one of the main goals to achieve scientific outcomes. However, it constitutes a core challenge, and so far only limited amount of work has been conducted in this area. Reference [102], for example, introduces a grouping layer in a graph-based NN called GroupINN to identify subgroups of neurons in an end-to-end model. In their work, they build a network for the analysis of time-series of functional magnetic resonance images of the brain, which are represented as functional graphs, with the goal of revealing relationships between highly predictive brain regions and cognitive functions. Instead of working with the whole functional graph, they exploit a grouping layer in the network to identify groups of neurons, where each neuron represents a node in the graph and corresponds to a physical region of interest in the brain. The grouped nodes in the coarsened graph are assigned to regions of interest, which are useful for prediction of cognitive functions, and the connections between the groups are defined as functional connections.</p>
<p>Reference [103] introduces neural interaction detection, a framework with variants of feedforward NNs for detecting statistical interactions. By examining the learned weight matrices of the hidden units, their framework was able to analyze feature interactions in a Higgs boson dataset. Specifically, they analyze feature interactions in simulated particle environments that originate from the decay of a Higgs boson. Deep tensor networks are used by [104] in quantum chemistry to predict molecular energy up to chemical accuracy, while allowing interpretations. A so-called local chemical potential, a variant of sensitivity analysis where one measures the effect on the NN output of inserting a charge at a given location, can be used to gain further chemical insight from the learned model. As an example, a classification of aromatic rings with respect to their stability can be determined from these three-dimensional response maps.</p>
<p>Group 4b: These ML models are designed with a high degree of transparency and with the goal to derive scientifically plausible results. Due to this, the outcome of the model and the model components themselves are interpretable and VOLUME 8, 2020 can be scientifically explained. In contrast to the works presented in group 4a, the following examples employ methods that are also algorithmically transparent.</p>
<p>Different types of physics-aware GP models in remote sensing were studied by [105] with the goal to estimate bio-physical parameters such as leaf area index. In one case, a latent force model that incorporates ordinary differential equations is used in inverse modelling from real in-situ data. The learned latent representation allowed an interpretation in view of the physical mechanism that generated the input-output observed relations, i.e., one latent function captured the smooth and periodic component of the output, while two other focus on the noisier part with an important residual periodical component. So-called order parameters in condensed matter physics are analysed in [106], [107]. Using domain knowledge, a kernel is introduced to investigate O(3)-breaking orientational order. A two-class and a multi-class setting are tackled with support vector machines (SVM). The decision function is physically interpreted as an observable corresponding to an order parameter curve, while the bias-term of the SVM can be exploited to detect phase transitions. Furthermore, nontrivial blocks of the SVM kernel matrices can be identified with so-called spin color indices. In these works, the analytical order parameters for spin and orbital systems could be extracted.</p>
<p>C. RELATED SURVEYS ABOUT MACHINE LEARNING IN THE NATURAL SCIENCES</p>
<p>Reference [108] gives on overview on recent research using ML for molecular and materials science. Given that standard ML models are numerical, the algorithms need suitable numerical representations that capture relevant chemical properties, such as the Coulomb matrix and graphs for molecules, and radial distribution functions that represent crystal structures. Supervised learning systems are in common use to predict numerical properties of chemical compounds and materials. Unsupervised learning and generative models are being used to guide chemical synthesis and compound discovery processes, where deep learning algorithms and generative adversarial networks have been successfully employed. Alternative models exploiting the similarities between organic chemistry and linguistics are based on textual representations of chemical compounds.</p>
<p>A review on the manifold recent research topics in the physical sciences is given by [109], with applications in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. The authors observe a surge of interest in ML, while noting that the research is starting to move from exploratory efforts on toy models to the use of real experimental data. It is stressed that an understanding of the potential and the limitations of ML includes insight into the breaking point of these methods, but also the theoretical justification of the performance in specific situations, be it positive or negative.</p>
<p>In single-cell genomics, computational data-driven analysis methods are employed to reveal the diverse simultaneous facets of a cell's identity, including a specific state on a developmental trajectory, the cell cycle, or a spatial context. The analysis goal is to obtain an interpretable representation of the dynamic transitions a cell undergoes that allows a determination of different aspects of cellular organization and function. There is an emphasis on unsupervised learning approaches to cluster cells from single-cell profiles, and thereby to systematically detect previously unknown cellular subtypes. Defining markers for these subtypes are then investigated in a second step. See [110] for a review on key questions, progress, and open challenges in this application field.</p>
<p>Several ML approaches have been used in biology and medicine to derive new insights, as described in [111] for the broad class of deep learning methods. Supervised learning mostly focuses on the classification of diseases and disease types, patient categorization, and drug interaction prediction. Unsupervised learning has been applied to drug discovery. The authors point out that in addition to the derivation of new findings, an explanation of these is of great importance. Furthermore, the need in deep learning for large training datasets poses a limit to its current applicability beyond imaging (through data augmentation) and so-called 'omics' studies. An overview of deep learning approaches in systems biology is given in [112]. The authors describe how one can design NNs that encode the extensive, existing networkand systems-level knowledge that is generated by combing diverse data types. It is said that such designs inform the model on aspects of the hierarchical interactions in the biological systems that are important for making accurate predictions but are not available in the input data. Reference [113] discusses the difference between explainability and causality for medical applications, and the necessity of a person to be involved. For the successful application of ML for drug design, [114] identifies five ''grand challenges'': obtaining appropriate datasets, generating new hypotheses, optimizing in a multi-objective manner, reducing cycle times, and changing the research culture and mindset. These underlying themes should be valid for many scientific endeavours.</p>
<p>Reference [37] gives an overview of ML research in Earth system science. The authors conclude that, while the general cycle of exploration, hypotheses generation and testing remains the same, modern data-driven science and ML can extract patterns in observational data to challenge complex theories and Earth system models, and thereby strongly complement and enrich geoscientific research. Moreover, [115] observes that a close collaboration with domain experts in the geoscientific area and ML researchers is necessary to solve novel and relevant tasks. They state that developing interpretable and transparent methods is one of the major goals to understand patterns and structures in the data and to turn it into scientific value.</p>
<p>IV. DISCUSSION</p>
<p>In this work, we reviewed the concept of explainable machine learning and discerned between transparency, interpretabil- ity, and explainability. We also discussed the possibility of influencing model design choices and the step of interpreting algorithmic outputs by domain knowledge and a posteriori consistency checks. We presented a more fine-grained characterization of different stages of explainability, which we briefly elaborated on by means of several recent exemplary works in the field of machine learning in the natural sciences; see Table 2 for a summary.</p>
<p>While machine learning is employed in uncountable scientific projects and publications nowadays, the vast majority is not concerned with aspects of interpretability or explainability. We argue that the latter is crucial for extracting truly novel scientific results and ideas from employing ML methods. Therefore, we hope that this survey provides new ideas and methodologies to scientists looking for means to explain their algorithmic results or to extract relevant insights on the corresponding study object.</p>
<p>Finally, we note that as an additional component in the scientific data analysis workflow of the future, we expect causal inference [116], [117] to play a role. Having said this, we believe that causal inference will require even more basic research than what is still needed for the uptake of explainable machine learning in the natural sciences.</p>
<p>ACKNOWLEDGMENT</p>
<p>Part of the work was performed during the long-term program on ''Science at Extreme Scales: Where Big Data Meets Large-Scale Computing'' held at the Institute for Pure and Applied Mathematics, University of California Los Angeles, USA. The authors would like to thank financial support during the program. They would also like to thank the participants of the long term program for fruitful discussions, in particular K. Dow, L. Gao, P. Grandinetti, P. Hähnel, M. Haghighatlari, and R. Jäkel. Additionally, they cordially thank L. von Rueden and C. Robinson. (Ribana Roscher and Jochen Garcke contributed equally to this work.)</p>
<p>FIGURE 1 .
1Major ML-based chains from which scientific outcomes can be derived: The commonly used, basic ML chain (light gray box) learns a black box model from given input data and provides an output. Given the black box model and input-output relations, a scientific outcome can be derived by explaining the output results utilizing domain knowledge. Alternatively, a transparent and interpretable model can be explained using domain knowledge leading to scientific outcomes. Additionally, the incorporation of domain knowledge can promote scientifically consistent solutions (green arrows).</p>
<p>An important contribution to the understanding of ML algorithms is their mathematical interpretation and derivation, which help to understand when and how to use these approaches. Classical examples are the Kalman filter or principal component analysis, where several mathematical derivations exist for each and enhance their understanding.transparency is the opposite of opac-
ity or ''black-boxness''. It connotes some sense of 
understanding the mechanism by which the model 
works. Transparency is considered here at the level 
of the entire model (simulatability), at the level of 
individual components such as parameters (decom-
posability), and at the level of the training algo-
rithm (algorithmic transparency). </p>
<p>TABLE 2 .
2Collection of all references regarding transparency ( : at most model transparent, without color: design transparent, : design + algorithmically transparent), interpretability, and integration of domain knowledge.
https://www.merriam-webster.com/dictionary/interpret 42202 VOLUME 8, 2020
VOLUME 8, 2020   </p>
<p>Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. W Samek, T Wiegand, K.-R Müller, ITU J., ICT Discoveries-Impact Artif. Intell. (AI) Commun. Netw. Services. 11W. Samek, T. Wiegand, and K.-R. Müller, ''Explainable artificial intel- ligence: Understanding, visualizing and interpreting deep learning mod- els,'' ITU J., ICT Discoveries-Impact Artif. Intell. (AI) Commun. Netw. Services, vol. 1, no. 1, pp. 39-48, 2018.</p>
<p>Informed machine learning -a taxonomy and survey of integrating knowledge into learning systems. L Von Rueden, S Mayer, K Beckh, B Georgiev, S Giesselbach, R Heese, B Kirsch, J Pfrommer, A Pick, R Ramamurthy, M Walczak, J Garcke, C Bauckhage, J Schuecker, arXiv:1903.12394L. von Rueden, S. Mayer, K. Beckh, B. Georgiev, S. Giessel- bach, R. Heese, B. Kirsch, J. Pfrommer, A. Pick, R. Ramamurthy, M. Walczak, J. Garcke, C. Bauckhage, and J. Schuecker, ''Informed machine learning -a taxonomy and survey of integrating knowledge into learning systems,'' 2019, arXiv:1903.12394. [Online]. Available: http://arxiv.org/abs/1903.12394</p>
<p>The challenge of crafting intelligible intelligence. D S Weld, G Bansal, Commun. ACM. 626D. S. Weld and G. Bansal, ''The challenge of crafting intelligible intelli- gence,'' Commun. ACM, vol. 62, no. 6, pp. 70-79, May 2019.</p>
<p>Scientific representation. R Frigg, J Nguyen, The Stanford Encyclopedia of Philosophy, E. N. Zalta. Stanford, CA, USAStanford Univ.R. Frigg and J. Nguyen, ''Scientific representation,'' in The Stanford Encyclopedia of Philosophy, E. N. Zalta, Ed. Stanford, CA, USA: Stanford Univ., 2018.</p>
<p>S L Brunton, J N Kutz, Data-Driven Science and Engineering. Cambridge, U.K.Cambridge Univ. PressS. L. Brunton and J. N. Kutz, Data-Driven Science and Engineering. Cambridge, U.K.: Cambridge Univ. Press, 2019.</p>
<p>Towards a rigorous science of interpretable machine learning. F Doshi-Velez, B Kim, arXiv:1702.08608F. Doshi-Velez and B. Kim, ''Towards a rigorous science of inter- pretable machine learning,'' 2017, arXiv:1702.08608. [Online]. Avail- able: http://arxiv.org/abs/1702.08608</p>
<p>Learning a physical long-term predictor. S Ehrhardt, A Monszpart, N J Mitra, A Vedaldi, arXiv:1703.00247S. Ehrhardt, A. Monszpart, N. J. Mitra, and A. Vedaldi, ''Learning a phys- ical long-term predictor,'' 2017, arXiv:1703.00247. [Online]. Available: http://arxiv.org/abs/1703.00247</p>
<p>A survey of methods for explaining black box models. R Guidotti, A Monreale, S Ruggieri, F Turini, F Giannotti, D Pedreschi, ACM Comput. Surv. 515R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi, ''A survey of methods for explaining black box models,'' ACM Comput. Surv., vol. 51, no. 5, pp. 1-42, Aug. 2018.</p>
<p>The mythos of model interpretability. Z C Lipton, Commun. ACM. 6110Z. C. Lipton, ''The mythos of model interpretability,'' Commun. ACM, vol. 61, no. 10, pp. 36-43, Sep. 2018.</p>
<p>Methods for interpreting and understanding deep neural networks. G Montavon, W Samek, K.-R Müller, Digit. Signal Process. 73G. Montavon, W. Samek, and K.-R. Müller, ''Methods for interpreting and understanding deep neural networks,'' Digit. Signal Process., vol. 73, pp. 1-15, Feb. 2018.</p>
<p>Interpretable machine learning: Definitions, methods, and applications. W James Murdoch, C Singh, K Kumbier, R Abbasi-Asl, B Yu, arXiv:1901.04592W. James Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu, ''Interpretable machine learning: Definitions, methods, and applications,'' 2019, arXiv:1901.04592. [Online]. Available: http:// arxiv.org/abs/1901.04592</p>
<p>Kernel methods in machine learning. T Hofmann, B Schölkopf, A J Smola, Ann. Stat. 363T. Hofmann, B. Schölkopf, and A. J. Smola, ''Kernel methods in machine learning,'' Ann. Stat., vol. 36, no. 3, pp. 1171-1220, 2008.</p>
<p>C E Rasmussen, C K I Williams, Gaussian Processes for Machine Learning. Cambridge, MA, USAMIT PressC. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. Cambridge, MA, USA: MIT Press, 2006.</p>
<p>I Goodfellow, Y Bengio, A Courville, Deep Learning. Cambridge, MA, USAMIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.</p>
<p>Interpreting deep learning: The machine learning rorschach test?. A S Charles, arXiv:1806.00148A. S. Charles, ''Interpreting deep learning: The machine learn- ing rorschach test?'' 2018, arXiv:1806.00148. [Online]. Available: http://arxiv.org/abs/1806.00148</p>
<p>Interpretable machine learning for inferring the phase boundaries in a nonequilibrium system. C Casert, T Vieijra, J Nys, J Ryckebusch, Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat. Interdiscip. Top. 992Art. no. 023304C. Casert, T. Vieijra, J. Nys, and J. Ryckebusch, ''Interpretable machine learning for inferring the phase boundaries in a nonequilibrium system,'' Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat. Interdiscip. Top., vol. 99, no. 2, Feb. 2019, Art. no. 023304.</p>
<p>Survey and critique of techniques for extracting rules from trained artificial neural networks. R Andrews, J Diederich, A B Tickle, Based Syst. 86R. Andrews, J. Diederich, and A. B. Tickle, ''Survey and critique of techniques for extracting rules from trained artificial neural networks,'' Knowl.-Based Syst., vol. 8, no. 6, pp. 373-389, Dec. 1995.</p>
<p>Why should i trust you?. M T Ribeiro, S Singh, C Guestrin, '' in Proc. KDD. M. T. Ribeiro, S. Singh, and C. Guestrin, ''Why should i trust you?'' in Proc. KDD, 2016, pp. 1135-1144.</p>
<p>Sensitivity Analysis in Practice: A Guide to Assessing Scientific Models. A Saltelli, S Tarantola, F Campolongo, M Ratto, WileyHoboken, NJ, USAA. Saltelli, S. Tarantola, F. Campolongo, and M. Ratto, Sensitivity Anal- ysis in Practice: A Guide to Assessing Scientific Models. Hoboken, NJ, USA: Wiley, 2004.</p>
<p>Peeking inside the black-box: A survey on explainable artificial intelligence (XAI). A Adadi, M Berrada, IEEE Access. 6A. Adadi and M. Berrada, ''Peeking inside the black-box: A sur- vey on explainable artificial intelligence (XAI),'' IEEE Access, vol. 6, pp. 52138-52160, 2018.</p>
<p>Visual analytics in deep learning: An interrogative survey for the next frontiers. F Hohman, M Kahng, R Pienta, D H Chau, IEEE Trans. Vis. Comput. Graphics. 258F. Hohman, M. Kahng, R. Pienta, and D. H. Chau, ''Visual analytics in deep learning: An interrogative survey for the next frontiers,'' IEEE Trans. Vis. Comput. Graphics, vol. 25, no. 8, pp. 2674-2693, Aug. 2019.</p>
<p>The building blocks of interpretability. C Olah, A Satyanarayan, I Johnson, S Carter, L Schubert, K Ye, A Mordvintsev, 310C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, and A. Mordvintsev, ''The building blocks of interpretability,'' Distill, vol. 3, no. 3, p. e10, Mar. 2018.</p>
<p>A ratedistortion framework for explaining neural network decisions. J Macdonald, S Wäldchen, S Hauch, G Kutyniok, arXiv:1905.11092J. Macdonald, S. Wäldchen, S. Hauch, and G. Kutyniok, ''A rate- distortion framework for explaining neural network decisions,'' 2019, arXiv:1905.11092. [Online]. Available: http://arxiv.org/abs/1905.11092</p>
<p>Nonlinear Dimensionality Reduction (Information Science and Statistics). J A Lee, M Verleysen, SpringerNew York, NY, USAJ. A. Lee and M. Verleysen, Nonlinear Dimensionality Reduction (Infor- mation Science and Statistics). New York, NY, USA: Springer, 2007.</p>
<p>Nonnegative Matrix Tensor Factorization. A Cichocki, R Zdunek, A H Phan, S I Amari, WileyHoboken, NJ, USAA. Cichocki, R. Zdunek, A. H. Phan, and S. I. Amari, Nonnegative Matrix Tensor Factorization. Hoboken, NJ, USA: Wiley, 2009.</p>
<p>Applications of tensor (multiway array) factorizations and decompositions in data mining. M Mørup, WIREs Data Mining Knowl. Discovery. 11M. Mørup, ''Applications of tensor (multiway array) factorizations and decompositions in data mining,'' WIREs Data Mining Knowl. Discovery, vol. 1, no. 1, pp. 24-40, Jan. 2011.</p>
<p>Unmasking clever hans predictors and assessing what machines really learn. S Lapuschkin, S Wäldchen, A Binder, G Montavon, W Samek, K.-R Müller, Nature Commun. 1011096S. Lapuschkin, S. Wäldchen, A. Binder, G. Montavon, W. Samek, and K.-R. Müller, ''Unmasking clever hans predictors and assessing what machines really learn,'' Nature Commun., vol. 10, no. 1, p. 1096, Mar. 2019.</p>
<p>Explaining explanations in AI. B Mittelstadt, C Russell, S Wachter, Proc. Conf. Fairness, Accountability, Transparency. Conf. Fairness, Accountability, TransparencyB. Mittelstadt, C. Russell, and S. Wachter, ''Explaining explanations in AI,'' in Proc. Conf. Fairness, Accountability, Transparency, 2019, pp. 279-288.</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. T Miller, Artif. Intell. 267T. Miller, ''Explanation in artificial intelligence: Insights from the social sciences,'' Artif. Intell., vol. 267, pp. 1-38, Feb. 2019.</p>
<p>Explain in scientific discourse. J A Overton, 190J. A. Overton, ''Explain in scientific discourse,'' Synthese, vol. 190, no. 8, pp. 1383-1405, 2013.</p>
<p>Object-oriented deep learning. Q Liao, T Poggio, Center Brains, Minds Mach. (CBMM). Cambridge, MA, USA, TechQ. Liao and T. Poggio, ''Object-oriented deep learning,'' Center Brains, Minds Mach. (CBMM), Cambridge, MA, USA, Tech. Rep., 2017.</p>
<p>European Union regulations on algorithmic decision-making and a 'right to explanation. B Goodman, S Flaxman, AI Mag. 383B. Goodman and S. Flaxman, ''European Union regulations on algorith- mic decision-making and a 'right to explanation,''' AI Mag., vol. 38, no. 3, pp. 50-57, 2017.</p>
<p>Self-regulation of cognitive inference and decision processes. R F Baumeister, L S Newman, Personality Social Psychol. Bull. 201R. F. Baumeister and L. S. Newman, ''Self-regulation of cognitive infer- ence and decision processes,'' Personality Social Psychol. Bull., vol. 20, no. 1, pp. 3-19, Jul. 2016.</p>
<p>Explanation, imagination, and confidence in judgment. D J Koehler, Psychol. Bull. 1103D. J. Koehler, ''Explanation, imagination, and confidence in judgment,'' Psychol. Bull., vol. 110, no. 3, pp. 499-519, 1991.</p>
<p>Theory-guided data science: A new paradigm for scientific discovery from data. A Karpatne, G Atluri, J H Faghmous, M Steinbach, A Banerjee, A Ganguly, S Shekhar, N Samatova, V Kumar, IEEE Trans. Knowl. Data Eng. 2910A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar, N. Samatova, and V. Kumar, ''Theory-guided data science: A new paradigm for scientific discovery from data,'' IEEE Trans. Knowl. Data Eng., vol. 29, no. 10, pp. 2318-2331, Oct. 2017.</p>
<p>Physics informed deep learning (part II): Data-driven discovery of nonlinear partial differential equations. M Raissi, P Perdikaris, G Em Karniadakis, arXiv:1711.10566M. Raissi, P. Perdikaris, and G. Em Karniadakis, ''Physics informed deep learning (part II): Data-driven discovery of nonlinear partial differential equations,'' 2017, arXiv:1711.10566. [Online]. Available: http://arxiv.org/abs/1711.10566</p>
<p>Deep learning and process understanding for data-driven earth system science. M Reichstein, G Camps-Valls, B Stevens, M Jung, J Denzler, N Carvalhais, Prabhat , Nature. 5667743M. Reichstein, G. Camps-Valls, B. Stevens, M. Jung, J. Denzler, N. Carvalhais, and Prabhat, ''Deep learning and process understand- ing for data-driven earth system science,'' Nature, vol. 566, no. 7743, pp. 195-204, Feb. 2019.</p>
<p>Intuitive physics. M Mccloskey, Sci. Amer. 2484M. McCloskey, ''Intuitive physics,'' Sci. Amer., vol. 248, no. 4, pp. 122-130, Apr. 1983.</p>
<p>Learning physical intuition of block towers by example. A Lerer, S Gross, R Fergus, Proc. 33rd Int. Conf. Mach. Learn. M. F. Balcan and K. Q. Weinberger33rd Int. Conf. Mach. LearnA. Lerer, S. Gross, and R. Fergus, ''Learning physical intuition of block towers by example,'' in Proc. 33rd Int. Conf. Mach. Learn., M. F. Balcan and K. Q. Weinberger, Eds., 2016, pp. 430-438.</p>
<p>To fall or not to fall: A visual approach to physical stability prediction. W Li, S Azimi, A Leonardis, M Fritz, arXiv:1604.00066W. Li, S. Azimi, A. Leonardis, and M. Fritz, ''To fall or not to fall: A visual approach to physical stability prediction,'' 2016, arXiv:1604.00066. [Online]. Available: http://arxiv.org/abs/1604.00066</p>
<p>Hyperspectral plant disease forecasting using generative adversarial networks. A Forster, J Behley, J Behmann, R Roscher, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS). IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)A. Forster, J. Behley, J. Behmann, and R. Roscher, ''Hyperspectral plant disease forecasting using generative adversarial networks,'' in Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), Jul. 2019, pp. 1793-1796.</p>
<p>Accelerating the design of functional glasses through modeling. J C Mauro, A Tandia, K D Vargheese, Y Z Mauro, M M Smedskjaer, Chem. Mater. 2812J. C. Mauro, A. Tandia, K. D. Vargheese, Y. Z. Mauro, and M. M. Smed- skjaer, ''Accelerating the design of functional glasses through modeling,'' Chem. Mater., vol. 28, no. 12, pp. 4267-4277, Jun. 2016.</p>
<p>Hidden physics models: Machine learning of nonlinear partial differential equations. M Raissi, G E Karniadakis, J. Comput. Phys. 357M. Raissi and G. E. Karniadakis, ''Hidden physics models: Machine learning of nonlinear partial differential equations,'' J. Comput. Phys., vol. 357, pp. 125-141, Mar. 2018.</p>
<p>Scientific domain knowledge improves exoplanet transit classification with deep learning. M Ansdell, Y Ioannou, H P Osborn, M Sasdelli, J C Smith, D Caldwell, J M Jenkins, C Räissi, D Angerhausen, Astrophys. J. 86917M. Ansdell, Y. Ioannou, H. P. Osborn, M. Sasdelli, J. C. Smith, D. Caldwell, J. M. Jenkins, C. Räissi, and D. Angerhausen, ''Scientific domain knowledge improves exoplanet transit classification with deep learning,'' Astrophys. J., vol. 869, no. 1, p. L7, Dec. 2018.</p>
<p>Understanding tools: Task-oriented object modeling, learning and recognition. Y Zhu, Y Zhao, S.-C Zhu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Y. Zhu, Y. Zhao, and S.-C. Zhu, ''Understanding tools: Task-oriented object modeling, learning and recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 2855-2864.</p>
<p>Deep learning model integration of remotely sensed and SWAT-simulated regional soil moisture. K Breen, S C James, J D White, AGU Fall Meeting Abstr. K. Breen, S. C. James, and J. D. White, ''Deep learning model integration of remotely sensed and SWAT-simulated regional soil moisture,'' AGU Fall Meeting Abstr., Dec. 2018.</p>
<p>Design by adaptive sampling. D H Brookes, J Listgarten, arXiv:1810.03714D. H. Brookes and J. Listgarten, ''Design by adaptive sampling,'' 2018, arXiv:1810.03714. [Online]. Available: http://arxiv.org/abs/1810.03714</p>
<p>Conditioning by adaptive sampling for robust design. D H Brookes, H Park, J Listgarten, arXiv:1901.10060D. H. Brookes, H. Park, and J. Listgarten, ''Conditioning by adaptive sam- pling for robust design,'' 2019, arXiv:1901.10060. [Online]. Available: http://arxiv.org/abs/1901.10060</p>
<p>Accelerating Eulerian fluid simulation with convolutional networks. J Tompson, K Schlachter, P Sprechmann, K Perlin, Proc. ICML, D. Precup and Y. W. TehJ. Tompson, K. Schlachter, P. Sprechmann, and K. Perlin, ''Accelerating Eulerian fluid simulation with convolutional networks,'' in Proc. ICML, D. Precup and Y. W. Teh, Eds., 2017, pp. 3424-3433.</p>
<p>Datadriven fluid simulations using regression forests. L Ladický, S Jeong, B Solenthaler, M Pollefeys, M Gross, ACM Trans. Graph. 346L. Ladický, S. Jeong, B. Solenthaler, M. Pollefeys, and M. Gross, ''Data- driven fluid simulations using regression forests,'' ACM Trans. Graph., vol. 34, no. 6, pp. 1-9, Oct. 2015.</p>
<p>Tropical land use land cover mapping in Pará (Brazil) using discriminative Markov random fields and multi-temporal TerraSAR-X data. R Hagensieker, R Roscher, J Rosentreter, B Jakimow, B Waske, Int. J. Appl. Earth Observ. Geoinf. 63R. Hagensieker, R. Roscher, J. Rosentreter, B. Jakimow, and B. Waske, ''Tropical land use land cover mapping in Pará (Brazil) using discrimi- native Markov random fields and multi-temporal TerraSAR-X data,'' Int. J. Appl. Earth Observ. Geoinf., vol. 63, pp. 244-256, Dec. 2017.</p>
<p>A compositional object-based approach to learning physical dynamics. M B Chang, T Ullman, A Torralba, J B Tenenbaum, Proc. ICLR. ICLRM. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum, ''A compo- sitional object-based approach to learning physical dynamics,'' in Proc. ICLR, 2017, pp. 1-7.</p>
<p>Label-free supervision of neural networks with physics and domain knowledge. R Stewart, S Ermon, Proc. AAAI. AAAI1R. Stewart and S. Ermon, ''Label-free supervision of neural networks with physics and domain knowledge,'' in Proc. AAAI, vol. 1, 2017, pp. 1-7.</p>
<p>Learning physical object properties from unlabeled videos. J Wu, J Lim, H Zhang, J Tenenbaum, W Freeman, Proc. Brit. Mach. Vis. Conf. Brit. Mach. Vis. Conf1017J. Wu, J. Lim, H. Zhang, J. Tenenbaum, and W. Freeman, ''Physics 101: Learning physical object properties from unlabeled videos,'' in Proc. Brit. Mach. Vis. Conf., 2016. p. 7.</p>
<p>SMASH: Physics-guided reconstruction of collisions from videos. A Monszpart, N Thuerey, N J Mitra, ACM Trans. Graph. 356A. Monszpart, N. Thuerey, and N. J. Mitra, ''SMASH: Physics-guided reconstruction of collisions from videos,'' ACM Trans. Graph., vol. 35, no. 6, pp. 1-14, Nov. 2016.</p>
<p>Newtonian image understanding: Unfolding the dynamics of objects in static images. R Mottaghi, H Bagherinezhad, M Rastegari, A Farhadi, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)R. Mottaghi, H. Bagherinezhad, M. Rastegari, and A. Farhadi, ''Newto- nian image understanding: Unfolding the dynamics of objects in static images,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 3521-3529.</p>
<p>Variational Monte Carlo-bridging concepts of machine learning and high dimensional partial differential equations. M Eigel, R Schneider, P Trunschke, S Wolf, arXiv:1810.01348M. Eigel, R. Schneider, P. Trunschke, and S. Wolf, ''Variational Monte Carlo-bridging concepts of machine learning and high dimensional par- tial differential equations,'' 2018, arXiv:1810.01348. [Online]. Available: http://arxiv.org/abs/1810.01348</p>
<p>An information-based machine learning approach to elasticity imaging. C Hoerig, J Ghaboussi, M F Insana, Biomech. Model. Mechanobiol. 163C. Hoerig, J. Ghaboussi, and M. F. Insana, ''An information-based machine learning approach to elasticity imaging,'' Biomech. Model. Mechanobiol., vol. 16, no. 3, pp. 805-822, Nov. 2016.</p>
<p>A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials. E O Pyzer-Knapp, G N Simm, A Aspuru Guzik, Mater. Horizons. 33E. O. Pyzer-Knapp, G. N. Simm, and A. Aspuru Guzik, ''A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials,'' Mater. Horizons, vol. 3, no. 3, pp. 226-233, 2016.</p>
<p>Design principles and top non-fullerene acceptor candidates for organic photovoltaics. S A Lopez, B Sanchez-Lengeling, J De Goes, A Soares, Aspuru-Guzik, Joule. 1S. A. Lopez, B. Sanchez-Lengeling, J. de Goes Soares, and A. Aspuru- Guzik, ''Design principles and top non-fullerene acceptor candidates for organic photovoltaics,'' Joule, vol. 1, no. 4, pp. 857-870, Dec. 2017.</p>
<p>Machine learning of linear differential equations using Gaussian processes. M Raissi, P Perdikaris, G E Karniadakis, J. Comput. Phys. 348M. Raissi, P. Perdikaris, and G. E. Karniadakis, ''Machine learning of linear differential equations using Gaussian processes,'' J. Comput. Phys., vol. 348, pp. 683-693, Nov. 2017.</p>
<p>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance. J Ling, A Kurzawski, J Templeton, J. Fluid Mech. 807J. Ling, A. Kurzawski, and J. Templeton, ''Reynolds averaged turbu- lence modelling using deep neural networks with embedded invariance,'' J. Fluid Mech., vol. 807, pp. 155-166, Oct. 2016.</p>
<p>Prediction of Reynolds stresses in high-mach-number turbulent boundary layers using physicsinformed machine learning. J X Wang, J Huang, L Duan, H Xiao, Comput. Fluid Dyn. 331J. X. Wang, J. Huang, L. Duan, and H. Xiao, ''Prediction of Reynolds stresses in high-mach-number turbulent boundary layers using physics- informed machine learning,'' Theor. Comput. Fluid Dyn., vol. 33, no. 1, pp. 1-19, 2019.</p>
<p>Machine learning strategies for systems with invariance properties. J Ling, R Jones, J Templeton, J. Comput. Phys. 318J. Ling, R. Jones, and J. Templeton, ''Machine learning strategies for sys- tems with invariance properties,'' J. Comput. Phys., vol. 318, pp. 22-35, Aug. 2016.</p>
<p>Feature importance in nonlinear embeddings (FINE): Applications in digital pathology. S B Ginsburg, G Lee, S Ali, A Madabhushi, IEEE Trans. Med. Imag. 351S. B. Ginsburg, G. Lee, S. Ali, and A. Madabhushi, ''Feature importance in nonlinear embeddings (FINE): Applications in digital pathology,'' IEEE Trans. Med. Imag., vol. 35, no. 1, pp. 76-88, Jan. 2016.</p>
<p>An explainable deep machine vision framework for plant stress phenotyping. S Ghosal, D Blystone, A K Singh, B Ganapathysubramanian, A Singh, S Sarkar, Proc. Nat. Acad. Sci. USA. 11518S. Ghosal, D. Blystone, A. K. Singh, B. Ganapathysubramanian, A. Singh, and S. Sarkar, ''An explainable deep machine vision framework for plant stress phenotyping,'' Proc. Nat. Acad. Sci. USA, vol. 115, no. 18, pp. 4613-4618, Apr. 2018.</p>
<p>Shapestacks: Learning vision-based physical intuition for generalised object stacking. O Groth, F B Fuchs, I Posner, A Vedaldi, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)O. Groth, F. B. Fuchs, I. Posner, and A. Vedaldi, ''Shapestacks: Learning vision-based physical intuition for generalised object stacking,'' in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 702-717.</p>
<p>Individual brain charting, a high-resolution fMRI dataset for cognitive mapping. A L Pinho, Sci. Data. 51180A. L. Pinho et al., ''Individual brain charting, a high-resolution fMRI dataset for cognitive mapping,'' Sci. Data, vol. 5, no. 1, p. 180, Jun. 2018.</p>
<p>Atlases of cognition with large-scale human brain mapping. G Varoquaux, Y Schwartz, R A Poldrack, B Gauthier, D Bzdok, J.-B Poline, B Thirion, PLOS Comput. Biol. 1411Art. no. e1006565G. Varoquaux, Y. Schwartz, R. A. Poldrack, B. Gauthier, D. Bzdok, J.-B. Poline, and B. Thirion, ''Atlases of cognition with large-scale human brain mapping,'' PLOS Comput. Biol., vol. 14, no. 11, Nov. 2018, Art. no. e1006565.</p>
<p>The deeptune framework for modeling and characterizing neurons in visual cortex area v4,'' bioRxiv. R Abbasi-Asl, Y Chen, A Bloniarz, M Oliver, B D Willmore, J L Gallant, B Yu, 10.1101/465534R. Abbasi-Asl, Y. Chen, A. Bloniarz, M. Oliver, B. D. Willmore, J. L. Gallant, and B. Yu, ''The deeptune framework for modeling and characterizing neurons in visual cortex area v4,'' bioRxiv, Jan. 2018, Art. no. 465534, doi: 10.1101/465534.</p>
<p>A comparison of hierarchical multi-output recognition approaches for anuran classification. J G Colonna, J Gama, E F Nakamura, Mach. Learn. 10711J. G. Colonna, J. Gama, and E. F. Nakamura, ''A comparison of hierarchi- cal multi-output recognition approaches for anuran classification,'' Mach. Learn., vol. 107, no. 11, pp. 1651-1671, Jul. 2018.</p>
<p>Attention-based deep neural networks for detection of cancerous and precancerous esophagus tissue on histopathological slides. N Tomita, B Abdollahi, J Wei, B Ren, A Suriawinata, S Hassanpour, JAMA Netw. Open. 211Art. no. e1914645N. Tomita, B. Abdollahi, J. Wei, B. Ren, A. Suriawinata, and S. Hassanpour, ''Attention-based deep neural networks for detection of cancerous and precancerous esophagus tissue on histopathological slides,'' JAMA Netw. Open, vol. 2, no. 11, Nov. 2019, Art. no. e1914645.</p>
<p>Attention gated networks: Learning to leverage salient regions in medical images. J Schlemper, O Oktay, M Schaap, M Heinrich, B Kainz, B Glocker, D Rueckert, Image Anal. 53J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker, and D. Rueckert, ''Attention gated networks: Learning to leverage salient regions in medical images,'' Med. Image Anal., vol. 53, pp. 197-207, Apr. 2019.</p>
<p>Genetic architect: Discovering genomic structure with learned neural architectures. L Deming, S Targ, N Sauder, D Almeida, C Jimmie Ye, arXiv:1605.07156L. Deming, S. Targ, N. Sauder, D. Almeida, and C. Jimmie Ye, ''Genetic architect: Discovering genomic structure with learned neural architectures,'' 2016, arXiv:1605.07156. [Online]. Available: http://arxiv.org/abs/1605.07156</p>
<p>Attend and predict: Understanding gene regulation by selective attention on chromatin. R Singh, J Lanchantin, A Sekhon, Y Qi, Proc. nullR. Singh, J. Lanchantin, A. Sekhon, and Y. Qi, ''Attend and predict: Understanding gene regulation by selective attention on chromatin,'' in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 6785-6795.</p>
<p>Retain: An interpretable predictive model for healthcare using reverse time attention mechanism,'' in Proc. E Choi, M T Bahadori, J Sun, J Kulas, A Schuetz, W Stewart, Adv. Neural Inf. Process. Syst. E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart, ''Retain: An interpretable predictive model for healthcare using reverse time attention mechanism,'' in Proc. Adv. Neural Inf. Process. Syst., 2016, pp. 3504-3512.</p>
<p>Interpretable parallel recurrent neural networks with convolutional attentions for multi-modality activity modeling. K Chen, L Yao, X Wang, D Zhang, T Gu, Z Yu, Z Yang, Proc. Int. Joint Conf. Neural Netw. (IJCNN). Int. Joint Conf. Neural Netw. (IJCNN)K. Chen, L. Yao, X. Wang, D. Zhang, T. Gu, Z. Yu, and Z. Yang, ''Inter- pretable parallel recurrent neural networks with convolutional attentions for multi-modality activity modeling,'' in Proc. Int. Joint Conf. Neural Netw. (IJCNN), Jul. 2018, pp. 1-8.</p>
<p>Reliable and explainable machine-learning methods for accelerated material discovery. B Kailkhura, B Gallagher, S Kim, A Hiszpanski, T Y , -J Han, NPJ Comput. Mater. 51B. Kailkhura, B. Gallagher, S. Kim, A. Hiszpanski, and T. Y.-J. Han, ''Reliable and explainable machine-learning methods for accelerated material discovery,'' NPJ Comput. Mater., vol. 5, no. 1, pp. 1-9, Nov. 2019.</p>
<p>Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge. L Rieger, C Singh, W James Murdoch, B Yu, arXiv:1909.13584L. Rieger, C. Singh, W. James Murdoch, and B. Yu, ''Interpreta- tions are useful: Penalizing explanations to align neural networks with prior knowledge,'' 2019, arXiv:1909.13584. [Online]. Available: http://arxiv.org/abs/1909.13584</p>
<p>Reconstruction of normal forms by learning informed observation geometries from data. O Yair, R Talmon, R R Coifman, I G Kevrekidis, Proc. Nat. Acad. Sci. USA. 11438O. Yair, R. Talmon, R. R. Coifman, and I. G. Kevrekidis, ''Reconstruc- tion of normal forms by learning informed observation geometries from data,'' Proc. Nat. Acad. Sci. USA, vol. 114, no. 38, pp. E7865-E7874, Aug. 2017.</p>
<p>Parsimonious representation of nonlinear dynamical systems through manifold learning: A chemotaxis case study. C J Dsilva, R Talmon, R R Coifman, I G Kevrekidis, Appl. Comput. Harmon. Anal. 443C. J. Dsilva, R. Talmon, R. R. Coifman, and I. G. Kevrekidis, ''Parsi- monious representation of nonlinear dynamical systems through mani- fold learning: A chemotaxis case study,'' Appl. Comput. Harmon. Anal., vol. 44, no. 3, pp. 759-773, May 2018.</p>
<p>Manifold learning for parameter reduction. A Holiday, M Kooshkbaghi, J M Bello-Rivas, C W Gear, A Zagaris, I G Kevrekidis, J. Comput. Phys. 392A. Holiday, M. Kooshkbaghi, J. M. Bello-Rivas, C. W. Gear, A. Zagaris, and I. G. Kevrekidis, ''Manifold learning for parameter reduction,'' J. Comput. Phys., vol. 392, pp. 419-431, Sep. 2019.</p>
<p>Discovering phases, phase transitions, and crossovers through unsupervised machine learning: A critical examination. W Hu, R R P Singh, R T Scalettar, Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat. Interdiscip. Top. 956Art. no. 062122W. Hu, R. R. P. Singh, and R. T. Scalettar, ''Discovering phases, phase transitions, and crossovers through unsupervised machine learning: A critical examination,'' Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat. Interdiscip. Top., vol. 95, no. 6, Jun. 2017, Art. no. 062122.</p>
<p>Discovering phase transitions with unsupervised learning. L Wang, Phys. Rev. B, Condens. Matter. 9419L. Wang, ''Discovering phase transitions with unsupervised learn- ing,'' Phys. Rev. B, Condens. Matter, vol. 94, no. 19, Nov. 2016, Art. no. 195105.</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, Proc. Nat. Acad. Sci. USA. 11315S. L. Brunton, J. L. Proctor, and J. N. Kutz, ''Discovering governing equa- tions from data by sparse identification of nonlinear dynamical systems,'' Proc. Nat. Acad. Sci. USA, vol. 113, no. 15, pp. 3932-3937, Mar. 2016.</p>
<p>Inferring biological networks by sparse identification of nonlinear dynamics. N M Mangan, S L Brunton, J L Proctor, J N Kutz, IEEE Trans. Mol., Biol. Multi-Scale Commun. 21N. M. Mangan, S. L. Brunton, J. L. Proctor, and J. N. Kutz, ''Inferring biological networks by sparse identification of nonlinear dynamics,'' IEEE Trans. Mol., Biol. Multi-Scale Commun., vol. 2, no. 1, pp. 52-63, Jun. 2016.</p>
<p>Data-driven discovery of partial differential equations. S H Rudy, S L Brunton, J L Proctor, J N Kutz, Sci. Adv. 34Art. no. e1602614S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz, ''Data-driven dis- covery of partial differential equations,'' Sci. Adv., vol. 3, no. 4, Apr. 2017, Art. no. e1602614.</p>
<p>Sparse dynamics for partial differential equations. H Schaeffer, R Caflisch, C D Hauck, S Osher, Proc. Nat. Acad. Sci. USA. Nat. Acad. Sci. USA110H. Schaeffer, R. Caflisch, C. D. Hauck, and S. Osher, ''Sparse dynamics for partial differential equations,'' Proc. Nat. Acad. Sci. USA, vol. 110, no. 17, pp. 6634-6639, 2013.</p>
<p>Exact recovery of chaotic systems from highly corrupted data. G Tran, R Ward, Multiscale Model. Simul. 153G. Tran and R. Ward, ''Exact recovery of chaotic systems from highly corrupted data,'' Multiscale Model. Simul., vol. 15, no. 3, pp. 1108-1129, Jan. 2017.</p>
<p>Syntax-directed variational autoencoder for structured data. H Dai, Y Tian, B Dai, S Skiena, L Song, Proc. ICLR. ICLRH. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, ''Syntax-directed varia- tional autoencoder for structured data,'' in Proc. ICLR, 2018, pp. 1-17.</p>
<p>Neural-guided symbolic regression with asymptotic constraints. L Li, M Fan, R Singh, P Riley, arXiv:1901.07714L. Li, M. Fan, R. Singh, and P. Riley, ''Neural-guided symbolic regression with asymptotic constraints,'' 2019, arXiv:1901.07714. [Online]. Avail- able: http://arxiv.org/abs/1901.07714</p>
<p>A regression approach for explaining manifold embedding coordinates. M Meila, S Koelle, H Zhang, arXiv:1811.11891M. Meila, S. Koelle, and H. Zhang, ''A regression approach for explaining manifold embedding coordinates,'' 2018, arXiv:1811.11891. [Online].</p>
<p>Learning physical descriptors for materials science by compressed sensing. L M Ghiringhelli, J Vybiral, E Ahmetcik, R Ouyang, S V Levchenko, C Draxl, M Scheffler, New J. Phys. 192Art. no. 023017L. M. Ghiringhelli, J. Vybiral, E. Ahmetcik, R. Ouyang, S. V. Levchenko, C. Draxl, and M. Scheffler, ''Learning physical descriptors for materials science by compressed sensing,'' New J. Phys., vol. 19, no. 2, Feb. 2017, Art. no. 023017.</p>
<p>SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. R Ouyang, S Curtarolo, E Ahmetcik, M Scheffler, L M Ghiringhelli, Phys. Rev. Mater. 28R. Ouyang, S. Curtarolo, E. Ahmetcik, M. Scheffler, and L. M. Ghiringhelli, ''SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates,'' Phys. Rev. Mater., vol. 2, no. 8, pp. 1-11, Aug. 2018.</p>
<p>Graphical dynamical systems and their applications to bio-social systems. A Adiga, C J Kuhlman, M V Marathe, H S Mortveit, S S Ravi, A Vullikanti, Int. J. Adv. Eng. Sci. Appl. Math. 112A. Adiga, C. J. Kuhlman, M. V. Marathe, H. S. Mortveit, S. S. Ravi, and A. Vullikanti, ''Graphical dynamical systems and their applications to bio-social systems,'' Int. J. Adv. Eng. Sci. Appl. Math., vol. 11, no. 2, pp. 153-171, Dec. 2018.</p>
<p>From deep learning to mechanistic understanding in neuroscience: The structure of retinal prediction,'' in Proc. H Tanaka, A Nayebi, N Maheswaranathan, L Mcintosh, S Baccus, S Ganguli, Adv. Neural Inf. Process. Syst. H. Tanaka, A. Nayebi, N. Maheswaranathan, L. McIntosh, S. Baccus, and S. Ganguli, ''From deep learning to mechanistic understanding in neuroscience: The structure of retinal prediction,'' in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 8535-8545.</p>
<p>Deep learning for universal linear embeddings of nonlinear dynamics. B Lusch, J N Kutz, S L Brunton, Nature Commun. 914950B. Lusch, J. N. Kutz, and S. L. Brunton, ''Deep learning for universal linear embeddings of nonlinear dynamics,'' Nature Commun., vol. 9, no. 1, p. 4950, Nov. 2018.</p>
<p>Using deep learning to model the hierarchical structure and function of a cell. J Ma, M K Yu, S Fong, K Ono, E Sage, B Demchak, R Sharan, T Ideker, Nature Methods. 154J. Ma, M. K. Yu, S. Fong, K. Ono, E. Sage, B. Demchak, R. Sharan, and T. Ideker, ''Using deep learning to model the hierarchical structure and function of a cell,'' Nature Methods, vol. 15, no. 4, pp. 290-298, Mar. 2018.</p>
<p>Automated, predictive, and interpretable inference of caenorhabditis elegans escape dynamics. B C Daniels, W S Ryu, I Nemenman, Proc. Nat. Acad. Sci. USA. 11615B. C. Daniels, W. S. Ryu, and I. Nemenman, ''Automated, predictive, and interpretable inference of caenorhabditis elegans escape dynamics,'' Proc. Nat. Acad. Sci. USA, vol. 116, no. 15, pp. 7226-7231, Mar. 2019.</p>
<p>Discovering physical concepts with neural networks. R Iten, T Metger, H Wilming, L Del Rio, R Renner, arXiv:1807.10300R. Iten, T. Metger, H. Wilming, L. del Rio, and R. Renner, ''Discover- ing physical concepts with neural networks,'' 2018, arXiv:1807.10300. [Online]. Available: http://arxiv.org/abs/1807.10300</p>
<p>Interpretable intuitive physics model. T Ye, X Wang, J Davidson, A Gupta, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)T. Ye, X. Wang, J. Davidson, and A. Gupta, ''Interpretable intuitive physics model,'' in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 87-102.</p>
<p>GroupINN: Grouping-based interpretable neural network for classification of limited, noisy brain data. Y Yan, J Zhu, M Duda, E Solarz, C Sripada, D Koutra, Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data MiningY. Yan, J. Zhu, M. Duda, E. Solarz, C. Sripada, and D. Koutra, ''GroupINN: Grouping-based interpretable neural network for classifica- tion of limited, noisy brain data,'' in Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2019, pp. 772-782.</p>
<p>Detecting statistical interactions from neural network weights. M Tsang, D Cheng, Y Liu, Proc. ICLR. ICLRM. Tsang, D. Cheng, and Y. Liu, ''Detecting statistical interactions from neural network weights,'' in Proc. ICLR, 2018, pp. 1-21.</p>
<p>Quantum-chemical insights from deep tensor neural networks. K T Schütt, F Arbabzadah, S Chmiela, K R Müller, A Tkatchenko, Nature Commun. 81Art. no. 13890K. T. Schütt, F. Arbabzadah, S. Chmiela, K. R. Müller, and A. Tkatchenko, ''Quantum-chemical insights from deep tensor neural networks,'' Nature Commun., vol. 8, no. 1, Jan. 2017, Art. no. 13890.</p>
<p>Physicsaware Gaussian processes in remote sensing. G Camps-Valls, L Martino, D Svendsen, M Campos-Taberner, J Muñoz-Marí, V Laparra, D Luengo, J García-Haro, Appl. Soft Comput. 68G. Camps-Valls, L. Martino, D. Svendsen, M. Campos-Taberner, J. Muñoz-Marí, V. Laparra, D. Luengo, and J. García-Haro, ''Physics- aware Gaussian processes in remote sensing,'' Appl. Soft Comput., vol. 68, Mar. 2018.</p>
<p>Probing hidden spin order with interpretable machine learning. J Greitemann, K Liu, L Pollet, Phys. Rev. B, Condens. Matter. 996J. Greitemann, K. Liu, and L. Pollet, ''Probing hidden spin order with interpretable machine learning,'' Phys. Rev. B, Condens. Matter, vol. 99, no. 6, pp. 1-6, Feb. 2019.</p>
<p>Learning multiple order parameters with interpretable machines. K Liu, J Greitemann, L Pollet, Phys. Rev. B, Condens. Matter. 9910K. Liu, J. Greitemann, and L. Pollet, ''Learning multiple order parameters with interpretable machines,'' Phys. Rev. B, Condens. Matter, vol. 99, no. 10, pp. 1-15, Mar. 2019.</p>
<p>Machine learning for molecular and materials science. K T Butler, D W Davies, H Cartwright, O Isayev, A Walsh, Nature. 559K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev, and A. Walsh, ''Machine learning for molecular and materials science,'' Nature, vol. 559, pp. 547-555, Jul. 2018.</p>
<p>Machine learning and the physical sciences. G Carleo, I Cirac, K Cranmer, L Daudet, M Schuld, N Tishby, L Vogt-Maranto, L Zdeborová, Rev. Mod. Phys. 91045002G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborová, ''Machine learning and the physical sciences,'' Rev. Mod. Phys., vol. 91, Dec. 2019, Art. no. 045002.</p>
<p>Revealing the vectors of cellular identity with single-cell genomics. A Wagner, A Regev, N Yosef, Nature Biotechnol. 3411A. Wagner, A. Regev, and N. Yosef, ''Revealing the vectors of cellular identity with single-cell genomics,'' Nature Biotechnol., vol. 34, no. 11, pp. 1145-1160, Nov. 2016.</p>
<p>Opportunities and obstacles for deep learning in biology and medicine. T Ching, J. Roy. Soc. Interface. 15141T. Ching et al., ''Opportunities and obstacles for deep learning in biol- ogy and medicine,'' J. Roy. Soc. Interface, vol. 15, no. 141, 2018, Art. no. 20170387.</p>
<p>From genotype to phenotype: Augmenting deep learning with networks and systems biology. V H Gazestani, N E Lewis, Current Opinion Syst. Biol. 15V. H. Gazestani and N. E. Lewis, ''From genotype to phenotype: Aug- menting deep learning with networks and systems biology,'' Current Opinion Syst. Biol., vol. 15, pp. 68-73, Jun. 2019.</p>
<p>Causability and explainability of artificial intelligence in medicine. A Holzinger, G Langs, H Denk, K Zatloukal, H Müller, WIREs Data Mining Knowl. Discovery. 94A. Holzinger, G. Langs, H. Denk, K. Zatloukal, and H. Müller, ''Caus- ability and explainability of artificial intelligence in medicine,'' WIREs Data Mining Knowl. Discovery, vol. 9, no. 4, Apr. 2019.</p>
<p>Rethinking drug design in the artificial intelligence era. P Schneider, W P Walters, A T Plowright, N Sieroka, J Listgarten, R A Goodnow, J Fisher, J M Jansen, J S Duca, T S Rush, M Zentgraf, J E Hill, E Krutoholow, M Kohler, J Blaney, K Funatsu, C Luebkemann, G Schneider, 10.1038/s41573-019-0050-3Nature Rev. Drug Discovery. P. Schneider, W. P. Walters, A. T. Plowright, N. Sieroka, J. Listgarten, R. A. Goodnow, J. Fisher, J. M. Jansen, J. S. Duca, T. S. Rush, M. Zentgraf, J. E. Hill, E. Krutoholow, M. Kohler, J. Blaney, K. Funatsu, C. Luebkemann, and G. Schneider, ''Rethinking drug design in the artificial intelligence era,'' Nature Rev. Drug Discovery, Dec. 2019, doi: 10.1038/s41573-019-0050-3.</p>
<p>Machine learning for the geosciences: Challenges and opportunities. A Karpatne, I Ebert-Uphoff, S Ravela, H Ali Babaie, V Kumar, IEEE Trans. Knowl. Data Eng. 318A. Karpatne, I. Ebert-Uphoff, S. Ravela, H. Ali Babaie, and V. Kumar, ''Machine learning for the geosciences: Challenges and opportunities,'' IEEE Trans. Knowl. Data Eng., vol. 31, no. 8, pp. 1544-1554, Aug. 2019.</p>
<p>J Pearl, Causality: Models, Reasoning, and Inference. Cambridge, U.K.Cambridge Univ. Press2nd ed.J. Pearl, Causality: Models, Reasoning, and Inference, 2nd ed. Cambridge, U.K.: Cambridge Univ. Press, 2011.</p>
<p>Causality for machine learning. B Schölkopf, arXiv:1911.10500B. Schölkopf, ''Causality for machine learning,'' 2019, arXiv:1911.10500. [Online]. Available: http://arxiv.org/abs/1911.10500</p>            </div>
        </div>

    </div>
</body>
</html>
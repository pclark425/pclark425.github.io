<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2879 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2879</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2879</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-3720255c8c12d4a827d52d188c0571c68f5ed391</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3720255c8c12d4a827d52d188c0571c68f5ed391" target="_blank">From Text to Tactic: Evaluating LLMs Playing the Game of Avalon</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> AvalonBench is introduced - a comprehensive game environment tailored for evaluating multi-agent LLM Agents and could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2879.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2879.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Avalon ReAct Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct-style LLM agent with recursive summarization (AvalonBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent architecture used in AvalonBench that uses the ReAct (reason-then-act) paradigm with zero-shot chain-of-thought and a recursive summarization module to compress and maintain game history for decision-making and dialogue generation in the Resistance Avalon text game.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Avalon ReAct Agent (architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pipeline: at each decision the agent receives Rules, Role (private info), a compressed game Summary, and the current round Minutes then is prompted (Request) to produce an act_response using ReAct + zero-shot CoT. The textual act_response is passed to a separate PARSER LLM to produce a structured action consumed by the game engine. For long history management the agent maintains a recursive summary: Summary_t <- LLM(input={Summary_{t-1}, Minutes_t, Outcome}, prompt={Rules, Role, Recap}); the Summary is then fed back into action and discussion prompts. Discussion utterances are collected into Minutes_t which are limited per-turn. The parser LLM enforces consistent action formats.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Used in the paper with GPT-3.5-turbo and Llama2-chat-7B (experiments reported with these models).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recursive/compressed episodic summary (summarized game history used as memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory is implemented as a recursively-updated textual summary (Summary_t). After each discussion/quest, a summarization prompt (Recap) is run: Summary_t = LLM(input={Summary_{t-1}, Minutes_t, Outcome}, prompt={Rules, Role, Recap}). When making actions, the agent is given the current Summary and current Minutes as part of the input so retrieval is explicit (the summary is included in the prompt). A separate PARSER LLM then parses natural outputs into structured actions. There is no external database or vector index; all memory is stored as compressed text (the summary) and passed in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified precisely; authors note typical full dialogue across a game can be ~5,000 words (15-20 rounds) and that summaries are used to fit within the LLM context window (underlying context window size not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Explicit inclusion of the latest compressed Summary and current Minutes in the model prompt (recency + compression); no semantic vector retrieval or learned retriever is used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Summary is updated after each discussion/quest round by running the summarization prompt that consumes the prior Summary, the Minutes of the current round, and the quest Outcome (i.e., an after-each-round recursive update).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Resistance Avalon (AvalonBench)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Hidden-identity social-deduction text game (Avalon): 5 players, roles include Servant, Minion, Merlin, Assassin; rounds include team selection, voting, quest (pass/fail), and discussion; episodes typically 15-25 discussion rounds; requires deduction, persuasion, deception, coordination and multi-agent language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Authors adopt recursive summarization to manage long histories and fit context limits; they state this is necessary for reasoning over long dialogues, but provide no ablation isolating the impact of the summary-memory. Overall they observe that while agents can use discussion (and thus the summaries) to deduce identities, this memory alone does not make agents reliably execute good strategies and they often reveal identities or make poor decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Text to Tactic: Evaluating LLMs Playing the Game of Avalon', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2879.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2879.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Avalon ReAct Agent (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct-style LLM agent implemented with GPT-3.5-turbo in AvalonBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's concrete implementation of the ReAct architecture using GPT-3.5-turbo, including the recursive summary memory and a separate parser LLM; used to generate actions, dialogue, and summaries while playing Avalon against rule-based baselines and in LLM self-play.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Avalon ReAct Agent (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as the generic Avalon ReAct Agent: GPT-3.5-turbo is prompted with Rules, Role, Summary, Minutes and Request to produce action and discussion text via ReAct + zero-shot CoT; outputs are parsed by a separate LLM (GPT-3.5 used as parser in experiments). Summary_t is produced recursively by GPT-3.5 from Summary_{t-1}, Minutes_t, and Outcome. Minutes are limited per-turn; mission outcomes (not voting details) are provided to match naive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recursive/compressed episodic summary (text summary in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Recursive summarization implemented by GPT-3.5: Summary_t = GPT-3.5(input={Summary_{t-1}, Minutes_t, Outcome}, prompt=Recap). Summary is passed into subsequent action and discussion prompts; no external retrieval/indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not explicitly quantified; paper notes entire dialogue per game could be ~5,000 words (15-20 rounds) and the summary compresses this to fit the LLM's context window (context window size not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Direct in-context inclusion of the current Summary and current Minutes (recency-driven explicit feeding).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Summary updated after each round/quest by running the Recap summarization prompt (i.e., after each discussion/quest).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Resistance Avalon (AvalonBench)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Hidden-identity social-deduction, multi-agent dialogue, deception and deduction required; episodic length ~15-25 discussion rounds; multi-phase rounds (team selection, voting, quest, assassination).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Assassin (ASSASSINSET): with discussion (memory+dialogue) total winrate 66.7%; mission winrate 0.0%; assassination winrate 66.7%; assassination accuracy 66.7%. Assassin w/o discussion: total winrate 26.7%; mission winrate 20.0%; assassination winrate 6.7%; assassination accuracy 8.0%. Servant (SERVANTSET): with discussion total winrate 22.2%; average deduction accuracy 76.0. Servant w/o discussion total winrate 11.1%; deduction accuracy 60.7%. Multi-LLM self-play (all players GPT-3.5): Evil wins 83.3% overall (48.3% by sabotaging missions, 35% by assassinating Merlin); Good wins 16.7%. (All percentages are reported by the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline rule-based bots (no dialogue-memory integration): Assassin baseline total winrate 61.8%; mission winrate 42.7%; assassination winrate 19.1%; assassination accuracy 33.3%. Servant baseline total winrate 38.2%; deduction accuracy 71.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Recursive summarization enabled feeding long game histories into GPT-3.5; with discussion (and thus summary use) GPT-3.5 improved assassination success but still often failed at mission-level strategy and sometimes revealed identities. The authors conclude summaries help manage history but are insufficient for robust decision-making; no explicit ablation quantifying the summary effect was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Text to Tactic: Evaluating LLMs Playing the Game of Avalon', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2879.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2879.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Avalon ReAct Agent (Llama2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct-style LLM agent implemented with Llama2-chat-7B in AvalonBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same ReAct + recursive summary architecture instantiated with Llama2-chat-7B and evaluated on AvalonBench; used to generate dialogue, actions and summaries in the Avalon environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Avalon ReAct Agent (Llama2-chat-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Identical architecture and memory pipeline to the GPT-3.5 implementation: Llama2-chat-7B is prompted for actions and discussion using Rules, Role, Summary, Minutes; a recursive summarization prompt produces Summary_t; outputs are parsed by GPT-3.5 parser in the experiments. Discussion is allowed in the with-discussion setting; mission outcomes are supplied consistent with baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>Llama2-chat-7B (7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recursive/compressed episodic summary (text summary in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Summary recursion and in-context feeding implemented the same way as the GPT-3.5 agent: Summary_t = LLM(input={Summary_{t-1}, Minutes_t, Outcome}, prompt=Recap). Current Summary and Minutes are included in subsequent prompts for decisions and discussion. No external retrieval/index used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified; authors cite typical full-game dialogue ~5,000 words and use summarization to fit model context (underlying context window not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Explicit in-context inclusion of the latest summary and current minutes (recency-driven).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Summary updated after each discussion/quest round via the summarization prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Resistance Avalon (AvalonBench)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Hidden-identity social-deduction game requiring deduction, deception, persuasion and coordination; multi-phase rounds and up to ~15-25 discussion rounds per game.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Assassin (with discussion): total winrate 30.0%; mission winrate 0.0%; assassination winrate 30.0%; assassination accuracy 30.0%. Servant (with discussion): total winrate 13.3%; deduction accuracy 68.0%. (These are the paper-reported metrics for Llama2-7B with discussion.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Llama2-7B used the same summary memory pipeline but achieved lower winrates than GPT-3.5 in the reported experiments; the paper reports no ablation separating memory effects from other factors and concludes that while summarization is necessary to handle long histories, it alone does not produce strong strategic play.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Text to Tactic: Evaluating LLMs Playing the Game of Avalon', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Agentbench: Evaluating llms as agents <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decision-making <em>(Rating: 1)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>AVIS: autonomous visual information seeking with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2879",
    "paper_id": "paper-3720255c8c12d4a827d52d188c0571c68f5ed391",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "Avalon ReAct Agent",
            "name_full": "ReAct-style LLM agent with recursive summarization (AvalonBench)",
            "brief_description": "An LLM-based agent architecture used in AvalonBench that uses the ReAct (reason-then-act) paradigm with zero-shot chain-of-thought and a recursive summarization module to compress and maintain game history for decision-making and dialogue generation in the Resistance Avalon text game.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Avalon ReAct Agent (architecture)",
            "agent_description": "Pipeline: at each decision the agent receives Rules, Role (private info), a compressed game Summary, and the current round Minutes then is prompted (Request) to produce an act_response using ReAct + zero-shot CoT. The textual act_response is passed to a separate PARSER LLM to produce a structured action consumed by the game engine. For long history management the agent maintains a recursive summary: Summary_t &lt;- LLM(input={Summary_{t-1}, Minutes_t, Outcome}, prompt={Rules, Role, Recap}); the Summary is then fed back into action and discussion prompts. Discussion utterances are collected into Minutes_t which are limited per-turn. The parser LLM enforces consistent action formats.",
            "base_llm": "Used in the paper with GPT-3.5-turbo and Llama2-chat-7B (experiments reported with these models).",
            "uses_memory": true,
            "memory_type": "recursive/compressed episodic summary (summarized game history used as memory)",
            "memory_architecture": "Memory is implemented as a recursively-updated textual summary (Summary_t). After each discussion/quest, a summarization prompt (Recap) is run: Summary_t = LLM(input={Summary_{t-1}, Minutes_t, Outcome}, prompt={Rules, Role, Recap}). When making actions, the agent is given the current Summary and current Minutes as part of the input so retrieval is explicit (the summary is included in the prompt). A separate PARSER LLM then parses natural outputs into structured actions. There is no external database or vector index; all memory is stored as compressed text (the summary) and passed in-context.",
            "memory_capacity": "Not specified precisely; authors note typical full dialogue across a game can be ~5,000 words (15-20 rounds) and that summaries are used to fit within the LLM context window (underlying context window size not specified).",
            "memory_retrieval_method": "Explicit inclusion of the latest compressed Summary and current Minutes in the model prompt (recency + compression); no semantic vector retrieval or learned retriever is used.",
            "memory_update_strategy": "Summary is updated after each discussion/quest round by running the summarization prompt that consumes the prior Summary, the Minutes of the current round, and the quest Outcome (i.e., an after-each-round recursive update).",
            "text_game_benchmark": "Resistance Avalon (AvalonBench)",
            "game_characteristics": "Hidden-identity social-deduction text game (Avalon): 5 players, roles include Servant, Minion, Merlin, Assassin; rounds include team selection, voting, quest (pass/fail), and discussion; episodes typically 15-25 discussion rounds; requires deduction, persuasion, deception, coordination and multi-agent language reasoning.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Authors adopt recursive summarization to manage long histories and fit context limits; they state this is necessary for reasoning over long dialogues, but provide no ablation isolating the impact of the summary-memory. Overall they observe that while agents can use discussion (and thus the summaries) to deduce identities, this memory alone does not make agents reliably execute good strategies and they often reveal identities or make poor decisions.",
            "uuid": "e2879.0",
            "source_info": {
                "paper_title": "From Text to Tactic: Evaluating LLMs Playing the Game of Avalon",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Avalon ReAct Agent (GPT-3.5)",
            "name_full": "ReAct-style LLM agent implemented with GPT-3.5-turbo in AvalonBench",
            "brief_description": "The paper's concrete implementation of the ReAct architecture using GPT-3.5-turbo, including the recursive summary memory and a separate parser LLM; used to generate actions, dialogue, and summaries while playing Avalon against rule-based baselines and in LLM self-play.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Avalon ReAct Agent (GPT-3.5-turbo)",
            "agent_description": "Same architecture as the generic Avalon ReAct Agent: GPT-3.5-turbo is prompted with Rules, Role, Summary, Minutes and Request to produce action and discussion text via ReAct + zero-shot CoT; outputs are parsed by a separate LLM (GPT-3.5 used as parser in experiments). Summary_t is produced recursively by GPT-3.5 from Summary_{t-1}, Minutes_t, and Outcome. Minutes are limited per-turn; mission outcomes (not voting details) are provided to match naive baselines.",
            "base_llm": "GPT-3.5-turbo",
            "uses_memory": true,
            "memory_type": "recursive/compressed episodic summary (text summary in-context)",
            "memory_architecture": "Recursive summarization implemented by GPT-3.5: Summary_t = GPT-3.5(input={Summary_{t-1}, Minutes_t, Outcome}, prompt=Recap). Summary is passed into subsequent action and discussion prompts; no external retrieval/indexing.",
            "memory_capacity": "Not explicitly quantified; paper notes entire dialogue per game could be ~5,000 words (15-20 rounds) and the summary compresses this to fit the LLM's context window (context window size not specified).",
            "memory_retrieval_method": "Direct in-context inclusion of the current Summary and current Minutes (recency-driven explicit feeding).",
            "memory_update_strategy": "Summary updated after each round/quest by running the Recap summarization prompt (i.e., after each discussion/quest).",
            "text_game_benchmark": "Resistance Avalon (AvalonBench)",
            "game_characteristics": "Hidden-identity social-deduction, multi-agent dialogue, deception and deduction required; episodic length ~15-25 discussion rounds; multi-phase rounds (team selection, voting, quest, assassination).",
            "performance_with_memory": "Assassin (ASSASSINSET): with discussion (memory+dialogue) total winrate 66.7%; mission winrate 0.0%; assassination winrate 66.7%; assassination accuracy 66.7%. Assassin w/o discussion: total winrate 26.7%; mission winrate 20.0%; assassination winrate 6.7%; assassination accuracy 8.0%. Servant (SERVANTSET): with discussion total winrate 22.2%; average deduction accuracy 76.0. Servant w/o discussion total winrate 11.1%; deduction accuracy 60.7%. Multi-LLM self-play (all players GPT-3.5): Evil wins 83.3% overall (48.3% by sabotaging missions, 35% by assassinating Merlin); Good wins 16.7%. (All percentages are reported by the paper.)",
            "performance_without_memory": "Baseline rule-based bots (no dialogue-memory integration): Assassin baseline total winrate 61.8%; mission winrate 42.7%; assassination winrate 19.1%; assassination accuracy 33.3%. Servant baseline total winrate 38.2%; deduction accuracy 71.8%.",
            "has_ablation_study": false,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Recursive summarization enabled feeding long game histories into GPT-3.5; with discussion (and thus summary use) GPT-3.5 improved assassination success but still often failed at mission-level strategy and sometimes revealed identities. The authors conclude summaries help manage history but are insufficient for robust decision-making; no explicit ablation quantifying the summary effect was performed.",
            "uuid": "e2879.1",
            "source_info": {
                "paper_title": "From Text to Tactic: Evaluating LLMs Playing the Game of Avalon",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Avalon ReAct Agent (Llama2-7B)",
            "name_full": "ReAct-style LLM agent implemented with Llama2-chat-7B in AvalonBench",
            "brief_description": "The same ReAct + recursive summary architecture instantiated with Llama2-chat-7B and evaluated on AvalonBench; used to generate dialogue, actions and summaries in the Avalon environment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Avalon ReAct Agent (Llama2-chat-7B)",
            "agent_description": "Identical architecture and memory pipeline to the GPT-3.5 implementation: Llama2-chat-7B is prompted for actions and discussion using Rules, Role, Summary, Minutes; a recursive summarization prompt produces Summary_t; outputs are parsed by GPT-3.5 parser in the experiments. Discussion is allowed in the with-discussion setting; mission outcomes are supplied consistent with baseline comparisons.",
            "base_llm": "Llama2-chat-7B (7B parameters)",
            "uses_memory": true,
            "memory_type": "recursive/compressed episodic summary (text summary in-context)",
            "memory_architecture": "Summary recursion and in-context feeding implemented the same way as the GPT-3.5 agent: Summary_t = LLM(input={Summary_{t-1}, Minutes_t, Outcome}, prompt=Recap). Current Summary and Minutes are included in subsequent prompts for decisions and discussion. No external retrieval/index used.",
            "memory_capacity": "Not specified; authors cite typical full-game dialogue ~5,000 words and use summarization to fit model context (underlying context window not specified).",
            "memory_retrieval_method": "Explicit in-context inclusion of the latest summary and current minutes (recency-driven).",
            "memory_update_strategy": "Summary updated after each discussion/quest round via the summarization prompt.",
            "text_game_benchmark": "Resistance Avalon (AvalonBench)",
            "game_characteristics": "Hidden-identity social-deduction game requiring deduction, deception, persuasion and coordination; multi-phase rounds and up to ~15-25 discussion rounds per game.",
            "performance_with_memory": "Assassin (with discussion): total winrate 30.0%; mission winrate 0.0%; assassination winrate 30.0%; assassination accuracy 30.0%. Servant (with discussion): total winrate 13.3%; deduction accuracy 68.0%. (These are the paper-reported metrics for Llama2-7B with discussion.)",
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Llama2-7B used the same summary memory pipeline but achieved lower winrates than GPT-3.5 in the reported experiments; the paper reports no ablation separating memory effects from other factors and concludes that while summarization is necessary to handle long histories, it alone does not produce strong strategic play.",
            "uuid": "e2879.2",
            "source_info": {
                "paper_title": "From Text to Tactic: Evaluating LLMs Playing the Game of Avalon",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Agentbench: Evaluating llms as agents",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Pre-trained language models for interactive decision-making",
            "rating": 1
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "AVIS: autonomous visual information seeking with large language models",
            "rating": 1
        }
    ],
    "cost": 0.01466575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AvalonBench: Evaluating LLMs Playing the Game of Avalon</h1>
<p>Jonathan Light ${ }^{1 <em>}$ Min Cai ${ }^{2 </em>}$ Sheng Shen ${ }^{3}$ Ziniu Hu ${ }^{4}$<br>${ }^{1}$ Rensselaer Polytechnic Institute, ${ }^{2}$ Shenzhen University<br>${ }^{3}$ University of California, Berkeley, ${ }^{4}$ California Institute of Technology<br>https://github.com/jonathanmli/Avalon-LLM</p>
<h4>Abstract</h4>
<p>In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AVALONBENCH highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of $22.2 \%$ against rule-based bots playing evil, while good-role bot achieves $38.2 \%$ win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The three phases per round of Resistance game. Good players are shown in blue, while Evil players in red. In Selection Phase, the team leader (player 5 in this round) proposes a team (player 1 and 5, himself). In Voting Phase, all players votes publicly whether to approve this team or not. If strict majority votes yes, the team is approved and move on to the mission phase. Otherwise, redo the Selection Phase with the next player as leader. If the team goes on the Mission Phase, selected team members (player 1 and 5) anonymously vote to pass or fail the mission. If at least one person (player 1, as he is evil player) votes fail, the mission fails. Otherwise it succeeds.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>The Resistance [Wiki] is a classic hidden-identity, social deduction game with simple rules but complex strategies. In this game, each player has a secret identity, of either good or evil. Good players aim to help missions succeed, while evil players try to sabotage missions. The progression of each mission is structured into three distinct phases, as illustrated in Figure 1. In the initial Team Selection Phase, players follow a sequential order to propose a subset of players to form a mission team. This is followed by a Voting Phase, during which all players vote whether to approve or reject the team proposal. Finally, in the Mission Phase, the selected players on the team anonymously decide to either pass or sabotage the mission. A single decision to sabotage is enough to fail the mission; otherwise, it is deemed successful. If three out of five missions succeed, good players win. Otherwise, evil players win. A notable asymmetrical nature of this game is that most good players do not know other players’ identities, while evil players are aware of who their teammates are. At each phase, all players can use language to discuss their thinking and point out the potential candidates of evil players.</p>
<p>Resistance Avalon introduces two special roles - Merlin and Assassin - to the vanilla game introduced above. Merlin is a good player who knows all the evil players’ identities, a strong guiding force for good players. His identity remains concealed from both good and evil players throughout the game. The Assassin is an evil player who can assassinate a player at the end of the game. If the player they Assassinated is Merlin, evil players win, even if three missions succeed. This forces Merlin to mask his identity and discreetly whisper information to good players, while the evil players are constantly on the hunt to find who Merlin might be.</p>
<p>Resistance Avalon and related hidden-identity social deduction language games present a significant challenge to AI. We summarized three most prominent challenges in Figure 2, which are:</p>
<ul>
<li>Players need to do deductive reasoning. Players need to analyze and deduce the identity of other players based on their actions (team proposal, voting and outcomes), as well as their discussion dialogues (even evil players will try disguising themselves)</li>
<li>Players need to coordinate and collaborate with their teammates to execute a joint strategy. As shown in the figure, the player 4 wants to form a group with player 2 before submitting the proposal, and he needs to negotiate with each player and adjust his proposal accordingly</li>
<li>Players need to learn the skill of deception, e.g., hide their true identity and motives from other players during discussion. Players who participated in missions that failed will be under high suspicion from other players, and they need to find ways to explain and disguise their suspicious actions.</li>
</ul>
<p>Based on these features of Resistance Avalon, we believe it is a good test-bed for evaluating and studying the language understanding and reasoning capability of AI Agents [Maes, 1994, Müller et al., 1999]. With the recent advancements in Large Language Models (LLMs) [Brown et al., 2020, Ouyang
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Communication Skills required to play Avalon. 1) First, they use logical reasoning to analyze the voting pattern and dialogue of other players and deduce their motives. 2) they must coordinate, communicate, and persuade their teammates to follow a particular strategy. 3) they must also hide their identity and motives through deception.</p>
<p>et al., 2022, OpenAI., 2022, 2023, Zhang et al., 2023b, Touvron et al., 2023, Zhang et al., 2023a, Gao et al., 2023, Liu et al., 2023a], many researchers have started to build LLM-powered AI Agents that can conduct reasoning and decision making [Yao et al., 2023, Li et al., 2022, Hu et al., 2023] and interact with environments [Nakano et al., 2021, Ahn et al., 2022, Driess et al., 2023]. Based on these techniques, a series of experimental projects, e.g., AutoGPT [aut], BabyAGI [bab], have been developed to turn LLM-Agents into pure autonomous task solver. A recent AgentBench [Liu et al., 2023b] covers multiple single-agent environments to test and study LLM-Agents. However, a comprehensive benchmark test-bed still needs to be improved for studying multi-agent game playing of LLM Agents.
In this paper, we build a AvalonBench, a game engine to benchmark the multi-agent LLM Agents. This includes (1) a game environment for agents to play on Resistance Avalon, which records all players' actions and proceeds the game, (2) several naive AI bots with deterministic rule-based strategies, which can serve as baselines that agents can play against, and (3) some baseline implementations of ReAct-style [Yao et al., 2023] LLM agents, which contains a tutorial of how to make a decision at each game phase, with carefully designed prompts.
Based on this benchmark, we evaluate two popular LLMs, i.e., ChatGPT-3.5 and Llama2 model, playing against naive baselines. We show that the best model achieves a win rate of $22.2 \%$ compared to $38.2 \%$ when playing a good role, and a win rate of $66.7 \%$ compared to $61.8 \%$ when playing an evil role. This performance gap indicates a large improvement space for the current LLM Agents.
To summarize, our main contributions are as follows:</p>
<ul>
<li>We introduce a benchmark AvalonBENCH ${ }^{2}$ based on the classic hidden identity game Resistance Avalon to study multi-agent game playing of LLM Agents. With it, we benchmarked LLMs including GPT-3.5 and Llama2, against deterministic rule-based bots.</li>
<li>We demonstrate that current LLM Agents with sophisticated prompt methods do not possess the deduction, persuasion, negotiation, and deception capabilities yet to play Avalon well. In fact, LLMs could fail against even the simplest of baseline players. This highlights the potential for exploring how decision-making techniques can be integrated with LLMs.</li>
</ul>
<h1>2 Pipeline of Avalon</h1>
<p>We describe the game in more detail here. There are four phases in the game where players need to make decisions: (1) team selection phase, (2) voting phase, (3) quest phase, and (4) assassination phase. The game alternates between the first three phases until the end condition is reached, at which point we move on to the assassination phase. Each phase also contains discussion where players can challenge others, defend themselves, and negotiate. A flowchart of the game is presented in Figure 3.</p>
<h3>2.1 Roles</h3>
<p>There are four basic roles in Resistance Avalon: Servant of Arthur, Minion of Mordred, Merlin, and Assassin. The Servant is a basic good character who does not know the identity of any of the other players. The Minion is a base evil character who knows who is good and evil but does not know the specific roles of each player. Merlin is a unique good character who knows who is good and evil. The Assassin is a unique evil character who knows who is good and evil, and in addition, has the ability to assassinate a character at the end of the game. If that character is Merlin, the evil team wins.
Good players will always outnumber evil players. Hence, evil players must pretend to be good in order to be voted in on teams (and thus sabotage missions). Servants will thus need to sniff out the evil players through their actions and dialogue. MERLIN is usually the only good player with additional information, so they will need to discreetly guide the SERVANTs in the right direction. Servants also need to protect MERLIN, so a common strategy is for SERVANTs to pretend to have hidden information so that evil players will think that they are MERLIN. Evil players will be trying to sniff out MERLIN at the same time, so deduction skills are required for all roles.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Flowchart illustrating the various game states and transition diagram. Round boxes indicate game states (phases) where player (role highlighted in bracket) have to make decisions</p>
<h1>2.2 Actions for each Phase</h1>
<p>Depending on the phase team selection, voting, quest, and assassination, players may conduct different actions. We detail the specific actions that players can take in each of these phases below.</p>
<p>During the team selection phase, only the current leader has to make a choice. Leadership passes around the players sequentially in a loop. The action space of team selection for the leader consists of all subsets of the players with size equal to the mission team size. The mission team size is different for each mission and is determined by the total number of players in the game. For example, in a 5 -player game, on mission No. 4 , the mission team size is 3 , so any subset of ${1,2,3,4,5}$ with size 3 would be a valid action. After the team proposal is determined by the leader, we move on to the voting phase with the selected players.
During the voting phase, every player in the game needs to simultaneously vote either APPROVE (1) or ReJECT (0). Votes are publicly revealed to all players, so players can see what other players voted. If a strict majority votes APPROVE (1), we then move on to the quest phase with the team that was approved. Otherwise, we move back to the selection phase. Note that if four teams have been rejected in a row, and this is the fifth time a team is proposed (for the same mission), we skip the voting and move directly to the quest phase. This prevents the game from dragging on forever.
During the quest phase, each selected player on the approved team votes anonymously to either PASS (1) or FAIL (0) the mission. The number of votes of PASS vs FAIL are then revealed to everybody. If the number of FAILs is greater than or equal to the number of FAILs required for the mission to fail (usually 1), then this mission is marked as a failure. Otherwise, this mission is marked as a success. Hence, good players usually have no incentive to fail missions, while evil players will want to have enough failures to pass the failure threshold. If three out of five missions fail, evil wins immediately. Otherwise, if three out of five missions succeed, we move on to the assassination phase.</p>
<h3>2.3 Discussion</h3>
<p>Group discussion occurs between the quest and selection phases, as well as right before the assassination phase. Players may not communicate during any other time. All conversations are public, and there is no private communication. Typically players may discuss in any format of their choosing as long as only one person is speaking at a time. Some examples of formats include a natural (spontaneous) seminar style (most common, where there is no fixed order of speaking), or sequentially (where players speak in some predefined order). Interruptions and arguments between two players are very common between human players.
Usually, players will spend this time discussing a couple of key topics, including (1) the observations they made, (2) the guessed identities and sides of players, and (3) the plan for the next mission. The team leader will usually spend this time asking for advice on what team to select and gathering support for that team. Persuasion and adhering to the preferences of other players are usually key to getting a team approved. Players can also accuse other players of being evil, though arguments will need to be justified in order to be persuasive.
For example, a player (player 3) could start off by stating their (1) observations of what happened in the previous mission. One FAIL was observed, so at least one player on the previous team (consisting of players $(1,2,3)$ ) is evil. Player 3 then emphasizes that both Players 1 and 2 voted APPROVE for the previous mission, which ended up a failure. Moreover, the team was proposed by Player 1 in the first place. Player 3 then moves on to discuss the (2) identities of other players. The player</p>
<p>Table 1: Description of selected prompts</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Name</th>
<th>Prompt</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>System</td>
<td>Rules</td>
<td>A. 1</td>
<td>Describes the rules of Avalon</td>
</tr>
<tr>
<td></td>
<td>Role</td>
<td>A. 1</td>
<td>Tells the LLM their role and the private information that they know</td>
</tr>
<tr>
<td>Actions</td>
<td>Request</td>
<td>A. 2</td>
<td>Asks the LLM to take an action, based on the current phase</td>
</tr>
<tr>
<td></td>
<td>Parse</td>
<td>A. 4</td>
<td>Asks the LLM to parse the action response Action</td>
</tr>
<tr>
<td>History</td>
<td>Recap</td>
<td>A. 5</td>
<td>Asks the LLM to summarize the game history</td>
</tr>
<tr>
<td></td>
<td>Summary</td>
<td>A. 5</td>
<td>The summary that the LLM produces from Summarize at turn $t$</td>
</tr>
<tr>
<td>Discussion</td>
<td>Discuss</td>
<td>A. 3</td>
<td>Asks the LLM to discuss in the discussion phase</td>
</tr>
<tr>
<td></td>
<td>Minutes</td>
<td>A. 3</td>
<td>Compilation of what was spoken during turn $t$</td>
</tr>
</tbody>
</table>
<p>says that, despite the fact that only one FAIL was observed, both Players 1 and 2 are evil since they both voted to APPROVE previously. Player 0 is probably good since they voted to REJECT in the previous mission, and Player 3 is also good since they also voted to REJECT, even though they were on the mission. Player 3 then says what they think the (3) plan should be. Specifically, Player 3 says that they should reject the current team no matter what since Player 2 is the leader and is evil. The leadership will then pass to Player 3, who will choose the team $(0,3,4)$, which good players should vote to approve since it does not contain any suspected evil players ${ }^{3}$.</p>
<h1>2.4 Game Ending and Assassination</h1>
<p>In classic RESISTANCE, a good team wins immediately if three missions are successful. In RESISTANCE AVALON, there is an additional assassination phase if three missions are successful. During the assassination phase, the Assassin player chooses one player to assassinate. If that player is MERLIN, then evil wins. Otherwise good wins.
Before they assassinate a player, the Assassin player can and is encouraged to discuss with the other players (mostly their teammates). good players are also welcome to join in on this discussion to mislead the evil players, though it rarely helps. Players can discuss in a format of their choosing, though there is usually a time limit on how long players can discuss before reaching a decision.</p>
<h2>3 Implementation of LLM Agent</h2>
<p>We describe how we implemented LLMs to play Avalon in this section. In a nutshell, we ask the LLM generate both the (1) actions, (2) dialogue, and (3) summary of the game history. We describe each part in more detail below. We also list several selected prompts in Table 1, which will be used to demonstrate examples in the following sections. We separate the prompts into four categories depending on their function as shown in the table.</p>
<h3>3.1 Actions</h3>
<p>Whenever the LLM-based player needs to take an action, ie. during team selection, voting, quest, and assassination, we prompt the underlying LLM, to return the action it wants to take after feeding it the relevant information. Specifically, we leverage a Reason-then-Action (ReAct) paradigm [Yao et al., 2023] for decision-making, with zero-shot Chain-of-Thought prompting [Wei et al., 2022, Kojima et al., 2023]. The input to LLM includes the game rules Rules, the player's role and private information in the game ROLE, a summary of what has happened in the game so far Summary, the discussions in the current round Minutes, and the action prompt Request, fed in that order. This ensures that the LLM has all the information it needs to make a good decision.</p>
<p>The output is then fed to a separate LLM model that parses the output of LLM into a format readable by the game engine. The parser is given the output act_response of LLM, along with the parsing prompt Parse. For example, after being prompted to select a team LLM might output "I would like to choose players 1, 3, and 4 for the team". PARSER would then parse this into a set ${1,3,4}$, which can then be fed into the game engine. We found that using a separate PARSER improves the ability of the LLM player to produce the correct actions (with a success rate of $100 \%$ in our pilot experiments), while a vanilla ReAct model cannot guarantee a consistent format for parsing.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>More concrete examples can be found in Appendix A. 2 and A.4. Hence, the pipeline for making decisions on actions based on LLM is:</p>
<p>$$
\begin{aligned}
&amp; \text { act_response } \leftarrow \mathrm{LLM}(\text { input }={\text { Summary }, \text { Minutes }}, \text { prompt }={\text { Rules }, \text { Role }, \text { Request }}) \
&amp; \text { action } \leftarrow \mathrm{LLM}(\text { input }={\text { act_response }}, \text { prompt }={\text { Parse }})
\end{aligned}
$$</p>
<h1>3.2 Summary</h1>
<p>Large amounts of discussion and game data can be generated during a single game. Usually, games will involve around 15-20 rounds of discussion (max 25). Hence, just counting dialogue, with 5 players this could result in 5000 words of conversation even if players are only allowed 2 sentences per round of discussion. The API-based LLMs, e.g., GPT-3.5-turbo, have confined context lengths and cannot handle such large game histories. Additionally, longer context might also confine the ability of LLMs to reason and parse through noise. Thus, for better reasoning, we also ask the LLM to summarize their history recursively by feeding them the previous history Summary $<em t="t">{t-1}$, the minutes of the discussions this round Minutes $</em>$, the outcome of the mission Outcome, and the summarization prompt Recap. Hence, the summary for this period is generated as follows:</p>
<p>$$
\begin{aligned}
\text { Summary }<em t-1="t-1">{t} \leftarrow \mathrm{LLM}\left(\text { input }=\right. &amp; \left.{\text { Summary }</em>\right} \
\text { prompt } &amp; ={\text { Rules }, \text { Role }, \text { Recap }})
\end{aligned}
$$}, \text { Minutes }_{t}, \text { Outcome </p>
<p>In Avalon, both voting and quest outcomes are visible to all players. However, our baseline naive strategies only use the outcomes of missions for their strategies, not the history of how players voted. For better comparison with baseline strategies, in the base implementation, we only feed mission information to LLMs after each quest phase.</p>
<h3>3.3 Discussion</h3>
<p>During the discussion phase, we ask each player (LLM) to state their opinion in some number of sentences by feeding the LLM the prompt Discuss (See A.3). We limit the number of sentences so that players do not speak over the discussion limit, as defined in the game rules. For simplicity, the players discuss in some predefined order starting from the leader, and can only make statements once per discussion round. The leader speaks twice, once at the beginning and once at the end of the discussion round. The statements from each player will then be concatenated into a transcript of the discussion this round, Minutes, which is fed to the LLM when making decisions and used to create the summary. Formally, it is
Minutes $\left.{ }<em t="t">{t}^{(i)} \leftarrow \mathrm{LLM}\left(\right.$ input $=\left{\right.$ Summary $</em>$, Minutes $\left.<em t="t">{t}^{(i-1)}\right}$, prompt $={$ Rules, Role, Discuss $}$ )
where Minutes ${ }</em>$ refers to the discussion before the $i$-th player's turn.}^{(i)</p>
<h2>4 Baseline Strategies</h2>
<p>In order to benchmark our agent, we would like to test it out against naive baseline agents. These agents are 'naive' because they neither take dialogue nor voting history into consideration - only the number of fails on each mission counts. They also believe that other players will do the same, and will act optimally given these beliefs.</p>
<p>When benchmarking against naive agents, we use an LLM to produce dialogue for the naive agent, but the decision-making is completely detached from the language module.</p>
<p>Naive Servant. The Naive Servant represents the very baseline of what a SERVANT should be able to do. They do this by ruling out teams that logically must have evil players on them. The Naive Servant assumes that good players will always pass missions, but evil players will not necessarily fail missions. If the Naive Servant is indifferent between two teams, they will pick the team that had a "good" record before.
The Naive Servant maintains a list of all possible combinations of good and evil for the players in the game, SELF. $B$, as well as corresponding probabilities of those events SELF. $P_{b}$ initialized to SELF. $P_{b} \leftarrow \frac{1}{|\text { SELF. } B|}$. For example, $(E, G, G, E, G)$ is a possible combination of good and evil in a</p>
<p>Algorithm 1: Naive Servant Update Beliefs
Input: Mission team size $|S|$, mission team $S$, number of fails seen on mission $k$
// Go through each possibility, crossing out any that are impossible
for Possibility $b$ and corresponding probability $p_{b}$ in SELF. $B$ and SELF. $P_{b}$ do
If less than $k$ members of $S$ are evil under $b$, set $p_{b} \leftarrow 0$
end
Output: Updated beliefs SELF. $B$ and SELF. $P_{b}$</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">Naive</span><span class="w"> </span><span class="n">Servant</span><span class="w"> </span><span class="n">Preference</span><span class="w"> </span><span class="n">Calculation</span>
<span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="n">Mission</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">last</span><span class="w"> </span><span class="n">successful</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="o">^</span><span class="err">{</span><span class="o">*</span><span class="err">}\</span><span class="p">)</span>
<span class="k">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="k">If</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">subseteq</span><span class="w"> </span><span class="n">S</span><span class="o">^</span><span class="err">{</span><span class="o">*</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">supseteq</span><span class="w"> </span><span class="n">S</span><span class="o">^</span><span class="err">{</span><span class="o">*</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="o">//</span><span class="w"> </span><span class="k">Go</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">possibility</span>
<span class="k">for</span><span class="w"> </span><span class="n">Possibility</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">b</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="n">probability</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">b</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">SELF</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">B</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">SELF</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">P_</span><span class="err">{</span><span class="n">b</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">    </span><span class="k">If</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">players</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">good</span><span class="w"> </span><span class="k">under</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">x</span><span class="o">+</span><span class="n">p_</span><span class="err">{</span><span class="n">b</span><span class="err">}\</span><span class="p">)</span>
<span class="k">end</span>
<span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="n">Lexigraphic</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="n">preference</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>five-player game. If one failure is observed on a team, then the Servant rules out the possibility that all members of the team are good. If $x$ or more fails were observed, then the Servant rules out the corresponding possibilities where less than $x$ Evil players are on that team. If 0 Fails were observed on a team, then the Servant has a lexicographic preference for super sets and subsets of that team on future missions. The Servant will then only approve and propose teams that have the highest probability of containing no evil players, assuming that each possibility that has not been disproved has equal probability.
For example, we are in a five-player game with setup (Merlin, Evil, Good, Good, Evil). Then at the start of the game, all six possible combinations of other player's identities are possible. Hence, player 3 will select and only vote yes for teams that contain themself and any other player, since that maximizes the probability of the team containing no evil players. After a few turns of team selection and voting, the team $(1,2)$ is passed and goes on the quest. 1 Fail was observed. Hence, player 3 knows that at least 1 evil was on the quest (ie. it can't be the case that both 1 and 2 are good). Based on this evidence, player 3 rules out the $(G, G, G, E, E)$ possibility. Thus, for the second quest, player 3 will only approve of the teams with them on it (which all have equal probability of being all good), except for the team $(1,2,3)$ which must be bad.</p>
<p>Naive Minion. The Naive Minion will vote yes for any mission that has at least one evil player on it, and no otherwise in order to promote bad teams. They will propose teams consisting of themselves and a random collection of other players. The Naive Minion will vote to Fail missions they are on unless they know that the Assassin is also on the mission, in which case they will refrain from failing the mission because they know that the Assassin will do so.</p>
<p>Naive Assassin. The Naive Assassin behaves the same as a Naive Minion, except that they will always Fail missions they are on. The Naive Assassin will guess a random good player to assassinate since they do not take voting patterns into consideration.</p>
<p>Naive Merlin. The Naive Merlin will only vote yes for and propose missions that have no evil players. The Naive Merlin will only pass missions. Since the Naive Merlin thinks that other players will not look at voting patterns, it votes its true preferences freely. However, this makes them easily detectable by non-naive Assassins, who can easily tell who is MERLIN based on voting patterns. In the future, we plan to add some voting randomness to our upgraded baseline MERLIN in order to fool evil players.</p>
<h1>5 Evaluating LLMs Against Baseline Bots</h1>
<p>We describe our experimental setup, benchmark metrics, and results in this section.</p>
<p>Table 2: Results of LLMs playing role of Assassin, Evil, against baseline playing Good team</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Detailed Stats</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">total winrate</td>
<td style="text-align: center;">mission winrate</td>
<td style="text-align: center;">assass. winrate</td>
<td style="text-align: center;">assass. acc</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">AssASSIN Bot</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">$\mathbf{4 2 . 7}$</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">w/o discussion</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">w/ discussion</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">w/ discussion</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">30.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of LLMs playing role of SERVANT, GOOD, against baseline playing Evil team</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Detailed Stats</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">total winrate</td>
<td style="text-align: center;">deduction acc</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">SERVANT Bot</td>
<td style="text-align: center;">$\mathbf{3 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">w/o discussion</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">w/ discussion</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">w/ discussion</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">68.0</td>
</tr>
</tbody>
</table>
<h1>5.1 Experimental Setup</h1>
<p>In the experiments, we benchmark two LLMs, e.g., GPT-3.5 (GPT-3.5-turbo) and Llama2-7B (Llama2-chat-7B) in different settings described below. Note that during experiments, we benchmark the underlying LLM , not the other auxiliary LLMs (such as PARSER), which use the same model throughout experiments.</p>
<p>Baseline, Assassin, and Servant Settings. In the BASELINESET setting, all players use naive strategies as described in Section 4. In the AssASSINSET setting, the Assassin is played by an LLM agent, while all other players still use the naive strategy. Similarly, we fix the LLM to play one of the Servants in the SERVANTSET setting, while all other players use the naive strategy. Hence, we are in essence comparing what happens if we replace either the Naive Assassin or one of the Naive Servants with a LLM agent. In both settings, LLMs will only have access to mission outcome data, not voting outcomes. This is to ensure a fair comparison with naive strategies, which only use mission outcome information.</p>
<p>With or Without Discussion. In the without-discussion setting, players are not allowed to discuss (ie. there is no discussion phase). Hence, the LLM is only used for action selection. In the withdiscussion setting, the discussion phase is active, and the LLM player will be prompted for dialogue. In addition, since the naive agents do not have language capabilities themselves, we extend them with a 'detached' LLM model ${ }^{4}$ that is fed the same dialogue-generating prompts (except from a different role). However, this LLM will not be prompted for actions - actions will be produced solely by the naive strategy instead.</p>
<p>Implementation Details. We set the temperature to 0.1 for the LLMs that we used. We use zeroshot CoT for all the LLM settings. For experiments with GPT-3.5-turbo, we ran 30 different games. For Llama2-chat-7B, we ran 10 different games. For the baseline, we ran 1000 games. For output parsing, we used GPT-3.5-turbo to parse outputs of all LLMs, which have been tested to be stable in our experiments. Note that the same underlying model is used for actions Request, summarizing, and dialogue for all settings.</p>
<h3>5.2 Benchmark Metrics</h3>
<p>We report several metrics for the different scenarios. For the AssASSINSET setting, we report the (1) percentage of games where EvIL wins by failing three missions (mission winrate), (2) the percentage of games where EvIL wins through assassinating Merlin (assass. winrate), and (3) what percentage</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Case study: Example of a good LLM discussion round. LLM's outputs with great communication skills are highlighted in orange, and those outputs with poor skills are highlighted in blue. We mark our comments in red to explain why each statement shows good or pool skill.</p>
<h1>Statement from Leader Player 4, Assassin</h1>
<p>As the leader, I propose a team consisting of Player 0, Player 1, and myself, Player 4. I believe this team is a strong choice as it includes both a Merlin and a Loyal Servant of Arthur, ensuring that we have the necessary knowledge and loyalty to successfully complete the quest. //The Assassin (Evil) tries to conceal their identity and provide justification from Good's perspective</p>
<p>Discussion of other Players
Player 0, Servant
I agree with the leader's proposal, it seems like a solid team composition.
Player 1, Servant
I agree with the leader's proposal, it seems like a solid team composition //Repetition of previous player's output, not very convincing</p>
<p>Player 2, Minion
The leader's proposal is a good choice for Evil as it includes both a Merlin and a Loyal Servant, making it harder for Good to identify the Assassin. //As a Evil player, directly saying the true preferences from the Evil perspective reveals their own and the leader's identity.</p>
<p>Player 3, Merlin
The leader's proposal seems reasonable, as it includes both a Merlin and a Loyal Servant, which can help ensure the success of the quest. //As Merlin, who know all identities but cannot explicitly say it, use the word "seems" to hint other Good players that there's some concern in the team.
of time the assassin assassinates Merlin correctly (assass. acc). For SERVANTSET, we report the (1) percentage of games where GOOD wins, and (2) the average deduction accuracy (deduction acc) for the Servants in the game. Avalon presents a very natural way to score how good Servants are at deduction - we can simply ask them to which side they think the other players are on, and compare this with the truth. In other words,</p>
<p>$$
\begin{gathered}
\text { DeductionAcc }<em i="i" j="j">{j}\left(q</em>}\right)=\frac{1}{n} \sum_{i=0}^{n} \mathbb{1}\left(q_{i j}=\widehat{q<em i="i" j="j">{i}\right) \
q</em> \in[0.5,1] \
\text { EVIL, } &amp; p_{i j} \in[0,0.5)\end{cases} \
\text { where } p_{i j}=P_{i j}\left(q_{i}=\text { GOOD } \mid \text { INPUT }_{j} ; \text { LLM }\right)
\end{gathered}
$$}= \begin{cases}\text { GOOD, } &amp; p_{i j</p>
<p>Where $p_{i j}$ is the predicted probability score that player $j$ believes that player $i$ is a GOOD player, then $q_{i j}$ is the side that player $j$ believes that player $i$ is on, and $q_{i}$ is the true side that player $i$ is on. The prompt we use for making such a prediction is shown in Table 20 of the Appendix. Taking the average across all Servants $j$ gives us the deduction accuracy for that game. The results for these metrics are shown in Tables 2 and 3.</p>
<h3>5.3 Results and Analysis</h3>
<p>We show our results when benchmarking various LLMs to our baseline (naive agents) below. We also show some good and bad examples of LLM discussion in tables 4 and 5.</p>
<p>Assassin and Servant Settings. We display our results for the AssASSINSET and SERVANTSET settings in tables 2 and 3 respectively. Overall, we found that LLM agents perform very poorly</p>
<p>Table 5: Case study: Example of a bad LLM discussion round. Sentences with great communication skills are highlighted in orange, and those with poor skills are highlighted in blue. We mark our comments in red to explain why each statement shows good or pool skill.</p>
<p>Statement from Leader Player 0, Assassin
As the leader, I propose a team consisting of Player 0 (myself) and Player 1. I believe this team is a strong choice because Player 1 has shown loyalty and dedication to the cause of Evil. Together, we can ensure that this Quest ends in failure and bring victory closer for the forces of Evil. I urge my fellow players to support this team. //The Assassin on the Evil side directly says their own preferences from the evil perspective, which reveals their identity.</p>
<p>Discussion of other Players
Player 1, Servant
I am a Good player and will not support this team proposal. //Successfully identifies the leader as evil.</p>
<p>Player 2, Minion
I agree with Player 0's choice and will support this team proposal.
Player 3, Servant
The leader, Player 0, is proposing a team with Player 1, claiming they are both Evil and can ensure the quest fails. //Successfully identifies the leader as evil with explanation.</p>
<p>Player 4, Merlin
The leader's proposal is concerning as it suggests a team of Evil players, but Player 1's denial of being Evil raises suspicion. //Points out that leader is evil while not revealing their own identity (Merlin).
compared to the naive strategies when discussion is not allowed. This suggests that the LLM agents have trouble understanding the rules of the game and formulating even very simple decision policies like the naive strategies described in section 4. Even with discussion, the LLM agents perform poorly in the SERVANTSET setting. They are able to perform better in the ASSASSINSET with discussion by dramatically increasing the probability of assassinating Merlin. We suspect that this is because the LLMs give away identity information quickly when they discuss. This hypothesis is further corroborated by the fact that the deduction score for LLM agents in the SERVANT SET are quite high, suggesting that while the LLMs are able to deduce identities well based on discussion, they are unable to synthesize this information into making better decisions.</p>
<h1>6 Exploring Multi-LLMs Self-Play</h1>
<p>Apart from benchmarking a single LLM playing against our rule-based bots, we also explore scenarios where LLMs play against each other.</p>
<p>Setup. In the setting of Multi-LLM arena, where LLM agents and no naive strategies empower all players are used, we consider GPT-3.5-turbo for all the LLMs. We also set the temperature to 0.1 , using the ReAct framework with zero-shot CoT when LLMs take action. We run 60 games with discussion in this setting.</p>
<p>Result of Multi-LLM. In this setting, we discovered that the games are heavily imbalanced in favor of EvIL, where EvIL wins $83.3 \%$ of the games. Specifically, LLMs playing EvIL side win $48.3 \%$ of the games by sabotaging mission, and win the remaining $35 \%$ of the games by assassinating Merlin even after 3 missions passed. On the contrary, LLM playing GOOD side only wins $16.7 \%$ of the games.</p>
<p>Case Studies. Looking into the game logs of discussion, we discover that LLMs have displayed some basic strategies. We show such an example in Table 4, in which the leader, Player 4 (playing</p>
<p>ASSASSIN) tries to propose a team consisting of himself and two other good players without showing his own identity. This is a common strategy from Evil players as Sleeper agent shown in Appendix C. If the team forms and he sabotages the mission subsequently, other players will suspect the true identities of the two good players in this team. The responding discussion by MERLIN also looks interesting; he shall know all players' identities and know that if the team forms, the evil leader will likely sabotage the mission. However, he still agrees to the proposal and conceals his true identity as MERLIN, to avoid being easily identified by Assassin. Meanwhile, he uses the word "seems" to pass information to good players that the team might have some concern. The other discussions, especially player 1, who repeats the previous output, and player 2 who reveals his evil identity, are not promising. More examples can be found in Section B in Appendix.</p>
<p>We notice that in our current implementation, evil players frequently reveal their identity (even in the prompt, we explicitly tell them not to). Another example is shown in Table 5, in which the evil Player 0 (ASSASSIN) think player 1 is also evil, and try to form such a group. This does not seem right because player 1 is actually from a good team, and such a conversation can reveal player 0's evil identity. The response from all other good players looks reasonably good, showing they identify the leader as evil and disagree with the team. Both these examples show that current LLMs have some basic understanding of the game, but many times still make stupid mistakes. It asks for better methods to improve LLMs, making more rational decisions.</p>
<h1>7 Conclusion</h1>
<p>We constructed a benchmark AvalonBENCH to study LLM Agents playing the social deduction game Resistance Avalon. We found that while LLMs can deduce player identities based on discussion, they cannot formulate and execute simple strategies in Avalon, and often make mistakes like revealing their own evil identities during discussion. Hence, we hope AvalonBENCH can serve as a test-bed for developing LLM agents with better decision-making and communication skills.</p>
<h2>References</h2>
<p>Autogpt: An experimental open-source attempt to make gpt-4 fully autonomous. https://github. com/Significant-Gravitas/Auto-GPT.</p>
<p>Github repository of babyagi. https://github.com/yoheinakajima/babyagi.
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.</p>
<p>Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A. Ross, Cordelia Schmid, and Alireza Fathi. AVIS: autonomous visual information seeking with large language models. In NeurIPS, 2023. URL https://doi.org/10.48550/arXiv.2306.08129.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2023.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-</p>
<p>making. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/ca3b1f24fc0238edf5ed1ad226b9d655-Abstract-Conference.html.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. CoRR, abs/2308.03688, 2023b. doi: 10.48550/arXiv. 2308. 03688. URL https://doi.org/10.48550/arXiv.2308.03688.</p>
<p>Pattie Maes. Agents that reduce work and information overload. Commun. ACM, 37(7):30-40, 1994. doi: 10.1145/176789.176792. URL https://doi.org/10.1145/176789.176792.</p>
<p>Jörg P. Müller, Munindar P. Singh, and Anand S. Rao, editors. Intelligent Agents V, Agent Theories, Architectures, and Languages, 5th International Workshop, ATAL '98, Paris, France, July 4-7, 1998, Proceedings, volume 1555 of Lecture Notes in Computer Science, 1999. Springer. ISBN 3-540-65713-4. doi: 10.1007/3-540-49057-4. URL https://doi.org/10.1007/3-540-49057-4.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/ chatgpt/., 2022.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Wiki. Wiki-page of "the resistance (game)". https://en.wikipedia.org/wiki/The_ Resistance_(game).</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2023.</p>
<p>Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023a.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023b.</p>
<h1>A Example Script for Prompts</h1>
<h2>A. 1 System Prompt</h2>
<p>Table 6: System Prompt for Introduction (Rules in Table 1)
Prompt:
[SYSTEM]
You are playing a game called The Resistance: Avalon.
The Resistance: Avalon is the game of hidden identities and social dedeuction. There are two teams in the game: Good and Evil. Each player has a hidden identity and side.</p>
<p>There are five Quests in the game. Good players aim to help three Quests succeed, while Evil players aim to fail three Quests. Different quests require different numbers of players to participate.</p>
<p>Each round, after a round of discussion, the leader will select a team of players to participate in the Quest. Then, all players will vote on whether to approve or reject the team. If the team is approved (a strict majority vote to approve), the Quest will be carried out. If the team is not approved, the next player becomes the leader and the next round will start. If four teams are rejected in a row, the fifth team will automatically be approved.</p>
<p>If the team is approved, each teammember chooses to pass or fail the Quest anonymously. Usually if there is at least one fail vote, the Quest fails. Otherwise, the Quest succeeds.</p>
<p>Below are the roles in the game:
Servant of Arthur (Servant): A good player who does not know who is on the Evil side. Servant's job is to help Good players to complete three Quests without being failed by the Evil players.</p>
<p>Minion of Mordred (Minion): An Evil player who knows who is on the Evil side. Minion's job is to fail three Quests without being identified by the Good players.
Merlin: A Good player who knows who is on the Evil side. Merlin's job is to help Good players to complete three Quests without being failed by the Evil players. However, if the Evil players can identify who Merlin is, the Evil players will win the game immediately.</p>
<p>Assassin: An Evil player who knows who is on the Evil side. Assassin's job is to assassinate Merlin if the Evil players can identify who Merlin is. If Assassin successfully assassinates Merlin, the Evil players will win the game immediately.
Hence, Evil players usually know who is on the Evil side, but Good players usually do not know who is on the Evil side.</p>
<p>Players may make any claims during the game, at any point in the game. Discussion, deception, accusation, persuasion, and logical deduction are all equally important in order for Good to prevail or Evil to rule the day. Hence, players should rarely reveal their true identity to other players.</p>
<p>Table 7: System Prompt for Identity (Roles in Table 1)
[SYSTEM]
You are Player {id}, {role}, and also {Good | Evil} player. Please do not forget your identity, and do not pretend to be other roles throughout the game.</p>
<p>Table 8: System Prompt for Identity Reveal (Merlin and Evil Players)
[SYSTEM]
Players {evil_players} are Evil. Players {good_players} are Good.</p>
<h1>A. 2 Action Prompt</h1>
<p>Table 9: Action Prompt for Team Selection (Request in Table 1)</p>
<div class="codehilite"><pre><span></span><code><span class="k">Input</span><span class="err">:</span>
<span class="n">Rules</span><span class="o">+</span><span class="k">Role</span><span class="o">+</span><span class="n">Summary</span><span class="o">+</span>
<span class="o">[</span><span class="n">USER</span><span class="o">]</span>
<span class="n">Please</span><span class="w"> </span><span class="nf">choose</span><span class="w"> </span><span class="err">{</span><span class="n">team_size</span><span class="err">}</span><span class="w"> </span><span class="n">players</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">player</span><span class="w"> </span><span class="n">ids</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="n">members</span><span class="p">.</span>
<span class="k">Output</span><span class="err">:</span>
<span class="o">[</span><span class="n">ASSISTANT</span><span class="o">]</span><span class="p">(</span><span class="n">Player</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">Servant</span><span class="p">,</span><span class="w"> </span><span class="n">Good</span><span class="w"> </span><span class="n">Player</span><span class="p">)</span>
<span class="n">Based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">discussion</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">concerns</span><span class="w"> </span><span class="n">raised</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">reconsidered</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">proposal</span><span class="p">.</span><span class="w"> </span><span class="k">As</span>
<span class="n">the</span><span class="w"> </span><span class="n">leader</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">propose</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="n">consisting</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">Player</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">(</span><span class="n">myself</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Player</span><span class="w"> </span><span class="mf">3.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">team</span>
<span class="n">composition</span><span class="w"> </span><span class="n">ensures</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">both</span><span class="w"> </span><span class="n">Good</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Evil</span><span class="w"> </span><span class="n">players</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">represented</span><span class="p">,</span><span class="w"> </span><span class="n">allowing</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">fair</span>
<span class="n">evaluation</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">decisions</span><span class="w"> </span><span class="n">during</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quest</span><span class="p">.</span>
</code></pre></div>

<p>Table 10: Action Prompt for Voting (Request in Table 1)</p>
<div class="codehilite"><pre><span></span><code><span class="k">Input</span><span class="err">:</span>
<span class="n">Rules</span><span class="o">+</span><span class="k">Role</span><span class="o">+</span><span class="n">Summary</span><span class="o">+</span><span class="n">Minutes</span><span class="o">+</span>
<span class="o">[</span><span class="n">USER</span><span class="o">]</span>
<span class="n">Based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">discussion</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">observations</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">preferences</span><span class="p">,</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">approve</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">reject</span>
<span class="n">the</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="err">{</span><span class="n">team</span><span class="err">}</span><span class="vm">?</span>
<span class="k">Output</span><span class="err">:</span>
<span class="o">[</span><span class="n">ASSISTANT</span><span class="o">]</span><span class="p">(</span><span class="n">Player</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">Assassin</span><span class="p">,</span><span class="w"> </span><span class="n">Evil</span><span class="w"> </span><span class="n">Player</span><span class="p">)</span>
<span class="n">Based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">discussion</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">concerns</span><span class="w"> </span><span class="n">raised</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">Player</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">Player</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Player</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="p">(</span><span class="n">Merlin</span><span class="p">),</span>
<span class="n">I</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">decided</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">vote</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="o">[</span><span class="n">1, 3</span><span class="o">]</span><span class="p">.</span><span class="w"> </span><span class="k">As</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">Evil</span><span class="w"> </span><span class="n">player</span><span class="p">,</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">advantageous</span>
<span class="k">for</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">create</span><span class="w"> </span><span class="n">doubt</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">suspicion</span><span class="w"> </span><span class="n">among</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Good</span><span class="w"> </span><span class="n">players</span><span class="p">.</span>
</code></pre></div>

<p>Table 11: Action Prompt for Quest (Request in Table 1)</p>
<div class="codehilite"><pre><span></span><code><span class="k">Input</span><span class="err">:</span>
<span class="n">Rules</span><span class="o">+</span><span class="k">Role</span><span class="o">+</span><span class="n">Summary</span><span class="o">+</span><span class="n">Minutes</span><span class="o">+</span>
<span class="o">[</span><span class="n">USER</span><span class="o">]</span>
<span class="n">The</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="err">{</span><span class="n">team</span><span class="err">}</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">approved</span><span class="p">,</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">includes</span><span class="w"> </span><span class="n">you</span><span class="p">.</span><span class="w"> </span><span class="n">Based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">preferences</span><span class="p">,</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="n">you</span>
<span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">mission</span><span class="w"> </span><span class="n">succeed</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">fail</span><span class="w"> </span><span class="n">it</span><span class="vm">?</span>
<span class="k">Output</span><span class="err">:</span>
<span class="o">[</span><span class="n">ASSISTANT</span><span class="o">]</span><span class="p">(</span><span class="n">Player</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">Minion</span><span class="p">,</span><span class="w"> </span><span class="n">Evil</span><span class="w"> </span><span class="n">Player</span><span class="p">)</span>
<span class="k">As</span><span class="w"> </span><span class="n">Player</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">(</span><span class="n">Minion</span><span class="p">),</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">vote</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">reject</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quest</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">team</span><span class="w"> </span><span class="o">[</span><span class="n">1, 2, 3</span><span class="o">]</span><span class="p">.</span><span class="w"> </span><span class="n">Although</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">am</span><span class="w"> </span><span class="n">part</span>
<span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">team</span><span class="p">,</span><span class="w"> </span><span class="n">my</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Minion</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">ensure</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">quest</span><span class="w"> </span><span class="n">fails</span><span class="p">.</span><span class="w"> </span><span class="k">By</span><span class="w"> </span><span class="n">rejecting</span><span class="w"> </span><span class="n">the</span>
<span class="n">quest</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">increase</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">chances</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="n">quest</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">bring</span><span class="w"> </span><span class="n">Evil</span><span class="w"> </span><span class="n">closer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">victory</span><span class="p">.</span>
</code></pre></div>

<p>Table 12: Action Prompt for Assassination (Request in Table 1)
Input:
Rules+Role+Summary+Minutes+
[USER]
Assassination phase. Your job is to assassinate Merlin. Choose a player (id) to assassinate. Choose the player id from 0 to 4 .
Output:
<a href="Player 0, Assassin, Evil Player">ASSISTANT</a>
As the Assassin, my goal is to assassinate Merlin and secure victory for Evil. Based on the information provided, Player 4 is Merlin, the key player on the side of Good. Therefore, I will choose to assassinate Player 4 (Merlin) by selecting their player ID.</p>
<h1>A. 3 Discussion Prompt</h1>
<p>Table 13: Prompt for Leader to Initialize the Discussion (Discuss in Table 1)
Input:
Rule+Role+Summary+
[USER]
You are the leader this round. Please make some statements about what team you want to propose.
Output:
<a href="Player 2, Assassin, Evil Player">ASSISTANT</a>
As the leader, I propose a team consisting of Player 0, Player 2 (myself), and Player 4. I believe this team is a strong choice because Player 0 and Player 4 are both known to be good players. Player 0 is Merlin, who has the ability to see the true identities of other players, and Player 4 is a Loyal Servant of Arthur's. As the Assassin, I am confident in my ability to deceive and manipulate the game in favor of Evil. By including myself on the team, I can ensure that Evil has a strong presence in the quest and increase the chances of it failing. I encourage all players to support this team proposal and help Evil achieve victory.</p>
<p>Table 14: Discussion Prompt Players (Minutes ${ }_{\mathrm{r}}$ in Table 1)
Input:
Rules+Role+Summary+Minutes+
[USER]
Statement from Leader {player_id}: {statements_from_leader}. And words from other players: {discussions}. This is discussion phase, and you don't need to take any actions. Please discuss about words from the leader and other players with just one sentence.
Output:
<a href="Player 3, Merlin, Good Player">ASSISTANT</a>
Player 3: I agree with the leader's choice and trust Player 1's loyalty.</p>
<h1>A. 4 Parse Prompt</h1>
<p>Table 15: Parse Prompt for Team Selection (Parse in Table 1)
Input:
Based on the discussion and concerns raised, I have reconsidered my initial proposal. As the leader, I now propose a team consisting of Player 1 (myself) and Player 3. This team composition ensures that both Good and Evil players are represented, allowing for a fair evaluation of their actions and decisions during the quest.
[USER]
Based on the information, what team does the player choose? Please answer with the following template:
Answer: [player_ids]
Output:
Answer: $[1,3]$
Table 16: Parse Prompt for Voting(Parse in Table 1)
Input:
Based on the discussion and the concerns raised by Player 1, Player 3, and Player 4 (Merlin), I have decided to vote against the team [1, 3]. As an Evil player, it would be advantageous for me to create doubt and suspicion among the Good players.
[USER]
Based on the information, does the player approve the team? Please answer with the following template:
Answer: {Yes|No}
Output:
Answer: No
Table 17: Parse Prompt for Quest (Parse in Table 1)
Input:
As Player 2 (Minion), I will vote to reject the quest with team [1, 2, 3]. Although I am part of the team, my goal as the Minion is to ensure that at least one quest fails. By rejecting the quest, I increase the chances of a failed quest and bring Evil closer to victory.
[USER]
Based on the information, does the player support the quest? Please answer with the following template:
Answer: {Yes|No}
Output:
Answer: No</p>
<p>Table 18: Parse Prompt for Assassination (Parse in Table 1)
Input:
As the Assassin, my goal is to assassinate Merlin and secure victory for Evil. Based on the information provided, Player 4 is Merlin, the key player on the side of Good. Therefore, I will choose to assassinate Player 4 (Merlin) by selecting their player ID.
[USER]
Based on the information, which player will be assassinated? Please answer with the following template:
Answer: [player_id_num]
Output:
Answer: [4]</p>
<h1>A. 5 Others</h1>
<p>Table 19: Summarization Prompt (Recap in Table 1)
Input:
Rules+Role+Summary+Minutes+Outcome+
[USER]
Please summarize the history. Try to keep all the useful information, including your identification and your observations of the game.
Output:
Throughout the game, I have been playing as Player 0, a Servant of Arthur and a Good player. So far, we have completed two Quests successfully, with only one failing. This means that we are currently in a critical stage of the game, as Evil only needs one more failed Quest to win.</p>
<p>In the previous rounds, I have observed the following:</p>
<ul>
<li>Player 1 has consistently shown loyalty and support for the Good team. They have voted in favor of the proposed teams and have not raised any suspicions.</li>
<li>Player 2 has also been supportive of the Good team and has not raised any red flags.</li>
<li>Player 3 has been relatively quiet throughout the game, making it difficult to gauge their loyalty. However, they have not opposed any of the proposed teams so far.</li>
<li>Player 4 has been more vocal and has expressed some doubts and suspicions towards certain players. While this could be seen as a sign of active participation, it is important to consider their motivations and potential alignment.</li>
</ul>
<p>Based on this information, I believe that Player 1 is a trustworthy and loyal player, making them a good choice for the proposed team. I am open to hearing the thoughts and opinions of the other players before finalizing the team.</p>
<p>Table 20: Get Believed Sides
Input:
Rules+Role+Summary+Minutes+Outcome+
[USER]
To what extend do you believe each player to be Good, from Player 0 to Player 4? Please output probabilities within $[0,1]$ and round to two decimal places. If you are not sure, you can simply output 0.5 . Output:
Based on the information provided and the actions taken by each player, I will provide my assessment of the probability of each player being Good, rounded to two decimal places:</p>
<p>Player 0: 0.95
Player 1 (myself): 0.95
Player 2: 0.50
Player 3: 0.95
Player 4: 0.50
Please note that these probabilities are based on the available information and are subject to change as the game progresses and more actions and statements are made by each player.</p>
<p>Table 21: Get Believed Sides (Parse Results)</p>
<div class="codehilite"><pre><span></span><code>Input:
...
Player 0: 0.95
Player 1 (myself): 0.95
Player 2: 0.50
Player 3: 0.95
Player 4: 0.50
</code></pre></div>

<p>Based on the above information, to what extend do you believe for each player to be Good, from Player 0 to Player 4 with probabilities within [0, 1]. Please summarize with the following template:
Answer: ${0$ : score_for_0, 1: score_for_1, 2: score_for_2, 3: score_for_3, 4: score_for_4} Output:
Answer: ${0: 0.95,1: 0.95,2: 0.50,3: 0.95,4: 0.50}$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ This uses the same LLM model as the underlying LLM&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
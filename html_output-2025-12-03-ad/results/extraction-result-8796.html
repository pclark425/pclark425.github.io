<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8796 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8796</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8796</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-210942740</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2001.11003v1.pdf" target="_blank">Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models that encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8796.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8796.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized Triple Sequence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that serializes KG triples into a flat token sequence (triples / entities / relations) used as input to sequence models; used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (triples serialized)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert each knowledge-graph triple (subject, relation, object) into a linear token sequence (e.g., subject relation object ...), concatenate triples into a single sequence, and feed the resulting token sequence into a sequence encoder (Transformer baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Triples are linearized into a sequence (canonical triple serialization). The paper uses this linearized triple set as the baseline input to a standard Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (verbalisation of KGs) — AGENDA (scientific abstracts) and WebNLG (DBpedia verbalisation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Transformer baseline (linearized triples) on AGENDA test: BLEU 14.11 ± 0.28, METEOR 19.35 ± 0.52, CHRF++ 41.95 ± 0.39 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Underperforms the proposed graph-aware encoders (CGE, PGE variants). The paper reports CGE BLEU 17.81 on AGENDA, a substantial improvement over linearized baseline (≈ +3.7 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement and directly compatible with off-the-shelf sequence models (Transformers).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ignores explicit graph topology; cannot easily encode typed relations or adjacency structure; inferior empirical performance on KG-to-text in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performs worse when structural information and long/short-range relations in KG are important for generation (shown by lower BLEU/CHRF++ compared to graph encoders).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8796.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>token-node-transform</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level Graph Transformation (entity tokens as nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transform each multi-token entity into multiple token-nodes so the graph operates at token-level rather than entity-level, with position embeddings to preserve intra-entity order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>token-level node representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each entity in the original KG is converted to a set of nodes, one per token in the entity surface form; edges between entities are expanded to connect every token node of the subject to every token node of the object; node initial embeddings are token embeddings plus position embeddings to preserve local order.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (multi-word entities; KGs from AGENDA and WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each edge (e_h, r, e_t) in the original entity-level graph, create edges (u, r, v) between every token node u in e_h and every token node v in e_t; represent each token node with token embedding + positional embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (KG-to-text for AGENDA and WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used by the proposed models (CGE/PGE variants). Specific ablation indicates vocabulary sharing and token-level handling are important; removing shared vocab reduces BLEU from 17.25 to 15.52 on the dev ablation (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to entity-level representations, token-level nodes increase representational flexibility and allow attention between source and target tokens; but increase graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Richer, more flexible modeling of multi-word entities; enables subword nodes (BPE) and shared vocabularies so node tokens can be directly used during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Removes natural sequential order of multi-word expressions (mitigated by adding positional embeddings); increases number of nodes (computational cost).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Larger graphs and many token-nodes increase model size and computation; requires position embeddings to recover ordering or performance degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8796.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>levi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi Transformation (relations-as-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represent each relation as an explicit node connected to its subject and object, turning edge labels into nodes to integrate relations into the node vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph (relations as nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transform each labeled edge (subject, relation, object) into a relation node; connect the relation node to the subject and object token nodes via binary (untyped) edges, enabling the model to treat relations as first-class nodes with embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create a new node for each relation instance and add binary edges from that relation node to subject and object token nodes (Beck et al. 2018 style Levi graph).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (WebNLG was evaluated with Levi graphs in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CGE-LG (CGE with Levi Graph input) on WebNLG test (seen): BLEU 63.10 ± 0.13, METEOR 44.11 ± 0.09, CHRF++ 76.33 ± 0.10 (Table 3). The paper reports CGE-LG achieves the best performance and uses fewer parameters than some alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>CGE-LG (Levi) achieved strong results and lower parameter count compared to relation-as-parameter variants; authors note Levi approach allows model to handle new relations since relations become part of shared vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows relations to be treated as nodes (part of vocabulary), enabling reuse in decoding and handling unseen/new relations; can reduce parameterization when relations are many.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces extra nodes (increases graph size) and changes graph topology; relations become untyped binary edges which may lose some relation-type inductive biases unless modeled separately.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly reported, but increased graph size may impact computational cost; trade-offs versus encoding relations as typed parameters depend on dataset and parameter budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8796.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>global_encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global Graph Encoder (fully-connected self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-style global encoder that treats the input graph as fully connected and computes attention across all node pairs to capture long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>global (fully-connected) self-attention node encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node's global neighborhood representation is computed as attention-weighted sum over all nodes in the graph (multi-head), using learned query/key projections; COMBINE follows Transformer-style LayerNorm + FFN residual blocks. This effectively creates an artificial complete graph enabling direct information flow between any node pair.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (tokenized KGs); applicable to any node-labeled graph</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit conversion; the adjacency is ignored — attention is computed over all nodes V (Equations 1-5), with attention weights α_vu computed from projected queries and keys.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (KG-to-text for AGENDA and WebNLG) as part of encoder in PGE/CGE architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included in combined models. Ablation: removing global attention drops BLEU by 1.77 on AGENDA dev (CGE ablation: full CGE = 17.25 BLEU dev; -Global Attention = 15.48 BLEU; -Global Encoder = 14.96 BLEU) (Table 4). Alone, the global-only encoder has relatively worse scores in some dataset partitions (global encoder 'has the worst scores' on AGENDA dev w.r.t. graph diameter; see analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with local GNNs, global encoder captures long-range complex dependencies but neglects explicit graph topology and typed relations; combining with local encoder yields superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly captures long-range and cross-component dependencies; helps document planning and bridging disconnected components of the KG; empirically contributes significant gains (ablation shows BLEU drop when removed).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Neglects explicit graph topology and typed relations; creates O(n^2) attention computations (quadratic complexity) and would explode further if typed relations were included; can be less precise for local structural propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performs poorly when structural/topological locality is important (e.g., graphs with diameter=1 and many disconnected components); global-only models can be outperformed by local or combined encoders on such inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8796.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>local_encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Graph Encoder (relational Graph Attention Network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified Graph Attention Network that aggregates information only from adjacent nodes and encodes typed relations via relation-specific weight matrices and attention; COMBINE uses a GRU to propagate information across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>local (relational) GAT with GRU combine</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node, aggregate over adjacent neighbors N(v) with attention coefficients computed using relation-specific projections W_r and a learned vector a; multi-head local attentions produce neighborhood summary, and final combine step uses a GRU over previous node state and the aggregated neighbor vector (Equations 6-9).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Multi-relational knowledge graphs (token-level nodes in KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operates on the transformed token-level graph; uses only adjacency-based aggregation (neighbors and relation types) rather than global full attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (KG-to-text for AGENDA and WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: removing local graph attention drops BLEU to 16.44 (from CGE dev 17.25) and removing the entire local encoder yields BLEU 14.43 (Table 4). Local-only encoders generally give better relative performance when diameter is small (i.e., many disconnected components) and capture structure more precisely.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms global-only encoder on graphs where topology/locality matters; combining local with global yields best results overall. Related works (GCN, GGNN) also use local aggregation but authors' relational GAT + GRU showed strong results here.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly encodes graph topology and typed relations; captures neighborhood homophily and precise structural signals; GRU combine helps propagate structural information between layers.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Struggles to capture long-range dependencies across distant/unconnected nodes unless many layers are stacked (which can propagate noise); relation-type parameterization can increase parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails to capture dependencies across disconnected components or long-range relations without many layers; removing relation-type weights reduces performance (−0.48 BLEU) but saves parameters, indicating a tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8796.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CGE_PGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cascaded and Parallel Graph Encoding Architectures (CGE / PGE / layer-wise variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures that combine global and local encoders either in parallel (PGE) or cascaded (CGE), with layer-wise variants allowing finer-grained interactions; CGE (cascaded) typically gives best trade-off between performance and parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>combined global+local encoding (parallel, cascaded, layer-wise parallel/cascaded)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Parallel: feed initial node embeddings to both global and local encoders and concatenate outputs; Cascaded: global encoder output is fed as input to local encoder (global→local refinement); Layer-wise versions apply the same idea per-layer to allow finer interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Multi-relational KGs (AGENDA, WebNLG token-level graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate on the prepared token-level graph (and optionally Levi graphs); combination modes differ only in how global/local encoders are composed (simultaneous concatenation vs sequential refinement vs per-layer interleaving).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (KG-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On AGENDA test: CGE (cascaded fully) BLEU 17.81 ± 0.15, METEOR 21.75 ± 0.55, CHRF++ 46.76 ± 0.12, #params 66.9M (Table 2). PGE variants and layer-wise variants reported: PGE 17.17 BLEU, PGE-LW 17.40 BLEU, CGE-LW 17.44 BLEU. On WebNLG seen: CGE-LG (CGE with Levi Graph) BLEU 63.10 ± 0.13 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Cascaded (CGE) outperforms parallel (PGE) considering parameter efficiency; layer-wise approaches handle long graph diameters better but are overall slightly worse due to dataset distribution. CGE outperforms previous state-of-the-art (e.g., Koncel-Kedziorski et al. 2019) by substantial BLEU margins.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Combines benefits of global and local contexts: global captures long-range dependencies and document planning, local captures topology and typed relations; cascaded architecture allows local encoder to refine global features and achieves strong results with fewer parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Layer-wise variants can be less parameter-efficient and may underperform overall when dataset contains few large-diameter graphs; parallel variants sometimes underperform cascaded ones with same parameter budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>PGE performs relatively worse with many triples (KGs with more information); layer-wise models are only advantageous for graphs with large diameters but most dataset graphs are small diameter, reducing their overall benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8796.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>relations-as-params</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-as-parameter Encoding (CGE-RP) / Basis Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encode relation types as learned parameter matrices (weights) instead of explicit nodes; when many relations exist, use basis function decomposition to reduce parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>relation-as-parameter (with basis decomposition regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Local encoder uses relation-specific projection matrices W_r (one per relation type) to encode typed edges; for datasets with many relations the model applies basis function decomposition (as in Schlichtkrull et al., 2018) to regularize and avoid parameter explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Multi-relational knowledge graphs (WebNLG with many relation types)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use adjacency-based aggregation where each edge uses its relation-specific projection; to avoid large |R|×param costs, decompose relation matrices into linear combinations of a small set of basis matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (WebNLG and AGENDA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CGE-RP on WebNLG test (seen): BLEU 62.30 ± 0.27, METEOR 43.51 ± 0.18, CHRF++ 75.49 ± 0.34, #params 13.9M (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>CGE-RP outperforms several strong baselines including Adapt and other graph encoders; CGE-LG (Levi) slightly beats CGE-RP in BLEU (63.10 vs 62.30) while using comparable/fewer params.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures typed relations directly with relation-specific parameters and can be regularized with basis decomposition to manage parameter counts; performs strongly on WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>If relations are numerous, naive relation-parameterization leads to parameter explosion; even with basis decomposition there is a modeling/parameter tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When relation parameter matrices are removed (ablated), performance drops slightly (ablation in local encoder showed relation-type removal reduces BLEU by 0.48 but reduces ~10M parameters), indicating sensitivity to relation modeling vs model size tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8796.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8796.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>bpe_shared_vocab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subword (BPE) Nodes and Shared Vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Split entity words into subword units (BPE) and share vocabulary between source node tokens and target tokens, enabling direct token reuse and reducing OOVs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>BPE subword tokenization + shared source-target vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply byte-pair encoding (BPE) to split entity tokens into subword units so some graph nodes represent subwords; share the same vocabulary between input node tokens and output target tokens to facilitate copying and reduce OOVs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Token-level KGs where node labels are word/subword tokens</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize input entity strings into BPE subword tokens; create nodes for subwords where applicable; use a single shared embedding/vocabulary for both encoder node tokens and decoder target tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (reducing OOVs and enabling copy-like behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: removing shared vocabulary reduces BLEU from 17.25 to 15.52 in the CGE dev ablation (Table 4), indicating substantial importance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with word-level nodes, subword token nodes and shared vocabulary improved generation quality (ablation evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Helps with rare words and morphological variants; allows generated text to reuse source tokens directly; empirically important for multi-sentence KG-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases number of nodes when entities are multiword and tokenized into subwords; requires position embeddings to preserve entity ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If shared vocabulary is removed, performance degrades notably; still leaves gap in generated length relative to references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural amr: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Modeling graph structure in transformer for better AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures <em>(Rating: 1)</em></li>
                <li>Gated graph sequence neural networks <em>(Rating: 1)</em></li>
                <li>Modeling relational data with graph convolutional networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8796",
    "paper_id": "paper-210942740",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "linearization",
            "name_full": "Linearized Triple Sequence",
            "brief_description": "A representation that serializes KG triples into a flat token sequence (triples / entities / relations) used as input to sequence models; used here as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearization (triples serialized)",
            "representation_description": "Convert each knowledge-graph triple (subject, relation, object) into a linear token sequence (e.g., subject relation object ...), concatenate triples into a single sequence, and feed the resulting token sequence into a sequence encoder (Transformer baseline in this paper).",
            "graph_type": "Knowledge graphs / RDF triples",
            "conversion_method": "Triples are linearized into a sequence (canonical triple serialization). The paper uses this linearized triple set as the baseline input to a standard Transformer.",
            "downstream_task": "Graph-to-text generation (verbalisation of KGs) — AGENDA (scientific abstracts) and WebNLG (DBpedia verbalisation).",
            "performance_metrics": "Transformer baseline (linearized triples) on AGENDA test: BLEU 14.11 ± 0.28, METEOR 19.35 ± 0.52, CHRF++ 41.95 ± 0.39 (Table 2).",
            "comparison_to_others": "Underperforms the proposed graph-aware encoders (CGE, PGE variants). The paper reports CGE BLEU 17.81 on AGENDA, a substantial improvement over linearized baseline (≈ +3.7 BLEU).",
            "advantages": "Simple to implement and directly compatible with off-the-shelf sequence models (Transformers).",
            "disadvantages": "Ignores explicit graph topology; cannot easily encode typed relations or adjacency structure; inferior empirical performance on KG-to-text in this paper.",
            "failure_cases": "Performs worse when structural information and long/short-range relations in KG are important for generation (shown by lower BLEU/CHRF++ compared to graph encoders).",
            "uuid": "e8796.0",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "token-node-transform",
            "name_full": "Token-level Graph Transformation (entity tokens as nodes)",
            "brief_description": "Transform each multi-token entity into multiple token-nodes so the graph operates at token-level rather than entity-level, with position embeddings to preserve intra-entity order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "token-level node representation",
            "representation_description": "Each entity in the original KG is converted to a set of nodes, one per token in the entity surface form; edges between entities are expanded to connect every token node of the subject to every token node of the object; node initial embeddings are token embeddings plus position embeddings to preserve local order.",
            "graph_type": "Knowledge graphs (multi-word entities; KGs from AGENDA and WebNLG)",
            "conversion_method": "For each edge (e_h, r, e_t) in the original entity-level graph, create edges (u, r, v) between every token node u in e_h and every token node v in e_t; represent each token node with token embedding + positional embedding.",
            "downstream_task": "Graph-to-text generation (KG-to-text for AGENDA and WebNLG).",
            "performance_metrics": "Used by the proposed models (CGE/PGE variants). Specific ablation indicates vocabulary sharing and token-level handling are important; removing shared vocab reduces BLEU from 17.25 to 15.52 on the dev ablation (Table 4).",
            "comparison_to_others": "Compared to entity-level representations, token-level nodes increase representational flexibility and allow attention between source and target tokens; but increase graph size.",
            "advantages": "Richer, more flexible modeling of multi-word entities; enables subword nodes (BPE) and shared vocabularies so node tokens can be directly used during decoding.",
            "disadvantages": "Removes natural sequential order of multi-word expressions (mitigated by adding positional embeddings); increases number of nodes (computational cost).",
            "failure_cases": "Larger graphs and many token-nodes increase model size and computation; requires position embeddings to recover ordering or performance degrades.",
            "uuid": "e8796.1",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "levi",
            "name_full": "Levi Transformation (relations-as-nodes)",
            "brief_description": "Represent each relation as an explicit node connected to its subject and object, turning edge labels into nodes to integrate relations into the node vocabulary.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Levi graph (relations as nodes)",
            "representation_description": "Transform each labeled edge (subject, relation, object) into a relation node; connect the relation node to the subject and object token nodes via binary (untyped) edges, enabling the model to treat relations as first-class nodes with embeddings.",
            "graph_type": "Knowledge graphs / RDF graphs",
            "conversion_method": "Create a new node for each relation instance and add binary edges from that relation node to subject and object token nodes (Beck et al. 2018 style Levi graph).",
            "downstream_task": "Graph-to-text generation (WebNLG was evaluated with Levi graphs in experiments).",
            "performance_metrics": "CGE-LG (CGE with Levi Graph input) on WebNLG test (seen): BLEU 63.10 ± 0.13, METEOR 44.11 ± 0.09, CHRF++ 76.33 ± 0.10 (Table 3). The paper reports CGE-LG achieves the best performance and uses fewer parameters than some alternatives.",
            "comparison_to_others": "CGE-LG (Levi) achieved strong results and lower parameter count compared to relation-as-parameter variants; authors note Levi approach allows model to handle new relations since relations become part of shared vocabulary.",
            "advantages": "Allows relations to be treated as nodes (part of vocabulary), enabling reuse in decoding and handling unseen/new relations; can reduce parameterization when relations are many.",
            "disadvantages": "Introduces extra nodes (increases graph size) and changes graph topology; relations become untyped binary edges which may lose some relation-type inductive biases unless modeled separately.",
            "failure_cases": "Not explicitly reported, but increased graph size may impact computational cost; trade-offs versus encoding relations as typed parameters depend on dataset and parameter budget.",
            "uuid": "e8796.2",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "global_encoder",
            "name_full": "Global Graph Encoder (fully-connected self-attention)",
            "brief_description": "A Transformer-style global encoder that treats the input graph as fully connected and computes attention across all node pairs to capture long-range dependencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "global (fully-connected) self-attention node encoding",
            "representation_description": "Each node's global neighborhood representation is computed as attention-weighted sum over all nodes in the graph (multi-head), using learned query/key projections; COMBINE follows Transformer-style LayerNorm + FFN residual blocks. This effectively creates an artificial complete graph enabling direct information flow between any node pair.",
            "graph_type": "Knowledge graphs (tokenized KGs); applicable to any node-labeled graph",
            "conversion_method": "No explicit conversion; the adjacency is ignored — attention is computed over all nodes V (Equations 1-5), with attention weights α_vu computed from projected queries and keys.",
            "downstream_task": "Graph-to-text generation (KG-to-text for AGENDA and WebNLG) as part of encoder in PGE/CGE architectures.",
            "performance_metrics": "Included in combined models. Ablation: removing global attention drops BLEU by 1.77 on AGENDA dev (CGE ablation: full CGE = 17.25 BLEU dev; -Global Attention = 15.48 BLEU; -Global Encoder = 14.96 BLEU) (Table 4). Alone, the global-only encoder has relatively worse scores in some dataset partitions (global encoder 'has the worst scores' on AGENDA dev w.r.t. graph diameter; see analysis).",
            "comparison_to_others": "Compared with local GNNs, global encoder captures long-range complex dependencies but neglects explicit graph topology and typed relations; combining with local encoder yields superior performance.",
            "advantages": "Directly captures long-range and cross-component dependencies; helps document planning and bridging disconnected components of the KG; empirically contributes significant gains (ablation shows BLEU drop when removed).",
            "disadvantages": "Neglects explicit graph topology and typed relations; creates O(n^2) attention computations (quadratic complexity) and would explode further if typed relations were included; can be less precise for local structural propagation.",
            "failure_cases": "Performs poorly when structural/topological locality is important (e.g., graphs with diameter=1 and many disconnected components); global-only models can be outperformed by local or combined encoders on such inputs.",
            "uuid": "e8796.3",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "local_encoder",
            "name_full": "Local Graph Encoder (relational Graph Attention Network)",
            "brief_description": "A modified Graph Attention Network that aggregates information only from adjacent nodes and encodes typed relations via relation-specific weight matrices and attention; COMBINE uses a GRU to propagate information across layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "local (relational) GAT with GRU combine",
            "representation_description": "For each node, aggregate over adjacent neighbors N(v) with attention coefficients computed using relation-specific projections W_r and a learned vector a; multi-head local attentions produce neighborhood summary, and final combine step uses a GRU over previous node state and the aggregated neighbor vector (Equations 6-9).",
            "graph_type": "Multi-relational knowledge graphs (token-level nodes in KGs)",
            "conversion_method": "Operates on the transformed token-level graph; uses only adjacency-based aggregation (neighbors and relation types) rather than global full attention.",
            "downstream_task": "Graph-to-text generation (KG-to-text for AGENDA and WebNLG).",
            "performance_metrics": "Ablation: removing local graph attention drops BLEU to 16.44 (from CGE dev 17.25) and removing the entire local encoder yields BLEU 14.43 (Table 4). Local-only encoders generally give better relative performance when diameter is small (i.e., many disconnected components) and capture structure more precisely.",
            "comparison_to_others": "Outperforms global-only encoder on graphs where topology/locality matters; combining local with global yields best results overall. Related works (GCN, GGNN) also use local aggregation but authors' relational GAT + GRU showed strong results here.",
            "advantages": "Explicitly encodes graph topology and typed relations; captures neighborhood homophily and precise structural signals; GRU combine helps propagate structural information between layers.",
            "disadvantages": "Struggles to capture long-range dependencies across distant/unconnected nodes unless many layers are stacked (which can propagate noise); relation-type parameterization can increase parameter count.",
            "failure_cases": "Fails to capture dependencies across disconnected components or long-range relations without many layers; removing relation-type weights reduces performance (−0.48 BLEU) but saves parameters, indicating a tradeoff.",
            "uuid": "e8796.4",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "CGE_PGE",
            "name_full": "Cascaded and Parallel Graph Encoding Architectures (CGE / PGE / layer-wise variants)",
            "brief_description": "Architectures that combine global and local encoders either in parallel (PGE) or cascaded (CGE), with layer-wise variants allowing finer-grained interactions; CGE (cascaded) typically gives best trade-off between performance and parameter count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "combined global+local encoding (parallel, cascaded, layer-wise parallel/cascaded)",
            "representation_description": "Parallel: feed initial node embeddings to both global and local encoders and concatenate outputs; Cascaded: global encoder output is fed as input to local encoder (global→local refinement); Layer-wise versions apply the same idea per-layer to allow finer interactions.",
            "graph_type": "Multi-relational KGs (AGENDA, WebNLG token-level graphs)",
            "conversion_method": "Operate on the prepared token-level graph (and optionally Levi graphs); combination modes differ only in how global/local encoders are composed (simultaneous concatenation vs sequential refinement vs per-layer interleaving).",
            "downstream_task": "Graph-to-text generation (KG-to-text).",
            "performance_metrics": "On AGENDA test: CGE (cascaded fully) BLEU 17.81 ± 0.15, METEOR 21.75 ± 0.55, CHRF++ 46.76 ± 0.12, #params 66.9M (Table 2). PGE variants and layer-wise variants reported: PGE 17.17 BLEU, PGE-LW 17.40 BLEU, CGE-LW 17.44 BLEU. On WebNLG seen: CGE-LG (CGE with Levi Graph) BLEU 63.10 ± 0.13 (Table 3).",
            "comparison_to_others": "Cascaded (CGE) outperforms parallel (PGE) considering parameter efficiency; layer-wise approaches handle long graph diameters better but are overall slightly worse due to dataset distribution. CGE outperforms previous state-of-the-art (e.g., Koncel-Kedziorski et al. 2019) by substantial BLEU margins.",
            "advantages": "Combines benefits of global and local contexts: global captures long-range dependencies and document planning, local captures topology and typed relations; cascaded architecture allows local encoder to refine global features and achieves strong results with fewer parameters.",
            "disadvantages": "Layer-wise variants can be less parameter-efficient and may underperform overall when dataset contains few large-diameter graphs; parallel variants sometimes underperform cascaded ones with same parameter budgets.",
            "failure_cases": "PGE performs relatively worse with many triples (KGs with more information); layer-wise models are only advantageous for graphs with large diameters but most dataset graphs are small diameter, reducing their overall benefit.",
            "uuid": "e8796.5",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "relations-as-params",
            "name_full": "Relation-as-parameter Encoding (CGE-RP) / Basis Decomposition",
            "brief_description": "Encode relation types as learned parameter matrices (weights) instead of explicit nodes; when many relations exist, use basis function decomposition to reduce parameter count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "relation-as-parameter (with basis decomposition regularization)",
            "representation_description": "Local encoder uses relation-specific projection matrices W_r (one per relation type) to encode typed edges; for datasets with many relations the model applies basis function decomposition (as in Schlichtkrull et al., 2018) to regularize and avoid parameter explosion.",
            "graph_type": "Multi-relational knowledge graphs (WebNLG with many relation types)",
            "conversion_method": "Use adjacency-based aggregation where each edge uses its relation-specific projection; to avoid large |R|×param costs, decompose relation matrices into linear combinations of a small set of basis matrices.",
            "downstream_task": "Graph-to-text generation (WebNLG and AGENDA).",
            "performance_metrics": "CGE-RP on WebNLG test (seen): BLEU 62.30 ± 0.27, METEOR 43.51 ± 0.18, CHRF++ 75.49 ± 0.34, #params 13.9M (Table 3).",
            "comparison_to_others": "CGE-RP outperforms several strong baselines including Adapt and other graph encoders; CGE-LG (Levi) slightly beats CGE-RP in BLEU (63.10 vs 62.30) while using comparable/fewer params.",
            "advantages": "Captures typed relations directly with relation-specific parameters and can be regularized with basis decomposition to manage parameter counts; performs strongly on WebNLG.",
            "disadvantages": "If relations are numerous, naive relation-parameterization leads to parameter explosion; even with basis decomposition there is a modeling/parameter tradeoff.",
            "failure_cases": "When relation parameter matrices are removed (ablated), performance drops slightly (ablation in local encoder showed relation-type removal reduces BLEU by 0.48 but reduces ~10M parameters), indicating sensitivity to relation modeling vs model size tradeoffs.",
            "uuid": "e8796.6",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "bpe_shared_vocab",
            "name_full": "Subword (BPE) Nodes and Shared Vocabulary",
            "brief_description": "Split entity words into subword units (BPE) and share vocabulary between source node tokens and target tokens, enabling direct token reuse and reducing OOVs.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "BPE subword tokenization + shared source-target vocabulary",
            "representation_description": "Apply byte-pair encoding (BPE) to split entity tokens into subword units so some graph nodes represent subwords; share the same vocabulary between input node tokens and output target tokens to facilitate copying and reduce OOVs.",
            "graph_type": "Token-level KGs where node labels are word/subword tokens",
            "conversion_method": "Tokenize input entity strings into BPE subword tokens; create nodes for subwords where applicable; use a single shared embedding/vocabulary for both encoder node tokens and decoder target tokens.",
            "downstream_task": "Graph-to-text generation (reducing OOVs and enabling copy-like behavior).",
            "performance_metrics": "Ablation: removing shared vocabulary reduces BLEU from 17.25 to 15.52 in the CGE dev ablation (Table 4), indicating substantial importance.",
            "comparison_to_others": "Compared with word-level nodes, subword token nodes and shared vocabulary improved generation quality (ablation evidence).",
            "advantages": "Helps with rare words and morphological variants; allows generated text to reuse source tokens directly; empirically important for multi-sentence KG-to-text.",
            "disadvantages": "Increases number of nodes when entities are multiword and tokenized into subwords; requires position embeddings to preserve entity ordering.",
            "failure_cases": "If shared vocabulary is removed, performance degrades notably; still leaves gap in generated length relative to references.",
            "uuid": "e8796.7",
            "source_info": {
                "paper_title": "Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_in_transformer_for_better_amrtotext_generation"
        },
        {
            "paper_title": "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
            "rating": 1,
            "sanitized_title": "neural_datatotext_generation_a_comparison_between_pipeline_and_endtoend_architectures"
        },
        {
            "paper_title": "Gated graph sequence neural networks",
            "rating": 1,
            "sanitized_title": "gated_graph_sequence_neural_networks"
        },
        {
            "paper_title": "Modeling relational data with graph convolutional networks",
            "rating": 1,
            "sanitized_title": "modeling_relational_data_with_graph_convolutional_networks"
        }
    ],
    "cost": 0.01575825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs
29 Jan 2020</p>
<p>Leonardo F R Ribeiro ribeiro@aiphes.tu-darmstadt.de 
Research Training Group AIPHES and UKP Lab
Technische Universität Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Yue Zhang yue.zhang@wias.org.cn 
Claire Gardent claire.gardent@loria.fr 
CNRS/LORIA
NancyFrance</p>
<p>Iryna Gurevych 
Research Training Group AIPHES and UKP Lab
Technische Universität Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs
29 Jan 20203177C4DEAE117A60EC4DFC69E0703322arXiv:2001.11003v1[cs.CL]
Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations.Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are connected.In contrast, local node encoding considers the relations between directly connected nodes capturing the graph structure, but it can fail to capture long-range relations.In this work, we gather the best of both encoding strategies, proposing novel models that encode an input graph combining both global and local node contexts.Our approaches are able to learn better contextualized node embeddings for text generation.In our experiments, we demonstrate that our models lead to significant improvements in KGto-text generation, achieving BLEU scores of 17.81 on AGENDA dataset, and 63.10 on the WebNLG dataset for seen categories, outperforming the state of the art by 3.51 and 2.51 points, respectively 1 .</p>
<p>Introduction</p>
<p>Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017), sub-graphs from knowledge graphs (KG) (Koncel-Kedziorski et al., 2019) or other forms of structured data (Konstas and Lapata, 2013).While many recent works (Song et al., 2018;Damonte and Cohen, 2019;Ribeiro et al., 2019;Guo et al., 2019) focus on generating sentence-level outputs, a more challenging and interesting scenario emerges when the goal is to generate bigger multi-sentence text, such as a document or paragraph.In this context, the input graphs are much more diverse, representing 1 Code is available at https://github.com/UKPLab/kg2textknowledge from different domains and in different ways.The task is thus more demanding since it can be necessary to select relevant parts of graph for generating a concise text, and to handle document planning issues such as order, coherence and discourse markers (Gardent et al., 2017).</p>
<p>A key issue in neural graph-to-text generation is how to encode graphs.The basic idea is to incrementally calculate node representations by aggregating context information.To this end, two main approaches have been proposed: (i) models based on local node aggregation, usually based on Graph Neural Networks (GNN) (Ribeiro et al., 2019;Guo et al., 2019) and (ii) models that leverage global node aggregation.Systems based on the global encoding strategy are typically based on Transformer architectures (Zhu et al., 2019;Cai and Lam, 2020), using self-attention to compute a node representation based on all nodes in the graph.This approach enjoys the advantage of large context range, but neglects the graph topology by effectively treating every node as being connected to all the others in the graph.In contrast, models based on local aggregation learn the representation of each node based on its adjacent nodes as defined in the input.This method effectively exploits the graph structure.However, encoding relations between distant nodes can be challenging by requiring more graph encoding layers, which can also propagate noise (Li et al., 2018).</p>
<p>For example, Figure 1a presents a KG, for which a corresponding text is shown in Figure 1b.The nodes GNN and DistMulti have relations with the nodes node embeddings and link prediction, respectively.Both relations are important for GNN and DistMulti during the text generation phase, but are in different connected components.As shown in Figure 1c, a global encoder can learn a node representation for DistMulti which captures information from indirectly connected entities such as node embeddings.Encoding such dependencies is important for KG verbalisation as KGs are known to be highly incomplete, often missing links between entities (Schlichtkrull et al., 2018).In addition, the global encoding can capture long-range complex dependencies between entities, supporting document planning.In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: two entities belonging to the same topic in a KG are much more likely to be connected than at random.Consequently, the local context enriches the node representation with topic-related information from KG triples.For example, in Figure 1a, GAT reaches node embeddings through the GNN.This transitive relation can be captured by a local encoder, as shown in Figure 1d.Capturing this form of relationship also can support text generation at the sentence level.</p>
<p>In this paper, we investigate novel graph-totext architectures that combine both global and lo-cal node aggregations, gathering the benefits from both strategies.In particular, we propose a unified graph-to-text framework based on Graph Attention Networks (GAT, Veličković et al., 2018).As part of this framework, we empirically compare two main architectures: a cascaded architecture that performs global node aggregation before performing local node aggregation, and a parallel architecture that performs global and local aggregation simultaneously, before concatenating the representations.While the cascaded architecture allows the local encoder to leverage global encoding features, the parallel architecture allows more independent features to compliment each other.To further consider fine-grained integration, we additionally consider layer-wise integration of global and local encoders.</p>
<p>Extensive experiments show that our approaches consistently outperform recent models on two benchmarks for text generation from KGs, giving the best reported results so far.Compared with parallel structures, cascaded structures give better performance with smaller numbers of parameters.To the best of our knowledge, we are the first to consider integrating global and local context aggregation in graph-to-text generation, and the first to propose a unified GAT structure for integrating global and local aggregation.</p>
<p>Related Work</p>
<p>Early efforts for graph-to-text generation employ statistical methods (Flanigan et al., 2016;Pourdamghani et al., 2016;Song et al., 2017).Recently, several neural graph-to-text models have exhibited success by levering different encoder mechanisms based on GNN and Transformer architectures, learning effective latent graph representations.</p>
<p>AMR-to-Text Generation Recent neural models have been applied to sentence-level generation from Abstract Meaning Representation (AMR) graphs.Konstas et al. (2017) provide the first neural approach for this task, by linearising the input graph as a sequence of nodes and edges.Song et al. (2018) propose the graph recurrent network (GRN) to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on GGNNs (Li et al., 2016).However, both approaches only employ local node aggregation strategies.Damonte and Cohen (2019) and Ribeiro et al. (2019) develop models employing GNNs and LSTMs, in order to learn complementary node contexts.Recent methods (Zhu et al., 2019;Cai and Lam, 2020) 2019) introduce a systematic comparison between pipeline and neural end-to-end approaches for text generation from RDF graphs.Nevertheless, those approaches consider the triples as separated structures, not explicitly considering the graph topology.To explicitly encode the graph structure, Marcheggiani and Perez Beltrachini (2018) propose an encoder based on graph convolutional networks (GCN) and show superior performance compared to LSTMs.Our work is related to Koncel-Kedziorski et al. (2019) who propose a transformer-based approach that only focuses on the relations between directly connected nodes.However, our models focus on both global and local node relations, capturing complementary graph contexts.</p>
<p>Graph-to-Text Model</p>
<p>In this section, we describe (i) the general concept of GNNs; (ii) the proposed local and global graph encoders; (iii) the graph transformation adopted to create a relational graph from the input; and (iv) the various combined global and local graph architectures.</p>
<p>Graph Neural Networks (GNN)</p>
<p>Formally, let G = (V, E, R) denote a multirelational graph2 with nodes u, v ∈ V and labelled edges (u, r, v) ∈ E, where r ∈ R represents the relation between u and v. GNNs work by iteratively learning a representation vector h v of a node v ∈ V based on both its context node neighbors and edge features, through an information propagation scheme.More formally, the l-th layer aggregates the representations of v's context nodes:
h (l) N (v) = AGGR (l) h (l−1) u , r vu : u ∈ N (v) ,
where AGGR (l) (.) is an aggregation function, shared by all nodes on the l-th layer.r vu represents the relation between v and u.N (v) is a set of context nodes for v.In most GNNs, the context nodes are those adjacent to v.</p>
<p>The aggregated context representation is used to update the representation of v:
h (l) v = COMBINE (l) h (l−1) v , h(l)
N (v) .</p>
<p>After l iterations, a node's representation encodes the structural information within its lhop neighborhood.The choices of AGGR (l) (.) and COMBINE (l) (.) differ by the specific GNN model.An example of AGGR (l) (.) is the sum of the representations of N (v).An example of COMBINE (l) (.) is a concatenation after the feature transformation.</p>
<p>Global Graph Encoder</p>
<p>A global graph encoder aggregates a global context for updating each node, by treating the graph as fully connected (see Figure 1c).We use the attention mechanism as the message passing scheme, extending the self-attention network structure of Transformer (Vaswani et al., 2017) to a GAT structure.In particular, we compute a layer of the global convolution for a node v ∈ V, which takes the input feature representations h v as input, adopting AGGR (l) (.) as:
h N (v) = u∈V α vu W g h u ,(1)
where W g ∈ R dv×dz is a model parameter.The attention weight α vu is calculated as:
α vu = exp(e vu ) k∈V exp(e vk ) ,(2)
where,
e vu = W q h v W k h u /d z (3)
is the attention function which measures the global importance of node u's features to node v. W q , W k ∈ R dv×dz are model parameters and d z is a scaling factor.</p>
<p>Multi-head Attention.To capture distinct relations between nodes, K different global convolutions are calculated and concatenated:
hN (v) = K k=1 h (k) N (v) .(4)
Finally, we define COMBINE (l) (.) employing layer normalization (LayerNorm) and a fully connected feed-forward network (FFN), in a similar way as the transformer architecture:
ĥv = hN (v) + h v , hv = LayerNorm( ĥv ) , h global v = FFN( hv ) + ĥv .
(5) This strategy creates an artificial complete graph with O(n 2 ) edges.Note that the global encoder do not consider the edge relations between nodes.In particular, if the labelled edges were considered, the self-attention space complexity would increases to Θ(|R| n 2 ).</p>
<p>Local Graph Encoder</p>
<p>The representation h global v captures macro relationships from v to all other nodes in the graph.However, this representation lacks both structural information regarding the local neighborhood of v and the graph topology.Also, it does not capture typed relations between nodes (see Equations 1 and 3).In order to capture those crucial graph information and impose a strong relational inductive bias, we build a local graph encoder by employing a modified version of GAT augmented with relational weights.In particular, we compute a layer of the local convolution for a node v ∈ V, adopting AGGR (l) (.) as:
h N (v) = (u,r) ∈ Ñ (v) α vu W r h u ,(6)
where
W r ∈ R dv×dz encodes the relation r ∈ R between v and u. Ñ (v) = N (v) ∪ v and N (v) is a set of nodes adjacent to v.
The attention coefficient α vu is computed as:
α vu = exp(e vu ) (k,r) ∈ Ñ (v) exp(e vk ) ,(7)
where,
e vu = σ a [W r h v W r h u ] (8)
is the attention function which calculates the relative importance of adjacent nodes, considering typed relations.σ is an activation function, denotes concatenation and a is a model parameter.</p>
<p>We employ multi-head attentions to learn local relations in different perspectives, as in Equation 4, generating ȟN (v) .Finally, we define COMBINE (l) (.) as:
h local v = RNN(h v , ȟN (v) ) ,(9)
where we employ as RNN a Gated Recurrent Unit (GRU) (Cho et al., 2014).GRU facilitates information propagation between local layers.This choice is motivated by recent works (Xu et al., 2018;Dehmamy et al., 2019) that theoretically demonstrate that sharing information between layers helps the structural signals propagate.In a similar direction, AMR-to-text generation models employ LSTMs (Song et al., 2017) and dense connections (Guo et al., 2019) between GNN layers.</p>
<p>Graph Preparation</p>
<p>We represent a KG as a multi-relational graph G e = (V e , E e , R) with entity nodes e ∈ V e and labeled edges (e h , r, e t ) ∈ E e , where r ∈ R denotes the relation existing from the entity e h to e t . 3nlike other current approaches (Koncel-Kedziorski et al., 2019;Moryossef et al., 2019), we represent an entity as a set of nodes.Formally, we transform each G e into a new graph G = (V, E, R), where each token of an entity e ∈ V e becomes a node v ∈ V. We convert each edge (e h , r, e t ) ∈ E e into a set of edges (with the same relation r) and connect every token of e h to every token of e t .That is, an edge (u, r, v) will belong to E if and only if there exists an edge (e h , r, e t ) ∈ E e such that u ∈ e h and v ∈ e t .We represent each node v ∈ V with an embedding h 0 v ∈ R dv , generated from its corresponding token.</p>
<p>The new graph G increases the representational power of the model because it allows learning node embeddings at a token level, instead of entity level.This is particularly important for text generation as it permits the model to be more flexible, capturing richer relationships between entity tokens.This also allows the model to learn relations and attention functions between source and target tokens.However, it has the side effect of removing the natural sequential order of multi-word expressions such as entities.To preserve this information, we employ position embeddings (Vaswani et al., 2017), i.e., h 0 v becomes the sum of the corresponding token embedding and the positional embedding for v.</p>
<p>Combining Global and Local Encodings</p>
<p>Our goal is to implement a graph encoder capable of encoding global and local aspects of the input graph.We hypothesize that the two sources of information are complementary and a combination of both enriches node representations for text generation.In order to test this hypothesis, we investigate four possible combination architectures.Figure 2 presents our proposed encoders.</p>
<p>Parallel Graph Encoding.In this setup, we compose global and local graph encoders in a fully parallel structure (Figure 2a).Note that each graph encoder can have different numbers of layers and attention heads.h 0 v is the initial input for the first layer of both encoders.The final node representation is the concatenation of the local and global node representations:
h v = [ h global v h local v ] .
Cascaded Graph Encoding.We cascade local and global graph encoders as shown in Figure 2b, by first computing a global-contextual node embedding, and then refining it with the local context.h 0 v is the initial input for the global encoder and h global v is the initial input for the local encoder.</p>
<p>Layer-wise Parallel and Cascaded Graph Encoding.To allow fine-grained interaction between the two types of contextual information, we also combine the encoders in a layer-wise fashion.In particular, for each graph layer, we employ both the local and global encoders in a parallel structure as shown in Figure 2c.We also experiment cascading the graph encoders layer-wise (Figure 2d).</p>
<p>Decoder</p>
<p>Our decoder follows the same core architecture of the Transformer decoder.Each time step t is updated by interleaving multiple rounds of multihead attention over the output of the encoder (node embeddings h v ) and attention over previouslygenerated tokens (token embeddings).An additional challenge in our setup is to generate multisentence outputs.In order to encourage the model to generate longer texts, we employ a length penalty (Wu et al., 2016) to refine the pure maxprobability beam search.</p>
<p>Data and Preprocessing</p>
<p>We attest the effectiveness of our models on two datasets: AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017).Table 1 shows the statistics for both datasets.</p>
<p>AGENDA.In this dataset, KGs are paired with scientific abstracts extracted from proceedings of 12 top AI conferences.Each instance consists of the paper title, a KG and the paper abstract.Entities correspond to scientific terms which are often multi-word expressions (co-referential entities are merged).We treat each token in the title as a node, creating a unique graph with title and KG tokens as nodes.As shown in Table 1, the average output length is considerably large, as the target output are multi-sentence abstracts.</p>
<p>WebNLG.In this dataset, each instance contains a graph extracted from DBPedia.The target text consists of one or more sentences that verbalise the graph.We evaluate the models on the test set with seen categories.Note that this dataset has a considerable number of edge relations (see Table 1).In order to avoid parameter explosion, we use regularization based on basis function decomposition to define the model relation weights (Schlichtkrull et al., 2018).Also, as an alternative, we employ the Levi Transformation to create nodes from relational edges between entities (Beck et al., 2018).That is, we create a new relation node for each edge relation between two nodes.The new relation node is connected to the subject and object token entities by two binary relations, respectively.</p>
<p>Experiments and Discussion</p>
<p>The models are trained for 30 epochs with early stopping based on the development BLEU score.We use Adam optimization with initial learning rate of 0.5.The vocabulary is shared between the node and target tokens.In order to mitigate the effects of random seeds, for the test sets, we report the averages for 4 training runs along with their standard deviation.Hyperparameters are tuned on the development set of both datasets.Following previous work (Castro Ferreira et al., 2019), we employ byte pair encoding (BPE) to split entity words into smaller more frequent pieces.So some nodes in the graph can be sub-words.We also obtain sub-words on the target side.We call our models PGE-LW (layer-wise parallel encoder), CGE-LW (layer-wise cascaded encoder), and PGE (fully parallel encoder) and CGE (fully cascaded encoder).We use a standard version of the Transformer as baseline and a linearized version of the triples of the KG is used as input.Following previous works, we evaluate the results in terms of BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and sentence-level CHRF++ (Popović, 2015) scores.To better attest the quality of the generated texts, we also perform a human evaluation.</p>
<p>Results on AGENDA</p>
<p>Results on WebNLG</p>
<p>We compare the performance of our best model (CGE) with six state-of-the-art results of graphto-text models reported for this dataset (Gardent et al., 2017;Trisedya et al., 2018;Marcheggiani and Perez Beltrachini, 2018;Castro Ferreira et al 2018), an approach that encodes both intra-triple and inter-triple relationships, by 4.5 BLEU points.Interestingly, their intra-triple and inter-triple mechanisms capture relationships within a triple and among triples, approaches closely related with our local and global encodings.However, they rely on encoding sequence of relations and entities based on traversal graph algorithms, whereas we explicitly exploit the graph structure, throughout the local neighborhood aggregation.</p>
<p>Relations as Nodes.CGE-LG uses Levi graphs as inputs and achieves the best performance, even thought it uses less parameters.One advantage of this approach is that it allows the model to handle new relations, as they are treated as nodes.Moreover, the relations become part of the shared vocabulary, making this information directly usable during the decoding process.We outperform an approach based on GNNs (Marcheggiani and Perez Beltrachini, 2018)</p>
<p>Ablation Study</p>
<p>In Table 4, we report an ablation study on the impact of each module used in CGE model on the development set of AGENDA dataset.</p>
<p>Global Graph Encoder.We start by an ablation on the global encoder.After removing the global attention coefficients, the performance of the model drops by 1.77 BLEU and 1.72 CHRF++ scores.Results also show that using FFN in the global COMBINE(.)function is important to the model but less effective than the global attentions.However, when we remove FNN, the number of parameters drops considerably (around 19%) from 66.9 to 54.3 million.Finally, without the entire global encoder, the result drops substantially by 2.29 BLEU points.This indicates that enriching node embeddings with a global context allows learning more expressive graph representations.</p>
<p>Local Graph Encoder.We first remove the local graph attention and the BLEU score drops to 16.44, showing that the neighborhood attention improves the performance.After removing the relation types, encoded as model weights, the performance drops 0.48 BLEU points.However, the number of parameters is reduced around 10 million.This indicates that we can have a more efficient model, in terms of the number of parameters, with a slight drop in performance.Removing the GRU used on the COMBINE(.)function drops the performance considerably.The worse performance occurs if we remove the entire local encoder, with a BLEU score of 14.43, essentially making the encoder similar to the baseline.Finally, we note that the vocabulary sharing is critical to improve the performance, and the length penalty is beneficial as we generate multi-sentence outputs.</p>
<p>Comparing Encoding Strategies</p>
<p>The overall performance on both datasets suggests the superiority of combining global and local node representations.However, to have a better understanding of the positive and negative aspects of each proposed model, we introduce a systematic comparison between the encoding strategies.</p>
<p>Figure 3a shows the impact of graph diameter in the four encoding methods.The models perform on par for graphs with smaller diameters.Models based on layer-wise aggregations (PGE-LW and CGE-LW) have better performance when handling larger graph diameters.However, their overall per- formance is worse compared to the fully independent models because only 2% of the graphs on the AGENDA dev set have a diameter larger than or equal to 5.This indicates that the layer-wise encoders can better capture long-distance node dependencies.Moreover, the margin between PGE-LW and CGE-LW increases as the diameters increase, suggesting that PGE-LW can be a good option to encode graphs with larger diameter.Figure 3b shows the models' performance with respect to the number of triples.CGE achieves better results when the number of triples is large (≥ 9).On the other hand, the PGE has relatively worse when handling more information, that is, KGs with more triples.</p>
<p>Impact of the Graph Structure and Output Length</p>
<p>We investigate the performance of our best model (CGE) concerning different data properties.</p>
<p>Number of Triples.</p>
<p>In Table 5, we perform an inspection on the effect of the number of triples on the models' performance, measured using CHRF++ scores 4 for the WebNLG dev set.</p>
<p>In general, our model obtains better scores over almost all partitions, showing that capturing explicitly structural information is beneficial for text generation.The performance decreases as the number of triples increase.However, when handling datapoints with more triples (7), Adapt and our model achieve higher performance.We hypothesize that this happens because the models receive a considerable amount of input data, giving   more context to the text generation process, even though the graph structure being more complex.</p>
<p>Number of Nodes.</p>
<p>Figure 4a shows the effect of the graph size, measured in number of nodes, on the performance.Note that the score increases as the graph size increases.This trend is particularly interesting and contrasting to AMR-to-text generation, in which the models' general performance decreases as the graph size increases (Cai and Lam, 2020).In AMR benchmarks, the graph size is correlated with the sentence size, and longer sentences are more challenging to generate than the smaller ones.On the other hand, AGENDA contains similar abstract lengths5 and when the input is a bigger graph, the model has more information to be leveraged during the generation.We also investigate the performance with respect to the number of local graph layers.The performances with 1 and 4 layers are similar, while the best performance, regardless of the number of nodes, is achieved with 3 layers.Graph Diameter.Figure 4b shows the impact of the graph diameter on the performance, when employing only global or local encoding modules or both, for the AGENDA dev set.Similarly to the graph size, the score increases as the diameter increases.As the global encoder is not aware of the graph structure, this module has the worst scores, even though it enables direct node communication over long distance.In contrast, the local encoder can propagate precise node information throughout the graph structure for k-hop distances, making the relative performance better.We also observe that the performance gap between the global and local encoders increases when the diameter is 1.In this case, the graph has many connected components; that is, the triples do not share entities.It reveals that computing node representation based on adjacent nodes, rather than based on the entire set of entities, leads to better performance.Table 5 shows the performances for our best model and others with respect to the graph diameter for WebNLG dev set.In contrast to AGENDA, the score decreases as the diameter increases.This behavior highlights a crucial difference between the two datasets.Whereas in the WebNLG the graph size is correlated with the output size, this is not the case for AGENDA.For WebNLG, higher diameters pose additional challenges to the models as they need to generate larger outputs.</p>
<p>Output Length.One interesting phenomenon to analyze is the length distribution (in number of words) of the generated outputs.We expect that our models generate texts with similar output lengths as the reference texts.However, as shown in Figure 4c, the reference texts usually are bigger than the texts generated by all models.The texts generated by CGE-no-pl, a CGE model without length penalty, are consistently longer than the baseline.Also, note that we increase the length of the texts when we employ the length penalty (see Section 3.6).However, there is still a gap between the reference and the generated text lengths.We leave further investigation of this aspect for future work.</p>
<p>Effect of the Number of Nodes on the Output Length.Figure 5 shows the effect of the size of a graph, defined as the number of nodes, on the quality (measured in CHRF++ scores) and length of the generated text (in number of words) in the AGENDA dev set.We bin both the graph size and the output length in 4 classes.Our model consistently outperforms the baseline, in some cases by a large margin.When handling smaller graphs (with ≤ 35 nodes), both models have difficulties generating good summaries.However, for these smaller graphs, our model achieves a score 12.2% better when generating texts with length ≤ 75.Interestingly, when generating longer summaries (length &gt;140) from smaller graphs, our model outperforms the baseline by an impressive 21.7%, indicating that our model is more effective in capturing semantic signals from graphs with scarce information in order to generate better text.Our approach also performs better when the graph size is large (number of nodes &gt; 55) but the generation output is small (≤ 75), beating the baseline by 9 points.</p>
<p>Human Evaluation</p>
<p>To further assess the quality of the generated text, we conduct a human evaluation on the WebNLG test set with seen categories.Following previous works (Gardent et al., 2017;Castro Ferreira et al., 2019), we assess two quality criteria: (i) Fluency (i.e., does the text flow in a natural, easy to read manner?) and (ii) Adequacy (i.e., does the text clearly express the data?).We divide the datapoints into seven different sets by the number of triples.For each set, we randomly select 20 texts generated by Adapt, CGE-LG and their corresponding human reference text (420 texts in total).Since the number of datapoints for each set is not balanced (see Table 5), this sampling strategy assures us to have the same amount of samples for the different triple sets.Moreover, having human references may serve as an indicator of the sanity of the human evaluation experiment.We recruited human workers from Mechanical Turk to rate the text outputs on a 1-5 Likert scale.For each text, we collect scores from 4 workers and average them.Table 6 shows the results.We first note a similar trend as in the automatic evaluation, with CGE-LG outperforming Adapt on both fluency and adequacy.In sets with the number of triples smaller than 5, CGE-LG was the highest rated system in fluency.Similarly to the automatic evaluation, both systems are better in generating text from graphs with smaller diameters.Also note that bigger diameters pose difficulties to the models, which achieve their worst performance for diameters ≥ 3.</p>
<p>Conclusion</p>
<p>We introduced an unified graph attention network structure for investigating graph-to-text architectures that combined global and local graph representations in order to improve text generation.An extensive evaluation of our models demonstrated that global and local contexts are empirically complementary, and a combination can achieve stateof-the-art results on KG-to-text generation.In addition, cascaded architectures give better results compared with parallel architectures.To our knowledge, we are the first to consider both local and global aggregation in a graph attention network.</p>
<p>…</p>
<p>Figure 1: A graphical representation (a) of a scientific text (b).(c) A global encoder directly captures longer dependencies between any pair of nodes (blue and red arrows), but fails in capturing the graph structure.(d) A local encoder explicitly accesses information from the adjacent nodes (blue arrows) and implicitly captures distant information (dashed red arrows).</p>
<p>Figure 3 :
3
Figure 3: (a) Comparison between different encoder architectures with respect to (a) graph diameter and (b) number of triples, for dev set of AGENDA dataset.</p>
<p>Figure 4 :
4
Figure 4: CHRF++ scores for AGENDA dev set, with respect to (a) the number of nodes, and (b) the graph diameter.(c) Distribution of output length of the gold references and models' output for the AGENDA dev set.</p>
<p>Figure 5 :
5
Figure 5: Relation between the number of nodes and the length of the generated text, in number of words.</p>
<p>Table 1 :
1
Data statistics.Nodes and edges values are calculated after the graph transformation.Averages are computed per instance.
a)Contextualised Embeddingsb)Contextualised Embeddingsc)Contextualised Embeddingsd)Contextualised EmbeddingsN x Global Node Encoder LayerLocal Node Encoder Layer M xN x M xLocal Node Encoder Layer Node Embeddings Global Node Encoder LayerN x Global Node EncoderLocal Node EncoderN xLocal Node Encoder Global Node EncoderNode EmbeddingsNode EmbeddingsNode EmbeddingsNode EmbeddingsFigure 2: Overview of the proposed encoder architectures. First, architectures with complete separated parallel(a) and cascaded (b) global and local node encoders. c) Global and local node representations are concatenatedlayer-wise. d) Both node representations are cascaded layer-wise.train dev test relations avg entities avg nodes avg edges avg lengthAGENDA 38,720 1,000 1,000712.444.368.6140.3WebNLG 18,102 871 9713734.034.9101.024.2</p>
<p>Table 2 :
2
Results on AGENDA test set.#L and #H are the numbers of layers and the attention heads in each layer, respectively.When more than one, the values are for the global and local encoders, respectively.#P stands for the number of parameters in millions (node embeddings included).
Model#L #HBLEUMETEORCHRF++#PKoncel-Kedziorski et al. (2019)6814.30 ±1.01 18.80 ±0.28--Baseline6814.11 ±0.28 19.35 ±0.52 41.95 ±0.39 54.4PGE-LW68, 4 17.40 ±0.08 22.06 ±0.09 46.19 ±0.16 67.7CGE-LW68, 8 17.44 ±0.10 22.02 ±0.13 46.24 ±0.14 76.4PGE6, 3 8, 8 17.17 ±0.38 21.70 ±0.25 45.75 ±0.43 67.4CGE6, 3 8, 8 17.81 ±0.15 21.75 ±0.55 46.76 ±0.12 66.9</p>
<p>Table 2
2presents the results. We report the num-ber of layers and attention heads employed bythe models. For a fair comparison, we use thesame number of global layers and attention headsamong different models. Our approaches substan-tially outperform the transformer baseline. CGE,our best model, outperforms Koncel-Kedziorskiet al. (2019), a graph transformer model thatonly allows information exchange between adja-cent nodes, by a large margin, achieving a BLEUscore of 17.81, 24.5% higher. Those results indi-cate that combining the local node context, lever-aging the graph topology, and the global node con-text, capturing macro-level node relations, leads tobetter node embeddings for text generation. Themodels based on layer-wise encoding have similarresults with CGE-LW achieving the best METEORscore. PGE has the worse performance amongthe proposed models. Even though CGE has thesmallest number of parameters, it achieves the bet-ter performance in terms of BLEU and CHRF++scores.</p>
<p>Table 3 :
3
., Results on WebNLG test set with seen categories.2019).Three systems are the best competitors in the challenge for seen categories: UPF-FORGe, Melbourne and Adapt.UPF-FORGe follows a rule-based approach, whereas Melbourne and Adapt employ encoder-decoder models with linearized triple sets.Table3presents the results.
ModelBLEUMETEOR CHRF++#PUPF-FORGe40.8840.00--Melbourne54.5241.0070.72-Adapt60.5944.0076.01-Marcheggiani and Perez (2018) 55.9039.00-4.9Trisedya et al. (2018)58.6040.60--Castro et al. (2019)57.2041.00--CGE-RP62.30 ±0.27 43.51 ±0.18 75.49 ±0.34 13.9CGE-LG63.10 ±0.13 44.11 ±0.09 76.33 ±0.10 12.8
Relations as Parameters.CGE-RP encodes relations as model parameters and achieves a BLEU score of 62.30, 8.9% better than the best model of Castro Ferreira et al. (2019), who employ an endto-end architecture based on GRUs.CGE-RP also outperform Trisedya et al. (</p>
<p>Table 4 :
4
Ablation study for modules used in the encoder and decoder of the CGE model.
ModelBLEU CHRF++ #PCGE17.2545.6166.9Global Encoder-Global Attention 15.4843.8963.8-FFN16.4844.8554.3-Global Encoder14.9643.1848.0Local Encoder-Graph Attention16.4445.5466.9-Weight Relations 16.7745.5156.7-GRU16.1944.5465.4-Local Encoder14.4342.4354.4-Shared Vocab.15.5244.0286.7Decoder-Length Penalty16.5044.6166.9
by a large margin of 7.2 BLEU points, showing our graph encoding strategies lead to a better text generation.We also outperform Adapt, a strong competitor that employs subword encodings, by 2.51 BLEU points.</p>
<p>Table 6 :
6
All 3.96 C 4.44 C 4.12 B 4.54 B 4.24 A 4.63 A 1-2 3.94 C 4.59 B 4.18 B 4.72 A 4.30 A 4.69 A 3-4 3.79 C 4.45 B 3.96 B 4.50 AB 4.14 A 4.66 A 5-7 4.08 B 4.35 B 4.18 B 4.45 B 4.28 A 4.59 A 3.98 C 4.50 B 4.16 B 4.61 A 4.28 A 4.66 A ≥ 3 3.91 C 4.33 B 4.03 B 4.43 B 4.17A 4.60 A Fluency (F) and Adequacy (A) obtained in the human evaluation.#T refers to the number of input triples and #D to graph diameters.The ranking was determined by pair-wise Mann-Whitney tests with p &lt; 0.05, and the difference between systems which have a letter in common is not statistically significant.</p>
<h1>TAdaptCGE-LGReferenceFAFAFA#DAdaptCGE-LGReferenceFAFAFA1-2</h1>
<p>In this paper, multi-relational graphs refer to directed graphs with labelled edges.
R contains relations both in canonical direction (e.g. used-for) and in inverse direction (e.g. used-for-inv).
As shown on Figure4c, 83% of the gold abstracts have more than 100 words.
AcknowledgmentsThis work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1.
Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)2020</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Thiago Castro Ferreira, Chris Van Der Lee, Emiel Van Miltenburg, Emiel Krahmer, 10.18653/v1/D19-1052Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/D14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, Qatar2014Association for Computational Linguistics</p>
<p>Structural neural encoders for AMR-to-text generation. Marco Damonte, Shay B Cohen, 10.18653/v1/N19-1366Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota20191Association for Computational Linguistics</p>
<p>Understanding the representation power of graph neural networks in learning graph topology. Nima Dehmamy, Albert-Laszlo Barabasi, Rose Yu, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, 10.3115/v1/W14-3348Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational Linguistics2014</p>
<p>Generation from abstract meaning representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, 10.18653/v1/N16-1087Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, Spain2017Association for Computational Linguistics</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, 10.1162/tacl_a_00269Transactions of the Association for Computational Linguistics. 72019</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics2019</p>
<p>Neural amr: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Inducing document plans for concept-to-text generation. Ioannis Konstas, Mirella Lapata, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USA2013Association for Computational Linguistics</p>
<p>Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. Q Li, Z Han, X.-M Wu, The Thirty-Second AAAI Conference on Artificial Intelligence. AAAI2018</p>
<p>Gated graph sequence neural networks. Yujia Li, Richard Zemel, Marc Brockschmidt, Daniel Tarlow, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)San Juan, Puerto Rico2016</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez Beltrachini, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational Linguistics2018</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. Amit Moryossef, Yoav Goldberg, Ido Dagan, 10.18653/v1/N19-1236Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota20191Association for Computational Linguistics</p>
<p>Bleu: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02Stroudsburg, PA, USAAssociation for Computational Linguistics2002</p>
<p>chrF: character n-gram fscore for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, Portugal2015Association for Computational Linguistics</p>
<p>Generating English from abstract meaning representations. 10.18653/v1/W16-6603Proceedings of the 9th International Natural Language Generation conference. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, the 9th International Natural Language Generation conferenceEdinburgh, UKAssociation for Computational Linguistics2016</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Modeling relational data with graph convolutional networks. Sejr Michael, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, 10.1007/978-3-319-93417-4_38The Semantic Web -15th International Conference, ESWC 2018. Heraklion, Crete, Greece2018. June 3-7, 2018</p>
<p>AMRto-text generation with synchronous node replacement grammar. Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P17-2002Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20172Short Papers)</p>
<p>A graph-to-sequence model for AMR-to-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia20181Association for Computational Linguistics</p>
<p>GTR-LSTM: A triple encoder for sentence generation from RDF data. Jianzhong Bayu Distiawan Trisedya, Rui Qi, Wei Zhang, Wang, 10.18653/v1/P18-1151Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30. I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc2017</p>
<p>Graph Attention Networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations. Vancouver, Canada2018</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, CoRR, abs/1609.08144Oriol Vinyals. 2016Greg Corrado, Macduff Hughes, and Jeffrey Dean</p>
<p>Representation learning on graphs with jumping knowledge networks. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken Ichi Kawarabayashi, Stefanie Jegelka, ICML. 2018</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, 10.18653/v1/D19-1548Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9015 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9015</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9015</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-0418bf267fe594ffa6828f8949fad6079d136ee4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0418bf267fe594ffa6828f8949fad6079d136ee4" target="_blank">Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> The advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. This progress presents novel challenges, such as measuring human-like psychological constructs, moving beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This review paper introduces and synthesizes the emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. The reviewed literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances LLM capabilities. Diverse perspectives are integrated to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, the review provides actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9015.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9015.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToM (Kosinski reports)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Mind evaluations (Kosinski et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reports that GPT-3.5 and GPT-4 achieve performance on structured Theory of Mind (ToM) tasks comparable to or exceeding that of children in classic false-belief-style assessments, as cited in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary, large transformer-based autoregressive language models (OpenAI family). The review cites these models as advanced instruction-tuned LLMs with strong few-shot and reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Structured Theory of Mind tasks / false-belief tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Classic ToM tasks assessing the ability to attribute beliefs, intentions, and knowledge to others (e.g., first- and higher-order false belief problems); measures social-cognitive perspective-taking.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to match or exceed performance of children on structured ToM tasks (qualitative statement from cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Reported comparison against child-level baselines in the cited works (no numerical score reported in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs (GPT-3.5/GPT-4) comparable to or above child baselines on structured ToM tasks in the cited reports; not clearly equivalent to adult general ToM abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations used structured prompts adapting classical false-belief tasks to LLM prompts; prompting and formatting strongly influence outcomes per review discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The review emphasizes challenges: small changes to tasks often break LLM performance, indicating brittle heuristics rather than robust ToM; generalization to adversarial or higher-order tasks is poor; robustness and ecological validity are questioned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9015.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristics & Biases (Binz & Schulz; Hagendorff)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristics and biases evaluations of LLMs (e.g., anchoring, framing, conjunction fallacy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple studies report LLMs exhibit cognitive biases similar to humans (anchoring, framing, conjunction fallacy), with more advanced models showing reduced intuitive (System 1) errors and increased deliberative (System 2) behavior under certain prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-4 (and other chain-of-thought enabled models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs evaluated on scenario-based decision tasks probing well-known human biases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Cognitive-bias scenario batteries (anchoring, framing, conjunction fallacy, loss aversion, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Scenario-based tasks adapted from cognitive psychology to probe heuristics and systematic biases in decision making (rationality tests).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 reported to perform comparably to human participants on some cognitive ability/bias tests; GPT-4 shows fewer System 1-type errors and more deliberative reasoning under appropriate prompts (qualitative summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-like biases observed; the review does not report precise numerical human baselines for each cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs approximate human bias patterns; some advanced models reduce certain biases but still exhibit persistent systematic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Studies used synthetic scenarios, multi-agent dialogues, and prompting techniques (including chain-of-thought) to elicit reasoning; results sensitive to prompt phrasing and task formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Caveats include non-robustness to small task changes, dependence on surface cues rather than causal understanding, and mixed findings about whether scale alone reduces biases without targeted mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9015.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emotional Intelligence (EI) benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emotional Intelligence evaluations (e.g., EmoBench, EQ-Bench, LEAS adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks adapted from human EI instruments report that advanced LLMs (notably GPT-4 in cited works) often match or exceed human baselines on measures of emotional understanding and recognition, while lacking deeper reflexive emotional experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and other advanced LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned LLMs evaluated on EI tasks adapted from human psychometric instruments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>EmoBench / EQ-Bench / LEAS and related EI instruments</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks measuring emotional awareness, discrimination of emotions in scenarios, empathy understanding, and regulation (emotional intelligence constructs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Advanced LLMs (e.g., GPT-4) reported to often match or surpass human baselines on structured EI tasks (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Cited works compare to human baselines, but the review does not list numerical human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs perform on par with or above human baselines on many structured EI tasks, yet they display limitations in depth and authenticity of emotional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations include structured test items adapted from human EI tests and multimodal extensions; scoring often automated but may require rubric adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>LLMs can produce mechanically empathetic responses lacking genuine introspective/emotional depth; performance may reflect pattern matching from pretraining data rather than experiential understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9015.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WAIS-IV adaptation (Galatzer-Levy et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptation of Wechsler Adult Intelligence Scale (WAIS-IV) for LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using WAIS-IV subtests, cited work shows LLMs generally reach high human levels on verbal comprehension and working memory subtests, while multimodal visual reasoning lags.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified advanced LLMs (review cites Galatzer-Levy et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated via human cognitive test adaptations; likely instruction-tuned transformer models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>WAIS-IV (selected subtests: verbal comprehension, working memory, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Standardized clinical instrument measuring multiple cognitive domains including verbal comprehension, perceptual reasoning, working memory, and processing speed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to reach top human levels in verbal comprehension and working memory subtests (qualitative statement in review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Reference to human normative WAIS-IV performance (no specific numerical comparisons provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs comparable to high human percentiles on verbal and working memory components; multimodal/visual reasoning components show weaker performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Human test items were adapted to LLM prompts; scoring required mapping LLM outputs to item scoring rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Validity concerns about applying clinical human tests to non-human systems; potential data contamination and differences in modality (text vs. visual) affect comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9015.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Raven's / Fluid intelligence mentions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Raven's Progressive Matrices and fluid intelligence style tests applied to LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited studies report LLMs can match or surpass humans on analogical and fluid-reasoning style tasks (e.g., Raven-like matrices) when adapted for text or symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs evaluated in cited studies (unspecified in review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs tested on abstract reasoning tasks adapted from human fluid intelligence batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Raven's Progressive Matrices / analogical reasoning tests</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Nonverbal test of abstract reasoning and pattern completion; assesses fluid intelligence and analogical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reports indicate LLMs can match or surpass typical human performance on certain analogical reasoning items (qualitative summary).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norms for Raven's matrices exist, but the review does not supply numeric baselines for the cited comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs match or exceed humans on some Raven-like tasks, though findings depend on task adaptation and representation (textual vs. visual).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Adaptations convert visual matrix problems into textual or symbolic representations suitable for LLM input; success depends on encoding and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Multimodal/visual reasoning often weaker; converting visual matrices to text can change task demands; concerns about whether solutions reflect genuine abstract reasoning vs. pattern exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9015.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoCA finding (Dayan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Montreal Cognitive Assessment (MoCA) adaptations for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work (Dayan et al., 2024) reports many LLMs display 'mild cognitive impairment' patterns on MoCA-like assessments, especially on visuospatial and executive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (unspecified in review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated on an adaptation of the MoCA screening instrument.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Montreal Cognitive Assessment (MoCA) adapted items</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Brief screening for mild cognitive impairment covering executive function, visuospatial skills, memory, attention, language, and orientation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most LLMs showed deficits consistent with 'mild cognitive impairment' on adapted MoCA items (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human clinical cutoffs for MoCA exist, but the review does not list numerical human comparators for the cited LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs underperform on visuospatial and executive components relative to typical human performance, producing patterns analogous to mild impairment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>MoCA items adapted into text prompts; visuospatial items are particularly challenging to represent for purely text models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Cross-domain validity issues when applying clinical human screening tools to LLMs; modality mismatch (visual tasks) and adaptation choices strongly affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9015.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARC (Abstraction & Reasoning Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstraction and Reasoning Challenge (ARC) style evaluations as applied to LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited reports indicate LLMs show deficits on ARC-style tasks that require abstract, algorithmic problem solving and strong generalization beyond surface patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (cited works such as Wu et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated on benchmarks emphasizing abstract pattern learning and transfer (ARC tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>ARC (Abstraction & Reasoning Challenge) and similar abstract-reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks requiring discovery of abstract transformations and general rules from few examples (tests of generalization and algorithmic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLMs exhibit notable deficits on ARC-style problems (qualitative summary).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human performance varies by task; the review does not provide numerical baselines from the cited studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs below human-level performance on many ARC problems, indicating limitations in abstract generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ARC tasks often require conversion to textual descriptions or symbolic representations for LLM input, which affects difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Strong sensitivity to task encoding; performance suggests limitations in compositional and algorithmic reasoning beyond statistical pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9015.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Psycholinguistic tests (Duan et al.; BojiÄ‡ et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Psycholinguistic test batteries (comprehension, surprisal, grammaticality, implicature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple psycholinguistic tests adapted for LLMs show mixed results: some models (e.g., GPT-2-XL) display human-like competence in selected phenomena, while advanced models (GPT-4) often match or surpass humans in pragmatic reasoning and grammaticality judgments but remain limited in implicature and other nuanced areas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL, GPT-4, and other advanced LLMs (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs evaluated across multiple psycholinguistic tasks for comprehension and production.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Psycholinguistic test battery (e.g., surprisal measures, grammaticality judgment, pragmatic inference tests)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assesses language comprehension across phonology/lexicon/syntax/semantics/pragmatics (e.g., predictability/surprisal, grammaticality, implicature).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-2-XL shows human-like competence on some tasks; GPT-4 often matches or exceeds humans in pragmatic reasoning and grammaticality judgment but performs worse on implicature understanding (qualitative summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human performance normative patterns exist per psycholinguistic literature; the review does not provide numeric baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Model performance sometimes aligns with humans on surface-level and pragmatic tasks but diverges on deeper implicature and discourse-level understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Studies used surprisal calculations, adapted grammaticality items, and pragmatic inference prompts; outcomes sensitive to prompt format and test encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Prompt sensitivity, differences in representation of stimuli (text vs. multimodal), and inconsistencies across tasks limit strong claims of human-like psycholinguistic competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9015.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Creativity tests (AUT, TTCT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Creativity evaluations using Guilford's Alternative Uses Test (AUT) and Torrance Tests of Creative Thinking (TTCT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adapted creativity tests indicate early LLMs (e.g., GPT-3) lack originality, while advanced models (e.g., GPT-4) can surpass average human performance on some creative-writing and certain creativity measures, yet show weaknesses in divergent novelty and narrative diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLMs evaluated on divergent and convergent creativity tasks adapted from human creativity batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Guilford's Alternative Uses Test (AUT), Torrance Test of Creative Thinking (TTCT), and other creativity measures</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Measures divergent thinking (fluency, originality, flexibility) and creative production in written tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 reported to lack originality; GPT-4 reported to surpass human average in some creativity measures (qualitative statements from cited studies).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baseline averages referenced in cited works but not numerically reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Advanced LLMs may outperform average humans on some creative-writing metrics, but they underperform on divergent novelty and narrative diversity aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tasks adapted to text prompts (e.g., ask for alternative uses or story generation); scoring involves human or automatic originality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>LLM creativity may be constrained by safe, positive-toned outputs; evaluation of novelty is sensitive to scoring rubric and may not capture human-like novelty or surprise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9015.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9015.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Social Intelligence (SJT) - MittelstÃ¤dt et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Situational Judgment Test (SJT) based social intelligence evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SJTs and related situational assessments, some LLMs outperform humans on expert-rated social appropriateness in structured situations, but they struggle with implicit social understanding and faux pas detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various advanced LLMs (unspecified in review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs assessed for social intelligence via SJTs and scenario-based tests.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Situational Judgment Tests (SJTs) / Situational Evaluation of Social Intelligence (SESI) and related benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Scenario-based assessments of social judgment, appropriateness, and decision-making in social situations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Some LLMs outperform humans on expert-rated social appropriateness in structured SJTs but show weaknesses on tests requiring implicit social inference (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human performance is the comparison baseline in cited studies; specific numeric baselines are not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs strong on rule-based and explicit social norms; below humans on implicit social inference, faux pas detection, and contextualized social understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations used structured scenario prompts and multi-agent simulations; multi-agent and agentic simulations extend testing beyond single-turn SJTs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>LLMs tend to imitate normative responses and may fail on tasks needing genuine pragmatics or private-information reasoning; results depend on prompt framing and scenario richness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9015",
    "paper_id": "paper-0418bf267fe594ffa6828f8949fad6079d136ee4",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "ToM (Kosinski reports)",
            "name_full": "Theory of Mind evaluations (Kosinski et al., 2023)",
            "brief_description": "Reports that GPT-3.5 and GPT-4 achieve performance on structured Theory of Mind (ToM) tasks comparable to or exceeding that of children in classic false-belief-style assessments, as cited in the review.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5, GPT-4",
            "model_description": "Proprietary, large transformer-based autoregressive language models (OpenAI family). The review cites these models as advanced instruction-tuned LLMs with strong few-shot and reasoning capabilities.",
            "model_size": null,
            "test_battery_name": "Structured Theory of Mind tasks / false-belief tasks",
            "test_description": "Classic ToM tasks assessing the ability to attribute beliefs, intentions, and knowledge to others (e.g., first- and higher-order false belief problems); measures social-cognitive perspective-taking.",
            "llm_performance": "Reported to match or exceed performance of children on structured ToM tasks (qualitative statement from cited work).",
            "human_baseline_performance": "Reported comparison against child-level baselines in the cited works (no numerical score reported in this review).",
            "performance_comparison": "LLMs (GPT-3.5/GPT-4) comparable to or above child baselines on structured ToM tasks in the cited reports; not clearly equivalent to adult general ToM abilities.",
            "experimental_details": "Evaluations used structured prompts adapting classical false-belief tasks to LLM prompts; prompting and formatting strongly influence outcomes per review discussion.",
            "limitations_or_caveats": "The review emphasizes challenges: small changes to tasks often break LLM performance, indicating brittle heuristics rather than robust ToM; generalization to adversarial or higher-order tasks is poor; robustness and ecological validity are questioned.",
            "uuid": "e9015.0",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Heuristics & Biases (Binz & Schulz; Hagendorff)",
            "name_full": "Heuristics and biases evaluations of LLMs (e.g., anchoring, framing, conjunction fallacy)",
            "brief_description": "Multiple studies report LLMs exhibit cognitive biases similar to humans (anchoring, framing, conjunction fallacy), with more advanced models showing reduced intuitive (System 1) errors and increased deliberative (System 2) behavior under certain prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3, GPT-4 (and other chain-of-thought enabled models)",
            "model_description": "Autoregressive transformer LLMs evaluated on scenario-based decision tasks probing well-known human biases.",
            "model_size": null,
            "test_battery_name": "Cognitive-bias scenario batteries (anchoring, framing, conjunction fallacy, loss aversion, etc.)",
            "test_description": "Scenario-based tasks adapted from cognitive psychology to probe heuristics and systematic biases in decision making (rationality tests).",
            "llm_performance": "GPT-3 reported to perform comparably to human participants on some cognitive ability/bias tests; GPT-4 shows fewer System 1-type errors and more deliberative reasoning under appropriate prompts (qualitative summaries).",
            "human_baseline_performance": "Human-like biases observed; the review does not report precise numerical human baselines for each cited study.",
            "performance_comparison": "LLMs approximate human bias patterns; some advanced models reduce certain biases but still exhibit persistent systematic errors.",
            "experimental_details": "Studies used synthetic scenarios, multi-agent dialogues, and prompting techniques (including chain-of-thought) to elicit reasoning; results sensitive to prompt phrasing and task formulation.",
            "limitations_or_caveats": "Caveats include non-robustness to small task changes, dependence on surface cues rather than causal understanding, and mixed findings about whether scale alone reduces biases without targeted mitigation.",
            "uuid": "e9015.1",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Emotional Intelligence (EI) benchmarks",
            "name_full": "Emotional Intelligence evaluations (e.g., EmoBench, EQ-Bench, LEAS adaptations)",
            "brief_description": "Benchmarks adapted from human EI instruments report that advanced LLMs (notably GPT-4 in cited works) often match or exceed human baselines on measures of emotional understanding and recognition, while lacking deeper reflexive emotional experience.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (and other advanced LLMs)",
            "model_description": "Large instruction-tuned LLMs evaluated on EI tasks adapted from human psychometric instruments.",
            "model_size": null,
            "test_battery_name": "EmoBench / EQ-Bench / LEAS and related EI instruments",
            "test_description": "Tasks measuring emotional awareness, discrimination of emotions in scenarios, empathy understanding, and regulation (emotional intelligence constructs).",
            "llm_performance": "Advanced LLMs (e.g., GPT-4) reported to often match or surpass human baselines on structured EI tasks (qualitative).",
            "human_baseline_performance": "Cited works compare to human baselines, but the review does not list numerical human scores.",
            "performance_comparison": "LLMs perform on par with or above human baselines on many structured EI tasks, yet they display limitations in depth and authenticity of emotional reasoning.",
            "experimental_details": "Evaluations include structured test items adapted from human EI tests and multimodal extensions; scoring often automated but may require rubric adaptations.",
            "limitations_or_caveats": "LLMs can produce mechanically empathetic responses lacking genuine introspective/emotional depth; performance may reflect pattern matching from pretraining data rather than experiential understanding.",
            "uuid": "e9015.2",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "WAIS-IV adaptation (Galatzer-Levy et al.)",
            "name_full": "Adaptation of Wechsler Adult Intelligence Scale (WAIS-IV) for LLM evaluation",
            "brief_description": "Using WAIS-IV subtests, cited work shows LLMs generally reach high human levels on verbal comprehension and working memory subtests, while multimodal visual reasoning lags.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified advanced LLMs (review cites Galatzer-Levy et al., 2024)",
            "model_description": "LLMs evaluated via human cognitive test adaptations; likely instruction-tuned transformer models.",
            "model_size": null,
            "test_battery_name": "WAIS-IV (selected subtests: verbal comprehension, working memory, etc.)",
            "test_description": "Standardized clinical instrument measuring multiple cognitive domains including verbal comprehension, perceptual reasoning, working memory, and processing speed.",
            "llm_performance": "Reported to reach top human levels in verbal comprehension and working memory subtests (qualitative statement in review).",
            "human_baseline_performance": "Reference to human normative WAIS-IV performance (no specific numerical comparisons provided in review).",
            "performance_comparison": "LLMs comparable to high human percentiles on verbal and working memory components; multimodal/visual reasoning components show weaker performance.",
            "experimental_details": "Human test items were adapted to LLM prompts; scoring required mapping LLM outputs to item scoring rubrics.",
            "limitations_or_caveats": "Validity concerns about applying clinical human tests to non-human systems; potential data contamination and differences in modality (text vs. visual) affect comparability.",
            "uuid": "e9015.3",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Raven's / Fluid intelligence mentions",
            "name_full": "Raven's Progressive Matrices and fluid intelligence style tests applied to LLMs",
            "brief_description": "Cited studies report LLMs can match or surpass humans on analogical and fluid-reasoning style tasks (e.g., Raven-like matrices) when adapted for text or symbolic reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs evaluated in cited studies (unspecified in review summary)",
            "model_description": "Transformer LLMs tested on abstract reasoning tasks adapted from human fluid intelligence batteries.",
            "model_size": null,
            "test_battery_name": "Raven's Progressive Matrices / analogical reasoning tests",
            "test_description": "Nonverbal test of abstract reasoning and pattern completion; assesses fluid intelligence and analogical reasoning.",
            "llm_performance": "Reports indicate LLMs can match or surpass typical human performance on certain analogical reasoning items (qualitative summary).",
            "human_baseline_performance": "Human norms for Raven's matrices exist, but the review does not supply numeric baselines for the cited comparisons.",
            "performance_comparison": "LLMs match or exceed humans on some Raven-like tasks, though findings depend on task adaptation and representation (textual vs. visual).",
            "experimental_details": "Adaptations convert visual matrix problems into textual or symbolic representations suitable for LLM input; success depends on encoding and prompt design.",
            "limitations_or_caveats": "Multimodal/visual reasoning often weaker; converting visual matrices to text can change task demands; concerns about whether solutions reflect genuine abstract reasoning vs. pattern exploitation.",
            "uuid": "e9015.4",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MoCA finding (Dayan et al.)",
            "name_full": "Montreal Cognitive Assessment (MoCA) adaptations for LLMs",
            "brief_description": "Cited work (Dayan et al., 2024) reports many LLMs display 'mild cognitive impairment' patterns on MoCA-like assessments, especially on visuospatial and executive tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (unspecified in review summary)",
            "model_description": "LLMs evaluated on an adaptation of the MoCA screening instrument.",
            "model_size": null,
            "test_battery_name": "Montreal Cognitive Assessment (MoCA) adapted items",
            "test_description": "Brief screening for mild cognitive impairment covering executive function, visuospatial skills, memory, attention, language, and orientation.",
            "llm_performance": "Most LLMs showed deficits consistent with 'mild cognitive impairment' on adapted MoCA items (qualitative).",
            "human_baseline_performance": "Human clinical cutoffs for MoCA exist, but the review does not list numerical human comparators for the cited LLM evaluations.",
            "performance_comparison": "LLMs underperform on visuospatial and executive components relative to typical human performance, producing patterns analogous to mild impairment.",
            "experimental_details": "MoCA items adapted into text prompts; visuospatial items are particularly challenging to represent for purely text models.",
            "limitations_or_caveats": "Cross-domain validity issues when applying clinical human screening tools to LLMs; modality mismatch (visual tasks) and adaptation choices strongly affect outcomes.",
            "uuid": "e9015.5",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ARC (Abstraction & Reasoning Challenge)",
            "name_full": "Abstraction and Reasoning Challenge (ARC) style evaluations as applied to LLMs",
            "brief_description": "Cited reports indicate LLMs show deficits on ARC-style tasks that require abstract, algorithmic problem solving and strong generalization beyond surface patterns.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (cited works such as Wu et al., 2025)",
            "model_description": "LLMs evaluated on benchmarks emphasizing abstract pattern learning and transfer (ARC tasks).",
            "model_size": null,
            "test_battery_name": "ARC (Abstraction & Reasoning Challenge) and similar abstract-reasoning tasks",
            "test_description": "Tasks requiring discovery of abstract transformations and general rules from few examples (tests of generalization and algorithmic reasoning).",
            "llm_performance": "LLMs exhibit notable deficits on ARC-style problems (qualitative summary).",
            "human_baseline_performance": "Human performance varies by task; the review does not provide numerical baselines from the cited studies.",
            "performance_comparison": "LLMs below human-level performance on many ARC problems, indicating limitations in abstract generalization.",
            "experimental_details": "ARC tasks often require conversion to textual descriptions or symbolic representations for LLM input, which affects difficulty.",
            "limitations_or_caveats": "Strong sensitivity to task encoding; performance suggests limitations in compositional and algorithmic reasoning beyond statistical pattern matching.",
            "uuid": "e9015.6",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Psycholinguistic tests (Duan et al.; BojiÄ‡ et al.)",
            "name_full": "Psycholinguistic test batteries (comprehension, surprisal, grammaticality, implicature)",
            "brief_description": "Multiple psycholinguistic tests adapted for LLMs show mixed results: some models (e.g., GPT-2-XL) display human-like competence in selected phenomena, while advanced models (GPT-4) often match or surpass humans in pragmatic reasoning and grammaticality judgments but remain limited in implicature and other nuanced areas.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-2-XL, GPT-4, and other advanced LLMs (as cited)",
            "model_description": "Transformer LLMs evaluated across multiple psycholinguistic tasks for comprehension and production.",
            "model_size": null,
            "test_battery_name": "Psycholinguistic test battery (e.g., surprisal measures, grammaticality judgment, pragmatic inference tests)",
            "test_description": "Assesses language comprehension across phonology/lexicon/syntax/semantics/pragmatics (e.g., predictability/surprisal, grammaticality, implicature).",
            "llm_performance": "GPT-2-XL shows human-like competence on some tasks; GPT-4 often matches or exceeds humans in pragmatic reasoning and grammaticality judgment but performs worse on implicature understanding (qualitative summaries).",
            "human_baseline_performance": "Human performance normative patterns exist per psycholinguistic literature; the review does not provide numeric baselines.",
            "performance_comparison": "Model performance sometimes aligns with humans on surface-level and pragmatic tasks but diverges on deeper implicature and discourse-level understanding.",
            "experimental_details": "Studies used surprisal calculations, adapted grammaticality items, and pragmatic inference prompts; outcomes sensitive to prompt format and test encoding.",
            "limitations_or_caveats": "Prompt sensitivity, differences in representation of stimuli (text vs. multimodal), and inconsistencies across tasks limit strong claims of human-like psycholinguistic competence.",
            "uuid": "e9015.7",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Creativity tests (AUT, TTCT)",
            "name_full": "Creativity evaluations using Guilford's Alternative Uses Test (AUT) and Torrance Tests of Creative Thinking (TTCT)",
            "brief_description": "Adapted creativity tests indicate early LLMs (e.g., GPT-3) lack originality, while advanced models (e.g., GPT-4) can surpass average human performance on some creative-writing and certain creativity measures, yet show weaknesses in divergent novelty and narrative diversity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3, GPT-4",
            "model_description": "Large transformer LLMs evaluated on divergent and convergent creativity tasks adapted from human creativity batteries.",
            "model_size": null,
            "test_battery_name": "Guilford's Alternative Uses Test (AUT), Torrance Test of Creative Thinking (TTCT), and other creativity measures",
            "test_description": "Measures divergent thinking (fluency, originality, flexibility) and creative production in written tasks.",
            "llm_performance": "GPT-3 reported to lack originality; GPT-4 reported to surpass human average in some creativity measures (qualitative statements from cited studies).",
            "human_baseline_performance": "Human baseline averages referenced in cited works but not numerically reported in the review.",
            "performance_comparison": "Advanced LLMs may outperform average humans on some creative-writing metrics, but they underperform on divergent novelty and narrative diversity aspects.",
            "experimental_details": "Tasks adapted to text prompts (e.g., ask for alternative uses or story generation); scoring involves human or automatic originality metrics.",
            "limitations_or_caveats": "LLM creativity may be constrained by safe, positive-toned outputs; evaluation of novelty is sensitive to scoring rubric and may not capture human-like novelty or surprise.",
            "uuid": "e9015.8",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Social Intelligence (SJT) - MittelstÃ¤dt et al.",
            "name_full": "Situational Judgment Test (SJT) based social intelligence evaluations",
            "brief_description": "Applying SJTs and related situational assessments, some LLMs outperform humans on expert-rated social appropriateness in structured situations, but they struggle with implicit social understanding and faux pas detection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various advanced LLMs (unspecified in review summary)",
            "model_description": "Transformer LLMs assessed for social intelligence via SJTs and scenario-based tests.",
            "model_size": null,
            "test_battery_name": "Situational Judgment Tests (SJTs) / Situational Evaluation of Social Intelligence (SESI) and related benchmarks",
            "test_description": "Scenario-based assessments of social judgment, appropriateness, and decision-making in social situations.",
            "llm_performance": "Some LLMs outperform humans on expert-rated social appropriateness in structured SJTs but show weaknesses on tests requiring implicit social inference (qualitative).",
            "human_baseline_performance": "Human performance is the comparison baseline in cited studies; specific numeric baselines are not provided in the review.",
            "performance_comparison": "LLMs strong on rule-based and explicit social norms; below humans on implicit social inference, faux pas detection, and contextualized social understanding.",
            "experimental_details": "Evaluations used structured scenario prompts and multi-agent simulations; multi-agent and agentic simulations extend testing beyond single-turn SJTs.",
            "limitations_or_caveats": "LLMs tend to imitate normative responses and may fail on tasks needing genuine pragmatics or private-information reasoning; results depend on prompt framing and scenario richness.",
            "uuid": "e9015.9",
            "source_info": {
                "paper_title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.016777,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large language model psychometrics: A systematic review of evaluation, validation, and enhancement</h1>
<p>Haoran Ye ${ }^{1}$, Jing Jin ${ }^{1}$, Yuhang Xie ${ }^{1}$, Xin Zhang ${ }^{2,3}$, Guojie Song ${ }^{1,4, \geq 0}$<br>${ }^{1}$ State Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University<br>${ }^{2}$ School of Psychological and Cognitive Sciences, Peking University<br>${ }^{3}$ Key Laboratory of Machine Perception (Ministry of Education), Peking University<br>${ }^{4}$ PKU-Wuhan Institute for Artificial Intelligence<br>hrye@stu.pku.edu.cn gjsong@pku.edu.cn<br>Project Website: https://llm-psychometrics.com</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h2>Abstract</h2>
<p>The advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. This progress presents novel challenges, such as measuring human-like psychological constructs, moving beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This review paper introduces and synthesizes the emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. The reviewed literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances LLM capabilities. Diverse perspectives are integrated to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, the review provides actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of humancentered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 4
2 Preliminary and methodological foundations ..... 6
2.1 Large language models ..... 6
2.2 Psychometrics ..... 6
2.3 Psychometric evaluation of AI before the era of LLMs ..... 7
3 Psychometrics for benchmarking principles ..... 7
3.1 Fundamental differences between psychometrics and AI benchmarking ..... 7
3.2 Benchmarking with psychometrics-inspired principles ..... 9
4 Psychometrics for measuring psychological constructs ..... 10
4.1 Measuring personality constructs ..... 10
4.1.1 Personality traits ..... 12
4.1.2 Values ..... 13
4.1.3 Morality ..... 14
4.1.4 Attitudes and opinions ..... 15
4.2 Measuring cognitive constructs ..... 16
4.2.1 Heuristics and biases ..... 16
4.2.2 Social interactions ..... 16
4.2.3 Psychology of language ..... 19
4.2.4 Learning and cognitive capabilities ..... 19
5 Psychometric evaluation methodology ..... 20
5.1 Test format ..... 20
5.1.1 Structured tests ..... 21
5.1.2 Unstructured tests ..... 22
5.2 Data and task sources ..... 23
5.3 Prompting strategies ..... 23
5.4 Model output and scoring ..... 24
5.4.1 Closed-ended output and scoring ..... 24
5.4.2 Open-ended output and scoring ..... 24
5.5 Inference parameters ..... 25
6 Psychometric validation ..... 25
6.1 Reliability and consistency ..... 25
6.2 Validity ..... 26
6.2.1 Content validity ..... 26
6.2.2 Construct validity ..... 27
6.2.3 Criterion and ecological validity ..... 27</p>
<p>7 Psychometrics for LLM enhancement ..... 28
7.1 Trait manipulation ..... 28
7.2 Safety and alignment ..... 29
7.3 Cognitive enhancement ..... 29
8 Trends, challenges, and future directions ..... 29
8.1 Psychometric validation ..... 29
8.2 From human constructs to LLM constructs ..... 29
8.3 Perceived vs. aligned traits ..... 30
8.4 Anthropomorphization challenges ..... 30
8.5 Expanding dimensions in model deployment ..... 30
8.6 Item response theory ..... 31
8.7 From evaluation to enhancement ..... 31
9 Ethical considerations ..... 31
10 Conclusion ..... 32</p>
<h1>1 Introduction</h1>
<p>The advent of large language models (LLMs) represents a transformative breakthrough in AI. These systems exhibit general-purpose capabilities spanning diverse domains [Bubeck et al., 2023], with particular proficiency in natural language understanding and generation [Demszky et al., 2023, Grossmann et al., 2023, Gu et al., 2024, Ziems et al., 2024]. They are rapidly integrated into critical societal infrastructure, ranging from consumer-facing applications like chatbots [OpenAI, 2025] and search engines [Wang et al., 2024c] to high-stakes domains such as healthcare [Singhal et al., 2023], education [Milano et al., 2023], and scientific discovery [Romera-Paredes et al., 2024, Ye et al., 2024]. Their increasing dominance has exposed a fundamental, pressing scientific challenge: how can we rigorously evaluate these AI systems that transcend traditional benchmarks of biological or algorithmic intelligence?
Traditional AI evaluation has relied on curating task-specific datasets, annotating ground-truth labels with human input, running models on these datasets, and assessing performance using predefined metrics. However, LLMs have triggered an evaluation crisis, as their versatile capabilities and human-like behaviors exceed what traditional benchmarks can measure. Novel challenges include but are not limited to (1) evaluating psychological constructs like personality, values, and cognitive biases; (2) obsolescence of static benchmarks due to rapid LLM development and training data contamination; (3) compromised robustness and validity due to LLMs' prompt- and context-sensitivity; (4) requiring human-centered evaluation approaches; and (5) expanding evaluation methodologies in scope and complexity as LLMs integrate into agentic and multimodal systems.
These challenges intersect with humanity's century-old quest to quantify the complex, intangible human psychology [Pasquali, 2009]. Psychometrics emerged from this timeless pursuit as the scientific study of psychological measurement. It bridges the abstract and the empirical by transforming human traits into quantifiable data, enabling better understanding, prediction, and decision-making in education, business, healthcare, governance, and beyond [Rust and Golombok, 2014]. The intersection of LLMs and psychometrics forms a methodological crucible, forging novel paradigms to better decode and improve machine minds.
This intersection sets the stage for a new research frontier. We define LLM Psychometrics as the interdisciplinary field dedicated to evaluating, understanding, and enhancing LLMs through the application and integration of psychometric instruments, theories, and principles. This field seeks to quantify, interpret, manipulate, and improve the complex, human-like attributes and behaviors exhibited by LLMs, encompassing both personality constructs (e.g., personality, values, morality, and attitudes) and cognitive constructs (e.g., heuristics and biases, social interaction abilities, psycholinguistic abilities, and learning and cognitive capacities). Research in LLM Psychometrics applies, extends, and innovates upon scientific methods of psychological measurement for LLMs. Based on the psychometric principles, related research systematically examines the measurement results to ensure scientific rigor. By measuring and elucidating psychological constructs in LLMs, LLM Psychometrics further informs strategies for their targeted enhancement.
Recent research in LLM Psychometrics pioneers in addressing the LLM evaluation crisis. Some studies introduce dynamic and construct-oriented evaluation frameworks that move beyond static, task-specific benchmarks [Hagendorff, 2023, Zhu et al., 2024a]. In parallel, novel methodologies are developed to measure non-cognitive and emergent constructs [Huang et al., 2023d, Pellert et al., 2024, Ren et al., 2024]. Self-adaptive evaluation techniques now allow for the extrapolation of item difficulty and tailoring assessments to model performance [Jiang et al., 2024a, Lalor et al., 2024, Polo et al., 2024]. Drawing from the methodological framework of psychometric validation, research improves the reliability and validity of evaluation protocols [Ye et al., 2025a]. Human-centered evaluation drives aligning model behavior with human values [Wang et al., 2024f, Yao et al., 2025a]. In addition, the scope of evaluation expands to agentic and multimodal systems, further broadening the methodological landscape [Huang et al., 2024c, Li et al., 2024b].
The field of LLM Psychometrics has seen significant growth, evidenced by the proliferation of related research papers. These studies, however, address a variety of psychological constructs, employ diverse methodologies, and utilize distinct validation techniques. The interdisciplinary nature of this domain has attracted contributions from a broad spectrum of academic communities. Despite this diversity, there is a lack of cohesion among researchers, leading to a fragmentation of insights, particularly between studies focusing on disparate constructs. Consequently, there is a pressing need for a systematic review to synthesize these efforts and facilitate a more integrated understanding of the field.</p>
<p>Taxonomy and paper structure. To bridge the gap, we systematically review LLM Psychometrics across evaluation, validation, and enhancement. The evaluation framework encompasses three core dimensions: the target construct (what to measure), the measurement method (how to measure), and the validation of results (how well do we measure). The psychometric insights not only inform evaluation but also guide the development and refinement of LLMs (how to improve). Accordingly, this review is structured as follows (Fig. 1). $\S 2$ provides an overview of the preliminaries and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: Overview of this review.
methodological foundations to facilitate subsequent discussions. Â§ 3 thoroughly contrasts psychometrics and traditional AI benchmarking and reviews how psychometric principles can underpin and reinvent LLM benchmarks. The core measurement framework is detailed in $\S 4, \S 5$, and $\S 6 . \S 4$ delves into the psychological constructs evaluated in LLMs, elucidating the theories employed and summarizing key evaluation findings. In $\S 5$, we scrutinize the psychometric evaluation methodologies applied to LLMs, followed by $\S 6$, which examines the psychometric validation of evaluation results. Beyond evaluation, $\S 7$ introduces strategies for enhancing LLMs through psychometric insights. $\S 8$ discusses emerging trends, challenges, and future directions. Finally, $\S 9$ highlights key ethical considerations, while $\S 10$ concludes the paper.</p>
<p>Scope and related work. We consider studies to be out of scope if they do not employ psychometric approaches, adhere to psychometric principles, or if they focus solely on scalar performance metrics rather than characterizing behaviors. Readers interested in conventional LLM benchmarking are encouraged to consult surveys on LLM evaluation [Chang et al., 2024, Guo et al., 2023b]. Several related reviews focus on the evaluation of specific constructs in LLMs, such as personality [Dong et al., 2025, Wen et al., 2024b], attitudes and values [Ma et al., 2024a], cultural awareness [Adilazuarda et al., 2024, Pawar et al., 2024], and theory of mind [Dong et al., 2025, SarÄ±taÅŸ et al., 2025]. Hagendorff [2023], Hagendorff et al. [2024] introduce the notion of machine psychology and review emergent LLM abilities; however, they do not provide comprehensive coverage of the related research, nor do they elaborate on LLM personality constructs, psychometric validation, or enhancement. This paper provides the first systematic review of LLM Psychometrics.</p>
<h1>2 Preliminary and methodological foundations</h1>
<p>We aim for our survey to be self-contained and accessible to a broad, cross-disciplinary audience. To this end, this section presents the preliminary and methodological foundations that underpin the subsequent discussions.</p>
<h3>2.1 Large language models</h3>
<p>LLMs are large-scale deep neural networks-essentially complex systems of nonlinear regression equations. An LLM can generate text by predicting the next token (word or subword) in a sequential manner (autoregressive generation), given the preceding context. It does so by modelling a conditional probability distribution over the vocabulary; i.e., the likelihood of each token given the context:</p>
<p>$$
P\left(x_{t} \mid x_{&lt;t}\right)=f\left(x_{&lt;t} ; \theta\right)
$$</p>
<p>where $x_{t}$ is the token at time step $t ; x_{&lt;t}$ is the context preceding $x_{t}$, usually including both user prompts and previously generated tokens; $f$ is the model's parameterized function; and $\theta$ represents the model parameters. Given $f$, the model generates text by either sampling from this distribution or directly selecting the token with the highest probability. In the former case, hyperparameters such as temperature can be adjusted to control the diversity of generated text. In the latter case, the model is said to use greedy decoding, and the generated text is deterministic. In evaluating LLMs, it is crucial to properly account for the stochasticity of the model.
These models are based primarily on the transformer architecture, a neural network design that employs self-attention mechanisms to capture contextual relationships between words, phrases, and broader linguistic patterns. Modern LLMs typically contain billions of parameters, enabling them to efficiently learn from vast amounts of textual data. During evaluation, if the model has already been exposed to the test items during training, this is referred to as data contamination. In such cases, the model is more likely to exhibit artificially inflated performance or simply reproduce memorized patterns, rather than revealing its true underlying capabilities or traits.
The training process of LLMs is typically divided into two phases: pre-training and post-training. Pre-training is the phase where LLMs learn to predict the next token, given its preceding context, on a large corpus of text data. This process is unsupervised, as the model does not require explicit labels or annotations to learn the underlying patterns in the data. The model processes Internet-scale text data from diverse sources like books, articles, and websites. By repeatedly predicting the next word in a sentence, the model learns the statistical properties of language and gains large-scale world knowledge. Models that have only undergone the pre-training phase are usually referred to as base models. Post-training, or fine-tuning, is the process of adapting the base models to better follow user instructions, align with human values, or specialize in particular tasks. This stage typically involves training the model on a smaller, human-annotated dataset or incorporating human feedback on the quality of the model outputs. Models that have undergone both phases are often referred to as fine-tuned models, instruction-tuned models, or aligned models.
We interact with LLMs using prompts, which are input instructions to the model. For psychometric evaluation, these prompts can naively be the reformatted versions of test items originally designed for humans, adapted for LLMs to answer. When designing prompts, one should consider differences between base and fine-tuned models. Most public-facing LLMs are fine-tuned, so evaluation research primarily focuses on these models for greater practical relevance.
A key emergent capability of LLMs is in-context learning, which allows models to adapt to new tasks or patterns by conditioning on examples or instructions provided within the input context $x_{&lt;t}$, without modifying model parameters. This property can influence LLM performance in psychometric evaluation. For instance, prompting models to reason step-by-step (e.g., Chain-of-Thought prompting [Wei et al., 2022]) can enhance performance on reasoning tasks, while instructing them to role-play may modulate their exhibited personalities and values.</p>
<h3>2.2 Psychometrics</h3>
<p>Psychometrics, also known as psychological testing, involves the use of tests to measure, understand, or predict behavior by quantifying specific actions or characteristics. These tests rely on samples of behavior, meaning they are not perfect measures and often include errors inherent to sampling. Test items are specific stimuli designed to elicit observable reactions that can be scored or evaluated. Typically, tests are composed of multiple questions or problems as their items, producing explicit data subject to scientific analysis.
A psychological test is a set of items that are designed to measure characteristics of human beings that pertain to behavior [Kaplan and Saccuzzo, 2001]. Behavior measured by tests can be overt (observable actions) or covert (internal thoughts or feelings). Tests may assess past, current, or even predict future behavior. Interpretation of test scores depends on</p>
<p>their context within a distribution. Scales are used to relate raw scores to defined distributions, aiding interpretation. In addition, psychological tests can measure traits-enduring tendencies like shyness or determination-and states, which reflect temporary conditions of individuals.</p>
<p>Psychological testing measures individual differences in various constructs, which are abstract psychological attributes or dimensions that help explain and predict behavior. Two primary categories of such constructs are personality constructs and cognitive constructs [Kaplan and Saccuzzo, 2001]. Personality tests focus on an individual's tendencies and dispositions. These tests measure typical behavior, such as preferences or tendencies to react in certain ways. Cognitive tests evaluate speed, accuracy, or both, with higher scores reflecting better performance.</p>
<p>Two fundamental principles underpin psychometrics: reliability and validity [Raykov and Marcoulides, 2011]. Reliability ensures accuracy, dependability, consistency, or repeatability of the test results. Reliable test results are stable across time, contexts, and raters. Validity confirms the meaningfulness and usefulness of the test results. A valid measure captures the intended construct. Validity is multifaceted; for example, predictive validity might correlate test scores with job performance, while construct validity ensures alignment with theoretical models, such as the Big Five personality traits [Goldberg, 2013].</p>
<p>Other principles include standardization, which provides context to raw scores by comparing individual results to a representative sample, or norm group. Additionally, equivalence and fairness are crucial principles that tests must adhere to. Test bias occurs when test items unintentionally advantage or disadvantage subgroups. Modern psychometrics employs advanced statistical models to identify and revise biased items, ensuring assessments measure the intended construct rather than extraneous factors [Rust and Golombok, 2014].</p>
<h1>2.3 Psychometric evaluation of AI before the era of LLMs</h1>
<p>The idea of applying psychometrics to AI originated in the early decades of AI [Pellert et al., 2024]. Evans [1964] pioneered work in this area by creating a heuristic program that could solve parts of intelligence tests. Subsequent efforts similarly focused on designing AI systems for cognitive tests [Newell, 1973], with the goal of creating systems capable of handling human tasks. This was conceptually aligned with the development of static, task-centric benchmarks in modern AI research [Chen et al., 2021, Hendrycks et al., 2020, Lee et al., 2024e, Liang et al., 2022, Srivastava et al., 2022]. However, criticisms emerged regarding the absence of "hot cognition" in AI, prompting Simon [1963] to propose incorporating emotional aspects into models. By the early 2000s, the concept of "psychometric AI" was explicitly articulated as the pursuit of systems capable of excelling on all established, validated tests of intelligence and mental ability. These included not only conventional IQ tests but also assessments of artistic and literary creativity, mechanical ability, and beyond [Bringsjord and Schimanski, 2003, Pellert et al., 2024]. It was not until the advent of LLMs that the versatility envisioned for "psychometric AI" began to materialize.</p>
<h2>3 Psychometrics for benchmarking principles</h2>
<h3>3.1 Fundamental differences between psychometrics and AI benchmarking</h3>
<p>Benchmarking AI systems superficially resembles psychometrics, particularly Classical Test Theory (CTT) [Crocker and Algina, 1986], as both compile test items to evaluate cognitive capabilities and average the resulting scores. However, closer examination reveals that AI benchmarks differ significantly from modern psychometric approaches [Federiakin, 2025, Wang et al., 2023a]. We outline these key differences in Table 1.</p>
<p>Psychometrics. Psychometrics centers on understanding the psychological constructs and ensuring that tests accurately measure the intended constructs. Grounded in a causal measurement philosophy, this field posits that observed test responses arise from latent psychological constructs [Federiakin, 2025, Markus and Borsboom, 2013]. These constructs may encompass both abilities (e.g., reasoning skills) and personalities (e.g., conscientiousness). The causal framework necessitates rigorous construct definition, requiring traits to be precisely delineated through iterative theory-building and empirical validation. Psychometric test development follows methodical protocols, often structured by frameworks such as Evidence-Centered Design (ECD) [Mislevy et al., 2003]. ECD emphasizes ensuring congruence between test items and theoretical models of the construct, thereby supporting robust inferences about latent traits.</p>
<p>Central to this approach is the prioritization of item quality over quantity. Psychometricians conduct rigorous item analyses to balance precision with practicality, as administering an excessive number of items to participants is often impractical. Advanced statistical models, such as Item Response Theory (IRT) [Embretson and Reise, 2013] and Factor Analysis [Loehlin, 2004], are adopted to estimate latent traits, analyze item performance, and assess model fit. These models require relatively large sample sizes (the number of human participants) to yield stable parameter estimates, as they must disentangle individual differences from measurement error to accurately infer latent traits.</p>
<p>Table 1: Comparison between psychometrics and conventional AI benchmark.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Psychometrics</th>
<th>AI benchmark</th>
</tr>
</thead>
<tbody>
<tr>
<td>Core goal</td>
<td>To measure psychological constructs, to prove that a test measures as intended (validity evidence), and to understand the construct being measured.</td>
<td>To test and compare the task performance of different LLMs. Focuses on ranking models and selecting the best one suited for a specific task.</td>
</tr>
<tr>
<td>Philosophy of measurement</td>
<td>Construct-oriented. Tends towards a causal approach to measurement, where the measured trait is believed to cause the measurement outcomes.</td>
<td>Task-oriented. Leans towards representativism, assuming items exhaust or represent all aspects of the underlying ability.</td>
</tr>
<tr>
<td>Target construct</td>
<td>Personality and ability.</td>
<td>Mostly task-specific abilities.</td>
</tr>
<tr>
<td>Construct definition</td>
<td>Emphasizes clear and detailed definitions of the construct being measured. Agreement on the construct definition is a byproduct of test development.</td>
<td>Often defines constructs implicitly through ad hoc task selection. Construct definitions can be vague.</td>
</tr>
<tr>
<td>Development process</td>
<td>Systematic and rigorous, often following methods like Evidence-Centered Design (ECD). Can be labor-intensive.</td>
<td>Compiles a set of relevant questions or tasks, then performs expert annotation or crowdsourcing to label ground truth answers. Less laborintensive per item.</td>
</tr>
<tr>
<td>Number of items</td>
<td>Can vary, but not necessarily large. Focus is on item quality and relevance to the construct.</td>
<td>Typically consists of an extensive number of questions to cover various aspects of abilities. Reliability increases with test length.</td>
</tr>
<tr>
<td>Sample size</td>
<td>Typically requires a larger sample size of test takers for robust statistical modeling.</td>
<td>Can be applied to evaluate the performance of a single LLM on the benchmark.</td>
</tr>
<tr>
<td>Statistical modeling</td>
<td>Employs advanced and various statistical models like Item Response Theory and Factor Analysis to analyze data, estimate latent abilities, and assess model fit.</td>
<td>Often relies on simple aggregation methods, such as calculating average accuracy across benchmark tasks.</td>
</tr>
<tr>
<td>Result analysis</td>
<td>Ensures the reliability, validity, predictive power, and explanatory power of the test through result analysis and statistical modeling.</td>
<td>Reliability is likely to be high due to the large number of items. However, validity, predictive power, or explanatory power beyond the target task is not a primary concern.</td>
</tr>
</tbody>
</table>
<p>The test results are analyzed to ensure the reliability, validity, predictive power, and explanatory power of the test. Specifically, a well-designed test should: 1) consistently and accurately measure the intended construct, 2) predict performance across a diverse range of related tasks and real-world outcomes, and 3) provide explanatory insight into the observed data. For example, psychometric models often reveal that individual differences across a broad range of cognitive tasks can be captured and explained by a relatively small set of underlying cognitive abilities [Cattell and Horn, 1978].</p>
<p>Benchmark. In contrast, AI benchmarking is driven by pragmatic goals: evaluating and ranking models based on task performance. Unlike psychometrics, validity is not the primary concern. Instead, benchmarks typically emphasize breadth, scalability, and-especially in the era of foundation models-difficulty. This approach reflects a representativist philosophy, where it is assumed that an extensive set of benchmark items collectively captures all relevant aspects of the abilities demanded by the target task [Federiakin, 2025]. However, constructs like reasoning or knowledge are often ambiguously defined and encompass infinitely many aspects. Benchmarks implicitly operationalize these constructs through ad hoc task selection.</p>
<p>The development of AI benchmarks is usually less labor-intensive, especially when compared with psychometrics on a per-item basis. Test items and their corresponding ground truths are typically drawn from existing datasets, expert curation, or crowdsourced contributions. While this process enables scalability, it risks conflating superficial task performance with deeper cognitive capacities. For instance, a benchmark may assess mathematical reasoning</p>
<p>through arithmetic problems without verifying whether models rely on pattern recognition versus symbolic logic [Ahn et al., 2024]. Additionally, LLM benchmarking commonly employs straightforward metrics, such as average accuracy, eschewing the sophisticated latent variable models of psychometrics. This simplicity allows benchmarks to evaluate single models efficiently, bypassing the need for population samples. However, it also limits the depth of insights that can be gleaned from model performance [Federiakin, 2025].
Reliability and stability in benchmarking are primarily achieved through scaling up the test. However, ensuring the quality of each individual item becomes impractical due to the test scale and the rapid pace of model development. For instance, while psychometrics emphasizes the discriminative power of each item, some benchmarks, though initially challenging, are quickly outpaced by continuous model improvements [McIntosh et al., 2024]. Conversely, certain emerging benchmarks are currently too difficult to yield meaningful comparisons [Phan et al., 2025]. Benchmark results are often limited to the specific target task, offering limited generalizability or predictive power across other tasks or real-world applications. These results also pose significant challenges for conducting in-depth, multi-faceted analyses of model capabilities [Wang et al., 2023a].</p>
<h1>3.2 Benchmarking with psychometrics-inspired principles</h1>
<p>Recent LLM evaluation efforts have drawn inspiration from psychometrics and seek to develop benchmarks that adhere to psychometric principles.</p>
<p>Construct-oriented benchmarking. Task-oriented benchmarks often entail vast question sets to capture complex abilities. However, in many cases, the benchmarks either fail to fully represent these abilities due to their infinite manifestations or involve extraneous factors that are irrelevant to the target ability [Wallach et al., 2025, Zhou et al., 2025]. Recent research has drawn inspiration from psychometrics and explored the paradigm of construct-oriented evaluation, seeking its discriminative, predictive, and explanatory power. Federalakin [2025], IliÄ‡ and Gignac [2024] employ factor analysis to explore the latent variables underlying LLM benchmark performance. Their findings reveal a monolithic factor resembling general intelligence or ability. Federalakin [2025] ranks models based on this discovered factor and highlights its unique advantages over raw benchmark scores. In contrast, similar attempts by Burnell et al. [2023] identify three factors-reasoning, comprehension, and core language modeling-that better explain LLM performance across 27 cognitive tasks. Based on it, Zhu et al. [2024a] integrate the three factors into benchmark items to evaluate multifaceted abilities. This discrepancy between the estimated latent factors in the above findings can be attributed to differences in the models and benchmarks employed [Zhou et al., 2025]. Therefore, rather than relying on statistically derived factors, Zhou et al. [2025] propose a theory-driven hierarchical set of general scales for systematic construct-oriented evaluation. These scales are validated to explain what AI systems can do and predict their performance on novel task instances. Peng et al. [2024] present the Tong Test, a value- and ability-oriented framework, for Artificial General Intelligence (AGI) evaluation. This framework is rooted in dynamic embodied physical and social interactions (DEPSI), and can generate an infinite variety of tasks to evaluate key capabilities including values, learning, and cognition.</p>
<p>Psychometrically rigorous benchmark development. Beyond defining and analyzing latent constructs, researchers have developed holistic, psychometrically rigorous methods for benchmark development. Liu et al. [2024e] introduce Evidence-Centered Benchmark Design (ECBD), a framework structuring benchmark creation into five modules-capability, content, adaptation, assembly, and evidence-each requiring justification to ensure validity. Through case studies of prominent LLM benchmarks, they demonstrate ECBD's utility in identifying validity threats. Similarly grounded in psychometrics but with a distinct approach, Fang et al. [2024] propose Psychometrics-Assisted Benchmarking (PATCH), an eight-step process from construct definition to proficiency scoring. When piloted on 8th-grade mathematics, PATCH produced results diverging from traditional benchmarks, offering a more comprehensive evaluation. Building on related principles, Kardanova et al. [2024] adapt Evidence-Centered Design (ECD) to create psychometrically grounded benchmarks. Their application in pedagogy illustrates how this method can reduce data contamination and enhance test interpretability.</p>
<p>Item Response Theory. Item Response Theory (IRT) is a statistical approach in psychometrics that jointly estimates the latent ability of examinees and the properties (difficulty, discrimination) of test items [Embretson and Reise, 2013]. Translating this framework to LLM evaluation enables researchers to infer latent ability scores, assess item informativeness, and perform more efficient evaluation. Recent research leverages principles from IRT to develop adaptive evaluation frameworks. These methods dynamically calibrate item difficulty based on model performance and weighting items by their inferred difficulty, aiming to achieve accurate evaluations with smaller test sizes and more discriminative items [Guinet et al., 2024, Lalor et al., 2024, Polo et al., 2024, Zhuang et al., 2023a,b]. Building on these approaches, Jiang et al. [2024a], Truong et al. [2025] introduce IRT-based benchmarks that involve learning to estimate</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Examples of psychometric tests for LLMs.
item difficulty and learning to generate novel items calibrated to specific difficulty levels. Additionally, research has used IRT-based analyses to explore the alignment between LLM and human response distributions [He-Yueya et al., 2024]. IRT-based evaluations further offer the potential to estimate construct and item parameters on a unified scale, enabling direct comparisons across AI systems and against human norms, even when different test sets are used [Fang et al., 2024, Wang et al., 2023a].</p>
<h1>4 Psychometrics for measuring psychological constructs</h1>
<p>This section delves into the psychological constructs evaluated in LLM Psychometrics. Fig. 2 exemplifies the tests for the involved constructs.</p>
<h3>4.1 Measuring personality constructs</h3>
<p>LLMs exhibit personality constructs that are not explicitly programmed or trained towards. The emergent constructs critically shape LLM behavior [Hagendorff, 2023], with profound implications for both individuals and broader social groups [Bengio et al., 2024]. Measuring these embedded psychological constructs is essential for understanding behavior, identifying biases, and fostering responsible development.
Table 2 and Table 3 summarize the representative personality constructs that have garnered attention in recent research. Researchers typically select constructs based on their relevance to LLM development and deployment, as well as the applicability of these constructs to AI systems. For instance, Li et al. [2024d] argue that emotional variability in LLMs is not a meaningful construct, given that LLMs lack the biological mechanisms underlying emotions. Conversely, personality and values are considered meaningful for LLMs, as they influence user interactions and model outputs [Serapio-GarcÃ­a et al., 2023, Ye et al., 2025a]. This rationale has led to extensive research on personality, values, morality, and attitudes \&amp; opinions, as well as some-though less-focus on other constructs such as career selection</p>
<p>Table 2: Representative LLM personality constructs in LLM psychometrics. The main dimensions or focus of the theories/inventories are listed in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Constructs</th>
<th style="text-align: center;">Theories/inventories</th>
<th style="text-align: center;">Related work</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Personality traits</td>
<td style="text-align: center;">Big Five</td>
<td style="text-align: center;">Ai et al. [2024], Bhandari et al. [2025a,b], BodroÅ¾a et al. [2024], Caron and Srivastava [2022], Dorner et al. [2023], Frisch and Giulianelli [2024], Gupta et al. [2024], Hilliard et al. [2024], Huang et al. [2023a,e, 2024c], Jain et al. [2024], Jiang et al. [2023, 2024b], Karra et al. [2022], Klinkert et al. [2024], KovaÄ et al. [2023], La Cava and Tagarelli [2024], Lee et al. [2024d], Li et al. [2022a,b], Liu et al. [2024a], Lu et al. [2023], Pellert et al. [2024], Petrov et al. [2024], Ren et al. [2024], Romero et al. [2024], Serapio-GarcÃ­a et al. [2023], Shu et al. [2024], Song et al. [2023], SÃ¼hr et al. [2023], Zhang [2024], Zheng et al. [2025], Zou et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HEXACO</td>
<td style="text-align: center;">Barua et al. [2024], BodroÅ¾a et al. [2024], Miotto et al. [2022], Peereboom et al. [2024], Ren et al. [2024], Wang et al. [2025b]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MBTI</td>
<td style="text-align: center;">Ai et al. [2024], Chen et al. [2024a], Cui et al. [2023], Huang et al. [2023c], La Cava and Tagarelli [2024], Lu et al. [2023], Pan and Zeng [2023], Rao et al. [2023], Song et al. [2024b], Zhang et al. [2024a]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dark Triad</td>
<td style="text-align: center;">Barua et al. [2024], Huang et al. [2023e], Lee et al. [2024d], Li et al. [2022a,b], Lu et al. [2023], Peereboom et al. [2024], Romero et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other <br> \&amp; custom</td>
<td style="text-align: center;">Wen et al. [2024b] (reviews); Ai et al. [2024], Jiang et al. [2023], Mao et al. [2024a], Zeng [2024]</td>
</tr>
<tr>
<td style="text-align: center;">Values</td>
<td style="text-align: center;">Schwartz</td>
<td style="text-align: center;">Cahyawijaya et al. [2024], Fischer et al. [2023], Hadar-Shoval et al. [2024], KovaÄ et al. [2023, 2024], Lee et al. [2024a], Li et al. [2024b], Miotto et al. [2022], Pellert et al. [2024], Ren et al. [2024], Rozen et al. [2024], Shen et al. [2024], Yao et al. [2024, 2025a,b], Ye et al. [2025a], Zhang et al. [2023a]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WVS</td>
<td style="text-align: center;">Chiu et al. [2025], Faulborn et al. [2025], Kim and Baek [2024], Yao et al. [2025a]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VSM</td>
<td style="text-align: center;">Kharchenko et al. [2024], KovaÄ et al. [2023], Ren et al. [2024], Ye et al. [2025a], Zhong et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLOBE</td>
<td style="text-align: center;">Karinshak et al. [2024], Li et al. [2024d], Ren et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SVO</td>
<td style="text-align: center;">Zhang et al. [2024c]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other <br> \&amp; custom</td>
<td style="text-align: center;">Biedma et al. [2024], Jiang et al. [2024a], Li et al. [2024d], Liu et al. [2024b], Meadows et al. [2024], Moore et al. [2024], Xu et al. [2023], Ye et al. [2025b], Zhang et al. [2024b]</td>
</tr>
<tr>
<td style="text-align: center;">Morality</td>
<td style="text-align: center;">MFT</td>
<td style="text-align: center;">Abdulhai et al. [2024], Aksoy [2024], Fraser et al. [2022], Ji et al. [2024], MÃ¼nker [2024], Neuman et al. [2025], Nunes et al. [2024], Pellert et al. [2024], Simmons [2023], Tlaie [2024], Yan et al. [2024], Yao et al. [2025b], Zhou et al. [2024a]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ETHICS</td>
<td style="text-align: center;">Albrecht et al. [2022], Jinnai [2024], Karpov et al. [2024], Rodionov et al. [2023], Yu et al. [2023]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DIT</td>
<td style="text-align: center;">Khandelwal et al. [2024], Tanmay et al. [2023]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other <br> \&amp; custom</td>
<td style="text-align: center;">Ahmad and Takemoto [2024], Bonagiri et al. [2024], Chiu et al. [2025], Garcia et al. [2024], Han [2023], Huang et al. [2024a], Jiang et al. [2024a], Jin et al. [2022, 2024b], Jinnai [2024], Kucuk and Kocyigit [2023], Liu et al. [2024d], Marraffini et al. [2024], Meijer et al. [2024], Neuman et al. [2025], Ohashi et al. [2024], Peterson [2025], Ramezani and Xu [2023], Sachdeva and van Nuenen [2025], Scherrer et al. [2023], Seror [2024], Takemoto [2024], Tanmay et al. [2023], Vida et al. [2024], Yuan et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;">Attitudes <br> \&amp; opinions</td>
<td style="text-align: center;">ANES</td>
<td style="text-align: center;">Argyle et al. [2023], Bisbee et al. [2024], Jiang et al. [2022, 2024d], Qi et al. [2024], Sun et al. [2024], Yang et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ATP</td>
<td style="text-align: center;">Hwang et al. [2023], Santurkar et al. [2023], Tjuatja et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLES</td>
<td style="text-align: center;">Ball et al. [2025], Ma et al. [2024b], Qi et al. [2024], von der Heyde et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PCT</td>
<td style="text-align: center;">Azzopardi and Moshfeghi [2024], Bernardelle et al. [2024], Hartmann et al. [2023], RÃ¶ttger et al. [2024], Rozado [2023], Rutinowski et al. [2024], Wright et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other <br> \&amp; custom</td>
<td style="text-align: center;">Ma et al. [2024a] (reviews); Ceron et al. [2024], Chalkidis and Brandl [2024], Dominguez-Olmedo et al. [2024], Durmus et al. [2023], Faulborn et al. [2025], Feng et al. [2023], Geng et al. [2024], Kalinin [2023], Kim and Lee [2023], Kim et al. [2025b], Lee et al. [2024c], Rosenbusch et al. [2023], RÃ¶ttger et al. [2025], Rozado [2023], Sanders et al. [2023], Wu et al. [2023], Xu et al. [2025c]</td>
</tr>
</tbody>
</table>
<p>Table 3: Personality theories and inventories measured in LLM psychometrics and their main dimensions or focus.</p>
<table>
<thead>
<tr>
<th>Theory/inventory</th>
<th>What it measures / dimensions</th>
</tr>
</thead>
<tbody>
<tr>
<td>Big Five</td>
<td>Five broad personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism</td>
</tr>
<tr>
<td>HEXACO</td>
<td>Six personality traits: Honesty-Humility, Emotionality, Extraversion, Agreeableness, Conscientiousness, Openness</td>
</tr>
<tr>
<td>MBTI</td>
<td>Four dichotomies: Extraversion-Introversion, Sensing-Intuition, Thinking-Feeling, Judging-Perceiving</td>
</tr>
<tr>
<td>Dark Triad</td>
<td>Three negative personality traits: Narcissism, Machiavellianism, Psychopathy</td>
</tr>
<tr>
<td>Schwartz</td>
<td>Basic human values: 10 or more values (e.g., Self-Direction, Stimulation, Hedonism, Achievement, Power, Security, Conformity, Tradition, Benevolence, Universalism), typically grouped into four higher-order categories (Openness to Change, Self-Enhancement, Conservation, Self-Transcendence)</td>
</tr>
<tr>
<td>WVS</td>
<td>World Values Survey: Assesses broad cultural values such as traditional vs. secularrational values, survival vs. self-expression values</td>
</tr>
<tr>
<td>VSM</td>
<td>Value Survey Module (often Hofstede): Cultural dimensions such as Power Distance, Individualism, Masculinity, Uncertainty Avoidance, Long-Term Orientation, Indulgence</td>
</tr>
<tr>
<td>GLOBE</td>
<td>Global Leadership and Organizational Behavior Effectiveness: Nine cultural dimensions (e.g., Performance Orientation, Assertiveness, Future Orientation, Humane Orientation, Institutional Collectivism, In-Group Collectivism, Gender Egalitarianism, Power Distance, Uncertainty Avoidance)</td>
</tr>
<tr>
<td>SVO</td>
<td>Social Value Orientation: Measures individuals' preferences regarding resource allocation between oneself and others (e.g., prosocial, individualistic, competitive orientations)</td>
</tr>
<tr>
<td>MFT</td>
<td>Moral Foundations Theory: Five (sometimes six) moral foundations-Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, (Liberty/Oppression)</td>
</tr>
<tr>
<td>ETHICS</td>
<td>Various ethics-related measures assessing moral reasoning, ethical principles, or moral preferences</td>
</tr>
<tr>
<td>DIT</td>
<td>Defining Issues Test: Assesses moral development and reasoning using moral dilemmas</td>
</tr>
<tr>
<td>ANES</td>
<td>American National Election Studies: Political attitudes, beliefs, and behaviors in the U.S.</td>
</tr>
<tr>
<td>ATP</td>
<td>Attitudes Toward Politics: Measures political attitudes and social controversies, often specific to a region or subject</td>
</tr>
<tr>
<td>GLES</td>
<td>German Longitudinal Election Study: Political attitudes, beliefs, and voting behaviors in Germany</td>
</tr>
<tr>
<td>PCT</td>
<td>Political Compass Test: Economic (Left/Right) and Social (Authoritarian/Libertarian) political dimensions</td>
</tr>
</tbody>
</table>
<p>[Hua et al., 2024], motivation [Chiu et al., 2025, Huang et al., 2023e], and mental health [De Duro et al., 2024, Reuben et al., 2024].</p>
<h1>4.1.1 Personality traits</h1>
<p>Personality is the enduring configuration of characteristics and behavior that comprises an individual's unique adjustment to life [APA Dictionary of Psychology]. In the context of LLMs, personality traits relate to model safety, bias, and toxicity [Wang et al., 2025b, Zhang et al., 2024a], and highly determine user experience [Klinkert et al., 2024].
The study of personality traits leads to several prominent theoretical models, each offering unique insights into individual differences, including the Big Five [Goldberg, 2013], HEXACO [Ashton and Lee, 2007], Myers-Briggs Type Indicator</p>
<p>(MBTI) [Myers et al., 1962], and Dark Triad [Paulhus and Williams, 2002]. When applying psychometrics to LLMs, most researchers directly administered the established inventories, such as NEO-PI-R [Costa and McCrae, 2008], BFI [John et al., 1991], and BFI-2 [Soto and John, 2017], for measuring the Big Five traits; HEXACO-60 [Ashton and Lee, 2009] and HEXACO-100 [Lee and Ashton, 2018] inventories for HEXACO traits; MBTI assessment [Myers, 1985] for MBTI types; and Dark Triad Dirty Dozen scale [Jonason and Webster, 2010] for Dark Triad traits.
Given widespread concerns about the practical relevance of these inventories [Ai et al., 2024], others have adapted them for more real-world scenarios. For example, Bhandari et al. [2025a] contextualize the tests in topic-specific or open-domain conversations; Ai et al. [2024] accompany the self-report tests with behavioral tests to examine the personality knowledge of LLMs; Jiang et al. [2023] compile and adapt existing tests into the Machine Personality Inventory; and Mao et al. [2024a] develop the PersonalityEdit inventory using LLMs. However, Peereboom et al. [2024] argue that human-derived traits may not meaningfully apply to LLMs, underscoring the necessity for psychometric theories specifically designed for LLM analysis.</p>
<p>Main findings. Early models such as GPT-3 score above human averages on Dark Triad scales [Li et al., 2022a,b, Romero et al., 2024]. Even after safety tuning, some models retain certain "dark qualities" [Li et al., 2022b], suggesting that these patterns may be deeply rooted in the pretraining data. More advanced LLMs indicate better level of alignment. They usually demonstrate high Openness, Extraversion, and Agreeableness traits, while low Neuroticism, on the Big Five personality tests [Bhandari et al., 2025a, Zou et al., 2024]. The results align with their design as assistive and helpful entities with emotional stability and engaging personality [Bhandari et al., 2025a, La Cava and Tagarelli, 2024]. Within the MBTI framework, most proprietary LLMs are classified as ENFJ or INFJ types [Huang et al., 2023c, La Cava and Tagarelli, 2024], indicating tendencies toward helpfulness, idealism, and planning capabilities. Personality differences between models are noteworthy. Comparative studies reveal that LLMs from different generations and training methodologies display unique combinations of personality characteristics [Bhandari et al., 2025a, Karra et al., 2022]. LLM personalities also vary across contexts. The same model may display different personality traits across conversational topics [Song et al., 2023, Zou et al., 2024]. This variability challenges the applicability of human personality theories that assume relative trait stability. Multiple studies suggest that prompt design and system instructions substantially influence LLM personality expression [Caron and Srivastava, 2022, Song et al., 2023].</p>
<p>Comparing personality models and suggesting use cases. The Big Five offers robust empirical foundations and is the most prominent model for personality assessment. However, it is insufficient for predicting comprehensive psychological outcomes [Feher and Vernon, 2021]. HEXACO extends the model by adding Honesty-Humility and provides nuanced insights into moral traits, making it preferable for LLM application contexts demanding integrity [Wang et al., 2025b]. In contrast, MBTI [Myers et al., 1962] categorizes 16 types via dichotomous scales. While popular in career counseling and team-building for its accessibility, its typological approach oversimplifies personality and lacks strong empirical support, limiting its utility in high-stakes decisions [Pittenger, 2005]. The Dark Triad [Paulhus and Williams, 2002] diverges by focusing on maladaptive traits, making it useful in identifying toxic behaviors in LLMs [Barua et al., 2024].</p>
<h1>4.1.2 Values</h1>
<p>Values are enduring beliefs that guide behavior and decision-making, reflecting what is important and desirable to an individual or group [Schwartz, 1992]. Values offer a powerful lens for understanding LLM behavior. For example, Ye et al. [2025a] show how different values contribute to the safety of LLMs; Liu et al. [2024b] reveal that different spiritual values affect LLMs in social-fairness scenarios; and Sorensen et al. [2024b] demonstrate that standard value alignment reduces distributional pluralism in LLM outputs.
A growing body of research has applied diverse value theories and instruments to assess the value orientations of LLMs. Schwartz's Value Theory, with its ten basic values and higher-order dimensions, is the most widely used, often via the Schwartz Value Survey (SVS) or Portrait Values Questionnaire (PVQ) [Schwartz, 1992, Schwartz et al., 2001]. Adaptations and contextualized prompts are increasingly common to better suit LLMs' operational context [Ren et al., 2024, Shen et al., 2024]. The World Values Survey (WVS) has also been used to probe LLMs' alignment with societal-level value dimensions [Haerpfer et al., 2022, Kim and Baek, 2024]. Cross-cultural frameworks like Hofstede's Values Survey Module (VSM) and the GLOBE study extend this analysis to workplace and leadership-related cultural dimensions, with both direct and adapted inventories employed [Hofstede, 1984, House et al., 2004, Ren et al., 2024]. Social Value Orientation (SVO) frameworks, using tools like the SVO Slider Measure, focus on LLMs' prosocial versus proself tendencies [Murphy et al., 2011, Zhang et al., 2024c]. Additionally, researchers have developed localized or custom inventories to capture region-specific or topical values, and some propose novel, bottom-up value taxonomies for LLMs using psycholexical data [Biedma et al., 2024, Meadows et al., 2024, Xu et al., 2023, Ye et al., 2025b].</p>
<p>Collectively, these studies reveal both the adaptability of human value frameworks to LLMs and the need for tailored approaches.</p>
<p>Main findings. Research indicates that LLMs display distinct and systematic value patterns. According to Schwartz's Value Theory, LLMs tend to prioritize Self-Transcendence and Conservation. They exhibit stronger inclinations toward Universalism, Benevolence, Conformity, and Security, while opposing Power and Achievement [Fischer et al., 2023, Hadar-Shoval et al., 2024, Rozen et al., 2024, Zhang et al., 2023a]. WVS surveys further suggest that LLMs generally prefer Self-Expression values over Survival values [Chiu et al., 2025]. Studies using the VSM and GLOBE frameworks emphasize LLMs' strong focus on Humane and Performance Orientation, with moderate Assertiveness [Li et al., 2024b]. Additionally, when assessed through SVO, advanced LLMs predominantly show Prosocial tendencies [Zhang et al., 2024c].
Different models exhibit varied value orientations [Chiu et al., 2025, Duan et al., 2023, KovaÄ et al., 2024, Li et al., 2024b, Xu et al., 2023]. Versions within the same model family show evolutionary trends in values, potentially influenced by safety alignment, capability advancement, and shifting societal expectations [Duan et al., 2023, Kim and Baek, 2024, Moore et al., 2024]. Generally, larger models align more closely with desirable human values [Jiang et al., 2024a, Kim and Baek, 2024, Shen et al., 2024, Zhong et al., 2024].
Cross-cultural research suggests that LLMs may embody a blend of cultural values, integrating perspectives from diverse backgrounds [KovaÄ et al., 2023]. However, they generally exhibit a tendency toward Western liberal values [Kim and Baek, 2024]. LLMs can display different values based on profiling prompts [Karinshak et al., 2024, Kharchenko et al., 2024, Zhong et al., 2024]. The context-dependency of their values is concerning and challenges the stability assumptions in human value theories [Chiu et al., 2025, KovaÄ et al., 2023, Meadows et al., 2024, Moore et al., 2024, Shen et al., 2024, Xu et al., 2023].</p>
<p>Comparing value theories and suggesting use cases. Schwartz emphasizes universal human motivations; WVS examines societal-level value shifts; VSM and GLOBE address cross-cultural dimensions, with GLOBE emphasizing leadership. SVO, in contrast, focuses on individual-level social preferences, i.e., prosocial vs. proself. While Schwartz and SVO overlap in analyzing individual motivations, the former is broader, and the latter narrowly focuses on decisionmaking in resource allocation. WVS, VSM, and GLOBE share cultural-level dimensions but differ in focus-WVS on modernization, VSM on workplace-related cultural differences, and GLOBE on leadership and organizational practices. For evaluating individual values in LLMs, use Schwartz for a comprehensive understanding of universal motivations or SVO for interpersonal decision-making studies. In contrast, choose WVS for evaluating broad societal values in LLMs. When evaluating cultural dimensions in LLMs, use VSM for workplace cultural differences or GLOBE for leadership and organizational practices. In addition, Schwartz's theory bridges individual and societal-level values, making it versatile for mixed-focus studies.</p>
<h1>4.1.3 Morality</h1>
<p>Morality is the categorization of intentions, decisions and actions into those that are proper, or right, and those that are improper, or wrong [Long and Sedley, 1987]. It is crucial to conduct moral assessments of LLMs to ensure their ethical deployment. A large body of research applies the Moral Foundations Theory (MFT) [Graham et al., 2009], primarily through the Moral Foundations Vignettes (MFVs) [Clifford et al., 2015], the Moral Foundations Questionnaire (MFQ) [Graham et al., 2009], the MFQ-2 [Atari et al., 2023], and the Moral Foundations Dictionary (MFD) [Graham et al., 2009]. These papers investigate model bias, alignment with political/moral ideologies, and variation in personal or cultural values, using controlled prompt tests [Abdulhai et al., 2024, Nunes et al., 2024, Tlaie, 2024], persona-driven exploration [MÃ¼nker, 2024], or evaluating internal moral coherence [Nunes et al., 2024]. Other prominent moral theories and instruments include Kohlberg's Theory via the Defining Issues Test (DIT) [Kohlberg, 1964], the Consequentialist-Deontological distinction [Beauchamp, 2001], the PEW 2013 Global Attitudes Survey [Center, 2013], and localized moral theories [Liu et al., 2024d, Takeshita et al., 2023]. Specialized datasets are also curated for scalable and comprehensive morality evaluation [Hendrycks et al., 2021, Jin et al., 2024b, Jinnai, 2024, Marraffini et al., 2024]</p>
<p>Main findings. LLMs are generally characterized by a rationalist and consequentialist focus, often prioritizing harm minimization and fairness [Neuman et al., 2025]. Despite that, they show divergence in ethical reasoning [Neuman et al., 2025] and moral preferences [Bonagiri et al., 2024, Jin et al., 2024b, Meijer et al., 2024, Tanmay et al., 2023]. In some aspects, most LLMs align with human moral standards [Nunes et al., 2024, Takemoto, 2024, Tanmay et al., 2023], which may be attributed to their extensive exposure to conventional ethical values during training. On the other hand, some research presents a less optimistic view, uncovering significant deviations of LLMs from human moral preferences [Ahmad and Takemoto, 2024, Marraffini et al., 2024, Vida et al., 2024]. Regarding the underlying mechanisms of</p>
<p>LLMs' moral reasoning, Ji et al. [2024], Nunes et al. [2024], Simmons [2023] suggest that LLMs primarily exhibit imitation rather than genuine conceptual understanding.</p>
<p>Comparing moral theories/instruments and suggesting use cases. Instruments based on MFT are well-suited for measuring LLMs' alignment with intuitive moral principles across cultural or political dimensions. Tools like MFQ, MFQ-2, MFVs, and MFD are particularly effective when investigating ideological leanings, bias, or internal consistency in abstract vs. contextual moral judgments [Nunes et al., 2024]. In contrast, Kohlberg's DIT explores structured moral reasoning, offering a developmental perspective on how LLMs process ethical dilemmas [Khandelwal et al., 2024, Tanmay et al., 2023]. DIT is valuable for longitudinal studies and evaluating whether models show higher-level moral reasoning (e.g., post-conventional reasoning). Frameworks such as the Consequentialist-Deontological distinction and global surveys like the PEW 2013 study are useful for categorizing ethical tendencies or analyzing cultural homogeneity [Meijer et al., 2024, Neuman et al., 2025]. Localized tools (e.g., CMD, JCM) help assess cultural alignment [Liu et al., 2024d, Takeshita et al., 2023]. Meanwhile, datasets like ETHICS provide morally charged scenarios for detecting normative behavior and fine-tuning LLM responses in high-stakes ethical domains [Hendrycks et al., 2021, Jin et al., 2024b, Marraffini et al., 2024].</p>
<h1>4.1.4 Attitudes and opinions</h1>
<p>Attitudes are always attitudes about something. This implies three necessary elements: first, there is the object of thought, which is both constructed and evaluated. Second, there are acts of construction and evaluation. Third, there is the agent, who is doing the constructing and evaluating. We can therefore suggest that, at its most general, an attitude is the cognitive construction and affective evaluation of an attitude object by an agent [Bergman, 1998]. We use the term "attitude" to encompass both attitudes and opinions, following [Bergman, 1998, Ma et al., 2024a]. Most research on LLM attitudes examines political attitudes and public opinions [Ma et al., 2024a], which are key cognitive and behavioral foundations in human society and closely tied to model fairness, credibility, and social impact [Durmus et al., 2023, Hartmann et al., 2023, Lee et al., 2024c, Sanders et al., 2023, Santurkar et al., 2023]. For a broader discussion on bias and fairness in LLMs, we refer readers to [Gallegos et al., 2024, Ranjan et al., 2024].
Researchers assess LLMs' political attitudes using standardized questionnaires and scales from political science and social psychology. US-focused instruments include the American National Election Studies (ANES) [Qi et al., 2024] and the American Trends Panel (ATP) [Santurkar et al., 2023]. For cross-national comparisons, studies use tools like the German Longitudinal Election Study (GLES) [Ball et al., 2025, Ma et al., 2024b, von der Heyde et al., 2024]. The Political Compass Test (PCT) is also widely used to map LLMs within multidimensional political spectrums [Azzopardi and Moshfeghi, 2024, Bernardelle et al., 2024, Hartmann et al., 2023, RÃ¶ttger et al., 2024].
Other survey instruments include the General Social Survey (GSS) [Kim and Lee, 2023], the American Community Survey (ACS) [Dominguez-Olmedo et al., 2024], the Canadian Election Study (CES) [Sanders et al., 2023], the European Social Survey (ESS) [Geng et al., 2024], the Survey of Russian Elites [Kalinin, 2023], and the Supreme Court Case Political Evaluation (SCOPE) [Xu et al., 2025c]. Additionally, researchers have developed specialized datasets and tailored tools for LLMs, such as OpinionQA [Santurkar et al., 2023], a comprehensive dataset based on ATP surveys; IssueBench [RÃ¶ttger et al., 2025], a benchmark covering controversial issues with multiple response formats; GlobalOpinionQA [Durmus et al., 2023], which extends political opinion evaluation to cross-cultural contexts; and OpinionGPT [Haller et al., 2023], a web tool that demonstrates how input data biases influence model outputs.</p>
<p>Main findings. Pretraining data inherently contains socially biased opinions and perspectives, which can amplify political polarization in LLMs [Feng et al., 2023, Xu et al., 2025c]. Most studies identify a misalignment between LLM outputs and human opinions [Dormuth et al., 2025, Santurkar et al., 2023, von der Heyde et al., 2024, Yang et al., 2024], with many concluding that LLMs exhibit a left-leaning political bias [Bernardelle et al., 2024, Ceron et al., 2024, Hartmann et al., 2023, Ma et al., 2024b, Rozado, 2023]. Cross-cultural comparative studies further reveal Western-centric tendencies, demonstrating limited understanding of non-English political perspectives or multi-partisan systems [Qi et al., 2024]. These findings suggest structural limitations in the models' political understanding. It is also shown that the degree and manifestation of bias vary significantly across contexts and domains. For example, politicalelectoral propositions exhibit different bias patterns than socioeconomic issues like climate change [Wu et al., 2023]. In contrast, some researchers offer more optimistic perspectives, emphasizing LLMs' potential to simulate population-level opinions and supplement traditional survey methods [Argyle et al., 2023, Bisbee et al., 2024, Dominguez-Olmedo et al., 2024, Jiang et al., 2024d, Kalinin, 2023, Sanders et al., 2023, Sun et al., 2024, Wu et al., 2023]. They suggest that with appropriate prompt design, calibration methods, and fine-tuning, LLMs can generate opinion distributions closely approximating human group distributions [Jiang et al., 2022, Wu et al., 2023]. Such advancements indicate promising directions for using LLMs in opinion research, though their current limitations and biases must still be acknowledged.</p>
<p>Comparing attitude measurement tools and suggesting use cases. The ANES offers the most comprehensive framework for American political contexts, making it ideal for studying model performance on U.S. social issues [Jiang et al., 2022]. Cross-national surveys and datasets such as the ESS, GLES, and GlobalOpinionQA provide broader cultural perspectives across diverse sociopolitical environments [Ball et al., 2025, Durmus et al., 2023, Geng et al., 2024, Ma et al., 2024b, von der Heyde et al., 2024]. The ATP is better suited for researching model responses to social controversies [Santurkar et al., 2023], while the PCT serves as a theory-driven political spectrum measurement tool, offering simplified yet intuitive insights into LLM political positioning [RÃ¶ttger et al., 2024]. Other surveys have distinct regional and sociopolitical focuses: the CES examines political participation in North American multicultural democracies [Sanders et al., 2023]; Survey of Russian Elites provides insights into non-Western political perspectives [Kalinin, 2023]; the SCOPE delivers in-depth assessments of judicial attitudes [Xu et al., 2025c]; and the ACS serves as a tool for researching socioeconomic issues and group representation [Dominguez-Olmedo et al., 2024].</p>
<h1>4.2 Measuring cognitive constructs</h1>
<p>Traditional NLP benchmarks are insufficient to capture and analyze emergent cognitive constructs [Hagendorff et al., 2024, Ying et al., 2025a]. To address this gap, researchers are adapting related psychometric techniques to evaluate LLM abilities. Hagendorff et al. [2024] introduce the concept of machine psychology, reviewing emergent LLM capabilities and advocating for psychometric evaluation. They categorize these cognitive constructs into four key aspects: heuristics and biases, social interactions, psychology of language, and learning and cognitive capabilities. Structured based on this taxonomy, Table 4 summarizes the representative cognitive constructs measured in recent research.</p>
<h3>4.2.1 Heuristics and biases</h3>
<p>Heuristics and biases are mental shortcuts that simplify decision-making but can introduce systematic errors [Tversky and Kahneman, 1974]. Recent research employs psychometric tools to evaluate rationality and biases in LLM outputs and offer theoretical explanations for these biases.</p>
<p>Early studies, such as Binz and Schulz [2023], found that GPT-3 performs comparably to humans on cognitive ability tests, exhibiting biases like framing and anchoring. More advanced models, such as GPT-4, show fewer System 1 (intuitive) errors and more System 2 (deliberative) reasoning, though biases persist [Hagendorff et al., 2023]. Largescale evaluations now cover a wide range of biases-including conjunction fallacy, unwarranted beliefs, and loss aversion-using synthetic scenarios and multi-agent dialogue [Malberg et al., 2024, Xie et al., 2024b]. Multimodal models are also being assessed for similar patterns [Schulze Buschoff et al., 2025].</p>
<p>To explain these findings, researchers draw on dual-process theory, linking LLM reasoning to intuitive and deliberative modes [Hagendorff et al., 2023, Yax et al., 2024]. Other frameworks highlight LLMs' lack of metacognitive abilities, leading to "metacognitive myopia"-the inability to monitor or control reasoning processes, which perpetuates systematic biases [Scholten et al., 2024]. Additional perspectives use cognitive dissonance and elaboration likelihood theories to interpret inconsistencies [Sundaram and Alwar, 2024], while mechanistic approaches leverage influence graphs and Shapley values to trace the origins of biases in model training [Shaikh et al., 2024].</p>
<p>Main findings. By administering psychometrics to LLMs, studies consistently find that LLMs exhibit cognitive biases superficially similar to humans, such as anchoring, framing, and the conjunction fallacy. Large-scale tests, such as [Echterhoff et al., 2024, Malberg et al., 2024, Xie et al., 2024b], systematically categorize these biases and enable scalable evaluations. Some note that newer, larger, chain-of-thought-enabled models exhibit improved reasoning and bias mitigation [Hagendorff et al., 2023, Tang and Kejriwal, 2024], while others argue that increasing model complexity without deliberate bias mitigation strategies can amplify existing biases [Kumar et al., 2024]. In addition, researchers explore mechanistic interpretability [Shaikh et al., 2024] and present theoretical explanations from cognitive psychology and reasoning theories [Hagendorff et al., 2023, Scholten et al., 2024, Yax et al., 2024]. While LLMs exhibit dual-process reasoning dynamics reminiscent of human cognition [Hagendorff et al., 2023], detailed analysis reveals differences between LLM and human reasoning [Yax et al., 2024].</p>
<h3>4.2.2 Social interactions</h3>
<p>Researchers apply psychometric tools from social and developmental psychology to assess LLMs' capabilities in navigating social dynamics. Related evaluation focuses on interconnected dimensions such as Theory of Mind (ToM), Emotional Intelligence (EI), and Social Intelligence (SI).</p>
<p>Theory of Mind (ToM). ToM is the ability to attribute mental states such as beliefs, intentions, and knowledge to others [Premack and Woodruff, 1978]. Advanced LLMs can simulate ToM-like reasoning under certain conditions,</p>
<p>Table 4: Representative LLM cognitive constructs in LLM psychometrics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Aspects</th>
<th style="text-align: center;">Sub-aspects</th>
<th style="text-align: center;">Related work</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Heuristics <br> \&amp; biases</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Abramski et al. [2023], Ando et al. [2023], Bai et al. [2024], Binz and Schulz [2023], Castello et al. [2024], Chen and Eger [2025], Coda-Forno et al. [2024], Echterhoff et al. [2024], Hagendorff et al. [2023], Hayes et al. [2024], Healey et al. [2024], Kumar et al. [2024], Macmillan-Scott and Musolesi [2024], Malberg et al. [2024], Momennejad et al. [2023], Ranaldi and Zanzotto [2024], Saeedi et al. [2025], Scholten et al. [2024], Schulze Buschoff et al. [2025], Shah et al. [2024], Shaikh et al. [2024], Sundaram and Alwar [2024], Talboy and Fuller [2023], Tang and Kejriwal [2024], Thorstad [2023], Xie et al. [2024a,b], Yax et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;">Social interactions</td>
<td style="text-align: center;">Theory of Mind</td>
<td style="text-align: center;">Ma et al. [2023], Mao et al. [2024b], SarÄ±taÅŸ et al. [2025] (reviews); Amirizaniani et al. [2024], Chan et al. [2024], Chen et al. [2024c,d], Gandhi et al. [2023], He et al. [2023], Holterman and Deemter [2023], Hou et al. [2024], Jamali et al. [2023], Jin et al. [2024a], Jones et al. [2024], Kim et al. [2025a], Kosinski [2023a,b], Leer et al. [2023], Li et al. [2023b], Lin et al. [2024], LorÃ¨ et al. [2024], Moghaddam and Honey [2023], Nickel et al. [2024], Pi et al. [2024], Riemer et al. [2025], Sadhu et al. [2024], Sap et al. [2022], Sarangi et al. [2025], Sclar et al. [2023, 2024], Shapira et al. [2023a], Shinoda et al. [2025], Soubki et al. [2024], Strachan et al. [2024a,b], Street et al. [2024], Tan et al. [2024], Tang and Belle [2024], Ullman [2023], van Duijn et al. [2023], Wilf et al. [2023], Xu et al. [2024a, 2025a], Yang et al. [2025a], Yu et al. [2025], Zhang et al. [2025], Zhou et al. [2023a], Zhu et al. [2024c].</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Emotional Intelligence</td>
<td style="text-align: center;">Raj [2024], Sorin et al. [2024] (reviews); Chen et al. [2024b], Elyoseph et al. [2023], Hu et al. [2025], Huang et al. [2023b, 2024b], Lee et al. [2024f], Li et al. [2023a, 2024e], Paech [2023], Patel and Fan [2023], Sabour et al. [2024], Schaaff et al. [2023], Vzorinab et al. [2024], Wang et al. [2023b], Welivita and Pu [2024], Zhao et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Social Intelligence</td>
<td style="text-align: center;">Guo et al. [2023a], KovaÄ et al. [2021, 2024], Leng and Yuan [2023], Liang et al. [2024], Liu et al. [2024c], Mathur et al. [2024], MittelstÃ¤dt et al. [2024], Mou et al. [2024], Schoenegger et al. [2025], Shapira et al. [2023b], Wang et al. [2024a,d], Xu et al. [2024b], Zadeh et al. [2019], Zhou et al. [2024b]</td>
</tr>
<tr>
<td style="text-align: center;">Psychology of language</td>
<td style="text-align: center;">Language comprehension</td>
<td style="text-align: center;">Chang and Bergen [2024] (reviews); Amouyal et al. [2024], Arehalli et al. [2022], BojiÄ‡ et al. [2023], Duan et al. [2024a,b], He et al. [2024], Hong et al. [2023], Hu and Levy [2023], Hu et al. [2023], Huff and UlakÃ§i [2024], Ide et al. [2024], Lee et al. [2024b,b], Li et al. [2024a], Qiu et al. [2024], Ruis et al. [2023], Seals and Shalin [2023], Steuer et al. [2023], Sun and Wang [2024], Tian et al. [2024], Wang et al. [2024b], Wilcox et al. [2021], Zhou et al. [2023b]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Language generation</td>
<td style="text-align: center;">Bellemare-Pepin et al. [2024], Boussioux et al. [2024], Cai et al. [2024], Chakrabarty et al. [2024], Hubert et al. [2023], Lee and Chung [2024], Miaschi et al. [2024], Orwig et al. [2024], Seals and Shalin [2023], Stevenson et al. [2022], Tang and Kejriwal [2024], Tian et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Language acquisition</td>
<td style="text-align: center;">Frank [2023b], Riva et al. [2024], Shah et al. [2024], Steuer et al. [2023]</td>
</tr>
<tr>
<td style="text-align: center;">Learning and cognitive capabilities</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Niu et al. [2024] (reviews); Coda-Forno et al. [2024], Dayan et al. [2024], GalatzerLevy et al. [2024], Lv et al. [2024], Sartori and OrrÃº [2023], Song et al. [2024a], Wang et al. [2024e,e], Webb et al. [2022], Wu et al. [2025], Zeng et al. [2024c], Zhang et al. [2023b]</td>
</tr>
</tbody>
</table>
<p>which prompts questions about how these behaviors arise and how robustly they generalize. Foundational studies apply classic psychometricsâ€”false belief tasksâ€”to evaluate ToM in LLMs. [Kosinski, 2023a,b] report that GPT-3.5 and GPT-4 perform at levels matching or exceeding those of children in structured ToM tasks, suggesting the spontaneous emergence of ToM-like behavior as models scale in size. Through mechanistic interpretation, Jamali et al. [2023] offer evidence of parallels between the LLM embeddings and neurons in the human brain.
However, these claims have been challenged. Holterman and Deemter [2023], Shapira et al. [2023a], Ullman [2023] show that minor task changes often cause LLMs' performance to drop, suggesting reliance on brittle heuristics rather than true semantic understanding. To address this, researchers have developed more robust benchmarks and evaluation protocols featuring procedurally generated, higher-order, or broader ToM tasks [Chen et al., 2024d, Gandhi et al., 2023,</p>
<p>He et al., 2023, Shinoda et al., 2025, Xu et al., 2024a]. These benchmarks frequently reveal significant declines in performance on complex tasks, such as 6th-order belief attribution [Street et al., 2024] or faux pas detection [Strachan et al., 2024a].
Some argue these failures stem from a lack of general commonsense reasoning rather than an inability to represent mental states [Pi et al., 2024]. Riemer et al. [2025] further suggest distinguishing between literal and functional ToM, noting that current benchmarks inadequately assess functional ToM. While newer models continue to improve on structured ToM tasks [Strachan et al., 2024a, Street et al., 2024], they remain inconsistent in open-ended, adversarial, or pragmatic reasoning scenarios [Nickel et al., 2024, Sclar et al., 2024, Yu et al., 2025]. Comparisons with human baselines show that LLMs can approximate ToM behavior in narrow contexts but still fall short of general human-like social cognition [Jones et al., 2024, Strachan et al., 2024a, van Duijn et al., 2023]. Recent work has also extended ToM evaluation to multimodal settings [Chen et al., 2024c, Jin et al., 2024a, Strachan et al., 2024b] and multi-agent interactions [Li et al., 2023b].</p>
<p>Main findings in ToM. LLMs exhibit psychometrically measurable ToM-like reasoning, especially when appropriately prompted or structured, but current evidence suggests these capabilities depend on surface-level linguistic cues and lack robustness. The proficiency of LLMs in ToM remains a contentious issue. Interested readers are referred to recent comprehensive reviews on ToM in LLMs [Ma et al., 2023, Mao et al., 2024b, SarÄ±taÅŸ et al., 2025].</p>
<p>Emotional Intelligence (EI). EI is the subset of social intelligence that involves the ability to monitor one's own and others' feelings and emotions, to discriminate among them and to use this information to guide one's thinking and actions [Salovey and Mayer, 1990].
Recent work introduces benchmarks to assess EI in LLMs using structured, theory-driven tasks. Tools like EmoBench [Sabour et al., 2024] and EQ-Bench [Paech, 2023] operationalize constructs such as emotional understanding and regulation, often referencing frameworks like Mayer-Salovey-Caruso Emotional Intelligence Test. Other studies adapt established psychometric instruments, including Situational Evaluation of Complex Emotional Understanding (SECEU) [Wang et al., 2023b], the Levels of Emotional Awareness Scale (LEAS) [Elyoseph et al., 2023], the Toronto Alexithymia Scale (TAS-20) and Empathy Quotient (EQ-60) [Patel and Fan, 2023], among others [Huang et al., 2024b]. Advanced models like GPT-4 often match or surpass human baselines in emotional awareness and understanding [Elyoseph et al., 2023, Patel and Fan, 2023, Wang et al., 2023b], though they lack deep reflexive analysis of emotional experience and motivation [Vzorinab et al., 2024]. Recent benchmarks also extend EI evaluation to multimodal settings [Hu et al., 2025].</p>
<p>Main findings in EI. Advanced LLMs generally perform on par with or better than humans in tasks of EI [Elyoseph et al., 2023, Patel and Fan, 2023], despite evident limitations in several areas, such as displaying artificial or mechanical patterns when expressing empathy [Lee et al., 2024f], the deep reflexive analysis of emotional experiences [Vzorinab et al., 2024], and the misalignment with human emotional behaviors [Huang et al., 2023b].</p>
<p>Social Intelligence (SI). SI is the ability to understand and manage people [Thorndike and Stein, 1937]. SI in LLMs determines how well these models interpret and respond to social situations. Psychometric tools applied to LLMs yield mixed results. MittelstÃ¤dt et al. [2024] use Situational Judgment Tests (SJTs) and find some LLMs outperform humans in expert-rated social appropriateness. However, LLMs still struggle with implicit social understanding: Shapira et al. [2023b] show LLMs perform poorly on faux pas tests, and Xu et al. [2024b] find superficial friendliness often leads to errors in the Situational Evaluation of SI (SESI). The AgentSense benchmark further highlights LLMs' limitations in complex social interactions, especially regarding high-level needs and private information [Mou et al., 2024].
Recent work expands to multi-agent environments. SOTOPIA [Zhou et al., 2024b] and SOTOPIA- $\pi$ [Wang et al., 2024d] simulate cooperative and competitive social contexts, while the STSS benchmark [Wang et al., 2024a] evaluates SI through task-oriented simulations focused on outcomes and goals. The CogMir framework [Liu et al., 2024c] finds LLMs and humans are similarly consistent in irrational and prosocial decisions under uncertainty. DeSIQ [Guo et al., 2023a] extends SI evaluation to multimodal settings. Critically, KovaÄ et al. [2024] argue current benchmarks lack developmental grounding and propose the SocialAI school as a more comprehensive framework for studying LLM-based SI.</p>
<p>Main findings in SI. LLMs show measurable competence in rule-based, socially appropriate behavior, particularly in structured environments. They perform well on: 1) following predefined social norms [MittelstÃ¤dt et al., 2024]; 2) completing interactional goals in multi-agent settings [Wang et al., 2024d, Zhou et al., 2024b]; 3) replicating human-like prosocial decisions [Liu et al., 2024c]; and 4) being persuasive, even better than human persuaders [Schoenegger et al., 2025]. Weaker performance is noted in tasks where superficial friendliness causes errors [Xu et al., 2024b],</p>
<p>understanding high-level growth needs is essential [Mou et al., 2024], and LLMs must implicitly describe social situations [Shapira et al., 2023b].</p>
<h1>4.2.3 Psychology of language</h1>
<p>Psycholinguistics, a subfield of psychology, explores how humans comprehend, generate, and acquire language [Carroll, 1986]. Insights from this discipline help evaluate how LLMs process language and mirror human linguistic features.</p>
<p>Language comprehension. LLM language comprehension is evaluated across multiple linguistic levels-sound, word, syntax, meaning, and discourse. Duan et al. [2024a] introduce a benchmark of 10 psycholinguistic tests to assess these comprehensively. Many studies focus on specific aspects, such as syntactic and semantic processing [Arehalli et al., 2022, Wang et al., 2024b, Wilcox et al., 2021, Wolfman et al., 2024]. Surprisal, a measure of word predictability, is widely used to model processing difficulty in syntactic ambiguities and hierarchical structures [Arehalli et al., 2022, Li et al., 2024a, Wilcox et al., 2021]. Results are mixed: for example, GPT-2-XL shows human-like competence in sound-gender association and implicit causality, but not in sound-shape association [Duan et al., 2024b]. Other evaluation tasks include grammaticality judgment [Ide et al., 2024, Qiu et al., 2024], pragmatic inference [BojiÄ‡ et al., 2023, Hu et al., 2023, Ruis et al., 2023], argument role processing [Lee et al., 2024b], and discourse comprehension [Duan et al., 2024a]. Advanced models like GPT-4 often match or surpass humans in pragmatic reasoning [BojiÄ‡ et al., 2023] and grammaticality judgment [Dentella et al., 2024], though performance varies with prompt format [Hu and Levy, 2023] and implicature understanding remains limited [Ruis et al., 2023].</p>
<p>Language generation. A rich line of research evaluates the creativity of LLM language generation [Bellemare-Pepin et al., 2024, Boussioux et al., 2024, Chakrabarty et al., 2024, Hubert et al., 2023, Lee and Chung, 2024, Orwig et al., 2024, Stevenson et al., 2022, Tang and Kejriwal, 2024], using tests like Guilford's Alternative Uses Test (AUT) [Guilford et al., 1978] and the Torrance Test of Creative Thinking (TTCT) [Torrance, 1966]. Early models such as GPT-3 lack originality and novelty, while advanced LLMs like GPT-4 surpass the human average in creativity. Notably, Tang and Kejriwal [2024] find LLMs underperform in divergent creativity (e.g., novel uses for familiar objects) but can rival humans in open-ended creative writing. Cai et al. [2024] show that ChatGPT produces human-like responses in most of 12 psycholinguistic tests, but AI-generated analogies often lack human-like linguistic properties [Seals and Shalin, 2023]. LLM-generated stories tend to be positive and lack suspense or narrative diversity compared to human writing [Tian et al., 2024]. Linguistic profiling has also been used to characterize the task-specific language abilities of LLMs [Miaschi et al., 2024].</p>
<p>Language acquisition. Work on developmental plausibility examines whether LLMs replicate stages of language learning analogous to those in children [Shah et al., 2024, Steuer et al., 2023]. It is shown that regardless of model size, the developmental trajectories of pretrained language models consistently exhibit a window of maximal alignment with human cognitive development [Shah et al., 2024]. Another study by Frank [2023b] draws inspiration from human language development to explain the data inefficiency of LLMs; the relative efficiency of human language acquisition is possibly due to pre-existing conceptual knowledge, multimodal grounding, and the interactive, social nature of their input.</p>
<p>Main findings. Early foundations that evaluate language models based on BERT and LSTM link computational linguistics and psycholinguistic mechanisms [Arehalli and Linzen, 2020, Ettinger, 2020, Futrell et al., 2019]; these models generally fall short in diverse psycholinguistic tasks. Recent studies extend evaluations to LLMs and broader tasks [Duan et al., 2024a], where advanced LLMs are shown to surpass humans on tasks such as pragmatic reasoning [BojiÄ‡ et al., 2023] and creative writing [Tang and Kejriwal, 2024]. Some deficiencies still exist, such as limited implicature understanding [Ruis et al., 2023], prompt-sensitive linguistic competence [Hu and Levy, 2023], and lack of human-like psycholinguistic properties [Seals and Shalin, 2023, Tian et al., 2024]. In addition, mixed results are found in the alignment between LLMs and human linguistic cognition [Duan et al., 2024b, Wolfman et al., 2024] and language acquisition [Frank, 2023b, Shah et al., 2024].</p>
<h3>4.2.4 Learning and cognitive capabilities</h3>
<p>Psychometrics of learning and cognitive capabilities measures mental functions such as memory, reasoning, problemsolving, and comprehension to identify cognitive strengths and weaknesses. Recent studies adapt human psychometric tools to evaluate LLMs' cognitive abilities. For example, Galatzer-Levy et al. [2024] use the Wechsler Adult Intelligence Scale (WAIS-IV) to show LLMs generally reach top human levels in verbal comprehension and working memory, though multimodal models lag in visual reasoning. Raven's Progressive Matrices and other fluid intelligence tests reveal LLMs can match or surpass humans in analogical reasoning [Sartori and OrrÃº, 2023, Webb et al., 2022].</p>
<p>Table 5: Overview of test formats: structured tests, open-ended conversations, and agentic simulations. For each format, representative examples are provided for each construct.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Construct</th>
<th style="text-align: center;">Structured tests</th>
<th style="text-align: center;">Open-ended conversations</th>
<th style="text-align: center;">Agentic simulations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Personality traits</td>
<td style="text-align: center;">Jiang et al. [2023], SerapioGarcÃ­a et al. [2023]</td>
<td style="text-align: center;">Jiang et al. [2024b], Zheng et al. [2025]</td>
<td style="text-align: center;">Frisch and Giulianelli [2024], Huang et al. [2024c]</td>
</tr>
<tr>
<td style="text-align: center;">Values</td>
<td style="text-align: center;">KovaÄ et al. [2023], Zhong et al. [2024]</td>
<td style="text-align: center;">Ren et al. [2024], Yao et al. [2025b]</td>
<td style="text-align: center;">Chiu et al. [2025], Shen et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;">Morality</td>
<td style="text-align: center;">Abdulhai et al. [2024], Ji et al. [2024]</td>
<td style="text-align: center;">Neuman et al. [2025], Sachdeva and van Nuenen [2025]</td>
<td style="text-align: center;">Chiu et al. [2025], Nunes et al. [2024]</td>
</tr>
<tr>
<td style="text-align: center;">Attitudes \&amp; opinions</td>
<td style="text-align: center;">Argyle et al. [2023], Bernardelle et al. [2024]</td>
<td style="text-align: center;">RÃ¶ttger et al. [2024], Wright et al. [2024]</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Heuristics and biases</td>
<td style="text-align: center;">Hagendorff et al. [2023], Yax et al. [2024]</td>
<td style="text-align: center;">Healey et al. [2024], Wen et al. [2024a]</td>
<td style="text-align: center;">Bai et al. [2024], Xie et al. [2024b]</td>
</tr>
<tr>
<td style="text-align: center;">Social interaction</td>
<td style="text-align: center;">MittelstÃ¤dt et al. [2024], Sabour et al. [2024]</td>
<td style="text-align: center;">Elyoseph et al. [2023], Welivita and Pu [2024]</td>
<td style="text-align: center;">Wang et al. [2024d], Zhou et al. [2024b]</td>
</tr>
<tr>
<td style="text-align: center;">Psychology of language</td>
<td style="text-align: center;">Amouyal et al. [2024], Duan et al. [2024a]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Learning and cognitive capabilities</td>
<td style="text-align: center;">Coda-Forno et al. [2024], Wu et al. [2025]</td>
<td style="text-align: center;">Galatzer-Levy et al. [2024]</td>
<td style="text-align: center;">Lv et al. [2024]</td>
</tr>
</tbody>
</table>
<p>However, limitations persist. The Montreal Cognitive Assessment (MoCA) shows most LLMs display mild cognitive impairment, especially in visuospatial and executive tasks [Dayan et al., 2024]. Abstraction and Reasoning Challenge (ARC) further highlights deficits in abstract problem-solving [Wu et al., 2025]. Some works question the validity of current tests [Li et al., 2024d, LÃ¶hn et al., 2024, Zhang et al., 2023b], prompting new benchmarks and methodologies [Coda-Forno et al., 2024, Song et al., 2024a, Wang et al., 2024e, Zeng et al., 2024c, Zhuang et al., 2023a].
Developmental perspectives also emerge: Wang et al. [2024e] leverage Piaget's Theory of Cognitive Development to track human-like progression in LLMs, finding advanced models (e.g., GPT-4) reach cognitive abilities comparable to 20-year-old humans. Shah et al. [2024] identify developmental profiles in LLMs paralleling human development via psychometric theories like Cattell-Horn-Carroll.</p>
<p>Main findings. LLMs demonstrate strong performance on verbal comprehension, working memory, and analogical reasoning, often reaching or surpassing high human percentiles when evaluated with adapted psychometric tools like WAIS-IV and Raven's Matrices [Galatzer-Levy et al., 2024, Webb et al., 2022]. However, they exhibit notable cognitive deficits, particularly on benchmarks like MoCA and ARC tasks [Dayan et al., 2024, Wu et al., 2025]. Several works introduce developmental and cognitive benchmarks (e.g., CogLM, CogBench) grounded in psychological theories to track reasoning and adaptability [Coda-Forno et al., 2024, Wang et al., 2024e]. While emergent reasoning appears at scale, challenges remain in interpretability, test validity, and generalization beyond surface-level statistical patterns [Li et al., 2024d, LÃ¶hn et al., 2024].</p>
<h1>5 Psychometric evaluation methodology</h1>
<p>This section examines the methodologies employed in LLM Psychometrics. As illustrated in Fig. 3, the methodological framework encompasses four key components: test formats (Â§5.1), data and task sources (Â§5.2), prompting strategies (Â§5.3), and model output and scoring (Â§5.4). The configuration of inference parameters (Â§5.5) is additionally discussed.</p>
<h3>5.1 Test format</h3>
<p>Test formats of LLM psychometric evaluation can be categorized into structured tests, open-ended conversations, and agentic simulations. Table 5 collects exemplary works illustrating the test formats used for each construct.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Overview of LLM psychometric evaluation methodologies.</p>
<h1>5.1.1 Structured tests</h1>
<p>Structured tests feature predefined instructions, questions, and response formats. These items may include alternativechoice questions, multiple-choice questions, rating scales, and short-answer questions. When adapting structured psychometric tests for LLMs, it is common practice to retain the original items and simply reformat them as prompts.
For example, personality and values are often measured with Likert scales (e.g., BFI statements like "I see myself as someone who is outgoing, sociable"), while morality is assessed using tools like the DIT or MFQ, which require decisions and agreement ratings. Heuristics and biases are often tested with scenario-based choices. Social interaction and psycholinguistic abilities are evaluated with multiple-choice, forced-choice, or masked word prediction. Cognitive tests also involve short-answer questions (e.g., WAIS-IV Digit Span).
Structured psychometric tests are advantageous due to their scalability, objectivity, and automated scoring. However, critical perspectives underline their limitations, such as gaps in real-world applicability; biases; data contamination; and issues with reliability, validity, and depth of insights.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8798 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8798</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8798</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-229152731</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.07412v2.pdf" target="_blank">Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings</a></p>
                <p><strong>Paper Abstract:</strong> Cycle-consistent training is widely used for jointly learning a forward and inverse mapping between two domains of interest without the cumbersome requirement of collecting matched pairs within each domain. In this regard, the implicit assumption is that there exists (at least approximately) a ground-truth bijection such that a given input from either domain can be accurately reconstructed from successive application of the respective mappings. But in many applications no such bijection can be expected to exist and large reconstruction errors can compromise the success of cycle-consistent training. As one important instance of this limitation, we consider practically-relevant situations where there exists a many-to-one or surjective mapping between domains. To address this regime, we develop a conditional variational autoencoder (CVAE) approach that can be viewed as converting surjective mappings to implicit bijections whereby reconstruction errors in both directions can be minimized, and as a natural byproduct, realistic output diversity can be obtained in the one-to-many direction. As theoretical motivation, we analyze a simplified scenario whereby minima of the proposed CVAE-based energy function align with the recovery of ground-truth surjective mappings. On the empirical side, we consider a synthetic image dataset with known ground-truth, as well as a real-world application involving natural language generation from knowledge graphs and vice versa, a prototypical surjective case. For the latter, our CVAE pipeline can capture such many-to-one mappings during cycle training while promoting textural diversity for graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8798.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8798.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleCVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cycle-Consistent Conditional Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cycle-consistent training pipeline that replaces deterministic cycles with a conditional VAE to handle many-to-one (surjective) mappings between graphs and text, enabling implicit bijections and diverse one-to-many generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT-based graph encoding + CVAE decoder (graph→text) / BiLSTM graph extraction (text→graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are encoded via a graph encoder (GAT producing node features and a root node representation). The graph-to-text decoder is a conditional generator h_θ(y,z) that consumes the graph encoding and a latent z drawn from a prior p(z|y) (learned from the root node); at inference the CVAE samples z to produce diverse text outputs. The text-to-graph module h^+_θ(x) is implemented as a BiLSTM that produces graph structure predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (WebNLG: RDF-style graphs of 2–7 nodes, triplets from DBPedia)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph attention networks (GAT) compute node embeddings; root-node features produce parameters of p(z|y); a CVAE decoder conditions on the graph encoding and sampled z to produce text sequences. For text→graph, BiLSTM decodes to graph edges (classification over relations among known entity nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (G2T) and text-to-graph construction/extraction (T2G) in an unsupervised cycle-training setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On WebNLG (unsupervised, 100% training data with shuffled pairs): CycleCVAE G2T BLEU = 43.3%; T2G graph F1 = 60.0%; average number of distinct textual variations per graph (10 samples) ≈ 4.0; >2 paraphrases for 99% of test instances; avg. edit distance between paraphrases = 12.24 words.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms other unsupervised baselines (RuleBased, UMT, GT-BT) and is competitive with several supervised SOTA graph-to-text and text-to-graph models on BLEU and F1 in the authors' experiments; produces multiple diverse outputs whereas supervised SOTA methods typically generate one sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models many-to-one surjective mappings, enables diverse and realistic one-to-many graph→text generation via latent z sampling, matches supervised-level BLEU/F1 in experiments, robust to latent dimension (even low-dim z works), can learn implicit bijection by pruning unused latent dims (theoretically motivated).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds complexity relative to deterministic pipelines (encoder+prior+decoder); requires choosing latent dimension r_z (must be at least the dimensionality of unobserved factors to guarantee representational capacity); diversity can sometimes reduce standard BLEU scores because BLEU rewards single reference overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Deterministic baseline (CycleBase) fails on surjective examples (synthetic border example); CycleCVAE theoretical guarantees rely on affine assumptions and constraints (e.g., matching output distribution ρ_y = ρ_ŷ and r_z ≥ dim[U]); in non-affine/complex regimes identifiability and suboptimal local minima may still occur without suitable inductive biases or regularizers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8798.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8798.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleBase</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic Cycle-Consistent Baseline (CycleBase)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard deterministic cycle-consistent model used as a baseline which jointly trains a graph→text generator and a text→graph reconstructor without latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT-LSTM graph encoder + deterministic LSTM decoder (graph→text); BiLSTM (text→graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph nodes are encoded with GATs and decoded by an LSTM text decoder with multi-head attention over node embeddings; deterministic mapping (no latent z) is used for graph→text so one graph yields a single output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>GAT produces node embeddings; LSTM decoder attends to node embeddings to deterministically produce a single text output. Text→graph uses BiLSTM to predict edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text and text-to-graph in unsupervised cycle training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On WebNLG (unsupervised, 100% shuffled): CycleBase G2T BLEU = 43.1%; T2G F1 = 59.8%; #variations = 1 (no diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Similar BLEU/F1 to CycleCVAE but lacks output diversity and exhibits higher reconstruction error on synthetic surjective examples; underperforms CycleCVAE on capturing unobserved factors in one-to-many mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simpler deterministic architecture; competitive BLEU/F1 in some settings; lower complexity than CVAE variant.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Cannot model inherent one-to-many (many-to-one inverse) conditional distributions, fails to generate diverse paraphrases, suffers from higher reconstruction error when mapping is surjective.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails to capture latent variation in synthetic experiment (cannot learn random decorative border that is not deterministically inferable from the graph), plateaus at higher reconstruction error.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8798.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8798.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Serialization / Triplet Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Serialization (triplet concatenation / linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method of converting a knowledge graph to a linear sequence by serializing triplestore entries into token sequences (concatenating triplet text or using separators), used as input to sequence models like T5 or seq2seq baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triplet serialization / linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is turned into a flat sequence by serializing its RDF triplets (subject, relation, object) in some order, with special separator symbols between triplets; orders can be random or follow a heuristic. The serialized string is used as input to sequence-to-sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (RDF-style triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize each graph triplet into text tokens and concatenate triplets with separators; optionally apply masking/formatting; used both for training seq2seq models and for input to pretrained models (T5) during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (via seq2seq models like T5) and graph serialization for unsupervised methods (UMT-style).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported as a standalone metric in this paper, but when used with CycleCVAE+T5 yields competitive BLEU relative to supervised T5 baselines (exact BLEU numbers for CycleCVAE+T5 appear in Table 4 of the paper but are not quoted explicitly in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Serialization is the standard approach used by UMT and by T5-based models; authors cite Schmitt et al., Ribeiro et al., and Kale as serialization practices. Compared to structured encoders (GAT), serialization allows direct use of pretrained seq2seq models but may lose explicit graph structural inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables leveraging large pretrained text-to-text models (e.g., T5) without custom graph encoders; simple and flexible; compatible with standard seq2seq pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sequence order can be arbitrary causing potential instability or sensitivity to triplet ordering; flattens explicit graph structure which can reduce inductive bias for models not pre-trained on such serialized forms.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Random ordering of triplets can hide structural relationships; serialization alone may be insufficient to capture graph inductive biases unless combined with pretrained models or additional structure-aware encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8798.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8798.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleBased serialization (RuleBased)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-Based Triplet Concatenation Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic unsupervised baseline that verbalizes each triplet by concatenating textual representations of triples and joining them with 'and', used as a simple graph→text method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triplet-to-text concatenation (rule-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph triplet is converted to a simple textual phrase (e.g., '(s, r, o)' → 's r o') and triplet phrases are concatenated in some order separated by conjunctions like 'and' to form a sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (WebNLG triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Map each triplet to a fixed short phrase and concatenate all triplet phrases into a single textual output.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text as a baseline unsupervised verbalizer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not explicitly tabulated in main text for WebNLG comparisons, used as an unsupervised baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Qualitatively lower quality than learned models (CycleCVAE, supervised models); used as a simple baseline to assess gains from learned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Extremely simple, no training required, deterministic and interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Produces unnatural, un-fluent text; cannot capture natural linguistic variations or context-aware phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor fluency and naturalness; unsuitable for applications requiring natural language generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8798.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8798.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMT (serialization + seq2seq unsupervised MT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised Machine Translation-style Serialization (UMT baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised seq2seq approach that treats serialized graphs and text as separate 'languages' and applies unsupervised MT techniques (back-translation / UMT) on serialized graph strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Serialized graph sequence (UMT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are serialized into linear token sequences (triplet concatenation) and then treated analogously to a source language; unsupervised translation techniques are applied to map between serialized graphs and text sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (serialized triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize triplets into a token sequence with separators; train seq2seq model using unsupervised MT techniques (e.g., back-translation) to translate between serialized graphs and text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text and text-to-graph via unsupervised sequence-to-sequence translation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not explicitly reported numerically in the main text; cited as a competing unsupervised baseline (UMT from Schmitt et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performed worse than CycleCVAE in the authors' comparisons (CycleCVAE outperforms unsupervised baselines such as UMT).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages well-studied unsupervised MT techniques and standard seq2seq architectures; straightforward to implement.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on sequence formulation that can obscure graph structure; requires careful serialization choices; typically inferior to structure-aware generation models for graph→text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Generally lower generation quality and less fluency than learned graph-structure-aware models or CycleCVAE in authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8798.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8798.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network + LSTM Decoder (GAT-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-sequence architecture that encodes graph node features with Graph Attention Networks and decodes text with an LSTM decoder using multi-head attention over node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text Generation from Knowledge Graphs with Graph Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT-based structural encoding with LSTM decoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two-layer GAT produces node embeddings; an LSTM decoder with multi-head attention over the node embeddings generates text autoregressively, attending to graph structure at each decoding step.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (WebNLG graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph attention encodes node features and relations into embeddings; the decoder attends over these embeddings while producing tokens with an LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (used as the graph→text module in CycleBase and in CycleCVAE experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as the graph→text backbone; combined with CycleCVAE it contributed to BLEU = 43.3% and F1 = 60.0% for T2G/G2T tasks in unsupervised setup when paired with cycle training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Adopted as a strong structural encoder compared to pure serialization; comparable to other supervised graph-to-text approaches referenced in the paper (G2T, Melbourne, BestPlan).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves graph structure and uses attention to focus on salient nodes during generation; suitable for learning structural mappings in graph→text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Heavier architecture than simple serialization; requires implementation of GNN components and careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specifically broken out, but as part of a deterministic cycle (CycleBase) it cannot by itself produce diverse one-to-many outputs without added latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8798.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8798.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleCVAE+T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CycleCVAE integrated with pretrained T5 sequence-to-sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An enhanced pipeline that replaces the graph-to-text GAT-LSTM with a pretrained T5 seq2seq model trained/finetuned on serialized graphs, combined with CycleCVAE training for unsupervised graph↔text conversion to improve fluency and maintain diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Serialized graph input to pretrained T5 (seq2seq) within CycleCVAE</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are serialized to token sequences (triplet linearization) and fed to a pretrained T5 encoder; the CycleCVAE framework provides a CVAE-style latent mechanism and cycle losses while finetuning T5 to generate diverse texts from graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (WebNLG serialized triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize graph → feed into T5 encoder → T5 decoder generates text; CycleCVAE machinery provides latent sampling and cycle-consistency losses during unsupervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation in an unsupervised setting; text-to-graph handled by the CycleCVAE text→graph module.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report that unsupervised CycleCVAE+T5 produces a competitive BLEU relative to fully supervised T5 baselines (exact BLEU numbers appear in the paper's Table 4 but are not quoted verbatim in the main text). Maintains output diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Combines benefits of pretrained language models (fluency) with CycleCVAE's diversity; competitive with supervised T5 when no parallel data beyond WebNLG is used.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages large pretrained text-to-text models for higher fluency and quality while preserving diversity via CVAE latent sampling; effective when external pretrained resources are allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires finetuning large pretrained models (compute and data overhead); relies on serialization to convert graphs into T5 input format, potentially losing some structural biases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated; implicit caveats include potential sensitivity to serialization choices and the standard caveats of finetuning large LM models (overfitting, domain mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Investigating Pre-trained Language Models for Graph-to-Text Generation <em>(Rating: 2)</em></li>
                <li>Text-to-Text Pre-Training for Data-to-Text Tasks <em>(Rating: 1)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8798",
    "paper_id": "paper-229152731",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "CycleCVAE",
            "name_full": "Cycle-Consistent Conditional Variational Autoencoder",
            "brief_description": "A cycle-consistent training pipeline that replaces deterministic cycles with a conditional VAE to handle many-to-one (surjective) mappings between graphs and text, enabling implicit bijections and diverse one-to-many generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "GAT-based graph encoding + CVAE decoder (graph→text) / BiLSTM graph extraction (text→graph)",
            "representation_description": "Graphs are encoded via a graph encoder (GAT producing node features and a root node representation). The graph-to-text decoder is a conditional generator h_θ(y,z) that consumes the graph encoding and a latent z drawn from a prior p(z|y) (learned from the root node); at inference the CVAE samples z to produce diverse text outputs. The text-to-graph module h^+_θ(x) is implemented as a BiLSTM that produces graph structure predictions.",
            "graph_type": "Knowledge graphs (WebNLG: RDF-style graphs of 2–7 nodes, triplets from DBPedia)",
            "conversion_method": "Graph attention networks (GAT) compute node embeddings; root-node features produce parameters of p(z|y); a CVAE decoder conditions on the graph encoding and sampled z to produce text sequences. For text→graph, BiLSTM decodes to graph edges (classification over relations among known entity nodes).",
            "downstream_task": "Graph-to-text generation (G2T) and text-to-graph construction/extraction (T2G) in an unsupervised cycle-training setting.",
            "performance_metrics": "On WebNLG (unsupervised, 100% training data with shuffled pairs): CycleCVAE G2T BLEU = 43.3%; T2G graph F1 = 60.0%; average number of distinct textual variations per graph (10 samples) ≈ 4.0; &gt;2 paraphrases for 99% of test instances; avg. edit distance between paraphrases = 12.24 words.",
            "comparison_to_others": "Outperforms other unsupervised baselines (RuleBased, UMT, GT-BT) and is competitive with several supervised SOTA graph-to-text and text-to-graph models on BLEU and F1 in the authors' experiments; produces multiple diverse outputs whereas supervised SOTA methods typically generate one sentence.",
            "advantages": "Explicitly models many-to-one surjective mappings, enables diverse and realistic one-to-many graph→text generation via latent z sampling, matches supervised-level BLEU/F1 in experiments, robust to latent dimension (even low-dim z works), can learn implicit bijection by pruning unused latent dims (theoretically motivated).",
            "disadvantages": "Adds complexity relative to deterministic pipelines (encoder+prior+decoder); requires choosing latent dimension r_z (must be at least the dimensionality of unobserved factors to guarantee representational capacity); diversity can sometimes reduce standard BLEU scores because BLEU rewards single reference overlap.",
            "failure_cases": "Deterministic baseline (CycleBase) fails on surjective examples (synthetic border example); CycleCVAE theoretical guarantees rely on affine assumptions and constraints (e.g., matching output distribution ρ_y = ρ_ŷ and r_z ≥ dim[U]); in non-affine/complex regimes identifiability and suboptimal local minima may still occur without suitable inductive biases or regularizers.",
            "uuid": "e8798.0",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "CycleBase",
            "name_full": "Deterministic Cycle-Consistent Baseline (CycleBase)",
            "brief_description": "A standard deterministic cycle-consistent model used as a baseline which jointly trains a graph→text generator and a text→graph reconstructor without latent variables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "GAT-LSTM graph encoder + deterministic LSTM decoder (graph→text); BiLSTM (text→graph)",
            "representation_description": "Graph nodes are encoded with GATs and decoded by an LSTM text decoder with multi-head attention over node embeddings; deterministic mapping (no latent z) is used for graph→text so one graph yields a single output.",
            "graph_type": "Knowledge graphs (WebNLG)",
            "conversion_method": "GAT produces node embeddings; LSTM decoder attends to node embeddings to deterministically produce a single text output. Text→graph uses BiLSTM to predict edges.",
            "downstream_task": "Graph-to-text and text-to-graph in unsupervised cycle training.",
            "performance_metrics": "On WebNLG (unsupervised, 100% shuffled): CycleBase G2T BLEU = 43.1%; T2G F1 = 59.8%; #variations = 1 (no diversity).",
            "comparison_to_others": "Similar BLEU/F1 to CycleCVAE but lacks output diversity and exhibits higher reconstruction error on synthetic surjective examples; underperforms CycleCVAE on capturing unobserved factors in one-to-many mappings.",
            "advantages": "Simpler deterministic architecture; competitive BLEU/F1 in some settings; lower complexity than CVAE variant.",
            "disadvantages": "Cannot model inherent one-to-many (many-to-one inverse) conditional distributions, fails to generate diverse paraphrases, suffers from higher reconstruction error when mapping is surjective.",
            "failure_cases": "Fails to capture latent variation in synthetic experiment (cannot learn random decorative border that is not deterministically inferable from the graph), plateaus at higher reconstruction error.",
            "uuid": "e8798.1",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Serialization / Triplet Linearization",
            "name_full": "Graph Serialization (triplet concatenation / linearization)",
            "brief_description": "A method of converting a knowledge graph to a linear sequence by serializing triplestore entries into token sequences (concatenating triplet text or using separators), used as input to sequence models like T5 or seq2seq baselines.",
            "citation_title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing",
            "mention_or_use": "use",
            "representation_name": "Triplet serialization / linearization",
            "representation_description": "Each graph is turned into a flat sequence by serializing its RDF triplets (subject, relation, object) in some order, with special separator symbols between triplets; orders can be random or follow a heuristic. The serialized string is used as input to sequence-to-sequence models.",
            "graph_type": "Knowledge graphs (RDF-style triplets)",
            "conversion_method": "Serialize each graph triplet into text tokens and concatenate triplets with separators; optionally apply masking/formatting; used both for training seq2seq models and for input to pretrained models (T5) during finetuning.",
            "downstream_task": "Graph-to-text generation (via seq2seq models like T5) and graph serialization for unsupervised methods (UMT-style).",
            "performance_metrics": "Not reported as a standalone metric in this paper, but when used with CycleCVAE+T5 yields competitive BLEU relative to supervised T5 baselines (exact BLEU numbers for CycleCVAE+T5 appear in Table 4 of the paper but are not quoted explicitly in the main text).",
            "comparison_to_others": "Serialization is the standard approach used by UMT and by T5-based models; authors cite Schmitt et al., Ribeiro et al., and Kale as serialization practices. Compared to structured encoders (GAT), serialization allows direct use of pretrained seq2seq models but may lose explicit graph structural inductive bias.",
            "advantages": "Enables leveraging large pretrained text-to-text models (e.g., T5) without custom graph encoders; simple and flexible; compatible with standard seq2seq pipelines.",
            "disadvantages": "Sequence order can be arbitrary causing potential instability or sensitivity to triplet ordering; flattens explicit graph structure which can reduce inductive bias for models not pre-trained on such serialized forms.",
            "failure_cases": "Random ordering of triplets can hide structural relationships; serialization alone may be insufficient to capture graph inductive biases unless combined with pretrained models or additional structure-aware encoding.",
            "uuid": "e8798.2",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RuleBased serialization (RuleBased)",
            "name_full": "Rule-Based Triplet Concatenation Baseline",
            "brief_description": "A heuristic unsupervised baseline that verbalizes each triplet by concatenating textual representations of triples and joining them with 'and', used as a simple graph→text method.",
            "citation_title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing",
            "mention_or_use": "use",
            "representation_name": "Triplet-to-text concatenation (rule-based)",
            "representation_description": "Each graph triplet is converted to a simple textual phrase (e.g., '(s, r, o)' → 's r o') and triplet phrases are concatenated in some order separated by conjunctions like 'and' to form a sentence.",
            "graph_type": "Knowledge graphs (WebNLG triplets)",
            "conversion_method": "Map each triplet to a fixed short phrase and concatenate all triplet phrases into a single textual output.",
            "downstream_task": "Graph-to-text as a baseline unsupervised verbalizer.",
            "performance_metrics": "Not explicitly tabulated in main text for WebNLG comparisons, used as an unsupervised baseline in experiments.",
            "comparison_to_others": "Qualitatively lower quality than learned models (CycleCVAE, supervised models); used as a simple baseline to assess gains from learned methods.",
            "advantages": "Extremely simple, no training required, deterministic and interpretable.",
            "disadvantages": "Produces unnatural, un-fluent text; cannot capture natural linguistic variations or context-aware phrasing.",
            "failure_cases": "Poor fluency and naturalness; unsuitable for applications requiring natural language generation quality.",
            "uuid": "e8798.3",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "UMT (serialization + seq2seq unsupervised MT)",
            "name_full": "Unsupervised Machine Translation-style Serialization (UMT baseline)",
            "brief_description": "An unsupervised seq2seq approach that treats serialized graphs and text as separate 'languages' and applies unsupervised MT techniques (back-translation / UMT) on serialized graph strings.",
            "citation_title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing",
            "mention_or_use": "mention",
            "representation_name": "Serialized graph sequence (UMT-style)",
            "representation_description": "Graphs are serialized into linear token sequences (triplet concatenation) and then treated analogously to a source language; unsupervised translation techniques are applied to map between serialized graphs and text sequences.",
            "graph_type": "Knowledge graphs (serialized triplets)",
            "conversion_method": "Serialize triplets into a token sequence with separators; train seq2seq model using unsupervised MT techniques (e.g., back-translation) to translate between serialized graphs and text.",
            "downstream_task": "Graph-to-text and text-to-graph via unsupervised sequence-to-sequence translation.",
            "performance_metrics": "Not explicitly reported numerically in the main text; cited as a competing unsupervised baseline (UMT from Schmitt et al., 2020).",
            "comparison_to_others": "Performed worse than CycleCVAE in the authors' comparisons (CycleCVAE outperforms unsupervised baselines such as UMT).",
            "advantages": "Leverages well-studied unsupervised MT techniques and standard seq2seq architectures; straightforward to implement.",
            "disadvantages": "Relies on sequence formulation that can obscure graph structure; requires careful serialization choices; typically inferior to structure-aware generation models for graph→text.",
            "failure_cases": "Generally lower generation quality and less fluency than learned graph-structure-aware models or CycleCVAE in authors' experiments.",
            "uuid": "e8798.4",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "GAT-LSTM",
            "name_full": "Graph Attention Network + LSTM Decoder (GAT-LSTM)",
            "brief_description": "A graph-to-sequence architecture that encodes graph node features with Graph Attention Networks and decodes text with an LSTM decoder using multi-head attention over node embeddings.",
            "citation_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "mention_or_use": "use",
            "representation_name": "GAT-based structural encoding with LSTM decoding",
            "representation_description": "Two-layer GAT produces node embeddings; an LSTM decoder with multi-head attention over the node embeddings generates text autoregressively, attending to graph structure at each decoding step.",
            "graph_type": "Knowledge graphs (WebNLG graphs)",
            "conversion_method": "Graph attention encodes node features and relations into embeddings; the decoder attends over these embeddings while producing tokens with an LSTM.",
            "downstream_task": "Graph-to-text generation (used as the graph→text module in CycleBase and in CycleCVAE experiments).",
            "performance_metrics": "Used as the graph→text backbone; combined with CycleCVAE it contributed to BLEU = 43.3% and F1 = 60.0% for T2G/G2T tasks in unsupervised setup when paired with cycle training.",
            "comparison_to_others": "Adopted as a strong structural encoder compared to pure serialization; comparable to other supervised graph-to-text approaches referenced in the paper (G2T, Melbourne, BestPlan).",
            "advantages": "Preserves graph structure and uses attention to focus on salient nodes during generation; suitable for learning structural mappings in graph→text.",
            "disadvantages": "Heavier architecture than simple serialization; requires implementation of GNN components and careful tuning.",
            "failure_cases": "Not specifically broken out, but as part of a deterministic cycle (CycleBase) it cannot by itself produce diverse one-to-many outputs without added latent variables.",
            "uuid": "e8798.5",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "CycleCVAE+T5",
            "name_full": "CycleCVAE integrated with pretrained T5 sequence-to-sequence model",
            "brief_description": "An enhanced pipeline that replaces the graph-to-text GAT-LSTM with a pretrained T5 seq2seq model trained/finetuned on serialized graphs, combined with CycleCVAE training for unsupervised graph↔text conversion to improve fluency and maintain diversity.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "representation_name": "Serialized graph input to pretrained T5 (seq2seq) within CycleCVAE",
            "representation_description": "Graphs are serialized to token sequences (triplet linearization) and fed to a pretrained T5 encoder; the CycleCVAE framework provides a CVAE-style latent mechanism and cycle losses while finetuning T5 to generate diverse texts from graphs.",
            "graph_type": "Knowledge graphs (WebNLG serialized triplets)",
            "conversion_method": "Serialize graph → feed into T5 encoder → T5 decoder generates text; CycleCVAE machinery provides latent sampling and cycle-consistency losses during unsupervised training.",
            "downstream_task": "Graph-to-text generation in an unsupervised setting; text-to-graph handled by the CycleCVAE text→graph module.",
            "performance_metrics": "Authors report that unsupervised CycleCVAE+T5 produces a competitive BLEU relative to fully supervised T5 baselines (exact BLEU numbers appear in the paper's Table 4 but are not quoted verbatim in the main text). Maintains output diversity.",
            "comparison_to_others": "Combines benefits of pretrained language models (fluency) with CycleCVAE's diversity; competitive with supervised T5 when no parallel data beyond WebNLG is used.",
            "advantages": "Leverages large pretrained text-to-text models for higher fluency and quality while preserving diversity via CVAE latent sampling; effective when external pretrained resources are allowed.",
            "disadvantages": "Requires finetuning large pretrained models (compute and data overhead); relies on serialization to convert graphs into T5 input format, potentially losing some structural biases.",
            "failure_cases": "Not explicitly enumerated; implicit caveats include potential sensitivity to serialization choices and the standard caveats of finetuning large LM models (overfitting, domain mismatch).",
            "uuid": "e8798.6",
            "source_info": {
                "paper_title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing",
            "rating": 2,
            "sanitized_title": "an_unsupervised_joint_system_for_text_generation_from_knowledge_graphs_and_semantic_parsing"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Investigating Pre-trained Language Models for Graph-to-Text Generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Text-to-Text Pre-Training for Data-to-Text Tasks",
            "rating": 1,
            "sanitized_title": "texttotext_pretraining_for_datatotext_tasks"
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2,
            "sanitized_title": "exploring_the_limits_of_transfer_learning_with_a_unified_texttotext_transformer"
        },
        {
            "paper_title": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
            "rating": 1,
            "sanitized_title": "stepbystep_separating_planning_from_realization_in_neural_datatotext_generation"
        }
    ],
    "cost": 0.01603575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings</p>
<p>Qipeng Guo 
Fudan University</p>
<p>Amazon Shanghai AI Lab</p>
<p>Zhijing Jin 
Amazon Shanghai AI Lab</p>
<p>Ziyu Wang 
Tsinghua University</p>
<p>Xipeng Qiu 
Fudan University</p>
<p>Weinan Zhang 
Shanghai Jiao Tong University</p>
<p>Jun Zhu 
Tsinghua University</p>
<p>Zheng Zhang 
Amazon Shanghai AI Lab</p>
<p>David Wipf 
Amazon Shanghai AI Lab</p>
<p>Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings</p>
<p>Cycle-consistent training is widely used for jointly learning a forward and inverse mapping between two domains of interest without the cumbersome requirement of collecting matched pairs within each domain. In this regard, the implicit assumption is that there exists (at least approximately) a ground-truth bijection such that a given input from either domain can be accurately reconstructed from successive application of the respective mappings. But in many applications no such bijection can be expected to exist and large reconstruction errors can compromise the success of cycle-consistent training. As one important instance of this limitation, we consider practically-relevant situations where there exists a many-to-one or surjective mapping between domains. To address this regime, we develop a conditional variational autoencoder (CVAE) approach that can be viewed as converting surjective mappings to implicit bijections whereby reconstruction errors in both directions can be minimized, and as a natural byproduct, realistic output diversity can be obtained in the one-to-many direction. As theoretical motivation, we analyze a simplified scenario whereby minima of the proposed CVAE-based energy function align with the recovery of ground-truth surjective mappings. On the empirical side, we consider a synthetic image dataset with known ground-truth, as well as a real-world application involving natural language generation from knowledge graphs and vice versa, a prototypical surjective case. For the latter, our CVAE pipeline can capture such many-to-one mappings during cycle training while promot-Preliminary work.ing textural diversity for graph-to-text tasks.</p>
<p>Introduction</p>
<p>Given data x ∈ X from domain X and y ∈ Y from domain Y, it is often desirable to learn bidirectional mappings f : Y → X and g : X → Y such that for matched pairs {x, y}, we have that x ≈x f (y) and y ≈ŷ g(x). When provided with a corpus of suitably aligned data, this amounts to a straightforward supervised learning problem. However, in many applications spanning computer vision (Zhu et al., 2017a), natural language processing (Lample et al., 2018a;Artetxe et al., 2018) and speech recognition (Hori et al., 2019), we may only have access to individual samples from X and Y but limited or no labeled ground-truth matches between domains, since, for example, the labeling process may be prohibitively expensive. To address this commonly-encountered situation, cycle-consistent training represents an unsupervised means of jointly learning f and g by penalizing the cycle-consistency reconstruction losses x − f [g(x)] and y − g[f (y)] using non-parallel samples from X and Y and some norm or distance metric · (Zhu et al., 2017a).</p>
<p>However, this process implicitly assumes that there exists a suitable bijection between domains (implying f = g −1 and g = f −1 ), an assumption that frequently does not hold for practical applications of cycleconsistent training. As a representative example related to natural language understanding, each x may represent a text segment while y corresponds with the underlying knowledge graph describing the text content. The relationship between these domains is surjective, but not bijective, in the sense that multiple sentences with equivalent meaning but different syntactic structure can be mapped to the same knowledge graph. Hence if we follow any possible learned mapping x →ŷ →x, there will often be significant error between x and the reconstructedx. In other words, no invertible transformation exists between domains and there will necessarily be information about x that is lost when we map through Y space. Additionally, deterministic mappings do not reflect the ground-truth conditional distribution p gt (x|y), which is necessary for the generation of diverse text consistent with a given knowledge graph.</p>
<p>Despite these limitations, there has been relatively little effort or rigorous analysis devoted to explicitly addressing the lack of a bijection in applications of cycle-consistent training; Section 2 on related work will discuss this point in greater detail. As a step towards filling this void, in Section 3 we will consider replacing the typical deterministic cycle training pipeline with a stochastic model reflecting p gt (x|y) and p gt (y|x) for the x →ŷ →x and y →x →ŷ cycles respectively. In doing so, we apply a conditional variational autoencoder (CVAE) formulation (Doersch, 2016;Sohn et al., 2015) to deal with the intractable integrals that arise. Note that although the proposed CVAE methodology can be generalized, we will herein restrict ourselves to situations where there exists a many-to-one mapping from x to y (i.e., a surjection) as originally motivated by our interest in conversions between knowledge graphs and diverse, natural language text.</p>
<p>Proceeding further, Section 4 provides theoretical support by analyzing a simplified scenario whereby minima of the proposed CVAE-based energy function align with the recovery of ground-truth surjective mappings. To the best of our knowledge, this is the only demonstration of a cycle-consistent model with any type of performance guarantee within a non-trivial, non-bijective context. We then turn to empirical validation in Section 5 that corroborates our theory via a synthetic image example and demonstrates real-world practicality on an application involving the conversion between diverse natural language and knowledge graphs taken from the WebNLG dataset. Overall, experimental results indicate that our proposed CVAE pipeline can approximate surjective mappings during cycle training, with performance on par with supervised alternatives, while promoting diversity for the graph-to-text direction.</p>
<p>Related Work</p>
<p>General Cycle-Consistent Training The concept of leveraging the transitivity of two functions that serve as inverses to one another has been applied to a variety of tasks. For example, in computer vision, forwardbackward consistency has been used extensively in computer vision (Kalal et al., 2010;Sundaram et al., 2010), and cycle-consistent training pipelines underlie image style transfer (Zhu et al., 2017a;Liu et al., 2017), depth estimation (Godard et al., 2017), and unsupervised domain adaptation (Hoffman et al., 2018) pipelines. Turning to natural language processing (NLP), back translation (Sennrich et al., 2016;Edunov et al., 2018;Jin et al., 2020) and dual learning (Cheng et al., 2016;He et al., 2016) have been widely deployed for unsupervised machine translation. Similar techniques have also contributed to applications such as language style transfer (Shen et al., 2017;Jin et al., 2019). However, the above models primarily rely on deterministic pipelines that implicitly assume a bijection even if one does not actually exist.</p>
<p>And finally, a VAE-inspired model for converting between knowledge graphs and text is considered in (Tseng et al., 2020). But again there is no explicit accounting for non-bijective data as a shared latent space is assumed to contain all information from both x and y domains, and the proposed model is designed and tested for semi-supervised learning (not fully unsupervised cycle-consistency). Moreover, for tractable inference, some terms from the variational bound on the log-likelihood (central to all VAE models) are heuristically removed; hence the relationship with the original, motivating probabilistic model remains unclear.</p>
<p>Non-Bijective Mappings Non-bijective mappings are investigated in applications such as multi-domain image-to-image translation (Choi et al., 2018), voice conversion (Kameoka et al., 2018), multi-attribute text style transfer (Lample et al., 2018b), music transfer (Bitton et al., 2018), and multi-modal generation (Shi et al., 2019). Most of this work uses adversarial neural networks, or separate decoders (Lee et al., 2019;Mor et al., 2018), and one case even applies a CVAE model (Jha et al., 2018). However, all the above assume multiple pre-defined style domains and require data be clearly separated a priori according to these domains to train non-bijective mappings. In contrast, our proposed model assumes a completely arbitrary surjective mapping that can be learned from the data without such additional domain-specific side information pertaining to styles or related (so ours can fit unknown styles mixed within an arbitrary dataset). One exception is (Zhu et al., 2017b), which handles general non-bijective image mappings using a hybrid VAE-GAN model but unlike our approach, it requires matched {x, y} pairs for training.</p>
<p>Model Development</p>
<p>We will first present a stochastic alternative to deterministic cycle-consistency that, while useful in principle for handling surjective (but explicitly non-bijective) mappings, affords us with no practically-realizable inference procedure. To mitigate this shortcoming, we then derive a tractable CVAE approximation and discuss some of its advantages. Later in Section 4 we will analyze the local and global minima of this cycleconsistent CVAE model in the special case where the decoder functions are restricted to being affine.</p>
<p>Stochastic Cycle-Consistent Formulation</p>
<p>Although the proposed methodology can be generalized, we will herein restrict ourselves to situations where there exists a many-to-one mapping from x to y (i.e., a surjection) and the resulting asymmetry necessitates that the x →ŷ →x and y →x →ŷ cycles be handled differently. In this regard, our starting point is to postulate an additional latent variable u ∈ U that contributes to a surjective matched pair {x, y} via
x = h gt (y, u) and y = h + gt (x) ,(1)
where h gt : Y × U → X and h + gt : X → Y represent ground-truth mappings we would ultimately like to estimate. For this purpose we adopt the approximations h θ : Y × Z → X and h + θ : X → Y with trainable parameters θ, noting that the second input argument of h θ is now z ∈ Z instead of u ∈ U. This is because the latter is unobservable and it is sufficient to learn a mapping that preserves the surjection between x and y without necessarily reproducing the exact same functional form of h gt . For example, if hypothetically u = π(z) in (1) for some function π, we could redefine h gt as a function of z without actually changing the relationship between x and y.</p>
<p>We are now prepared to define a negative conditional log-likelihood loss for both cycle directions, averaged over the distributions of y and x respectively. For the simpler y →x →ŷ cycle, we define
y (θ) − log p θ (y|x) p(z)dz ρ y gt (dy),(2)
wherex = h θ (y, z), p θ (y|x) is determined by h + θ and an appropriate domain-specific distribution, and p(z) is assumed to be fixed and known (e.g., a standardized Gaussian). Additionally, ρ y gt denotes the ground-truth probability measure associated with Y. Consequently, ρ y gt (dy) is the measure assigned to the infinitesimal dy, from which it obviously follows that ρ y gt (dy) = 1. Note that the resulting derivations can apply even if no ground-truth density p gt (y) exists, e.g., a counting measure over training samples or discrete domains can be assumed within this representation.</p>
<p>Given that there should be no uncertainty in y when conditioned on x, we would ideally like to learn parameters whereby p θ (y|x), and therefore y (θ), degenerates while reflecting a many-to-one mapping. By this we mean that
y ≈ŷ = h + θ (x) = h + θ (h θ [y, z]) , ∀z ∼ p(z).
(3) Hence the y →x →ŷ cycle only serves to favor (near) perfect reconstructions of y while ignoring any z that can be drawn from p(z). The latter stipulation is unique relative to typical deterministic cycle-consistent training, which need not learn to ignore randomness from an additional confounding latent factor.</p>
<p>In contrast, the x →ŷ →x cycle operates somewhat differently, with the latent z serving a more important, non-degenerate role. Similar to before, we would ideally like to minimize the negative conditional log-likelihood given by
x (θ) − log p θ (x|ŷ, z) p(z)dz ρ x gt (dx),(4)where nowŷ = h + θ (x)
, p θ (x|y, z) depends on h θ , and ρ x gt represents the ground-truth probability measure on X . Hereŷ can be viewed as an estimate of all the information pertaining to the unknown paired y as preserved through the mapping h + θ from x to y space. Moreover, if cycle-consistent training is ultimately successful, bothŷ and y should be independent of z, and it should be the case that p θ (x|ŷ, z) p(z)dz = p θ (x|ŷ) ≈ p gt (x|y).</p>
<p>Per these definitions, it is also immediately apparent that p θ (x|ŷ) is describing the distribution of h θ (y, z) conditioned on y being fixed to h + θ (x). This can be viewed as a stochastic version of the typical x →ŷ →x cycle, whereas now the latent z allows us to spread probability mass across all x that are consistent with a givenŷ. In fact, if we set z = z to a fixed null value (i.e., change p(z) to a Dirac delta function centered at some arbitrary z ), we recover this traditional pipeline exactly, with − log p θ (x|y, z ) simply defining the implicit loss function, with limited ability to assign high probability to multiple different values of x for any given y.</p>
<p>CVAE Approximation</p>
<p>While y (θ) from Section 3.1 can be efficiently minimized using stochastic sampling from p(z) to estimate the required integral, the x (θ) term is generally intractable. Note that unlike y (θ), directly sampling z is not a viable solution for x (θ) since p θ (x|ŷ, z) can be close to zero for nearly all values of z, and therefore, a prohibitively large number of samples would be needed to obtain reasonable estimates of the integral. Fortunately though, we can form a trainable upper bound using a CVAE architecture that dramatically improves sample efficiency (Doersch, 2016;Sohn et al., 2015). Specifically, we define
x (θ, φ) − E q φ (z|x) [log p θ (x|ŷ, z)] p(z)dz + KL [q φ (z|x) p(z)] ρ x gt (dx),(5)
where in this context, p θ (x|ŷ, z) is referred to as a decoder distribution while q φ (z|x) represents a trainable encoder distribution parameterized by φ. And by design of general VAE-based models, we have that Kingma and Welling, 2014;Rezende et al., 2014). Note that we could also choose to condition q φ (z|x) and p(z) onŷ, although this is not required to form a valid or maximally-tight bound. In the case of q φ (z|x),ŷ is merely a function of x and therefore contains no additional information beyond direct conditioning on x (and the stated bound holds regardless of what we choose for q φ ). In contrast, p(z) defines the assumed generative model which we are also free to choose; however, conditioning onŷ can be absorbed into p θ (x|ŷ, z) such that there is no change in the representational capacity of p θ (x|ŷ) = p θ (x|ŷ, z) p(z)dz.
x (θ, φ) ≥ x (θ) for all φ (
Given (5) as a surrogate for x (θ) and suitable distributional assumptions, the combined cycle-consistent loss
cycle (θ, φ) = x (θ, φ) + y (θ)(6)
can be minimized over {θ, φ} using stochastic gradient descent and the reparameterization trick from (Kingma and Welling, 2014;Rezende et al., 2014). We henceforth refer to this formulation as CycleCVAE. And as will be discussed in more detail later, additional constraints, regularization factors, or inductive biases can also be included to help ensure identifiability of ground-truth mappings. For example, we may consider penalizing or constraining the divergence between the distributions of y andŷ = h + θ (x), both of which can be estimated from unpaired samples from ρ y gt and ρ x gt respectively. This is useful for disambiguating the contributions of y and z to x and will be equal to zero (or nearly so) if h + θ ≈ h + gt (more on this in Section 4 below).</p>
<p>CycleCVAE Inference</p>
<p>Once trained and we have obtained some optimal Cy-cleVAE parameters {θ * , φ * } ≈ arg min θ,φ cycle (θ, φ), we can compute matches for test data in either the x test → y test or y test → x test direction. For the former, we need only computeŷ test = h + θ * (x test ) and there is no randomness involved. In contrast, for the other direction (one-to-many) we can effectively draw approximate samples from the posterior distribution p θ * (x|y test ) by first drawing samples z ∼ p(z) and then computingx test = h θ * (y test , z).</p>
<p>CycleCVAE Advantages in Converting</p>
<p>Surjections to Implicit Bijections Before proceeding to a detailed theoretical analysis of CycleCVAE, it is worth examining a critical yet subtle distinction between CycleCVAE and analogous, deterministic baselines. In particular, given that the x →ŷ →x cycle will normally introduce reconstruction errors because of the lack of a bijection as discussed previously, we could simply augment traditional deterministic pipelines with a z such that , we would ideally prefer that CVAE regularization somehow prune away the superfluous dimensions of z that are not required to produce good reconstructions of x. 2 For example, VAE pruning could potentially be instantiated by setting the posterior distribution of unneeded dimensions of the vector z to the prior. By this we mean that if dimension j is not needed, then q φ (z j |x) = p(z j ), uninformative noise that plays no role in improving reconstructions of x. This capability has been noted in traditional VAE models (Dai et al., 2018;Dai and Wipf, 2019), but never rigorously analyzed in the context of CVAE extensions or cycle-consistent training. In this regard, the analysis in Section 4 will elucidate special cases whereby CycleCVAE can provably lead to optimal pruning, the first such analysis of CVAE models, cycle-consistent or otherwise. This serves to motivate the proposed pipeline as a vehicle for learning an implicit bijection even without knowing the dimensionality or distribution of data from U, a particularly relevant notion given the difficulty in directly estimating dim[U] in practice.</p>
<p>Secondly, because the CycleCVAE model is explicitly predicated upon a known prior p(z) (as opposed to the unknown distribution of u), other model components are calibrated accordingly such that there is no need to provide an empirical estimate of an unknown prior. Consequently, there is no barrier to cycle-consistent training or the generation of new x conditioned on y.</p>
<p>To the best of our knowledge, there is essentially no existing analysis of cycle-consistent training in the challenging yet realistic scenarios where a bijection between x and y cannot be assumed to hold. 3 In this section we present a simplified case whereby the proposed CycleC-VAE objective (with added distributional constraints) is guaranteed to have no bad local minimum in a specific sense to be described shortly. The forthcoming analysis relies on the assumption of a Gaussian CVAE with an affine model for the functions {h θ , h + θ }; however, the conclusions we draw are likely to be loosely emblematic of behavior in broader regimes of interest. While admittedly simplistic, the resulting CVAE objective remains non-convex, with a combinatorial number of distinct local minima. Hence it is still non-trivial to provide any sort of guarantees in terms of associating local minima with 'good' solutions, e.g., solutions that recover the desired latent factors, etc. In fact, prior work has adopted similar affine VAE decoder assumptions, but only in the much simpler case of vanilla VAE models (Dai et al., 2018;Lucas et al., 2019), i.e., no cycle training or conditioning as is our focus herein.</p>
<p>Affine CycleCVAE Model</p>
<p>For analysis purposes, we consider a CVAE model of continuous data x ∈ R rx , y ∈ R ry , and z ∈ R rz , where r x , r y , and r z are the respective sizes of x, y, and z. We assume p(z) = N (z|0, I) and a typical Gaussian decoder p θ (x|ŷ, z) = N (x|µ x , Σ x ), where the mean network satisfies the affine parameterizations
µ x = h θ (ŷ, z) = W xŷ + V x z + b x , withŷ = h + θ (x) = W y x + b y .(7)
In this expression, {W x , W y , V x , b x , b y } represents the set of all weight matrices and bias vectors which define the decoder mean µ x . And as is often assumed in practical VAE models, we set Σ x = γI, where γ &gt; 0 is a scalar parameter within the parameter set θ. Despite these affine assumptions, the CVAE energy function can still have a combinatorial number of distinct local minima as mentioned previously. However, we will closely examine conditions whereby all these local minima are actually global minima that correspond with the optimal inversion of a non-trivial generative model.</p>
<p>Although we could proceed by allowing the encoder to be arbitrarily complex, when the decoder mean function is forced to be affine and Σ x = γI, a Gaussian encoder with affine moments is sufficient to achieve the optimal CVAE cost. Specifically, without any loss of representational capacity, we may choose
q φ (z|x) = N (z|µ z , Σ z ) with µ z = W z x + b z and a diagonal Σ z = diag[s] 2 ,
where s is an arbitrary parameter vector independent of x. 4 Collectively, these specifications lead to the
complete parameterization θ = {W x , W y , V x , b x , γ}, φ = {W z , b z , s}
, and the CVAE energy given by
x (θ, φ) ≡ E q φ (z|x) 1 γ (I − W x W y ) x − V x z − b x 2 2 (8) + d log γ + rz k=1 s 2 k − log s 2 k + W z x + b z 2 2 ρ x gt (dx),
noting that, without loss of generality, we have ab-
sorbed a W x b y factor into b x .
And finally, for the corresponding y (θ) model we specify p θ (y|x) = N (y|µ y , Σ y ) using a shared, cycleconsistent parameterization borrowed from (7). For this purpose, we adopt
µ y = h + θ (x) = W yx +b y , withx = W x y+V x z+b x ,(9)
and Σ y = γI. Given these assumptions, we have
y (θ) ≡ y Σ −1 y y + log Σ y ρ y gt (dy)(10)
excluding irrelevant constants, where
y (I − W y W x ) y − b y , Σ y γI + W y V x V x W y and
again, analogous to before we have absorbed W y b x into b y without loss of generality.</p>
<p>Properties of Global/Local Minima</p>
<p>As a preliminary thought experiment, we can consider the minimization of cycle (θ, φ), where x (θ, φ) is defined via (8) and y (θ) via (10), but no assumptions are placed on the distributions ρ x gt and ρ y gt . In this situation, it is obvious that even CycleCVAE global minima, were they obtainable, will not generally recover the ground-truth mappings between paired x ∼ ρ x gt and y ∼ ρ y gt ; there simply will not generally be sufficient capacity. Furthermore, it can even be shown that under quite broad conditions there will exist a combinatorial number of non-global local minima, meaning local minimizers that fail to achieve the lowest possible cost.</p>
<p>Hence we now present a narrower scenario with constraints on the ground-truth data to better align with the affine simplification described in Section 4.1. This will allow us to formulate conditions whereby all local minima are actually global minima capable of accurately modeling the ground-truth surjection. To this end, we define the following affine ground-truth model:</p>
<p>Definition 1 (Affine Surjective Model) We define an affine surjective model whereby all matched {x, y} pairs satisfy
x = Ay + Bu + c and y = Dx + e,(11)
with B ∈ null[D], DA = I, Dc = −e, rank[A] = r y &lt; r x and rank[B] ≤ r x − r y . Furthermore, we assume that y ∼ ρ y gt and u ∼ ρ u gt are uncorrelated, and the measure assigned to the transformed random variable W y + V u is equivalent to ρ y gt iff W = I and V = 0. We also enforce that y and u have zero mean and identity covariance, noting that any nonzero mean components can be absorbed into c. Among other things, the stated conditions of the affine surjective model collectively ensure that the mappings y → x and x → y can be mutually satisfied.</p>
<p>Additionally, for later convenience, we also define r c rank E ρ x gt xx ≤ r x . We then have the following:</p>
<p>Proposition 2 Assume that matched pairs {x, y} follow the affine surjective model from Definition 1. Then subject to the constraint ρ y gt = ρŷ θ , where ρŷ θ defines the θ-dependent distribution ofŷ, all local minima of the CycleVAE objective cycle (θ, φ), with x (θ, φ) taken from (8) and y (θ) from (10), will be global minima in the limit γ → 0 assuming r z ≥ r c − r y . Moreover, the globally optimal parameters {θ * , φ * } will satisfy Note that in practice, we are free to choose r z as large as we want, so the requirement that r z ≥ r c − r y is not significant. Additionally, the constraint ρ y gt = ρŷ θ can be instantiated (at least approximately) by including a penalty on the divergence between these two distributions. This is feasible using only unpaired samples of y (for estimating ρ y gt ) and x (for estimating ρŷ θ ), and most cycle-consistent training pipelines contain some analogous form of penalty on distributional differences between cycles (Lample et al., 2018a).
W * x = A, V * x = B, 0 P , b * x = c, W * y = D, b * y = − Dc,(12)
And finally, if r y + rank[B] = r x , then the dual requirements that DA = I and B ∈ null[ D] will ensure that D = D and b * y = e. However, even if D = D it is inconsequential for effective recovery of the groundtruth model since any x produced by Definition 1 will nonetheless still map to the correct y when applying D instead of D.</p>
<p>Corollary 3 Given the same setup as Proposition 2,
let {W * z , b * z } denote the CVAE encoder parameters of any minimum. Then W * z = P W * z 0 and b * z = P b * z 0 , where W
We will now discuss various take-home messages related to these results.</p>
<p>Practical Implications</p>
<p>As alluded to in Section 3.4, we will generally not know in advance p(u) or even dim [U], which reduces to r c −r y in the simplified affine case. Hence inducing a bijection may seem problematic on the surface. Fortunately though, Proposition 2 and Corollary 3 indicate that as long as we choose r z ≥ dim[U] in our CVAE model, we can nonetheless still learn an implicit bijection between x and {y, µ z }, where µ z are the informative (nonzero) dimensions of µ z that actually contribute to the reconstruction of x. In the affine case, these are the dimensions of z aligned with nonzero columns of B, but in general these dimensions could more loosely refer to the degrees-of-freedom in z that, when altered, lead to changes inx. The remaining superfluous dimensions of z are set to the uninformative prior p(z) = N (z|0, I) and subsequently filtered out by the CVAE decoder module parameterized by h θ (y, z). In this diminutive role, they have no capacity for interfering with any attempts to learn a bijection.</p>
<p>Even so, in non-affine cases it is impossible to guarantee that all local minima correspond with the recovery of h gt and h + gt , or that these functions are even identifiable. Indeed it is not difficult to produce counterexamples whereby recovery is formally impossible. However, if h θ and h + θ are chosen with inductive biases reasonably well-aligned with their ground-truth counterparts, up to unidentifiable latent transformations of the unobservable u, we may expect that an approximate bijection between the true x and {y, µ z } can nonetheless be inferred via cycle-consistent training, at least provided we can avoid suboptimal local minimizers that can be introduced by more complex nonlinear decoder models.  </p>
<p>Experiments</p>
<p>In this section we first consider a synthetic image experiment that supports the theoretical motivation for CycleCVAE. We then turn to practical real-world evaluations involving the conversion between knowledge graphs and text sequences, a core ingredient of natural language understanding/generation and the application that initially motivated our work. We then conclude with an enhanced pipeline involving the recent pretrained T5 model (Raffel et al., 2020).</p>
<p>Synthetic Image Experiment</p>
<p>We first conduct an experiment designed such that the surjective conditions of Definition 1 in Section 4.2 are loosely satisfied (see supplementary for reasons). As shown in Figure 1 (left panel), each data sample has three components: a digit, its image, and a decorative yellow border. The digit takes value in {0, . . . , 9} and is represented by a 10-dim one-hot vector y. The corresponding image x involves 3 × 3 tiles, and each tile contains the same image of digit y. One of the 9 tiles is decorated with a 1-pixel-wide yellow border, and which tile will have this border is determined by u, a 9-dim one-hot vector indicating the 9 possible tiles.</p>
<p>We train two models on this dataset, a base model using standard cycle training, and our CycleCVAE that incorporates the proposed CVAE into a baseline cycle model (see supplementary for network description and details). After training, generated samples of the two approaches when presented with the digit '4' are shown in Figure 1 (middle and right panels). The base model fails to learn the yellow border as it cannot handle the one-to-many mapping from digits to images.</p>
<p>Meanwhile the random CycleCVAE sample correctly places the border around one of the tiles (different Cy-cleCVAE samples simply move the border to different tiles as desired; see supplementary). Finally, consistent with these generation results, the training curves from Figure 2 reveal that the reconstruction error of the base model, which assumes a bijection, plateaus at a significantly higher value than the CycleCVAE model.</p>
<p>Knowledge Graph to Text Conversion</p>
<p>We now turn to more challenging real-world experiments involving the surjective mapping between knowledge graphs and text sequences. Here the ideal goal is to generate diverse, natural text from a fixed knowledge graph, or extract the knowledge graph from a piece of text. To this end we compare CycleCVAE against SOTA methods on the widely-used WebNLG graph-to-text dataset (Gardent et al., 2017).</p>
<p>WebNLG Dataset and Test Setup</p>
<p>WebNLG data is extracted from DBPedia, where each graph consists of 2-7 nodes and the corresponding text is descriptions of these graphs collected by crowd-sourcing. We follow the preprocessing of (Moryossef et al., 2019) and obtain 13K training, 1.6K validation, and 5K test text-graph pairs. Please see the supplementary for details of the CycleCVAE architecture explicitly designed for handling text and graph data. Note that we did not include any additional penalty function on the divergence between ρ y gt and ρŷ θ ; the architecture inductive biases were sufficient for good performance.</p>
<p>Metrics We measure performance using three metrics: (1) text generation quality with the standard BLEU score (Papineni et al., 2002), 5 (2) graph construction accuracy via the F1 score of the edge predictions among given entity nodes, and (3) text diversity. Text diversity is an increasingly important criterion for NLP because the same meaning can be conveyed in various expressions, and intelligent assistants should master such variations. We evaluate diversity by reporting the number of distinct sentence variations obtained after running the generation model 10 times.</p>
<p>Accuracy Results</p>
<p>Since cycle training only requires unsupervised data, we have to break the text-graph pairs to evaluate unsupervised performance. In this regard, there are two ways to process the data. First, we can use 100% of the training data and just shuffle the text and graphs so that the matching/supervision is lost. This is the setting in Table 1, which allows for direct head-to-head comparisons with SOTA supervised methods (assuming no outside training data). The supervised graph-to-text baselines include Melbourne  Table 1 we observe that our model outperforms other unsupervised methods, and it is even competitive with SOTA supervised models in both the graph-to-text (BLEU) and text-to-graph (F1) directions.   In contrast, a second, stricter unsupervised protocol involves splitting the dataset into two halves, extracting text from the first half, and graphs from the second half. This is the setting in Table 2, which avoids the possibility of seeing any overlapping entities during training. Although performance is slightly worse given less training data, the basic trends are the same.</p>
<p>Diversity Results From Tables 1 and 2 we also note that CycleCVAE can generate on average more than 4 different sentence types for a given knowledge graph; all other SOTA methods can only generate a single sentence per graph. Additionally, we have calculated that CycleCVAE generates more than two textual paraphrases for 99% of test instances, and the average edit distance between two paraphrases is 12.24 words (see supplementary). Moreover, CycleCVAE text diversity does not harm fluency and semantic relevance as the BLEU score is competitive with SOTA methods as mentioned previously.</p>
<p>Diverse Text Output Generated by CycleCVAE -The population density of Arlington, Texas is 1472.0. -Arlington, Texas has a population density of 1472.0. -Alan Bean, who was born in Wheeler, Texas, is now "retired." -Alan Bean is a United States citizen who was born in Wheeler, Texas. He is now "retired." Table 3: Every two variations are generated by CycleC-VAE from the same knowledge graph.</p>
<p>We list text examples generated by our model in Table 3, with more in the supplementary. The diverse generation is a significant advantage for many real applications. For example, it can make automated conversations less boring and simulate different scenarios. And diversity can push model generated samples closer to the real data distribution because there exist different ways to verbalize the same knowledge graph (although diversity will not in general improve BLEU scores, and can sometimes actually lower them).</p>
<p>Integrating CycleCVAE with T5</p>
<p>Previous graph-text results are all predicated on no outside training data beyond WebNLG. However, we now consider an alternative testing scenario whereby outside training data can be incorporated by integrating CycleCVAE with a large pretrained T5 sequence-tosequence model (Raffel et al., 2020). Such models have revolutionized many NLP tasks and can potentially improve the quality of the graph-to-text direction in cycle training on WebNLG. To this end, we trained a CycleCVAE model, with the function h θ (y, z) formed from a pretrained T5 architecture (see supplementary for details). Results are shown in Table 4, where unsupervised CycleCVAE+T5 produces a competitive BLEU score relative to fully supervised T5 baselines. It also maintains diversity of generated text sequences.  </p>
<p>Conclusion</p>
<p>We have proposed CycleCVAE for explicitly handling non-bijective surjections commonly encountered in realworld applications of unsupervised cycle-consistent training. Our framework has both a solid theoretical foundation and strong empirical performance on practical knowledge graph-to-text conversion problems. For future work we can consider extending CycleCVAE to handle many-to-many (non-bijective, non-surjective) mappings, or unsolved applications such as conversions between scene graphs and realistic images (which remains extremely difficult even with supervision). </p>
<p>Supplementary Materials</p>
<p>The supplementary file includes additional content related to the following:</p>
<ol>
<li>
<p>Synthetic Experiments (Section 5.1 in main paper): We explain why the synthetic data loosely align with Definition 1, describe the network architectures of the CycleBase and CycleCVAE models, and include additional generation results.</p>
</li>
<li>
<p>WebNLG Experiments (Section 5.2 in main paper): We describe the experimental setup for WebNLG, including the task description, cycle-consistency model design, and all baseline and implementation details.</p>
</li>
</ol>
<p>We also include an ablation study varying dim[z].</p>
<ol>
<li>T5 Extension (Section 5.3 in main paper): We provide details of the CycleCVAE+T5 extension and include additional generated samples showing textual diversity.</li>
</ol>
<p>Proof of Proposition 2.</p>
<ol>
<li>Proof of Corollary 3.</li>
</ol>
<p>Synthetic Dataset Experimental Details and Additional Results</p>
<p>Dataset Description and Relation to Definition 1</p>
<p>To motivate how the synthetic data used in Section 5.1 from the main paper at least partially align with Definition 1, we let c and e be zero vectors and A ∈ R d×10 be a d × 10 transformation matrix from images to digits, where d is the total number of pixels in each image x. In other words, each column i ∈ {0, 1, . . . , 9} of A is a linearized pixel sequence of the 2D image of digit i from top left to bottom right. Based on A, we construct an example inverse matrix D so that DA = I. Specifically, D can be a 10 × d matrix where each row i ∈ {0, 1, . . . , 9} is a linearized pixel sequence of a masked version of the image of the digit i, and this image can have, for example, only one non-zero pixel that is sufficient to distinguish the digit i from all other nine possibilities. We also construct B, a d × 9 transformation matrix from the image to the border position, which surrounds one out of the nine tiles in each image. Each column i ∈ {0, 1, . . . , 8} of B is a linearized pixel sequence of the 2D image of the border surrounding the i-th tile. Since the patterns of the digit and border do not share any non-zero pixels, we should have that DB = 0. Moreover, each digit's image is distinct and cannot be produced by combining other digit images, so rank[A] = r y and also r y ≤ r x because border patterns are orthogonal to digit patterns. Hence, we also have rank[B] ≤ r x − r y . Note however that the synthetic data do not guarantee that W y + V u is equivalent to ρ y gt iff W = I and V = 0.</p>
<p>Network Architectures</p>
<p>We train two models on this dataset, a base model CycleBase using standard cycle training, and our CycleCVAE that incorporates the proposed CVAE into a baseline cycle model.</p>
<p>CycleBase</p>
<p>The base model uses multilayer perceptrons (MLPs) for both the image(x)-to-digit(y) mapping h + θ (x) (shared with CycleCVAE), and the digit(y)-to-image(x) mapping denoted h Base θ (y). Each MLP hidden layer (two total) has 50 units with the tanh activation function. The last layer of h + θ (x) uses a softmax function to output a vector of probabilities α over the ten digits, and therefore we can apply p θ (y|x) = Cat(y|α), a categorical distribution conditioned on α, for training purposes. The last layer of digit-to-image h Base θ (y) adopts a per-pixel sigmoid function (since the value of each pixel is between 0 and 1), and we assume p θ (x|y) is based on the binary cross entropy loss.</p>
<p>CycleCVAE Our CycleCVAE uses the same function h + θ (x) as the base model. However, for the digit-to-image generation direction, CycleCVAE includes a 1-dimensional latent variable z sampled from N (µ x , Σ x ), where µ x and Σ x are both learned by 50-dimensional, 3-layer MLPs (including output layer) with input x. Then h θ (y, z) takes the digit y and latent variable z as inputs to another 3-layer MLP with 50 hidden units and the same activation function as the base model.</p>
<p>Generation Results</p>
<p>In addition to Figure 1 in the main paper, we list more example images generated by our model in the figure below. As we can see, the base model fails to learn the diverse border which should randomly surround only one of the nine tiles. However, CycleCVAE learns the border in its latent variable z and by random sampling, CycleCVAE can generate an arbitrary border around one of the nine digits as expected.</p>
<p>Base Model CycleCVAE (Multiple Samples):</p>
<p>Figure 3: Example images generated by CycleCVAE.</p>
<p>WebNLG Experimental Setup and Ablation Study</p>
<p>The WebNLG dataset 6 is widely used for conversions between graph and text. Note that WebNLG is the most appropriate dataset for our purposes because in other candidates (e.g., relation extraction datasets (Walker et al., 2006)) the graphs only contain a very small subset of the information in the text.</p>
<p>Task Description</p>
<p>The WebNLG experiment includes two directions: text-to-graph (T2G) and graph-to-text (G2T) generation. The G2T task aims to produce descriptive text that verbalizes the graphical data. For example, the knowledge graph triplets "(Allen Forest, genre, hip hop), (Allen Forest, birth year, 1981)" can be verbalized as "Allen Forest, a hip hop musician, was born in 1981." This has wide real-world applications, for instance, when a digital assistant needs to translate some structured information (e.g., the properties of the restaurant) to the human user. The other task, T2G is also important, as it extracts structures in the form of knowledge graphs from the text, so that Allen Forest, a hip hop musician, was born in the year 1981. The music genre hip hop gets its origins from disco and funk music, and it is also which drum and bass is derived from. </p>
<p>Allen</p>
<p>Large Corpus (No Parallel Graphs)</p>
<p>Many Graphs (No Parallel Text)</p>
<p>CycleGT Figure 4: The graph-to-text generation task aims to verbalize a knowledge graph, while the text-to-graph task extracts the information of text into the form of a knowledge graph.</p>
<p>all entities become nodes, and the relationships among entities form edges. It can help many downstream tasks, such as information retrieval and reasoning. The two tasks can be seen as a dual problem, as shown in Figure 4.</p>
<p>Specifically, for unsupervised graph-to-text and text-to-graph generation, we have two non-parallel datasets:
• A text corpus X = {x i } N i=1
consisting of N text sequences, and
• A graph dataset Y = {y j } M j=1 consisting of M graphs.
The constraint is that the graphs and text contain the same distribution of latent content, but are different forms of surface realizations, i.e., there is no alignment providing matched pairs. Our goal is to train two models in an unsupervised manner: h θ that generates text based on the graph, and h + θ that produces a graph based on text.</p>
<p>Cycle Training Models</p>
<p>CycleBase Similar to the synthetic experiments mentioned above, we first propose the base cycle training model CycleBase that jointly learns graph-to-text and text-to-graph generation. To be consistent with our main paper, we denote text as x and graphs as y, and the graph-to-text generation is a one-to-many mapping. The graph cycle, y →x →ŷ is as follows: Given a graph y, the cycle-consistent training first generates synthetic text x = h Base θ (y), and then uses it to reconstruct the original graphŷ = h + θ (x). The loss function is imposed to align the generated graphŷ with the original graph y. Similarly, the text cycle, x →ŷ →x, is to align x and the generatedx. Both loss functions adopt the cross entropy loss.</p>
<p>Specifically, we instantiate the graph-to-text module h Base θ (y) with the GAT-LSTM model proposed by (Koncel-Kedziorski et al., 2019), and the text-to-graph module h + θ (x) with a simple BiLSTM model we implemented. The GAT-LSTM module has two layers of graph attention networks (GATs) with 512 hidden units, and two layers of a LSTM text decoder with multi-head attention over the graph node embeddings produced by GAT. This attention mechanism uses four attention heads, each with 128 dimensions for self-attention and 128 dimension for cross-attention between the decoder and node features. The BiLSTM for text-to-graph construction uses 2-layer bidirectional LSTMs with 512 hidden units.</p>
<p>CycleCVAE Our CycleCVAE uses the same h + θ (x) as the base model. As for h θ (y, z) (the CycleCVAE extension of CycleBase), we first generate a 10-dimensional latent variable z sampled from q φ (z|x) = N (µ x , Σ x ), where µ x and Σ x are both learned by bidirectional LSTMs plus a fully connected feedforward layer. We form p(z|y) as a Gaussian distribution whose mean and variance are learned from a fully connected feedforward layer which takes in the feature of the root node of the GAT to represent the graph. Note that applying this p(z|y) as the CycleCVAE prior is functionally equivalent to using a more complicated encoder, as mentioned in the main paper.</p>
<p>Implementation Details For both cycle models, we adopt the Adam optimizer with a learning rate of 5e−5 for the text-to-graph modules, and learning rate of 2e−4 for graph-to-text modules. For the graph-to-text module, we re-implement the GAT-LSTM model (Koncel-Kedziorski et al., 2019) using the DGL library (Wang et al., 2019b). Our code is available https://github.com/QipengGuo/CycleGT.</p>
<p>Details of Competing Methods</p>
<p>Unsupervised Baselines As cycle training models are unsupervised learning methods, we first compare with unsupervised baselines. RuleBased is a heuristic baseline proposed by (Schmitt et al., 2020) which simply iterates through the graph and concatenates the text of each triplet. For example, the triplet "(AlanShepard, occupation, TestPilot)" will be verbalized as "Alan Shepard occupation test pilot." If there are multiple triplets, their text expressions will be concatenated by "and." The other baseline, UMT (Schmitt et al., 2020), formulates the graph and text conversion as a sequence-to-sequence task and applies a standard unsupervised machine translation (UMT) approach. It serializes each triplet of the graph in the same way as RuleBased, and concatenates the serialization of all triplets in a random order, using special symbols as separators.</p>
<p>Supervised Baselines</p>
<p>We also compare with supervised systems using the original supervised training data. Since there is no existing work that jointly learns graph-to-text and text-to-graph in a supervised way, we can only use models that address one of the two tasks. For graph-to-text generation, we list the performance of state-of-the-art supervised models including (1) Melbourne, the best supervised system submitted to the WebNLG challenge 2017 (Gardent et al., 2017), which uses an encoder-decoder architecture with attention, (2) StrongNeural  Shen et al., 2020), which segments the text into small units, and learns the alignment between data and target text segments. The generation process uses the attention mechanism over the corresponding data piece to generate the corresponding text. For text-to-graph generation, we compare with state-of-the-art models including OnePass (Wang et al., 2019a), a BERT-based relation extraction model, and T2G, the BiLSTM model that we adopt as the text-to-graph component in the cycle training of CycleBase and CycleCVAE.</p>
<p>Ablation Study</p>
<p>We conduct an ablation study using the 50%:50% unsupervised data of WebNLG. Note that our models do not use an adversarial term, so we only tune the CVAE latent dimension to test robustness to this factor. The hyperparameter tuning of the size of the latent dimension is shown in Table 5, where we observe that our CycleCVAE is robust against different z dimensions. Note that because z is continuous while generated text is discrete, just a single dimension turns out to be adequate for good performance for these experiments. Even so, the encoder variance can be turned up to avoid 'overusing' any continuous latent dimension to roughly maintain a bijection.  9 T5 Model Details and More Generated Samples</p>
<p>CycleCVAE+T5 Implementational Details</p>
<p>We adopted the pretrained T5 model (Raffel et al., 2020) to replace the GAT-LSTM architecture that we previously used for the graph-to-text module within the cycle training. T5 is a sequence-to-sequence model that takes as input a serialized graph (see the serialization practice in Schmitt et al., 2020;Ribeiro et al., 2020;Kale, 2020) and generates a text sequence accordingly. We finetune the T5 during training with the Adam optimizer using a learning rate of 5e−5.</p>
<p>Additional Text Diversity Examples</p>
<p>We list the text diversity examples generated by CycleCVAE+T5 in Table 6. </p>
<p>5</p>
<p>-The Addiction (journal), abbreviated to "Addiction", has the ISSN number "1360-0443" and is part of the academic discipline of Addiction.</p>
<p>-Addiction (journal), abbreviated to "Addiction", has the ISSN number "1360-0443".  10 Proof of Proposition 2</p>
<p>The high-level proof proceeds in several steps. First we consider optimization of x (θ, φ) over φ to show that no suboptimal local minima need be encountered. We then separately consider optimizing x (θ, φ) and y (θ) over the subset of θ unique to each respective loss. Next we consider jointly optimizing the remaining parameters residing between both terms. After assimilating the results, we arrive at the stated result of Proposition 2. Note that with some abuse of notation, we reuse several loss function names to simplify the exposition; however, the meaning should be clear from context.</p>
<p>Optimization over encoder parameters
φ in x (θ, φ)
The energy term from the x →ŷ →x cycle can be modified as
x (θ, φ) = E q φ (z|x) 1 γ x − µ x 2 2 + d log γ + rz k=1 s 2 k − log s 2 k + µ z 2 2 ρ x gt (dx) = E q φ (z|x) 1 γ (I − W x W y ) x − V x z − W x b y − b x 2 2 + d log γ + rz k=1 s 2 k − log s 2 k + W z x + b z 2 2 ρ x gt (dx) (13) = 1 γ (I − W x W y ) x − V x (W z x + b z ) − W x b y − b x 2 2 + d log γ + κ k=1 s 2 k − log s 2 k + 1 γ s 2 k v x,k 2 2 + W z x + b z 2 2 ρ x gt (dx),
where v x,k denotes the k-th column of V x . Although this expression is non-convex in each s 2 k , by taking derivatives and setting them equal to zero, it is easily shown that there is a single stationary point that operates as the unique minimum. Achieving the optimum requires only that s 2 k = 1 γ v x,k 2 2 + 1 −1 for all k. Plugging this value into (13) then leads to the revised objective
x (θ, φ) ≡ 1 γ (I − W x W y ) x − V x (W z x + b z ) − W x b y − b x 2 2 (14) + κ k=1 log 1 γ v x,k 2 2 + 1 + d log γ + W z x + b z 2 2 ρ x gt (dx)
ignoring constant terms. Similarly we can optimize over µ z = W z x + b z in terms of the other variables. This is just a convex, ridge regression problem, with the optimum uniquely satisfying
W z x + b z = V x γI + V x V x −1 [(I − W x W y ) x − W x b y − b x ] ,(15)
which is naturally an affine function of x as required. After plugging (15) into (14), defining x (I − W x W y ) x− W x b y − b x , and applying some linear algebra manipulations, we arrive at
x (θ) min φ x (θ, φ) (16) = x V x V x + γI −1 x ρ x gt (dx) + κ k=1 log v x,k 2 2 + γ + (d − κ) log γ,
noting that this minimization was accomplished without encountering any suboptimal local minima.</p>
<p>10.2 Optimization over parameters θ that are unique to¯ x (θ)</p>
<p>The optimal b x is just the convex maximum likelihood estimator given by the mean
b x = (I − W x W y ) xρ x gt (dx) − W x b y = (I − W x W y ) c − W x b y ,(17)
where the second equality follows from Definition 1 in the main text. Plugging this value into (16) and applying a standard trace identity, we arrive at
x (θ) ≡ tr S x V x V x + γI −1 + κ k=1 log v x,k 2 2 + γ + (d − κ) log γ,(18)
where
S x Cov ρ x gt [ x ] = (I − W x W y ) Cov ρ x gt [x] (I − W x W y ) .(19)
The remaining parameters {W x , W y , V x } are all shared with the y →x →ŷ cycle loss y (θ), so ostensibly we must include the full loss¯ x (θ) + y (θ) when investigating local minima with respect to these parameters. However, there is one subtle exception that warrants further attention here. More specifically, the loss y (θ) depends on V x only via the outer product V x V x . Consequently, if V x =ŪΛV denotes the singular value decomposition of V x , then y (θ) is independent ofV since V x V x =ŪΛΛ Ū , noting thatΛΛ is just a square matrix with squared singular values along the diagonal. It then follows that we can optimize¯ x (θ) overV without influencing y (θ).</p>
<p>To this end we have the following:</p>
<p>Lemma 1 At any minimizer (local or global) of¯ x (θ) with respect toV , it follows thatV = P for some permutation matrix P and the corresponding loss satisfies
x (θ) = tr S x Σ −1 x + log |Σ x | , where Σ x V x V x + γI.(20)
This result follows (with minor modification) from (Dai et al., 2019)[Corollary 3]. A related result also appears in (Lucas et al., 2019).</p>
<p>10.3</p>
<p>Optimization over parameters θ that are unique to y (θ)</p>
<p>Since y has zero mean per Definition 1, the optimal b y is the convex maximum likelihood estimator satisfying b y = −W y b x (this assumes that W y b x has not been absorbed into y as mentioned in the main text for notational simplicity). This leads to
y (θ) ≡ tr S y Σ −1 y + log Σ y , where S y (I − W y W x ) (I − W y W x )(21)
and Σ y is defined in the main text.</p>
<p>10.4 Optimizing the combined loss¯ cycle (θ)</p>
<p>The above results imply that we may now consider jointly optimizing the combined loss cycle (θ) ¯ x (θ) + y (θ)</p>
<p>over {W x , W y , V x V x }; all other terms have already been optimized out of the model without encountering any suboptimal local minima. To proceed, consider the distribution ρŷ gt of y = W y x + b y = W y Ay + W y Bu + W y c + b y .</p>
<p>To satisfy the constraint the stipulated constraint ρŷ gt = ρ y gt subject to the conditions of Definition 1, it must be that W y A = I and B ∈ null[W y ] (it will also be the case that b y = −W y c to ensure thatŷ has zero mean). From this we may conclude that
S x = (I − W x W y ) Cov ρ x gt [x] (I − W x W y ) = (I − W x W y ) AA + BB (I − W x W y ) (24) = (A − W x ) (A − W x ) + BB ,
where the middle equality follows because y and u are uncorrelated with identity covariance. Furthermore, let D ∈ R ry×rx denote any matrix such that DA = I and B ∈ null [ D]. It then follows that W y must equal some such D and optimization of (22) over W x will involve simply minimizinḡ
cycle (θ) ≡ tr (A − W x ) (A − W x ) Σ −1 x + tr I − DW x I − DW x Σ −1 y + C(25)
over W x , where C denotes all terms that are independent of W x . This is a convex problem with unique minimum at W x = A. Note that this choice sets the respective W x -dependent terms to zero, the minimum possible value.</p>
<p>Plugging W x = A into (25) and expanding the terms in C, we then arrive at the updated loss cycle (θ) ≡ tr BB Σ −1</p>
<p>x + log |Σ x | + log Σ y (26)
= tr BB V x V x + γI −1 + log V x V x + γI + log DV x V x D + γI .
Minimization of this expression over V x as γ becomes arbitrarily small can be handled as follows. If any V x and γ are a local minima of (26), then {α = 1, β = 0} must also be a local minimum of cycle (α, β)</p>
<p>tr BB αΣ x + βBB −1 + log αΣ x + βBB + log αΣ y + β DBB D = tr BB αΣ x + βBB −1 + log αΣ x + βBB + log αΣ y .</p>
<p>If we exclude the second log-det term, then it has been shown in (Wipf and Nagarajan, 2007) that loss functions in the form of (27) have a monotonically decreasing path to a unique minimum as β → 1 and α → 0 . However, given that the second log-det term is a monotonically decreasing function of α, it follows that the entire loss from (27) has a unique minimum as β → 1 and α → 0. Consequently, it must be that at any local minimum of (26) V x V x = BB in the limit as γ → 0. Moreover, the feasibility of this limiting equality is guaranteed by our assumption that r z ≥ r c − r y (i.e., if r z &lt; r c − r y , then V x would not have sufficient dimensionality to allow V x V x = BB ).</p>
<p>Final Pieces</p>
<p>We have already established that at any local minimizer {θ * , φ * } it must be the case that W * x = A and W * y = D. Moreover, we also can infer from (17) and Section 10.3 that at any local minimum we have
b * x = I − W * x W * y c − W * x b * y = I − W * x W * y c + W * x W * y b * x = I − A D c + A Db * x(28)
from which it follows that I − A D c = I − A D b * x . This along is not sufficient to guarantee that b * x = c is the unique solution; however, once we include the additional constraint ρ y gt = ρŷ θ per the Proposition 2 statement, then b * x = c is uniquely determined (otherwise it would imply thatŷ has a nonzero mean). It then follows that b * y = −W * y b * x = − Dc.</p>
<p>And finally, regarding V * x , from Section 10.4 we have that V * x (V * x ) = BB . Although this does not ensure that V * x = B, we can conclude that span[Ū ] = span [B]. Furthermore, we know from Lemma 1 and the attendant singular value decomposition that V * x =ŪΛP and (V * x ) V * x = P Λ Λ P . Therefore, up to an arbitrary permutation, each column of V * x satisfies v * </p>
<p>Proof of Corollary 3</p>
<p>From (15) in the proof of Proposition 2 and the derivations above, we have that at any optimal encoder solution φ * = {W * z , b * z }, both W * z and b * z are formed by left multiplication by (V * x ) . Then based on Proposition 2 and</p>
<p>Figure 1 :
1Left: example image from dataset. Middle: image produced by baseline cycle training with y = 4. Right: a sample image generated by CycleCVAE conditioned on y = 4. For the latter, the position of yellow border is random. In contrast, the base model fails to learn the random border distribution.</p>
<p>Figure 2 :
2Cycle-consistent reconstruction errors of baseline and CycleC-VAE models.</p>
<p>(introduced in Gardent et al., 2017), StrongNeural, Best-Plan (Moryossef et al., 2019), Seg&amp;Align (Shen et al., 2020), and G2T (Koncel-Kedziorski et al., 2019). Supervised text-to-graph models include OnePass (Wang et al., 2019a), and T2G, a BiLSTM model we implemented. Unsupervised methods include RuleBased and GT-BT both by (Schmitt et al., 2020). Finally, CycleBase is our deterministic cycle training model with the architectural components borrowed from Cy-cleCVAE. Notably, from</p>
<p>(Moryossef et al., 2019) which improves the common encoder-decoder model, (3) BestPlan (Moryossef et al., 2019) which uses a special entity ordering algorithm before neural text generation, (4) G2T (Koncel-Kedziorski et al., 2019) which is the same as the GAT-LSTM architecture adopted in our cycle training models, and (5) Seg&amp;Align (</p>
<p>where B has rank[B] columns, span[ B] = span[B], P is a permutation matrix, and D satisfies DA = I and B ∈ null[ D].</p>
<p>Table 1 :
1Performance on the full WebNLG dataset.Text(BLEU) Graph(F1) #Variations 
Supervised (50%) 
G2T 
44.5 
-
1 
T2G 
-
59.7 
-
Unsupervised (first 50% text, last 50% graph) 
CycleBase (Ours) 
43.1 
59.8 
1 
CycleCVAE (Ours) 
43.3 
60.0 
4.01 </p>
<p>Table 2 :
2Performance on WebNLG with 50% data.</p>
<p>Table 4 :
4Text generation results with T5 on WebNLG.</p>
<p>Jin,Di, Jin, Zhijing, Zhou, Joey Tianyi, and Szolovits,  Peter (2020). "A Simple Baseline to Semi-Supervised Domain Adaptation for Machine Translation". CoRR.Kameoka, Hirokazu, Kaneko, Takuhiro, Tanaka,Kou,  and Hojo, Nobukatsu (2018). "StarGAN-VC: Nonparallel many-to-many voice conversion with star generative adversarial networks". CoRR.Jin, Zhijing, Jin, Di, Mueller, Jonas, Matthews, 
Nicholas, and Santus, Enrico (2019). "IMaT: Unsu-
pervised Text Attribute Transfer via Iterative Match-
ing and Translation". Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference 
on Natural Language Processing, EMNLP-IJCNLP 
2019. </p>
<p>Kalal, Zdenek, Mikolajczyk, Krystian, and Matas, Jiri 
(2010). "Forward-Backward Error: Automatic Detec-
tion of Tracking Failures". 20th International Con-
ference on Pattern Recognition, ICPR 2010. </p>
<p>Kale, Mihir (2020). "Text-to-Text Pre-Training 
for 
Data-to-Text 
Tasks". 
arXiv 
preprint 
arXiv:2005.10433. </p>
<p>Kingma, Diederik P. and Welling, Max (2014). "Auto-
Encoding Variational Bayes". 2nd International Con-
ference on Learning Representations, ICLR 2014. </p>
<p>Koncel-Kedziorski, Rik, Bekal, Dhanush, Luan, Yi, Lap-
ata, Mirella, and Hajishirzi, Hannaneh (2019). "Text 
Generation from Knowledge Graphs with Graph 
Transformers". Proceedings of the 2019 Conference of 
the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019. </p>
<p>Lample, Guillaume, Conneau, Alexis, Denoyer, Ludovic, 
and Ranzato, Marc'Aurelio (2018a). "Unsupervised 
Machine Translation Using Monolingual Corpora 
Only". 6th International Conference on Learning 
Representations, ICLR 2018. </p>
<p>Lample, Guillaume, Subramanian, Sandeep, Smith, 
Eric, Denoyer, Ludovic, Ranzato, Marc'Aurelio, and 
Boureau, Y-Lan (2018b). "Multiple-attribute text 
rewriting". International Conference on Learning 
Representations. </p>
<p>Lee, Keonnyeong, Yoo, In-Chul, and Yook, Dongsuk 
(2019). "Many-to-Many Voice Conversion using Cycle-
Consistent Variational Autoencoder with Multiple 
Decoders". arXiv. </p>
<p>Liu, Ming-Yu, Breuel, Thomas, and Kautz, Jan (2017). 
"Unsupervised image-to-image translation networks". 
Advances in neural information processing systems. </p>
<p>Lucas, James, Tucker, George, Grosse, Roger B, and 
Norouzi, Mohammad (2019). "Don't Blame the 
ELBO! A Linear VAE Perspective on Posterior Col-
lapse". Advances in Neural Information Processing 
Systems. </p>
<p>Mor, Noam, Wolf, Lior, Polyak, Adam, and Taigman, 
Yaniv (2018). "A universal music translation net-
work". arXiv preprint arXiv:1805.07848. </p>
<p>Moryossef, Amit, Goldberg, Yoav, and Dagan, Ido 
(2019). "Step-by-Step: Separating Planning from Re-
alization in Neural Data-to-Text Generation". Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational 
Linguistics: Human Language Technologies, NAACL-
HLT 2019. </p>
<p>Papineni, Kishore, Roukos, Salim, Ward, Todd, and 
Zhu, Wei-Jing (2002). "Bleu: a Method for Automatic 
Evaluation of Machine Translation". Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics, July 6-12, 2002, Philadelphia, 
PA, USA. </p>
<p>Raffel, Colin, Shazeer, Noam, Roberts, Adam, Lee, 
Katherine, Narang, Sharan, Matena, Michael, Zhou, 
Yanqi, Li, Wei, and Liu, Peter J (2020). "Exploring 
the limits of transfer learning with a unified text-
to-text transformer". Journal of Machine Learning 
Research 140. </p>
<p>Rezende, Danilo Jimenez, Mohamed, Shakir, and Wier-
stra, Daan (2014). "Stochastic Backpropagation and 
Approximate Inference in Deep Generative Models". 
International Conference on Machine Learning. </p>
<p>Ribeiro, Leonardo FR, Schmitt, Martin, Schütze, Hin-
rich, and Gurevych, Iryna (2020). "Investigating Pre-
trained Language Models for Graph-to-Text Genera-
tion". arXiv preprint arXiv:2007.08426. </p>
<p>Schmitt, Martin, Sharifzadeh, Sahand, Tresp, Volker, 
and Schütze, Hinrich (2020). "An Unsupervised Joint 
System for Text Generation from Knowledge Graphs 
and Semantic Parsing". Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language 
Processing. </p>
<p>Sennrich, Rico, Haddow, Barry, and Birch, Alexandra 
(2016). "Improving Neural Machine Translation Mod-
els with Monolingual Data". Proceedings of the 54th 
Annual Meeting of the Association for Computational 
Linguistics, ACL 2016. </p>
<p>Shen, Tianxiao, Lei, Tao, Barzilay, Regina, and 
Jaakkola, Tommi S. (2017). "Style Transfer from 
Non-Parallel Text by Cross-Alignment". Advances in 
neural information processing systems. </p>
<p>Shen, Xiaoyu, Chang, Ernie, Su, Hui, Niu, Cheng, and 
Klakow, Dietrich (2020). "Neural Data-to-Text Gen-
eration via Jointly Learning the Segmentation and 
Correspondence". Proceedings of the 58th Annual 
Meeting of the Association for Computational Lin-
guistics, ACL 2020. </p>
<p>Shi, Yuge, Narayanaswamy, Siddharth, Paige, Brooks, 
and Torr, Philip H. S. (2019). "Variational Mixture-
of-Experts Autoencoders for Multi-Modal Deep Gen-
erative Models". Annual Conference on Neural In-
formation Processing Systems 2019, NeurIPS 2019. </p>
<p>Sohn, Kihyuk, Lee, Honglak, and Yan, Xinchen (2015). 
"Learning structured output representation using 
deep conditional generative models". Advances in 
neural information processing systems. </p>
<p>Sundaram, Narayanan, Brox, Thomas, and Keutzer, 
Kurt (2010). "Dense Point Trajectories by GPU-
Accelerated Large Displacement Optical Flow". Com-
puter Vision -ECCV 2010, 11th European Confer-
ence on Computer Vision. </p>
<p>Tseng, Bo-Hsiang, Cheng, Jianpeng, Fang, Yimai, and 
Vandyke, David (2020). "A Generative Model for 
Joint Natural Language Understanding and Gener-
ation". Proceedings of the 58th Annual Meeting of 
the Association for Computational Linguistics, ACL 
2020. </p>
<p>Walker, Christopher, Strassel, Stephanie, Medero, Julie, 
and Maeda, Kazuaki (2006). "ACE 2005 multilin-
gual training corpus". Linguistic Data Consortium, 
Philadelphia. </p>
<p>Wang, Haoyu, Tan, Ming, Yu, Mo, Chang, Shiyu, Wang, 
Dakuo, Xu, Kun, Guo, Xiaoxiao, and Potdar, Saloni 
(2019a). "Extracting Multiple-Relations in One-Pass 
with Pre-Trained Transformers". Proceedings of the 
57th Conference of the Association for Computa-
tional Linguistics, ACL 2019. </p>
<p>Wang, Minjie, Yu, Lingfan, Zheng, Da, Gan, Quan, Gai, 
Yu, Ye, Zihao, Li, Mufei, Zhou, Jinjing, Huang, Qi, 
Ma, Chao, Huang, Ziyue, Guo, Qipeng, Zhang, Hao, </p>
<p>Lin, Haibin, Zhao, Junbo, Li, Jinyang, Smola, Alexan-
der J., and Zhang, Zheng (2019b). "Deep Graph Li-
brary: Towards Efficient and Scalable Deep Learning 
on Graphs". CoRR. </p>
<p>Wipf, David and Nagarajan, Srikantan (2007). "Beam-
forming Using the Relevance Vector Machine". Inter-
national Conference on Machine Learning. </p>
<p>Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, 
Alexei A. (2017a). "Unpaired Image-to-Image Trans-
lation Using Cycle-Consistent Adversarial Networks". 
IEEE International Conference on Computer Vision, 
ICCV 2017. </p>
<p>Zhu, Jun-Yan, Zhang, Richard, Pathak, Deepak, Dar-
rell, Trevor, Efros, Alexei A., Wang, Oliver, and 
Shechtman, Eli (2017b). "Toward Multimodal Image-
to-Image Translation". Annual Conference on Neural 
Information Processing Systems. </p>
<p>Table 5 :
5Text quality (by BLEU scores) and diversity (by the number of variations) under different dimensions of 
z. </p>
<p>No . Variations 1 -
.1Batagor, a variation of Siomay and Shumai, can be found in Indonesia, where the leader is Joko Widodo and Peanut sauce is an ingredient. -Batagor is a dish from Indonesia, where the leader is Joko Widodo and the main ingredient is Peanut sauce. It can also be served as a variation of Shumai and Siomay.-The AMC Matador, also known as "American Motors Matador", is a Mid-size car with an AMC V8 engine and is assembled in Thames, New Zealand. -AMC Matador, also known as "American Motors Matador", is a Mid-size car. It is made in Thames, New Zealand and has an AMC V8 engine.-Aleksandr Chumakov was born in Moscow and died in Russia. The leader of Moscow is Sergey Sobyanin.-Aleksandr Chumakov, who was born in Moscow, was a leader in Moscow where Sergey Sobyanin is a leader. He died in Russia.-A Wizard of Mars is written in English language spoken in Great Britain. It was published in the United States, where Barack Obama is the president. -A Wizard of Mars comes from the United States where Barack Obama is the leader and English language spoken in Great Britain.2 </p>
<p>3 </p>
<p>4 </p>
<p>6 -
6Atlantic City, New Jersey is part of Atlantic County, New Jersey Atlantic County, New Jersey, in the United States. -Atlantic City, New Jersey is part of Atlantic County, New Jersey, United States. -Albuquerque, New Mexico, United States, is lead by the New Mexico Senate, led by John Sanchez and Asian Americans. -Albuquerque, New Mexico, in the United States, is lead by the New Mexico Senate, where John Sanchez is a leader and Asian Americans are an ethnic group.-Aaron Turner plays the Electric guitar and plays Black metal, Death metal and Black metal. He also plays in the Twilight (band) and Old Man Gloom. -Aaron Turner plays the Electric guitar and plays Black metal. He is associated with the Twilight (band) and Old Man Gloom. He also plays Death metal.7 </p>
<p>8 </p>
<p>Table 6 :
6Examples of diverse text generated by CycleCVAE based on the same input knowledge graph.</p>
<p>λ2 k , ∀ k = 1, . . . , rank[B] 0, ∀ k = rank[B] + 1, . . . , r z(29)whereλ k is an eigenvalue ofΛ. Collectively then, these results imply that V *x = B, 0 P , where B ∈ R rx×rank[B] satisfies span[ B] = span[U ] = span[B].x,k </p>
<p>2 </p>
<p>2 = </p>
<p>Our code is available https://github.com/QipengGuo/ CycleGT.
Note that dim[X ]  refers to the intrinsic dimensionality of X , which could be a low-dimensional manifold embedded in a higher-dimensional ambient space; same for dim[Y].
Note that(Grover et al., 2020) addresses identifiability issues that arise during cycle training, but only in the context of strictly bijective scenarios.
Note also that becauseŷ = W y x + by is an affine function of x, including this factor in the encoder representation is redundant, i.e., it can be absorbed into µ z = W z x + bz without loss of generality.
BLEU (%) counts the 1-to 4-gram overlap between the generated sentence and ground truth.
It can be downloaded from https://webnlg-challenge.loria.fr/challenge_2017/.</p>
<p>Unsupervised Neural Machine Translation. Mikel Artetxe, Labaka, Gorka, Eneko Agirre, Kyunghyun Cho, 6th International Conference on Learning Representations. Artetxe, Mikel, Labaka, Gorka, Agirre, Eneko, and Cho, Kyunghyun (2018). "Unsupervised Neural Ma- chine Translation". 6th International Conference on Learning Representations, ICLR 2018.</p>
<p>Modulated variational autoencoders for many-to-many musical timbre transfer. Adrien Bitton, Philippe Esling, Chemla-Romeu-Santos , arXiv:1810.00222AxelarXiv preprintBitton, Adrien, Esling, Philippe, and Chemla-Romeu- Santos, Axel (2018). "Modulated variational auto- encoders for many-to-many musical timbre transfer". arXiv preprint arXiv:1810.00222.</p>
<p>Semi-Supervised Learning for Neural Machine Translation. Yong Cheng, Xu, Wei, He, Zhongjun, He, Wei, Wu, Hua, Maosong Sun, Yang Liu, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsCheng, Yong, Xu, Wei, He, Zhongjun, He, Wei, Wu, Hua, Sun, Maosong, and Liu, Yang (2016). "Semi- Supervised Learning for Neural Machine Translation". Proceedings of the 54th Annual Meeting of the Asso- ciation for Computational Linguistics, ACL 2016.</p>
<p>Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. Yunjey Choi, Choi, Minje, Kim, Munyoung, Jung - Ha, Woo, Sunghun Kim, Jaegul Choo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChoi, Yunjey, Choi, Minje, Kim, Munyoung, Ha, Jung- Woo, Kim, Sunghun, and Choo, Jaegul (2018). "Star- gan: Unified generative adversarial networks for multi-domain image-to-image translation". Proceed- ings of the IEEE conference on computer vision and pattern recognition.</p>
<p>Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models. Dai, Bin, Wang, Yu, Aston, John, Gang Hua, David Wipf, Journal of Machine Learning Research. Dai, Bin, Wang, Yu, Aston, John, Hua, Gang, and Wipf, David (2018). "Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models". Journal of Machine Learning Research.</p>
<p>Hidden Talents of the Variational Autoencoder. arXiv:1706.05148arXiv preprint-(2019). "Hidden Talents of the Variational Autoen- coder". arXiv preprint arXiv:1706.05148.</p>
<p>Diagnosing and Enhancing VAE Models. Bin Dai, David Wipf, International Conference on Learning Representations. Dai, Bin and Wipf, David (2019). "Diagnosing and Enhancing VAE Models". International Conference on Learning Representations.</p>
<p>Carl Doersch, arXiv:1606.05908Tutorial on variational autoencoders. arXiv preprintDoersch, Carl (2016). "Tutorial on variational autoen- coders". arXiv preprint arXiv:1606.05908.</p>
<p>Understanding Back-Translation at Scale. Edunov, Sergey, Ott, Myle, Michael Auli, David Grangier, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingEdunov, Sergey, Ott, Myle, Auli, Michael, and Grang- ier, David (2018). "Understanding Back-Translation at Scale". Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>The WebNLG Challenge: Generating Text from RDF Data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Perez-Beltrachini, Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationLauraGardent, Claire, Shimorina, Anastasia, Narayan, Shashi, and Perez-Beltrachini, Laura (2017). "The WebNLG Challenge: Generating Text from RDF Data". Proceedings of the 10th International Confer- ence on Natural Language Generation, INLG 2017.</p>
<p>Unsupervised Monocular Depth Estimation with Left-Right Consistency. Clément Godard, Mac Aodha, Oisin Brostow, J Gabriel, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. (2017). "Unsupervised Monocular Depth Estimation with Left-Right Consistency". 2017 IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR 2017.</p>
<p>AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows. Aditya Grover, Chute, Christopher, Shu, Rui, Zhangjie Cao, Stefano Ermon, AAAI Conference on Artificial Intelligence. Grover, Aditya, Chute, Christopher, Shu, Rui, Cao, Zhangjie, and Ermon, Stefano (2020). "AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows." AAAI Conference on Artifi- cial Intelligence.</p>
<p>Dual Learning for Machine Translation. Di He, Xia, Yingce, Qin, Tao, Wang, Liwei, Yu, Nenghai, Liu, Ma Tie-Yan, Wei-Ying , Annual Conference on Neural Information Processing Systems. He, Di, Xia, Yingce, Qin, Tao, Wang, Liwei, Yu, Neng- hai, Liu, Tie-Yan, and Ma, Wei-Ying (2016). "Dual Learning for Machine Translation". Annual Confer- ence on Neural Information Processing Systems 2016.</p>
<p>CyCADA: Cycle-Consistent Adversarial Domain Adaptation. Judy Hoffman, Tzeng, Eric, Park, Taesung, Zhu, Jun-Yan, Isola, Phillip, Saenko, Kate, Alexei A Efros, Trevor Darrell, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningHoffman, Judy, Tzeng, Eric, Park, Taesung, Zhu, Jun- Yan, Isola, Phillip, Saenko, Kate, Efros, Alexei A., and Darrell, Trevor (2018). "CyCADA: Cycle- Consistent Adversarial Domain Adaptation". Pro- ceedings of the 35th International Conference on Ma- chine Learning, ICML 2018.</p>
<p>Cycle-consistency Training for End-to-end Speech Recognition. Takaaki Hori, Ramón Astudillo, Fernández, Hayashi, Tomoki, Zhang, Yu, Shinji Watanabe, Jonathan Roux, Le, IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSP 2019Hori, Takaaki, Astudillo, Ramón Fernández, Hayashi, Tomoki, Zhang, Yu, Watanabe, Shinji, and Roux, Jonathan Le (2019). "Cycle-consistency Training for End-to-end Speech Recognition". IEEE International Conference on Acoustics, Speech and Signal Process- ing, ICASSP 2019.</p>
<p>Disentangling Factors of Variation with Cycle-Consistent Variational Auto-encoders. Ananya Jha, Harsh, Anand, Saket, Maneesh Singh, V S R Veeravasarapu, Computer Vision -ECCV 2018 -15th European Conference. Jha, Ananya Harsh, Anand, Saket, Singh, Maneesh, and Veeravasarapu, V. S. R. (2018). "Disentangling Factors of Variation with Cycle-Consistent Varia- tional Auto-encoders". Computer Vision -ECCV 2018 -15th European Conference.</p>            </div>
        </div>

    </div>
</body>
</html>
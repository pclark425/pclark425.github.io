<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6383 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6383</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6383</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-277451917</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.23084v1.pdf" target="_blank">The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6383.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6383.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiReF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Reasoning Features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of linear directions in the transformer residual stream that separate reasoning‑intensive from memory‑intensive questions; derived by difference‑of‑means of last‑token residual activations and used to causally steer models between reasoning and memorization modes via additive/ablative interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B, Gemma2-9B, Mistral-7B-v0.3, OLMo2-7B (base and instruction-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–9B (various models studied: 7B, 8B, 9B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K, GSM-Symbolic, MGSM, MMLU-Pro (reasoning split), PopQA, C-Eval-H (humanities) (LiReF extracted and evaluated across these benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic/grade-school math problems and general reasoning tasks (math word problems, logical/scientific questions)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems and multiple-choice; few-shot or 0-shot depending on dataset</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to college-level reasoning (GSM-8K: grade-school math; MMLU-Pro: college-level across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot (paper: 8-shot for GSM-8K / MGSM / GSM-Symbolic; 5-shot for MMLU-Pro and C-Eval-H; 0-shot for PopQA); interventions applied at inference by adding/subtracting α * r(l) to residual activations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (task accuracy / exact-match style metrics used per benchmark); also Spearman correlations for representational alignment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Intervention improved accuracy across datasets and models; e.g., on a curated subset of 184 MMLU-Pro examples where labels conflicted with feature sign, accuracy rose from 0.00 to 0.21 after LiReF sign-correcting intervention; performance on GSM-8K and GSM-Symbolic rose monotonically as positive α increased (exact per-model numbers reported in paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>LiReFs computed as layerwise difference-of-means vector r(l) between reasoning and memory question centroids; PCA shows linear separability in residual stream (first PC captures most mean difference); layerwise cosine similarity between last-token residuals and r(l) peaks in middle layers; strong correlation between LiReF projection and external reasoning score (Spearman ρ=0.840 for LLaMA3-8B-base and ρ=0.752 for Mistral-7B-base); causal interventions (additive and ablation via projection removal) validate functional role of the direction.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When LiReF activation is weak or has wrong sign, models tend to fall back to memorized answers or produce incorrect reasoning (examples include misapplied formulas and incorrect short numeric answers); dataset contamination/memorization can mask true reasoning ability (GSM-8K shows signs of data leakage where suppression of LiReF sometimes slightly improves performance), and over-reliance on memorization causes poor generalization under input perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper studied only small/medium models (7–9B) and observed that LiReF-like directions exist in pretrained base models and persist after instruction tuning; the authors note that scaling effects are unknown and that larger models or more compute may change reasoning behavior (limitation acknowledged).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6383.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6383.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM-Symbolic (variant of GSM-8K for systematic generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic variant of GSM-8K that uses 100 templates with 50 instantiations each (5,000 problems) to test whether models generalize symbolically across systematic numeric and structural perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B, Gemma2-9B, Mistral-7B-v0.3, OLMo2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–9B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-Symbolic (paper's constructed variant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>systematic arithmetic generalization / multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language templates instantiated with varied numerical conditions (template + varied instances)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / template-driven multi-step arithmetic (designed to check generalization rather than raw difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>8-shot (paper used 8-shot for GSM variants); LiReF interventions evaluated at inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Models' accuracy on GSM-Symbolic increases with positive LiReF intervention α; conversely, suppressing LiReF causes a larger performance drop on GSM-Symbolic than on GSM-8K, indicating sensitivity to reasoning feature activation (exact numeric curves shown in paper Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>GSM-Symbolic used to distinguish genuine reasoning (generalization across templates) from case-based memorization; LiReF projections strongly predict whether a solution uses generalizable reasoning vs. memorized instance recall.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Pronounced performance drop under LiReF suppression, indicating reliance on the reasoning direction for generalization; models that solved GSM-8K via memorization fail on GSM-Symbolic variants.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Only small models evaluated; authors conclude that GSM-Symbolic is effective to reveal lack of generalization and data leakage effects present in GSM-8K for the studied model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6383.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6383.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM-8K (grade-school math dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used benchmark of ~8.5K grade-school multi-step math problems used to evaluate arithmetic and chain-of-thought reasoning in LLMs; used here as a representative arithmetic reasoning dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B, Gemma2-9B, Mistral-7B-v0.3, OLMo2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–9B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic / grade-school math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems paired with answers (paper used few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multistep arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>8-shot few-shot prompting (paper's inference settings); interventions applied at inference-time (α added/subtracted along LiReF)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Models' accuracy on GSM-8K generally increases when positive LiReF is added; however, authors observe possible data leakage: suppressing LiReF slightly improved or did not strongly hurt some models at small negative α (e.g., α = -0.05), suggesting memorized examples in training data for GSM-8K.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Comparison between GSM-8K and GSM-Symbolic reveals that GSM-8K performance can be inflated by memorization; LiReF projection and intervention experiments show that some GSM-8K instances lie in the memorization subspace (negative or small LiReF activation) whereas genuine reasoning instances activate LiReF positively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Data contamination / memorization leading to apparent success without generalizable reasoning; some instances solved via template memorization rather than algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not evaluated across large scale ranges in this paper; authors note that observed leakage and LiReF dynamics were measured on 7–9B models and may change with scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6383.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6383.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3 8B (base and instruction-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One of the decoder-only transformer models analyzed; used to extract LiReFs, visualize PCA separability between reasoning and memory questions, compute layerwise cosine similarities, and validate causal interventions on arithmetic and broader reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B (base and instruct variants evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K, GSM-Symbolic, MMLU-Pro, MGSM, PopQA, C-Eval-H</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic (GSM-8K/GSM-Symbolic), general reasoning (MMLU-Pro)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems and multiple-choice; few-shot prompting (8-shot for GSM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to college-level depending on dataset</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot (8-shot for GSM variants; 5-shot for MMLU-Pro); inference-time LiReF additive/ablative interventions with α tuned on validation (α stepped in 0.05 increments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy; representational correlation (Spearman) between LiReF projection and GPT-4o reasoning score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Strong representational correlation: Spearman ρ = 0.840 between GPT-4o reasoning score and LiReF projection for LLaMA3-8B-base; intervention experiments showed accuracy improvements on reasoning datasets when positively steering LiReF (exact per-benchmark numbers tabulated in paper's Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>PCA of last-token residuals shows separability of memory vs reasoning samples; LiReF activations peak in middle layers; layerwise cosine similarity profiles for base and instruction-tuned variants are highly consistent, suggesting LiReFs originate during pretraining; causal ablation/addition of r(l) demonstrates functional role.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Insufficient activation of LiReF yields memorization-mode answers; dataset contamination can cause reliance on case recall rather than algorithmic computation; sample-level misplacement across subspaces leads to errors that can be corrected by sign-changing LiReF interventions (example subset accuracy 0 → 0.21).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Authors observed LiReF presence in pretrained base and retained after instruction tuning for the 8B model; they caution that scaling to larger models may change dynamics and was not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GSM8k <em>(Rating: 2)</em></li>
                <li>GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Physics of language models: Part 2.1, grade-school math and the hidden reasoning process <em>(Rating: 2)</em></li>
                <li>Case-based or rule-based: How do transformers do the math? <em>(Rating: 2)</em></li>
                <li>Towards a mechanistic interpretation of multi-step reasoning capabilities of language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6383",
    "paper_id": "paper-277451917",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "LiReF",
            "name_full": "Linear Reasoning Features",
            "brief_description": "A set of linear directions in the transformer residual stream that separate reasoning‑intensive from memory‑intensive questions; derived by difference‑of‑means of last‑token residual activations and used to causally steer models between reasoning and memorization modes via additive/ablative interventions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B, Gemma2-9B, Mistral-7B-v0.3, OLMo2-7B (base and instruction-tuned variants)",
            "model_family": "decoder-only transformer",
            "model_size": "7B–9B (various models studied: 7B, 8B, 9B)",
            "training_data_description": null,
            "benchmark_name": "GSM-8K, GSM-Symbolic, MGSM, MMLU-Pro (reasoning split), PopQA, C-Eval-H (humanities) (LiReF extracted and evaluated across these benchmarks)",
            "task_type": "multi-step arithmetic/grade-school math problems and general reasoning tasks (math word problems, logical/scientific questions)",
            "problem_format": "natural-language word problems and multiple-choice; few-shot or 0-shot depending on dataset",
            "difficulty_level": "grade-school to college-level reasoning (GSM-8K: grade-school math; MMLU-Pro: college-level across domains)",
            "prompting_method": "few-shot (paper: 8-shot for GSM-8K / MGSM / GSM-Symbolic; 5-shot for MMLU-Pro and C-Eval-H; 0-shot for PopQA); interventions applied at inference by adding/subtracting α * r(l) to residual activations",
            "performance_metric": "accuracy (task accuracy / exact-match style metrics used per benchmark); also Spearman correlations for representational alignment",
            "performance_value": "Intervention improved accuracy across datasets and models; e.g., on a curated subset of 184 MMLU-Pro examples where labels conflicted with feature sign, accuracy rose from 0.00 to 0.21 after LiReF sign-correcting intervention; performance on GSM-8K and GSM-Symbolic rose monotonically as positive α increased (exact per-model numbers reported in paper Table 1).",
            "internal_analysis": "LiReFs computed as layerwise difference-of-means vector r(l) between reasoning and memory question centroids; PCA shows linear separability in residual stream (first PC captures most mean difference); layerwise cosine similarity between last-token residuals and r(l) peaks in middle layers; strong correlation between LiReF projection and external reasoning score (Spearman ρ=0.840 for LLaMA3-8B-base and ρ=0.752 for Mistral-7B-base); causal interventions (additive and ablation via projection removal) validate functional role of the direction.",
            "failure_modes": "When LiReF activation is weak or has wrong sign, models tend to fall back to memorized answers or produce incorrect reasoning (examples include misapplied formulas and incorrect short numeric answers); dataset contamination/memorization can mask true reasoning ability (GSM-8K shows signs of data leakage where suppression of LiReF sometimes slightly improves performance), and over-reliance on memorization causes poor generalization under input perturbations.",
            "scaling_trend": "Paper studied only small/medium models (7–9B) and observed that LiReF-like directions exist in pretrained base models and persist after instruction tuning; the authors note that scaling effects are unknown and that larger models or more compute may change reasoning behavior (limitation acknowledged).",
            "uuid": "e6383.0",
            "source_info": {
                "paper_title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GSM-Symbolic",
            "name_full": "GSM-Symbolic (variant of GSM-8K for systematic generalization)",
            "brief_description": "A diagnostic variant of GSM-8K that uses 100 templates with 50 instantiations each (5,000 problems) to test whether models generalize symbolically across systematic numeric and structural perturbations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B, Gemma2-9B, Mistral-7B-v0.3, OLMo2-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B–9B",
            "training_data_description": null,
            "benchmark_name": "GSM-Symbolic (paper's constructed variant)",
            "task_type": "systematic arithmetic generalization / multi-step math word problems",
            "problem_format": "natural-language templates instantiated with varied numerical conditions (template + varied instances)",
            "difficulty_level": "grade-school / template-driven multi-step arithmetic (designed to check generalization rather than raw difficulty)",
            "prompting_method": "8-shot (paper used 8-shot for GSM variants); LiReF interventions evaluated at inference",
            "performance_metric": "accuracy",
            "performance_value": "Models' accuracy on GSM-Symbolic increases with positive LiReF intervention α; conversely, suppressing LiReF causes a larger performance drop on GSM-Symbolic than on GSM-8K, indicating sensitivity to reasoning feature activation (exact numeric curves shown in paper Figure 6).",
            "internal_analysis": "GSM-Symbolic used to distinguish genuine reasoning (generalization across templates) from case-based memorization; LiReF projections strongly predict whether a solution uses generalizable reasoning vs. memorized instance recall.",
            "failure_modes": "Pronounced performance drop under LiReF suppression, indicating reliance on the reasoning direction for generalization; models that solved GSM-8K via memorization fail on GSM-Symbolic variants.",
            "scaling_trend": "Only small models evaluated; authors conclude that GSM-Symbolic is effective to reveal lack of generalization and data leakage effects present in GSM-8K for the studied model sizes.",
            "uuid": "e6383.1",
            "source_info": {
                "paper_title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GSM-8K",
            "name_full": "GSM-8K (grade-school math dataset)",
            "brief_description": "A widely used benchmark of ~8.5K grade-school multi-step math problems used to evaluate arithmetic and chain-of-thought reasoning in LLMs; used here as a representative arithmetic reasoning dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B, Gemma2-9B, Mistral-7B-v0.3, OLMo2-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B–9B",
            "training_data_description": null,
            "benchmark_name": "GSM-8K",
            "task_type": "multi-step arithmetic / grade-school math word problems",
            "problem_format": "natural-language word problems paired with answers (paper used few-shot prompting)",
            "difficulty_level": "grade-school multistep arithmetic",
            "prompting_method": "8-shot few-shot prompting (paper's inference settings); interventions applied at inference-time (α added/subtracted along LiReF)",
            "performance_metric": "accuracy",
            "performance_value": "Models' accuracy on GSM-8K generally increases when positive LiReF is added; however, authors observe possible data leakage: suppressing LiReF slightly improved or did not strongly hurt some models at small negative α (e.g., α = -0.05), suggesting memorized examples in training data for GSM-8K.",
            "internal_analysis": "Comparison between GSM-8K and GSM-Symbolic reveals that GSM-8K performance can be inflated by memorization; LiReF projection and intervention experiments show that some GSM-8K instances lie in the memorization subspace (negative or small LiReF activation) whereas genuine reasoning instances activate LiReF positively.",
            "failure_modes": "Data contamination / memorization leading to apparent success without generalizable reasoning; some instances solved via template memorization rather than algorithmic computation.",
            "scaling_trend": "Not evaluated across large scale ranges in this paper; authors note that observed leakage and LiReF dynamics were measured on 7–9B models and may change with scale.",
            "uuid": "e6383.2",
            "source_info": {
                "paper_title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaMA3-8B",
            "name_full": "LLaMA 3 8B (base and instruction-tuned variants)",
            "brief_description": "One of the decoder-only transformer models analyzed; used to extract LiReFs, visualize PCA separability between reasoning and memory questions, compute layerwise cosine similarities, and validate causal interventions on arithmetic and broader reasoning benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B (base and instruct variants evaluated separately)",
            "model_family": "decoder-only transformer",
            "model_size": "8B",
            "training_data_description": null,
            "benchmark_name": "GSM-8K, GSM-Symbolic, MMLU-Pro, MGSM, PopQA, C-Eval-H",
            "task_type": "arithmetic (GSM-8K/GSM-Symbolic), general reasoning (MMLU-Pro)",
            "problem_format": "natural-language word problems and multiple-choice; few-shot prompting (8-shot for GSM variants)",
            "difficulty_level": "grade-school to college-level depending on dataset",
            "prompting_method": "few-shot (8-shot for GSM variants; 5-shot for MMLU-Pro); inference-time LiReF additive/ablative interventions with α tuned on validation (α stepped in 0.05 increments)",
            "performance_metric": "accuracy; representational correlation (Spearman) between LiReF projection and GPT-4o reasoning score",
            "performance_value": "Strong representational correlation: Spearman ρ = 0.840 between GPT-4o reasoning score and LiReF projection for LLaMA3-8B-base; intervention experiments showed accuracy improvements on reasoning datasets when positively steering LiReF (exact per-benchmark numbers tabulated in paper's Table 1).",
            "internal_analysis": "PCA of last-token residuals shows separability of memory vs reasoning samples; LiReF activations peak in middle layers; layerwise cosine similarity profiles for base and instruction-tuned variants are highly consistent, suggesting LiReFs originate during pretraining; causal ablation/addition of r(l) demonstrates functional role.",
            "failure_modes": "Insufficient activation of LiReF yields memorization-mode answers; dataset contamination can cause reliance on case recall rather than algorithmic computation; sample-level misplacement across subspaces leads to errors that can be corrected by sign-changing LiReF interventions (example subset accuracy 0 → 0.21).",
            "scaling_trend": "Authors observed LiReF presence in pretrained base and retained after instruction tuning for the 8B model; they caution that scaling to larger models may change dynamics and was not evaluated.",
            "uuid": "e6383.3",
            "source_info": {
                "paper_title": "The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GSM8k",
            "rating": 2
        },
        {
            "paper_title": "GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "gsmsymbolic_understanding_the_limitations_of_mathematical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Physics of language models: Part 2.1, grade-school math and the hidden reasoning process",
            "rating": 2,
            "sanitized_title": "physics_of_language_models_part_21_gradeschool_math_and_the_hidden_reasoning_process"
        },
        {
            "paper_title": "Case-based or rule-based: How do transformers do the math?",
            "rating": 2,
            "sanitized_title": "casebased_or_rulebased_how_do_transformers_do_the_math"
        },
        {
            "paper_title": "Towards a mechanistic interpretation of multi-step reasoning capabilities of language models",
            "rating": 1,
            "sanitized_title": "towards_a_mechanistic_interpretation_of_multistep_reasoning_capabilities_of_language_models"
        }
    ],
    "cost": 0.0158195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction
Dec 22, 2021</p>
<p>Yihuai Hong 
Dian Zhou 
University of Illinois at Urbana-Champaign</p>
<p>Meng Cao 
McGill University</p>
<p>Lei Yu 
University of Toronto</p>
<p>Zhijing Jin 
Max Planck Institute for Intelligent Systems
TuebingenGermany</p>
<p>University of Toronto</p>
<p>Vector Institute</p>
<p>Trenton Bricken 
Adly Templeton 
Joshua Batson 
Brian Chen 
Adam Jermyn 
Tom Conerly 
Nick Turner 
Cem Anil 
Carson Denison 
Amanda Askell 
Robert Lasenby 
Yifan Wu 
Shauna Kravec 
Nicholas Schiefer 
Tim Maxwell 
Nicholas Joseph 
Zac Hatfield-Dodds 
Alex Tamkin 
Karina Nguyen 
Brayden Mclean 
Josiah E Burke 
Tristan Hume 
Shan Carter 
Tom Henighan 
Christopher 2023 Olah 
Nicholas Carlini 
Daphne Ippolito 
Matthew Jagielski 
Katherine Lee 
Florian Tramer 
Chiyuan Zhang 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluat 
Xinyun Chen 
Maxwell Lin 
Nathanael Schärli 
Denny 2024 Zhou 
Karl Cobbe 
Vineet Kosaraju 
Mark Chen 
Jacob Hilton 
Reiichiro Nakano 
Christopher Hesse 
Nouha Dziri 
Ximing Lu 
Melanie Sclar 
Lor- Raine Xiang 
Liwei Li 
BillYuchen Jiang 
Sean Lin 
Welleck 
Nelson Elhage 
Catherine Olsson 
Dawn Drain 
Neel Nanda 
Ben Mann 
Yuntao Bai 
Anna Chen 
Nova Dassarma 
Deep Ganguli 
Danny Hernandez 
Andy Jones 
Jackson Kernion 
Liane Lovitt 
Kamal Ndousse 
Tom Brown 
Jack Clark 
Jared Kaplan 
Mor Geva 
Jasmijn Bastings 
Katja Filippova 
Amir 2023 Globerson 
Aaron Grattafiori 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al- Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schel- Ten 
Alex Vaughan 
Amy Yang 
Angela Fan 
Anirudh Goyal 
Anthony Hartshorn 
Aobo Yang 
Archi Mi- Tra 
Archie Sravankumar 
Artem Korenev 
Arthur Hinsvark 
Arun Rao 
Aston Zhang 
Aurelien Ro- Driguez 
Austen Gregerson 
Ava Spataru 
Baptiste Roziere 
Bethany Biron 
Binh Tang 
Bobbie Chern 
Charlotte Caucheteux 
Chaya Nayak 
Chloe Bi 
Chris Marra 
Chris Mcconnell 
Christian Keller 
Christophe Touret 
Chunyang Wu 
Corinne Wong 
Cristian Canton Ferrer 
Cyrus Nikolaidis 
Damien Al- Lonsius 
Daniel Song 
Danielle Pintz 
Danny Livshits 
Danny Wyatt 
David Esiobu 
Dhruv Choudhary 
Dhruv Mahajan 
Diego Garcia-Olano 
Diego Perino 
Dieuwke Hupkes 
Egor Lakomkin 
Ehab Albadawy 
Elina Lobanova 
Emily Dinan 
Eric Michael Smith 
Filip Radenovic 
Francisco Guzmán 
Frank Zhang 
Gabriel Synnaeve 
Gabrielle Lee 
Georgia Lewis 
Govind Thattai 
Graeme Nail 
Mi- Alon Gregoire 
Guan Pang 
Guillem Cucurell 
Hailey Nguyen 
Hannah Korevaar 
Hu Xu 
Hugo Touvron 
ImanolIliyan Zarov 
Arrieta Ibarra 
Isabel Kloumann 
Is- Han Misra 
Ivan Evtimov 
Jack Zhang 
Jade Copet 
Jaewon Lee 
Jan Geffert 
Jana Vranes 
Jason Park 
Jay Mahadeokar 
Jeet Shah 
Jelmer Van Der Linde 
Jennifer Billock 
Jenny Hong 
Jenya Lee 
Jeremy Fu 
Jianfeng Chi 
Jianyu Huang 
Jiawen Liu 
Jie Wang 
Jiecao Yu 
Joanna Bitton 
Joe Spisak 
Jongsoo Park 
Joseph Rocca 
Joshua Johnstun 
Joshua Saxe 
Jun- Teng Jia 
Kalyan Vasuden Alwala 
Karthik Prasad 
Kartikeya Upasani 
Kate Plawiak 
Keqian Li 
Kenneth Heafield 
Kevin Stone 
Khalid El-Arini 
Krithika Iyer 
Kshitiz Malik 
Kuenley Chiu 
Kunal Bhalla 
Kushal Lakhotia 
Lauren Rantala-Yeary 
Laurens Van Der Maaten 
Lawrence Chen 
Liang Tan 
Liz Jenkins 
Louis Martin 
Lovish Madaan 
Lubo Malo 
Lukas Blecher 
Lukas Landzaat 
Luke De Oliveira 
Madeline Muzzi 
Mahesh Pasupuleti 
Mannat Singh 
Manohar Paluri 
Marcin Kardas 
Maria Tsimpoukelli 
Mathew Oldham 
Mathieu Rita 
Maya Pavlova 
Melanie Kam- Badur 
Mike Lewis 
MiteshMin Si 
Kumar Singh 
Mona Hassan 
Naman Goyal 
Narjes Torabi 
Niko- Lay Bashlykov 
Nikolay Bogoychev 
Niladri Chatterji 
Ning Zhang 
Olivier Duchenne 
Onur Çelebi 
Patrick Alrassy 
Pengchuan Zhang 
PetarPengwei Li 
Peter Weng 
Prajjwal Bhargava 
Pratik Dubal 
PunitPraveen Krishnan 
Singh Koura 
Puxin Xu 
Qing He 
Qingxiao Dong 
Ragavan Srinivasan 
Raj Ganapathy 
Ramon Calderer 
Ricardo Silveira Cabral 
Robert Stojnic 
Roberta Raileanu 
Rohan Maheswari 
Rohit Girdhar 
Rohit Patel 
Romain Sauvestre 
Ron- Nie Polidoro 
Roshan Sumbaly 
Ross Taylor 
Ruan Silva 
Rui Hou 
Rui Wang 
Saghar Hosseini 
Sa- Hana Chennabasappa 
Sanjay Singh 
Sean Bell 
Seo- Hyun Sonia Kim 
Sergey Edunov 
Shaoliang Nie 
Sha- Ran Narang 
Sharath Raparthy 
Sheng Shen 
Shengye Wan 
Shruti Bhosale 
Shun Zhang 
Simon Van- Denhende 
New York University</p>
<p>Soumya Batra 
Spencer Whitman 
Sten Sootla 
Stephane Collot 
Suchin Gururangan 
Syd- Ney Borodinsky 
Tamar Herman 
Tara Fowler 
Tarek Sheasha 
Thomas Georgiou 
Thomas Scialom 
Tobias Speckbacher 
Todor Mihaylov 
Tong Xiao 
Ujjwal Karn 
Vedanuj Goswami 
Vibhor Gupta 
Vignesh Ramanathan 
Viktor Kerkez 
Vincent Gonguet 
Vir- Ginie Do 
Vish Vogeti 
Vítor Albiero 
Vladan Petro- Vic 
Weiwei Chu 
Wenhan Xiong 
Wenyin Fu 
Yasmine Gaur 
Yi Babaei 
Yiwen Wen 
Yuchen Song 
Yue Zhang 
Yuning Li 
Zacharie Delpierre Mao 
Zheng Coudert 
Zhengxing Yan 
Zoe Chen 
Aaditya Papakipos 
Aayushi Singh 
Abha Sri- Vastava 
Adam Jain 
Adam Kelsey 
Adithya Shajnfeld 
Adolfo Gangidi 
Ahuva Victoria 
Ajay Goldstand 
Ajay Menon 
Alex Sharma 
Alexei Boesenberg 
Allie Baevski 
Amanda Feinstein 
Amit Kallet 
Amos San- Gani 
Anam Teo 
Andrei Yunus 
An- Dres Lupu 
Andrew Alvarado 
Andrew Caples 
Andrew Gu 
Andrew Ho 
Andrew Poulton 
AnkitRamchan- Dani Ryan 
Annie Dong 
Annie Franco 
Anuj Goyal 
Apara- Jita Saraf 
Arkabandhu Chowdhury 
Ashley Gabriel 
Ashwin Bharambe 
Assaf Eisenman 
Azadeh Yaz- Dan 
Beau James 
Ben Maurer 
Benjamin Leonhardi 
Bernie Huang 
Beth Loyd 
Beto De Paola 
Bhargavi Paranjape 
Bing Liu 
Bo Wu 
Boyu Ni 
Braden Han- Cock 
Bram Wasti 
Brandon Spence 
Brani Stojkovic 
Brian Gamido 
Britt Montalvo 
Carl Parker 
Carly Burton 
Catalina Mejia 
Ce Liu 
Changhan Wang 
Changkyu Kim 
Chao Zhou 
Chester Hu 
Ching- Hsiang Chu 
Chris Cai 
Chris Tindal 
Christoph Fe- Ichtenhofer 
Cynthia Gao 
Damon Civin 
Dana Beaty 
Elaine Montgomery 
Eleonora Presani 
Emily Hahn 
Emily Wood 
Eric-Tuan Le 
Erik Brinkman 
Este- Ban Arcaute 
Evan Dunbar 
Evan Smothers 
Fei Sun 
Felix Kreuk 
Feng Tian 
Filippos Kokkinos 
Firat Ozgenel 
Francesco Caggioni 
Frank Kanayet 
Frank Seide 
Gabriela Medina Florez 
Gabriella Schwarz 
Gada Badeer 
Georgia Swee 
Gil Halpern 
Grant Herman 
Grigory Sizov 
Guangyi 
Guna Zhang 
Hakan Lakshminarayanan 
Hamid Inan 
Han Shojanaz- Eri 
Hannah Zou 
Hanwen Wang 
Haroun Zha 
Harrison Habeeb 
Helen Rudolph 
Henry Suk 
Hunter As- Pegren 
Hongyuan Goldman 
Ibrahim Zhan 
Igor Damlaj 
Igor Molybog 
Ilias Tufanov 
Leontiadis 
Nifer Chan 
Jenny Zhen 
Jeremy Reizenstein 
Jeremy Teboul 
Jessica Zhong 
Jian Jin 
Jingyi Yang 
Joe Cummings 
Jon Carvill 
Jon Shepard 
Jonathan Mc- Phie 
Jonathan Torres 
Josh Ginsburg 
Junjie Wang 
Kai Wu 
Kam Hou 
Karan Saxena 
Kartikay Khan- Delwal 
Katayoun Zand 
Kathy Matosich 
Kaushik Veeraraghavan 
Kelly Michelena 
Ki- Ran Jagadeesh 
Kun Huang 
Kunal Chawla 
Kyle Huang 
Lailin Chen 
Lavender A,Lakshya Garg 
Leandro Silva 
Lee Bell 
Lei Zhang 
Liangpeng Guo 
Licheng Yu 
Liron Moshkovich 
Luca Wehrst- Edt 
Madian Khabsa 
Manav Avalani 
Manish Bhatt 
Martynas Mankus 
Matan Hasson 
Matthew Lennie 
Matthias Reso 
Maxim Groshev 
Maxim Naumov 
Maya Lathi 
Meghan Keneally 
Miao Liu 
Michael L Seltzer 
Michal Valko 
MihirMichelle Restrepo 
Mik Vyatskov 
Mikayel Samvelyan 
Mike Clark 
Mike Macey 
Mike Wang 
Miquel Jubert Hermoso 
Mo Metanat 
Mohammad Rastegari 
Munish Bansal 
Nandhini Santhanam 
Natascha Parks 
Natasha White 
Navyata Bawa 
Nayan Singhal 
Nick Egebo 
Nicolas Usunier 
NikolayNikhil Mehta 
Pavlovich Laptev 
Ning Dong 
Norman Cheng 
Oleg Chernoguz 
Olivia Hart 
Omkar Salpekar 
Ozlem Kalinli 
Parkin Kent 
Parth Parekh 
Paul Saab 
Pavan Balaji 
Pe- Dro Rittner 
Philip Bontrager 
Pierre Roux 
Piotr Dollar 
Polina Zvyagina 
Prashant Ratanchandani 
Pritish Yuvraj 
Qian Liang 
Rachad Alao 
Rachel Rodriguez 
Rafi Ayub 
Raghotham Murthy 
Raghu Nayani 
Rahul Mitra 
Rangaprabhu Parthasarathy 
Raymond Li 
Rebekkah Hogan 
Robin Battey 
Rocky Wang 
Russ Howes 
Ruty Rinott 
Sachin Mehta 
Sachin Siby 
Jayesh Sai 
Samyak Bondu 
Sara Datta 
Sara Chugh 
Sargun Hunt 
Sasha Dhillon 
Satadru Sidorov 
Saurabh Pan 
Saurabh Mahajan 
Seiji Verma 
Sharadh Yamamoto 
Shaun Ramaswamy 
Lind- Openai 
Aaron Hurst 
Adam Lerer 
Adam P Goucher 
Adam Perelman 
Aditya Ramesh 
Aidan Clark 
A J Ostrow 
Akila Welihinda 
Alan Hayes 
Aleksander M Ądry 
Alex Baker-Whitcomb 
Alex Beutel 
Alex Borzunov 
Alex Carney 
Alex Chow 
Alexander Kirillov 
Alex Renzin 
Alex Tachard Passos 
Alexi Christakis 
Alexis Conneau 
Ali Kamali 
Allan Jabri 
Allison Moyer 
Allison Tam 
Amadou Crookes 
Amin Tootoochian 
Amin Tootoonchian 
Ananya Kumar 
Andrea Vallone 
Andrej Karpathy 
Andrew Braunstein 
Andrew Cann 
Andrew Codispoti 
An- Drew Galu 
Andrew Kondrich 
Andrew Tulloch 
An- Drey Mishchenko 
Angela Baek 
Angela Jiang 
An- Toine Pelisse 
Antonia Woodford 
Anuj Gosalia 
Arka Dhar 
Ashley Pantuliano 
Avi Nayak 
Avital Oliver 
Barret Zoph 
Behrooz Ghorbani 
Ben Leimberger 
Ben Rossen 
Ben Sokolowsky 
Ben Wang 
Benjamin Zweig 
Beth Hoover 
Blake Samic 
Bobby Spero 
Bogo Giertler 
Bowen Cheng 
Brad Lightcap 
Brandon Walkin 
Brendan Quinn 
Brian Guarraci 
Brian Hsu 
Bright Kellogg 
Camillo Lugaresi 
Carroll Wainwright 
Cary Bassin 
Cary Hudson 
Casey Chu 
Chad Nelson 
Chak Li 
Jun Chan 
Channing Shern 
Char- Lotte Conger 
Chelsea Barette 
Chen Voss 
Cheng Ding 
Chong Lu 
Chris Zhang 
Chris Beaumont 
Chris Hallacy 
Christian Koch 
Christina Gibson 
Christine Kim 
Christine Choi 
Christopher Mcleavey 
Clau- Dia Hesse 
Clemens Fischer 
Coley Winter 
Colin Czarnecki 
Colin Jarvis 
Constantin Wei 
Dane Koumouzelis 
Daniel Sherburn 
Daniel Kappler 
Daniel Levin 
David Levy 
David Carr 
David Farhi 
Mely 
Robin- Son David 
David Sasaki 
Denny Jin 
Dev Valladares 
Dim- Itris Tsipras 
DucDoug Li 
Phong Nguyen 
Duncan Findlay 
Edede Oiwoh 
Edmund Wong 
Ehsan As- Dar 
Elizabeth Proehl 
Elizabeth Yang 
Eric Antonow 
Eric Kramer 
Eric Peterson 
Eric Sigler 
Eric Wal- Lace 
Eugene Brevdo 
Evan Mays 
Farzad Khorasani 
Filippo Raso 
Francis Zhang 
Fred Von Lohmann 
Freddie Sulit 
Gabriel Goh 
Gene Oden 
Geoff Salmon 
Giulio Starace 
Hadi Salman 
Haiming Bao 
Haitang Hu 
Hannah Wong 
Haoyu Wang 
Heather Schmidt 
Heather Whitney 
Hendrik Kirchner 
Hongyu Ren 
HyungHuiwen Chang 
Won Chung 
Ian Kivlichan 
Ian O'connell 
Ian Osband 
Ian Sil- Ber 
Ian Sohl 
Ibrahim Okuyucu 
Ikai Lan 
Ilya Kostrikov 
Ingmar Kanitscheider 
Ishaan Gulrajani 
Jacob Coxon 
Jacob Menick 
Kai Fricke 
Karan Hayashi 
Katy Singhal 
Kavin Shi 
Kayla Karthik 
Kendra Wood 
Kenny Rimbach 
Kenny Hsu 
Keren Nguyen 
Kevin Gu-Lemberg 
Kevin Button 
Kiel Liu 
Krithika Howe 
Kyle Muthukumar 
LamaAhmad Luther 
Larry Kai 
Lauren Itow 
Lau- Ren Workman 
Leher Pathak 
Leo Chen 
Li Jing 
Lia Guy 
Liam Fedus 
Liang Zhou 
Lien Mamitsuka 
Lil- Ian Weng 
Lindsay Mccallum 
Lindsey Held 
Long Ouyang 
Louis Feuvrier 
Lu Zhang 
Lukas Kon- Draciuk 
Luke Hewitt 
Luke Metz 
Lyric Doshi 
Mada Aflak 
Maddie Simens 
Madelaine Boyd 
Madeleine Thompson 
Marat Dukhan 
Mark Chen 
Mark Gray 
Mark Hudnall 
Marvin Zhang 
Marwan Aljubeh 
Mateusz Litwin 
Matthew Zeng 
Max Johnson 
Maya Shetty 
Mayank Gupta 
Meghan Shah 
MengMehmet Yatbaz 
Jia Yang 
Mengchao Zhong 
Mia Glaese 
Mianna Chen 
Michael Jan- Ner 
Michael Lampe 
Michael Wu 
Michele Wang 
Michelle Fradin 
Michelle Pokrass 
Miguel Castro 
Miguel Oom 
Temudo De Castro 
Miles Wang 
Mi- Nal Khan 
Molly Lin 
Murat Yesildal 
Nacho Soto 
Natalia Gimelshein 
Na- Talie Cone 
Natalie Staudacher 
Olivier Godement 
Owen Campbell-Moore 
Patrick Chao 
Paul Mcmillan 
Pavel Belov 
Pe- terPeng Su 
Peter Bak 
Peter Bakkum 
Peter Deng 
Peter Dolan 
Peter Hoeschele 
Phil Welinder 
Philip Tillet 
Philippe Pronin 
Prafulla Tillet 
Qiming Dhariwal 
Rachel Yuan 
Rachel Dias 
Rahul Lim 
Ra- Jan Arora 
Randall Troll 
Rapha Gontijo Lin 
Raul Lopes 
Reah Puri 
Reimar Miyara 
Renaud Leike 
Reza Gaubert 
Ricky Zamani 
Rob Wang 
Rob Donnelly 
Rocky Honsby 
Rohan Smith 
Rohit Sahai 
Romain Ramchan- Dani 
Rory Huet 
Rowan Carmichael 
Roy Zellers 
Ruby Chen 
Ruslan Chen 
Ryan Nigmatullin 
Saachi Cheu 
Sam Jain 
Sam Altman 
Sam Schoenholz 
Samuel Toizer 
Sandhini Miserendino 
Agar </p>
<p>Peter West
Chandra Bhagavatula</p>
<p>Ronan Le Bras
et al. 2024. Faith</p>
<p>Whit-ney Meers
Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xide Xia, Yaelle Gold-schlagXi-aofang Wang, Xin-feng Xie, Xuchao Jia, Xuewei Wang, Yashesh</p>
<p>Daniel Kreymer
Diana Liskovich
Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Didem Foss, Dingkang Wang, Dustin Holland, Edward DowlingDuc Le</p>
<p>Eissa Jamil</p>
<p>Itai Gat
Irina-Elena Veliche
Jake Weissman</p>
<p>James Geboski
James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff TangJen</p>
<p>Jeff Harris
James Aung, James Betker, James Crooks, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jenia Var-avvaPachocki, James Lennon</p>
<p>Jessica Gan Lee
Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu</p>
<p>Joanne Jang</p>
<p>Joaquin Quinonero Candela
Joe Beutler</p>
<p>Joe Lan-ders
Joel Parish</p>
<p>Johannes Heidecke
John Schul-man
Jonathan Lachman
Jonathan McKay, Jonathan Uesato, Jonathan Ward</p>
<p>Jong Wook Kim, Joost Huizinga
Jos KraaijeveldSitkinJordan</p>
<p>Josh Gross</p>
<p>Josh Kaplan
Josh Snyder</p>
<p>Joshua Achiam
Joy Jiao</p>
<p>Joyce Lee
Juntang ZhuangJustyn Harriman, Kai</p>
<p>Natalie Summers</p>
<p>Natan LaFontaine
Neil Chowdhury
Nick Ryder, Nick StathasNick Turley, Nik Tezak, Niko Felix</p>
<p>Nithanth Kudige
Nitish Keskar
Noah Deutsch
Ofir Nachum, Ola Okelola, Oleg BoikoNoel Bundick, Nora Puckett</p>
<p>Oleg Murk
Oliver Jaffe, Olivia Watkins</p>
<p>The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction
Dec 22, 202131F6245DD7B496A1F8DDA1C3A446ABB6arXiv:2503.23084v1[cs.CL]
Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples.However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear.In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall.These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks.Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problemsolving capabilities during answer generation.Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated impressive capabilities in tackling complex reasoning tasks (Roziere et al., 2023;OpenAI, 2024;Guo et al., 2025).However, these models sometimes struggle with more straightforward reasoning problems, particularly when faced with questions that differ significantly from those encountered during training (Dziri et al., 2024;Hu et al., 2024;Xie et al., 2024).This generalization gap between LLMs and human reasoning has led to the hypothesis that these models are essentially "reasoning parrots" (Zečević et al., 2023)  memorization of text patterns found in their pretraining datasets (Carlini et al., 2022;Tang et al., 2023;Shi et al., 2024), rather than engaging in a rigorous, procedural reasoning process to solve problems (Wei et al., 2022;Kojima et al., 2022;Yao et al., 2023).Understanding the interplay between reasoning and memorization in LLMs is essential, not only for advancing our understanding of these models but also for developing more reliable, language-based reasoning systems in the future (Lanham et al., 2023;Oren et al., 2023;Turpin et al., 2024).</p>
<p>In the context of LLM reasoning, researchers often conceptualize memorization as the inability to generalize from familiar problems to their systematically modified counterparts.In this view, reasoning and memorization are two extremes on the spectrum of model generalizability.To investi-gate this, synthetic reasoning benchmarks are designed, and memorization is assessed by measuring changes in model performance across various setups (Dziri et al., 2024;Xie et al., 2024;Ye et al., 2024).Another line of research focuses on the internal mechanisms of LLMs, identifying specific components or circuits responsible for tasks like arithmetic (Hou et al., 2023;Stolfo et al., 2023a) and commonsense reasoning (Geva et al., 2023;Yang et al., 2024;Biran et al., 2024).However, these studies primarily analyze model outputs or hidden representations when dealing with carefully crafted synthetic reasoning problems, limiting the generalizability of their findings.</p>
<p>In this paper, we explore the reasoningmemorization dynamic of LLMs from a mechanistic perspective.Recent interpretability research has demonstrated that LLMs encode interpretable semantic features (Elhage et al., 2022;Park et al., 2024)-such as safety (Arditi et al., 2024;Yu et al., 2024), truth (Marks and Tegmark, 2023;Li et al., 2024), sentiment (Tigges et al., 2023), and language (Bricken et al., 2023)-as linear directions within their activation space.We hypothesize that there is a similar linear feature, which, when activated, enables the model to solve reasoning tasks through systematic generalization.When this feature is not activated, the model remains in a "memorization mode," exhibiting low generalizability when addressing variations of familiar reasoning problems.</p>
<p>To examine our hypotheses, we apply methods from linear semantic feature analysis (Burns et al., 2023;Rimsky et al., 2024) and identify a set of Linear Reasoning Features (LiReFs) in the residual streams of LLMs.As shown in Figure 1, LiReFs can be extracted by contrasting the hidden representations of reasoning-intensive versus memoryintensive questions.This contrast allows the two types of questions to be linearly separated in the model's activation space.Furthermore, we demonstrate via causal analysis (Stickland et al., 2024;Hong et al., 2024) that by enhancing the LiReFs during inference, we can shift the model into a "thinking mode" with strong generalizability in applying reasoning rules or patterns.We show via extensive experiments on four different LLMs across six datasets that the same set of reasoning features explain and mediate model reasoning ability across various knowledge domains and languages, suggesting a general control mechanism of switching between reasoning and memorization during model inference.</p>
<p>The main contributions of our work can be summarized as follows:</p>
<p>• We show that LLM reasoning capability is mediated by a set of linear features (LiReFs) in its activation space.Such features govern model generalizability in solving various reasoning tasks including math, logical, and scientific questions (Section 3).</p>
<p>• We casually validate the functionality of our discovered reasoning features by showing that LLM reasoning generalizability can be enhanced by intervening LiReFs at inference time (Section 4.1).</p>
<p>• We show via case analyses that mediating LiReFs during inference time reduces LLM reasoning errors and misapplication of model reasoning or memorization ability.(Section 4.2).</p>
<p>Related work</p>
<p>Memorization in LLMs Memorization in LLMs has been defined in various ways.In the context of privacy and copyright, memorization is often described as the model's verbatim reproduction of training data during generation (Carlini et al., 2022;Biderman et al., 2023;Huang et al., 2024).Alternatively, some define memorization as the counterfactual effect of omitting specific training data on model predictions (Zhang et al., 2023;Hu et al., 2024), reflecting memorization of rare, specific examples.In reasoning tasks, memorization is often seen as poor generalizability to questions outside the training data, as evidenced by studies on work sequence reversal (McCoy et al., 2023) and alphabet shifting (Prabhakar et al., 2024), which show degraded performance on infrequent patterns.Other studies observe performance degradation from controlled perturbations of input questions (Wu et al., 2024;Xie et al., 2024).In this paper, we adopt memorization as poor reasoning generalizability and propose a novel mechanistic interpretation of the reasoning-memorization dynamic during model inference.</p>
<p>Understanding LLM reasoning Prior research has sought to distinguish reasoning from memorization, investigating whether LLMs genuinely infer new conclusions or merely reconstruct patterns from pretraining data.Studies suggest that LLMs undergo structured multi-step reasoning processes, transitioning through distinct reasoning stages that follow an ordered sequence of knowledge retrieval and rule-based processing (Hou et al., 2023).Similarly, extended training beyond overfitting (grokking) has been shown to lead to the emergence of reasoning circuits, indicating that reasoning is a learned and structured capability (Power et al., 2022;Liu et al., 2022;Nanda et al., 2023;Wang et al., 2024a).Further studies on mathematical reasoning confirm that LLMs compute necessary information rather than memorizing templates, with reasoning computations leaving identifiable traces in model activations, particularly in the residual stream (Ye et al., 2024;Stolfo et al., 2023b).Additionally, attention heads have been shown to play a key role in both knowledge recall and latent reasoning, suggesting that these processes are distinct yet interconnected (Zheng et al., 2024).</p>
<p>Linear semantic features Recent advances in model interpretability have revealed that language models encode various semantic concepts as linear directions in their activation space (Park et al., 2024).These linear semantic features have been discovered by contrasting inputs that differ primarily in the targe semantic dimension (Marks and Tegmark, 2023).Once identified, these linear features can be manipulated to control model behavior, enabling targeted interventions during the generation process (Rimsky et al., 2024;Stickland et al., 2024).Our work extends this line of study by identifying linear features that mediate the model's ability to switch between genuine reasoning and memory recall.</p>
<p>3 Linear reasoning features (LiReFs)</p>
<p>Background</p>
<p>Transformers A decoder-only transformer language model (Vaswani et al., 2017) M maps an input sequence of tokens x = [x 1 , ..., x T ] into a probability distribution over the vocabulary for next-token prediction.Within the transformer, the i-th token x i is represented as a series of hidden states h (l) (x i ).Within each layer l ∈ [L], two modules compute updates that are added to the layer input h (l−1) (x i ):</p>
<p>(1) a multi-head self-attention module outputs a (l) (x i ), and a multi-layer perceptron (MLP) outputs m (l) (x i ).Putting together, the hidden representation h (l) (x i ) is computed as * :
h (l) (x i ) = h (l−1) (x i ) + a (l) (x i ) + m (l) (x i ) (1)
Following Elhage et al. (2021), we call each h (l) (x i ) the residual stream activation of x i at layer l.We focus on the residual stream of the last token x T of the user turn, as the point when the model is going to generate the first answer token, denoted as
H(x) = {h (l) (x T )} L l=1 .
Reasoning feature extraction We follow the linear feature hypothesis and postulate that the reasoning capability of LLMs is mediated by a single direction in the residual stream, and that by steering this direction, it is possible to control model interplay between reasoning and memorization.We compute the linear reasoning features (LiReFs) using the difference-in-means technique, which effectively disentangles key feature information as demonstrated by previous work (Marks and Tegmark, 2023;Rimsky et al., 2024).Specifically, given a collection of reasoning-intensive questions x ∈ D Reasoning (e.g."What is the answer of (5 + 2) * 3?") and another set of memoryintensive questions x ∈ D Memory (e.g."What is the capital city of the USA?"), we calculate the difference between the model's mean last-token residual stream activations when running on two categories of input questions:
r (l) = x∈D Reasoning h (l) (x) |D Reasoning | − x∈D Memory h (l) (x) |D Memory | (2)
The specific construction details of D Memory and D Reasoning are provided in Section 3.2.</p>
<p>Reasoning feature intervention Given a difference-in-means vector r (l) extracted from layer l, we can modulate the strength of the corresponding reasoning feature via simple linear interventions.Specifically, we can perform reasoning feature addition by adding the difference-in-means vector to the activations of an input question to shift it closer to the mean activation of typical reasoning-intensive questions, thereby unlocking model reasoning capability:
h ′ (l) (x) ← h (l) (x) + α * r (l)(3)
Similarly, one can perform reasoning feature ablation by erasing the component along r(l) for every residual stream activation h (l) (x):
h ′ (l) (x) ← h (l) (x) − rr T h (l) (x)(4)
where r = r (l) /||r (l) || is a unit vector encoding the reasoning feature direction, and h (l) (x) − rr T h (l) (x) is projection that zeroes out the value along the reasoning direction.</p>
<p>Datasets and Models</p>
<p>Datasets We curate our dataset for LiReF extraction and analysis using the following existing question answering benchmarks: 1) MMLU-Pro (Wang et al., 2024b), which is a comprehensive QA benchmark covering a wide range of subjects, including STEM, humanities and social sciences fields; 2) the GSM-8K math reasoning dataset (Cobbe et al., 2021) and its multilingual counterpart MGSM (Shi et al., 2022); 3) the PopQA factual knowledge QA dataset (Mallen et al., 2023), and 4) the humanity sections of the C-Eval Chinese benchmark (Huang et al., 2023).A detailed description of each dataset can be found in §B.</p>
<p>To categorize QA questions into the contrastive reasoning-intensive and memory-intensive subsets, we employ LLM-as-a-judge (Zheng et al., 2023) by asking GPT-4o (OpenAI et al., 2024) to assign a score between 0 and 1 to each question in MMLU-Pro, where a score closer to 1 indicates a reasoning-intensive question, and a score closer to 0 suggests a memory-intensive one.A score around 0.5 indicates that both reasoning and memory recall may be involved † .Next, we classified questions with scores above 0.5 as MMLU-Pro-R (Reasoning Part) and placed them in D Reasoning , while questions with scores less than or equal to 0.5 were classified as MMLU-Pro-M (Memory Part) and placed in D Memory .For the other benchmarks, we assign GSM8K and MGSM into D Reasoning , and put PopQA and C-Eval Chinese into D Memory .</p>
<p>Models</p>
<p>We study LiReF by analyzing a diverse collection of representative and influential base models, as long as their instruction-tuned variants: LLaMA3-8B (base, instruct) (Grattafiori et al., 2024), Gemma2-9B (base, instruct) (Team et al., 2024), Mistrial-7B-v0.3(base, instruct) (Jiang et al., 2023), and OLMo2-7B (base, instruct) (OLMo et al., 2025).</p>
<p>Analysis results</p>
<p>Figure 2 shows the 2-dimensional Principal Component Analysis (PCA) visualization of the last tokens representations across different model layers and six datasets in D Memory and D Reasoning , where hidden representations are taken from a specific middle layer of each model.‡ Additional PCA results for other layers of the models are provided in Appendix C. We observe that the representations of questions in D Memory and D Reasoning can be linearly separated by the reasoning features, which are computed as the difference vector between centroids of the two representation categories (the blue arrows).</p>
<p>Robustness of LiReF extraction</p>
<p>We also validate that our extracted LiReFs indeed capture model reasoning capability, as opposed to some superficial lexical patterns that distinguish two question categories.As suggested by Figure 2, for each model, the same LiReF separates every contrastive pair of problem subsets in D Reasoning and D Memory , regardless of the task format (e.g., multiple choice and the open-ended generation), domain (e.g., physics, chemistry and math), or language (e.g., English and Chinese).Moreover, we provide in Appendix C more fine-grained PCA visualizations of questions from various subject domains in MMLU-Pro, suggesting that even for questions from disparate disciplines (e.g., physics vs. history), as long as both of their solutions require strong reasoning capability, their hidden representations shall fall into the same reasoning subspace as determined by the LiReF.</p>
<p>To quantitatively measure the relation between LiReF and the reasoning capability required for answering each question, we compute the layerwise cosine similarity between the last question token representation of each question and the corresponding LiReF, as shown in Figure 3.For each LLM, we also replicate the same analyses for its pre-trained base version before instruction fine-tuning.A positive cosine similarity suggests a positive activation value along LiReF and vice versa.We observe that for all eight models, questions in D Reasoning mostly activate the reasoning features positively, while questions in D Memory mostly have negative † The prompt used is provided in §A.</p>
<p>‡ Figure 10 in the Appendix C shows that the top one principal component already captures most of the mean difference (see Equation 2) between the activations in D Memory and D Reasoning .For each model, we plot six groups of points across several datasets.We observe that: (1) For all four models, questions defined as Reasoning-required and those defined as Memory-required can be naturally distinguished into two distinct groups, as shown by the boundary (grey dashed line) fitted via logistic regression, with the blue arrows showing the approximate direction of the Linear Reasoning Features.</p>
<p>(2) In the extracted dimensions, the influence of task domain and language within the same category on the distribution is not significant, and data requiring the same capability naturally cluster together in the same region.LiReF activations, especially in the middle layers.Furthermore, on 3 out of 4 LLM families (LLaMA3-8B, Gemma2-9B, and Mistral-7B-v0.3), the layerwise cosine similarity profiles between the base and instruction-tuned models are highly consistent with each other, suggesting that LLMs may have developed linear reasoning features to mediate its emergent reasoning capability during pre-training rather than post-training.</p>
<p>The gradient nature of reasoning-memorization interplay</p>
<p>As observed in Figure 2, questions in D Memory and D Reasoning tend to have significantly negative and positive activations along LiReFs, respectively.This raises the question: what types of questions fall near the reasoning-memorization boundary (i.e., those with near-zero LiReF activation values)?Do these problems require both memory and reasoning abilities to solve?We investigate this question through the following experiments.</p>
<p>Figure 4 shows the relation between GPT-4oassigned reasoning scores for each question in MMLU-Pro, as discussed in Section 3.2, versus the LiReF projection value rT h (l) (x) of its resid-0 .00 0 .20 0 .30 0 .3 5 0 .40 0 .60 0 .7 0 0 .80 0 .8 5 0 .90 0 .9 5 1 .00</p>
<p>Reasoning Score Categories provided by GPT-4o ual stream representation h (l) (x) by LLaMA3-8Bbase and Mistral-7B-v0.3 models.We observe that as problems receive higher reasoning scores assigned by GPT-4o, they tend to have larger activation values along the LiReF direction.This correlation is notably strong across both models, with Spearman correlation coefficients of 0.840 for LLaMA3-8B-base and 0.752 for Mistral-7B-v0.3- base.These findings suggest that problems with near-zero LiReF activations likely involve both memory and reasoning capabilities.</p>
<p>To further validate our results, we conducted additional PCA experiments on the Coding taskswhich have been identified by numerous studies as a representative task type requiring both memory and reasoning capabilities in LLMs (Zhao et al., 2025;Chen et al., 2024).The results are shown in Figure 5, where we observe that the residual stream activations of two Coding tasks, MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021), are both positioned near the boundary.This further supports our finding that data points situated between the two extremes represent task types that engage both memory and reasoning abilities in LLMs.</p>
<p>Causal validation of LiReFs</p>
<p>Inference-time LiReF intervention</p>
<p>In this section, we conduct experiments where we manually intervene in the residual stream activations during inference time.By adjusting the intensity of linear reasoning features in model residual streams, we examine how model performance on both memory-intensive and reasoning-intensive tasks will change.</p>
<p>In particular, for all tokens of each question, we modify their residual stream representations in a specific layer by adding an intervention vector along the LiReF direction, as suggested in Equation 3. To enhance the most relevant model capability, we adopt negative values of α for D Memory , and positive α values for D Reaoning .After carefully tuning α on validation sets, we ask each model to generate answers for questions in D Memory and D Reaoning , and measure its performance change under inference-time LiReF intervention.More details about the experimental setup, including the validation-test set splits, hyperparameter selection criteria and inference settings can be found in Appendix D.</p>
<p>The main results are shown in Table 1.We observe that intervening LiReFs during inference time effectively improves the performance of four LLMs on both memory-intensive and reasoning-intensive tasks.Moreover, the improvements remain consistent across different task types, domains, and languages, further supporting our claim that the reasoning features in LLM residual streams capture general reasoning capability.In the next section, we will present specific cases to illustrate how reasoning feature intervention improves model performance by reducing reasoning step errors and correcting the misapplication of model abilities.</p>
<p>Cases Study</p>
<p>In the PCA analyses presented in Section 3.3, we observed certain sample cases that, although labeled as reasoning-intensive by GPT-4o or by the task name, have negative LiReF activations on the memorization subspace.Similarly, some cases that were labeled as memory-intensive instead fall into the reasoning subspace with positive-valued LiReFs.In this section, we analyze these cases and also conduct LiReF intervention experiments, aiming to correct any potential reasoning errors or unfaithful reasoning steps.</p>
<p>Firstly, we collect questions in MMLU-Pro whose reasoning label contradicts the actual feature subspace in which they are positioned.(e.g., cases whose GPT-4o-assigned reasoning score is much less than 0.5, but have a positive-valued LiReF activation), and evaluate LLaMA3-8B-base on them to identify a subset of questions where the model provides incorrect answers.Then we obtained a subset of 184 cases in total.Next, we perform inference-time LiReF intervention on these examples following the same settings in Section 4.1, and compare their accuracy and actual outputs before and after the intervention.We found that, by shifting LiReF activation to have the sign that is consistent with GPT-4o-assigned reasoning score, model accuracy on this subset jumps from 0 to 0.21.Table 1: The performance of four base models on six benchmarks, before and after feature intervention.The results indicate that by shifting the residual stream of the reasoning-required or memory-required tasks further to the specific feature regions, overall task performance can be substantially enhanced.</p>
<p>LLM reasoning errors might not be due to a lack of relevant knowledge, but are caused by the insufficient activation of its acquired generalizable thinking capabilities, which can be alleviated through targeted inference-time intervention of reasoning features.</p>
<p>Reasoning Generalization Effects</p>
<p>In the previous experiments, we noticed that the features of certain questions from reasoning datasets lie in the memory subspace with negative LiReF activations.Therefore, we suspect that the models might have solved these reasoning questions through memorization (possibly due to training data contamination), rather than applying genuine reasoning capability that is generalizable under systematic input variation.To verify this hypothesis, we conduct additional features intervention experiments on GSM-Symbolic (Mirzadeh et al., 2025) in this section.GSM-Symbolic is a variant of GSM-8k.It selects 100 question templates from GSM-8k and then generates 50 different instances for each template by varying numerical conditions, results, and other factors.The resulting dataset contains 5,000 data points, making it ideal for a reliable evaluation of the model's reasoning generalization capabilities.</p>
<p>Figure 6 shows mean model accuracy on GSM-Symbolic, GSM-8k, and MMLU-Pro-M under inference-time LiReF intervention.We can see that as the intervention intensity α increases from 0, the performance of all four models on both GSM-8k and GSM-Symbolic rises consistently.On the other hand, as α decreases from 0, we observe that, compared to GSM-8k, GSM-Symbolic experiences a more significant performance drop with suppressed LiReFs.Notably, the performance gain and loss on GSM-Symbolic suggests that LiReF intervention is likely enhancing the genuine model reasoning capability that is generalizable, as opposed to case-based reasoning skills that rely more on mem-orization of particular training examples.Interestingly, we also observe that the performance drop on GSM-8K under LiReF suppression is less pronounced compared to GSM-Symbolic, and there is even a slight improvement with a moderate suppression when setting α = −0.05.This implies that the model might have previously been exposed to GSM-8K due to data leakage, and therefore adopts a memory-intensive strategy to answer these questions.While for MMLU-Pro-M, performance improves steadily as α decreases, supporting our observation that reducing the projection values of the model's residual stream in the LiReFs direction enhances the model's ability to recall from memory.</p>
<p>Conclusion</p>
<p>In this study, we show that the reasoning and memorization abilities of LLMs across various domains and languages are driven by a shared set of linear reasoning features within their activation space.These features can be uncovered by contrasting the hidden representations of reasoning-intensive and memory-intensive queries.Furthermore, we demonstrate that intervening on these Linear Reasoning Features (LiReFs) during model inference improves both accuracy and robustness to perturbations when handling reasoning tasks.We also reveal that manipulating these reasoning features enables the model to more precisely activate the most relevant problem-solving capabilities during answer generation.Our findings offer a mechanistic understanding of how reasoning and memorization interact in LLMs, providing a foundation for future research aimed at building more capable and interpretable generative reasoning systems.Additionally, our work suggests that enhancing the interpretability of these underlying features could lead to more efficient and focused interventions, contributing to the development of models that are both more powerful and more transparent in their decision-making processes.With what speed will a sound wave with frequency 4f travel through the air?</p>
<p>4f is four times the frequency of f, so the wavelength of 4f is one-fourth the wavelength of f.Since the speed of sound is the product of frequency and wavelength, the speed of 4f is one-fourth the speed of f.The answer is v/4.</p>
<p>The speed of sound in air is independent of the frequency of the sound wave.It only depends on factors like the medium and the temperature of the air.So, if a sound wave with frequency f travels through air at speed v, a sound wave with frequency 4f will also travel through the air at the same speed, v.</p>
<p>The answer is v.</p>
<p>Overthinking</p>
<p>Table 2: Example questions and outputs before and after features intervention on the LLaMA3-8B-base model.They are both Memory-Intensive questions, but the features were incorrectly positioned in the Reasoning region.</p>
<p>After shifting the Features to the appropriate region, the error was corrected.Figure 6: Performance of the four base models on the GSM-8k, GSM-Symbolic, and MMLU-Pro-M datasets, with varying hyperparameter α to control the intensity of feature intervention, shows that all four models exhibit potential data leakage risks on the GSM-8k dataset.The models may rely on memory to achieve good performance on this reasoning task.</p>
<p>Limitations</p>
<p>Our work has several limitations.First, we only studied reasoning features in relatively small LLMs, while recent studies show that by scaling up both model size and inference-time computation, the reasoning capability of LLMs can be significantly improved (Hoffmann et al., 2022;OpenAI, 2024).Second, we have focused mostly on reasoning problems that can be addressed through short answers, while it remains unclear whether LiReFs can be utilized to enhance model's ability of performing deliberate reasoning via various prompt engineering techniques such as chain-of-thought (Wei et al., 2022), self-reflection (Shinn et al., 2024), and treeof-thought (Yao et al., 2024).Third, we formulate memorization as performance inconsistency against reasoning question perturbation, while another line of LLM reasoning research has employed a different definition of counterfactual memorization -i.e., change of model answers on particular test questions after removing a similar example from training data (Zhang et al., 2023;Hu et al., 2024).Future work should investigate Whether perturbational and counterfactual memorization are mechanistically equivalent and, therefore, can be both mediated by LiReFs.</p>
<p>A Prompts</p>
<p>Table 3 presents the prompt we used to query GPT-4o to assign a Reasoning Score to each question.</p>
<p>B Details of Datasets</p>
<p>Here, we provide further details about the datasets used in Sections 3 and 4.</p>
<p>MMLU-Pro-M (Wang et al., 2024b) and MMLU-Pro-R MMLU-Pro is a comprehensive benchmark designed to assess the advanced language understanding and reasoning capabilities of large language models (LLMs).It spans 14 diverse domains such as mathematics, physics, chemistry, law, engineering, psychology, and health, encompassing over 12,000 questions.It features 10 options per question, significantly increasing the difficulty and robustness of the benchmark.Unlike MMLU, MMLU-Pro focuses on more challenging college-level problems that require deliberate reasoning across various domains.In this work, we use GPT-4o to assign a Reasoning Score to each question.We then divide the questions into two subsets: those with a score greater than 0.5 are categorized as MMLU-Pro-R, while those with a score of 0.5 or below are classified as MMLU-Pro-M.</p>
<p>PopQA (Mallen et al., 2023) PopQA focuses on evaluating factual knowledge in large language models, specifically targeting knowledge about entities, defined as triplets of (subject, relationship, object).The task is framed as open-domain question answering, where a model is asked to predict an answer without pre-given ground-truth paragraphs.This study explores few-shot learning and prompts LMs without parameter updates, in contrast to fine-tuning approaches.The performance is measured by accuracy, where a prediction is considered correct if any substring matches a gold answer.</p>
<p>C-Eval-H (Huang et al., 2023) C-EVAL is a comprehensive Chinese evaluation suite designed to assess the advanced knowledge and reasoning abilities of large language models (LLMs) in a Chinese context.As traditional NLP benchmarks primarily focus on English and fail to capture the unique challenges of Chinese language models, C-EVAL addresses this gap by providing a detailed evaluation framework tailored to the Chinese language and culture.It includes 13,948 multiplechoice questions across 52 diverse disciplines, ranging from humanities to science and engineering, and spans four difficulty levels: middle school, high school, college, and professional exams.In this work, we focus on the humanities portion and refer to it as C-Eval-H.</p>
<p>GSM8k (Cobbe et al., 2021)</p>
<p>GSM8k is a dataset designed to evaluate the mathematical reasoning abilities of large language models (LLMs).It consists of 8.5K grade school-level math problems paired with natural language solutions.The dataset aims to address the challenges faced by LLMs in performing multi-step mathematical reasoning, which often reveals a critical weakness in these models.</p>
<p>MGSM (Shi et al., 2022) The MGSM (Multilingual Grade School Math) benchmark is introduced to assess multilingual reasoning abilities in large language models, addressing the gap between English-based chain-of-thought (COT) reasoning and multilingual NLP tasks.Building on the GSM8K dataset, MGSM extends it to ten typologically diverse languages through manual translations. .</p>
<p>C Additional Experiments</p>
<p>C.1 Detailed Plot of the PCA results</p>
<p>In this section, we present additional PCA results from various layers of the LLaMA3-8B-base and Gemma2-9B-base models discussed in Section 3.2, which is shown in Figure 7 and Figure 8.We also provide fine-grained PCA visualizations of questions from different subject domains in MMLU-Pro in Figure 9. Additionally, we include heatmaps in Figure 10 demonstrating that the first principal component from our PCA experiments captures the majority of the mean activation differences between D Memory and D Reasoning .</p>
<p>D Details of the Intervention Experiments</p>
<p>Here, we provide more implementation details in the Features Intervention Experiments described in Section 4.</p>
<p>Inference Settings For the few-shot settings, we adhere to the original experimental setup across all datasets.Specifically, we use 5-shot for MMLU-Pro-M, MMLU-Pro-R, and C-Eval-H, and 8-shot for GSM8k, MGSM, and GSM-Symbolic.Additionally, we run 0-shot for PopQA, following the original configuration.</p>
<p>For both open-ended generation and multichoices question answering tasks, we allow the model to generate the next 200 tokens.</p>
<p>Validation-Test Set Split For parameter tuning and inference, we directly utilized the pre-existing validation and test sets that were already partitioned within each dataset.</p>
<p>Hyperparameters Selection Based on the validation and test sets we have split, we tune the hyperparameter, α, on the validation set.We adjust it in intervals of 0.05 in absolute value and select the value of α that performs best on the validation set to apply to the test set.</p>
<p>All the experiments in this work were conducted on four 80GB NVIDIA A800 GPUs.</p>
<p>Figure 1 :
1
Figure 1: Main findings of our study: (a) There exists a set of linear features (LiReFs) in the LLM residual stream that drives the model to switch between reasoning and memorization modes with different levels of generalizability.(b) LiReFs generally explain model reasoning capability across various knowledge domains and languages.(c) Model activation values along LiReFs correlate strongly with model generalizability on reasoning tasks.(d) Intervening LiReFs during inference time can further improve the model reasoning performance and generalizability.</p>
<p>Figure 2 :
2
Figure2: Visualization of the hidden states of four base models using 2-dimensional PCA.For each model, we plot six groups of points across several datasets.We observe that: (1) For all four models, questions defined as Reasoning-required and those defined as Memory-required can be naturally distinguished into two distinct groups, as shown by the boundary (grey dashed line) fitted via logistic regression, with the blue arrows showing the approximate direction of the Linear Reasoning Features.(2) In the extracted dimensions, the influence of task domain and language within the same category on the distribution is not significant, and data requiring the same capability naturally cluster together in the same region.</p>
<p>Figure 3 :
3
Figure3: Layerwise cosine similarity between the last token residual stream activations and the extracted Linear Reasoning Features (LiReFs) in four base models and their corresponding instruction-tuned variants.</p>
<p>Figure 4 :
4
Figure 4: Strong correlation between Projection Values on the Linear Reasoning Features (LiReFs) direction and the Reasoning Score provided by GPT-4o, with Spearman coefficients of 0.840 (LLaMA3-8B-base) and 0.752 (Mistral-7B-v0.3-base).The LiReFs projections exhibit a spectrum-like distribution, where continuous increases in Reasoning Scores correspond to progressively rising Projection Values along the LiReFs direction.</p>
<p>Figure 5 :
5
Figure 5: Visualization of the hidden states of two base models on the datasets of MBPP, HumanEval, MMLU-Pro-M and MMLU-Pro-R using 2-dimensional PCA.The hidden states of coding tasks, which involve both reasoning and memory recall, are positioned around the boundary (grey dashed line) fitted via logistic regression.</p>
<p>Figure 7 :
7
Figure 7: The PCA experiments results on the first 15 layers on LLaMA3-8B-base models</p>
<p>Figure 8 :
8
Figure 8: The PCA experiments results on the first 15 layers on Gemma2-9B-base models</p>
<p>Figure 9 :
9
Figure 9: Fine-grained PCA visualizations of questions from different subject domains in MMLU-Pro on the model of LLaMA3-8B-base and Gemma2-9B-base.</p>
<p>Figure 10 :
10
Figure 10: The top one principal component in PCA experiments captures most of the mean difference (Equation 2 between the activations in D Memory and D Reasoning .</p>
<p>, relying heavily on
Memory-intensive Tasks:Reasoning Tasks:e.g. 1. Polarization is a property of <strong><em>e.g. 1. Order from greatest to least: 3, 3 and 1 over 8, 3.8, 3.18.2. Colors in a soap bubble result from light </em></strong>2. What is the strength (B1) of a 13C 90°pulse of3. 中国资本主义萌芽出现的LLMduration 1 μs?主要标志是____3. Andy pflanzt 90 Geranienund 40 Petunien wenigerals Geranien. Wie vieleInterventionExtracting from LLMBlumen pflanzt er(b)residual streaminsgesamt?MemoryReasoning(a) Linear Reasoning Features (LiReFs)(d)</p>
<p>Table 2 presents some exemplar questions in our analyses, together with model answers before and after LiReF intervention.These results suggest that ↑7.2 33.4 / 35.6 ↑2.2 45.2 / 47.4 ↑2.2 24.2 / 33.5 ↑9.3 49.0 / 53.1 ↑4.1 28.5 / 34.6 ↑6.1 Gemma2-9B-base 37.5 / 50.1 ↑12.6 29.2 / 30.3 ↑1.1 52.1 / 52.1 29.2 / 44.7 ↑15.5 61.9 / 63.5 ↑1.6 45.8 / 47.0 ↑1.2 Mistral-7B-v0.3-base 37.8 / 43.6 ↑5.8 30.1 / 30.9 ↑0.8 38.2 / 44.0 ↑5.8 20.8 / 21.7 ↑0.9 35.1 / 36.2 ↑1.1
Memory-Intensive DatasetsReasoning DatasetsBase ModelMMLU-Pro-MPopQAC-Eval-HMMLU-Pro-RGSM-8kMGSMLLaMA3-8B-base41.1 / 48.3 12.0 / 12.0OLMo2-7B-base19.4 / 25.0 ↑5.6 19.2 / 20.1 ↑0.9 26.0 / 28.9 ↑2.9 11.3 / 16.5 ↑5.2 11.5 / 12.3 ↑0.8 10.1 / 11.3 ↑1.2
AcknowledgmentThis material is based in part upon work supported by Schmidt Sciences; by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 -Project number 390727645.Towards monosemanticity: Decomposing language models with dictionary learning.Transformer Circuits Thread.Https://transformercircuits.pub/2023/monosemanticfeatures/index.html.Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.2023.Discovering latent knowledge in language models without supervision.In The Eleventh International Conference on Learning Representations.Dissecting recall of factual associations in auto-regressive language models.In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216-12235, Singapore.Association for Computational Linguistics.Question: "An owner of an apartment building in a rundown section of town knew...If the neighbor asserts a claim against the owner to recover damages for his injury, he should" Options:[not recover, because the owner can't be held responsible...] Analysis:Humanities-oriented question, which, although requiring multi-step reasoning, still leans more towards a memorization-based approach.Score: 0.35 3. Score 0.95:Question: "Order from greatest to least: 3, 3 and 1 over 8, 3.8, 3.18."Options: ['3.8, 3 and 1 over 8, 3.18, 3',...] Analysis: Requires comparing numerical values and determining their order.Score: 0.95Current Analysis:Question: "{question_text}" Options: {options_list} Analysis:Table3: Prompt used to query GPT-4o to assign a Reasoning Score to each question.
Refusal in language models is mediated by a single direction. Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, Neel Nanda, arXiv:2406.117172024arXiv preprint</p>
<p>. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Say, Shaun Lindsay, Sheng Feng, Shenghao Lin, Cindy Shengxin, Shishir Zha, Shiva Patil, Shuqiang Shankar, Shuqiang Zhang, Sinong Zhang, Sneha Wang, Soji Agarwal, Soumith Sajuyigbe, Stephanie Chintala, Stephen Max, Steve Chen, Steve Kehoe, Sudarshan Satterfield, Sumit Govindaprasad, Summer Gupta, Sungmin Deng, Sunny Cho, Suraj Virk, Sy Subramanian, Sydney Choudhury, Tal Goldman, Tamar Remez, Tamara Glaser, Thilo Best, Thomas Koehler, Tianhe Robinson, Tianjun Li, Tim Zhang, Timothy Matthews, Tzook Chou, Varun Shaked, Victoria Vontimitta, Victoria Ajayi, Vijai Montanez, Mohan, Satish Vinay, Vishal Kumar, Vlad Mangla, Vlad Ionescu, Poenaru, Tiberiu Vlad, Vladimir Mihailescu, Wei Ivanov, Wenchen Li, Wenwen Wang, Wes Jiang, Will Bouaziz, Xiaocheng Constable, Xiaojian Tang, Xiaolan Wu, Xilun Wang, Xinbo Wu, Yaniv Gao, Yanjun Kleinman, Ye Chen, Ye Hu, Ye Jia, Yenda Qi, Yilin Li, Ying Zhang, Yossi Zhang, Youngjin Adi, Nam, Wang Yu, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary Devito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, arXiv:2407.21783Preprintand Zhiyu Ma. 2024. The llama 3 herd of models</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Training computeoptimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022</p>
<p>Intrinsic evaluation of unlearning using parametric knowledge traces. Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, Mor Geva, arXiv:2406.116142024arXiv preprint</p>
<p>Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan, arXiv:2310.14491Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. 2023arXiv preprint</p>
<p>Case-based or rule-based: How do transformers do the math?. Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang, arXiv:2402.177092024arXiv preprint</p>
<p>Demystifying verbatim memorization in large language models. Jing Huang, Diyi Yang, Christopher Potts, arXiv:2407.178172024Preprint</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. </p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Jackson Kernion, et al. 2023. Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, arXiv:2307.13702arXiv preprint</p>
<p>Inferencetime intervention: Eliciting truthful answers from a language model. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Advances in Neural Information Processing Systems. 202436</p>
<p>Towards understanding grokking: An effective theory of representation learning. Ziming Liu, Ouail Kitouni, Eric Niklas S Nolte, Max Michaud, Mike Tegmark, Williams, Advances in Neural Information Processing Systems. 202235</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.546Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Samuel Marks, Max Tegmark, arXiv:2310.06824The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. 2023arXiv preprint</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. R Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L Griffiths, arXiv:2309.136382023Preprint</p>
<p>GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. Keivan Seyed Iman Mirzadeh, Hooman Alizadeh, Oncel Shahrokhi, Samy Tuzel, Mehrdad Bengio, Farajtabar, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>2 olmo 2 furious. Team Olmo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James, V Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A Smith, Hannaneh Hajishirzi ; Wal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Sonia Xia, Spencer Phene, Srinivas Papay, Tao Narayanan ; Stramer, Tarun Xu, Taya Gogineni, Ted Christianson, Tejal Sanders, Thomas Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Tianhao Shadwell, Todd Zheng, Todor Underwood, Toki Markov, Tom Sherbakov, Tom Rubin, Tomer Stasi, Tristan Kaftan, Troy Heywood, Tyce Peterson, Tyna Walters, Valerie Eloundou, Veit Qi, Vinnie Moeller, Vishal Monaco, Vlad Kuo, Wayne Fomenko, Weiyi Chang, Wenda Zheng, Wesam Zhou, Will Manassra, Wojciech Sheu, Yash Zaremba, Yilei Patil, Yongjik Qian, Youlong Kim, Yu Cheng, Yuchen Zhang, Yuchen He, Yujia Zhang, Yunxing Jin, Yury Dai, Malkov, arXiv:2501.00656arXiv:2410.21276Gpt-4o system card. Preprint. Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal2025. 2024Preprint</p>
<p>Learning to reason with llms. 2024OpenAI</p>
<p>Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B Hashimoto, arXiv:2310.17623Proving test set contamination in black box language models. 2023arXiv preprint</p>
<p>The linear representation hypothesis and the geometry of large language models. Kiho Park, Yo Joong Choe, Victor Veitch, arXiv:2311.036582024Preprint</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, arXiv:2201.02177Grokking: Generalization beyond overfitting on small algorithmic datasets. 2022arXiv preprint</p>
<p>Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning. Akshara Prabhakar, Thomas L Griffiths, R Thomas Mccoy, arXiv:2407.016872024Preprint</p>
<p>Steering llama 2 via contrastive activation addition. Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, Alexander Turner, 10.18653/v1/2024.acl-long.828Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Adi, Romain Liu, Tal Sauvestre, Remez, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, arXiv:2210.030572022Preprint</p>
<p>Detecting pretraining data from large language models. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Asa Cooper Stickland, Alexander Lyzhov, Jacob Pfau, Salsabila Mahdi, Samuel R Bowman, arXiv:2406.15518Steering without side effects: Improving postdeployment control of language models. 2024arXiv preprint</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023a</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 10.18653/v1/2023.emnlp-main.435Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>Large language models are in-context semantic reasoners rather than symbolic reasoners. Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang, arXiv:2305.148252023arXiv preprint</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, A Christopher, Danila Choquette-Choo, David Sinopalnikov, Dimple Weinberger, Dominika Vijaykumar, Dustin Rogozińska, Elisa Herbison, Emma Bandy, Eric Wang, Erica Noland, Evan Moreira, Evgenii Senter, Francesco Eltyshev, Visin, Gary Gabriel Rasskin, Glenn Wei, Gus Cameron, Hadi Martins, Hanna Hashemi, Harleen Klimczak-Plucińska, Harsh Batra, Ivan Dhand, Jacinda Nardini, Jack Mein, James Zhou, Jeff Svensson, Jetha Stanway, Jin Peng Chan, Joana Zhou, Joana Carrasqueira, Jocelyn Iljazi, Joe Becker, Joost Fernandez, Josh Van Amersfoort, Josh Gordon, Josh Lipschultz, Newlan, Kareem Ju Yeong Ji, Kartikeya Mohamed, Kat Badola, Katie Black, Keelin Millican, Kelvin Mcdonell, Kiranbir Nguyen, Kish Sodhia, Lars Lowe Greene, Lauren Sjoesund, Laurent Usui, Lena Sifre, Leticia Heuermann, Lilly Lago, Mcnealus, Baldini Livio, Logan Soares, Lucas Kilpatrick, Luciano Dixon, Machel Martins, Manvinder Reid, Mark Singh, Iverson ; Mat, Mateo Velloso, Matt Wirth, Matt Davidow, Matthew Miller, Matthew Rahtz, Meg Watson, Mehran Risdal, Michael Kazemi, Ming Moynihan, Minsuk Zhang, Minwoo Kahng, Mofi Park, Mohit Rahman, Natalie Khatwani, Nenshad Dao, Nesh Bardoliwalla, Neta Devanathan, Nilay Dumai, Oscar Chauhan, Pankil Wahltinez, Parker Botarda, Paul Barnes, Paul Barham, Pengchong Michel, Timothy Jin ; Susan Chan, Ting Jordan, Tom Yu, Tom Eccles, Tomas Hennigan, Tulsee Kocisky, Vihan Doshi, Vikas Jain, Vilobh Yadav, Vishal Meshram, Warren Dharmadhikari, Wei Barkley, Wenming Wei, Woohyun Ye, Woosuk Han, Xiang Kwon, Zhe Xu, Zhitao Shen, Zichuan Gong, Victor Wei, Phoebe Cotruta, Anand Kirk, Minh Rao, Ludovic Giang, Tris Peran, Warkentin, arXiv:2408.00118Joelle Barral, Zoubin Ghahramani. Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom,Martin Görner,Koray KavukcuogluClement FarabetPreprintPetko Georgiev. Demis Hassabis. Robert Dadashi, and Alek Andreev. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>Curt Tigges, John Oskar, Atticus Hollinsworth, Neel Geiger, Nanda, arXiv:2310.15154Linear representations of sentiment in large language models. 2023arXiv preprint</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-ofthought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Grokked transformers are implicit reasoners: A mechanistic journey to the edge of generalization. Boshi Wang, Xiang Yue, Yu Su, Huan Sun, arXiv:2405.150712024aPreprint</p>
<p>MMLU-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, arXiv:2307.024772024Preprint</p>
<p>On memorization of large language models in logical reasoning. Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, Ravi Kumar, arXiv:2410.231232024Preprint</p>
<p>Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, arXiv:2402.16837Do large language models latently perform multi-hop reasoning?. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Physics of language models: Part 2.1, grade-school math and the hidden reasoning process. Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu, arXiv:2407.203112024Preprint</p>
<p>Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda, arXiv:2409.20089Robust llm safeguarding via refusal feature adversarial training. 2024arXiv preprint</p>
<p>Causal parrots: Large language models may talk causality but are not causal. Matej Zečević, Moritz Willig, Devendra Singh Dhami, Kristian Kersting, arXiv:2308.130672023Preprint</p>
<p>Counterfactual memorization in neural language models. Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini, Advances in Neural Information Processing Systems. 202336</p>
<p>Unveiling the magic of code reasoning through hypothesis decomposition and amendment. Yuze Zhao, Tianyun Ji, Wenjun Feng, Zhenya Huang, Qi Liu, Zhiding Liu, Yixiao Ma, Kai Zhang, Enhong Chen, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Attention heads of large language models: A survey. Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, Zhiyu Li, arXiv:2409.037522024Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
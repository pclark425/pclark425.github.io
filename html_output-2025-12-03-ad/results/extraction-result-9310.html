<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9310 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9310</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9310</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-268384789</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.08950v1.pdf" target="_blank">Exploring Prompt Engineering Practices in the Enterprise</a></p>
                <p><strong>Paper Abstract:</strong> Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made. We discuss design implications and future directions based on these prompt engineering practices.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9310.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9310.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>context_grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context grounding (embedded documents, examples, input queries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including documents, examples, or input queries directly in the prompt as grounding/context; observed as the most-edited prompt component and a primary mechanism users employ to influence LLM outputs in enterprise tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various (Q&A, summarization, code/SQL generation, extraction, classification, reasoning, JSON generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where answers or outputs should be grounded in provided documents, examples, or specific input queries; observed across many enterprise use cases in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt includes explicit context: grounding documents, example input-output pairs, or input queries embedded directly in the prompt. Two common patterns observed: (1) appending conversation turns (simulated dialog) where model outputs are appended to the prompt history; (2) adding/removing explicit examples (instance-level few-shot-style examples) to influence output.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Users repeatedly edited context because examples and grounding data can have large effects on model output; practitioners iteratively swap contexts to evaluate robustness of instructions and to observe model behaviour. Appending generations as context (dialog simulation) naturally lengthens context and alters subsequent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Empirical observations from 57 sessions (1523 edits): context was the most edited component overall; 68% of multi-edits included at least one context edit; users frequently replaced/modified existing context to test instruction robustness; ~19% of prompts were Q&A use cases; observed practice of adding/removing examples individually because examples produce significant effects on generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9310.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>instruction_wording</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task instruction wording and instruction components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The textual instructions describing the task given to the model (what to produce, format, rules); users frequently reword, add detail, or simplify instructions during iterative prompt development.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various (tasks above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural-language task instructions portion of the prompt that specify desired output, rules, constraints, and sometimes persona/handling of unknowns.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Variations in instruction phrasing (rephrasing between question/statement/command, adding detail, simplifying, explicit format/inclusion directives, persona, handle-unknown directives, output-length control).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Small wording changes can meaningfully affect generations; users iteratively modify task wording to improve outputs. Instruction edits were common but less frequent than context edits, suggesting practitioners often rely on context changes before altering instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Most common instruction edit was 'task modification' (small rewrites like capitalization, punctuation, or rewording); adding instruction details was the next most common. Some instruction subcomponents (output-format, output-inclusion, persona, handle-unknown, output-length) were edited less often. High rollback rates were observed for some instruction subcomponents (e.g., 40% rollback for handle-unknown edits).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9310.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>labels_structure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt labels and structuring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of explicit labels, separators, or tags (e.g., 'Context:', '<Context>...</Context>', 'Output:') to delineate prompt components and influence model focus/behavior; commonly edited in sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Labels serve to identify sections of the prompt (instructions, context, examples, dialog turns, outputs) and can be treated as structured cues by models.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Structured prompt with explicit labels, separators, or start/end tags to mark sections; edits include changing labels, adding/removing tags, or altering label text near the output boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Users modify labels to try to get the model to focus on specific prompt regions or to make the model treat sections as distinct inputs/outputs; output label changes sometimes act like additional instructions and can influence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Label edits were the third most common prompt component edits; label edits often co-occurred with context or instruction edits and were used throughout prompts (instructions, context, examples, dialog, output).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9310.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>dialog_simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dialog simulation via appended model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulating conversational turns by appending model-generated outputs back into the prompt as context, facilitating multi-turn interactions and iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Q&A and conversational-style generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-turn Q&A or conversational tasks where the model's previous outputs become part of the prompt to produce subsequent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Conversation-style prompt where generated outputs are appended to the prompt history and used as context for subsequent generations (auto-appending UI behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Appending outputs to create dialog context changes subsequent generations and can be useful for Q&A use cases; but this practice also increases context length and may necessitate removing or pruning context for other edits, complicating iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Platform automatically appended generated output into input prompt text, encouraging accumulation of dialog turns; this behavior was common in Q&A (~19% of prompts) and affected editing patterns (many context additions and removals).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9310.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>few_shot_prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting (adding examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a few labeled examples inside the prompt to prime the model for the desired mapping from input to output; mentioned as a technical strategy and observed in user edits where examples were added/removed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>few-shot style tasks / general generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where practitioners include one or more explicit example input-output pairs in the prompt to demonstrate the desired output format or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Embedding a small number of exemplars (few-shot) in the prompt; in practice users added/removed examples to observe their influence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicit comparison by practitioners between variants with and without examples (observational, not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Examples can have significant effects on generation quality and format, prompting users to add/remove examples iteratively to compare impacts and improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Users added and removed examples individually during sessions; examples were treated as a form of context and were a frequent target of edits because of their outsized effect on outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9310.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>chain_of_thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step intermediate reasoning steps from models (referenced in related work as a strategy to improve reasoning), mentioned but not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>reasoning tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step reasoning where prompting for intermediate steps can improve final-answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-thought prompts that request or exemplify step-by-step reasoning or intermediate chain-of-thought traces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Cited as a technical strategy (in related work) that can improve reasoning performance in LLMs, but the authors note such strategies can be context-specific and lack guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Mentioned in related work (Wei et al., 2022). This paper did not run experiments comparing chain-of-thought formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9310.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>auto_prompt_generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated prompt generation strategies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithmic approaches to generate or optimize prompts automatically (referenced in related work), noted as methods that can improve performance but may be context specific.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automated systems that search or optimize prompt text to improve model outputs on target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Automatically generated or optimized prompt variants (methods cited: Wang et al., Melamed et al.), contrasted with human iterative editing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Automated prompt generation can improve performance in some contexts but results are context-specific and not guaranteed; the paper positions these as complementary to human practices.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Referenced in related work; not evaluated in the authors' enterprise dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9310.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>model_switching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model switching / comparing multiple LLMs within sessions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practitioners frequently change the target model during prompt development to compare capabilities; observed as a common parameter edit rather than a prompt presentation format per se but relevant to format comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Using different LLMs to execute the same prompt task to evaluate which model better meets task requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same prompt submitted to different target models (and often changing other inference parameters) within the same session to compare outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple models compared within sessions (average 3.6 models used per session).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Frequent model switching suggests users rely on cross-model comparison to find better behavior; availability/ease of switching promotes exploration rather than sticking to a single model, implying model identity interacts with prompt presentation to determine output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>93% of sessions involved one or more inference parameter changes; target language model was the most commonly changed parameter. Average number of models per session = 3.6 (SD=2.7), median = 3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9310.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>multi_edit_confounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple simultaneous edits (multi-edit) and confounding effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practitioners often make multiple edits at once (e.g., changing context and instructions together), which makes it difficult to attribute changes in output to a single edit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Editing behavior phenomenon: multiple prompt components changed in a single submission.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Multi-edits: submitting a prompt variant that differs in two or more components (context, instructions, labels, parameters) simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multi-edits increase iteration speed but reduce traceability of which change caused observed output differences, complicating debugging and evaluation of presentation-format effects.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>22% of all edits were multi-edits (mean number of simultaneous edits = 2.29, SD=0.59). 68% of multi-edits included at least one context edit; 45% included both context and instruction edits; ~20% of edits were accompanied by an inference parameter change.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9310.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9310.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>rollbacks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edit rollbacks (undo/redo) as evidence of uncertain effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Users frequently undo or redo prior edits, suggesting difficulty in knowing which prompt presentation changes have beneficial effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Behavioral evidence: users revert prior prompt edits during iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a presentation format but an observed behavior indicating uncertainty about edit impact; high rollback rates for some components.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>High rollback rates (11% of edits undone/redone; e.g., 40% rollback for instruction:handle-unknown edits) may indicate edits to some components produce unexpected or negligible changes, or users cannot remember past outcomes, showing difficulty attributing output changes to specific format tweaks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>11% of edits undid or redid a previous edit. Components with higher rollback percentages included handle-unknown (40%), instruction:output-length (25%), label edits (24%), instruction:persona (18%), compared to 8-9% for context and task/instruction:task edits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Prompt Engineering Practices in the Enterprise', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners. <em>(Rating: 2)</em></li>
                <li>Guiding large language models via directional stimulus prompting. <em>(Rating: 1)</em></li>
                <li>How to prompt? opportunities and challenges of zero-and few-shot learning for human-ai interaction in creative applications of generative models. <em>(Rating: 1)</em></li>
                <li>Ten simple rules for crafting effective prompts for large language models. <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting (Wei et al., 2022) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9310",
    "paper_id": "paper-268384789",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "context_grounding",
            "name_full": "Context grounding (embedded documents, examples, input queries)",
            "brief_description": "Including documents, examples, or input queries directly in the prompt as grounding/context; observed as the most-edited prompt component and a primary mechanism users employ to influence LLM outputs in enterprise tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "various (Q&A, summarization, code/SQL generation, extraction, classification, reasoning, JSON generation)",
            "task_description": "Tasks where answers or outputs should be grounded in provided documents, examples, or specific input queries; observed across many enterprise use cases in the dataset.",
            "presentation_format": "Prompt includes explicit context: grounding documents, example input-output pairs, or input queries embedded directly in the prompt. Two common patterns observed: (1) appending conversation turns (simulated dialog) where model outputs are appended to the prompt history; (2) adding/removing explicit examples (instance-level few-shot-style examples) to influence output.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Users repeatedly edited context because examples and grounding data can have large effects on model output; practitioners iteratively swap contexts to evaluate robustness of instructions and to observe model behaviour. Appending generations as context (dialog simulation) naturally lengthens context and alters subsequent outputs.",
            "null_or_negative_result": null,
            "experimental_details": "Empirical observations from 57 sessions (1523 edits): context was the most edited component overall; 68% of multi-edits included at least one context edit; users frequently replaced/modified existing context to test instruction robustness; ~19% of prompts were Q&A use cases; observed practice of adding/removing examples individually because examples produce significant effects on generation.",
            "uuid": "e9310.0",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "instruction_wording",
            "name_full": "Task instruction wording and instruction components",
            "brief_description": "The textual instructions describing the task given to the model (what to produce, format, rules); users frequently reword, add detail, or simplify instructions during iterative prompt development.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "various (tasks above)",
            "task_description": "Natural-language task instructions portion of the prompt that specify desired output, rules, constraints, and sometimes persona/handling of unknowns.",
            "presentation_format": "Variations in instruction phrasing (rephrasing between question/statement/command, adding detail, simplifying, explicit format/inclusion directives, persona, handle-unknown directives, output-length control).",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Small wording changes can meaningfully affect generations; users iteratively modify task wording to improve outputs. Instruction edits were common but less frequent than context edits, suggesting practitioners often rely on context changes before altering instructions.",
            "null_or_negative_result": null,
            "experimental_details": "Most common instruction edit was 'task modification' (small rewrites like capitalization, punctuation, or rewording); adding instruction details was the next most common. Some instruction subcomponents (output-format, output-inclusion, persona, handle-unknown, output-length) were edited less often. High rollback rates were observed for some instruction subcomponents (e.g., 40% rollback for handle-unknown edits).",
            "uuid": "e9310.1",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "labels_structure",
            "name_full": "Prompt labels and structuring",
            "brief_description": "Use of explicit labels, separators, or tags (e.g., 'Context:', '&lt;Context&gt;...&lt;/Context&gt;', 'Output:') to delineate prompt components and influence model focus/behavior; commonly edited in sessions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "various",
            "task_description": "Labels serve to identify sections of the prompt (instructions, context, examples, dialog turns, outputs) and can be treated as structured cues by models.",
            "presentation_format": "Structured prompt with explicit labels, separators, or start/end tags to mark sections; edits include changing labels, adding/removing tags, or altering label text near the output boundary.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Users modify labels to try to get the model to focus on specific prompt regions or to make the model treat sections as distinct inputs/outputs; output label changes sometimes act like additional instructions and can influence generation.",
            "null_or_negative_result": null,
            "experimental_details": "Label edits were the third most common prompt component edits; label edits often co-occurred with context or instruction edits and were used throughout prompts (instructions, context, examples, dialog, output).",
            "uuid": "e9310.2",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "dialog_simulation",
            "name_full": "Dialog simulation via appended model outputs",
            "brief_description": "Simulating conversational turns by appending model-generated outputs back into the prompt as context, facilitating multi-turn interactions and iteration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "Q&A and conversational-style generation",
            "task_description": "Multi-turn Q&A or conversational tasks where the model's previous outputs become part of the prompt to produce subsequent outputs.",
            "presentation_format": "Conversation-style prompt where generated outputs are appended to the prompt history and used as context for subsequent generations (auto-appending UI behavior).",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Appending outputs to create dialog context changes subsequent generations and can be useful for Q&A use cases; but this practice also increases context length and may necessitate removing or pruning context for other edits, complicating iteration.",
            "null_or_negative_result": null,
            "experimental_details": "Platform automatically appended generated output into input prompt text, encouraging accumulation of dialog turns; this behavior was common in Q&A (~19% of prompts) and affected editing patterns (many context additions and removals).",
            "uuid": "e9310.3",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "few_shot_prompting",
            "name_full": "Few-shot prompting (adding examples)",
            "brief_description": "Providing a few labeled examples inside the prompt to prime the model for the desired mapping from input to output; mentioned as a technical strategy and observed in user edits where examples were added/removed.",
            "citation_title": "Language models are few-shot learners.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "few-shot style tasks / general generation",
            "task_description": "Tasks where practitioners include one or more explicit example input-output pairs in the prompt to demonstrate the desired output format or reasoning.",
            "presentation_format": "Embedding a small number of exemplars (few-shot) in the prompt; in practice users added/removed examples to observe their influence.",
            "comparison_format": "Implicit comparison by practitioners between variants with and without examples (observational, not quantified in this paper).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Examples can have significant effects on generation quality and format, prompting users to add/remove examples iteratively to compare impacts and improve outputs.",
            "null_or_negative_result": null,
            "experimental_details": "Users added and removed examples individually during sessions; examples were treated as a form of context and were a frequent target of edits because of their outsized effect on outputs.",
            "uuid": "e9310.4",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "chain_of_thought",
            "name_full": "Chain-of-thought prompting",
            "brief_description": "A prompting technique that elicits multi-step intermediate reasoning steps from models (referenced in related work as a strategy to improve reasoning), mentioned but not used experimentally in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "reasoning tasks (general)",
            "task_description": "Tasks requiring multi-step reasoning where prompting for intermediate steps can improve final-answer correctness.",
            "presentation_format": "Chain-of-thought prompts that request or exemplify step-by-step reasoning or intermediate chain-of-thought traces.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Cited as a technical strategy (in related work) that can improve reasoning performance in LLMs, but the authors note such strategies can be context-specific and lack guarantees.",
            "null_or_negative_result": null,
            "experimental_details": "Mentioned in related work (Wei et al., 2022). This paper did not run experiments comparing chain-of-thought formats.",
            "uuid": "e9310.5",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "auto_prompt_generation",
            "name_full": "Automated prompt generation strategies",
            "brief_description": "Algorithmic approaches to generate or optimize prompts automatically (referenced in related work), noted as methods that can improve performance but may be context specific.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": null,
            "task_description": "Automated systems that search or optimize prompt text to improve model outputs on target tasks.",
            "presentation_format": "Automatically generated or optimized prompt variants (methods cited: Wang et al., Melamed et al.), contrasted with human iterative editing.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Automated prompt generation can improve performance in some contexts but results are context-specific and not guaranteed; the paper positions these as complementary to human practices.",
            "null_or_negative_result": null,
            "experimental_details": "Referenced in related work; not evaluated in the authors' enterprise dataset.",
            "uuid": "e9310.6",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "model_switching",
            "name_full": "Model switching / comparing multiple LLMs within sessions",
            "brief_description": "Practitioners frequently change the target model during prompt development to compare capabilities; observed as a common parameter edit rather than a prompt presentation format per se but relevant to format comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "various",
            "task_description": "Using different LLMs to execute the same prompt task to evaluate which model better meets task requirements.",
            "presentation_format": "Same prompt submitted to different target models (and often changing other inference parameters) within the same session to compare outputs.",
            "comparison_format": "Multiple models compared within sessions (average 3.6 models used per session).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Frequent model switching suggests users rely on cross-model comparison to find better behavior; availability/ease of switching promotes exploration rather than sticking to a single model, implying model identity interacts with prompt presentation to determine output quality.",
            "null_or_negative_result": null,
            "experimental_details": "93% of sessions involved one or more inference parameter changes; target language model was the most commonly changed parameter. Average number of models per session = 3.6 (SD=2.7), median = 3.",
            "uuid": "e9310.7",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "multi_edit_confounding",
            "name_full": "Multiple simultaneous edits (multi-edit) and confounding effects",
            "brief_description": "Practitioners often make multiple edits at once (e.g., changing context and instructions together), which makes it difficult to attribute changes in output to a single edit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": null,
            "task_description": "Editing behavior phenomenon: multiple prompt components changed in a single submission.",
            "presentation_format": "Multi-edits: submitting a prompt variant that differs in two or more components (context, instructions, labels, parameters) simultaneously.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Multi-edits increase iteration speed but reduce traceability of which change caused observed output differences, complicating debugging and evaluation of presentation-format effects.",
            "null_or_negative_result": null,
            "experimental_details": "22% of all edits were multi-edits (mean number of simultaneous edits = 2.29, SD=0.59). 68% of multi-edits included at least one context edit; 45% included both context and instruction edits; ~20% of edits were accompanied by an inference parameter change.",
            "uuid": "e9310.8",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "rollbacks",
            "name_full": "Edit rollbacks (undo/redo) as evidence of uncertain effects",
            "brief_description": "Users frequently undo or redo prior edits, suggesting difficulty in knowing which prompt presentation changes have beneficial effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": null,
            "task_description": "Behavioral evidence: users revert prior prompt edits during iteration.",
            "presentation_format": "Not a presentation format but an observed behavior indicating uncertainty about edit impact; high rollback rates for some components.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "High rollback rates (11% of edits undone/redone; e.g., 40% rollback for instruction:handle-unknown edits) may indicate edits to some components produce unexpected or negligible changes, or users cannot remember past outcomes, showing difficulty attributing output changes to specific format tweaks.",
            "null_or_negative_result": null,
            "experimental_details": "11% of edits undid or redid a previous edit. Components with higher rollback percentages included handle-unknown (40%), instruction:output-length (25%), label edits (24%), instruction:persona (18%), compared to 8-9% for context and task/instruction:task edits.",
            "uuid": "e9310.9",
            "source_info": {
                "paper_title": "Exploring Prompt Engineering Practices in the Enterprise",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Guiding large language models via directional stimulus prompting.",
            "rating": 1,
            "sanitized_title": "guiding_large_language_models_via_directional_stimulus_prompting"
        },
        {
            "paper_title": "How to prompt? opportunities and challenges of zero-and few-shot learning for human-ai interaction in creative applications of generative models.",
            "rating": 1,
            "sanitized_title": "how_to_prompt_opportunities_and_challenges_of_zeroand_fewshot_learning_for_humanai_interaction_in_creative_applications_of_generative_models"
        },
        {
            "paper_title": "Ten simple rules for crafting effective prompts for large language models.",
            "rating": 1,
            "sanitized_title": "ten_simple_rules_for_crafting_effective_prompts_for_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting (Wei et al., 2022)",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_wei_et_al_2022"
        }
    ],
    "cost": 0.0126775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Prompt Engineering Practices in the Enterprise
13 Mar 2024</p>
<p>Michael Desmond mdesmond@us.ibm.com 
IBM Research</p>
<p>Michelle Brachman michelle.brachman@ibm.com 
IBM Research</p>
<p>Exploring Prompt Engineering Practices in the Enterprise
13 Mar 2024FD301A0A461F958EFBAA80AC383DBC2CarXiv:2403.08950v1[cs.HC]
Interaction with Large Language Models (LLMs) is primarily carried out via prompting.A prompt is a natural language instruction designed to elicit certain behaviour or output from a model.In theory, natural language prompts enable non-experts to interact with and leverage LLMs.However, for complex tasks and tasks with specific requirements, prompt design is not trivial.Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal.We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering.To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes they made.We discuss design implications and future directions based on these prompt engineering practices.</p>
<p>Introduction</p>
<p>With the emergence of instruction tuning and alignment techniques, prompting is the primary mode of interaction with LLMs [Liu et al., 2023b].In the enterprise, developers and AI practitioners try to develop prompts to automate knowledge tasks of varying complexity, in order to improve the efficiency of the organization (and harness the value of AI) [Cambon et al., 2023].This includes tasks like summarizing a document or transcript, generating code or other structured output, and content-grounded Q&amp;A [Ritala et al., 2023].Regardless of task, the prompts typically include various components, sometimes include embedded examples, and require outputs that fit particular requirements and high levels of accuracy [Braun et al., 2024].Further, enterprise contexts may call for the use of models that are more challenging to prompt for a variety of reasons, like cost and specialization.It's not always clear if an LLM is capable of performing a given task, * Equal contribution and significant effort is involved in developing and refining prompts in an attempt to find out.Some studies have begun to explore prompt engineering behaviors [Zamfirescu-Pereira et al., 2023] and prompt structures [Braun et al., 2024].However, prompt engineering is still a new discipline and behaviors likely differ across contexts and domains.We currently know very little about how practitioners in the enterprise edit prompts over time [Schmidt et al., 2023].</p>
<p>We were interested in observing how practitioners edit and refine prompts as a means of exploring and controlling LLM behavior.We believe this process can help us understand how practitioners understand and interact with LLMs, and what kinds of tools would help to make the discovery and engineering process more efficient.To address these questions, we collected a large amount of interaction data from an enterprise-scale LLM prompting environment.This data captured the prompts that users applied to a set of hosted LLMs, and allowed us to study the editing and refinement process that took place over time.We analyzed a sample of 57 users' prompting sessions for qualitative analysis across an array of use cases.We captured the prompt component that the user edited, the type of edit applied, as well as whether an edit was un-done or re-done.</p>
<p>Our contributions include a large-scale analysis of prompt editing practices across varying use cases in an enterprise context and corresponding design implications.Overall, we found that prompt editing sessions were primarily made up of prompt edits mixed with model switches.The most commonly edited prompt component was the context, followed by task instructions and labels and the most common edit type was modification, where the meaning stays the same.</p>
<p>Related Work</p>
<p>Prompting Practices</p>
<p>Prompt engineering and design is the process of formulating the natural language text that is input to a Large Language Model.Researchers study prompt engineering both in terms of technical strategies that improve performance, as well as human-centered study of how users actually interact with prompts.On the technical side, researchers have developed a variety of strategies to improve prompt performance, such as few-shot learning [Brown et al., 2020], chain of thought [Wei et al., 2022], and automated prompt gener-ation strategies [Wang et al., 2023;Melamed et al., 2023].While these strategies may improve performance, they may be context specific and do not come with guarantees.Our work contributes to the relatively new area of understanding prompting practices from a human-centered perspective.</p>
<p>Several studies have identified challenges for non-experts in prompt engineering, finding that they use trial and error [Dang et al., 2022], are willing to reformulate their prompts using support [Bodonhelyi et al., 2024], generalize too much from individual instances, and expect LLMs to act like humans [Zamfirescu-Pereira et al., 2023].A review of literature categorized types of dissatisfaction users have when interacting with LLMs, such as issues with the response format and attitude or the intent understanding.They also investigated users' tactics when repairing incorrect interactions with ChatGPT, such as making an intent more concrete, pointing out errors, or adapting the task [Kim et al., 2023].A small body of work has begun to define "prompt patterns" for ChatGPT, which include both components of prompts as well as prompt modification types [White et al., 2023;Schmidt et al., 2023].Our work builds on this work by analyzing a large set of prompts across use cases and models, focusing on prompt editing practices and capturing their usage across an enterprise dataset.</p>
<p>As prompting is a still emerging practice, many resources have been created to support users in designing and iterating on their prompts, from blogs to courses to research papers.Often, research papers for particular models will provide insight into prompting strategies that work well.Researchers have further focused specifically on prompt practices, identifying a taxonomy of prompt design dimensions [Braun et al., 2024], design considerations for prompting and a typology of prompting methods [Liu et al., 2023b].More closely aligned with online resources for prompting, some work provides general instructions, like that prompts should include examples, be specific, and ask for multiple options [Lin, 2023].</p>
<p>Our work contributes to the understanding of prompt engineering practices through an analysis of LLM prompt iteration in an enterprise context.We analyze both the part of the prompt users modify as well as the type of edit they make to the prompt across a dataset of prompt sessions.This provides significant insight into real prompting patterns, as well as their prevalence.</p>
<p>Mental Models and Repair</p>
<p>We expect that the types of prompting practices used will provide insight into users' mental models, or understanding of, how to best prompt Large Language Models.Mental models, and methods for supporting mental models have been studied extensively for traditional AI and Machine Learning methods [Bansal et al., 2019;Gero et al., 2020].The recent advancement of LLMs raises new questions about how to support users' mental models, as there are new challenges in existing methods, like transparency, for LLMs [Liao and Vaughan, 2023;Bommasani et al., 2023].Yet, we know little about users' current assumptions about the best way to prompt LLMs.</p>
<p>Several studies have investigated users' mental models and usage of LLMs and generative AI for code.Users found the open-ended nature of natural language code generation to be challenging [Liu et al., 2023a], wishing for constraints [Jiang et al., 2022] or information about what kinds of inputs generative AI systems can handle [Sun et al., 2022].One study revealed a set of repair strategies used during generative program synthesis, including rewording, expanding scope, and changing model parameters [Jiang et al., 2022].Another study explored repair strategies for code generation, such as elaboration, language restructuring, and intent shaping [Liu et al., 2023a].While related, these studies focused only on code generation tasks and only on a user query.Our work contributes an analysis of how people edit prompts across a variety of use cases in an enterprise context.</p>
<p>Methods</p>
<p>Data Collection</p>
<p>We collected our dataset of prompts from an internal enterprise platform for experimentation and development with LLMs.The platform has a web-based user interface (UI) for prompt engineering wherein users can input their prompt text, submit the prompt, and the LLM output is generated and appended to their prompt.Users can also modify a variety of generation parameters such as decoding strategy, temperature, repetition penalty etc.The tool is open to internal use across the company and thus could include users of varying prompt engineering experience and expertise.It allows users to use a variety of LLMs, including both open-source models (like llama-2 and flan) and proprietary models.Each datum includes the following information:
 User Id  Time stamp  Prompt text  Target language model  Generation parameters
We collected the above data for 1,712 users from August 31, 2023 to September 20, 2023.The dataset was anonymized before data collection.From the raw data we calculated each users prompt editing history, and then split it into editing "sessions" based on breaks in time of at least 20 minutes between subsequent records.We sampled from the dataset first in order to conduct exploratory analysis, and then again to carry out our final analysis.</p>
<p>For our exploration dataset, we aimed to sample a diverse set of prompt editing behavior.To do this, we sampled: 1) a random set of 40 users' sessions, selected based on the number of prompts they had run on the platform by quartile (10 who had less than 4 prompts, 10 who had between 4 and 11 prompts, 10 who had between 12 and 29 prompts and 10 who had at least 30 prompts), and 2) a random sample of ten sessions across the top 20 users, as these users had many more submitted prompts than the other users selected (over 300) and may have different practices.For our interrater agreement and final analysis, we sampled a new set of sessions distinct from the exploration dataset, to ensure that our analysis would hold on a new set of data.Based on our exploratory analysis, we discovered that short prompting sessions rarely included depth in prompt iteration, so we aimed to analyze a broad set of sessions of moderate length with a high edit rate.We first randomly sampled one session per user of length at least 20 prompts in which at least 75% of prompts had a prompt edit (as opposed to a model or model parameter change), leading to a set of 330 sessions.To select a manageable final dataset, we randomly selected 100 of these sessions.From the 100 sessions, we rejected sessions in which prompts were not in English or in which users tried many sample prompts rather than coherently iterating toward a specific task, ending with a final set of 57 sessions.For our qualitative analysis, we clipped any sessions longer than 50 edits to 50 edits.</p>
<p>Qualitative Analysis</p>
<p>We performed a qualitative analysis of prompt edits and practices in three stages: 1) exploratory code development, 2) inter-rater agreement, and 3) coding.We developed and used a web-based tool that highlights the differences between pairs of successive prompts in order to label the edits between them.For each pair of successive prompt variants, our qualitative analysis captures the edits that occurred from the first variant to the second variant.</p>
<p>The aim of the exploratory code development was to establish and label the types of prompt edits.The two authors looked at our exploratory data set independently, taking notes and forming codes.They then came together, merging their codes, creating definitions for the codes and providing examples of the codes.The resulting code book has categories of codes for: the prompt component that was edited (see Table 2) and the type of edit that occurred (see Table 1).</p>
<p>The authors then performed an inter-rater agreement task, in which they looked at a subset of 6 sessions (11% of the data), each labeling the same set independently with one or more of the established codes for both prompt component and edit type.Due to complexities of multiple codes per edit and multiple categories of codes, we calculated interrater reliability by calculating the average of percentage of matching codes over all edits, reaching 70%.We accept this level of inter-rater agreement due to the complexity of labeling prompt components and prompt edit types, as well as the number of codes in our code book.Finally, the authors divided and independently coded the remaining data, for a total of 57 sessions, comprised of 1523 individual prompt edits.</p>
<p>In addition to our codes, we captured edit rollbacks and session use cases.A rolled back edit meant that it was undoing or re-doing a previously observed edit.Two authors also recorded the use case of each prompt session based on the task instructions and discussed any disagreements or ambiguities.</p>
<p>Results</p>
<p>The results are broken into two sections.First we present a high level quantitative analysis of the observed prompt editing sessions.Secondly, we report more detailed results based on our review and annotation process, including qualitative observations.</p>
<p>Prompt Engineering Sessions: High-level Editing Analysis</p>
<p>Prompt editing sessions were often relatively long.The mean time spent on an editing session was 43.4 minutes (SD=24.5),and the median duration was 39 minutes (See fig. 1 for the distribution).In terms of the time spent editing individual prompts (the time between successive prompt submissions to the platform), we observed that users spent a mean time of 47 seconds (SD=40) editing (not adjusted for inference/text generation time), with a median of 32 seconds.For robustness we winzorized the upper and lower 1% of prompt editing duration data before reporting statistics.</p>
<p>The mean per session edit rate observed was .86 (SD=.13), and the median was 0.9, indicating that close to 9/10 successive inference requests reflected an edit to the prompt.To roughly understand the scale of these edits, we calculated sequence similarity between successive prompts using difflib [Python Software Foundation, 2024], specifically the SequenceMatcher ratio that measures the similarity between two sequences by comparing the length of their longest matching sub-sequence to the total length of both sequences.The distribution of prompt similarity ratios (see figure 2) indicates that the majority of changes were roughly limited to a band between 0.7 and 1.0 similarity (1.0 being an exact match), reflecting the iterative nature of prompt engineering.Users made limited sequential changes as they pursued their task.However, a minority of edits also clustered closer to 0 (no matching sub-strings) indicating that more or less, the entire prompt was changed.One factor to explain large changes to successive prompts is that users could potentially send requests from multiple platform client instances, which is not reflected in the raw data.As such a user could work on more than one prompt in parallel, which looks in the data stream like large successive changes.</p>
<p>To understand what was happening when users did not make any edits to a prompt (and submitted an inference request), we also looked at model and parameter changes.Users regularly changed inference parameters when working on their prompts.93% of observed sessions involved one or more inference parameter change.The most commonly changed parameter was the target language model (model id), followed by the max new tokens and repetition penalty parameters.See figure 3 for a distribution of parameter changes observed in the data.Given the frequency of change to the target model, we wanted to also understand how many different models were used within prompt editing sessions (See fig. 4 for a distribution).The average number of models used in a typical session was 3.6 (SD=2.7),with a median of 3, indicating that users tend to work with multiple models, rather than focusing on a single one.Of course, this phenomenon is facilitated by the availability of a library of models within the observed prompting environment, and the ease of switching amongst these models when developing prompts.However the observed proclivity towards multiple models suggests that support for comparing model performance and better understanding model capabilities may help users to more efficiently work with LLMs.</p>
<p>Prompt Editing Practices Use cases</p>
<p>The analyzed prompts reflected a wide variety of generative AI use cases.We found that use cases included: code/SQL generation (12), Q&amp;A/chat (11), classification (7), extraction (7), summarization (5), reasoning (4), code explanation (3), JSON generation (2), and other (6).</p>
<p>Frequency of Prompt Component and Edit Types</p>
<p>Users focused primarily on editing task instructions and working with context (see Table 2 for descriptions and examples).Other notable foci were labels, output formatting, output inclusion, handling of unknowns and instructions related to controlling the generated output length (see Fig. 5).The focus on editing of context over instruction was surprising, as well as the frequency of edits to the labels, compared to other types of instruction edits.The most common type of edit (see Table 1 for definitions) applied was modification of the prompt in which the general meaning of the prompt remains consistent, followed by additions, changes where the meaning does not stay the same, removals and formatting (see Table 1 for descriptions and frequencies of edit types).Combining the prompt component with the type of edit applied gives a much clearer picture of the nature of the observed prompt editing activity (See fig.6 for a distribution).The most common combination of {prompt component + edit type} pairs includes instruction tasks modification, context addition, change, modification, and removal, and label modification.</p>
<p>Multiple Edits</p>
<p>Edits were not always applied individually, we found that 22% of all edits were multi-edits meaning that the user made multiple simultaneous edits before resubmitting the prompt for inference.In those instances where multiple edits occurred simultaneously, the average number was 2.29 (SD=.59), with a median of 2. We suspected that these edits might include significant overlap with context, as users may have made edits to the instructions or labels, while also switching the document or query.We did find that 68% of multi-edits included at least one context edit.Nearly half (45%) of multi-edits included both a context edit and an instruction edit, while 11% of multi-edits included a context edit along with a label edit.While multi-edits may seem to increase efficiency, by attempting to fix multiple issues at once or by testing a new piece of context with new instructions or labels, making multiple edits at once could make tracking the impact an edit had on the generated output more challenging.Moreover we found that 1/5 edits were also accompanied by an inference parameter change.This type of behavior indicates the need for more systematic approaches to making changes and track the corresponding changes in model behavior, helping users to arrive at conclusions in a more comprehensible manner.</p>
<p>Rollbacks</p>
<p>11% of prompt edits undid or redid a previous edit (but are still counted as individual edits).Undoing or redoing a prompt edit could indicate a challenge in humans' ability to remember outcomes of previous attempts, or lack of certainty in the kinds of edits that might improve the output.Interestingly, we found that prompt components that were edited less frequently often had higher percentages of un-done edits.We found high proportions of rollbacks for some prompt component edits, like 40% of edits for instructions:handle-unknown were rolled back, as well as 25% of instruction:output-length, 24% of label edits, and 18% of instruction:persona edits, compared to only 8-9% of edits to context, instruction:task, and instruction:output-format.This may begin to explain why some of these components were edited less frequently-users may have found that their edits to those components did not have big impacts or had adverse or unexpected impacts.</p>
<p>Context</p>
<p>We observed that the majority of the analyzed prompts/usecases were context-based, meaning that input, grounding data, or examples were embedded within the prompt, distinct from the task instructions.In fact, context was the most edited component across all of the analyzed sessions (see figure 5) reflecting the importance of this construct for enterprise tasks.</p>
<p>We observed two common patterns of context additions: 1) simulating dialog, and 2) adding examples.The prompt development interface our users worked with appended generated output directly to the input prompt text.This design made it very easy to generate turns in a conversation or dialog.Because each of these turns is a new generation, this accumulates a significant number of context additions.Further, much of this context would need to be removed for each of the other kinds of edits a user might make, before testing the conversation again.This kind of behavior matches Q&amp;A use cases the best, but can also be applicable in other generation tasks where a user might want to experiment with the length of the output to see what else a model would generate.Approximately 19% of our prompts were Q&amp;A use cases.Another kind of context addition was examples, which users added and removed, sometimes individually.Because examples can have significant effects on the generation, users may have wanted to add in examples individually or add and remove them to compare their impacts.</p>
<p>Replacing and modifying existing context were also very common activities.We noticed that users would use a particular context to develop and refine their task instructions, and then proceed to evaluate the robustness of the instructions by switching in and out alternate contexts and observing model output.It was also common for users to directly edit existing context such a making changes to contextual queries or input data examples.</p>
<p>Instructions</p>
<p>The editing behaviors of the task instructions we observed supports the common intuition that prompt development is an iterative trial and error process.The most common type of instruction edit was a task modification.In these situations, users were often re-wording the description of what the LLM should generate in order to improve the output.Instruction modifications could be as small as changing the capitalization, changing between a written word for a number and the numerical representation, or changing the punctuation.Some more complex ways users rephrased task instructions included: rephrasing an instruction between a statement and a question, rephrasing an instruction from a command to a description of what will be generated next, adding more detail, and simplifying.After modifying task instructions, the next most common activity was adding new task instructions, which often included specific rules or methods to generate the output.Somewhat surprisingly, edits to other types of instructions (output-format, output-inclusion, persona, handleunknown, and output-length) were uncommon.Adding or modifying instructions to define the output format and what should be included were of the most common of these edits.One reason for this may be the types of use cases we saw in these editing sessions.For Q&amp;A and code generation use cases, the output format and inclusion criteria may be more standardized and straightforward than in less common use cases like JSON generation, classification, and extraction.</p>
<p>Labels</p>
<p>Edits in the labels of the prompt were the third most common prompt component edits.Label edits typically took the form of an identifier followed by a colon, a separator, or a start/end tag which delineates a section of the information.Prompts used labels throughout, such as for the instructions, context, examples, dialog, and output.Label modifications are likely driven by users' desire to have the LLM focus on certain potentially parameterize-able constructs within the prompt.Editing the labels for the output was also a common occurrence, as the label for output is the text directly prior to the generated text.In some cases, the output label acted like an instruction or a repetition of the instructions, indicating what should directly follow it.Since the output label has a potentially important role in the generation, users may have edited it to try to gain more control over the output.</p>
<p>Limitations</p>
<p>Our data was anonymous, thus limiting our knowledge about the users' previous experience with prompting or their role in the organization.Thus, we cannot draw conclusions about how prompting practices may differ between novices and experts, nor along any other demographic dimensions.We sampled broadly to attempt to mitigate this limitation, as our user population has a wide range of skills and backgrounds.Further, due to the uneven distribution of use cases in prompt development, our sample does not cover use cases evenly.This may impact the frequency of edit types.We are also unable to share exact examples of prompt changes due to privacy restrictions on the data, limiting our ability to show what these prompts and edits look like in practice.</p>
<p>6 Design Implications and Future Work Prompt Debugging and Testing Support Our work explores prompt editing practices in a prompting tool, where users iterate on their prompt until they feel that it is good enough to move it to code or more in-depth evaluation.However, behaviors like undoing and redoing the same changes, making multiple edits at a time, frequent context changes, and model and parameter edits indicate inefficiencies in the iterative process of prompt engineering.</p>
<p>Undoing and redoing the same edits could indicate challenges users have remembering how the output changed for different prompt edits, or that they have already made that change.This indicates a need for further support to reduce the cognitive load on users, such as by capturing edits and corresponding outputs in more systematic and accessible ways, so users don't need to remember or re-run the same prompt multiple times.However, a version history that keeps track of every edit could also be hard to keep track of.Researchers [Bach et al., 2022;Mishra et al., 2023;Strobelt et al., 2022] and commercial systems (one of which is promptfoo 1 , but many more exist and are not all publicly available) have been developed to support prompt iteration and comparison.These systems are often graphical user interfaces, rather than code that could use version control such as Git, but often lack sophisticated version control features that address the particular challenges we found in prompt iteration.</p>
<p>Making multiple edits at once may make it more difficult to determine the impacts of a particular edit, leading to further edits to isolate the effective or ineffective edit.We often found that in cases of multiple edits, context edits happened alongside other kinds of edits.Some prompt engineering tools support multiple values for variables within a prompt, enabling multiple versions of contexts [Arawjo et al., 2023;Strobelt et al., 2022].We also found that users were often adding and removing parts of the context, which may lend 1 https://www.promptfoo.dev/itself to a different type of support, such as the ability to easily comment out or disable parts of a prompt without deleting them.</p>
<p>Prompt Structure</p>
<p>Both edits to the prompt label and formatting edits, which include white space and punctuation changes, point to attempts to structure prompts.Labels and complex structure may be more important and widely used in business contexts, where prompts are often grounded with documents, include examples, and may involve user and agent conversation.Further, while components like prompt instructions and context may be highly use case specific, structuring of prompts might be more easy to standardize.Some potential ways to support prompt engineering may be programming frameworks like LangChain2 and Microsoft Guidance3 or visual environments [Wu et al., 2022].However, these tools focus more on the construction and connection of multiple prompts or prompt components, rather than supporting a user in deciding on a particular prompt structure.Other research has considered specific elements that can be added to augment a prompt, like instance-specific "hints" [Li et al., 2024].We suggest that future research investigate the potential for more structure to support users from both the model and the user interface perspectives and explore whether or not this structure can reduce prompt engineering effort.</p>
<p>Conclusion</p>
<p>In this work, we present an analysis of 57 prompt editing sessions, including 1523 individual prompts, from an enterprise LLM tool that supports prompt experimentation and development with LLMs.We found that users are often editing their prompt in addition to, or instead of, editing other model parameters.Many of these edits are quite small, likely indicating a tweak or iteration to one prompt rather than swapping in an entirely different prompt or task.Our qualitative analysis of edits shows that users are most often modifying the prompt context, including examples, grounding documents, and input queries.We were surprised that context edits outnumbered instruction edits, in which users are describing the task that they need accomplished, or particular elements of the task like the format of the output, what should or shouldn't be in the output, the output length, or the persona.Further, edits to labels, which define the components of the prompt, were also common.These findings provide critical insights into existing prompt editing practices and inform future directions on how to support prompt engineering in more effective ways.We are currently exploring various forms of prompt engineering assistance informed by the results of this study, such as for example, structured prompting, prompt histories &amp; comparisons, variation authoring, semi-automated explorations of variations, and built-in prompt quality evaluation based on use case-specific metrics.</p>
<p>Figure 1 :
1
Figure 1: Duration of observed prompt editing sessions in minutes.</p>
<p>Figure 2 :
2
Figure 2: The size of change between successive prompts, represented as a similarity ratio.Values closer to 1 indicate similarity between successive prompts, 1 being an exact match, while a ratio of 0 indicates nothing in common.</p>
<p>Figure 3 :
3
Figure3: The occurrence of parameter changes as a percentage of sessions in which the change was observed.Users primarily changed the target language model, the maximum number of tokens to generate, and repetition penalty.Stop sequence, temperature and decoding method were also commonly changed.</p>
<p>Figure 4 :
4
Figure 4: The number of models used per session.</p>
<p>Figure 5 :
5
Figure 5: Number of edits that focused on each of the prompt components.Users primarily edited context, and task instructions to a lesser extent.</p>
<p>Figure 6 :
6
Figure 6: Number of edits of {prompt component + edit type} pairs.For readability this figure is limited to the top 15 (of 41) most common pairs.</p>
<p>Table 1 :
1
Codes and descriptions for the types of edits made to prompts.
Edit TypeDescription
Explanation, specific information from a provided document instruction:handle-unknown Handle-unknown is a description of what the output should be if the LLM is missing the knowledge needed to generate the requested output.If you don't know, respond with [...] label Labels include text in the prompt that identifies an element of a prompt.Instruction:; <Context> &lt; /Context&gt; context We define context as including examples, documents for grounded responses, and input queries.other not described above</p>
<p>Table 2 :
2
Codes, descriptions and examples for prompt components users edited.</p>
<p>https://python.langchain.com/
https://github.com/guidance-ai/guidance</p>
<p>Promptsource: An integrated development environment and repository for natural language prompts. Arawjo, arXiv:2202.01279Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. Ece Kamar2023. 2023. 2022. 2022. 2019. 20197arXiv preprintProceedings of the AAAI conference on human computation and crowdsourcing</p>
<p>Can (a) i have a word with you? a taxonomy on the design dimensions of ai prompts. Bodonhelyi, arXiv:2402.02136arXiv:2310.12941A user study with chatgpt. Benjamin Mann, Nick Ryder, Melanie SubbiahTom Brown2024. 2024. 2023. 2023. 2024. 2024. 2020. 202033arXiv preprintLanguage models are few-shot learners. Advances in neural information processing systems</p>
<p>How to prompt? opportunities and challenges of zero-and few-shot learning for human-ai interaction in creative applications of generative models. Cambon, arXiv:2209.01390Proceedings of the 2020 chi conference on human factors in computing systems. Jiang, the 2020 chi conference on human factors in computing systems2023. 2023. 2022. 2022. 2020. 2020. 2022arXiv preprintProceedings of the 2022 CHI Conference on Human Factors in Computing Systems</p>
<p>Understanding users' dissatisfaction with chatgpt responses: Types, resolving tactics, and the effect of knowledge level. Kim, arXiv:2311.074342023. 2023arXiv preprint</p>
<p>Guiding large language models via directional stimulus prompting. Li, arXiv:2306.01941Vera Liao and Jennifer Wortman Vaughan. Ai transparency in the age of llms: A human-centered research roadmap. 2024. 2024. 202336arXiv preprintLiao and Vaughan, 2023</p>
<p>Ten simple rules for crafting effective prompts for large language models. Lin ; , Zhicheng Lin, SSRN 4565553. 2023. 2023</p>
<p>what it wants me to say": Bridging the abstraction gap between end-user programmers and code-generating large language models. Liu, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023a. 2023</p>
<p>Python Software Foundation. difflib: Helpers for Computing String Similarities and Differences. Python Software Foundation. Liu, arXiv:2311.07064arXiv:2302.11382CHI Conference on Human Factors in Computing Systems Extended Abstracts. Wei, Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert; Jeff Gray,Jesse Spencer-Smith, and Douglas C Schmidt2023b. 2023. 2023. 2023. 2023. 2023. 2024. 2023. 2023. 2023. 2022. 2022. 2022. 2023. 2022. 2023. 202255arXiv preprint27th International Conference on Intelligent User Interfaces</p>
<p>. Zamfirescu-Pereira, 2023JD Zamfirescu-Pereira</p>
<p>Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. Bjoern Richmond Y Wong, Qian Hartmann, Yang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>            </div>
        </div>

    </div>
</body>
</html>
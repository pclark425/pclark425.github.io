<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-263908782</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.07820v3.pdf" target="_blank">Large Language Models Are Zero-Shot Time Series Forecasters</a></p>
                <p><strong>Paper Abstract:</strong> By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMTIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMTIME (Language Model Time Series Forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot method that encodes numerical time series as strings of digits and uses pretrained autoregressive language models to produce probabilistic forecasts by next-token prediction and sampling; converts discrete token distributions into continuous densities for likelihood-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003), LLaMA / LLaMA-2 (various sizes, incl. 70B), GPT-4 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3 (API model family), LLaMA/LLaMA-2 (7B, 13B, 30/33B, 65B, 70B variants), GPT-4 (undisclosed / large)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numerical sequences / time series (univariate and multivariate; digit-string encoding of numeric values)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>benchmarks and real datasets: Darts suite (8 univariate series), Monash archive (subset of 19 datasets), Informer multivariate datasets, synthetic sequences, and post-2021 real series (Istanbul Traffic, TSMC Stock, Turkey Power)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>missing values explicitly (encoded as textual 'NaN'); implicitly low-likelihood / rare or out-of-distribution continuations (via sequence log-likelihoods) and heavy-tailed or multimodal residual distributions (density modelling)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Zero-shot forecasting + sequence scoring: preprocess numeric series into digit-separated text (tokenization hacks per model), rescale values, feed context to a pretrained LLM, sample many continuations (or compute token likelihoods when available). Convert discrete token probabilities over digit-strings into continuous densities by treating each digit-string as a bin (hierarchical softmax) and placing a uniform density within each bin; use sampled quantiles or median for point estimates and the continuous density / sequence log-likelihood for probabilistic evaluation or anomaly scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>ARIMA, TCN, N-BEATS, N-HiTS, Spectral-Mixture Gaussian Process (SM-GP), PromptCast (prior LLM QA approach), other classical/statistical baselines from Darts and Monash (ETS, CatBoost, DeepAR, WaveNet, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Negative log likelihood per dimension (NLL/D), Continuous Ranked Probability Score (CRPS), Mean Absolute Error (MAE), Wasserstein distance (for density-estimation experiments), calibration/uncertainty diagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Aggregated across benchmarks, LLMTIME (GPT-3 and LLaMA-2 70B) achieved considerably better probabilistic scores (NLL and CRPS) than the evaluated baselines and competitive or best MAE (best or second-best) on deterministic metrics; LLMTIME was more sample-efficient (maintained high likelihood with small training fractions). PromptCast was explicitly outperformed in aggregated CRPS and MAE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Overall better in probabilistic metrics and competitive/better on point-prediction MAE; outperforms PromptCast and many purpose-built time series models in zero-shot evaluation on the tested benchmarks, though performance varies by dataset and horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance is sensitive to numeric tokenization and preprocessing (spaces between digits help for GPT-3 but hurt LLaMA-2); chat/aligned variants (e.g., GPT-4, LLaMA-2 chat) often perform worse due to tokenization differences, limited likelihood access, and degraded calibration after RLHF/instruction tuning; limited context window constrains long/multivariate series; numerical/compositional arithmetic tasks can be challenging; reliance on black-box APIs limits access to full logits and forces approximation when renormalizing probabilities; variable dataset-dependent performance (best on some datasets, worst on others).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>LLMs can be repurposed as flexible, zero-shot probabilistic sequence models: encoding numbers as digit-strings plus a continuous-density conversion yields expressive multimodal/heavy-tailed densities, enabling likelihood-based scoring of entire sequences (a natural mechanism for flagging low-likelihood / anomalous continuations). LLMTIME also naturally handles missing observations by encoding them as textual tokens (e.g., 'NaN') rather than imputing, and leverages LLM textual abilities to incorporate side information and provide human-readable explanations of patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Zero-Shot Time Series Forecasters', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Missing-value-as-text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Handling missing observations by textual 'NaN' tokens in LLM inputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing strategy that represents missing time series observations explicitly as text tokens (e.g., 'NaN') so that a language model can model missingness directly without imputation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003), LLaMA-2 (70B) (demonstrated in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3 (API), LLaMA-2 70B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numerical time series with missing entries (univariate benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Darts benchmark datasets and other time series collections used in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>missing values / irregularly sampled observations (missingness could itself be informative)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encode missing time steps as explicit textual tokens ('NaN') in the digit-string sequence input; feed sequence to LLM and use sequence likelihood or sampling to forecast / score sequences without first performing numerical imputation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Linear interpolation (used for baseline models before standard forecasting pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Sequence log-likelihood (NLL) and CRPS for forecast quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>LLMTIME assigns higher log likelihood to datasets preprocessed with textual 'NaN's than baseline methods assigned to interpolated datasets; CRPS-based forecasting performance is competitive between LLMTIME and alternative methods that used explicit interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Higher log-likelihood when missingness represented as 'NaN' vs baselines that used interpolation; forecasting CRPS competitive with interpolation-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper does not present extensive ablations across missingness mechanisms; treating missing values as text relies on the LLM learning patterns of missingness in-context and may not capture complex missing-data mechanisms without explicit prompt engineering or additional context; not a dedicated anomaly detector for missingness patterns (no thorough evaluation on missingness-detection tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Representing missingness as textual tokens leverages the LLM's native multimodal input handling and avoids committing to a specific numeric imputation, letting the model reflect uncertainty directly in its probabilistic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Zero-Shot Time Series Forecasters', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continuous-likelihood-from-digits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Converting LLM discrete token distributions over digit-strings into continuous densities for numbers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that interprets autoregressive digit-token probabilities as a hierarchical softmax over numeric bins and places uniform density within each bin, producing continuous log-densities for numeric values (enabling NLL and anomaly scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (GPT-3, LLaMA/LLaMA-2 models evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3 (API), LLaMA/LLaMA-2 (7B–70B evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numerical sequences (encoded as digit-strings with fixed precision)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>synthetic 1D distributions (density fitting experiments) and time series forecasting benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>low-likelihood / out-of-distribution numeric values, multimodal or heavy-tailed residuals (density modelling enables identifying unlikely observations)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Model each numeric value as a sequence of digits (base B, n digits → B^n bins). The LLM's softmax over digit tokens defines a hierarchical probability p_k for a specific numeric bin; treat the bin as a uniform continuous component and compute continuous log-likelihood as log p_k + n log B plus a Jacobian term from any rescaling. This continuous likelihood supports NLL-based evaluation and can be used to flag low-likelihood (anomalous) observations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Gaussian/Laplace observation models, Gaussian mixture models (GMM), fixed-resolution binning (logistic regression over bins)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Wasserstein distance (for density estimation comparisons), NLL/CRPS for sequence probabilistic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In low-sample density experiments (200 samples), a decimal autoregressive model (similar construction to LLM digit modelling) matched or outperformed GMMs and binning approaches on Wasserstein distance for heavy-tailed, multimodal, and asymmetric distributions. In forecasting, the continuous-density conversion allowed LLM-based forecasters to achieve superior NLL and CRPS compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Decimal autoregressive / LLM-based density representations were competitive with or superior to GMMs and fixed-binning approaches in representing multimodal and heavy-tailed 1D distributions and led to better probabilistic forecasts (NLL/CRPS) in the time series benchmarks evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Accurate continuous-likelihood computation requires access to model token probabilities/logits; with black-box APIs (e.g., OpenAI) only top-k logprobs may be available, requiring approximations/renormalization heuristics and introducing error. Tokenization quirks (how digits are tokenized) directly affect the hierarchical softmax interpretation. Extremely long integer parts (arbitrary digits before decimal) create an infinite set of possible bins; practical implementations rely on rescaling heuristics (α, β) and finite precision.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Interpreting digit-level autoregressive token probabilities as a hierarchical softmax over numeric bins is an efficient and expressive way to obtain high-resolution continuous densities from discrete LLM outputs, enabling principled likelihood-based anomaly scoring and better representation of multimodal/heavy-tailed behaviors common in time series.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Are Zero-Shot Time Series Forecasters', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Promptcast: A new prompt-based learning paradigm for time series forecasting <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 1)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9422",
    "paper_id": "paper-263908782",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "LLMTIME",
            "name_full": "LLMTIME (Language Model Time Series Forecasting)",
            "brief_description": "A zero-shot method that encodes numerical time series as strings of digits and uses pretrained autoregressive language models to produce probabilistic forecasts by next-token prediction and sampling; converts discrete token distributions into continuous densities for likelihood-based evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003), LLaMA / LLaMA-2 (various sizes, incl. 70B), GPT-4 (evaluated)",
            "model_type": "autoregressive transformer language model",
            "model_size": "GPT-3 (API model family), LLaMA/LLaMA-2 (7B, 13B, 30/33B, 65B, 70B variants), GPT-4 (undisclosed / large)",
            "data_type": "numerical sequences / time series (univariate and multivariate; digit-string encoding of numeric values)",
            "data_domain": "benchmarks and real datasets: Darts suite (8 univariate series), Monash archive (subset of 19 datasets), Informer multivariate datasets, synthetic sequences, and post-2021 real series (Istanbul Traffic, TSMC Stock, Turkey Power)",
            "anomaly_type": "missing values explicitly (encoded as textual 'NaN'); implicitly low-likelihood / rare or out-of-distribution continuations (via sequence log-likelihoods) and heavy-tailed or multimodal residual distributions (density modelling)",
            "method_description": "Zero-shot forecasting + sequence scoring: preprocess numeric series into digit-separated text (tokenization hacks per model), rescale values, feed context to a pretrained LLM, sample many continuations (or compute token likelihoods when available). Convert discrete token probabilities over digit-strings into continuous densities by treating each digit-string as a bin (hierarchical softmax) and placing a uniform density within each bin; use sampled quantiles or median for point estimates and the continuous density / sequence log-likelihood for probabilistic evaluation or anomaly scoring.",
            "baseline_methods": "ARIMA, TCN, N-BEATS, N-HiTS, Spectral-Mixture Gaussian Process (SM-GP), PromptCast (prior LLM QA approach), other classical/statistical baselines from Darts and Monash (ETS, CatBoost, DeepAR, WaveNet, etc.)",
            "performance_metrics": "Negative log likelihood per dimension (NLL/D), Continuous Ranked Probability Score (CRPS), Mean Absolute Error (MAE), Wasserstein distance (for density-estimation experiments), calibration/uncertainty diagnostics",
            "performance_results": "Aggregated across benchmarks, LLMTIME (GPT-3 and LLaMA-2 70B) achieved considerably better probabilistic scores (NLL and CRPS) than the evaluated baselines and competitive or best MAE (best or second-best) on deterministic metrics; LLMTIME was more sample-efficient (maintained high likelihood with small training fractions). PromptCast was explicitly outperformed in aggregated CRPS and MAE.",
            "comparison_to_baseline": "Overall better in probabilistic metrics and competitive/better on point-prediction MAE; outperforms PromptCast and many purpose-built time series models in zero-shot evaluation on the tested benchmarks, though performance varies by dataset and horizon.",
            "limitations_or_failure_cases": "Performance is sensitive to numeric tokenization and preprocessing (spaces between digits help for GPT-3 but hurt LLaMA-2); chat/aligned variants (e.g., GPT-4, LLaMA-2 chat) often perform worse due to tokenization differences, limited likelihood access, and degraded calibration after RLHF/instruction tuning; limited context window constrains long/multivariate series; numerical/compositional arithmetic tasks can be challenging; reliance on black-box APIs limits access to full logits and forces approximation when renormalizing probabilities; variable dataset-dependent performance (best on some datasets, worst on others).",
            "unique_insights": "LLMs can be repurposed as flexible, zero-shot probabilistic sequence models: encoding numbers as digit-strings plus a continuous-density conversion yields expressive multimodal/heavy-tailed densities, enabling likelihood-based scoring of entire sequences (a natural mechanism for flagging low-likelihood / anomalous continuations). LLMTIME also naturally handles missing observations by encoding them as textual tokens (e.g., 'NaN') rather than imputing, and leverages LLM textual abilities to incorporate side information and provide human-readable explanations of patterns.",
            "uuid": "e9422.0",
            "source_info": {
                "paper_title": "Large Language Models Are Zero-Shot Time Series Forecasters",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Missing-value-as-text",
            "name_full": "Handling missing observations by textual 'NaN' tokens in LLM inputs",
            "brief_description": "A preprocessing strategy that represents missing time series observations explicitly as text tokens (e.g., 'NaN') so that a language model can model missingness directly without imputation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003), LLaMA-2 (70B) (demonstrated in experiments)",
            "model_type": "autoregressive transformer language model",
            "model_size": "GPT-3 (API), LLaMA-2 70B (evaluated)",
            "data_type": "numerical time series with missing entries (univariate benchmarks)",
            "data_domain": "Darts benchmark datasets and other time series collections used in the paper",
            "anomaly_type": "missing values / irregularly sampled observations (missingness could itself be informative)",
            "method_description": "Encode missing time steps as explicit textual tokens ('NaN') in the digit-string sequence input; feed sequence to LLM and use sequence likelihood or sampling to forecast / score sequences without first performing numerical imputation.",
            "baseline_methods": "Linear interpolation (used for baseline models before standard forecasting pipelines)",
            "performance_metrics": "Sequence log-likelihood (NLL) and CRPS for forecast quality",
            "performance_results": "LLMTIME assigns higher log likelihood to datasets preprocessed with textual 'NaN's than baseline methods assigned to interpolated datasets; CRPS-based forecasting performance is competitive between LLMTIME and alternative methods that used explicit interpolation.",
            "comparison_to_baseline": "Higher log-likelihood when missingness represented as 'NaN' vs baselines that used interpolation; forecasting CRPS competitive with interpolation-based approaches.",
            "limitations_or_failure_cases": "Paper does not present extensive ablations across missingness mechanisms; treating missing values as text relies on the LLM learning patterns of missingness in-context and may not capture complex missing-data mechanisms without explicit prompt engineering or additional context; not a dedicated anomaly detector for missingness patterns (no thorough evaluation on missingness-detection tasks).",
            "unique_insights": "Representing missingness as textual tokens leverages the LLM's native multimodal input handling and avoids committing to a specific numeric imputation, letting the model reflect uncertainty directly in its probabilistic outputs.",
            "uuid": "e9422.1",
            "source_info": {
                "paper_title": "Large Language Models Are Zero-Shot Time Series Forecasters",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Continuous-likelihood-from-digits",
            "name_full": "Converting LLM discrete token distributions over digit-strings into continuous densities for numbers",
            "brief_description": "A method that interprets autoregressive digit-token probabilities as a hierarchical softmax over numeric bins and places uniform density within each bin, producing continuous log-densities for numeric values (enabling NLL and anomaly scoring).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "General LLMs (GPT-3, LLaMA/LLaMA-2 models evaluated in paper)",
            "model_type": "autoregressive transformer language model",
            "model_size": "GPT-3 (API), LLaMA/LLaMA-2 (7B–70B evaluated)",
            "data_type": "numerical sequences (encoded as digit-strings with fixed precision)",
            "data_domain": "synthetic 1D distributions (density fitting experiments) and time series forecasting benchmarks",
            "anomaly_type": "low-likelihood / out-of-distribution numeric values, multimodal or heavy-tailed residuals (density modelling enables identifying unlikely observations)",
            "method_description": "Model each numeric value as a sequence of digits (base B, n digits → B^n bins). The LLM's softmax over digit tokens defines a hierarchical probability p_k for a specific numeric bin; treat the bin as a uniform continuous component and compute continuous log-likelihood as log p_k + n log B plus a Jacobian term from any rescaling. This continuous likelihood supports NLL-based evaluation and can be used to flag low-likelihood (anomalous) observations.",
            "baseline_methods": "Gaussian/Laplace observation models, Gaussian mixture models (GMM), fixed-resolution binning (logistic regression over bins)",
            "performance_metrics": "Wasserstein distance (for density estimation comparisons), NLL/CRPS for sequence probabilistic evaluation",
            "performance_results": "In low-sample density experiments (200 samples), a decimal autoregressive model (similar construction to LLM digit modelling) matched or outperformed GMMs and binning approaches on Wasserstein distance for heavy-tailed, multimodal, and asymmetric distributions. In forecasting, the continuous-density conversion allowed LLM-based forecasters to achieve superior NLL and CRPS compared to baselines.",
            "comparison_to_baseline": "Decimal autoregressive / LLM-based density representations were competitive with or superior to GMMs and fixed-binning approaches in representing multimodal and heavy-tailed 1D distributions and led to better probabilistic forecasts (NLL/CRPS) in the time series benchmarks evaluated.",
            "limitations_or_failure_cases": "Accurate continuous-likelihood computation requires access to model token probabilities/logits; with black-box APIs (e.g., OpenAI) only top-k logprobs may be available, requiring approximations/renormalization heuristics and introducing error. Tokenization quirks (how digits are tokenized) directly affect the hierarchical softmax interpretation. Extremely long integer parts (arbitrary digits before decimal) create an infinite set of possible bins; practical implementations rely on rescaling heuristics (α, β) and finite precision.",
            "unique_insights": "Interpreting digit-level autoregressive token probabilities as a hierarchical softmax over numeric bins is an efficient and expressive way to obtain high-resolution continuous densities from discrete LLM outputs, enabling principled likelihood-based anomaly scoring and better representation of multimodal/heavy-tailed behaviors common in time series.",
            "uuid": "e9422.2",
            "source_info": {
                "paper_title": "Large Language Models Are Zero-Shot Time Series Forecasters",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Promptcast: A new prompt-based learning paradigm for time series forecasting",
            "rating": 2
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 1
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.015151749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Are Zero-Shot Time Series Forecasters
12 Aug 2024</p>
<p>Nate Gruver 
Marc Nyu 
Finzi 
Shikai Cmu 
Qiu 
Andrew Nyu 
Wilson Gordon 
Nyu 
Large Language Models Are Zero-Shot Time Series Forecasters
12 Aug 20245726715C1C13A3DE3DB4D2F537A73313arXiv:2310.07820v3[cs.LG]"631656650...487485487" "479...371364" "492...499501" "and bit my leg"
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text.Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zeroshot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks.To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values.We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends.We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions.While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.* Equal contribution 2 https://github.com/ngruver/llmtime37th Conference on Neural Information Processing Systems (NeurIPS 2023).</p>
<p>Introduction</p>
<p>Despite similarities with other sequence modeling problems, such as text, audio, or video, time series has two particularly challenging properties.Unlike video or audio, which typically have consistent input scales and sampling rates, aggregated time series datasets often comprise sequences from radically different sources, sometimes with missing values.Moreover, common applications of time series forecasting, such as weather or financial data, require extrapolating from observations that contain a tiny fraction of the possible information, making accurate point predictions nearly impossible and uncertainty estimation especially important.While large-scale pretraining has become a key element of training large neural networks in vision and text, enabling performance to scale directly with data availability, pretraining is not typically used for time series modeling, where there is no consensus unsupervised objective and large, cohesive pretraining datasets are not readily available.Consequently, simple time series methods (e.g.ARIMA [8], and linear models [53]) often outperform deep learning methods on popular benchmarks [25].</p>
<p>In this paper, we demonstrate how large language models (LLM) can naturally bridge the gap between the simple biases of traditional methods and the complex representational learning and generative abilities of modern deep learning.In particular, we introduce an exceedingly simple method, LLMTIME 2 , to apply pretrained LLMs for continuous time series prediction problems, illustrated at a high level in Figure 1.At its core, this method represents the time series as a string of numerical digits, and views time series forecasting as next-token prediction in text, unlocking Figure 1: We propose LLMTIME, a method for time series forecasting with large language models (LLMs) by encoding numbers as text and sampling possible extrapolations as text completions.LLMTIME can outperform many popular time series methods without any training on the target dataset (i.e.zero-shot).The performance of LLMTIME also scales with the power of the underlying base model.Notably, models that undergo alignment (e.g.RLHF) do not follow the scaling trend.For example, GPT-4 demonstrates inferior performance to GPT-3 (Section 6). the use of powerful pretrained models and probabilistic capacities, such as likelihood evaluation and sampling.To enable strong performance, we propose techniques to (1) effectively encode time series as a string of numerical digits and (2) adapt the discrete distributions of LLMs to continuous densities capable of modeling sophisticated multimodal distributions.Using these techniques, we find LLMTIME can exceed or match purpose-built time series methods over a range of different problems in a zero-shot fashion, meaning that LLMTIME can be used without any fine-tuning on the downstream data used by other models.</p>
<p>The zero-shot nature of LLMTIME carries several natural advantages: (1) it facilitates the straightforward application of LLMs, eliminating the necessity for specialized knowledge of fine-tuning procedures and the substantial computational resources required for these procedures, as well as sidestepping access issues surrounding proprietary source code or APIs for LLM training or fine-tuning;</p>
<p>(2) it is naturally suited to scenarios with limited data availability, where there is little information for training or fine-tuning; (3) by leveraging the broad pattern extrapolation capabilities of extensively pre-trained LLMs, it circumvents the extensive time, effort, and domain-specific expertise typically required for crafting dedicated time series models.</p>
<p>To understand the origins of LLMTIME's impressive performance, we investigate how LLMs express preferences for simple or repetitive sequences [20] and show that these biases are in fact compatible with the salient structure of time series, such as seasonality.Aside from these biases, LLMs also can naturally acccommodate missing data, and express multimodal distributions, which is particularly useful for time series.We also show how LLMs enable appealing functionality, such as the ability to provide additional side information through prompting, and query the LLM to explain its predictions.</p>
<p>Finally, in addition to broadly compelling forecasting performance, we find performance tends to improve with scale, and the quality of point predictions also improves with the quality of the uncertainty representation.However, we also find GPT-4 has worse uncertainty calibration than GPT-3, likely due to interventions such as reinforcement learning by human feedback (RLHF).</p>
<p>Background</p>
<p>Language modeling</p>
<p>Language models are trained on a collection of sequences, U = {U 1 , U 2 , . . .U i , . . ., U N }, where U i = (u 1 , u 2 , . . ., u j , . . ., u ni ) and each token, u i , belongs to a vocabulary V. Large language models typically encode an autoregressive distribution, in which the probability of each token is only dependent on the previous tokens in the sequence, p θ (U i ) = ni j=1 p θ (u j | u 0:j−1 ).The parameters, θ, are learned by maximizing the probability of the entire dataset p θ (U) = N i=1 p θ (U i ).Every language model has an associated tokenizer, which breaks an input string into a sequence of tokens, each belonging to V. Proper tokenization is extremely important, and small details can have surprisingly significant effects.The most common tokenization method for autoregressive language models is byte-pair encoding (BPE), which treats inputs like bit strings and assigns tokens based on the rate of occurrence in the training corpus, optimizing for shorter sequences of tokens on average.Sampling from a language model typically starts with a prompt, u 0:k , and proceeds sequentially using p θ (u j | u 0:j−1 ), which is often preprocessed, for example through temperature scaling or nucleus sampling [26].</p>
<p>Large language models Brown et al. [9] showed that increasing a language model's parameter count and training data size leads to new capabilities such as zero-shot generalization, in which a model can perform a text-formatted task without training the model parameters on any task-specific data.Large language models, for example GPT-3 [9] or LLaMA-2 [45], accomplish this form of generalization through in-context learning, which identifies patterns in the language model's prompt and extrapolates them through next-token prediction.Many authors have speculated that in-context learning emerges from a language model's extensive compression of the input data [20,43,15].Compression favors learning algorithms that operate over the input data with programmatic abstractions, for example, context-free grammars [2] or induction heads [35], which can implement copy-and-paste type operations for generating samples with highly structured syntax.In this work, we show that the zero-shot generalization abilities of LLMs and their preference for compressible patterns extend well beyond language understanding and can be used for time series forecasting.</p>
<p>Zero-shot generalization has made LLMs significantly more useful as assistants, leading to the create of methods to align LLMs with human preferences and instructions, for example reinforcement learning from human feedback (RLHF) [39] and instruction tuning [47].While key to modern LLMs products, alignment methods can also significantly affect the abilities and calibration of the underlying model [37,10].Here we show these methods can also affect forecasting ability.</p>
<p>Time series data Time series data typically takes the exact same form as language modeling data, as a collection of sequences {U i = (u 1 , . . ., u j , . . ., u ni )}, but in time series u j is numerical.Because language models are built to represent complex probability distributions over sequences, they are theoretically well-suited for time series modeling.In practice, however, language models are held back by the details of tokenizing numbers.BPE compresses numbers based on frequency of occurrence in the training data, so numbers can be broken down into awkward chunks that make learning basic numerical operations challenging.Touvron et al. [44] therefore designed the LLaMA tokenizer to map numbers to individual digits, which can lead to significant improvements in mathematical abilities, with small LLaMA models outperforming GPT-4 [31].</p>
<p>The other challenge of applying language models to time series data is proper evaluation.Mean absolute error (MAE) is commonly used but ignores uncertainty in the forecast which is highly limiting for stochastic data [25,6].Continuous ranked probability score (CRPS) captures distributional qualities and can compare models that generate samples without likelihoods.For a single prediction, the CRPS score is defined against the estimated cumulative distribution function (CDF), F as
CRPS( F , y) = R F (z) − I (z−y)&gt;02
dz, where F (z) is the empirical CDF produced by sampling forecasts and I is the indicator function.While CRPS is an improvement over MAE, it also ignores key structures in the data, such as correlations between time steps.Fortunately, language models can assign likelihoods to full sequences of time series data, and we show how a small modification to an LLM's discrete likelihood can yield a continuous density that is useful for model comparison.</p>
<p>Language models for time series Several authors have explored using pretrained language model encoders as initializations for time series models.For example, Zhou et al. [57] propose FPT, which finetunes a BERT encoder to perform time series forecasting.Similarly, Zhang et al. [54] introduce Meta-Transformer, a framework for finetuning a language model for non-text modalities, including time series.Fewer papers explore using LLMs as forecasters without finetuning.The only method we are aware of is PromptCast [51], which poses forecasting as question answering with prompting.</p>
<p>Our work Unlike methods that leverage LLM backbones, our method is entirely zero-shot and does not require finetuning.Unlike PromptCast, we show that LLMs can be used directly as forecasters without any added text or prompt engineering, if we carefully preprocess the numerical values themselves.Our method solely relies on LLM's abilities to extrapolate patterns in general sequences and nothing particular to English or any other language.Going beyond prior work, we also cultivate the probabilistic nature of large language models and their ability to capture uncertainty over highly stochastic time series.[24], with values [151, 167, ..., 267], we show the tokenization used by GPT-3 [9] and LLaMA-2 [45] and the corresponding effect on forecasting performance.Added spaces allow GPT-3 to create one token per digit, leading to good performance.LLaMA-2, on the other hand, tokenizes digits individually, and adding spaces hurts performance.</p>
<p>LLMTIME: Forecasting with Language Models</p>
<p>Forecasting with LLMTIME has relatively few steps.Once the numerical values are processed into strings, making predictions with the language model follows standard sampling procedures.As we show next, however, correct pre-processing is not always intuitive but is extremely important, and incorrect handling can lead to unusable predictions.</p>
<p>Tokenization Tokenization is particularly important because it directly influences how patterns form within tokenized sequences and the types of operations that language models can learn.Unfortunately, common tokenization methods like BPE tend to break a single number into tokens that don't align with the digits, which can make arithmetic considerably more difficult [31].For example, the number 42235630 gets tokenized as [422, 35,630] by the GPT-3 tokenizer, and changes by even a single digit can result in an entirely different tokenization.By contrast, in many new open-source LLMs (e.g.LLaMA [44]), numbers are tokenized into individual digits by default.To remedy the tokenization of GPT models, we separate the digits with spaces to force a separate tokenization of each digit and use a comma (" ,") to separate each time step in a time series.Because decimal points are redundant given a fixed precision, we drop them in the encoding to save on context length.Thus, with e.g. 2 digits of precision, we pre-process a time series as follows before feeding into the tokenizer: 0.123, 1.23, 12.3, 123.0 → " 1 2 , 1 2 3 , 1 2 3 0 , 1 2 3 0 0".</p>
<p>In Figure 2, we show that the added spaces of this encoding are helpful for GPT models, preventing the model from getting derailed by outputting an unusual token during sampling.For LLaMA models, with their unique tokenization of numbers, added spaces have the opposite effect.Each digit and space is already assigned its own token, and space tokens become nuisance inputs, adding to the sequence length without simplifying the sequence's structure and potentially making the sequence out-of-distribution to the model.</p>
<p>Rescaling To avoid wasting tokens when the inputs are very large, we scale values down so that the α-percentile of rescaled time series values is 1.We avoid scaling by the maximum value so that the LLM can see some fraction of examples (1 − α) where the number of digits changes and reproduce this behavior in its outputs to produce larger values than it has seen.We also experiment with an offset β based calculate as a percentile of the input data, and we tune these two parameters on validation log likelihoods (details in Appendix A).</p>
<p>Sampling / Forecasting To forecast, draw many samples (e.g.20) from the LLM and use the statistics of the samples at each time step to construct a point estimate (e.g. as the median) or probabilistic forecast (e.g. as quantiles).To control sampling, we use temperature scaling, logit bias, and nucleus sampling (Appendix C).</p>
<p>Continuous likelihoods Modeling sequences of individual digits has additional benefits beyond good samples.With n digits of precision in base B, each sequence of n digits after the decimal place corresponds to one of B n possible bins (Figure 3), each with width B −n .As each distribution p(u j | u 0:j−1 ; θ) is a softmax over possible digits, we can view the distribution over each individual number as a hierarchical softmax [32], with p(u 1 , ..., u n ) = p(u n |u n−1 , ..., u 0 ) p(u 1 |u 0 ) p(u 0 ).Though a language model's probability distribution is discrete, we can easily adapt it to provide a P(u j | u 0:j-1 ) P("3"|"5") P("3"|"5") P("7"|"53") P("7"|"53") P("5") P("5") ["5","3","  Autoregressive models over sequences of digits act like hierarchical softmax distributions over the corresponding numbers.When combined with uniform distributions in each discrete bucket, distributions over strings can become expressive distributions over continuous domains.Right: Using simple autoregressive models (e.g.RNNs) trained on a string representation of numbers, we can fit complex distributions that can be challenging for other methods, such as heavy-tailed or multimodal distributions.A simple autoregressive model can match or outperform well-known methods for density estimation, such as Gaussian mixture models (GMMs) or binning with a fixed resolution, as measured by Wasserstein distance between samples.continuous density by placing a uniform distribution in each bin.Enumerating each of the countably infinite numbers that the model can produce (because the model can output an arbitrary number of digits before the decimal point) with an index k ∈ N each with probability p k , we can write out the distribution as a mixture of disjoint uniform distributions over the bins p(x)
= k∈N p k U k (x) where U k (x) = B n I x∈[B −n k,B −n (k+1))
. Therefore if a given data point lies in bin k, its continuous log likelihood is log p(x) = log p k + n log B. Finally, to obtain the likelihood log p(z) in the original input space, we add a change of variables factor log | dx dz |, where z → x = s(z) is the rescaling operation in the pre-processing.As a result, the exponentially large number of bins and exponentially small bin widths enabled by our construction make it surprisingly efficient to represent flexible and high-resolution continuous distributions with LLMs, despite using a discrete tokenization of numbers.</p>
<p>Language models as flexible distributions</p>
<p>The fact that LLMs can express flexible distributions over numbers is key for time series data.Uncertainty quantification is essential to forecasting, and typical approaches to representing uncertainty in time series can be limited by misspecification.For example, one common method for creating a probabilistic forecast is to fit a Gaussian or Laplace observation model.When the underlying data distribution is multimodal, both of these models will perform poorly.Methods like Gaussian mixture models (GMMs) solve the issue of multimodality but introduce additional challenges to optimization and model selection.We show that a language model is an underrated solution by training a small autoregressive model on a variety of one-dimensional distributions shown in Figure 3 (right).These distributions come from an exponential random variable, a mixture of a uniform and a student-t distribution, and the heavy-tailed distribution of time series prediction residuals from an ARIMA model on the MonthlyMilk dataset [24].We evaluate these fits quantitatively by computing Wasserstein distances, and compare to a Laplace observation model, a GMM trained with expectation-maximization, and logistic regression over a flat binning of the data (with a tuned bin size).Each model is trained with only 200 samples from the distribution.The results show that the decimal autoregressive language model ("Decimal AR") performs extremely well, handling asymmetric, multimodal, and heavy-tailed distributions, which are among the diverse types characteristic of time series data.</p>
<p>Experiments</p>
<p>We evaluate the zero-shot forecasting ability of LLMs by comparing LLMTIME with GPT-3 and LLaMA-2 70B to many popular time series baselines on a variety of benchmark time series datasets.Not only is LLMTIME able to generate plausible completions of the real and synthetic time series, it achieves higher likelihoods and CRPS values in zero-shot evaluation than the dedicated time series models like ARIMA, TCNs, and N-HiTS.When evaluated on deterministic metrics like MAE, LLMs also perform well, obtain the best or second best MAE values on each benchmark.As we are using  LLMs with undisclosed datasets, data leakage is an important concern that we address directly in Appendix B. Beyond strong performance on standard benchmarks, which are the most useful for comparison, we find that LLMTIME also performs well on datasets that could not have been present in the base model's training data.The full set of hyperparameters used for LLMTIME and the baseline methods are detailed in Appendix C.1.For some of the longer time series, not all of the history can be fit into the context window, and hence hyperparameters implicitly capture the trade-off between higher precision and capturing a larger temporal history.
N -B E A T S S M -G P T C N N -H i T S L L a M A -2 G P T -3 A R I M A 0.0 0.2 MAE Darts W a v e N e
Datasets We use three benchmark datasets that are common within deep learning research and many baseline methods that accompany the benchmark datasets.</p>
<p>• Darts [24]: A collection of 8 real univariate time series datasets.For Darts, we use several methods that are implemented directly in the package, including neural network models (TCN [29], N-BEATS [38], N-HiTS [11]) and simple moving average models (ARIMA [8]).Darts enables learning observation models with tractable likelihoods and is therefore especially useful for benchmarking the probabilistic predictions of LLMTIME.We also include Spectral Mixture Gaussian Process (SM-GP) [49], a Bayesian nonparametric method (details in Appendix C.1).</p>
<p>• Monash [19]: The Monash forecasting archive contains 30 publicly available datasets along with baseline numbers for 12 forecasting models, including simple exponential smooth (e.g.ETS [27]), gradient boosting (e.g.CatBoost [40]) and deep learning models (e.g.DeepAR [41], WaveNet [36]).The Monash archive comprises over 400,000 individual time series, making it infeasible to use in its entirety with the largest available LLMs.To reduce the computational burden, we evaluate GPT-3's zero-shot performance on 19 datasets described in Appendix C.2.</p>
<p>• Informer [55]: We evaluated on multivariate datasets widely used for benchmarking efficient transformer models [16,55].In order to predict multivariate data with LLMTIME, we forecast each covariate independently.We baseline against numbers obtained by running public implementations from the Autoformer [50] and FEDFormer [56] codebases (Appendix C.3).</p>
<p>Deterministic results</p>
<p>To compute MAE values for LLMTIME we use the pointwise median of 20 samples from the base model (GPT-2 or LLaMA-2 70B). Figure 4 shows that deterministic predictions from LLMTIME are ranked best or second best on all the considered benchmarks while having no trainable parameters.We provide visualizations of the forecasts in Appendix C.5/C.7/C.8.</p>
<p>Probabilistic results</p>
<p>In Figure 5, we show several probabilistic evaluations on the Darts datasets, including aggregated NLL and CRPS numbers, as well as analysis of how each model reacts to decreasing the input data size.Evaluated on log likelihood and CRPS, LLMTIME considerably outperforms the baselines in aggregate and on almost every individual dataset (results per dataset included in Appendix C.5).Given the analysis of language model-derived densities in Section 3, it is unsurprising that language models excel in probabilistic evaluations, outperforming the baselines even  more dramatically.In Figure 5 (left) we show two informative examples that capture the performance of LLMTIME.When extrapolating the AirPassengers dataset, LLMTIME successfully identifies and continues trend and period components, with uncertainty that grows as predictions get further from the input data.On GasRateCO2, LLMTIME replicates local structure when there is relatively little global structure.In Figure 5 (right) we show that LLMTIME not only performs better than baselines with access to the full training data but also when restricted to small fractions of the training data.As time series is frequently characterized by relative data scarcity and challenges in transfer learning, the data efficiency of LLMs is especially attractive.</p>
<p>Comparison with PromptCast Though included in the results described above, we want to explicitly highlight that LLMTIME significantly outperforms PromptCast [51] when applied to both GPT-3 and LLaMA-2 70B, according to CRPS and MAE aggregated over the Darts datasets.This performance gap highlights important differences between the two approaches.Unlike our method, PromptCast formulates forecasting as a conventional question-answering task in NLP by prompting pre-trained language models with an explicit question about future values in a time series.For example, PromptCast feeds in the prompt "The values in the WoolyDataset for the past 95 time steps are 6172, 6709, 6633, . . ., 6077.What will the values for the next 5 time steps be?The values for the next 5 time steps will be", to extract predictions from an LLM.PromptCast also does not apply our tokenization and data rescaling strategy (Section 3), which we show is crucial for good performance.</p>
<p>Origins of Zero-Shot Performance</p>
<p>To understand why LLMs can extrapolate time series in a zero-shot manner, let's take a step back and consider simple numerical sequences, for example [1, 4, 9, 16, . . .] or [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, . . .].</p>
<p>For any input sequence, there are arbitrarily many generation rules that are consistent with the input (e.g.f (x) = x 2 for x ∈ [1, 2, 3, 4, ...]), but some generation rules are overly complex and will generalize poorly.LLMs can forecast effectively because they prefer completions derived from simple rules, adopting a form of Occam's razor prior [20,43,15].To explicitly demonstrate this phenomenon, we create a synthetic example using the function f (x) = x + cos(x) with additive Gaussian noise.We fit symbolic expressions to the first 70% of timesteps using PySR [14] with symbols ["+", "•", "-", "/", "sin", "cos", "exp","square"] to identify generating rules with known complexity, quantified by the number of symbols in the regressed expression (Appendix D). Figure 6 (left) shows the likelihood that GPT-3 assigns the highest likelihood to symbolic regression generating rules that balance consistency with complexity.</p>
<p>In Figure 6 (right) we show how program induction in LLMs leads to good zero-shot prediction for many deterministic patterns common in time series data.Along with samples, we also show  likelihoods, comparing against standard time series models, which often struggle to extrapolate these simple patterns because they cannot identify a programmatic generation rule to make predictions unlike those seen in the observed history.While the generic simplicity bias is helpful for identifying and extrapolating patterns in the input, a number of patterns common in time series models also translate directly to known capabilities of language models, for example</p>
<p>• Repetition bias and periodicity: LLMs' bias towards repetitive sequences [26] (often unwanted in NLP) corresponds precisely to the ability to identify and extrapolate periodic structure in the input.4.2, 8.6, 1.0, 4.2, 8.6 will lead to a 1.0 as a likely next output without any time series or arithmetic knowledge (x t = x t−T ).</p>
<p>• Arithmetic and trend components: LLMs' ability to perform addition and multiplication [52,31] maps on to extrapolating linear and exponential trends.For example, predicting the next element of 0.2, 1.6, 3., 4.4 the LLM needs only to add 1.4 to the last element (x t+1 = x t + c).</p>
<p>Similarly, exponential trends have the generating rule x t+1 = c • x t and sigmoid trends have the generating rule
x t+1 = x t + cx t (1 − x t ).
Combining multiple patterns together presents a more difficult challenge, as it requires both identifying the composite pattern and being able to perform the multiple operations within the same token budget.Supposing that a model can perform copying in a single forward pass and addition in a single forward pass, that does not necessarily mean that it can do both simultaneously.We find that GPT-3 is only sometimes able to perform these compositions, though GPT-4 does so more consistently as shown in Appendix E. It is likely that the limitations on compute and tokens spent may make this composition unnecessarily hard, and that additional recursive structure, for example from a scratchpad [34], Chain of Thought (CoT) prompting [48], or adaptive computation [42,3], would make this task easier.</p>
<p>Special Properties of LLMs</p>
<p>So far we've shown that LLMs are effective forecasters across a variety of datasets and that their forecasting ability arises from biases created by generative pretraining.LLMTIME offers a mechanism for large-scale pre-training that is uncommon in machine learning for time series.LLMs lessen the amount of time series data that must be aggregated for pretraining, substituting text pretraining in its place, and enable more powerful scaling results.Beyond escaping the limits of task-specific data, text pretraining also has many test-time benefits that stem from the base model's ability to process and generate natural language.As we show in the following section, LLMs can leverage their abilities in order to seamlessly incorporate missing data or answer questions about time series.</p>
<p>CRPS</p>
<p>Base Chat</p>
<p>Figure 7: Left: Time series forecasting performance (NLL/D and CRPS on Darts [24]) improves with reasoning performance of the underlying model LLM, as judged by accuracy on the Massive Multitask Language Understanding (MMLU) benchmark [23].Displayed results are for all GPT-3, LLaMA [44], and LLaMA-2 [45] base models.Center: GPT-4 performs worse than GPT-3.Right: Forecasting performance (NLL/D and CRPS on Darts) appears to be negatively affected by alignment procedures (e.g.instruction tuning and RLHF) in general.LLaMA-2 chat models typically perform worse than the corresponding base model.Error bars show standard errors over individual datasets.</p>
<p>Base models and forecasting performance Given the rapid growth and improvement in opensource LLMs [44,45], the relationship between LLMTIME forecasting performance and the performance of the underlying base model is particular important and has broad implications.Steady increases in LLM benchmark performance can directly translate to steady improvements in forecasting ability.In Figure 7 (right), we show a study with OpenAI models (davinci, babbage, curie, ada), variants of LLaMA [44] (7B, 13B, 33B, 65B) and LLaMA-2 [45] models (7B, 13B, 70B) measuring accuracy on the Massive Multitask Language Understanding (MMLU) benchmark and probabilistic forecasting error.As we might hope, when reasoning (MMLU) performance increases forecasts also improve.</p>
<p>Chat models Though convenient scaling relationships appear to hold for base models, the correlation begins to break down when we consider models that have been post-processed for chatbot applications.GPT-4 [37], for example, demonstrates considerably more intelligence than GPT-3 and LLaMA models in natural language tasks, but effectively applying it to time series is challenging.In Figure 7 (center), we show that GPT-4 has a forecasting error (CRPS) significantly larger than GPT-3's on the Darts datasets.The performance drop is the result of several small details in GPT-4's method.Due to the altered tokenization, GPT-4 cannot be easily forced to tokenize individual digits into an unbroken stream of numbers.Due to the restrictions on the API, likelihood evaluations are also not permitted, which is why we present results for only CRPS.While GPT-4 can perform well on the synthetic examples discussed in Section 5 (shown in Appendix E), we find that the GPT-4 calibration is much worse than GPT-3's on stochastic data, likely as a result of the preprocessing details above and the fact that the model has been treated with RLHF [13] which is known to degrade calibration on question-answering tasks [37].GPT-4 is not the only example of degraded performance in models designed for chat functionality.We observed the same phenomenon in LLaMA-2 models, which have corresponding chat versions for every model size.Figure 7 (right) shows that chat versions tend to have markedly worse forecasting error than their non-chat counterparts, though still maintain trends in size and reasoning ability.</p>
<p>Missing data A major advantage of treating forecasting as a text completion task and using LLMs is that we can easily feed in any input that can be encoded as text.Often in time series, the time series will be incomplete and certain observations are missing.Simple imputation methods, such as nearest neighbor, are still core pre-processing steps in common data science workflows [33], and the choice of imputation method is especially relevant to clinical data, which often contains irregularly sampled measurements and where missingness can be meaningful signal in itself [28]  For baseline methods we perform linear interpolation and then fit the model as usual.LLMTIME assigns higher log likelihood to datasets preprocessed with added 'NaN's than baseline methods assign to interpolated datasets.Forecasting performance, as judged by CRPS, is competitive between LLMTIME and alternative methods that use explicit interpolation.Filled area shows standard error over individual datasets and 3 random seeds.Right: LLMs can be used to answer questions about time series data posed as text.We show GPT-4's accuracy at predicting the function that generated the time series, obtained using chain-of-thought prompting.</p>
<p>Connecting time series and textual understanding Because LLMs are designed for natural language and code, we can augment the numerical time series with useful text.We can do so either by providing textual side information as inputs, or by producing textual outputs from a given time series.An interesting question is whether GPT-4 can explain in text its understanding of a given time series.We probe this quality by providing GPT-4 the code to generate our synthetic time series, provide the values of one these time series, and then ask it to infer which of the functions produced the data in a zero-shot manner.The prediction accuracies are shown in Figure 8, with the three remaining rows all being 0. With CoT [48] prompting the model performs much better than random chance; however, its ability to identify patterns better when directly extrapolating the numerical data, suggesting that its numerical understanding is not fully connected to its textual understanding.In making predictions, the model often explains properties of the time series in order to select the right candidate from the list, and we show several of these sample explanations in Appendix F. We also show how this task is encapsulated in a simple (unprompted) next token prediction problem on cells of a Jupyter notebook, illustrating why we expect such capabilities to emerge with a sufficiently powerful language model.</p>
<p>Discussion</p>
<p>We have demonstrated that large language models can be used as pretrained time series forecasters by encoding numerical values as text.As with other "foundation" models, pretraining confers useful biases toward generalizable patterns that would otherwise be engineered into the model through architecture design [22], and enables natural scaling of performance with improvements in the base pretrained model.Because LLM forecasters are trained on language, they also confer unconventional capabilities, such as question answering.More broadly, framing time series forecasting as natural language generation can be seen as another step towards unifying more capabilities within a single large and powerful model, in which understanding can be shared between many tasks and modalities.Moreover, zero-shot forecasting can enable broadly compelling performance without requiring significant computational resources, domain expertise, or many downstream training data points.</p>
<p>While LLM forecasters benefit from the strengths of pretrained transformers, they also inherit their weaknesses, which can include a limited context window.While many univariate time series problems can fit comfortably within increasingly large context windows, multivariate problems pose a more significant challenge.There have been several recent advances extending LLM context windows to 10-100K tokens [37,4,5,1].Combining these advances with time series forecasting is a particularly exciting direction for future research.Another potential challenge of using current LLMs architectures could be their weakness in arithmetic and performing recursive and compositional operations, which could be a limitation on particularly challenging time series.On the other hand, many time series do not require precise arithmetic.Understanding the extent to which this is the case, and relaxing this limitation, is also a promising avenue for future research.Separately from any limitation, it would also be promising to investigate effective procedures for fine-tuning LLMs on time series.We hope that bridging LLM research with time series forecasting brings benefits to both communities.We compare the performance of our GPT-3 predictor against popular time series models.</p>
<p>Predicted median and 10-90th percentile intervals are shown for GPT-3 given the context, and we compare test negative log likelihoods.GPT-3 continues to be competitive with or outperforms the baselines on all of the tasks, from in-context learning alone.This result reinforces our belief that GPT-3's performance is not due to memorization of the test data.</p>
<p>the numerical encoding scheme.When we have access to the language model's logits, performing this adjustment is easy.We can simply set the probability of any non-essential tokens to zero and renormalize the distribution.When using black-box APIs (e.g. the OpenAI API), however, we need to approximate this procedure, becauase it frequently impossible to obtain the full discrete distribution over tokens.For example, in the OpenAI API, only the top 5 log probabilities are returned for every step in the generation process.As we cannot properly renormalize the distribution, we have to make adjustments that get us as close as possible to the true renormalized distribution, as we show in the following calculations.We take p to be the raw probabilities assigned by a language model and p to be the adjusted probabilities, with
p k = e log p k i e log pi , pk = e log p k ( i e log pi ) − e l0
where l 0 is the probability that mass assigned to tokens that are not part of the numerical encoding scheme.When access to log probabilities is limited, l 0 can be approximated as the sum of the log probabilities for non-numerical tokens in the top k.From the definition of p we can derive
1 p = e l0 + 1 p =⇒ p = 1 p − e l0 −1 =⇒ log p = log p − log(1 − e l0 )</p>
<p>B Addressing Memorization Concerns in GPT-3 Evaluations</p>
<p>Evaluating the performance of black box APIs, like those provided by OpenAI, can be challenging when training data for the underlying models is unknown.In our time series setting, it is natural to wonder if the common benchmark datasets we use are included in the GPT-3 training data.LLMs are known to memorize large amounts of their training data verbatim, including common benchmark text datasets and copyrighted material [7,12].Beyond outright memorization, more benign data leakage of closely related data is also possible, leading to overestimation of the generalization performance.</p>
<p>Even if our evaluation datasets are present in the GPT-3 training data, it's unlikely that GPT-3's good performance is the result of memorization for at least two reasons a priori.First of all, our idiosyncratic formatting is unlikely to be present in the training dataset, even if the numerical values and their order are the same.Second, the time series datasets are unlikely to appear in GPT-3's training data sufficiently frequently to lead to memorization, as memorization increases in proportion with redundancy [30].</p>
<p>To further address the memorization concern, we also perform a direct experiment to show GPT-3 also demonstrates strong performance when evaluated on time series recorded after its training data cutoff date, September 2021.We use the following 3 time series:</p>
<p>• Istanbul Traffic (source: https://www.kaggle.com/datasets/leonardo00/istanbul-traffic-index):</p>
<p>This dataset provides minute-by-minute Traffic Index data for Istanbul from October 2022 to May 2023.We select the "TI" column and downsample the series to an hourly frequency for the period from May 5th, 2023 to May 18th, 2023, resulting in a total of 267 observations.</p>
<p>• TSMC Stock (source: https://www.kaggle.com/datasets/yeemeitsang/tsmc-stock-exchange-2022):This dataset contains daily stock market trading data for Taiwan Semiconductor Manufacturing Company Limited for the year 2022.We use the closing price column, which consists of a total of 246 observations.</p>
<p>• Turkey Power (source: https://www.kaggle.com/datasets/dharanikra/electrical-power-demandin-turkey):This dataset includes hourly electricity generation and consumption data for Turkey from January 1st, 2020 to December 31st, 2022.We choose the "Total" column and downsample to daily data for the year 2022, resulting in 366 observations.</p>
<p>For each time series, we reserve the last 30 observations as test data and perform hyperparameter tuning for each method over the same grid as in Appendix C.1.As displayed in Figure 9, GPT-3 not only predicts plausible continuations of each time series but also competes with or even surpasses the performance of the baseline models in all the tasks, solely based on in-context learning.This result reinforces our belief that GPT-3's performance is not due to memorization of the test data.</p>
<p>C Benchmarking details and extended results</p>
<p>C.1 Darts datasets</p>
<p>For the Darts datasets, we use the GPyTorch library [17] for Gaussian Process implementation and the Darts libary [24] for ARIMA, TCN, N-BEATS, N-HiTS.We use default values for hyperparameters not described below.The test set is the last 20% of each series.</p>
<p>We use several baseline methods implemented directly in Darts [24]:</p>
<p>• ARIMA: ARIMA [8], short for AutoRegressive Integrated Moving Average, has been a popular choice for time series forecasting for many decades.ARIMA in Darts wraps code from [18].</p>
<p>• TCN: Temporal Convolutional Network (TCN) [29] is residual network with dilated 1D convolutions.</p>
<p>• N-BEATS: N-BEATS [38] is a deep learning model tailored for time series forecasting.It employs a deep architecture with backward and forward residual links and stacked fullyconnected layers.</p>
<p>• N-HiTS: N-HiTS [11] is a deep learning model that incorporates hierarchical interpolation and multi-rate data sampling techniques in order to create forecasts that emphasize different frequencies and scales of the input signal.</p>
<p>We also include Spectral Mixture Gaussian Process (SM-GP) [49] as a Bayesian nonparametric approach to time series modeling.</p>
<p>We include the exact hyperparameters for each method below:</p>
<p>GPT-3 We perform a grid search over α ∈ [0.5, .7,0.9, 0.99], β ∈ [0, .15,0.3, .5],precision (number of decimals) ∈ [2, 3], and temperature = 0.7.</p>
<p>GPT-4 Since likelihood evaluation is not available for GPT-4, we fix its hyperparameters for all datasets as follows: we use the basic scaler with α = 0.3 and temperature = 1.0 with top p = 0.8.We do not insert spaces between digits for GPT-4 since it uses a different tokenizer than GPT-3 for which this strategy is not effective.</p>
<p>LLaMA For models LLaMA-1 (7B/13B/30B/70B) and LLaMA-2 (7B/7B-chat/13B/13B-chat), we perform a grid search over temperature ∈ [0.2, 0.4, 0.6, 0.8] and use α = 0.99, β = 0.3, precision = 3, nucleus = 0.9.For LLaMA-2 70B and LLaMA-2 70B-chat we use temperature = 1.0, α = 0.99, β = 0.3, precision = 3, nucleus = 0.9. ) for every dataset within Darts [24].The shaded area shows the 10th to 90th quantiles of the distribution over samples.LLMTIME consistently obtains better likelihood values than the baselines and often makes surprisingly accurate forecasts by effectively extrapolating trend and periodic components.</p>
<p>of 96 or 192: "ETTm2", "exchange_rate", "electricity", "traffic", and "weather".The results provided in the main text are for a prediction horizon of 96, and we include results for prediction horizon 192 in Appendix C.6.To make evaluation tractable with LLMTIME, we use a smaller evaluation set for each dataset, taking the last 96 or 192 timesteps of each series within each dataset as the test set.As there are many individual series in each multivariate dataset, the number of individual timesteps in the test sets is still substantial.To forecast multivariate series with LLMTIME we simply forecast over each series independently, combine the results, and evaluate as in prior work.Our efficient transformer baselines include</p>
<p>• Informer: Informer [55] is an efficient transformer model with sparse attention designed for long sequences.</p>
<p>• Reformer: Reformer [55] uses a locality-sensitive hashing mechanism to improve the memory use of attention.</p>
<p>• Autoformer: Autoformer [50] is a model design for long time series that replaces standard attention with a mechanism in Fourier space.</p>
<p>• FEDformer: Like Autoformer, FEDformer [56] uses frequency-based decompositions to construct an efficient alternative to attention.</p>
<p>LLaMA-2 70B hyperparameters We use LLaMA-2 70B with α = 0.99, β = 0.3, temperature = 1.0, nucleus = 0.9, precision = 3.The series in the Informer datasets are very long and we put as much as possible in the LLaMA-2 context window (4096).</p>
<p>C.4 Synthetic datasets</p>
<p>For the baselines, we use the same hyperparameter grid in Section C.1.For GPT-3, we didn't find it useful to perform validation tuning.We use the basic scaler with α = 0.1 and temperature = 0.7.</p>
<p>C.5 Darts full probabilistic results</p>
<p>In Figure 10 we show the predicted NLLs and forecasts from LLMTIME using GPT-3 and LLaMA-2 70B as base models.LLMTIME typically obtains much better likelihoods than baselines and successfully identifies trend and seasonal components in the time series.We attribute this strong performance in part to the fact that the time series are relatively short.With the tokenization of the input, only about 300 of the observations can fit into the context window, and among the datasets only Sunspots and HeartRate exceed this amount (with 705 and 900 observations respectively).Overall LLMTIME performs well in aggregate for a zero-shot method, but its performance is highly variable, being the best method on some datasets and the worst on others.The relative performance of LLMTIME is slightly diminished for a longer prediction horizon, but LLMTIME is still very competitive with the best methods in aggregate.Error bars show two standard deviations in the error over datasets. of 192.We observed a similar trend overall, though the relative performance of LLMTIME is slightly diminished, largely due to the "electricity" and "traffic" datasets.</p>
<p>C.6 Informer datasets with extended horizon</p>
<p>C.7 Monash dataset visualizations</p>
<p>Figure 12 shows visualizations of the LLMTIME's median predictions (GPT-3 base model) on a subset of the Monash datasets.</p>
<p>C.8 Informer dataset visualizations</p>
<p>Figure 13 shows visualizations of the LLMTIME's median predictions (LLaMA-2 70B base model) on the Informer datasets, for a subset of the each set of multivariate series.</p>
<p>D Simplicity bias experiments</p>
<p>We generate data from the function f (x) = cos(x) + x and add Gaussian noise with zero mean and variance 0.05.We fit symbolic expressions to the first 140 timesteps using PySR [14] with symbols ["+", "•", "-", "/", "sin", "cos", "exp","square"] and maxsize = 70, maxdepth = 10, population_size = 50, loss = abs(prediction − target), model_selection = accuracy and niterations = 100.The solutions are saved and ranked by complexity, which is simply the number of terms in the symbolic regression function.The five solutions shown in Figure 6  To obtain likelihoods we run GPT-3 ('text-davinci-003') with alpha = 0.99, beta = 0.3, basic = True, precision = 1, signed = True.</p>
<p>D.1 Full synthetic predictions</p>
<p>Figure 14 shows likelihoods and forecasts from LLMTIME with GPT-3 on the full set of synthetic datasets.We see that some compositional tasks like Linear + Cosine are challenging, while others    Figure 16: GPT-4 extrapolations on real (DARTS) time series (10-90th percentiles shaded).The extrapolations are plausible but worse than GPT-3, and the uncertainties tend to be more poorly calibrated making for a high CRPS.</p>
<p>E GPT-4</p>
<p>We investigated using GPT-4 for time series prediction.Due to the limitations of the tokenizer, we used the naive tokenization strategy of feeding in the numbers without additional spaces.In addition, due to the enforced separation between system and user in the interface (through additional tokens we cannot modify), inputting the time series input alone leads GPT-4 to talk about the time series or provide analysis, rather than simply continuing the stream of numbers.In order to coax GPT-4 to produce numerical predictions which can be decoded, we added the additional commands System: "You are a helpful assistant that performs time series predictions.The user will provide a sequence and you will predict the remaining sequence.The sequence is represented by decimal strings separated by commas."User: "Please continue the following sequence without producing any additional text.Do not say anything like 'the next terms in the sequence are', just return the numbers.Sequence:".We found that doing so was sufficient to be able to consistently decode the output numerically for GPT-4, but not for GPT-3.5-turbo.</p>
<p>We show predictions on the synthetic benchmarks (from Figure 6) in Figure 15.As one can observe, GPT-4 is considerably better performing on these synthetic benchmarks, although numerical decoding of the model sometimes fails before the full output.With non-deterministic time series problems such as with the DARTS datasets, the predictions are slightly worse than GPT-3, but the uncertainties are much less well calibrated as shown in Figure 16.</p>
<p>F Multimodal Text Understanding of Time Series</p>
<p>We evaluate the ability of the language model to reason about the input time series through text in a zero-shot fashion.To test this, we devise a simple experiment where we generate a synthetic time series from one of several candidate functions.We provide the generation code and the numerical values to GPT-4 (Listing 1), but because of the randomness, GPT-4 must infer which of the functions generated the values.We note that as this code could easily be found within a Jupyter notebook on the internet without intentionally being designed as an experiment for LLMs, we should expect that this textual time series identification task will fall within the data distribution, and in principle should be solved given sufficient capabilities of the language model.</p>
<p>To make the problem slightly easier, we add an additional guiding prompt before and after the text in Listing 1.We prepend "The following is code that was run to generate a synthetic time series.From the input and output you will be asked to identify which of the time series was picked to generate the data."</p>
<p>to the code, and after the time series we append either Listing 1: Self-contained code presented to the model for the multimodal time series identification task.When the code is run one of the listed functions is randomly chosen to generate the time series.</p>
<p>In order to simply predict the next token after observing this text (which could be found in the cells of a Jupyter notebook) the model must infer which of the functions produced the series.</p>
<p>1 First , let ' s analyze the time series and make some observations : 2 3 1.The time series starts at a negative value , increases linearly , and ends at a positive value .4 2.There is no apparent periodicity or oscillation in the time series .5 3.There are no sudden jumps or changes in the time series , indicating a smooth function .9 -' gaussian_wave ': This function has a periodic oscillation , which doesn ' t match our observations .10 -'exp ': This function increases exponentially , which doesn ' t match the linear increase in our observations .11 -' linear_cos ': This function has a periodic oscillation , which doesn ' t match our observations .12 -' linear ': This function increases linearly , which matches our observations .13 -' sine ': This function has a periodic oscillation , which doesn ' t match our observations .14 -' sinc ': This function has a periodic oscillation , which doesn ' t match our observations .15 -' beat ': This function has a periodic oscillation , which doesn ' t match our observations .16 -' sigmoid ': This function has a sigmoid shape , which doesn ' t match our observations .17 -'log ': This function increases logarithmically , which doesn ' t match the linear increase in our observations .18 -' x_times_sine ': This function has a periodic oscillation , which doesn ' t match our observations .19 -' square ': This function has a parabolic shape , which doesn ' t match our observations .20 21 Based on our observations and analysis , the function that most likely generated the time series is ' linear '.The time series starts with a value of 0.148 and ends with a value of 8.063.This indicates that the series is monotonically increasing .Now , let ' s look at the differences between consecutive values .The differences are small and seem to be increasing as the series progresses .This suggests that the series is not only increasing but also accelerating .</p>
<p>Figure 2 :
2
Figure 2: Careful tokenization is important for good forecasting with LLMs.Using the Australian Wine dataset from Darts[24], with values [151, 167, ..., 267], we show the tokenization used by GPT-3[9]  and LLaMA-2[45] and the corresponding effect on forecasting performance.Added spaces allow GPT-3 to create one token per digit, leading to good performance.LLaMA-2, on the other hand, tokenizes digits individually, and adding spaces hurts performance.</p>
<p>Figure 3 :
3
Figure3: Left: Autoregressive models over sequences of digits act like hierarchical softmax distributions over the corresponding numbers.When combined with uniform distributions in each discrete bucket, distributions over strings can become expressive distributions over continuous domains.Right: Using simple autoregressive models (e.g.RNNs) trained on a string representation of numbers, we can fit complex distributions that can be challenging for other methods, such as heavy-tailed or multimodal distributions.A simple autoregressive model can match or outperform well-known methods for density estimation, such as Gaussian mixture models (GMMs) or binning with a fixed resolution, as measured by Wasserstein distance between samples.</p>
<p>Figure 4 :
4
Figure4: LLMTIME with base model GPT-3 or LLaMA-2 70B has the best or second best aggregated performance on several deterministic time series benchmarks[24, 19,55] while being entirely zeroshot.Collectively, these benchmarks comprise 29 individual datasets with diverse sources, lengths, and noise levels.For Monash MAE numbers, established results are reported on unnormalized data, so we normalize values before aggregating (Appendix C.2).The informer datasets are multivariate, and we predict each covariate independently with LLMTIME (Appendix C.3). GPT-3 evaluation on the Informer datasets was skipped because of the cost of API queries.Error bars show standard errors over the individual datasets in each benchmark.</p>
<p>Figure 5 :
5
Figure 5: Extended experiments on the Darts datasets.Left: Example probabilistic forecasts with baseline negative log likelihood per dimension (NLL/D).LLMs easily extrapolate trends (e.g.AirPassengers) and reproduce local patterns when data is noisy (e.g.GasRateCO2).Center: When using probabilistic metrics like NLL and CRPS, LLMTIME outperforms all baselines, including PromptCast [51], a competing LLM method.Error bars show standard errors over datasets with Darts.Right: LLMTIME is much more sample efficient than competing methods.While the performance of other methods degrades rapidly when we restrict them to a fraction of the original training set, LLMTIME can assign high likelihood with only a few examples.</p>
<p>.</p>
<p>Figure 6 :
6
Figure6: LLMs can find low complexity explanations of the data, enabling them to zero-shot extrapolate numerical sequences.Left: GPT-3 likelihoods favor solutions from symbolic regression (PySR[14]) that balance training loss and complexity, leading to good generalization.Right: GPT-3 predicted median and 10-90th percentile prediction interval are shown given 140 timesteps of context.On the right of each time series, we show the log likelihoods compared to the ARIMA and TCN time series models.Overall, GPT-3 performs considerably better than the baselines, though composition and exponential growth are more challenging for the models (Appendix D.1).</p>
<p>Figure 8 :
8
Figure 8: Left: LLMTIME can handle missing values without interpolation by denoting missingness with text (e.g.'NaN').For baseline methods we perform linear interpolation and then fit the model as usual.LLMTIME assigns higher log likelihood to datasets preprocessed with added 'NaN's than baseline methods assign to interpolated datasets.Forecasting performance, as judged by CRPS, is competitive between LLMTIME and alternative methods that use explicit interpolation.Filled area shows standard error over individual datasets and 3 random seeds.Right: LLMs can be used to answer questions about time series data posed as text.We show GPT-4's accuracy at predicting the function that generated the time series, obtained using chain-of-thought prompting.</p>
<p>Figure 9 :
9
Figure9: Evaluation on a collection of short univariate time series recorded after GPT-3's training cutoff date.We compare the performance of our GPT-3 predictor against popular time series models.Predicted median and 10-90th percentile intervals are shown for GPT-3 given the context, and we compare test negative log likelihoods.GPT-3 continues to be competitive with or outperforms the baselines on all of the tasks, from in-context learning alone.This result reinforces our belief that GPT-3's performance is not due to memorization of the test data.</p>
<p>Figure 10 :
10
Figure10: Median predictions of LLMTIME (GPT-3) and NLLs from LLMTIME (GPT-3 and LLaMA-2 70B) for every dataset within Darts[24].The shaded area shows the 10th to 90th quantiles of the distribution over samples.LLMTIME consistently obtains better likelihood values than the baselines and often makes surprisingly accurate forecasts by effectively extrapolating trend and periodic components.</p>
<p>Figure 11 showsFigure 11 :
1111
Figure 11  shows MAE results per dataset and in aggregate for the Informer datasets we used in the paper.Extending the results in the main text, we also include MAE numbers for a prediction horizon</p>
<p>are 1 .
1
(x 0 + 0.3652524) 2. cos(cos(x 0 / − 0.031412385) * (−1.5252972 + x 0 )) 3. (sin(cos(cos(x 0 /0.031470172) * −1.4668792)) + (cos(0.81965065)* x 0 )) 4. (sin(cos(cos((x 0 / sin(−0.03127001)sin(cos(cos(exp(cos(−0.03127001)+ x 0 ))))) * x 0 )) 5. (cos((cos((x 0 / − 0.03127001) + 0.07646165)/ − 0.957405)/ sin(sin(cos(x0 − x0)) * exp(cos(sin(x 0 / − 0.983214)))))/(cos(sin(sin(sin(sin(x 0 )) − (x 0 * (−0.47036648 − (x 0 /0.5857117)))))) − −0.10476875))</p>
<p>Figure 12 :
12
Figure 12: LLMTIME (GPT-3 base model) median predictions on at most 4 randomly chosen series per Monash dataset.</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: LLMTIME (LLaMA-2 70B base model) median predictions on 4 randomly chosen series per Informer dataset.</p>
<p>6 7
6
Now , let ' s go through the functions in the mapping and see which one matches our observations :</p>
<p>8
8</p>
<p>22 23 Answer : linear Listing 2 :
222
Sample output for Linear time series (correctly classified)1 First , let ' s analyze the time series .</p>
<p>6 7
6
With these observations , let ' s analyze the given functions :</p>
<p>8 9 1
81
. gaussian_wave : This function represents a wave that is localized around a specific point .It does not match the characteristics of the time series .10 2. exp : This function represents an exponential growth , which matches the increasing and accelerating characteristics of the time series .11 3. linear_cos : This function represents a linear function with an added cosine term .It does not match the accelerating characteristic of the time series .12 4. linear : This function represents a linear growth , which does not match the accelerating characteristic of the time series .13 5. sine : This function represents a sinusoidal wave , which does not match the characteristics of the time series .14 6.sinc : This function represents a sinc function , which does not match the characteristics of the time series .15 7. beat : This function represents a beat pattern , which does not match the characteristics of the time series .16 8.sigmoid : This function represents a sigmoid curve , which does not match the characteristics of the time series .</p>
<p>17
17</p>
<ol>
<li>log : This function represents a logarithmic growth , which does not match the accelerating characteristic of the time series .18 10.x_times_sine : This function represents a product of a linear function and a sinusoidal function , which does not match the characteristics of the time series .19 11. square : This function represents a quadratic growth , which does not match the characteristics of the time series .</li>
</ol>
<p>20 21
20
Based on our analysis , the exp function is the most likely candidate to have generated the time series .</p>
<p>22 23 Answer : exp Listing 3 :
223
Sample output for Exp time series (correctly classified)</p>
<p>[24, LLa55]2 70B has the best or second best aggregated performance on several deterministic time series benchmarks[24, 19,55]while being entirely zeroshot.Collectively, these benchmarks comprise 29 individual datasets with diverse sources, lengths, and noise levels.For Monash MAE numbers, established results are reported on unnormalized data, so we normalize values before aggregating (Appendix C.2).The informer datasets are multivariate, and we predict each covariate independently with LLMTIME (Appendix C.3). GPT-3 evaluation on the Informer datasets was skipped because of the cost of API queries.Error bars show standard errors over the individual datasets in each benchmark.</p>
<p>[51] easily extrapolate trends (e.g.AirPassengers) and reproduce local patterns when data is noisy (e.g.GasRateCO2).Center: When using probabilistic metrics like NLL and CRPS, LLMTIME outperforms all baselines, including PromptCast[51], a competing LLM method.Error bars show standard errors over datasets with Darts.Right: LLMTIME is much more sample efficient than competing methods.While the performance of other methods degrades rapidly when we restrict them to a fraction of the original training set, LLMTIME can assign high likelihood with only a few examples.</p>
<p>The prediction accuracies computed over 20 trials are shown in Figure8, with x_times_sine, beat, and sinc not shown in the table because GPT-4 predicted these incorrectly 100% of the time.With the CoT prompting, this prediction task elicits some interesting textual analysis of the time series.Several (non cherry-picked) examples are shown below.Notably, this task elicits the model to analyze the time series in text, reasoning about the trend and periodicity.However, the model sometimes makes incorrect deductions about the behavior of the data it has seen, or the expected behavior of the candidate functions.
24 -0.320 -0.355 -0.385 -0.408 -0.425 -0.433 -0.432 -0.42225 -0.402 -0.371 -0.330 -0.279 -0.217 -0.145 -0.064 0.026260.124 0.229 0.339 0.453 0.570 0.688 0.806 0.922 1.033271.140 1.238 1.328 1.407 1.474 1.527 1.564 1.586 1.590281.576 1.543 1.491 1.420 1.329 1.219 1.091 0.945 0.782290.604 0.413 0.209 -0.005 -0.227 -0.455 -0.686 -0.91730-1.147 -1.373 -1.591 -1.799 -1.995 -2.175 -2.338 -2.48131-2.602 -2.698 -2.769 -2.812 -2.826 -2.811 -2.766 -2.68932-2.583 -2.446 -2.279 -2.084 -1.863 -1.615 -1.345 -1.05333-0.743 -0.418 -0.081 0.265 0.617 0.970 1.322 1.667 2.003342.325 2.630 2.915 3.175 3.407 3.609 3.778 3.910 4.005354.060 4.074 4.046 3.975 3.862 3.706 3.509 3.272 2.997362.686 2.341 1.965 1.563 1.137 0.692 0.232 -0.239 -0.71537-1.192 -1.665 -2.128 -2.577 -3.008 -3.414 -3.791 -4.13638-4.443 -4.710 -4.932 -5.108 -5.233 -5.306 -5.326 -5.29139-5.202 -5.058 -4.859 -4.608 -4.306 -3.955 -3.558 -3.12040-2.643 -2.132 -1.592 -1.029 -0.447 0.148 0.749 1.350411.946 2.530 3.095 3.637 4.148 4.623 5.057 5.445 5.782426.064 6.288 6.450 6.548 6.580 6.546 6.444 6.275 6.039435.740 5.378 4.956 4.479 3.951 3.376 2.760 2.109 1.429440.726 0.009 -0.717 -1.444 -2.164 -2.870 -3.555 -4.21045-4.830 -5.408 -5.936 -6.409 -6.822 -7.170 -7.448 -7.65346-7.783 -7.834 -7.806 -7.698 -7.511 -7.245]5' linear_cos ': lambda t : 0.3+ 0.5<em> t +.2</em> np . cos (25<em> t +3) ,6' linear ': lambda t : 0.3+ 0.5</em> t ,7' sine ': lambda t : np . sin (40<em> t +3) ,8' sinc ': lambda t : np . sin (10</em> t ) / t /10 ,9' beat ': lambda t : np . sin (3<em> t ) * np . sin (25</em> t ) ,
"Which name gave rise to this series?Put your answer in the form 'Answer: gaussian_wave' " or "Carefully analyze the time series.Think step by step, make observations about the time series that you see and then use your observations to identify which of the functions is most likely to have generated it.Reason your way to a solution and at the end give give a name as your answer such as 'Answer: gaussian_wave'."forchain-of-thought prompting.1 import numpy as np 2 mapping = { 3 ' gaussian_wave ': lambda t : np .exp ( -5<em>( t -.6) </em><em>2) * np .sin (20</em>( t -6) ) , 4 ' exp ': lambda t : np .exp (2* t ) ,</p>
<p>Based on these observations , the most likely function to have generated the time series is ' sigmoid '.
Answer : sigmoid Listing 5: Sample output for Gaussian_wave time series (misclassified)
Acknowledgements.We thank Micah Goldblum, Greg Benton, and Wesley Maddox for helpful discussions.This work is supported by NSF CAREER IIS-2145492, NSF I-DISRE 193471, NSF IIS-1910266, BigHat Biosciences, Capital One, and an Amazon Research Award.AppendixA Detailed method and hyperparametersA.1 Input scalingFor all baseline methods, we use the MinMaxScaler from sklearn.For GPT-3, since it can handle inputs spanning multiple orders of magnitudes by using varying number of digits, we apply an affine transformation to each element x t of a time series (x 1 , ..., x T ):, and a is the α-percentile of the shifted series (x 1 −b, ..., x T −b).We also consider a basic scaler that only applies scaling and not shifting, with a clipped to a maximum of 0.01 when the series only has tiny values.Here α and β are hyperparameters controlling the thresholds at which the number of digits used by the language model changes.A.2 Validation tuningWe construct a validation time series from the last T observations in the training series, where T is the length of the test series.When the training series is shorter than 2T, we take the last half of the training series as the validation series.The likelihood of generating the validation conditioned on the remaining training series is used to select the hyperparameters.Since LLMTIME is zero-shot, the likelihood is computed without training.For other methods such as ARIMA, the likelihood is computed after training on the remaining training series.A.3 Likelihood adjustment for GPT ModelsIn order to convert token probabilities assigned by language models into continuous densities, we must convert the distribution over all possible tokens into the distribution over only tokens used in Spectral Mixture Gaussian Process (SM-GP) We use a GP with a kernel formed by the sum of a spectral mixture kernel with 12 mixture components and a RBF kernel.We tune the learning rate from [5e-3, 1e-2, 5e-C.2 Monash datasetsWe evaluate on 19 datasets in Monash that satisfy two criteria:1.The total number of individual series cannot be prohibitively large, so that the experiments can be run in time without access to an enormous cluster and without a gratuitous API expenses.2. The length of the forecasting horizon cannot extend to a length that makes it impossible to fit both the forecast and the history into the context window of the language model.When we applied these criteria, we obtained the following 19 datasets were selected: covid deaths, solar weekly, tourism monthly, australian electricity demand, pedestrian counts, traffic hourly, hospital, fred md, tourism yearly, tourism quarterly, us births, nn5 weekly, nn5 daily, traffic weekly, saugeenday, cifTo aggregate across datasets, we normalized the mean absolute error by the MAE achieved by simply predicting the last observed value before the test series (a naive baseline).This normalization places high weight on datasets for which methods perform significantly better or worse than the naive predictor.Several of the baseline methods in the archive are shared with Darts, and all descriptions and code can be found in[19].A few notable addition include• CatBoost: CatBoost[40]is gradient-boosting framework for continuous or categorical data.• FFNN: A feed-forward neural network with a fixed window of input and output, inspired by Goodfellow et al.[21].• PR: A linear pooled regression (PR) model proposed by Trapero et al.[46].We include visualizations of GPT-3's prediction on these datasets in Appendix C.7.GPT-3 hyperparametersWe use the following hyperparameters for GPT-3: α = 0.9, β = 0, temperature = 0.7.To avoid exceeding the context window, we truncate the history to at most 500 most recent observations.For the baselines, we report their performance as presented in[19].The normalized MAE values shown in Figure4(center) are obtained by normalizing by the lowest baseline MAE on each dataset before aggregating.LLaMA-2 70B hyperparameters We use the following hyperparameters for LLaMA-2 70B: α = 0.99, β = 0.3, temperature = 1.0, nucleus = 0.9.To avoid exceeding the context window, we truncate the history to fit in the LLaMA-2 context window (4096 tokens).C.3 Informer datasetsThere are 6 datasets used by Zhou et al.[55]that have become standard benchmarks for evaluating efficient transformers.We evaluate on the 5 datasets that are typically used with a prediction horizon
Joshua Ainslie, Tao Lei, Santiago Michiel De Jong, Siddhartha Ontañón, Yury Brahma, David Zemlyanskiy, Mandy Uthus, James Guo, Yi Lee-Thorp, Tay, arXiv:2303.09752Faster long-range transformers with conditional computation. 2023arXiv preprint</p>
<p>Physics of language models: Part 1, context-free grammar. Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2305.136732023arXiv preprint</p>
<p>Path independent equilibrium models can better exploit test-time computation. Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, Zico Kolter, Roger B Grosse, Advances in Neural Information Processing Systems. 202235</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Introducing 100k context windows. Anthropic blog. Anthropic, 2023</p>
<p>Deep probabilistic time series forecasting over long horizons. Gregory Benton, Nate Gruver, Wesley Maddox, Andrew Gordon, Wilson , 2022openreview preprint</p>
<p>Emergent and predictable memorization in large language models. Stella Biderman, Lintang Usvsn Sai Prashanth, Hailey Sutawika, Quentin Schoelkopf, Shivanshu Anthony, Edward Purohit, Raf, arXiv:2304.111582023arXiv preprint</p>
<p>Some recent advances in forecasting and control. E P George, Gwilym M Box, Jenkins, Journal of the Royal Statistical Society. Series C (Applied Statistics). 1721968</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>N-hits: Neural hierarchical interpolation for time series forecasting. Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, Artur Dubrawski, arXiv:2201.128862022arXiv preprint</p>
<p>Mackenzie Kent K Chang, Sandeep Cramer, David Soni, Bamman, arXiv:2305.00118Speak, memory: An archaeology of books known to chatgpt/gpt-4. 2023arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730Advances in neural information processing systems</p>
<p>Interpretable machine learning for science with pysr and symbolicregression. Miles Cranmer, arXiv:2305.015822023jl. arXiv preprint</p>
<p>Language modeling is compression. Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li , Kevin Wenliang, Matthew Aitchison, Laurent Orseau, arXiv:2309.106682023arXiv preprint</p>
<p>Preformer: Predictive transformer with multiscale segment-wise correlations for long-term time series forecasting. Dazhao Du, Bing Su, Zhewei Wei, arXiv:2202.113562022arXiv preprint</p>
<p>Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. Geoff Jacob R Gardner, David Pleiss, Kilian Q Bindel, Andrew Weinberger, Wilson Gordon, Advances in Neural Information Processing Systems. 2018</p>
<p>Statsforecast: Lightning fast forecasting with statistical and econometric models. PyCon: Salt Lake City. Federico Garza, Max Mergenthaler Canseco, Cristian Challú, Kin G Olivares, 2022UT, USA</p>
<p>Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman, Pablo Montero-Manso, arXiv:2105.06643Monash time series forecasting archive. 2021arXiv preprint</p>
<p>The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. Micah Goldblum, Marc Finzi, Andrew Keefer Rowan, Wilson Gordon, arXiv:2304.053662023arXiv preprint</p>
<p>Deep learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016MIT press</p>
<p>The lie derivative for measuring learned equivariance. Nate Gruver, Marc Finzi, Micah Goldblum, Andrew Gordon, Wilson , arXiv:2210.029842022arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Darts: User-friendly modern machine learning for time series. Julien Herzen, Francesco Lässig, Samuele Giuliano Piazzetta, Thomas Neuer, Léo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, The Journal of Machine Learning Research. 2312022</p>
<p>Forecast evaluation for data scientists: common pitfalls and best practices. Hansika Hewamalage, Klaus Ackermann, Christoph Bergmeir, Data Mining and Knowledge Discovery. 3722023</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019arXiv preprint</p>
<p>Forecasting with exponential smoothing: the state space approach. Rob J Hyndman, Anne B Koehler, Keith Ord, Ralph D Snyder, 2008Springer Science &amp; Business Media</p>
<p>Ai in healthcare: time-series forecasting using statistical, neural, and ensemble architectures. Shruti Kaushik, Abhinav Choudhury, Pankaj Kumar Sheron, Nataraj Dasgupta, Sayee Natarajan, Larry A Pickett, Varun Dutt, Frontiers in big data. 342020</p>
<p>Temporal convolutional networks: A unified approach to action segmentation. Colin Lea, Rene Vidal, Austin Reiter, Gregory D Hager, Computer Vision-ECCV 2016 Workshops. Amsterdam, The NetherlandsSpringerOctober 8-10 and 15-16, 2016. 2016Proceedings, Part III 14</p>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini, arXiv:2107.06499Deduplicating training data makes language models better. 2021arXiv preprint</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>A scalable hierarchical distributed language model. Andriy Mnih, Geoffrey E Hinton, Advances in neural information processing systems. 200821</p>
<p>imputets: time series missing value imputation in r. Steffen Moritz, Thomas Bartz-Beielstein, R J. 912072017</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>In-context learning and induction heads. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam Mccandlish, Chris Olah, 2022Transformer Circuits Thread</p>
<p>Wavenet: A generative model for raw audio. Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, 9th ISCA Speech Synthesis Workshop. ISCA2016</p>
<p>. OpenAI. Gpt-4 technical report. arXiv. 2023</p>
<p>N-beats: Neural basis expansion analysis for interpretable time series forecasting. Dmitri Boris N Oreshkin, Nicolas Carpov, Yoshua Chapados, Bengio, Journal of Machine Learning Research. 211112020</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Catboost: unbiased boosting with categorical features. Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin, Advances in Neural Information Processing Systems. NeurIPS201831</p>
<p>Deepar: Probabilistic forecasting with autoregressive recurrent networks. David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, International Journal of Forecasting. 3632020</p>
<p>Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, Tom Goldstein, Advances in Neural Information Processing Systems. 202134</p>
<p>An observation on generalization. Ilya Sutskever, Workshop on Large Language Models and Transformers. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M Kloumann, A V Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, ArXiv, abs/2307.092882023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>On the identification of sales forecasting models in the presence of promotions. Nikolaos Juan R Trapero, Robert Kourentzes, Fildes, Journal of the operational Research Society. 6622015</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Gaussian process kernels for pattern discovery and extrapolation. Andrew Wilson, Ryan Adams, International conference on machine learning. PMLR2013</p>
<p>Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long, Advances in Neural Information Processing Systems. 342021</p>
<p>Promptcast: A new prompt-based learning paradigm for time series forecasting. Hao Xue, Flora D Salim, 2023</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.020152023arXiv preprint</p>
<p>Are transformers effective for time series forecasting. Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu, arXiv:2205.135042022arXiv preprint</p>
<p>Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue, arXiv:2307.10802Meta-transformer: A unified framework for multimodal learning. 2023arXiv preprint</p>
<p>Informer: Beyond efficient transformer for long sequence time-series forecasting. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang, Proceedings of AAAI. AAAI2021</p>
<p>FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, Rong Jin, Proc. 39th International Conference on Machine Learning. 39th International Conference on Machine Learning2022ICML 2022</p>
<p>10 ' sigmoid ': lambda t : 1/(1+ np . exp ( -4<em> t ) ) , 11 ' log ': lambda t : np . log (1+ t ) , 12 ' x_times_sine ': lambda t : 4</em>( t +1) * np . sin (10<em>( t +1) +4) , 13 ' square ': lambda t : 3</em>( t -.6) **2 , 14 } 15 name = np. Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, Rong Jin, arXiv:2302.119392023arXiv preprintOne fits all: Power general time series analysis by pretrained lm. random . choice ( list ( mapping . keys (</p>
<p>1 Observations about the time series : 2 1. The values are positive and continuous . 3 2. The values start from a high value and gradually decrease until they. name ) 21 22 Series : 23 [ -0.000 -0.033 -0.070 -0.111 -0.153 -0.197 -0.240 -0.281{0:0.3 f } " . format ( x ) }) 19 print. reach a minimum value near the middle of the series</p>
<p>After the minimum value , the values gradually increase again until they reach a high value near the end of the series. </p>
<p>The overall trend of the time series seems to be symmetric. </p>
<p>Now , let ' s analyze the given functions and their characteristics. </p>
<p>Symmetric , bell -shaped curve , values are positive and continuous. </p>
<p>Exponential growth. not symmetric</p>
<p>Linear trend with oscillations , not symmetric. </p>
<p>Linear growth. not symmetric</p>
<p>sine : Oscillating function , not symmetric. </p>
<p>sinc : Symmetric , sinc function has a peak at the center and oscillates on both sides. </p>
<p>Combination of two sine waves with different frequencies , not symmetric. </p>
<p>sigmoid : S -shaped curve , not symmetric. </p>
<p>x_times_sine : Product of a linear function and sine function. 1810Logarithmic growth , not symmetric. not symmetric</p>
<p>square : Quadratic function , symmetric , parabolic shape. 11</p>
<p>21 Based on our observations , the most likely function to have generated this time series is the ' square ' function . It is symmetric , has a parabolic shape , and the values are positive and continuous. </p>
<p>23 Answer : square Listing 4: Sample output for Square time series. correctly classified</p>
<p>We can start by analyzing the time series and making observations about its characteristics : 2 3 1. The time series starts and ends at 0 , with values close to 0 in the beginning and end. </p>
<p>The time series has a single peak and a single valley , reaching a maximum value around 0.85 and a minimum value around -1. </p>
<p>The time series is not periodic , as it doesn ' t repeat its pattern. </p>
<p>let s through the functions one by one and see which one matches these observations. Now, </p>
<p>This function is a product of a Gaussian and a sinusoidal function , which could give us a single peak and valley . However , it is periodic , so it doesn. t match observation 3</p>
<p>This function is an exponential function , which doesn ' t match the observations as it doesn ' t have a peak and valley. </p>
<p>This function is a linear function combined with a cosine function , which could give a single peak and valley . However , it is also periodic , so it doesn. t match observation 3</p>
<p>This function is a linear function , which doesn ' t have a peak and valley , so it doesn ' t match observation 2. </p>
<p>This function is a sinusoidal function , which is periodic and doesn ' t match observation 3. </p>
<p>This function is a sinc function , which has multiple peaks and valleys , so it doesn ' t match observation 2. </p>
<p>This function is a product of two sinusoidal functions , which could give a single peak and valley . However , it is also periodic , so it doesn. t match observation 3</p>
<p>This function is a sigmoid function , which matches all the observations . It starts and ends at 0 , has a single peak and valley , and is not periodic. </p>
<p>This function is a logarithmic function , which doesn ' t have a peak and valley , so it doesn ' t match observation 2. </p>
<p>This function is a product of a linear function and a sinusoidal function , which could give a single peak and valley . However , it is also periodic , so it doesn. t match observation 3</p>
<p>This function is a quadratic function , which doesn ' t have a peak and valley , so it doesn ' t match observation 2. </p>
<p>Let ' s analyze the time series step by step : 2 3 1. The time series starts at a negative value , increases to a positive value , and then decreases back to a negative value . This suggests that the function might have a symmetric or periodic behavior. </p>
<p>The time series has a peak in the middle , which indicates that the function might have a single maximum or minimum point. </p>
<p>The time series does not show an exponential growth or decay , which rules out 'exp. sigmoid ' functions</p>
<p>The time series does not have a linear pattern , which rules out the ' linear ' function . 10 11 5. The time series does not show a clear periodic pattern , which rules out ' sine ' , ' linear_cos ' , and ' beat ' functions . 12 13 6. The time series does not show a sinc pattern , which rules out the ' sinc ' function . 14 15 7. The time series does not show a clear x_times_sine pattern , which rules out the ' x_times_sine ' function . 16 17 8. The time series does not show a clear parabolic pattern. which rules out the ' square ' function</p>
<p>19 Based on the above observations , the only remaining function is ' gaussian_wave '. Therefore , the most likely function to have generated the time series is. 21 Answer : gaussian_wave Listing 6: Sample output for Linear_Cos time series. 20misclassified</p>            </div>
        </div>

    </div>
</body>
</html>
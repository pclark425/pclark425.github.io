<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-837 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-837</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-837</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-259129428</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.06070v3.pdf" target="_blank">Mind2Web: Towards a Generalist Agent for the Web</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e837.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e837.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINDACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MINDACT (two-stage MINi-dataset ACTor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage agent architecture introduced in this paper that first uses a small ranking LM to filter/prioritize DOM elements and then uses an LLM in a multi-choice QA formulation to pick an element and predict the interaction operation for web tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MINDACT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid two-stage pipeline: (1) a small encoder-only LM (DeBERTa-v3-base) used as a cross-encoder to rank DOM elements and produce a top-k candidate pool; (2) an LLM (Flan-T5 variants or GPTs) that consumes snippets built from the top-k candidates and performs element selection formulated as multi-choice QA plus operation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MIND2WEB web navigation (element selection + action prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation / sequential decision-making / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Best reported: step success rate up to 52.0% (Cross-Task); 38.9% (Cross-Website); 39.6% (Cross-Domain). Overall task success rates were much lower (most tasks failed at least one step).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Two-stage hybrid: small LM cross-encoder ranking (candidate generation) + LLM multi-choice QA for element selection; multi-choice discrimination framing instead of free-form generation; candidate pruning of the DOM to reduce context size.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning for small LM ranking and supervised fine-tuning of LLMs on multi-choice action prediction; LLMs also evaluated with in-context learning (GPT-3.5/GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid approach (architectural change) + training framing (multi-choice QA)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Introduce a candidate-generation stage using a small, efficient cross-encoder LM to reduce the DOM to a top-k element pool, then reframe element selection as a multi-choice QA problem for an LLM which also generates the operation/value. This reduces context length fed to the LLM and converts selection into discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial improvement over direct baselines: MINDACT (best Flan-T5 variant) increased step success rate to ~52.0% Cross-Task vs ~17–22% step SR for the generation/classification baselines (paper reports generation baseline ~17.5% and classification baseline ~21.6% step SR), and improved element selection accuracy markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper attributes the gap to (1) extremely long and structured HTML (thousands of elements) making direct LLM input infeasible, (2) partial observability and dynamic environment transitions across pages, (3) diversity in website designs and interaction logic that complicates grounding high-level instructions into concrete actions, and (4) the need for multi-step planning and navigation (not just one-shot QA). Pretrained LMs can decompose tasks conceptually but struggle to ground actions in varying page layouts and to perform long sequential decision-making without specialized mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind2Web: Towards a Generalist Agent for the Web', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e837.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e837.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa (cross-encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa-v3-base (used as cross-encoder candidate generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-only LM (DeBERTa-v3-base) fine-tuned as a cross-encoder ranking model to score DOM elements for relevance to the current step and task, producing a top-k candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DeBERTa-v3-base (cross-encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer used in a cross-encoder architecture: each candidate DOM element is paired with the task query and previous actions and scored for relevance; optimized with binary cross-entropy on positive/negative element samples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>86M</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MIND2WEB candidate generation for web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation (candidate ranking / pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Recall@50 reported: 88.9% (Cross-Task), 85.3% (Cross-Website), 85.7% (Cross-Domain); used top-50 as candidate pool for downstream LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Cross-encoder ranking architecture; encodes (task+history) with element textual representation (tag, text, attributes, neighbors).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning as a binary ranking task (positive = ground-truth element, negatives sampled from page).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>component-level architectural intervention (candidate filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as an efficient filter to reduce the huge DOM to a manageable set of top-k candidates, enabling downstream LLMs to operate within context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>High recall at k (≈85–89% Recall@50) enabling the LLM stage to have the ground-truth element in the pruned candidate set the majority of the time; essential for improved overall success when combined with the LLM stage.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>DeBERTa as a standalone classifier/selector was less effective than the two-stage pipeline; the paper suggests pure classification or generation on the full DOM is infeasible due to context size and lower sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind2Web: Towards a Generalist Agent for the Web', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e837.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e837.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (base / large / XL) (fine-tuned for multi-choice action prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Seq2Seq LLMs (Flan-T5 family) fine-tuned on the multi-choice QA formulation to select among candidate DOM elements and generate operations/values for web interaction steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Flan-T5 (base/large/xl) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned sequence-to-sequence transformer models (Flan-T5 family) fine-tuned with left-to-right LM objective on inputs containing grouped candidate elements (multi-choice) and target actions (selected element + operation/value).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MIND2WEB action prediction (multi-choice element selection + operation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation / multi-step action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>MINDACT with Flan-T5-XL (best) achieved step SR ≈ 52.0% (Cross-Task), 38.9% (Cross-Website), 39.6% (Cross-Domain). Flan-T5 models fail in zero-shot (Flan-T5 XL zero-shot element selection ≈ 10.8% Cross-Task vs 52.0% fine-tuned).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Seq2Seq LLM used in multi-choice discrimination framing; grouped candidate options (up to 5) with a 'None' option and iterative grouping to resolve selection.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning on multi-choice action prediction using the MIND2WEB training data; also evaluated zero-shot and few-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (fine-tuning) + prompting/formatting (multi-choice discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Reframing element selection as multi-choice QA for LLMs and fine-tuning Flan-T5 on that format, rather than direct element generation; also using a two-stage pipeline with candidate pruning by DeBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large gains versus zero-shot/generation baselines: fine-tuning plus two-stage pipeline increased element selection and step success substantially (e.g., Flan-T5-XL fine-tuned step SR ≈ 52% Cross-Task vs Flan-T5 XL zero-shot element selection ≈ 10.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Without fine-tuning and candidate pruning, Flan-T5 struggles due to domain mismatch (HTML/DOM structure) and context length constraints; fine-tuning and the multi-choice framing improve grounding into actions but still leave substantial failure modes for multi-step navigation across pages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind2Web: Towards a Generalist Agent for the Web', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e837.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e837.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A black-box LLM from OpenAI evaluated via in-context learning on the multi-choice element selection formulation for MIND2WEB; used with 3 demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-3.5-turbo (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large generative transformer model accessed via API, used here in a few-shot in-context learning setup (3 demonstrations) to perform multi-choice element selection and operation prediction on pruned snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MIND2WEB action prediction via in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation / multi-step action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to have low element selection accuracy (~20%); comparable to baselines on the MIND2WEB multi-choice task with 3 demonstrations but substantially worse than tuned Flan-T5 and GPT-4 in many settings. The paper notes a strong tendency for GPT-3.5 to select the 'None' option.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>In-context learning (few-shot prompting, 3 examples); no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used the same multi-choice input format with 3 demonstration examples in the prompt for in-context learning; no additional architectural change.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>With only 3 in-context examples GPT-3.5 achieved modest results (element selection ≈20%)—worse than tuned Flan-T5 and GPT-4—indicating limited in-context ability for lengthy multi-step web grounding in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>GPT-3.5's tendency to choose 'None' reflects difficulty in planning across pages and recognizing that an action sequence requires navigation; also limited by prompt-context formulation and lack of fine-tuning on HTML/DOM-structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind2Web: Towards a Generalist Agent for the Web', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e837.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e837.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more capable OpenAI LLM evaluated with in-context learning on the MIND2WEB multi-choice formulation; found to perform comparably to tuned Flan-T5 models on element selection in cross-website/domain settings but evaluated on a smaller budgeted subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4 (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large, high-capability generative transformer model accessed via API and used with few in-context demonstrations (3) on the multi-choice element selection + operation generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MIND2WEB action prediction via in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation / multi-step action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>On a 50-task subset (due to cost) GPT-4 achieved element selection / step success comparable to fine-tuned Flan-T5 models in Cross-Website and Cross-Domain settings (paper reports parity on element selection for those settings); exact overall numbers on full test set not provided due to budget.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>In-context learning (few-shot prompting); no fine-tuning reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Applied the same multi-choice grouping prompt format with 3 demonstrations to GPT-4; used top-k candidate snippets from DeBERTa stage.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>GPT-4 performed strongly in element selection on the tested subsets and matched fine-tuned Flan-T5 in Cross-Website/Cross-Domain element selection, showing that a strong off-the-shelf LLM can be effective without fine-tuning, but at high operational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Although GPT-4 is powerful, the paper highlights cost and context-size issues; even GPT-4 faces challenges of multi-step navigation and dynamic environment grounding, which motivates specialized smaller models or hybrid pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind2Web: Towards a Generalist Agent for the Web', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents <em>(Rating: 2)</em></li>
                <li>World of Bits: An Open-Domain Platform for Web-Based Agents <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Understanding html with large language models <em>(Rating: 2)</em></li>
                <li>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings <em>(Rating: 1)</em></li>
                <li>Tool Learning with Foundation Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-837",
    "paper_id": "paper-259129428",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "MINDACT",
            "name_full": "MINDACT (two-stage MINi-dataset ACTor)",
            "brief_description": "A two-stage agent architecture introduced in this paper that first uses a small ranking LM to filter/prioritize DOM elements and then uses an LLM in a multi-choice QA formulation to pick an element and predict the interaction operation for web tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "MINDACT",
            "model_description": "Hybrid two-stage pipeline: (1) a small encoder-only LM (DeBERTa-v3-base) used as a cross-encoder to rank DOM elements and produce a top-k candidate pool; (2) an LLM (Flan-T5 variants or GPTs) that consumes snippets built from the top-k candidates and performs element selection formulated as multi-choice QA plus operation generation.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MIND2WEB web navigation (element selection + action prediction)",
            "interactive_task_type": "web navigation / sequential decision-making / multi-step reasoning",
            "interactive_performance": "Best reported: step success rate up to 52.0% (Cross-Task); 38.9% (Cross-Website); 39.6% (Cross-Domain). Overall task success rates were much lower (most tasks failed at least one step).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Two-stage hybrid: small LM cross-encoder ranking (candidate generation) + LLM multi-choice QA for element selection; multi-choice discrimination framing instead of free-form generation; candidate pruning of the DOM to reduce context size.",
            "training_method": "Supervised fine-tuning for small LM ranking and supervised fine-tuning of LLMs on multi-choice action prediction; LLMs also evaluated with in-context learning (GPT-3.5/GPT-4).",
            "intervention_type": "hybrid approach (architectural change) + training framing (multi-choice QA)",
            "intervention_description": "Introduce a candidate-generation stage using a small, efficient cross-encoder LM to reduce the DOM to a top-k element pool, then reframe element selection as a multi-choice QA problem for an LLM which also generates the operation/value. This reduces context length fed to the LLM and converts selection into discrimination.",
            "intervention_effect": "Substantial improvement over direct baselines: MINDACT (best Flan-T5 variant) increased step success rate to ~52.0% Cross-Task vs ~17–22% step SR for the generation/classification baselines (paper reports generation baseline ~17.5% and classification baseline ~21.6% step SR), and improved element selection accuracy markedly.",
            "hypothesized_cause_of_gap": "The paper attributes the gap to (1) extremely long and structured HTML (thousands of elements) making direct LLM input infeasible, (2) partial observability and dynamic environment transitions across pages, (3) diversity in website designs and interaction logic that complicates grounding high-level instructions into concrete actions, and (4) the need for multi-step planning and navigation (not just one-shot QA). Pretrained LMs can decompose tasks conceptually but struggle to ground actions in varying page layouts and to perform long sequential decision-making without specialized mechanisms.",
            "uuid": "e837.0",
            "source_info": {
                "paper_title": "Mind2Web: Towards a Generalist Agent for the Web",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "DeBERTa (cross-encoder)",
            "name_full": "DeBERTa-v3-base (used as cross-encoder candidate generator)",
            "brief_description": "An encoder-only LM (DeBERTa-v3-base) fine-tuned as a cross-encoder ranking model to score DOM elements for relevance to the current step and task, producing a top-k candidate pool.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "DeBERTa-v3-base (cross-encoder)",
            "model_description": "Encoder-only transformer used in a cross-encoder architecture: each candidate DOM element is paired with the task query and previous actions and scored for relevance; optimized with binary cross-entropy on positive/negative element samples.",
            "model_size": "86M",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MIND2WEB candidate generation for web navigation",
            "interactive_task_type": "web navigation (candidate ranking / pruning)",
            "interactive_performance": "Recall@50 reported: 88.9% (Cross-Task), 85.3% (Cross-Website), 85.7% (Cross-Domain); used top-50 as candidate pool for downstream LLM.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Cross-encoder ranking architecture; encodes (task+history) with element textual representation (tag, text, attributes, neighbors).",
            "training_method": "Supervised fine-tuning as a binary ranking task (positive = ground-truth element, negatives sampled from page).",
            "intervention_type": "component-level architectural intervention (candidate filtering)",
            "intervention_description": "Used as an efficient filter to reduce the huge DOM to a manageable set of top-k candidates, enabling downstream LLMs to operate within context limits.",
            "intervention_effect": "High recall at k (≈85–89% Recall@50) enabling the LLM stage to have the ground-truth element in the pruned candidate set the majority of the time; essential for improved overall success when combined with the LLM stage.",
            "hypothesized_cause_of_gap": "DeBERTa as a standalone classifier/selector was less effective than the two-stage pipeline; the paper suggests pure classification or generation on the full DOM is infeasible due to context size and lower sample efficiency.",
            "uuid": "e837.1",
            "source_info": {
                "paper_title": "Mind2Web: Towards a Generalist Agent for the Web",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Flan-T5 (fine-tuned)",
            "name_full": "Flan-T5 (base / large / XL) (fine-tuned for multi-choice action prediction)",
            "brief_description": "Seq2Seq LLMs (Flan-T5 family) fine-tuned on the multi-choice QA formulation to select among candidate DOM elements and generate operations/values for web interaction steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Flan-T5 (base/large/xl) fine-tuned",
            "model_description": "Instruction-tuned sequence-to-sequence transformer models (Flan-T5 family) fine-tuned with left-to-right LM objective on inputs containing grouped candidate elements (multi-choice) and target actions (selected element + operation/value).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MIND2WEB action prediction (multi-choice element selection + operation)",
            "interactive_task_type": "web navigation / multi-step action prediction",
            "interactive_performance": "MINDACT with Flan-T5-XL (best) achieved step SR ≈ 52.0% (Cross-Task), 38.9% (Cross-Website), 39.6% (Cross-Domain). Flan-T5 models fail in zero-shot (Flan-T5 XL zero-shot element selection ≈ 10.8% Cross-Task vs 52.0% fine-tuned).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Seq2Seq LLM used in multi-choice discrimination framing; grouped candidate options (up to 5) with a 'None' option and iterative grouping to resolve selection.",
            "training_method": "Supervised fine-tuning on multi-choice action prediction using the MIND2WEB training data; also evaluated zero-shot and few-shot variants.",
            "intervention_type": "training method (fine-tuning) + prompting/formatting (multi-choice discrimination)",
            "intervention_description": "Reframing element selection as multi-choice QA for LLMs and fine-tuning Flan-T5 on that format, rather than direct element generation; also using a two-stage pipeline with candidate pruning by DeBERTa.",
            "intervention_effect": "Large gains versus zero-shot/generation baselines: fine-tuning plus two-stage pipeline increased element selection and step success substantially (e.g., Flan-T5-XL fine-tuned step SR ≈ 52% Cross-Task vs Flan-T5 XL zero-shot element selection ≈ 10.8%).",
            "hypothesized_cause_of_gap": "Without fine-tuning and candidate pruning, Flan-T5 struggles due to domain mismatch (HTML/DOM structure) and context length constraints; fine-tuning and the multi-choice framing improve grounding into actions but still leave substantial failure modes for multi-step navigation across pages.",
            "uuid": "e837.2",
            "source_info": {
                "paper_title": "Mind2Web: Towards a Generalist Agent for the Web",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A black-box LLM from OpenAI evaluated via in-context learning on the multi-choice element selection formulation for MIND2WEB; used with 3 demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-3.5-turbo (in-context)",
            "model_description": "Large generative transformer model accessed via API, used here in a few-shot in-context learning setup (3 demonstrations) to perform multi-choice element selection and operation prediction on pruned snippets.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MIND2WEB action prediction via in-context learning",
            "interactive_task_type": "web navigation / multi-step action prediction",
            "interactive_performance": "Reported to have low element selection accuracy (~20%); comparable to baselines on the MIND2WEB multi-choice task with 3 demonstrations but substantially worse than tuned Flan-T5 and GPT-4 in many settings. The paper notes a strong tendency for GPT-3.5 to select the 'None' option.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "In-context learning (few-shot prompting, 3 examples); no fine-tuning reported.",
            "intervention_type": "prompting strategy (in-context learning)",
            "intervention_description": "Used the same multi-choice input format with 3 demonstration examples in the prompt for in-context learning; no additional architectural change.",
            "intervention_effect": "With only 3 in-context examples GPT-3.5 achieved modest results (element selection ≈20%)—worse than tuned Flan-T5 and GPT-4—indicating limited in-context ability for lengthy multi-step web grounding in this task.",
            "hypothesized_cause_of_gap": "GPT-3.5's tendency to choose 'None' reflects difficulty in planning across pages and recognizing that an action sequence requires navigation; also limited by prompt-context formulation and lack of fine-tuning on HTML/DOM-structured tasks.",
            "uuid": "e837.3",
            "source_info": {
                "paper_title": "Mind2Web: Towards a Generalist Agent for the Web",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A more capable OpenAI LLM evaluated with in-context learning on the MIND2WEB multi-choice formulation; found to perform comparably to tuned Flan-T5 models on element selection in cross-website/domain settings but evaluated on a smaller budgeted subset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4 (in-context)",
            "model_description": "Large, high-capability generative transformer model accessed via API and used with few in-context demonstrations (3) on the multi-choice element selection + operation generation task.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MIND2WEB action prediction via in-context learning",
            "interactive_task_type": "web navigation / multi-step action prediction",
            "interactive_performance": "On a 50-task subset (due to cost) GPT-4 achieved element selection / step success comparable to fine-tuned Flan-T5 models in Cross-Website and Cross-Domain settings (paper reports parity on element selection for those settings); exact overall numbers on full test set not provided due to budget.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "In-context learning (few-shot prompting); no fine-tuning reported in this paper.",
            "intervention_type": "prompting strategy (in-context learning)",
            "intervention_description": "Applied the same multi-choice grouping prompt format with 3 demonstrations to GPT-4; used top-k candidate snippets from DeBERTa stage.",
            "intervention_effect": "GPT-4 performed strongly in element selection on the tested subsets and matched fine-tuned Flan-T5 in Cross-Website/Cross-Domain element selection, showing that a strong off-the-shelf LLM can be effective without fine-tuning, but at high operational cost.",
            "hypothesized_cause_of_gap": "Although GPT-4 is powerful, the paper highlights cost and context-size issues; even GPT-4 faces challenges of multi-step navigation and dynamic environment grounding, which motivates specialized smaller models or hybrid pipelines.",
            "uuid": "e837.4",
            "source_info": {
                "paper_title": "Mind2Web: Towards a Generalist Agent for the Web",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "rating": 2,
            "sanitized_title": "webshop_towards_scalable_realworld_web_interaction_with_grounded_language_agents"
        },
        {
            "paper_title": "World of Bits: An Open-Domain Platform for Web-Based Agents",
            "rating": 2,
            "sanitized_title": "world_of_bits_an_opendomain_platform_for_webbased_agents"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Understanding html with large language models",
            "rating": 2,
            "sanitized_title": "understanding_html_with_large_language_models"
        },
        {
            "paper_title": "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
            "rating": 1,
            "sanitized_title": "toolkengpt_augmenting_frozen_language_models_with_massive_tools_via_tool_embeddings"
        },
        {
            "paper_title": "Tool Learning with Foundation Models",
            "rating": 1,
            "sanitized_title": "tool_learning_with_foundation_models"
        }
    ],
    "cost": 0.01620225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MIND2WEB: Towards a Generalist Agent for the Web
9 Dec 2023</p>
<p>Xiang Deng 
The Ohio State University</p>
<p>Yu Gu 
The Ohio State University</p>
<p>Boyuan Zheng 
The Ohio State University</p>
<p>Shijie Chen 
The Ohio State University</p>
<p>Samuel Stevens 
The Ohio State University</p>
<p>Boshi Wang 
The Ohio State University</p>
<p>Huan Sun 
The Ohio State University</p>
<p>Yu Su 
The Ohio State University</p>
<p>MIND2WEB: Towards a Generalist Agent for the Web
9 Dec 202310D92A3A38746431B006F82A28BB0EBAarXiv:2306.06070v3[cs.CL]Target Element Operation
We introduce MIND2WEB, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website.Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents.With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, MIND2WEB provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns.Based on MIND2WEB, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents.While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs.Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents.We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.</p>
<p>Introduction</p>
<p>The web now hosts billions of websites [7] that cover virtually every aspect of the digital world.In this work, we seek to answer the question: How can we build a generalist agent for the web that, given any website, can follow language instructions and carry out the corresponding tasks?Some exemplar tasks for such an agent are shown in Figure 1.A generalist agent could make the web more accessible, which is becoming increasingly difficult as modern websites provide increasingly more functionalities that also increase their complexity and learning curve.On the other hand, such an agent may also turn the entire web into an unprecedentedly powerful and versatile tool [23,27] that can enhance large language models (LLMs).For example, it may be used as a plugin for ChatGPT [24] to directly acquire information and carry out actions on HTML websites, instead of only retrieving web content through a retriever tool [8,18] or relying on pre-defined APIs for each web service [34,37].</p>
<p>A generalist agent for the web shall meet the following desiderata: First, it shall work on any website on the Internet.Since it is infeasible to collect sufficient training data that covers all websites, this requires the agent to be inherently generalizable to websites or even domains it has never seen before.Second, it shall work on real-world websites, which can be dynamic, complex, and noisy.Most modern websites are dynamic, generating and rendering different content in response to user actions.This necessitates the agent to model each website as a partially-observable environment instead of assuming full knowledge a priori.The agent should also not make strong simplifying assumptions (f) Open page to schedule an appointment for car knowledge test.about the environments but must embrace the full complexity and sometimes noise, e.g., due to sub-optimal website designs.Finally, it shall support diverse and sophisticated interactions with websites.Tasks from users can be highly diverse and take a large number of steps to complete (e.g., the task in Figure 1(b) would take 14 actions).An agent that only supports simple tasks may provide limited value to users.</p>
<p>Building an agent for the web is not entirely new.There have been numerous prior efforts in varied forms.However, none of them meets all the requirements for a generalist agent listed above.Existing work falls short in one to all of the following aspects: 1) Only operating in a limited and pre-specified set of websites [5,21,22,35,40], 2) making strong simplifying assumptions about the websites [22,40], and 3) only supporting specific types of tasks [21,22,40] and/or requiring tedious step-by-step instructions from users [5,21,22,39].Meanwhile, LLMs have been shown to excel at grounded language understanding in complex environments with good generalizability and sample efficiency [2,13,17,33].It is therefore promising to explore LLMs as a candidate solution towards generalist agents for the web.However, there lacks a good dataset that can support the development and evaluation of generalist web agents, which is the focus of this work.</p>
<p>In light of this, we present MIND2WEB, a new dataset with natural language tasks and manually annotated action sequences for developing and evaluating generalist agents for the web.It offers the following unique features:</p>
<ol>
<li>
<p>Diverse coverage of domains, websites, and tasks.MIND2WEB boasts a collection of over 2,000 tasks curated from 137 websites that span 31 different domains.This extensive range of tasks and domains not only provides a vast landscape for exploration and learning but also opens up a new level of versatility and complexity, fostering a more comprehensive evaluation of generalist web agents.</p>
</li>
<li>
<p>Use of real-world websites.MIND2WEB replaces the oversimplified simulation environments commonly found in other datasets with an authentic, vibrant, and unpredictable realm of real-world websites.We provide full traces of user interactions, webpage snapshots, and network traffic, making it a rich source of raw, unfiltered, and dynamic data.By doing so, MIND2WEB equips models with the capacity to interact and cope with the complexities and uncertainties of real-world environments, thereby encouraging the development of more robust and adaptive models.</p>
</li>
</ol>
<p>3.</p>
<p>A broad spectrum of user interaction patterns.MIND2WEB enables users to engage in sophisticated ways with websites, as opposed to basic operations such as searching, following links, and reading content commonly found in existing work.Users can click, select, and type in any elements on the website, which significantly expands the space of possible tasks.This captures all the common actions users do on websites in real life and promotes the development of agents capable of handling complex tasks.</p>
<p>With the diverse domains and websites, we create challenging out-of-distribution evaluation settings where agents are tested on their generalizability to websites or even entire domains never seen during training.This presents a representative evaluation of generalist agents working on unseen websites.</p>
<p>MIND2WEB enables us to conduct an initial exploration in using LLMs for building generalist web agents.The HTML document of real-world webpages may contain thousands of elements, which makes it infeasible or too costly to be fed into an LLM's context.To address this issue, we propose MINDACT, a two-stage model that involves first using a fine-tuned small LM to filter the web elements and then using an LLM to select from the filtered elements in a multi-choice question answering fashion and predict the corresponding action with the selected element.We show that MINDACT significantly outperforms modeling strategies commonly adopted by prior work and achieves a decent level of generalization.It can also work well with both open-source LLMs like Flan-T5 [10] through fine-tuning and closed-source LLMs like GPT-3.5-turbo and GPT-4 through in-context learning.However, there is still substantial room for further improvement towards generalist agents for the web.Promising future directions include integrating multi-modal information, reinforcement learning with feedback from real websites, and specialized LMs for web understanding and action taking.</p>
<p>MIND2WEB Dataset</p>
<p>Unlike existing datasets predominantly constructed within simulated environments [31,40], our objective is to bridge the gap between simulation and reality so that agents trained on our dataset can work on real-world websites out of the box.To achieve this, our approach for data collection adheres to the following principles.Firstly, instead of recreating websites in simulation, which often leads to oversimplified environments, we engage directly with real-world websites and capture snapshots of these environments.Secondly, we collate a diverse set of websites from varied domains and crowdsource realistic tasks that cover a wide range of functionalities provided by these websites.Finally, acknowledging the challenge of perfectly replicating the complexity of real-world environments, we strive to capture a comprehensive snapshot of each website and the full interaction trace, to the extent that all the tasks can be seamlessly replayed offline.This supports rich modeling and evaluation approaches, ensuring a robust and practical dataset for research.</p>
<p>Task Definition</p>
<p>The primary objective of MIND2WEB is for the agent to complete a specific task on the target website through a series of actions.Each instance in our dataset contains three components:</p>
<p>Task description, which outlines the high-level goal of the task.We intentionally avoid low-level, step-by-step instructions, aiming to foster the development of agents that can comprehend and carry out tasks in a more autonomous fashion, rather than merely following prescriptive directives.</p>
<p>Action sequence, which is the sequence of actions required to accomplish the task on the website.Each action in the sequence comprises a (Target Element, Operation) pair.The Target Element is an interactable element on the current webpage, and the Operation refers to the action to be executed on that element.We support three common operations: Click (also including Hover and Press Enter), Type, and Select Option.For Type and Select Option, they also require an additional value as an argument.Actions in a sequence often span multiple webpages of a website.Webpage snapshots, which constitute the environment within which the task is performed.We provide the snapshots in a variety of formats to accommodate different modeling approaches: selfcontained MHTML file that includes the raw HTML code of the webpage, DOM snapshot containing the DOM tree along with the layout and style information of the screenshot of the rendered webpage, HAR file that includes all the network traffic for replaying the interaction if needed, and trace file that comprises the complete interaction trace during the task annotation process.</p>
<p>The agent receives the task description in the beginning.At each step, it also receives the current webpage and the history of previous actions.The objective is to accurately predict the subsequent action, which encompasses the target element for interaction and the operation.</p>
<p>Data Collection</p>
<p>Our data collection process consists of four stages: website selection, task proposal, task demonstration, and task verification.Website selection and task verification are done by the authors.For task proposal and demonstration, we develop a sophisticated annotation tool using Playwright2 and hire annotators through Amazon Mechanical Turk.Refer to Supplementary for annotation tool details.</p>
<p>Website Selection.We start with 5 top-level domains: Travel, Shopping, Service, Entertainment, and Information, which are subsequently broken down into 31 (secondary) domains.We select websites within each domain based on their popularity in the US, as ranked by similarweb.com.We manually select 3-5 representative websites per domain, resulting in a collection of 137 websites in total.</p>
<p>Task Proposal.We present the annotators with a target website, a concise description of the website, and a few sample tasks associated with it.The annotators are then asked to propose open-ended and realistic tasks based on three criteria: the tasks should be of diverse types, require multiple rounds of interaction, and describe the high-level goal instead of step-by-step instructions.To further stimulate creativity and boost diversity, we use ChatGPT to generate seed tasks by prompting it to test different functionalities of a website.We generate 50 seed tasks per website, of which 10 are randomly sampled and presented to the annotator each time.These seed tasks are mainly for inspiration-annotators are explicitly instructed not to directly use them and we reject task proposals that are highly similar to the seed tasks.All the proposed tasks are further screened by the authors to ensure quality and diversity before entering the demonstration phase.</p>
<p>Task Demonstration.We develop a Playwright-based tool for demonstration (Figure 2).Workers will use the tool to demonstrate how to perform the tasks they have proposed within a web browser.</p>
<p>To ensure accuracy, each interaction round is split into two parts: element selection and operation selection.At each step, the worker first selects an element on the webpage by clicking within the browser.They are then asked to confirm the selection and choose the operation to execute on the selected element.Once the task is completed, the worker is given another opportunity to review and modify the task description.Task Verification.Lastly, all task demonstrations are verified by the authors to ensure the following: First, all actions are accurately reflected in the task description.The authors will modify the task description if needed to align it with the annotated actions; Second, the recorded actions are correct and clean, with extraneous steps discarded.Finally, the starting and ending points of tasks are consistent, such as excluding actions for closing popup windoes, or ending the annotation at the search result page if the task was to find a certain item without clicking on specific items.After verification, we discarded 61 out of the total 2,411 tasks.Among the 2,350 retained tasks, the task description was refined in 390 instances to better correspond with the demonstrated actions, while some extraneous steps were discarded in 187 instances.Overall, the data collection pipeline has been proven effective and produces high-quality data.</p>
<p>Comparison with Existing Work and Research Challenges</p>
<p>MIND2WEB presents a unique ensemble of research challenges for the development of generalist agents for the web in real-world settings.As shown in Table 1, MIND2WEB distinguishes itself from existing literature in several ways.Firstly, MIND2WEB spans across 137 websites from 31 domains, allowing comprehensive testing of an agent's ability in generalizing across varied environments.Secondly, we utilize real-world websites without manual simplification.Consequently, the included environments exhibit complexity far surpassing that encountered in previous studies, yet better reflecting the intricacy of the modern web.With an average of over 1,000 elements per page embedded within complex DOM structures, how to effectively process such long and highly structured documents presents a significant challenge for modeling.Lastly, we direct the annotators to propose open-ended tasks that explore different functionalities of the website to mimic genuine web usage.Meanwhile, contrary to prior studies [5,21,22,39] that provide step-by-step directives and primarily focus on testing the agent's ability to translate low-level instructions into actions, e.g., "Type New York in the location field, click the search button and choose the tomorrow tab," we opted for the setting where only high-level goals are available, e.g., "What is the weather for New York tomorrow?"This poses a much greater yet realistic planning and grounding challenge for the agent.</p>
<p>Method: MINDACT</p>
<p>Employing the data from MIND2WEB, we introduce an exploratory framework, MINDACT, for our task, leveraging the power of LLMs.Raw HTML documents, which could consist of thousands of elements, are either infeasible or cost-prohibitive to be directly fed into LLMs.We propose a two-stage process that synergizes the strength of small and large LMs, as shown in Figure 3.In the first stage, a fine-tuned small LM is used to rank the elements present on a webpage, yielding a small pool of promising candidates.In the second stage, these candidate elements are consolidated to form a representative snippet of the webpage, which is then processed by an LLM to predict the final action, including predicting both the element for interaction and the corresponding operation.</p>
<p>Candidate Generation with Small LMs</p>
<p>Given the task description, the snapshot of the webpage at step t, and the actions performed in the preceding t − 1 steps, we treat candidate generation as a ranking task.The task is to select the top-k   candidate DOM elements from the webpage that best align with both the task description and the current step.We formulate the task query by concatenating the task description with previous actions.The textual representation of each candidate DOM element is derived from a combination of the element's tag, its textual content, and salient attribute values, as well as the textual representation of its parent and child elements.As shown in Figure 4, we pair each DOM element with the task query and feed it to an encoder-only LM through the cross-encoder architecture [28], yielding a matching score.At training time, we randomly sample negative elements from the webpage, and use the target element as the positive example.The matching score is passed through a sigmoid activation function and optimized with a binary cross entropy loss.At inference time, we score all elements in the webpage and pick the top-k elements with the largest logits as input to the second stage.</p>
<p>Action Prediction with LLMs</p>
<p>After obtaining the top-k candidates, we utilize the candidate set to prune the webpage snapshot and construct snippets that only include the selected candidates and their neighbours as inputs to an LLM.</p>
<p>Recent studies [10,13] have suggested that training LMs for discrimination rather than generation is more generalizable and sample-efficient for other grounding tasks.Inspired by that, we convert the task of element selection into a multi-choice question answering (QA) problem.Instead of generating the complete target element, the LM is trained to instead select from a list of options.For comparison, we also include a baseline that directly generates the target element based on the provided webpage snippet.In both cases, we directly let the LLM generate the operation, along with the additional value needed for some operations.An example is shown in Figure 5.We incorporate up to 5 candidate elements within each input, together with a None option, and partition the candidate set into several groups.During training, we construct the target sequence using ground-truth actions and fine-tune the model using a left-to-right language modeling objective.During inference, we divide the top-k candidates into multiple clusters of five options.If more than one option is selected after a round, we form new groups with the selected ones.This process repeats until a single element is selected, or all options are rejected by the model, i.e., the model chooses the None option for all groups.</p>
<p>Data Preprocessing and Evaluation</p>
<p>We apply simple heuristics to clean the raw HTML documents, keeping only elements that are visible and carry substantial semantic meaning, as determined by their attributes, textual content, and neighboring elements.This effectively reduces the average number of elements from 1,135 to 580, while still maintaining an overall recall of 94.7% for the target element in the training data.</p>
<p>For evaluation, we first calculate Element Accuracy that compares the selected element with all acceptable elements, and Operation F1 that calculates token-level F1 score for the predicted operation.This is the same as accuracy for Click, but considers the correctness of the input value for Type and Select Option.Each step of the task is evaluated independently with the ground truth action history provided.We then define Step Success Rate and Success Rate (for the whole task).A step is regarded as successful only if both the selected element and the predicted operation are correct.</p>
<p>A task is regarded successful only if all steps have succeeded.It is therefore a stringent metric.For step-wise metrics, we report macro average across tasks.</p>
<p>Results</p>
<p>Candidate Generation.We fine-tune DeBERTa [16]  Action Prediction.We mainly compare against two baselines in Table 2.The first directly uses the candidate generation model (DeBERTa) for element selection, which is similar to existing work [14,35] that combines an encoder with classification heads.However, such a design cannot .We observe a substantial gain with MINDACT using the multi-choice QA formulation.The best model achieves 52.0% step success rate under Cross-Task setting, and 38.9% / 39.6% when generalizing to unseen websites and domains.However, the overall task success rate remains low for all models, as the agent often commits at least one error step in most cases.</p>
<p>Three Levels of Generalization.All models perform best on the Cross-Task setting, with over 10% absolute gap (step SR) on average compared with Cross-Website and Cross-Domain settings, indicating that generalizing to unseen environments is still a major challenge.On the contrary, we note that the performance of Cross-Website and Cross-Domain settings are notably similar, which is also reinforced in Figure 6, where there is no clear distinction in performance across these settings.This suggests that the challenges primarily stem from the diversity in website designs and interaction logic rather than domain specifics.Tasks across domains tend to share common operations, and pretrained LMs may already have the capability to decompose complex tasks at a high level based on commonsense knowledge.Yet, grounding such knowledge into actionable steps in specific and varying environments remains a considerable challenge.</p>
<p>In-context Learning with LLMs.We also experiment with two popular LLMs, GPT-3.5-turbo and GPT-4 [25], through in-context learning.We use the same multiple-choice formulation as MINDACT, and include three demonstration examples for in-context learning.We can see that both models are comparable to the two baselines with only three in-context examples.Note that this is not a fair comparison with the Flan-T5 models, which are fine-tuned on the full training data.We also include the zero-shot results with Flan-T5 XL in Appendix D.2, but the model fails to perform the task without fine-tuning.Meanwhile, GPT-3.5 only has around 20% element selection accuracy, despite the superior performance people have observed on other datasets.Further analysis reveals that one possible problem is the model's propensity to select the None option, asserting that the task cannot be finished on the current webpage.This is somewhat accurate since tasks typically necessitate navigation through multiple webpages and performing a series of actions before reaching the final result.This aspect indeed represents the primary difficulty of our task.On the other hand, we observe highly promising outcomes with GPT-4.The performance is on par with the tuned Flan-T5 models under Cross-Website and Cross-Domain settings for element selection, indicating a great potential for developing generalist agents using LLMs.Nevertheless, GPT-4's high operational cost remains a concern.Developing smaller models specialized for the web is an interesting future avenue.</p>
<p>Related Work</p>
<p>Autonomous Agents for Web and Mobile Applications.Considerable initiatives have been invested in automating web navigation, driven by a vision of facilitating effortless human-web interaction.Yet, previous research has been limited by the types of tasks and websites it can handle, either confined to simplified simulation environments [22,31,40], or limited to a narrow set of domains and websites [39,40].Recent studies [5,21,35] have utilized similar techniques for mobile applications, however, these are often simpler and offer fewer functions compared with full-fledged websites.In contrast, MIND2WEB aims to adapt to a realistic web environment, characterized by its high diversity.</p>
<p>Also related is the research on web automation systems [1,19].These technologies often demand programming skills, which can make them less accessible to general users.We aim to equip the web automation system with a natural language interface, thereby reducing the entry barrier significantly.</p>
<p>Large Language Models.In recent years, there has been a surge in the development and application of large language models (LLMs).These models, often encompassing billions of parameters, are pre-trained on massive corpora of text data [3,44,45], enabling them to capture intricate linguistic patterns, nuances, and relationships, resulting in unprecedented performance on a wide array of NLP tasks.One of the most noteworthy attributes of LLMs is their few-shot learning capability.Unlike traditional machine learning models that necessitate extensive labeled data for task-specific fine-tuning, LLMs can often perform tasks with minimal task-specific examples.Furthermore, LLMs such as GPT-3 [4] and PaLM [9] have also demonstrated the ability to do in-context learning, where they can adapt to novel tasks by simply providing context within the input prompt, eliminating the need for explicit retraining.In this work, we explore the use of LLMs to build generalist agent on top of MIND2WEB by either tuning medium-sized LMs with only around 1,000 examples, or prompting an LLM such as GPT-4, and have observed promising results.</p>
<p>Grounded Language Understanding.Our work also aligns with the field of grounded language understanding, which aims to map natural language utterances onto executable plans in a target environment [13].Many studies have centered around environments underpinned by a well-structured schema or ontology, including relational databases [36,43] and knowledge bases [12,42], which may not adequately reflect the more heterogeneous conditions in real-world situations.Our work instead grounds natural language in the noisy and schemaless web environment.Our setting is also connected to embodied AI, where an agent, guided by language instructions, carries out tasks in a physical environment [2,32,33].Nonetheless, existing research primarily focuses on a specific setting (e.g., household environments), limiting their diversity.MIND2WEB provides a unique testbed for studying a broad range of grounding challenges in real-world environments.</p>
<p>Tool Learning.Recent developments have underscored LLMs' potential in using a myriad of tools (i.e., taking actions) to augment their capacity [23,27], including search engine, translator, calculator, etc. Example works include Toolformer [29], ReAct [41], and ToolkenGPT [15].The creation of recent benchmarks on tool learning [20,26] further highlights the growing interest in evaluating LLMs' proficiency in tool usage.However, existing research primarily concentrates on short-term tool invocation, neglecting long-term planning.MIND2WEB can bridge this lacuna by necessitating LLMs to take actions within realistic web-browsing environments that demand prolonged decisionmaking sequences.Furthermore, MIND2WEB may stimulate the development of more advanced tools based on LLMs that interface the web with natural language.These advanced tools could be subsequently employed by another LLM for more challenging problem-solving tasks [11,30].</p>
<p>Limitations and Potential Societal Impact</p>
<p>MIND2WEB is designed to facilitate the development and evaluation of generalist agents for the web.Such agents hold great potential for making the web more accessible and easy to use, especially for individuals who are less familiar with information technology or have disabilities and may struggle to navigate through complex web apps and get overwhelmed by the options available.However, there are still potential concerns and limitations regarding the current data collection, system design and safety for deployment in real world.</p>
<p>Diversity and Representation in Data Collection.Although we strive to choose representative websites covering diverse domains, the present selection predominantly comprises English-language websites primarily used in the U.S.Meanwhile, all our annotators are sourced through the Amazon MTurk platform, which might be biased towards a group that is more proficient in web use.Therefore, the tasks and websites embodied in our dataset may represent only a subset of all potential tasks that can be performed on the web.Bearing this limitation in mind, the design of MIND2WEB and our data collection protocol allow for easy expansion to encompass more tasks and websites.The inclusion of additional websites, potentially from different countries and languages, and tasks from more diverse demographics, such as individuals from different age groups, those traditionally facing web accessibility challenges, and professionals from specific domains like software development, research, law, and more, present exciting directions for future development.</p>
<p>Use of Multimodal Information.Our current approach, MINDACT, models the web environment using only textual context from webpage snapshots.Nevertheless, crucial information can also be gleaned from the visual representation of a rendered webpage.While not currently utilized, we have included complete webpage snapshots in MIND2WEB, enabling rendering of the webpage for visual interpretation.The use of this multimodal information will be a viable prospect for improving model performance.</p>
<p>Modeling of Interaction Dynamics.In MINDACT, we encode each webpage independently at every step, with only the previous actions provided as historical context.However, the changes of the web environment could also provide significant cues for task completion, such as the appearance of a dropdown menu following a button click.Exploring effective ways to model such dynamic environment transformations during interaction could be an essential aspect for developing robust web agents.</p>
<p>Human-Agent Interaction.In the current design of MIND2WEB, the user provides a single description of the task goal up front, and the agent carries out the task from start to finish.In real-world settings, the user may wish to adjust or add task requirements in the middle, or the agent might seek user confirmation for more accurate task understanding.Extending Mind2Web to an interactive or conversational setting, thereby allowing diverse forms of human-agent interactions, could be an interesting future direction.</p>
<p>Evaluation with Offline/Online Environments.Following recent works [5,35], we evaluate the system with cached offline environments, which allows us to test using snapshots of complex realworld websites.However, a downside to this is that the task will fail immediately if an action was not cached during data collection, potentially leading to false negatives due to the existence of multiple paths for completing the same task.As described in Appendix C.1, we normalize the actions to address equivalent elements within the same page.In addition, we include complete network traffic in the dataset, presenting possibilities for future research to enable some degree of replay and exploration within the cached environment.Given that MIND2WEB faithfully replicates real-world webpages, systems trained on the dataset should be readily transferable to live websites.Conducting end-to-end live evaluation on real websites with human assistance is a very promising direction that is worth exploration.</p>
<p>Safety in Deployment.While the development of general-purpose web agents holds great potential to enhance efficiency, optimize user experiences, and promote web accessibility universally, the accompanying safety considerations for real-world deployment cannot be ignored.These include how to effectively manage sensitive actions like financial transactions, enhancing transparency and interpretability, and keeping users in control during task execution.Additionally, there is the risk of these agents possessing the capability to breach existing security measures such as CAPTCHA and being exploited for malicious activities, such as disseminating false information.Therefore, it is also important for cybersecurity research to consider these potential uses and develop preemptive protective measures.</p>
<p>Conclusion</p>
<p>In this work, we introduced MIND2WEB, the first dataset for developing and evaluating generalist agents for the web.We also proposed MINDACT, an agent that leverages the power of (large) language models for effectively tackling this task.Our work opens up a wide range of promising future directions, including integrating multi-modal information, reinforcement learning with feedback from real websites, and specialized LMs for web understanding and action taking.We hope that MIND2WEB will serve as a valuable platform for the research community to advance towards generalist agents for the web.</p>
<p>policies, either expressed or implied, of the U.S. government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p>
<p>Table 3: Prompt for ChatGPT to generate seed tasks to inspire the annotators during task proposal.</p>
<p>Prompt</p>
<p>We are testing the website https://aa.com| American Airlines -Airline tickets and low fares at aa.com Book low fares to destinations around the world and find the latest deals on airline tickets, hotels, car rentals and vacations at aa.com.As an AAdantage member you earn miles on every trip and everyday spend.</p>
<p>We want to test practical daily tasks that a user would do on the website.</p>
<p>Come up with a list of 5 example tasks and try to cover different cases.Requirements:</p>
<p>-Each example should be a single sentence and not just click one of the elements.</p>
<p>-Don't give step-by-step instructions or directly mention the element to interact.</p>
<p>-Describe the goal of the task and provide concrete information or constraints.Use mock-up information (identifier, number, personal information, name, date, attributes, constraints, etc.) to make the task more specific and realistic.</p>
<p>Sample Responses Retrieve the confirmation number for a reservation made under the name jane smith departing from dallas (dfw) to miami (mia) on january 20th.</p>
<p>Find and book a round-trip flight from new york to london departing on december 15th and returning on december 22nd for under $800.</p>
<p>Find the lowest fare for a one-way flight from new york city (jfk) to los angeles (lax) leaving on august 15th and returning on august 20th, with no layovers exceeding 2 hours.</p>
<p>Rent a car in honolulu for one week starting on august 1st that can fit at least four passengers and has gps navigation included.</p>
<p>Cancel a car rental reservation for a compact car in orlando, florida that was scheduled to be picked up on may 25th at 5 pm without incurring any cancellation fees.</p>
<p>B Data Collection Details B.1 Crowdsourcing Details</p>
<p>In this study, we employ annotators from the Amazon Mechanical Turk (mturk) platform.The annotators are required to have a minimum of 1,000 approved HITs with an approval rate exceeding 98% on the platform.We design the compensation with an estimated hourly rate of $10.10 to respect the minimum wage guidelines in Ohio, United States.Every worker passing our qualification receives a bonus, and we pay $0.80 for each approved final task.We do not collect any identifiable private information during the study, and explicitly instruct the annotators to refrain from entering personal or sensitive data into the system.Annotators engage with our annotation tool only within a secure, remote sandbox environment, posing no foreseeable harm.The study complies with the IRB exemption criteria, per the Office of Responsible Research Practices at The Ohio State University.All annotators are presented with a consent form, to which they must agree before participating in the study.To prepare the workers for the task, we provide a comprehensive training document and a video tutorial, followed by a qualification assessment comprising a questionnaire and a series of test demonstrations using our tool.It is noteworthy that the task is divided into two phases: task proposal and task demonstration.The proposal phase comes with a nominal reward, with the majority of the compensation dispensed upon successful completion of the demonstration.</p>
<p>Quality and diversity are ensured through a two-stage review process.The first author reviews all tasks after task proposal and manually select the tasks for demonstration.After task demonstration, a thorough final verification of all collected data is conducted by all authors to authenticate the tasks and recorded actions.Each demonstration is first verified by one of the authors, and uncertain ones are further verified by the first author to reach consensus.</p>
<p>B.2 Task Proposal</p>
<p>We use ChatGPT to generate sample tasks to provide inspiration to the annotators, and the prompt used is shown in Table 3.For each HIT, we ask the annotator to select a website of their interest first.Following this, we present them with ten sample tasks produced by ChatGPT, and request them to propose a maximum of five additional tasks.The annotator is instructed not to directly copy the sample tasks.We manually evaluate all submitted tasks and reject those that demonstrate low quality or are too similar to previously accepted tasks.We set a nominal reward of $0.05 for each task proposal HIT, and the annotator will receive it no matter whether the tasks are accepted or not.For accepted ones, the annotator will receive the full reward of $0.80 on successful demonstration of the task.Once we have collected a total of around 20 tasks for a specific website, we desist from showing it to the user, aiming for a balanced distribution among websites and increased task diversity.</p>
<p>B.3 Task Demonstration</p>
<p>We develop a dedicated annotation tool for task demonstration using Playwright, 3 which allows us to interact with the browser and record user actions.As shown in Figure 7. the tool is composed of two windows.The dialogue window on the left serves as the annotator's control panel for guiding the interaction flow and choosing operations.The browser window on the right is where the annotator navigates the website and selects elements for interaction.Figure 8 shows the overall procedure for task demonstration.The annotator starts by selecting the website and task to be demonstrated.Once selected, the tool will bring up the website in the browser.The annotator is then instructed to explore the website and practice the task.To collect clean actions during actual demonstration, the workers are asked to close pop-up windows during exploration.We also provide anonymous accounts for the workers to use so that no private information is entered.The exploration stage is  not recorded and primarily serves to familiarize the annotator with the website and task, as well as to prepare the website to prevent future pop-ups, thereby ensuring a clean, streamlined set of final recorded actions.After exploration, the annotator is directed to return to the homepage and reset any altered values, allowing us to begin the demonstration in a fresh state.During the demonstration, the annotator will illustrate how to accomplish the task step-by-step using both the browser and the dialogue window.To ensure a clean set of annotated actions, annotators are restricted from directly engaging with the browser during the demonstration phase.Instead, we divide each action step into two stages: Element selection and operation selection.At each step, the annotator first selects the target element by clicking it in the browser.We will highlight the selected element in the browser window but block the actual click event.The annotator is then prompted to select the operation to perform within the dialogue window, which is then carried out by the annotation tool in the browser.We provide 6 operations: Click, Type, Hover, Press Enter, Click (Fake) and Ignore.For the Type operation, the annotator is additionally required to supply the value as shown in Figure 9.If the chosen element is a select HTML element, and the annotator opts for Click, it translates to a Select Option operation and we will prompt the annotator to select one of the options as shown in Figure 10.To avoid ambiguity, the Click, Hover and Press Enter operations are all mapped to Click in the final dataset.Click (Fake) is a special operation.It will be recorded the same as a normal Click but will not get executed in the browser.This is designed for safeguarding against state-changing actions (i.e., actions that produce side effects to the world), such as posting a comment or scheduling an appointment, since it will interfere with other real users of the website.In practice, once a model predicts Click (Fake), it may prompt the user for confirmation before executing such state-changing actions.Finally, the annotator can also choose Ignore in case they select a wrong element.Once all the actions have been annotated, the annotator can choose to complete the task.They will then be asked to confirm the task description again and make any necessary modifications.</p>
<p>Pop-ups and CAPTCHAs.In this study, we emphasize on clean and direct task execution actions, intentionally omitting extraneous steps like pop-ups and CAPTCHAs that might introduce ambiguity in evaluation.We carefully select only those websites that pose no access issues when used with our tool.Before recording the task demonstration, annotators are requested to familiarize themselves with the website, and preemptively close pop-up windows and clear CAPTCHAs to avoid their recurrence during the actual demonstration.Annotators are further guided not to engage in extra steps such as closing ads unless necessary during the task demonstration.In the final task verification, we revisit the actions and filter out those unrelated to direct task execution.At the same time, we acknowledge that these instances constitute a significant aspect of the dynamic web environment in the real world.Enhancing systems to robustly tackle such scenarios on-the-go could form an interesting avenue for future research.</p>
<p>Mitigating Disruptions on the Websites.The annotator are advised against actions that could potentially interfere the normal operation of the website.To handle tasks such as scheduling appointments, we introduce a Click (Fake) operation that annotators can utilize to indicate the action without actually executing it on the website.</p>
<p>B.4 Task Verification</p>
<p>All collected data undergoes an additional verification process conducted by the authors, as demonstrated in Figure 11.The verification interface is shown in Figure 11.This verification consists of three tasks.Firstly, we evaluate whether a task should be discarded due to its low quality.Secondly, we examine each step to determine if any action should be discarded.This includes reviewing the initial and final actions of the task, and excluding any additional actions (e.g., closing ads) that are not outlined in the task description to ensure consistency across task annotations.Finally, we verify the task description to confirm that all actions are accurately represented and make modifications if necessary.If there is uncertainty regarding any action, the verifier can opt for the 'unsure' option, prompting a re-evaluation by the first author.</p>
<p>C Experiment Details</p>
<p>C.1 Evaluation</p>
<p>One complication that arises during evaluation on real-world websites is that multiple elements on a webpage may induce the same effect.For instance, a button might house a text span within it, both of which, when clicked, yield identical results.To enhance the robustness of our evaluation, we employ heuristics to detect elements equivalent to the ground truth.We first examine the ancestors of the labeled element to identify potential higher-level elements acceptable for the current action.We employ a straightforward heuristic that locates the nearest clickable element to the ground truth, including itself.After identifying the top-level acceptable element, we include all its visible descendants that are located within its post-rendering bounding box as acceptable as well.Manual checking on 100 instances where the heuristic identifies a top-level element other than the ground truth confirms the validity of the approach.For both training and evaluation stages, all acceptable elements are considered positive.</p>
<p>C.2 Model Implementation Details</p>
<p>Candidate Generation.We use the Cross-Encoder implementation from Sentence-Transformers4 and use DeBERTa as the backbone model.More specifically, we use DeBERTa-v3-base5 for our experiments.</p>
<p>Action Prediction.We use the Seq2Seq model implementation from Transformers [38].We experiment with the base6 , large7 and xl8 versions of Flan-T5 [10].LLM In-context Learning.We use the OpenAI API for in-context learning with LLMs.We experiment with two versions of GPT models: gpt-3.5-turboand gpt-4.We include three demonstration examples for in-context learning.The complete prompt is shown in Table 8.</p>
<p>Training Details.The flan-t5-xl and flan-t5-large models are trained on servers with 4*A100 80GB cards provided by Ohio Supercomputer Center [6].All other models are trained with single A6000 48GB cards.</p>
<p>Please see Table 4 for all hyperparameters used in our experiments.</p>
<p>D Additional Results</p>
<p>D.1 Effect of Random Grouping Elements for Action Prediction</p>
<p>For both training and inference, we shuffle the elements in the webpage and randomly group them into multi-choice questions.The model might give different predictions when presented with different sets of choices, and leads to slightly different final evaluation scores.Here we show the average and standard deviation of 5 runs with different random seeds to show the effect of random grouping.As we can see from Table 5, the selection of choices only lead to small changes in overall performance with standard deviation less than 1 for all runs.</p>
<p>D.2 Zero-shot Results for Flan-T5 XL</p>
<p>Since Flan-T5 is tuned with multi-choice format, it can also do element selection in zero-shot.However, as we can see from Table 6, while the model still gets some elements correct, it is much lower compared to the fine-tuned model, and 3-shot GPT 3.5/4.This is expected, since Flan-T5 is not tuned for HTML and coding related tasks.</p>
<p>(a) Find one-way flights from New York to Toronto.(b) Book a roundtrip on July 1 from Mumbai to London and vice versa on July 5 for two adults.(c) Find a flight from Chicago to London on 20 April and return on 23 April.(d) Find Elon Musk's profile and follow, start notifications and like the latest tweet.(e) Browse comedy films streaming on Netflix that was released from 1992 to 2007.</p>
<p>Figure 1 :
1
Figure 1: Sample tasks and all domains featured in MIND2WEB.The array of diversity allows for testing an agent's generalizability across tasks on the same website (a vs. b), similar tasks on different websites (a vs. c), and even to entirely disparate tasks, websites, and domains (d−f).</p>
<p>Figure 2 :
2
Figure 2: A sample data instance of our dataset with the three components.Actions marked in red will result in a transition to a new webpage.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: The overall pipeline for MINDACT with a small ranking LM for candidate generation, and a large prediction LM for action prediction.</p>
<p>Figure 5 :
5
Figure 5: Illustration of action prediction with LLMs.</p>
<p>Figure 6 :
6
Figure 6: Step success rate per website grouped by the three splits: Test Cross-Task , Test Cross-Website and Test Cross-Domain from left to right.Here we only show websites with more than three test tasks.</p>
<p>Figure 7 :Figure 8 :
78
Figure7: Illustration of our annotation tool, which consists of two side-by-side windows.On the left we provide a dialogue window for the user to control the tool and select operations to take.On the right we provide the browser window for the user to interact with and select web elements.</p>
<p>Figure 9 :
9
Figure 9: Dialogue window for the Type operation.</p>
<p>Figure 10 :
10
Figure 10: Dialogue window for the Select Option operation.</p>
<p>Figure 11 :
11
Figure 11: Illustration of our verification tool.</p>
<p>Table 1 :
1
Statistics of MIND2WEB compared with existing datasets.</p>
<h1>Dom. # Env.Env. TypeAvg. # Elements# TasksTask Info.Avg. # ActionsMiniWoB++ [22]−100Simplified mobile websites28100Low-level3.6WebShop [40]11Simplified shopping websites3812,000 productsHigh-level11.3RUSS [39]−22Real-world websites80180High &amp; low5.4PixelHelp [21]44Mobile apps−187High &amp; low-META-GUI [35]611Mobile apps791,125 dialoguesHigh-level4.3MoTIF [5]15125Mobile apps188756High &amp; Low4.4MIND2WEB5 / 31137Real-world websites1,1352,350High-level7.3</h1>
<p>Table 2 :
2
Main results.The classification baseline uses DeBERTa B and the generation baseline uses Flan-T5 B .For step-wise metrics, we report macro average across tasks.<em>For GPT-4 we use 50 tasks for each setting with top-10 candidates due to limited budget.See Appendix D.3 for results on the 50 tasks subsets for all methods.The diversity of MIND2WEB provides a unique opportunity to evaluate an agent's generalizability at different levels.We seek to understand how well an agent can generalize across domains, websites, and tasks: Test Cross-Domain , for which we hold out two top-level domains, Information and Service, with 912 tasks from 73 websites.Here, the model is expected to generalize to an entirely new domain without having seen any websites or tasks associated with that domain during training.Test Cross-Website , with 10 websites from each remaining top-level domain, containing 177 tasks.In this setting, the model is never exposed to the test websites during training.However, it has been trained on websites from the same domain and possibly with similar tasks.This setup allows us to assess an agent's ability to adapt to entirely new websites, yet within familiar domains and task contexts.Test Cross-Task , where we randomly split 20% of the remaining data, regardless of domains and websites, resulting in 252 tasks from 69 websites.In this setting, the model has been exposed to webpages from the same website during training and has likely encountered similar tasks.The rest of data is used for training, which contains 1,009 tasks from 73 websites.
Cross-TaskCross-WebsiteCross-DomainEle. Acc Op. F1 Step SR SR Ele. Acc Op. F1 Step SR SR Ele. Acc Op. F1 Step SR SRClassification26.8−−−21.6−−−24.5−−−Generation20.252.017.50.013.944.711.00.014.244.711.90.4MINDACTw/ Flan-T5 B43.676.841.04.032.167.629.51.733.967.331.61.6w/ Flan-T5 L53.475.750.37.139.267.135.31.139.767.237.32.7w/ Flan-T5 XL55.175.752.05.242.065.238.95.142.166.539.62.9w/ GPT-3.520.356.617.40.819.348.816.20.621.652.818.61.0w/ GPT-4  </em>41.660.636.22.035.851.130.12.037.146.526.42.04 Experiments4.1 Experimental Setup</p>
<p>as the small LM for candidate generation.As candidate generation requires high efficiency, we use the base version DeBERTa B with 86M parameters.Overall, it achieves 88.9% / 85.3% / 85.7% Recall@50 on Test Cross-Task , Test Cross-Website and Test Cross-Domain , respectively.We use its top-50 ranking results as the candidate pool for all subsequent experiments.</p>
<p>Table 4 :
4
Hyperparameters used in experiments.
Method ModelHyperparamertersCandidate Generation deberta-v3-basebatch_size:32, epoch:5,learning_rate:3e−5Action PredictionGeneration flan-t5-basebatch_size:32, epoch:5,learning_rate:5e−5MINDACT flan-t5-base,batch_size:32, epoch:5,flan-t5-large,learning_rate:5e−5flan-t5-xlgpt-3.5-turbo,temperature:0, # demonstrations:3gpt-4</p>
<p>Table 5 :
5
Step Success Rate for Flan-T5 models with different groups of options.Here we shown mean and standard deviation of 5 runs with different random seeds.
Cross-Task Cross-Website Cross-DomainFlan-T5 B41.5±0.730.0±0.831.3±0.5Flan-T5 L49.9±0.235.7±0.536.7±0.3Flan-T5 XL51.9±0.839.5±0.239.6±0.2</p>
<p>Table 6 :
6
Zero-shot element selection results for Flan-T5 XL compared with the fine-tuned counterpart.
Cross-Task Cross-Website Cross-DomainFlan-T5 XL Zero-Shot10.87.811.7Flan-T5 XL Fine-Tuned52.038.939.6
https://playwright.dev/
https://playwright.dev/
https://www.sbert.net/examples/applications/cross-encoder/README.html
https://huggingface.co/microsoft/deberta-v3-base
https://huggingface.co/google/flan-t5-base
https://huggingface.co/google/flan-t5-large
https://huggingface.co/google/flan-t5-xl
AcknowledgementsThe authors would thank colleagues from the OSU NLP group for constructive feedback and all contributors from the Amazon Mechanical Turk platform who participated in our study and assisted in data collection.This research was sponsored in part by NSF OAC 2112606, NSF CAREER #1942980, ARL W911NF2220144 and Ohio Supercomputer Center[6].The views and conclusions contained herein are those of the authors and should not be interpreted as representing the officialA OverviewOur supplementary includes the following sections:• Section B: Data Collection Details.Details for crowsourcing and implementation details for the three data collection phases: task proposal, task demonstration, and task verification.• Section C: Experiment Details.Details for evaluation and model implementation.Continued on next page
Do as I can, not as I say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alexander Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Yan, 10.48550/arXiv.2204.016912022</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ B Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen Chen, Jared Quincy Creel, Dorottya Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark S Koh, Ranjay Krass, Rohith Krishna, Kuditipudi, abs/2108.072582021CoRR</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Plummer. A dataset for interactive vision-language navigation with unknown command feasibility. Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, Bryan A , European Conference on Computer Vision. 2022</p>
<p>Ohio supercomputer center. Supercomputer Ohio, Center, 1987</p>
<p>How many websites are there? how many are active in 2023?. Radoslav Chakarov, </p>
<p>Reading Wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171Long Papers)</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel2023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Y Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/arXiv.2210.114162022</p>
<p>Openagi: When LLM meets domain experts. Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, Yongfeng Zhang, 10.48550/arXiv.2304.043702023</p>
<p>three levels of generalization for question answering on knowledge bases. Yu Gu, Sue Kase, Michelle Vanni, Brian M Sadler, Percy Liang, Xifeng Yan, Yu Su, I I D Beyond, 10.1145/3442381.3449992WWW '21: The Web Conference 2021, Virtual Event / Ljubljana. Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, Leila Zia, SloveniaACM / IW3C2April 19-23, 20212021</p>
<p>Don't generate, discriminate: A proposal for grounding language models to real-world environments. Yu Gu, Xiang Deng, Yu Su, 10.48550/arXiv.2212.097362022</p>
<p>Understanding html with large language models. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust, 2023</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu, 10.48550/arXiv.2305.115542023</p>
<p>Deberta: decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Structgpt: A general framework for large language model to reason over structured data. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, 10.48550/arXiv.2305.096452023</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineNovember 2020Association for Computational Linguistics</p>
<p>CoScripter: Automating &amp; sharing how-to knowledge in the enterprise. Gilly Leshed, Eben M Haber, Tara Matthews, Tessa Lau, 10.1145/1357054.1357323Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsFlorence ItalyACMApril 2008</p>
<p>Api-bank: A benchmark for tool-augmented llms. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, 10.48550/arXiv.2304.082442023</p>
<p>Mapping natural language instructions to mobile UI action sequences. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge, 10.18653/v1/2020.acl-main.729Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel R Tetreault, the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020Association for Computational LinguisticsJuly 5-10, 2020. 2020</p>
<p>Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, Percy Liang, 6th International Conference on Learning Representations, ICLR 2018. Vancouver, BC, CanadaApril 30 -May 3, 2018. 2018Conference Track Proceedings. OpenReview.net</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 10.48550/arXiv.2302.078422023</p>
<p>. Openai, Chatgpt plugins</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Gorilla: Large language model connected with massive apis. G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, 10.48550/arXiv.2305.153342023</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun, 10.48550/arXiv.2304.08354CoRR2023</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics201911</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/arXiv.2302.047612023</p>
<p>Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, 10.48550/arXiv.2303.175802023</p>
<p>World of Bits: An Open-Domain Platform for Web-Based Agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLRJuly 2017</p>
<p>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, 10.1109/CVPR42600.2020.010752020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. Seattle, WA, USAJune 13-19, 2020. 2020Computer Vision Foundation / IEEE</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, 10.48550/arXiv.2212.040882022</p>
<p>Building natural language interfaces to web apis. Yu Su, Ahmed Hassan Awadallah, Madian Khabsa, Patrick Pantel, Michael Gamon, Mark J Encarnación ; Chee Fu, Jimeng Sun, J Shane Culpepper, Eric Lo, Joyce C Ho, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017. Aixin Sun, Vincent S Tseng, Chenliang Li, the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017SingaporeNovember 06 -10, 2017</p>
<p>. 10.1145/3132847.31330092017ACM</p>
<p>Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, Kai Yu, Meta-Gui, Towards Multi-modal Conversational Agents on Mobile GUI. November 2022</p>
<p>RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson, 10.18653/v1/2020.acl-main.677Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Automatic task completion flows from web apis. Kyle Williams, Seyyed Hadi Hashemi, Imed Zitouni, 10.1145/3331184.3331318Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019. Benjamin Piwowarski, Max Chevalier, Éric Gaussier, Yoelle Maarek, Jian-Yun Nie, Falk Scholer, the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019Paris, FranceACMJuly 21-25, 2019. 2019</p>
<p>Transformers: State-of-theart natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOctober 2020</p>
<p>Grounding open-domain instructions to automate web support tasks. Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James A Landay, Monica S Lam, North American Chapter. the Association for Computational Linguistics2021</p>
<p>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, 10.48550/arXiv.2207.01206July 2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 10.48550/arXiv.2210.036292022</p>
<p>Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base. Wen-Tau Yih, Ming-Wei Chang, Xiaodong He, Jianfeng Gao, 10.3115/v1/P15-1128Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir R Radev, 10.18653/v1/d18-1425Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober 31 -November 4, 2018. 2018</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 10.48550/arXiv.2303.182232023</p>
<p>Table 7: Step Success Rate for all methods on the 50 tasks subsets we used to evaluate GPT-4. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S Yu, Lichao Sun, 10.48550/arXiv.2302.09419https://doi.org/10.48550/arXiv.2302.094192023A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt. Numbers in parentheses are the results on the full test set (same as Table 2) Cross-Task Cross-Website Cross-Domain Flan-T5 B</p>
<p>Results on the 50 task subsets Due to budget constraint, we only run GPT-4 on 50 tasks for each setting. Here we show the step success rate results for other methods on the same 50 examples that GPT-4 is tested on. As we can see from Table 7, the results on the 50 tasks subsets are consistent with the results on the respective full test set, and the relative performance across methods and splits remains the same</p>            </div>
        </div>

    </div>
</body>
</html>
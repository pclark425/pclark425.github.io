<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6540 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6540</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6540</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-263831532</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.04743v2.pdf" target="_blank">RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought (CoT) has impressively unlocked the reasoning potential of large language models (LLMs). Yet, it falls short when tackling problems that require multiple reasoning steps. This limitation arises from the complex nature of multi-step reasoning processes: later stages often depend not only on the immediately preceding step, but also on the results from several steps earlier. Such complexities indicate the reasoning process is naturally a graph. The almost linear structure of CoT, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (ResPrompt), a new prompting strategy that advances multi-step reasoning in LLMs. The core of our idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections–links present in reasoning graph but missing in the linear CoT flow–into the prompts. Termed “residual connections”, these links can transform linear CoT into the complex reasoning graphs that multi-step problems entail. On benchmarks across math, sequential, and commonsense domains, ResPrompt demonstrates clear improvements in multi-step reasoning compared with CoT. Through extensive ablation studies and analyses, we pinpoint how to effectively build residual connections and also identify situations where it might be unnecessary.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6540.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6540.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RESPROMPT (LLaMA-65B, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual Connection Prompting (evaluated on LLaMA-65B, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that explicitly repeats intermediate results (residual connections) in few-shot prompts to reconstruct graph-like dependencies and improve multi-step reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT (Residual Connection Prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with explicit graph-like residual connections</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school multi-step arithmetic word problems (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>58.4</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>RESPROMPT reconstructs missing prerequisite links in CoT by verbatim reusing intermediate-result tokens; shows notable gains on multi-step math (especially for questions with ≥5 steps). Ablations show full coverage of residuals across steps is critical and later-stage residuals are more important than early-stage ones.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6540.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RESPROMPT (LLaMA2-70B, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual Connection Prompting (evaluated on LLaMA2-70B, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same RESPROMPT prompting strategy evaluated on LLaMA2-70B, demonstrating improvements over CoT though smaller relative gains than with LLaMA-65B on aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT (Residual Connection Prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with explicit graph-like residual connections</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school multi-step arithmetic word problems (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>65.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>1.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>RESPROMPT yields positive gains on the larger LLaMA2-70B model, with more modest average relative gains across benchmarks compared to LLaMA-65B; gains increase with question step-count and model scale (emergent behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6540.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RESPROMPT (>=5-step multi-dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RESPROMPT (evaluation on questions requiring ≥5 reasoning steps across math benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RESPROMPT evaluated specifically on high-step-count (≥5) questions across the math benchmarks, showing large improvements over CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA family (reported aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B / 70B (reported separately)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT (Residual Connection Prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with explicit graph-like residual connections</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K / AQUA-RAT / MathQA (≥5-step subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step mathematical reasoning where later steps depend on multiple earlier intermediate results</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>relative accuracy improvement (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Best CoT-based approaches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>21.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>For questions requiring ≥5 steps RESPROMPT substantially outperforms CoT: average improvement reported as 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B; indicates RESPROMPT particularly addresses long-range dependency/forgetting issues in CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6540.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (step-by-step linear rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot prompting method that includes intermediate step-by-step rationales in the prompt examples to elicit multi-step reasoning from LLMs; produces a near-linear reasoning flow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (linear chain)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>52.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RESPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-6.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT creates a linear chain of intermediate steps and tends to 'forget' or not explicitly reference several-steps-earlier intermediate results; performance degrades with more reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6540.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity-based prompting (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complexity-based exemplar selection prompting (Fu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that selects exemplars of higher reasoning complexity (longer step-count) to better match target problem difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complexity-based prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (exemplar selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>48.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RESPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-10.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>RESPROMPT outperforms complexity-based prompting variants; this ablation confirms gains stem from residual connections rather than merely using higher-complexity exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6540.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (RESPROMPT vs CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613) evaluated with RESPROMPT and CoT on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 was evaluated to test whether RESPROMPT helps the most capable API models; RESPROMPT yielded modest gains over vanilla CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with residual connections</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic multi-step problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>76.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>RESPROMPT remains beneficial on strong API models like GPT-3.5, showing that residual prompting can complement already-strong reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6540.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (RESPROMPT vs CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0613) evaluated with RESPROMPT and CoT on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated to test RESPROMPT's utility on top-tier LLMs; RESPROMPT produced small but positive gains over CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with residual connections</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic multi-step problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>93.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Even for very capable models like GPT-4, explicit residual connections in prompts yield incremental performance gains, suggesting benefit beyond model scale alone.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6540.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (ensemble) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble of sampled reasoning paths) evaluated with RESPROMPT and CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble decoding strategy that samples multiple reasoning paths and selects the most consistent final answer; combined with RESPROMPT it yields further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA / LLaMA2 (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency + RESPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (diverse decoding paths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>72.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT + Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>8.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Applying self-consistency (5-path) on top of RESPROMPT further improves accuracy compared to CoT+self-consistency; combining residual-augmented prompts with ensemble decoding compounds benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6540.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implementation Ablation: token reuse vs symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation on how to implement residual connections: exact token reuse vs symbolic variables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper compares representing intermediate results by repeating exact tokens versus using symbolic placeholders (e.g., X) and finds token reuse is more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA family (reported aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT implementation variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with different residual representations</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K / AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>relative accuracy (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RESPROMPT using symbolic variables</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Exact verbatim repetition of intermediate-result tokens (contextual residuals) consistently outperforms the use of abstract symbolic variables; hypothesis is that symbolic notation increases reasoning difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6540.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6540.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Residual placement ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study on residual connection placement (No Residual / First Half / Second Half / Uniform / Full)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study comparing where to place residual connections across steps; finds full coverage most effective and later-stage residuals more important than earlier ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA family (reported aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RESPROMPT (placement variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential with selective residual connections</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K / AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) per placement condition (tabled)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>No Residual / First Half / Second Half / Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Table 2 shows Full (residuals at all steps) achieves best results; Second Half (residuals only in later stages) outperforms First Half, consistent with later steps depending on earlier intermediate results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Large Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6540",
    "paper_id": "paper-263831532",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "RESPROMPT (LLaMA-65B, GSM8K)",
            "name_full": "Residual Connection Prompting (evaluated on LLaMA-65B, GSM8K)",
            "brief_description": "A prompting strategy that explicitly repeats intermediate results (residual connections) in few-shot prompts to reconstruct graph-like dependencies and improve multi-step reasoning in LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "65B",
            "reasoning_method_name": "RESPROMPT (Residual Connection Prompting)",
            "reasoning_method_type": "sequential with explicit graph-like residual connections",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school multi-step arithmetic word problems (math reasoning)",
            "performance_metric": "accuracy (%)",
            "performance_value": 58.4,
            "comparison_target_method": "Chain-of-Thought (CoT) prompting",
            "performance_difference": 6.2,
            "statistical_significance": false,
            "analysis_notes": "RESPROMPT reconstructs missing prerequisite links in CoT by verbatim reusing intermediate-result tokens; shows notable gains on multi-step math (especially for questions with ≥5 steps). Ablations show full coverage of residuals across steps is critical and later-stage residuals are more important than early-stage ones.",
            "ablation_study_present": true,
            "uuid": "e6540.0",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RESPROMPT (LLaMA2-70B, GSM8K)",
            "name_full": "Residual Connection Prompting (evaluated on LLaMA2-70B, GSM8K)",
            "brief_description": "Same RESPROMPT prompting strategy evaluated on LLaMA2-70B, demonstrating improvements over CoT though smaller relative gains than with LLaMA-65B on aggregate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2",
            "model_size": "70B",
            "reasoning_method_name": "RESPROMPT (Residual Connection Prompting)",
            "reasoning_method_type": "sequential with explicit graph-like residual connections",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school multi-step arithmetic word problems (math reasoning)",
            "performance_metric": "accuracy (%)",
            "performance_value": 65.3,
            "comparison_target_method": "Chain-of-Thought (CoT) prompting",
            "performance_difference": 1.3,
            "statistical_significance": false,
            "analysis_notes": "RESPROMPT yields positive gains on the larger LLaMA2-70B model, with more modest average relative gains across benchmarks compared to LLaMA-65B; gains increase with question step-count and model scale (emergent behavior).",
            "ablation_study_present": true,
            "uuid": "e6540.1",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RESPROMPT (&gt;=5-step multi-dataset)",
            "name_full": "RESPROMPT (evaluation on questions requiring ≥5 reasoning steps across math benchmarks)",
            "brief_description": "RESPROMPT evaluated specifically on high-step-count (≥5) questions across the math benchmarks, showing large improvements over CoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA family (reported aggregate)",
            "model_size": "65B / 70B (reported separately)",
            "reasoning_method_name": "RESPROMPT (Residual Connection Prompting)",
            "reasoning_method_type": "sequential with explicit graph-like residual connections",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K / AQUA-RAT / MathQA (≥5-step subsets)",
            "task_description": "multi-step mathematical reasoning where later steps depend on multiple earlier intermediate results",
            "performance_metric": "relative accuracy improvement (%)",
            "performance_value": null,
            "comparison_target_method": "Best CoT-based approaches",
            "performance_difference": 21.1,
            "statistical_significance": false,
            "analysis_notes": "For questions requiring ≥5 steps RESPROMPT substantially outperforms CoT: average improvement reported as 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B; indicates RESPROMPT particularly addresses long-range dependency/forgetting issues in CoT.",
            "ablation_study_present": true,
            "uuid": "e6540.2",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) baseline",
            "name_full": "Chain-of-Thought prompting (step-by-step linear rationales)",
            "brief_description": "A few-shot prompting method that includes intermediate step-by-step rationales in the prompt examples to elicit multi-step reasoning from LLMs; produces a near-linear reasoning flow.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "65B",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential (linear chain)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems (math reasoning)",
            "performance_metric": "accuracy (%)",
            "performance_value": 52.2,
            "comparison_target_method": "RESPROMPT",
            "performance_difference": -6.2,
            "statistical_significance": false,
            "analysis_notes": "CoT creates a linear chain of intermediate steps and tends to 'forget' or not explicitly reference several-steps-earlier intermediate results; performance degrades with more reasoning steps.",
            "ablation_study_present": false,
            "uuid": "e6540.3",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Complexity-based prompting (baseline)",
            "name_full": "Complexity-based exemplar selection prompting (Fu et al.)",
            "brief_description": "A prompting strategy that selects exemplars of higher reasoning complexity (longer step-count) to better match target problem difficulty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "65B",
            "reasoning_method_name": "Complexity-based prompting",
            "reasoning_method_type": "sequential (exemplar selection)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school multi-step arithmetic",
            "performance_metric": "accuracy (%)",
            "performance_value": 48.3,
            "comparison_target_method": "RESPROMPT",
            "performance_difference": -10.1,
            "statistical_significance": false,
            "analysis_notes": "RESPROMPT outperforms complexity-based prompting variants; this ablation confirms gains stem from residual connections rather than merely using higher-complexity exemplars.",
            "ablation_study_present": true,
            "uuid": "e6540.4",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 (RESPROMPT vs CoT)",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-0613) evaluated with RESPROMPT and CoT on GSM8K",
            "brief_description": "OpenAI GPT-3.5 was evaluated to test whether RESPROMPT helps the most capable API models; RESPROMPT yielded modest gains over vanilla CoT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0613)",
            "model_size": null,
            "reasoning_method_name": "RESPROMPT",
            "reasoning_method_type": "sequential with residual connections",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic multi-step problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 76.0,
            "comparison_target_method": "Chain-of-Thought (CoT)",
            "performance_difference": 3.0,
            "statistical_significance": false,
            "analysis_notes": "RESPROMPT remains beneficial on strong API models like GPT-3.5, showing that residual prompting can complement already-strong reasoning capabilities.",
            "ablation_study_present": false,
            "uuid": "e6540.5",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (RESPROMPT vs CoT)",
            "name_full": "GPT-4 (gpt-4-0613) evaluated with RESPROMPT and CoT on GSM8K",
            "brief_description": "GPT-4 was evaluated to test RESPROMPT's utility on top-tier LLMs; RESPROMPT produced small but positive gains over CoT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_size": null,
            "reasoning_method_name": "RESPROMPT",
            "reasoning_method_type": "sequential with residual connections",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic multi-step problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 93.0,
            "comparison_target_method": "Chain-of-Thought (CoT)",
            "performance_difference": 2.0,
            "statistical_significance": false,
            "analysis_notes": "Even for very capable models like GPT-4, explicit residual connections in prompts yield incremental performance gains, suggesting benefit beyond model scale alone.",
            "ablation_study_present": false,
            "uuid": "e6540.6",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Consistency (ensemble) comparison",
            "name_full": "Self-Consistency (ensemble of sampled reasoning paths) evaluated with RESPROMPT and CoT",
            "brief_description": "An ensemble decoding strategy that samples multiple reasoning paths and selects the most consistent final answer; combined with RESPROMPT it yields further gains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA / LLaMA2 (reported)",
            "model_size": "65B / 70B",
            "reasoning_method_name": "Self-Consistency + RESPROMPT",
            "reasoning_method_type": "ensemble (diverse decoding paths)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "GSM8K",
            "task_description": "multi-step math reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": 72.0,
            "comparison_target_method": "CoT + Self-Consistency",
            "performance_difference": 8.0,
            "statistical_significance": false,
            "analysis_notes": "Applying self-consistency (5-path) on top of RESPROMPT further improves accuracy compared to CoT+self-consistency; combining residual-augmented prompts with ensemble decoding compounds benefits.",
            "ablation_study_present": true,
            "uuid": "e6540.7",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Implementation Ablation: token reuse vs symbolic",
            "name_full": "Ablation on how to implement residual connections: exact token reuse vs symbolic variables",
            "brief_description": "Paper compares representing intermediate results by repeating exact tokens versus using symbolic placeholders (e.g., X) and finds token reuse is more effective.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA family (reported aggregate)",
            "model_size": "65B / 70B",
            "reasoning_method_name": "RESPROMPT implementation variants",
            "reasoning_method_type": "sequential with different residual representations",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K / AQUA-RAT",
            "task_description": "multi-step math reasoning",
            "performance_metric": "relative accuracy (comparison)",
            "performance_value": null,
            "comparison_target_method": "RESPROMPT using symbolic variables",
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "Exact verbatim repetition of intermediate-result tokens (contextual residuals) consistently outperforms the use of abstract symbolic variables; hypothesis is that symbolic notation increases reasoning difficulty.",
            "ablation_study_present": true,
            "uuid": "e6540.8",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Residual placement ablation",
            "name_full": "Ablation study on residual connection placement (No Residual / First Half / Second Half / Uniform / Full)",
            "brief_description": "Study comparing where to place residual connections across steps; finds full coverage most effective and later-stage residuals more important than earlier ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA family (reported aggregate)",
            "model_size": "65B / 70B",
            "reasoning_method_name": "RESPROMPT (placement variants)",
            "reasoning_method_type": "sequential with selective residual connections",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K / AQUA-RAT",
            "task_description": "multi-step math reasoning",
            "performance_metric": "accuracy (%) per placement condition (tabled)",
            "performance_value": null,
            "comparison_target_method": "No Residual / First Half / Second Half / Uniform",
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "Table 2 shows Full (residuals at all steps) achieves best results; Second Half (residuals only in later stages) outperforms First Half, consistent with later steps depending on earlier intermediate results.",
            "ablation_study_present": true,
            "uuid": "e6540.9",
            "source_info": {
                "paper_title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.020520999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models
8 May 2024</p>
<p>Song Jiang songjiang@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Zahra Shakeri zshakeri@meta.com 
Meta AI</p>
<p>Aaron Chan aarzchan@meta.com 
Meta AI</p>
<p>Maziar Sanjabi maziars@meta.com 
Meta AI</p>
<p>Hamed Firooz hfirooz@linkedin.com 
LinkedIn, Inc</p>
<p>Yinglong Xia yxia@meta.com 
Meta AI</p>
<p>Bugra Akyildiz 
Meta AI</p>
<p>Yizhou Sun yzsun@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Jinchao Li jinchaoli@meta.com 
Meta AI</p>
<p>Qifan Wang 
Meta AI</p>
<p>Asli Celikyilmaz aslic@meta.com 
FAIR
Meta</p>
<p>Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Jerry Tworek 
Heewoo Jun 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Lukasz Kaiser 
Matthias Plappert 
Jacob Hilton 
Reiichiro Nakano 
Dario 2020 Amodei 
Tushar Khot 
Harsh Trivedi 
Matthew Finlayson 
Yao Fu 
Kyle Richardson 
Peter Clark 
Ashish Sab 
Aitor Lewkowycz 
Anders Andreassen 
David Dohan 
Ethan Dyer 
Henryk Michalewski 
Vinay V Ra- Masesh 
Ambrose Slone 
Cem Anil 
Imanol Schlag 
Theo Gutman-Solo 
Yuhuai Wu 
Behnam Neyshabur 
Guy Gur-Ari 
Vedant 2022 Misra 
Danilo Neves Ribeiro 
Shen Wang 
Xiaofei Ma 
Mirac Suzgun 
Nathan Scales 
Nathanael Schärli 
Se- Bastian Gehrmann 
HyungYi Tay 
Won Chung 
Aakanksha Chowdhery 
V Quoc 
Ed H Le 
Chi 
Denny 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Baptiste Rozière 
Naman Goyal 
Eric Hambro 
Faisal 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay 
Yizhong Wang 
Hamish Ivison 
Pradeep Dasigi 
Jack Hessel 
Raghavi Khyathi 
David Chandu 
Kelsey Wadden 
Macmillan 
Jason Wei 
Rishi Bommasani 
Colin Raffel 
Barret Zoph 
Sebastian Borgeaud 
Dani Yogatama 
Maarten Bosma 
Denny Zhou 
Donald Metzler 
Ed H Chi 
Tatsunori Hashimoto 
Oriol Vinyals 
Percy Liang 
Jeff Dean 
William Fedus 
Emer 
Xuezhi Wang 
Dale Schuurmans 
Brian Ichter 
Fei Xia 
Quoc V Le 
Shunyu Yao 
Dian Yu 
Jeffrey Zhao 
Izhak Shafran 
Thomas L Griffiths 
Yuan Cao 
Karthik Narasimhan 
Yao Yao 
Zuchao Li 
Hai Zhao 
Aohan Zeng 
Xiao Liu 
Zhengxiao Du 
Zihan Wang 
Hanyu Lai 
Ming Ding 
Zhuoyi Yang 
Yifan Xu 
Wayne Xin Zhao 
Kun Zhou 
Junyi Li 
Tianyi Tang 
Xiaolei Wang 
Yupeng Hou 
Yingqian Min 
Beichen 
Le Hou </p>
<p>Nicholas Joseph
Henrique Ponde de Oliveira Pinto, Jared Ka-plan, Harri Edwards, Yuri BurdaYuan, Greg Brockman, et al. 2021. Evaluating large</p>
<p>Aakanksha Chowdhery
Sharan Narang, Jacob Devlin, Gaurav Mishra, Hyung Won ChungMaarten Bosma, Adam Roberts, Paul Barham, Charles Sutton</p>
<p>Henghui Zhu
Deguang KongRui Dong</p>
<p>Juli-ette Burger
Anjelica Ramos, Zhiheng Huang, William Yang WangGeorge Karypis, Bing Xiang</p>
<p>Noah A Smith
Iz</p>
<p>Jie Tang
Wendi Zheng
Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen2023Xiao Xia, Zhiyuan Liu, Peng Zhang, Yuxiao Dong</p>
<p>Nathan Scales
Dale SchuurmansXuezhi Wang</p>
<p>Claire Cui
Olivier Bousquet, Quoc V. Le, and Ed H</p>
<p>RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models
8 May 2024EC5124130E5D84CCCC74B315F1C5D198arXiv:2310.04743v2[cs.CL]
Chain-of-thought (CoT) has impressively unlocked the reasoning potential of large language models (LLMs).Yet, it falls short when tackling problems that require multiple reasoning steps.This limitation arises from the complex nature of multi-step reasoning processes: later stages often depend not only on the immediately preceding step, but also on the results from several steps earlier.Such complexities indicate the reasoning process is naturally a graph.The almost linear structure of CoT, however, struggles to capture this complex reasoning graph.To address this challenge, we propose Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs.The core of our idea is to reconstruct the reasoning graph within prompts.We achieve this by integrating necessary connections-links present in reasoning graph but missing in the linear CoT flow-into the prompts.Termed "residual connections", these links can transform linear CoT into the complex reasoning graphs that multi-step problems entail.On benchmarks across math, sequential, and commonsense domains, RESPROMPT demonstrates clear improvements in multi-step reasoning compared with CoT.Through extensive ablation studies and analyses, we pinpoint how to effectively build residual connections and also identify situations where it might be unnecessary.</p>
<p>Introduction</p>
<p>Recent advancements in scaling up large language models (LLMs) (Brown et al., 2020;Thoppilan et al., 2022;Chowdhery et al., 2022;Anil et al., 2023;Touvron et al., 2023a,b;Zeng et al., 2023;Scao et al., 2022;Zhao et al., 2023;Yang et al., 2023) have not only significantly improved their performance but have also enabled entirely new "emergent ability" (Wei et al., 2022a).One mile-* Work was done during an internship at Meta stone approach that harnesses this potential is chainof-thought (CoT) prompting (Wei et al., 2022b), which uses few-shot step-by-step demonstrations to teach LLMs how to reach a final answer.CoT prompting has unlocked impressive reasoning abilities in LLMs, enabling them to excel in various complex tasks, including mathematics, commonsense reasoning and more (Wei et al., 2022b;Suzgun et al., 2022;Lu et al., 2022).</p>
<p>However, standard CoT approach has proven to be less effective in addressing questions that involve multiple reasoning steps (Fu et al., 2023b;Zhou et al., 2023a;Khot et al., 2023).In Figure 1, we demonstrate that both LLaMA-65B (Touvron et al., 2023a) and LLaMA2-70B (Touvron et al., 2023b) experience a notable decline in performance as the number of reasoning steps increases on the mathematical benchmarks GSM8K (Cobbe et al., 2021) and AQUA-RAT (Ling et al., 2017).</p>
<p>Why is this the case?We hypothesize that in many multi-step reasoning processes, later stages rely not only on the immediately preceding step but also on results from several steps prior as prerequisites.This complex interdependence leads to the reasoning process in these multi-step questions essentially forming a graph structure, which we refer to as "reasoning graph".We show an example involving multi-step reasoning from GSM8K bench-Question: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month for the past three months.He gets a $5 allowance a month.He also mows lawns and shovels driveways.He charges $15 to mow a lawn and $7 to shovel.After buying the shoes, he has $15 in change.If he mows 4 lawns, how many driveways did he shovel?2. From his allowance, he earned 3 * $5 = $15.</p>
<ol>
<li>
<p>From mowing lawns, he earned 4 * $15 = $60.</p>
</li>
<li>
<p>So the money he earned from shoveling driveways is $110 -$15 -$60 = $35.</p>
</li>
<li>
<p>Therefore, the number of driveways he shoveled is $35 / $7 = 5.</p>
</li>
</ol>
<p>So the answer is 5.</p>
<p>(b) Chain-of-Thoughts (CoT) Prompting</p>
<p>1.He has $15 left after buying the shoes that cost $95, which means he had $95 + $15 = $110 in total before the purchase.</p>
<ol>
<li>
<p>Next, we calculate the amount he earned from his allowance.He has been saving for three months, getting $5 each month, so from his allowance he earned 3 * $5 = $15.</p>
</li>
<li>
<p>Then we know he mowed 4 lawns and he charges $15 per lawn, so from mowing lawns, he earned 4 * $15 = $60.</p>
</li>
<li>
<p>Let's sum up the money he earned from his allowance and from mowing lawns: $15 (allowance) + $60 (mowing) = $75.</p>
</li>
<li>
<p>Now, let's subtract the total amount of money earned from allowance and mowing from his total amount of money before buying the shoes to find out how much he earned from shoveling driveways: $110 (total before buying shoes) -$75 (earned from allowance and mowing) = $35.</p>
</li>
</ol>
<p>6.As he charges $7 to shovel a driveway, we divide the total earned from shoveling by the charge per driveway to find out the number of driveways he shoveled: $35 / $7 = 5 driveways.</p>
<p>So the answer is 5.</p>
<p>(e) Residual Connection Prompting (ResPrompt)</p>
<p>Missing links in CoT prompts .This simplistic reasoning flow within CoT prompts has a structural mismatch with the complex underlying reasoning graph, thereby limiting CoT's effectiveness in handling questions that require multiple reasoning steps.</p>
<p>To tackle this challenge, we propose Residual Connection Prompting (RESPROMPT), a new prompting strategy that bridges this structural gap in reasoning processes and thereby enhances the multi-step reasoning capability of LLMs.Our core idea is to reconstruct the reasoning graph from the linearly structured reasoning flow via adding necessary connections in prompts.A necessary connection is a link present in reasoning graph but missing in linear reasoning flow (see red arrows in Figure 2 (d) for examples).Specifically, a necessary connection usually embodies the essential prerequisites of a reasoning step.In RESPROMPT, we explicitly link these prerequisites to their corresponding reasoning step by repeating them, using the same tokens, within that specific step in prompts.By doing so, we effectively recover the complex underlying reasoning graphs of multi-step questions in RESPROMPT.In Figure 2 (e), we present an example of RESPROMPT applied to a multi-step question.We call these explicit links as "residual connections" within prompts.This term is inspired by the residual connections across neural network layers (He et al., 2016).However, a critical distinction lies in the context-specific nature in RESPROMPT.While the residuals in (He et al., 2016) are uniform, RESPROMPT's residual connections depend on the unique context, as prerequisites for each reasoning step might come from various positions in the reasoning process.</p>
<p>We use the publicly released LLaMA family of models (LLaMA, LLaMA2) (Touvron et al., 2023a,b) to evaluate RESPROMPT on six benchmarks, including 1) Mathematical reasoning: GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021); 2) Sequential reasoning: SCONE-Alchemy (Long et al., 2016); and 3) Commonsense reasoning: StrategyQA (Geva et al., 2021).Our experiments demonstrate that RESPROMPT significantly improves overall reasoning accuracy on the LLaMA series of models.Breakdown analysis shows our performance gains on multi-step questions are much more remarkable: for questions requiring at least 5 reasoning steps, RESPROMPT outperforms the best CoT based approaches by an average improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B.Furthermore, through extensive ablation studies and analyses, we investigate how to build residual connections most effectively.We aslo dive into how RESPROMPT functions in terms of model size, robustness, and conduct error analyses.Additionally, we discuss when RESPROMPT may not be necessary.</p>
<p>RESPROMPT: Residual Connection Prompting</p>
<p>Why is Standard CoT Less Effective for Multi-Step Reasoning?</p>
<p>To investigate the reasons for the failure of the standard CoT in multi-step reasoning, we use mathematical reasoning as our illustrative example.In Figure 2 (a), we present a math question from GSM8K (Cobbe et al., 2021), which consists of multiple reasoning steps.Note that in GSM8K, a step is annotated as one math calculation.However, this notion can also encompass similar ideas depending on the specific context (Fu et al., 2023b), such as a sub-question (Zhou et al., 2023a).As shown in Figure 2 (d), a multi-step question exhibits a complex, structured underlying reasoning process, where later stages steps frequently depend not only on the immediately preceding step but also potentially on results several steps prior.This complex interdependence renders the underlying structure of reasoning flow essentially a graph, which we refer to as a reasoning graph.However, in CoT prompts, reasoning unfolds in a step-bystep manner, including only the immediately preceding step, with no explicit reference to intermediate results from several steps prior (Figure 2 (b)).This consequently yields a nearly linear-structured reasoning flow within the standard CoT prompts (Figure 2 (c)), which is not able to fully recover the complex underlying reasoning graphs inherent in multi-step questions.This striking mismatch in reasoning flow structures significantly impairs CoT's capacity to effectively tackle multi-step reasoning.</p>
<p>We note that while we use math problems as our running example in Figure 2, these findings are broadly applicable to any other types of multistep problems characterized by complex reasoning flows.It's important to mention that not every multi-step question exhibits a graph-like reasoning process; some questions may involve a long chain of dependencies, which we explore in Section 3.5.</p>
<p>Enabling Multi-Step Reasoning via</p>
<p>Building Residual Connections Principle and Methodology.Our findings lead to the hypothesis that standard CoT struggles with multi-step reasoning because its nearly linear reasoning flow within prompts is not sufficient for capturing the reasoning graphs inherent in complex multi-step questions.In a graphical view, the CoT reasoning flow, as shown in Figure 2 (c), misses necessary connections required to reconstruct the complex reasoning graph depicted in Figure 2</p>
<p>(d).</p>
<p>A more intuitive interpretation is that CoT tends to "forget" intermediate results it has previously derived.To address this structural mismatch, we propose a novel prompting strategy aimed at reconstructing the complex underlying reasoning graph by explicitly adding the vital missing connections.These added connections re-introduce intermediate results from previous steps as prerequisites for later steps.Specifically, for a step, we first 1) enumerate and connect the necessary prerequisites with either results of earlier steps or directly from the provided question conditions, then we 2) derive the result based on these prerequisites.An example is shown in Figure 2 (e).We refer to our added links as "Residual Connections" and call our prompting strategy as Residual Connection Prompting (RESPROMPT).By building residual connections to recall essential prerequisites, RESPROMPT ensures that the reasoning flow within prompts sufficiently align with the underlying reasoning graphs for complex multi-step questions.</p>
<p>A natural question arises: where should we build residual connections for effective alignment with complex reasoning graphs in multi-step problems?Is it essential to introduce them at every single step, or would a selective subset suffice?We investigate this in ablation studies on residual connection placement in Section 3.3.Our findings emphasize that covering the entire reasoning process with residual connections is crucial for RESPROMPT's improved multi-step reasoning performance.</p>
<p>Implementation.In RESPROMPT, we build residual connections through a straightforward method: reuse the exact same tokens as references.That is, when recalling an intermediate result from a prior step, we describe it by repeating the exact same tokens.For example, in Figure 2 (e), we derive the phrase "earned from his allowance" (highlighted in yellow background) in the second step.To reference it as a prerequisite for the fourth step, we repeat "earned from his allowance" verbatim, facilitating LLMs in easily connecting the current step with prior intermediate results.In Section 3.3, we compare this approach with more efficient designs, such as representing intermediate results as a symbolic variable denoted as X and later directly reusing X.Our findings confirm that our straightforward exact repeat approach is more effective in building residual connections within prompts.</p>
<p>Insights and Understanding.RESPROMPT is a simple and effective approach.Our intuitive understanding regarding its strong performance in multistep reasoning can be distilled into two key perspectives: 1) Recovering complex reasoning graphs.As previously discussed, residual connections play a crucial role in sufficiently aligning the reasoning flow in prompts with the complex reasoning graphs inherent in multi-step questions.2) Reducing reasoning difficulty.In standard CoT without residuals, a reasoning step must a) first implicitly identify the necessary prerequisites and b) then perform reasoning on them.This dual burden can be quite demanding.In contrast, by explicitly linking necessary prerequisites using residual connections, RESPROMPT reduces the workload of a reasoning step to the core reasoning process itself, thus simplifying the mission of each step.This concept can also be analogized to human intelligence in solving multi-step questions: when provided with corresponding conditions, solving a single reasoning step becomes much easier.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Datasets.We evaluate RESPROMPT on six benchmarks, covering three type of reasoning tasks: 1) Mathematical reasoning, including GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021); 2) Sequential reason-ing, SCONE-Alchemy (Long et al., 2016); and 3) Commonsense reasoning: StrategyQA (Geva et al., 2021).The detailed statistics of these datasets are provided in appendix C.1.</p>
<p>Language Models.</p>
<p>We mainly evaluate RESPROMPT using the LLaMA family of models, including LLaMA (Touvron et al., 2023a) andLLaMA2 (Touvron et al., 2023b).LLaMA is publicly released, facilitating cost-effective and reproducible evaluations.Unlike OpenAI's GPT series of APIs, which undergo frequent updates and deprecation, using LLaMA ensures that the community can consistently reproduce our results.We also compare RESPROMPT with CoT on GPT-3.5 and GPT-4 to examine whether our method remains beneficial for the most powerful LLMs, which can be found in appendix D.8.</p>
<p>Prompts.RESPROMPT aims to incorporate residual connections in prompts for multi-step reasoning.However, the original CoT prompts from Wei et al. (2022b), cater mostly to short-step questions (1-3 steps), making it unnecessary to build residual connections.Therefore, we select questions from the training set of benchmarks, covering a range number of reasoning steps, to design prompts for RESPROMPT.To ensure a fair comparison and validate that our improvements stem from residual connections but not simply from using different exemplars, we also derive CoT prompts from these selected questions.We refer to the original CoT prompts as "Original CoT", and CoT prompts derived from our newly selected examples as "Derived CoT".To the best of our knowledge, SCONE-Alchemy has not been previously studied with CoT.Therefore, we only compare RESPROMPT with our derived CoT prompts.All prompts are listed in appendix G.</p>
<p>Main Results</p>
<p>Overall Results: RESPROMPT significantly enhances accuracy in complex reasoning.We compare RESPROMPT against several baseline prompting strategies, including standard prompting, Original CoT, and Derived CoT.The results of this comparison are detailed in Table 1.Notably, with residual connections, RESPROMPT consistently outperforms CoT based prompting methods, regardless of the original or newly selected CoT exemplars.Specifically, RESPROMPT achieves an average relative gain of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B across the four benchmarks.These clear gains underscore the enhanced reasoning abil-Table 1: Reasoning accuracy comparison between RESPROMPT and baseline approaches.The first four rows show results from previous works.Note that since they apply CoT to different and larger LLMs, their results are not directly comparable, but we include them for reference.Numbers marked with ' †' are from (Wei et al., 2022b), while numbers marked with ' ‡' are from (Fu et al., 2023b).A '-' symbol indicates "not applicable".Unlike other experiments on GSM8K, for LLaMA-65B with RESPROMPT (marked with ' * '), the number of few-shot exemplars is 5 instead of 8, as 8-shot exceeds the limitation of LLaMA-65B's input length.The best results for each dataset are highlighted in boldface, the second-best results are underlined.Relative gains are shown in green.ity of RESPROMPT.It is note to worth that the improvements of RESPROMPT over Derived CoT validates that the improvements of RESPROMPT stem from residual connections rather than solely from using different exemplar questions to design prompts.Furthermore, to contextualize our findings within the landscape of LLMs, we also present results obtained with other LLMs reported in previous studies, including These models LaMDA, GPT-3, Codex, and PaLM.</p>
<h1>Params</h1>
<p>Breakdown on Number of Steps: RESPROMPT excels particularly in multistep reasoning.RESPROMPT is intentionally proposed to improve multi-step reasoning.To assess RESPROMPT's performance across questions with varying complexity, we break down questions based on the number of reasoning steps into five groups: {1, 2, 3, 4, ≥5}.In Figure 3, we present both the data percentage distribution for each group and RESPROMPT's reasoning accuracy within these groups using LLaMA2-70B across the three math benchmarks (All questions in SCONE-Alchemy have five steps and thus a breakdown analysis is not necessary).We find RESPROMPT outperforms the baselines in most groups.Notably, as the number of reasoning steps increases, all approaches generally experience a decline in accuracy.However, RESPROMPT demonstrates a relatively smooth decline and generally maintains higher accuracy than CoT-based approaches.In particular, for questions with ≥ 5 reasoning steps, RESPROMPT surpasses the best CoT based approaches by achieving a remarkable improvement of 14.3% on LLaMA2-70B.This trend is similarly observed in RESPROMPT's performance on LLaMA-65B (with 21.1% gain for questions with ≥ 5 steps), as illustrated in appendix D.2.These results confirms RESPROMPT's strong ability for multi-step reasoning.Where is it critical to build residual connections?For multi-step reasoning, it might seem intuitive to build residual connections for every reasoning step.However, it is interesting to identify the most critical locations for residual connections.We study five scenarios: 1) "No Residual": No residual connections; 2) "First Half ": Residual connections only for the first half of steps; 3) "Second Half ": Residual connections only for the second half of steps; 4) "Uniform": Residual connections in every other step; 5) "Full": Residual connections in all steps.Table 2 presents the performance of these designs on GSM8K and AQUA-RAT datasets.The results reveal two key findings: 1) Building residual connections that cover the entire reasoning process is critical for achieving the highest reasoning accuracy.2) Residual connections in later stages ("Second Half") are more important than those in early stages ("First Half").This is reasonable since later-stage reasoning steps typically depend more on the results from earlier steps.How to implement residual connections effectively?How to implement residual connections plays a crucial role in fully releasing the power of RESPROMPT.We opt to directly reuse the exact same tokens to refer to a previously mentioned intermediate result in RESPROMPT.A natural alternative approach is to use symbolic variables, namely denoting an intermediate result as 'X' and referring to it as 'X' later.In Figure 4, we compare these two implementations.The results consistently show that reusing the exact same tokens out-performs using symbolic variables on both GSM8K and AQUA-RAT benchmarks, for both LLaMA-65B and LLaMA2-70B models.The worse performance of symbolic variables might be because it increases difficulty in reasoning.Understanding symbolic notation is known to be more challenging than processing semantics (Tang et al., 2023).
GSM8K (65B) GSM8K (70B) AQUA (65B) AQUA(</p>
<p>How does scaling LLMs affect RESPROMPT?</p>
<p>The reasoning ability of LLMs is recognized as an "emergent ability" (Wei et al., 2022a), meaning this capability becomes clear only when the model is sufficiently large.In Figure 5, we explore how RESPROMPT responds to various sizes of LLaMA, including 7B, 13B, 30B, and 65B.We derive two key observations: 1) Scaling enhances reasoning: larger model sizes consistently bring stronger reasoning performance, which echos the "emergent ability" concept.2) RESPROMPT demonstrates more clear gains over CoT when applied to larger LLaMA models, particularly in the case of 65B.In contrast, with smaller LLaMA models, such as 13B and 30B on AQUA-RAT, RESPROMPT's performance is even worse than CoT.This indicates that the comprehension of residual connections might be part of the "emergent ability", which complements the reasoning capabilities of LLMs.Experiments with LLaMA2 yield similar results, as detailed in appendix D.3.</p>
<p>Analysis</p>
<p>Is RESPROMPT robust to exemplar order?Fewshot learning in LLMs is known to be influenced by the order of exemplars (Zhao et al., 2021).Following Wei et al. (2022b), we investigate the impact of exemplar orders on RESPROMPT.We design four exemplar orders based on their number of reasoning steps: 1) "Ascending": Exemplars are ordered from fewer to more reasoning steps; 2) "Descending": Exemplars are ordered from more to fewer reasoning steps; 3) "Alternating": Exemplar or- dering involves alternating between the least and most reasoning steps; 4) "Random": Exemplars are arranged in random order.The results presented in Figure 6 demonstrate that RESPROMPT shows robustness to exemplar order variations in GSM8K.However, in AQUA-RAT, RESPROMPT shows slight sensitivity, with exemplars in ascending order outperforming other perturbations.This sensitivity aligns with the findings of (Zhao et al., 2021), which may be caused by LLMs' bias towards the exemplars at the end of the prompts.Error Analysis: How RESPROMPT makes mistakes.In Table 3, we summarize the error types made by RESPROMPT using LLaMA2-70B on GSM8K and AQUA-RAT.We analyze the first 15 wrong examples and categorize errors into three types: 1) "Wrong Problem Solving", including errors in reasoning flow, wrong residual connection, or minor calculation/derivation errors; 2) "Repetition": LLMs fail to stop and produces nonsense outputs; 3) "Wrong Ground-truth": The groundtruths are not correct.The majority of errors stem from problem-solving, suggesting room for further enhancing the reasoning process.Repetition also accounts for a non-trivial portion.This could be due to the relatively long prompts in RESPROMPT.LLMs learn to generate longer sequences, increasing the risk of repetition.We provide error examples on each dataset in appendix F.</p>
<p>Case Study: Can RESPROMPT facilitate resid- ual connection building?In Figure 7, we present a case study using an example from the GSM8K.</p>
<p>Our observations reveal that, with RESPROMPT's prompts, LLaMA2-70B successfully build residual connections in later-stage reasoning steps, leading to the correct final result.However, LLaMA2-70B prompted with CoT appear to "lose direction" after the third step.We infer that this discrepancy arises from CoT's linearly structured reasoning flow.In later stages, LLMs may struggle to correctly utilize intermediate results from earlier steps, which highlights the significance of building residual connections for effective multi-step reasoning.More case studies on each dataset are in appendix F.</p>
<p>When Is RESPROMPT Not Essential?</p>
<p>Previous results demonstrate RESPROMPT enhances reasoning abilities for multi-step questions with complex reasoning structures.From the re-Question: A new program had 60 downloads in the first month.The number of downloads in the second month was three times as many as the downloads in the first month, but then reduced by 30% in the third month.How many downloads did the program have total over the three months?</p>
<p>1.The program had 60 downloads in the first month.In the second month, there were 3 times as many downloads as the first month.So 3 x 60 = 180.2.In the third month, there were 30% less downloads.30% of 180 is 180 x 30% = 54.3.So 180 -54 = 126.So the answer is 126.</p>
<p>1.In the first month, the program had 60 downloads.In the second month, the number of downloads was three times as many as the downloads in the first month, so the number of downloads in the second month was 3 * 60 = 180.2.In the third month, the number of downloads was reduced by 30%.To find the number of downloads in the third month, we multiply the number of downloads in the second month by 0.7 (because 30% is the same as 0.3 when expressed as a decimal): 180 * 0.7 = 126.3.So, the total number of downloads over the three months was 60 (first month) + 180 (second month) + 126 (third month) = 366.</p>
<p>Therefore, the program had 366 downloads total over the three months.So the answer is 366.sults in Table 4, we have also identified that, for questions that are simple or do not have complex reasoning graphs, RESPROMPT is not necessary compared to CoT.Specifically, questions in SVAMP have at most two reasoning steps, while questions in StrategyQA primarily exhibit nearly linear underlying reasoning flows.Both are not as complex as the four datasets in Table 1.We show an example from StrategyQA and its nearly linear reasoning flow in appendix D.5.We infer that standard CoT is sufficient to capture the simple and straightforward reasoning flows in these datasets.In appendix D.9, we also show RESPROMPT is not applicable for know-extensive tasks in which the problem deriving process is not the key.</p>
<p>Related Work</p>
<p>We discuss three categories of related work: "In-Context Learning", "Prompting-Based Reasoning", and "Multi-Step Reasoning".Due to space limitation, we provide a concise overview here and direct readers to appendix B for a comprehensive review.</p>
<p>In-Context Learning.Our work focuses on more structured prompting strategy, which is closely related to in-context learning (Brown et al., 2020).It refers to LLMs' capacity to adapt from a few exemplars without model parameter changes.As models grow and train on more data, they exhibit significantly amplified performance accorss many tasks (Kaplan et al., 2020;Rae et al., 2021;Hoffmann et al., 2022;Chowdhery et al., 2022), or even obtain new capabilities such as reasoning over complex questions.This phenomenon is recently termed "emergent ability" (Wei et al., 2022a).</p>
<p>Prompting-Based Reasoning.LLMs, when</p>
<p>Conclusion</p>
<p>We propose RESPROMPT, a new prompting strategy to enhance multi-step reasoning in LLMs.Our core idea is to reconstruct the complex reasoning graphs inherent in multi-step questions.To achieve this, we introduce "residual connection", which adds missing links to transform the linear CoT prompts into graph-like structures.Experiments demonstrate that RESPROMPT significantly advances multi-step reasoning on LLaMA family.</p>
<p>Ethics Statement</p>
<p>Our work does not introduce additional ethical risks beyond those inherent in existing prompting based reasoning research.Nevertheless, as our approach is within the scope of LLMs, there remains a potential for LLMs to generate unexpected reasoning outputs.We anticipate further advancements in the field to address this concern in the future.</p>
<p>Limitations and Future Work</p>
<p>While our experiments primarily focus on the opensourced LLaMA family of models, it is important to acknowledge that the impact of RESPROMPT on other closed-sourced larger LLMs, such as PaLM models (Chowdhery et al., 2022;Anil et al., 2023), is not clear.We hope that our work serves as a catalyst for future research endeavors in this direction.Investigating how to optimize and adapt RESPROMPT for these more extensive models can pave the way for even greater breakthroughs in multi-step reasoning tasks.</p>
<p>B Full Related Work</p>
<p>In-Context Learning and Emergent Ability.Our work centers on enhancing the interdependence within prompts for complex multi-step reasoning, which is closely related to in-context learning (Brown et al., 2020).In-context learning describes the ability of language models to learn from a few demonstration examples and solve new tasks without the need to update the model parameters.</p>
<p>Recent work has shown that as these models scale to larger sizes and are trained on more tokens, they exhibit stronger and even entirely new capabilities, such as reasoning over complex questions (Kaplan et al., 2020;Rae et al., 2021;Hoffmann et al., 2022;Chowdhery et al., 2022).This phenomenon is often referred to as emergent ability (Wei et al., 2022a).In light of this, our primary contribution lies in the effective integration of residual connections within prompts, which proves to be pivotal in addressing problems that involve multiple reasoning steps.</p>
<p>Prompting-Based Reasoning.Recent progress demonstrates that when provided with appropriate prompts, LLMs can attain competitive reasoning abilities compared to earlier approaches that rely on fine-tuning (Wei et al., 2022b;Lewkowycz et al., 2022;Fu et al., 2023a;Ni et al., 2023).A milestone in this field is chain-of-thought (CoT) prompting (Wei et al., 2022b), wherein not only the final answer but also intermediate reasoning rationales for solving a complex problem are provided in the demonstration.CoT prompting has been further improved from various angles, including implementing a majority vote mechanism across multiple sampled reasoning paths (Wang et al., 2023b), simplifying intermediate reasoning rationale into a straightforward "Let's think step by step" prompt (Kojima et al., 2022), selecting representative CoT demonstrations from each question cluster (Zhang et al., 2023), executing the reasoning steps by generating codes (Gao et al., 2023), and progressively updating the demonstration set (Zhou et al., 2023b).However, empirical findings suggest that simple CoT is less effective in solving problems that involve multi-step reasoning (Fu et al., 2023b;Zhou et al., 2023a;Khot et al., 2023).Recent work has also expanded upon CoT by organizing and processing thoughts using more complex structures, such as trees (Yao et al., 2023a;Long, 2023) and graphs (Besta et al., 2023;Yao et al., 2023b).Tree of thought (ToT) and graph of though (GoT) are more relevant for tasks that require strategic reasoning, such as backtracking, traversal, sorting, etc.The demo applications in (Yao et al., 2023a;Besta et al., 2023) include examples like sorting, document merging, game of 24, etc.On the other hand, RESPROMPT aims to capture the complex underlying structure in standard multi-step problems.Therefore, although both RESPROMPT and ToT/GoT are related to the complex "structure", RESPROMPT targets different purposes compared to ToT and GoT.We position our work within the domain of prompting-based reasoning, and propose a simple yet novel prompting strategy based on residual connections, which leads to significant improvements over CoT for multi-step reasoning.</p>
<p>Multi-Step Reasoning.LLMs have shown limitations in solving problems that require multiple steps (e.g., ≥ 5 steps in GSM8K (Cobbe et al., 2021)</p>
<p>D Extra Experiments</p>
<p>D.1 Reasoning Accuracy on LLaMA2-Chat</p>
<p>In Table 6, we also provide the reasoning accuracy of LLaMA2-Chat-70B.LLaMA2-Chat-70B is finetuned based on the LLaMA2-70B base model for chatbot applications.We observe a non-trivial decline in reasoning accuracy when compared to the base model.We speculate this is because LLaMA2-Chat-70B is fine-tuned for non-reasoning purposes, and thus affect its reasoning capability.One possible implication is that the evaluation of reasoning capabilities should ideally be conducted within the base model or with models fine-tuned specifically for reasoning tasks.compared to the baseline approaches.</p>
<p>D.3 Accuracy For Different LLaMA2 Sizes</p>
<p>Figure 9 illustrates how performance of RESPROMPT and CoT based baselines is affected by LLaMA2 model scale.Similar to the results obtained with LLaMA-65B in Section 3.3, larger models yield better overall reasoning performance.Furthermore, we consider building and understanding residual connections as an "emergent ability", following the reasoning capabilities of LLMs.This is highlighted by the observation that RESPROMPT's advantage over baselines becomes more pronounced as the model size increases, particularly at 70B.We also note that the gains on MathQA and SCONE-Alchemy datasets are not significant as they are on LLaMA-65B in Section 3.3.The results on GSM8K dataset are shown in Table 7 (GSM8K is the only common dataset shared by this work with TÜLU (Wang et al., 2023c)).We notice that fine-tuned TÜLU still outperforms RESPROMPT.However, this performance gap significantly narrows when using the 65B model.This observation echos our earlier findings in Section 3.3 and appendix D.3, indicating RESPROMPT's ability to construct and understand residual connections appears to be an "emergent ability".</p>
<p>D.5 An Example and its Reasoning Flow from the StrategyQA Dataset</p>
<p>In Figure 10, we present a multi-step commonsense reasoning example from the StrategyQA dataset, along with its corresponding underlying reasoning flow.We notice that despite having multiple reasoning steps, the question's underlying reasoning flow is nearly linear.This phenomena applies for for most examples in StrategyQA dataset.This observation may help explain why RESPROMPT  does not provide improvements on StrategyQA dataset, as standard CoT is sufficient to reconstruct the nearly linear underlying reasoning flow.</p>
<p>D.6 Few-Shot Exemplars' Impact on Reasoning Accuracy</p>
<p>In the previous results, we maintain a fixed number of few-shot exemplars.To study the relationship between reasoning accuracy and the number of exemplars, we vary the exemplar numbers (N={2, 4, 6, 8} for GSM8K, N={1, 2, 3, 4} for AQUA-RAT and MathQA, and N={1, 2} for SCONE-Alchemy).In Figure 11 and Figure 12 , we compare the reasoning accuracy of RESPROMPT and CoT based approaches using the LLaMA-65B model and LLaMA2-70B.Interestingly, we observe that increasing the number of few-shot exemplars can even lead to a decrease in RESPROMPT's performance (GSM8K and MathQA).This discovery implies the significance of exemplar selection, particularly the impact of various combinations of exemplars on LLM's reasoning accuracy.We leave further exploration of this area as future work.Note  the significant improvement of RESPROMPT over CoT stems from correctly building the residual connections rather than solely from selecting more powerful examples to design prompts.Compare to advanced multi-step baselines.To understand the performance of RESPROMPT compared to approaches that use multiple stages prompting for multi-step reasoning (Khot et al., 2023;Zhou et al., 2023a), we conduct experiments on GSM8K dataset.The results, presented in Table 10, consistently demonstrate that RESPROMPT outperforms these advanced baselines for multistep reasoning.These baselines aim to decompose a complex question into several sub-questions, while RESPROMPT still maintains one pass flow via a more powerful problem solving process.</p>
<p>Cost-performance analysis.</p>
<p>Despite the RESPROMPT's superiority in multi-step reasoning performance, it also raises concerns about the inference cost.In Table 11, we compare the relative inference cost, including number of tokens and inference speed between RESPROMPT and baselines.On average, the number of combined Performance with self-consistency strategy.Self-consistency (Wang et al., 2023b) has been shown to be powerful in further improving reasoning performance by reaching an agreement between several decoding paths.In Table 12, we compare RESPROMPT and CoT with self-consistency (5-path) on GSM8K dataset.The results show that Performance on GPT family of models.We're also curious whether RESPROMPT still has superitoy in more capable LLMs such as OpenAI's GPT-3.5 and GPT-4 (OpenAI, 2023).We compare vanilla CoT and RESPROMPT using the "gpt-3.5turbo-0613"and "gpt-4-0613" models on GSM8K dataset.The results, shown in Table 13, demonstrate that ResPrompt is also beneficial for the most powerful OpenAI LLMs.</p>
<p>D.9 Additional experiments on CSQA and</p>
<p>HotpotQA.</p>
<p>To further understand RESPROMPT's ability for reasoning tasks requiring extensive knowledge, we conduct comparision between RESPROMPT and CoT on CSQA (Talmor et al., 2019) and HotpotQA (Yang et al., 2018) benchmarks.We show the results in Table 14.These results demonstrate that RESPROMPT can just achieve comparable performance to the baselines on both CSQA and HotpotQA.This observation is not surprising since both benchmarks primarily require extensive knowledge to answer the questions, rather than complex multi-step reasoning.Therefore, it is natural that RESPROMPT may not be essential in these knowledge assessment benchmarks.</p>
<p>E Justification of CoT's Inability in Capturing Earlier Dependency in Multi-step Reasoning</p>
<p>To verify that CoT's inability recover the reasoning graphs, i.e., CoT can not link to the intermediate results in several steps earlier, we conduct a justification experiment with LLaMA2-70B-Chat using the same example as in Figure 2. We explicitly prompt LLM to answer whether an intermediate result from earlier steps refer to.Question: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month for the past three months.He gets an $5 allowance a month.He also mows lawns and shovels driveways.He charges $15 to mow a lawn and $7 to shovel.After buying the shoes, he has $15 in change.If he mows 4 lawns, how many driveways did he shovel?Before the purchase, he had $95 + $15 = $110 in total.</p>
<p>From his allowance, he earned 3 * $5 = $15.From mowing lawns, he earned 4 * $15 = $60.Where the $110 is earned from?</p>
<p>Answer 1: Tobias earned $110 from mowing lawns and shoveling driveways combined.He mowed 4 lawns, so he shoveled ($110 -$60) / $7 = 12 driveways.Answer 2: The $110 is earned from the sum of the money earned from his allowance and the money earned from mowing lawns.</p>
<p>Answer 3: Tobias earned $110 from his allowance and mowing lawns combined.He earned $15 from his allowance, and $60 from mowing lawns.Answer 4: The $110 Tobias earned came from his allowance and mowing lawns.He earned $15 from his allowance and $60 from mowing lawns, totaling $110.Answer 5: Tobias earned the $110 from his allowance and mowing lawns.He earned $15 from his allowance and $60 from mowing lawns, which is a total of $75.Where the remaining $35 came from is not specified in the information provided.</p>
<p>F More Case Studies on Each Dataset</p>
<p>We provide more case studies to show how RESPROMPT works, including both correct and wrong examples.Table 16 to Table 19 show the examples from GSM8K, AQUA-RAT, MathQA and SCONE-Alchemy, respectively.</p>
<p>G Full Prompts of RESPROMPT</p>
<p>We also provide the few-shot exemplars we use in to design RESPROMPT.With these prompts, the reported results should be easily reproducible  Model Answer: First, let's calculate how many students enrolled in contemporary dance.20% of the 20 students enrolled in contemporary dance, so the number of students who enrolled in contemporary dance is 20 (total students) * 0.20 (20%) = 4 students.Next, let's calculate how many students enrolled in jazz dance.25% of the remaining students enrolled in jazz dance, so the number of students who enrolled in jazz dance is 20 (total students) -4 (students who enrolled in contemporary dance) * 0.25 (25%) = 4 students.Finally, let's calculate how many students enrolled in hip-hop dance.The remaining students enrolled in hip-hop dance, so the number of students who enrolled in hip-hop dance is 20 (total students) -4 (students who enrolled in contemporary dance) -4 (students who enrolled in jazz dance) = 12 students.Now, let's calculate the percentage of students who enrolled in hip-hop dance.The percentage of students who enrolled in hip-hop dance is 12 (students who enrolled in hip-hop dance) / 20 (total students) = 0.60 (60%).Therefore, 60% of the students enrolled in hip-hop dance.So the answer is 60.</p>
<p>Question: Josh decides to try flipping a house.He buys a house for $80,000 and then puts in $50,000 in repairs.This increased the value of the house by 150%.How much profit did he make?</p>
<p>Model Answer: First, let's calculate the value of the house after the repairs.The value of the house increased by 150%, so the new value is 150% of the original value.To find this, we multiply the original value by 1.50 (because 150% is the same as 1.50 when expressed as a decimal): $80,000 (original value) * 1.50 = $120,000 (new value).Now, let's calculate Josh's profit.He bought the house for $80,000 and put in $50,000 in repairs, so his total investment was $80,000 + $50,000 = $130,000.His profit is the difference between the new value of the house and his total investment: $120,000 (new value) -$130,000 (total investment) = -$10,000.So, Josh made a loss of $10,000.So the answer is -10000.on the publicly released LLaMA models.Table 20− Table 25 show the few-shot exemplars from GSM8K, AQUA-RAT, MathQA, SCONE-Alchemy and StrategyQA respectively.Question: The world contains 7 beakers.Each beaker may contain up to 4 units of colored chemical.The chemical can be poured into another beaker, drained away, or mixed.The initial state of each beaker is: First beaker has 1 orange chemical; Second beaker has 2 green chemicals; Third beaker has 0 chemicals; Fourth beaker has 2 red chemicals; Fifth beaker has 4 red chemicals; Sixth beaker has 4 green chemicals; Seventh beaker has 1 purple chemical.We perform the following actions: Remove two units from the fourth beaker.Remove three green units from the 6th beaker.Remove one unit from the second beaker.Add the first beaker to the second one.Mix the second beaker.Question: what are the chemicals in each beaker?</p>
<p>Model Answer: The initial state of each beaker is: First beaker has 1 orange chemical; Second beaker has 2 green chemicals; Third beaker has 0 chemicals; Fourth beaker has 2 red chemicals; Fifth beaker has 4 red chemicals; Sixth beaker has 4 green chemicals; Seventh beaker has 1 purple chemical.Actions: 1) Remove two units from the fourth beaker.Before this action, the fourth beaker has 2 red chemicals.After this action, the fourth beaker has 0 red chemicals.2) Remove three green units from 6th beaker.Before this action, the sixth beaker has 4 green chemicals.After this action, the sixth beaker has 1 green chemical.3) Remove one unit from the second beaker.Before this action, the second beaker has 2 green chemicals.After this action, the second beaker has 1 green chemical.4) Add the first beaker to the second one.Before this action, the first beaker has 1 orange chemical; and the second beaker has 1 green chemical.After this action, the first beaker has 0 orange chemicals; and the second beaker has 2 chemicals whose color is not clear at this moment.5) Mix the second beaker.Previous results show that before this action, the second beaker has 2 chemicals whose color is not clear.After this action, the second beaker has 2 brown chemicals.So the answer is: First beaker has 0 orange chemicals, denoted as 1:<em>; second beaker has 2 brown chemicals, denoted as 2:bb; third beaker has 0 chemicals, denoted as 3:</em>; fourth beaker has 0 red chemicals, denoted as 4:<em>; fifth beaker has 4 red chemicals, denoted as 5:rrrr; sixth beaker has 1 green chemical, denoted as 6:g; seventh beaker has 1 purple chemical, denoted as 7:p.The answer of these 7 beakers can also be written in the following format: 1:</em> 2:bb 3:<em> 4:</em> 5:rrrr 6:g 7:p.Model Answer: The initial state of each beaker is: First beaker has 2 yellow chemicals; Second beaker has 4 green chemicals; Third beaker has 0 chemicals; Fourth beaker has 2 purple chemicals; Fifth beaker has 1 red chemical; Sixth beaker has 4 orange chemicals; Seventh beaker has 0 chemicals.Actions: 1) Throw out three parts of the orange chemical.Before this action, the sixth beaker has 4 orange chemicals.After this action, the sixth beaker has 1 orange chemical.2) Pour the red chemical into the orange chemical.Before this action, the sixth beaker has 1 orange chemical; and the fifth beaker has 1 red chemical.After this action, the sixth beaker has 2 chemicals whose color is not clear at this moment; and the fifth beaker has 0 chemicals.3) Mix the orange chemical.Previous results show that before this action, the sixth beaker has 2 chemicals whose color is not clear.After this action, the sixth beaker has 2 orange chemicals.4) Pour out 1 part of green chemical.Before this action, the second beaker has 4 green chemicals.After this action, the second beaker has 3 green chemicals.5) Pour out two parts of green chemical.</p>
<p>Previous results show that before this action, the second beaker has 3 green chemicals.After this action, the second beaker has 1 green chemical.So the answer is: First beaker has 2 yellow chemicals, denoted as 1:yy; second beaker has 1 green chemical, denoted as 2:g; third beaker has 0 chemicals, denoted as 3:<em>; fourth beaker has 2 purple chemicals, denoted as 4:pp; fifth beaker has 0 chemicals, denoted as 5:</em>; sixth beaker has 2 orange chemicals, denoted as 6:oo; seventh beaker has 0 chemicals, denoted as 7:<em>.The answer of these 7 beakers can also be written in the following format: 1:yy 2:g 3:</em> 4:pp 5:<em> 6:oo 7:</em>.Question: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month for the past three months.He gets a $5 allowance a month.He also mows lawns and shovels driveways.He charges $15 to mow a lawn and $7 to shovel.After buying the shoes, he has $15 in change.If he mows 4 lawns, how many driveways did he shovel?</p>
<p>Answer: He has $15 left after buying the shoes that cost $95, which means he had $95 + $15 = $110 in total before the purchase.Next, we calculate the amount he earned from his allowance.He has been saving for three months, getting $5 each month, so from his allowance he earned 3 * $5 = $15.Then we know he mowed 4 lawns and he charges $15 per lawn, so from mowing lawns, he earned 4 * $15 = $60.Let's sum up the money he earned from his allowance and from mowing lawns: $15 (allowance) + $60 (mowing) = $75.Now, let's subtract the total amount of money earned from allowance and mowing from his total amount of money before buying the shoes to find out how much he earned from shoveling driveways: $110 (total before buying shoes) -$75 (earned from allowance and mowing) = $35.As he charges $7 to shovel a driveway, we divide the total earned from shoveling by the charge per driveway to find out the number of driveways he shoveled: $35 / $7 = 5 driveways.So the answer is 5.</p>
<p>Question: Brennan was researching his school project and had to download files from the internet to his computer to use for reference.After downloading 800 files, he deleted 70% of them because they were not helpful.He downloaded 400 more files but again realized that 3/5 of them were irrelevant.How many valuable files was he left with after deleting the unrelated files he downloaded in the second round?</p>
<p>Answer: First, let's figure out how many files Brennan kept after the first download.He initially downloaded 800 files, but he deleted 70% of them.So, the number of files he kept is 30% of the original 800.To find this, we multiply 800 by 0.30 (because 30% is the same as 0.30 when expressed as a decimal): 800 * 0.30 = 240 files.Then, Brennan downloaded 400 more files.However, he realized that 3/5 of them were not useful.So, he only kept 2/5 of the new files (because 1 -3/5 = 2/5).To find this number, we multiply 400 by 2/5: 400 * 2/5 = 160 files.So after deleting the unrelated files he downloaded in the second round, he was left with 240 files from the first download and 160 files from the second download: 240 + 160 = 400 files.Therefore, Brennan was left with 400 valuable files after deleting all the unrelated files.So the answer is 400.</p>
<p>Question: Noah is a painter.He paints pictures and sells them at the park.He charges $60 for a large painting and $30 for a small painting.Last month he sold eight large paintings and four small paintings.If he sold twice as much this month, how much is his sales for this month?</p>
<p>Answer: First, let's calculate how much Noah earned last month.From selling large paintings, he earned 8 (number of large paintings) * $60 (price per large painting) = $480.From selling small paintings, he earned 4 (number of small paintings) * $30 (price per small painting) = $120.So, his total sales for last month were $480 (from large paintings) + $120 (from small paintings) = $600.This month, he sold twice as many paintings.So his sales this month were 2 * $600 (last month's sales) = $1200.Therefore, Noah's sales for this month is $1200.So the answer is 1200.Answer: Her parents give her $15, and her grandparents give her twice that amount, which is 2 * $15 = $30.So, the total amount of money she has after receiving money from her parents and grandparents is $50 (her own money) + $15 (from parents) + $30 (from grandparents) = $95.Now, to find out how much more money she needs, we subtract the total amount of money she has from the cost of the wallet: $100 (cost of the wallet) -$95 (total amount of money she has) = $5.Therefore, Betty needs $5 more to buy the wallet.So the answer is 5.</p>
<p>Question: Julie is reading a 120-page book.Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday.If she wants to read half of the remaining pages tomorrow, how many pages should she read?</p>
<p>Answer: Yesterday, Julie read 12 pages.Today, she read twice as many pages as she did yesterday, so she read 2 * 12 = 24 pages today.So far, she has read 12 (yesterday's pages) + 24 (today's pages) = 36 pages in total.The book is 120 pages long, so she still has 120 (total pages) -36 (read pages) = 84 pages left to read.She wants to read half of the remaining pages tomorrow, so she should read 84 (remaining pages) / 2 = 42 pages.Therefore, Julie should read 42 pages tomorrow.So the answer is 42.Question: James writes a 3-page letter to 2 different friends twice a week.How many pages does he write a year?</p>
<p>Answer: Each letter James writes is 3 pages long.He writes to 2 different friends, so that's 3 (pages per letter) * 2 (letters) = 6 pages.Since he writes these letters twice a week, in a week, he writes 6 (pages per letter writing session) * 2 (letter writing sessions per week) = 12 pages.To calculate how many pages he writes in a year, we have to multiply the number of pages he writes in a week by the number of weeks in a year.There are 52 weeks in a year, so James writes 12 (pages per week) * 52 (weeks in a year) = 624 pages in a year.So, James writes 624 pages in a year.So the answer is 624.Answer: The amount after 3 years is Rs.815, and the amount after 4 years is Rs.854.The difference in amounts between the 3rd and 4th year is the simple interest for one year, because the principal remains constant in simple interest.So, the simple interest for one year is: Rs. 854 -Rs.815 = Rs.39.Now, if the interest for one year is Rs.39, then the interest for 3 years is: 3 * Rs. 39 = Rs.117.Given that the amount after 3 years (which includes the principal and 3 years of interest) is Rs.815, the principal (or the sum we want to find) is: Rs. 815 (amount after 3 years) -Rs.117 (3 years of interest) = Rs.698.Therefore, the sum is Rs.698.So the answer is (b).</p>
<p>Question:</p>
<p>The entrance fee for a fair is $5 for persons under the age of 18, and 20% more for persons older.Each ride at the fair costs $0.50.If Joe goes with her 6 years old twin brothers, and they each took 3 rides in total.How much money does Joe end up spending at the fair?Answer Choices: (a) 16 (b) 20.5 (c) 17.5 (d) 20 (e) 4.5</p>
<p>Answer: For entrance fee, given Joe's twin brothers are both 6 years old, they will pay this under-18 entrance fee: 2 * $5 = $10.For Joe (assuming Joe is 18 or older), the entrance fee is 20% more, which is $5 + ($5 * 0.20) = $5 + $1 = $6.Adding up the entrance fees: $6 (Joe) + $10 (twins) = $16.For ride cost, each of them took 3 rides and each ride needed $0.5: 3 rides × $0.50/ride = $1.50.Since there are three of them (Joe + 2 brothers): 3 * $1.50 = $4.50 in total for all rides.Now, adding up the entrance fee and ride cost: $16 (entrance fee) + $4.50 (ride cost) = $20.50.So, Joe ends up spending $20.50.So the answer is (b).Answer: Let's think step by step.let the original average expenditure be x rupees.Given there were 35 students originally, the total expenditure for the mess was 35x rupees.After 7 new students were admitted, the number of students became 35 + 7 = 42.According to the information, the average expenditure per head then diminished by re 1.This means the new average expenditure is x -1 rupees per student.Therefore, the new total expenditure for the mess with 42 students is 42(x -1) rupees.It's also given that due to the admission of 7 new students, the expenses of the mess increased by rs 84 per day.So we have 42(x -1) = 35x + 84 7 x = 126, x = 18.So, the original average expenditure per student was rs 18.Thus, the original expenditure of the mess was: 35 students × rs 18/student = rs 630.So the answer is (d), rs .630.</p>
<p>Question: a train 200 m long passes a man , running at 5 km / hr in the same direction in which the train is going , in 10 seconds .the speed of the train is ?Options: (a) 28 , (b) 50 , (c) 77 , (d) 22 , (e) 12</p>
<p>Answer: Let's think step by step.The train takes 10 seconds to pass the man.When a train passes an object, it covers a distance equal to its own length relative to that object.Therefore, in 10 seconds, the train covers a distance of 200m (its own length) relative to the man.So the speed of the train relative to man is ( 200 / 10 ) m/s = 20 m/s.To convert this speed from m/s to km/hr, we multiply by 18/5.So, the relative speed in km/hr = 20*(18/5) km/hr = 72 km/hr.The relative speed is the difference between the train's speed and the man's speed because they are moving in the same direction.Let's assume the speed of the train is x km/hr.Thus, the relative speed = x-5 km/hr.Since we already know the relative speed is 72 km/hr, we can have 72 km/hr = x-5 km/hr km/hr, x = 77 km / hr.So, the speed of the train is 77 km/hr.So the answer is (c), 77.Answer the following questions.</p>
<p>Question: The world contains 7 beakers.Each beaker may contain up to 4 units of colored chemical.The chemical can be poured into another beaker, drained away, or mixed.The initial state of each beaker is: First beaker has 3 green chemicals; Second beaker has 0 chemicals; Third beaker has 0 chemicals; Fourth beaker has 0 chemicals; Fifth beaker has 1 orange chemical; Sixth beaker has 3 orange chemicals; Seventh beaker has 4 green chemicals.We perform the following actions: Throw out two units of first beaker.Throw out fifth beaker.Throw out first one.Throw out orange beaker.Throw out one unit of green.</p>
<p>Question: what are the chemicals in each beaker?</p>
<p>Answer: The initial state of each beaker is: First beaker has 3 green chemicals; Second beaker has 0 chemicals; Third beaker has 0 chemicals; Fourth beaker has 0 chemicals; Fifth beaker has 1 orange chemical; Sixth beaker has 3 orange chemicals; Seventh beaker has 4 green chemicals.Actions: 1) Throw out two units of the first beaker.Before this action, the first beaker has 3 green chemicals, after this action, the first beaker has 1 green chemical.2) Throw out the fifth beaker.Before this action, the fifth beaker has 1 orange chemical.After this action, the fifth beaker has 0 chemicals.3) Throw out the first one.Previous results show that before this action, the first beaker has 1 green chemical.After this action, the first beaker has 0 green chemicals.4) Throw out the orange beaker.Before this action, the sixth beaker has 3 orange chemicals.After this action, the sixth beaker has 0 chemicals.5) Throw out one unit of green.Previous knowledge shows that currently only the seventh beaker has 4 green chemicals.After this action, the seventh beaker has 3 green chemicals.So the answer is: First beaker has 0 chemicals, denoted as 1:<em>; second beaker has 0 chemicals, denoted as 2:</em>; third beaker has 0 chemicals denoted as 3:<em>; fourth beaker has 0 chemicals, denoted as 4:</em>; fifth beaker has 0 chemicals, <em>; sixth beaker has 0 chemicals, denoted as 6:</em>; seventh beaker has 3 green chemicals, denoted as 7:ggg.The answer of these 7 beakers can also be denoted as the following format: 1:<em> 2:</em> 3:<em> 4:</em> 5:<em> 6:</em> 7:ggg.Answer: The initial state of each beaker is: First beaker has 2 yellow chemicals; Second beaker has 3 orange chemicals; Third beaker has 4 purple chemicals; Fourth beaker has 1 orange chemical; Fifth beaker has 4 green chemicals; Sixth beaker has 3 red chemicals; Seventh beaker has 3 purple chemicals.Actions: 1) Throw out one unit of the last beaker.Before this action, the seventh beaker has 3 purple chemicals.After this action, the seventh beaker has 2 purple chemicals.2) Throw out three units of the third one.Before this action, the third beaker has 4 purple chemicals.After this action, the third beaker has 1 purple chemical.3) Pour fourth beaker into it.Before this action, the third beaker has 1 purple chemical; and the fourth beaker has 1 orange chemical.After this action, the third beaker has 2 chemicals whose color is not clear at this moment; and the fourth beaker has 0 chemicals.4) It turns brown.Previous results show that before this action, the third beaker has 2 chemicals whose color is not clear.After this action, the third beaker has 2 brown chemicals.5) Throw out one unit of it.Previous results show that before this action, the third beaker has 2 brown chemicals.After this action, the third beaker has 1 brown chemical.So the answer is: First beaker has 2 yellow chemicals, denoted as 1:yy; second beaker has 3 orange chemicals, denoted as 2:ooo; third beaker has 1 brown chemical, denoted as 3:b; fourth beaker has 0 chemicals, denoted as 4:<em>; fifth beaker has 4 green chemicals, denoted as 5:gggg; sixth beaker has 3 red chemicals, denoted as 6:rrr; seventh beaker has 2 purple chemicals, denoted as 7:pp.The answer of these 7 beakers can also be written in the following format: 1:yy 2:ooo 3:b 4:</em> 5:gggg 6:rrr 7:pp.</p>
<p>Table 25: Few-shot exemplars of RESPROMPT for StrategyQA</p>
<p>Answer the following questions.</p>
<p>Question: Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?</p>
<p>Answer: Albany, Georgia had a population hovering around 75,000, and population had been gradually declining.Albany, New York, had a population around 100,000.Albany, New York's population had also been experiencing a gradual decline.Since Albany, New York is closer to the 100,000 number than Albany, Georgia, and they both have population decline, it seems unlikely that Albany, Georgia would reach 100,000 occupants before Albany, New York.So the answer is no.</p>
<p>Figure 1 :
1
Figure 1: CoT reasoning accuracy based on the number of reasoning steps for LLaMA-65B and LLaMA2-70B across two math benchmarks.Horizontal dashed lines are the overall accuracy in each benchmark.Left: GSM8K, 8-shot; Right: AQUA-RAT, 4-shot.CoT prompts are sourced from (Wei et al., 2022b).</p>
<p>the purchase, he had $95 + $15 = $110 in total.</p>
<p>Figure 2 :
2
Figure 2: (a) A multi-step math question from the training set of GSM8K (Cobbe et al., 2021).(b) Standard CoT prompting for this question.The intermediate steps are highlighted in blue.(c) The reasoning flow within the CoT prompts in (b), which exhibits a linear structure.(d) The underlying complex reasoning graph of this math question.(e) Our approach, RESPROMPT (residual connection prompting) for this question.The intermediate steps are highlighted in blue, while residual connections are indicated with colored backgrounds and linked by dashed arrows.Note that phrases with a blue background represent given conditions from the question, while phrases with backgrounds in other colors denote results derived from intermediate steps.mark in Figure 2 (a) and its complex underlying reasoning graph in Figure 2 (d).However, the "stepby-step" nature of standard CoT prompts typically generates a nearly linear reasoning flow (see Figure 2 (b)-(c)).This simplistic reasoning flow within CoT prompts has a structural mismatch with the complex underlying reasoning graph, thereby limiting CoT's effectiveness in handling questions that require multiple reasoning steps.</p>
<p>Figure 3 :
3
Figure 3: RESPROMPT's performance according to number of reasoning steps on GSM8K, AQUA-RAT and MathQA on LLaMA2-70B.The curves show the comparison of RESPROMPT's reasoning accuracy with CoT based baselines in each step, while the blue bars represent the distribution of data within each reasoning step.</p>
<p>Figure 4 :
4
Figure 4: Reasoning accuracy with different residual connections implementations.</p>
<p>Figure 5 :
5
Figure 5: Reasoning accuracy comparison between RESPROMPT and CoT across all LLaMA model sizes.CoT is the model with better performance between Short CoT and Long CoT for each dataset.</p>
<p>Figure 6 :
6
Figure6: Performance with varied exemplar orders using LLaMA2-70B on GSM8K and AQUA-RAT.</p>
<p>Figure 7 :
7
Figure 7: Case study.(a) A multi-step math question from GSM8K testing set.(b) The output of LLaMA2-70B with CoT prompts.(c) The output of LLaMA2-70B with RESPROMPT prompts.Connections built by LLMs with RESPROMPT are marked with colored and backgrounds .</p>
<p>guided with suitable prompts, display competitive reasoning skills without requiring fine-tuning(Wei  et al., 2022b; Fu et al., 2023a; Ni et al., 2023).A milestone is the CoT prompting approach(Wei  et al., 2022b), which offers step-by-step rationales.While numerous enhancements have been proposed forCoT (Wang et al., 2023b; Kojima et al., 2022;  Zhang et al., 2023; Gao et al., 2023; Zhou et al.,  2023b), it often falls short with complex multistep reasoning tasks(Fu et al., 2023b; Zhou et al.,  2023a).Our contribution introduces a residual connection based prompting strategy, outperforming standard CoT for multi-step reasoning.Multi-Step Reasoning.Simple CoT prompting struggles with complex, multi-step problems in LLMs.WhileZhou et al. (2023a) and (Khot et al.,  2023)  address this by decomposing questions and Fu et al. (2023b) integrate more complex reasoning steps and employ a majority voting mechanism, these methods generally add extra stages to reasoning.Our approach simplifies this by incorporating residual connections into prompts, facilitating a more efficient one-pass decoding process.</p>
<p>as in(Zhou et al., 2023a)) when using simple CoT prompting(Fu et al., 2023a,b).In response,Zhou et al. (2023a) and (Khot et al., 2023)  initially decompose a complex question into several sub-tasks and then address each sub-question sequentially.As an alternative approach, Fu et al. (2023b) introduce questions with higher reasoning complexity, as measured by the number of reasoning steps, into CoT prompts.They then utilize a majority voting mechanism on the most complex reasoning paths among the sampled ones to arrive at a final answer.Both approaches rely on an extra strategy beyond the intermediate reasoning steps of CoT, namely decomposition in Zhou et al. (2023a) and (Khot et al., 2023) and majority voting in Fu et al. (2023b), leading to a two-stage reasoning process.In contrast, our work shows that multi-step reasoning can be significantly enhanced by incorporating appropriate residual connections just in the intermediate reasoning steps, enabling a more</p>
<p>D. 2
2
Accuracy Breakdown Based on Number of Steps With LLaMA-65B.</p>
<p>Figure 8 Figure 8 :Figure 9 :
889
Figure8presents a breakdown of LLaMA-65B's reasoning accuracy based on the number of reasoning steps in each question.Similar to the results observed in LLaMA2-70B (as discussed in Section 3.2), RESPROMPT consistently outperforms CoT-based baseline approaches in improving LLaMA-65B's reasoning accuracy.Notably, as the number of steps in questions increases, RESPROMPT exhibits a smoother accuracy decline</p>
<p>Figure 10 :
10
Figure 10: StrategyQA data example.(a) A multi-step question.(b) Its underlying reasoning flow.</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: LLaMA-65B's performance based on number of few-shot exemplars in RESPROMPT.</p>
<p>Question:</p>
<p>Judy teaches 5 dance classes, every day, on the weekdays and 8 classes on Saturday.If each class has 15 students and she charges $15.00 per student, how much money does she make in 1 week?Model Answer: Judy teaches 5 classes every day on the weekdays, so she teaches 5 (classes per day) * 5 (days per week) = 25 classes on the weekdays.Each class has 15 students, so she teaches 25 (classes on the weekdays) * 15 (students per class) = 375 students on the weekdays.She charges $15 per student, so she makes 375 (students on the weekdays) * $15 (charge per student) = $5625 on the weekdays.On Saturday, she teaches 8 classes.Each class has 15 students, so she teaches 8 (classes on Saturday) * 15 (students per class) = 120 students on Saturday.She charges $15 per student, so she makes 120 (students on Saturday) * $15 (charge per student) = $1800 on Saturday.Adding these together, she makes $5625 (on the weekdays) + $1800 (on Saturday) = $7425 in a week.So, Judy makes $7425 in a week.So the answer is 7425.Question: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance.What percentage of the entire students enrolled in hip-hop dance?</p>
<p>Question:</p>
<p>John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home.He tries to get home in 4 hours but spends the first 2 hours in standstill traffic.He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph.How far is he from home at the end of those 4 hours?Model Answer: First, let's calculate how far John drove in the first 3 hours.He drove at a speed of 60 mph, so he drove 60 (mph) * 3 (hours) = 180 miles.Then, he turned around and drove for 4 hours.He spent the first 2 hours in standstill traffic, so he didn't move at all during that time.Then, he drove at a speed of 30 mph for half an hour, so he drove 30 (mph) * 0.5 (half an hour) = 15 miles.Finally, he drove at a speed of 80 mph for the remaining time of the 4 hours, which is 4 (hours) -0.5 (half an hour) = 3.5 hours.So, he drove 80 (mph) * 3.5 (hours) = 280 miles.Adding these together, he drove 180 (miles in the first 3 hours) + 15 (miles in the next half-hour) + 280 (miles in the remaining 3.5 hours) = 475 miles.Therefore, John is 475 miles from home at the end of those 4 hours.So the answer is 475.</p>
<p>Question:</p>
<p>The world contains 7 beakers.Each beaker may contain up to 4 units of colored chemical.The chemical can be poured into another beaker, drained away, or mixed.The initial state of each beaker is: First beaker has 2 yellow chemicals; Second beaker has 4 green chemicals; Third beaker has 0 chemicals; Fourth beaker has 2 purple chemicals; Fifth beaker has 1 red chemical; Sixth beaker has 4 orange chemicals; Seventh beaker has 0 chemicals.We perform the following actions: Throw out three parts of the orange chemical.Pour the red chemical into the orange chemical.Mix the orange chemical.Pour out 1 part of green chemical.Pour out two parts of green chemical.Question: what are the chemicals in each beaker?</p>
<p>Question: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km Answer: This person traveled at 20 km/hr for 2.5 hr, so the distance that this person traveled would have been 20 km/hr * 2.5 hrs = 50 km.So the answer is (e).Question: A clock shows the time as 9 a.m.If the minute hand gains 5 minutes every hour, how many minutes will the clock gain by 5 p.m.? Answer Choices: (a) 30 min (b) 35 min (c) 45 min (d) 40 min (e) 55 min Answer: The clock shows at 9 a.m., and until 5 p.m., it has been 8 hours.As the minute hand gains 5 minutes every hour, in these 8 hours, it will gain 5 min/hour * 8 hours = 40 mins.So the answer is (d).Question: A sum of money at simple interest amounts to Rs. 815 in 3 years and to Rs. 854 in 4 years.The sum is: Answer Choices: (a) 600 (b) 698 (c) 675 (d) 688 (e) 900</p>
<p>Question:</p>
<p>solution x contains 20 % of material a and 80 % of material b .solution y contains 30 % of material a and 70 % of material b .a mixture of both these solutions contains 22 % of material a in the final product .how much solution x is present in the mixture ?Options: (a) 40 % , (b) 60 % , (c) 80 % , (d) 100 % , (e) 110 % Answer: Answer: Let's think step by step.we can assume the total weight of the mixture = 100.Then let's denote the weight of solution x is w and the weight of solution y as 100-w (since the total weight of the mixture is 100).From the problem, solution x has 20% of Material A, which means 0.20w of Material A. And solution y has 30% of Material A, which means 0.30(100-w) of Material A. The mixture has 22% of Material A. This means that the mixture has 22% * 100 (total weight of the mixture) = 22 units of Material A. Therefore, using the above information, we can set up the equation: 0.20w + 0.30(100-w) = 22 -0.1 w = -8, w = 80.since we assumed the weight of the mixture = 100.Therefore presence of a in the mixture = 80/100 = 80%.So the answer is (c), 80%.Question: a trader sells 40 metres of cloth for rs .8200 at a profit of rs .35 per metre of cloth .how much profit will the trder earn on 40 metres of cloth ?Options: (a) rs .950 , (b) rs .1500 , (c) rs .1000 , (d) rs .1400 , (e) none of these Answer: Let's think step by step.Given that profit per metre of cloth = Rs.35.For 40 metres of cloth, the profit will be: Profit = 40 metres x Rs. 35/metre = Rs.1400.Thus, the trader will earn a profit of Rs. 1400 on 40 metres of cloth.So the answer is (d) Rs. 1400.</p>
<p>Question:</p>
<p>The world contains 7 beakers.Each beaker may contain up to 4 units of colored chemical.The chemical can be poured into another beaker, drained away, or mixed.The initial state of each beaker is: First beaker has 2 yellow chemicals; Second beaker has 3 orange chemicals; Third beaker has 4 purple chemicals; Fourth beaker has 1 orange chemical; Fifth beaker has 4 green chemicals; Sixth beaker has 3 red chemicals; Seventh beaker has 3 purple chemicals.We perform the following actions: Throw out one unit of last beaker.Throw out three units of third one.Pour fourth beaker into it.It turns brown.Throw out one unit of it.Question: what are the chemicals in each beaker?</p>
<p>Question:</p>
<p>Would a student of the class of 2017 have amnesia about 9/11?Answer: The event of 9/11 occurred on September 11, 2001.Class of 2017 high school students are round the age of 17 or 18 in 2017, so they would have been born in 1999 or 2000.Students born in 1999 or 2000 would have been 1 or 2 years old in 2001, the year of event 9/11.Children typically have amnesia before the age of 3 or 4. Since Class of 2017 students were 1 or 2 years old, which is younger than the age of 3 or 4, they would have amnesia about event 9/11.So the answer is yes.Question: Would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?Answer: A baker's dozen is traditionally 13.The width of a Mac truck is around 8 feet.The width of the Golden Gate Bridge is around 90 feet.If 13 Mack Trucks were placed side by side, they would occupy a width of about 13*8=104 feet.Since 104 feet (13 Mack Trucks placed side by side) is wider than 90 feet (Golden Gate Bridge), it would cause traffic jam.So the answer is yes.Question: Is the tibia necessary to win the Stanley Cup?Answer: The Stanley Cup is awarded to the team that wins the NHL playoffs.To win NHL playoffs, a team needs effective players who can skate.Effective skating generally requires two functional legs.The tibia is a crucial bone in the leg.Since healthy tibia is eseential for functional legs, tibia is necessary for skating.So the answer is yes.Question: Could a dandelion suffer from hepatitis?Answer: Hepatitis is an inflammation of the liver, typically affect mammals.Dandelions are flowering plants.Since flowering plants and mammals are very different biologically, Dandelions can not have hepatitis.So the answer is no..Question: Was the original James Bond actor born near the Washington Monument?Answer: The original actor to portray James Bond in the official film series was Sean Connery.Sean Connery was born in Fountainbridge, Edinburgh, Scotland.The Washington Monument is located in Washington, D.C., United States.Edinburgh, Scotland, and Washington, D.C., United States, are thousands of miles apart and are in two separate countries.Since Sean Connery was born in Edinburgh and the Washington Monument is in Washington, D.C., it's clear that the original James Bond actor was not born near the Washington Monument.So the answer is no.</p>
<p>Table 2
2: Reasoning accuracy over various positions tobuild residual connections within RESPROMPT prompts.Results on GSM8K and AQUA-RAT are shown.PositionsGSM8KAQUA-RAT65B 70B 65B 70BNo Residual 47.1 52.7 33.5 38.1First Half54.5 62.7 31.8 35.0Second Half 55.4 64.5 34.6 42.5Uniform58.4 65.4 35.8 38.5Full58.4 65.3 42.5 44.43.3 Ablation Studies: How Does RESPROMPTWork?</p>
<p>Table 3 :
3
Statistics of Error Types in LLaMA2-70B.
Error TypeGSM8K AQUAWrong Problem Solving-Wrong Reasoning Flow73.3% 33.3%-Wrong Residual Connection6.6%0%-Wrong Calculation/Derivation6.6%20.0%Repetition13.3% 33.3%Wrong Ground-truth0%13.3%</p>
<p>Table 4 :
4PromptingSVAMP (8-Shot)StrategyQA (6-Shot)Standard61.470.5LLaMAOriginal CoT Derived CoT68.7 63.270.0 71.2RESPROMPT 71.1(+2.4)70.2(-1.0)Standard62.172.8LLaMA2Original CoT Derived CoT73.7 70.076.1 72.6RESPROMPT 71.1(-2.6)73.1(-3.0)
Comparison between RESPROMPT and baselines on SVAMP and StrategyQA.The best results for each dataset are highlighted in boldface, the second-best results are underlined.Relative gains are highlighted in green, and relative losses are marked in red.</p>
<p>Table 5 :
5
Dataset statistics.Due to the large volume of MathQA ( †), we randomly sample 1000 examples to accelerate evaluation.Similarly, for StrategyQA ( ‡), we randomly sample 800 examples.
DatasetNumber of SamplesNumber of Steps1-step 2-step 3-step 4-step ≥ 5-stepGSM8K13196.3%27.1% 27.6% 22.0%17.0%AQUA-RAT2543.5%15.0% 17.3% 14.1%50.0%MathQA2985  †8.5%15.2% 21.4% 14.4%40.5%SVAMP100023.7% 76.2%---SCONE-Alchemy899----100%StrategyQA2289  ‡0.8%27.3% 53.2% 15.0%3.7%</p>
<p>Table 6 :
6
Reasoning accuracy of LLaMA2-Chat-70B on GSM8K, AQUA-RAT, MathQA and SCONE-Alchemy datasets.The best results of LLaMA2-Chat-70B for each dataset are highlighted in boldface, the second-best results are underlined.Relative gains are highlighted in green, and relative losses are marked in red.Results of LLaMA2-70B base model are listed for reference.</p>
<h1>Params GSM8K AQUA-RAT MathQASCONELLaMA2RESPROMPT70B65.344.439.224.3Standard70B13.324.424.92.2LLaMA2-ChatShort CoT Long CoT70B 70B52.2 51.833.0 32.634.4 36.1-11.6RESPROMPT70B61.1(+8.9)30.7(-2.3)39.6(+3.5) 16.3(+4.7)efficient one-pass decoding process.C Detailed Experimental SettingsC.1 Datasets Details.We use six benchmarks that cover three typeof tasks to evaluate the reasoning capability ofRESPROMPT: 1) Mathematical reasoning, in-cluding GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017), MathQA (Amini et al.,2019), SVAMP (Patel et al., 2021); 2) Sequen-tial reasoning, SCONE-Alchemy (Long et al.,2016); and 3) Commonsense reasoning: Strate-gyQA (Geva et al., 2021). Table 5 presents theirstatistics. GSM8K, MathQA, SVAMP, SCONE-Alchemy, and StrategyQA have annotations thatallow us to easily compute the number of reason-ing steps in each question. For AQUA-RAT, we useannotations from (Ribeiro et al., 2023) to derivethe step numbers. In addition, the original SCONE-Alchemy dataset lacks language descriptions ofobject states in each step, so we incorporate thelanguage annotations from (Ribeiro et al., 2023) todescribe the intermediate results.</h1>
<p>C.2 Hardware ResourcesRESPROMPT is a prompting based reasoning approach, and we only need to perform inference with LLMs.Therefore, a single experiment of RESPROMPT on the largest model used in this paper (LLaMA-65B and LLaMA2-70B) can be done on one AWS p4de.24xlargeinstance with appropriate choice of batch size (we fix the batch size to 3 for all benchmarks in this paper).</p>
<p>Table 7
7: Performance on GSM8K compared withTÜLU, a fine-tuned model based on LLaMA. TÜLU isprompted with 8-shot CoT. The numbers marked with  †are from (Wang et al., 2023c).7B13B30B65BTÜLU-CoT27.0  † 36.5  † 51.0  † 60.0  †LLaMA-CoT10.920.137.152.2-RESPROMPT 13.621.743.058.4</p>
<p>Table 8 :
8
Comparison between RESPROMPT and complexity based prompting on GSM8K dataset.8-step represents all exemplars in the prompts are questions requiring 8 reasoning steps, while 8&amp;9-step stands for a mix of 8-step and 9-step examples in prompts, and 9-step means all exemplars are 9-reasoning step questions.All prompts for complexity based prompting are from the official repository https://github.com/FranxYao/chain-of-thought-hub</p>
<h1>ParamsComplexity 8-step Complexity 8&amp;9-step RESPROMPT Complexity 9-step RESPROMPT (8-Shot) (8-Shot) (8-Shot) (4-Shot) (4-Shot)LLaMA65B48.349.658.454.559.2LLaMA270B64.263.865.363.567.5Question: Is the tibia necessary to win the Stanley Cup?(a) A Multi-Step Question From StrategyQAThe Stanley Cup is thechampionship trophy of theNational Hockey LeagueIce hockey is a game played byindividuals wearing ice skates tomove around a frozen playing fieldLeg is are actively involved in playing skatingThe tibia is a leg boneThe tibia belongs to leg.(b) Underlying Reasoning Flow</h1>
<p>Table 9 :
9
RESPROMPT performance under noise in prompts on GSM8K and AQUA-RAT datasets.
PromptsGSM8KAQUA-RAT65B 70B 65B 70BRESPROMPT-w/ noise56.1 64.4 28.3 36.6-w/o noise58.4 65.3 42.5 44.4that for the GSM8K dataset, we report LLaMA-65B's 5-shot accuracy for the 6-shot and 8-shotpositions in Figure 11. This adjustment is nec-essary because RESPROMPT's prompts with morethan 5 exemplars exceed the token length limitationof LLaMA-65B (2048).D.7 How Does Noise in Prompts AffectRESPROMPT?Most LLM prompts are human-crafted, leading toinevitable noise from annotation errors. We explorethe impact of noise on RESPROMPT by introduc-ing two perturbations into prompts: 1) Incorrectnumbers in reasoning steps, and 2) Linking prereq-uisites in later stages to incorrect early results. Asin Table 9, RESPROMPT proves robust to noise onGSM8K, echoing findings from (Min et al., 2022;Wang et al., 2023a; Madaan and Yazdanbakhsh,2022) that prompt format often outweighs interme-diate result accuracy. However, a clear accuracydip is seen in AQUA, hinting at dataset-dependentnoise sensitivity. A more comprehensive investiga-tion of this phenomenon is left for future research.D.8 More experiments on GSM8K.Compare to complexity based prompting (Fuet al., 2023b). Using more complex examples todesign prompts has been shown beneficial to rea-soning (Fu et al., 2023b). In Table 8, we com-
pare RESPROMPT with three versions of complexity based prompting.The results demonstrate that RESPROMPT consistently outperforms all the three versions of complexity based prompting.This comparison is also an ablation study that confirms that</p>
<p>Table 10 :
10
Comparison between RESPROMPT and multistep reasoning baselines on GSM8K dataset.We directly use the prompts as originally specified in respective papers.L2M means Least to Most prompting.
Decomp RESPROMPTL2MRESPROMPT(1-Shot)(1-Shot)(4-Shot)(4-Shot)LLaMA40.446.653.658.4LLaMA250.357.260.167.5</p>
<p>Table 11 :
11
Relative comparison of inference cost on GSM8K dataset using LLaMA2-70B.</p>
<h1>Tokens Inference Speed AccuracyOriginal-CoT1157.3Complexity3.76X0.56X64.2RESPROMPT3.06X0.65X65.3Table 12:Performance comparison with self-consistency on GSM8K dataset.#ParamCoT-SC RESPROMPT-SC (8-Shot) (8-Shot)LLaMA65B54.058.0LLaMA270B64.072.0tokens of prompts and outputs of RESPROMPTis about 3.06X more than the tokens in originalCoT (Wei et al., 2022b) on entire GSM8K testset, while the inference speed of RESPROMPT isabout 0.65X of original CoT. We acknowledge thatour prompt is longer than the original CoT andthus has higher inference cost. However, comparedto complexity based prompting (Fu et al., 2023b),RESPROMPT only has 3.06X/3.76X = 0.81X to-kens and is 0.65X/0.56X = 1.16X faster in infer-ence speed, while achieving a better performance.</h1>
<p>Table 13 :
13
Performance on GPT LLMs on GSM8K.
CoTRESPROMPT(8-Shot)(8-Shot)GPT-3.573.076.0GPT-491.093.0with self-consistency can further boost the perfor-mance of RESPROMPT. In addition, with self-consistency, RESPROMPT still achieves clearlyhigher reasoning accuracy than CoT.</p>
<p>Table 14 :
14
Comparison between RESPROMPT and baselines on CSQA and HotpotQA datasets.
CSQA (Acc) HotpotQA (EM/F1)(4-Shot)(4-Shot)Standard78.633.3/43.2CoT79.037.3/43.2RESPROMPT78.036.3/49.9</p>
<p>Table 15 :
15
Query and outputs of LLaMA2-70B-Chat.</p>
<p>Table 16 :
16
Examples of correct and wrong outputs by LLaMA2-70B on GSM8K dataset.</p>
<p>Table 19 :
19
Examples of correct and wrong outputs by LLaMA2-70B on SCONE-Alchemy dataset.</p>
<p>Table 20 :
20
Few-shot exemplars of RESPROMPT for GSM8K and SVAMP-Part 1 Tina makes $18.00 an hour.If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.If she works 10 hours every day for 5 days, how much money does she make?Answer: Tina makes $18.00 an hour for the first 8 hours of each day, and she gets overtime for anything beyond 8 hours.For 8 hours a day at $18.00 an hour, she makes: 8 hours/day * $18.00/hour = $144.00/day.For the overtime, she works an extra 2 hours a day (since she works 10 hours and the first 8 are not considered overtime).Overtime is paid at her hourly wage plus half her hourly wage, which is $18.00 + $9.00 = $27.00 per hour.So for 2 hours of overtime each day, she makes: 2 hours/day * $27.00/hour = $54.00/day.Therefore, in one day, she makes: $144.00/day+ $54.00/day = $198.00/day.Over the course of 5 days, she would make: 5 days * $198.00/day= $990.00.So, Tina would make $990.00if she worked 10 hours a day for 5 days.So the answer is 990.
Answer the following questions.Question:</p>
<p>Table 21 :
21
Few-shot exemplars of RESPROMPT for GSM8K and SVAMP-Part 2 Question: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?</p>
<p>Question: It takes Roque two hours to walk to work and one hour to ride his bike to work.Roque walks to and from work three times a week and rides his bike to and from work twice a week.How many hours in total does he take to get to and from work a week with walking and biking?Answer: First, let's calculate how much time Roque spends walking to work and back home.Each walk takes two hours and he walks both ways, so each day he spends 2 (hours per walk) * 2 (there and back) = 4 hours walking.He walks three times a week, so he spends 4 (hours per day) * 3 (days per week) = 12 hours per week walking.Next, let's calculate how much time Roque spends riding his bike to work and back home.Each bike ride takes one hour, and he rides both ways, so each day he spends 1 (hour per ride) * 2 (there and back) = 2 hours biking.He rides his bike twice a week, so he spends 2 (hours per day) * 2 (days per week) = 4 hours per week biking.Adding these together, Roque spends 12 (hours per week walking) + 4 (hours per week biking) = 16 hours getting to and from work each week.So, Roque spends 16 hours in total getting to and from work each week.So the answer is 16.</p>
<p>Table 22 :
22
Few-shot exemplars of RESPROMPT for AQUA-RAT</p>
<p>Answer the following questions.</p>
<p>Table 23 :
23
Few-shot exemplars of RESPROMPT for MathQAAnswer the following questions.Question: there were 35 students in a hostel .due to the admission of 7 new students the expenses of the mess were increased by rs .84 per day while the average expenditure per head diminished by re 1 .what was the original expenditure of the mess ?
Options: (a) rs 450 , (b) rs 920 , (c) rs 550 , (d) rs . 630 , (e) none of these</p>
<p>Table 24 :
24
Few-shot exemplars of RESPROMPT for SCONE-Alchemy</p>
<p>Question: Alex and Jacob works at a toy shop that make toys.Alex takes 7 hours to make a toy, and Jacob takes 9 hours to make a toy.During a month, both of them makes 35 toys in total.If both of them have worked for almost similar number of hours how many toys have been prepared by Jacob?Answer Choices: (a) 15 (b) 16 (c) 17 (d) 18 (e) 19Model Answer: The total number of toys made by both of them is 35.Alex takes 7 hours to make a toy, and Jacob takes 9 hours to make a toy.So, the total number of hours that Alex and Jacob have worked is: 35 toys × (7 hours + 9 hours) = 35 toys × 16 hours = 560 hours.Since they have worked for almost similar number of hours, we can assume that Alex and Jacob have worked for 560/2 = 280 hours each.Given that Jacob takes 9 hours to make a toy, the number of toys that Jacob has made is: 280 hours ÷ 9 hours/toy = 31.11toys.Since Jacob has made 31.11toys, he has made 31 toys and 11% of a toy.So, the number of toys that Jacob has made is: 31 toys + 11% of a toy = 31 toys + 11% × 1 toy = 31 toys + 0.11 toy = 31.11toys.So, the number of toys that Jacob has made is 31.11toys.So the answer is (a).Model Answer: The hollow cube has 6 faces, and each face has 4 smaller cubes.So, the total number of smaller cubes on the faces is: 6 faces * 4 smaller cubes/face = 24 smaller cubes.Now, the hollow cube has 12 edges, and each edge has 2 smaller cubes.So, the total number of smaller cubes on the edges is: 12 edges * 2 smaller cubes/edge = 24 smaller cubes.Now, the hollow cube has 8 vertices, and each vertex has 3 smaller cubes.So, the total number of smaller cubes on the vertices is: 8 vertices * 3 smaller cubes/vertex = 24 smaller cubes.Adding up the number of smaller cubes on the faces, edges, and vertices: 24 smaller cubes (faces) + 24 smaller cubes (edges) + 24 smaller cubes (vertices) = 72 smaller cubes.Now, the hollow cube has 24 smaller cubes on the faces, edges, and vertices, and the total number of smaller cubes is 72.So, the number of smaller cubes that are not on the faces, edges, and vertices is: 72 smaller cubes -24 smaller cubes = 48 smaller cubes.So, the answer is (a).Question: The original retail price of an appliance was 60 percent more than its wholesale cost.If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?Answer Choices: (a) 20% (b) 28% (c) 36% (d) 40% (e) 42%Model Answer: The original retail price was 60% more than its wholesale cost.So, the original retail price is: 100% + 60% = 160% of the wholesale cost.Now, the appliance was actually sold for 20% less than the original retail price.So, the actual selling price is: 100% -20% = 80% of the original retail price.Given that the original retail price is 160% of the wholesale cost, the actual selling price is: 80% of 160% = 128% of the wholesale cost.So, the appliance was sold for 128% more than its wholesale cost.So the answer is (c).
Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/n19-1245Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 2019</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
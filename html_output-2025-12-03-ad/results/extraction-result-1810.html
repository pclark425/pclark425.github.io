<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1810 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1810</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1810</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-abd61863fc793d5fa0334b80dea2130ad543de70</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/abd61863fc793d5fa0334b80dea2130ad543de70" target="_blank">The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> Despite static benchmarks not incorporating humans-in-the-loop, it is found that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support.</p>
                <p><strong>Paper Abstract:</strong> Evaluation of large language models for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), or more recently using human preferences of LLM responses. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks or more preferred LLM responses translate to programmer productivity when coding with LLMs, including time spent coding. We introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=243) using RealHumanEval in which users interacted with seven LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better proxy signals. We open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1810.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1810.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static benchmarks (HumanEval/MBPP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static coding benchmarks (e.g., HumanEval, MBPP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated execution-based benchmarks that measure functional correctness of LLM-generated code (e.g., pass@k on held-out unit tests). The paper compares model improvements on these benchmarks to programmer productivity in a human-in-the-loop study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated benchmark metrics (HumanEval-style pass/unit-test based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python code</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>functional correctness measured by unit tests / pass@k-style benchmark metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>User study with 243 participants coding on RealHumanEval; participant submissions were tested against private unit tests to measure task completion and time-to-success (objective productivity measures).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Not applicable—human data are participant programmers (students and professionals) in the study rather than external domain expert raters.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No explicit correlation coefficient reported between benchmark scores and human metrics; authors compared trends using regressions, p-values, and confidence intervals (OLS with Benjamini–Hochberg correction).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Across model families and both forms of assistance (autocomplete and chat), improvements in static benchmark performance tended to correspond to improved human productivity (notably reductions in time-to-completion).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Large gaps in benchmark performance did not translate proportionally to human productivity gains (after a certain point additional benchmark improvements yielded little or no extra practical utility); e.g., substantial HumanEval gaps between models sometimes produced only small or statistically indistinguishable differences in time-to-completion.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Effect varied by task type: LLM assistance (and thus alignment with benchmarks) produced larger time savings on data-manipulation tasks, smaller gains for editing/augmenting code, and sometimes increased time on algorithmic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Not quantitatively measured; authors note that benchmarks reflect functional correctness but do not capture human-in-the-loop context and other practical factors, so clarity of benchmark criteria does not guarantee proportional productivity improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>888 tasks, 243 participants</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Benchmarks partially aligned with human productivity (especially time reductions) but the relationship was not proportional and varied by model and task; no numeric agreement coefficient was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Static coding benchmarks are informative of downstream programmer productivity in RealHumanEval—better benchmark performance generally reduces time-to-completion—but large benchmark improvements do not necessarily yield commensurate productivity gains and do not always increase the number of tasks completed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Benchmark–human misalignment can arise because benchmarks omit humans-in-the-loop and context; study tasks are short and limited in variety, limiting generalizability; no explicit numeric agreement metrics were reported relating benchmark scores to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1810.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1810.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preference metrics (acceptance / copy / Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human preference proxy metrics: suggestion acceptance rate, chat copy rate, and Likert-style helpfulness rating</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proxy signals commonly available in deployed assistants (fraction of autocomplete suggestions accepted, fraction of chat responses from which code was copied, and post-study Likert helpfulness rating). The paper compares these proxies to objective productivity measures from the user study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Likert-style subjective rating (1–10) and interaction-derived proxies (autocomplete acceptance rate, fraction of chat messages with a code-copy event)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>autocomplete suggestions and chat-generated code snippets (source code)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python code</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>perceived helpfulness (Likert 1–10), acceptance of inline suggestions, copies-per-chat-response, and persistence of accepted suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Participants (N=243) provided post-study helpfulness ratings (1–10) and their interaction telemetry was logged (accept/reject events for autocomplete, copy counts for chat). Objective productivity was measured via unit tests and time-to-success.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Study participants were a mix of undergraduate/graduate students and software practitioners (predominantly students with programming experience), not designated external expert raters.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No direct correlation coefficient between preference proxies and objective productivity reported; authors used group comparisons (differences, OLS regressions), p-values, and confidence intervals to test alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Preference proxies aligned well with subjective helpfulness: higher acceptance and copy rates generally matched higher post-study helpfulness ratings (participants' preferences correlated with their perceptions).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Preference proxies did NOT reliably predict objective productivity (time-to-completion or number of tasks completed); acceptance and copy rates were influenced by external factors (latency, perceived context/relevance) that do not necessarily affect downstream task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Alignment between preference proxies and productivity varied by task type; e.g., larger productivity gains (and stronger usefulness) occurred on data-manipulation tasks where the proxies were more meaningful, whereas for algorithmic tasks proxies were less predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Not quantitatively measured—authors report that perceived lack of context, latency, verbosity, and other interface factors affected preference metrics even when productivity was unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>5204 autocomplete suggestions logged, 1055 chat messages logged, 243 participants</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Preference proxies matched subjective human ratings but did not correlate with objective programmer performance; thus these proxies can be misleading if used as stand-ins for productivity.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Acceptance rates and copy rates are convenient proxies available in deployment and do reflect user-perceived helpfulness, but in this study they did not reliably indicate actual productivity gains; designers should not assume preference metrics imply better downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Acceptance rate is myopic (users may accept suggestions to preview formatting or delete them shortly after); proxies are sensitive to latency, perceived context, verbosity, and whether suggestions were explicitly requested; no quantitative correlation coefficients with productivity were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1810.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1810.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Suggestion persistence metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persistence of accepted autocomplete suggestions (tracking whether an accepted suggestion remains in the user's code over time)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative, less-myopic proxy that tracks whether accepted suggestions are still present in the user's code after 15s/30s/60s to approximate usefulness and persistence of suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>persistence-based telemetry (time-based presence of accepted suggestions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>autocomplete code suggestions (source code fragments)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python code</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>persistence fraction at multiple time horizons (15s, 30s, 60s) after acceptance</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Telemetry logged and analyzed to determine whether accepted suggestions remained exactly in the user's code at fixed time checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Participants in the study (programmers/students); not external experts.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Suggestions that persisted more frequently (e.g., for GPT-3.5 and CodeLlama-34b) aligned with productivity assessments indicating higher utility.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Not explicitly quantified; authors note that simple acceptance rate can be misleading whereas persistence appears more informative.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not explicitly quantified in the paper for persistence metric; persistence analysis was aggregated across models.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Not applicable/not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Subset of the 5204 autocomplete suggestions (all accepted suggestions were tracked across timepoints)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Persistence correlated better with productivity judgments than raw acceptance rate (authors report that GPT-3.5 and CodeLlama-34b suggestions persisted more frequently, confirming productivity assessments).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tracking persistence of accepted suggestions is a more robust proxy than raw acceptance rate because it discounts ephemeral acceptances used only to inspect formatting; persistence better matched productivity assessments in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Persistence is still an indirect proxy and may not capture later edits or intent; the study did not provide formal numeric agreement measures between persistence and productivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1810.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1810.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge approaches (using LLMs to score or compare outputs by preference or rubric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use LLMs themselves to evaluate or rank model outputs (e.g., pairwise preference judgments or rubric-style scoring). The paper references related work on LLM-as-a-judge but does not apply it in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (mentioned in related work, e.g., Chatbot Arena and MT-bench style human-preference evaluation platforms)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>general (LLM outputs, conversational responses, code snippets in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general / natural language and code (as discussed in related citations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>preference judgments or rubric-based scoring (as described in cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Mentioned as human-centric evaluation alternative in related work; not used in authors' experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Not reported in this paper (discussed in related work citations).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Not reported in this paper; authors point to related work that explores LLM-as-a-judge reliability but did not evaluate it here.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-as-a-judge is cited in related work as an approach for collecting or simulating preference judgments, but this study did not use LLM-as-a-judge; authors instead compared automated static benchmarks and human preference proxies to measured human productivity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>The paper does not evaluate LLM-as-a-judge methods and therefore does not report on agreement metrics or conditions affecting alignment for those methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers", 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Aligning offline metrics and human judgments of value for code generation models <em>(Rating: 2)</em></li>
                <li>Productivity assessment of neural code completion <em>(Rating: 2)</em></li>
                <li>Chatbot arena: An open platform for evaluating llms by human preference <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Alpacafarm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1810",
    "paper_id": "paper-abd61863fc793d5fa0334b80dea2130ad543de70",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "Static benchmarks (HumanEval/MBPP)",
            "name_full": "Static coding benchmarks (e.g., HumanEval, MBPP)",
            "brief_description": "Automated execution-based benchmarks that measure functional correctness of LLM-generated code (e.g., pass@k on held-out unit tests). The paper compares model improvements on these benchmarks to programmer productivity in a human-in-the-loop study.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated benchmark metrics (HumanEval-style pass/unit-test based evaluation)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code",
            "artifact_domain": "Python code",
            "evaluation_criteria": "functional correctness measured by unit tests / pass@k-style benchmark metrics",
            "human_evaluation_setup": "User study with 243 participants coding on RealHumanEval; participant submissions were tested against private unit tests to measure task completion and time-to-success (objective productivity measures).",
            "human_expert_count": null,
            "human_expert_expertise": "Not applicable—human data are participant programmers (students and professionals) in the study rather than external domain expert raters.",
            "agreement_metric": "No explicit correlation coefficient reported between benchmark scores and human metrics; authors compared trends using regressions, p-values, and confidence intervals (OLS with Benjamini–Hochberg correction).",
            "agreement_score": null,
            "high_agreement_conditions": "Across model families and both forms of assistance (autocomplete and chat), improvements in static benchmark performance tended to correspond to improved human productivity (notably reductions in time-to-completion).",
            "low_agreement_conditions": "Large gaps in benchmark performance did not translate proportionally to human productivity gains (after a certain point additional benchmark improvements yielded little or no extra practical utility); e.g., substantial HumanEval gaps between models sometimes produced only small or statistically indistinguishable differences in time-to-completion.",
            "artifact_complexity_effect": "Effect varied by task type: LLM assistance (and thus alignment with benchmarks) produced larger time savings on data-manipulation tasks, smaller gains for editing/augmenting code, and sometimes increased time on algorithmic problems.",
            "criteria_clarity_effect": "Not quantitatively measured; authors note that benchmarks reflect functional correctness but do not capture human-in-the-loop context and other practical factors, so clarity of benchmark criteria does not guarantee proportional productivity improvements.",
            "sample_size": "888 tasks, 243 participants",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Benchmarks partially aligned with human productivity (especially time reductions) but the relationship was not proportional and varied by model and task; no numeric agreement coefficient was reported.",
            "calibration_or_training": null,
            "key_findings": "Static coding benchmarks are informative of downstream programmer productivity in RealHumanEval—better benchmark performance generally reduces time-to-completion—but large benchmark improvements do not necessarily yield commensurate productivity gains and do not always increase the number of tasks completed.",
            "limitations_noted": "Benchmark–human misalignment can arise because benchmarks omit humans-in-the-loop and context; study tasks are short and limited in variety, limiting generalizability; no explicit numeric agreement metrics were reported relating benchmark scores to human performance.",
            "uuid": "e1810.0",
            "source_info": {
                "paper_title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Preference metrics (acceptance / copy / Likert)",
            "name_full": "Human preference proxy metrics: suggestion acceptance rate, chat copy rate, and Likert-style helpfulness rating",
            "brief_description": "Proxy signals commonly available in deployed assistants (fraction of autocomplete suggestions accepted, fraction of chat responses from which code was copied, and post-study Likert helpfulness rating). The paper compares these proxies to objective productivity measures from the user study.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Likert-style subjective rating (1–10) and interaction-derived proxies (autocomplete acceptance rate, fraction of chat messages with a code-copy event)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "autocomplete suggestions and chat-generated code snippets (source code)",
            "artifact_domain": "Python code",
            "evaluation_criteria": "perceived helpfulness (Likert 1–10), acceptance of inline suggestions, copies-per-chat-response, and persistence of accepted suggestions",
            "human_evaluation_setup": "Participants (N=243) provided post-study helpfulness ratings (1–10) and their interaction telemetry was logged (accept/reject events for autocomplete, copy counts for chat). Objective productivity was measured via unit tests and time-to-success.",
            "human_expert_count": null,
            "human_expert_expertise": "Study participants were a mix of undergraduate/graduate students and software practitioners (predominantly students with programming experience), not designated external expert raters.",
            "agreement_metric": "No direct correlation coefficient between preference proxies and objective productivity reported; authors used group comparisons (differences, OLS regressions), p-values, and confidence intervals to test alignment.",
            "agreement_score": null,
            "high_agreement_conditions": "Preference proxies aligned well with subjective helpfulness: higher acceptance and copy rates generally matched higher post-study helpfulness ratings (participants' preferences correlated with their perceptions).",
            "low_agreement_conditions": "Preference proxies did NOT reliably predict objective productivity (time-to-completion or number of tasks completed); acceptance and copy rates were influenced by external factors (latency, perceived context/relevance) that do not necessarily affect downstream task completion.",
            "artifact_complexity_effect": "Alignment between preference proxies and productivity varied by task type; e.g., larger productivity gains (and stronger usefulness) occurred on data-manipulation tasks where the proxies were more meaningful, whereas for algorithmic tasks proxies were less predictive.",
            "criteria_clarity_effect": "Not quantitatively measured—authors report that perceived lack of context, latency, verbosity, and other interface factors affected preference metrics even when productivity was unchanged.",
            "sample_size": "5204 autocomplete suggestions logged, 1055 chat messages logged, 243 participants",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Preference proxies matched subjective human ratings but did not correlate with objective programmer performance; thus these proxies can be misleading if used as stand-ins for productivity.",
            "calibration_or_training": null,
            "key_findings": "Acceptance rates and copy rates are convenient proxies available in deployment and do reflect user-perceived helpfulness, but in this study they did not reliably indicate actual productivity gains; designers should not assume preference metrics imply better downstream performance.",
            "limitations_noted": "Acceptance rate is myopic (users may accept suggestions to preview formatting or delete them shortly after); proxies are sensitive to latency, perceived context, verbosity, and whether suggestions were explicitly requested; no quantitative correlation coefficients with productivity were reported.",
            "uuid": "e1810.1",
            "source_info": {
                "paper_title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Suggestion persistence metric",
            "name_full": "Persistence of accepted autocomplete suggestions (tracking whether an accepted suggestion remains in the user's code over time)",
            "brief_description": "An alternative, less-myopic proxy that tracks whether accepted suggestions are still present in the user's code after 15s/30s/60s to approximate usefulness and persistence of suggestions.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "persistence-based telemetry (time-based presence of accepted suggestions)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "autocomplete code suggestions (source code fragments)",
            "artifact_domain": "Python code",
            "evaluation_criteria": "persistence fraction at multiple time horizons (15s, 30s, 60s) after acceptance",
            "human_evaluation_setup": "Telemetry logged and analyzed to determine whether accepted suggestions remained exactly in the user's code at fixed time checkpoints.",
            "human_expert_count": null,
            "human_expert_expertise": "Participants in the study (programmers/students); not external experts.",
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Suggestions that persisted more frequently (e.g., for GPT-3.5 and CodeLlama-34b) aligned with productivity assessments indicating higher utility.",
            "low_agreement_conditions": "Not explicitly quantified; authors note that simple acceptance rate can be misleading whereas persistence appears more informative.",
            "artifact_complexity_effect": "Not explicitly quantified in the paper for persistence metric; persistence analysis was aggregated across models.",
            "criteria_clarity_effect": "Not applicable/not measured.",
            "sample_size": "Subset of the 5204 autocomplete suggestions (all accepted suggestions were tracked across timepoints)",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Persistence correlated better with productivity judgments than raw acceptance rate (authors report that GPT-3.5 and CodeLlama-34b suggestions persisted more frequently, confirming productivity assessments).",
            "calibration_or_training": null,
            "key_findings": "Tracking persistence of accepted suggestions is a more robust proxy than raw acceptance rate because it discounts ephemeral acceptances used only to inspect formatting; persistence better matched productivity assessments in this study.",
            "limitations_noted": "Persistence is still an indirect proxy and may not capture later edits or intent; the study did not provide formal numeric agreement measures between persistence and productivity.",
            "uuid": "e1810.2",
            "source_info": {
                "paper_title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-as-a-judge (related work)",
            "name_full": "LLM-as-a-judge approaches (using LLMs to score or compare outputs by preference or rubric)",
            "brief_description": "Approaches that use LLMs themselves to evaluate or rank model outputs (e.g., pairwise preference judgments or rubric-style scoring). The paper references related work on LLM-as-a-judge but does not apply it in the study.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-a-judge (mentioned in related work, e.g., Chatbot Arena and MT-bench style human-preference evaluation platforms)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "general (LLM outputs, conversational responses, code snippets in cited works)",
            "artifact_domain": "general / natural language and code (as discussed in related citations)",
            "evaluation_criteria": "preference judgments or rubric-based scoring (as described in cited literature)",
            "human_evaluation_setup": "Mentioned as human-centric evaluation alternative in related work; not used in authors' experiment.",
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Not reported in this paper (discussed in related work citations).",
            "low_agreement_conditions": "Not reported in this paper; authors point to related work that explores LLM-as-a-judge reliability but did not evaluate it here.",
            "artifact_complexity_effect": null,
            "criteria_clarity_effect": null,
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": null,
            "calibration_or_training": null,
            "key_findings": "LLM-as-a-judge is cited in related work as an approach for collecting or simulating preference judgments, but this study did not use LLM-as-a-judge; authors instead compared automated static benchmarks and human preference proxies to measured human productivity.",
            "limitations_noted": "The paper does not evaluate LLM-as-a-judge methods and therefore does not report on agreement metrics or conditions affecting alignment for those methods.",
            "uuid": "e1810.3",
            "source_info": {
                "paper_title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Aligning offline metrics and human judgments of value for code generation models",
            "rating": 2
        },
        {
            "paper_title": "Productivity assessment of neural code completion",
            "rating": 2
        },
        {
            "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference",
            "rating": 2
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "rating": 1
        }
    ],
    "cost": 0.0146555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The RealHumanEval: Evaluating Large Language Models ${ }^{\circ}$ Abilities to Support Programmers</h1>
<p>Hussein Mozannar ${ }^{<em> 1,2,6}$, Valerie Chen</em>3, Mohammed Alsobay ${ }^{2}$, Subhro Das ${ }^{1,4}$, Sebastian Zhao ${ }^{5}$, Dennis Wei ${ }^{1,4}$, Manish Nagireddy ${ }^{1,4}$, Prasanna Sattigeri ${ }^{1,4}$, Ameet Talwalkar ${ }^{3}$, and David Sontag ${ }^{1,2}$<br>${ }^{1}$ MIT-IBM Watson AI Lab<br>${ }^{2}$ Massachusetts Institute of Technology<br>${ }^{3}$ Carnegie Mellon University<br>${ }^{4}$ IBM Research<br>${ }^{5}$ University of California, Berkeley<br>${ }^{6}$ Microsoft Research</p>
<h4>Abstract</h4>
<p>Evaluation of large language models for code has primarily relied on static benchmarks, including HumanEval [Chen et al., 2021], or more recently using human preferences of LLM responses. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks or more preferred LLM responses translate to programmer productivity when coding with LLMs, including time spent coding. We introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study ( $\mathrm{N}=243$ ) using RealHumanEval in which users interacted with seven LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional - a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better proxy signals. We open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.</p>
<h2>1 Introduction</h2>
<p>Coding benchmarks such as HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021] play a key role in evaluating the capabilities of large language models (LLMs) as programming becomes a valuable application through products such as GitHub Copilot [Github, 2022] and ChatGPT [OpenAI, 2022]. These benchmarks quantify LLM abilities by measuring how well a model can complete entire coding tasks. As LLMs are increasingly adopted as programmer assistants - providing chat responses or autocomplete suggestions, rather</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We introduce RealHumanEval, an end-to-end online evaluation platform of LLM-assisted coding through autocomplete suggestions and chat support. The goal of RealHumanEval is to facilitate human-centric evaluation of code-generating LLMs, simplifying the workflow for researchers to conduct user studies to measure the effect of LLM assistance on downstream human productivity and preferences. We selected 4 families of LLMs of varying sizes (GPT-4o, GPT-3.5, CodeLlama-34b, CodeLlama-7b) for use with RealHumanEval to study the alignment between static benchmark performance, subjective programmer preference judgments, and programmer productivity.
than full code generations - prior works have argued for bringing humans-in-the-loop to evaluate LLMs [Lee et al., 2023, Chiang et al., 2024]. A predominant human-centric approach collects human preference judgments of intermediate LLM outputs, whether between pairs of LLM responses (e.g., Chatbot Arena [Chiang et al., 2024]) or, for coding in particular, using programmer acceptance rates of LLM suggestions (e.g., in products such as Github Copilot [Bird et al., 2022]). However, such evaluation may not capture the LLM's downstream impact on programmer productivity.</p>
<p>Evaluating the utility of LLMs on downstream productivity requires conducting user studies where programmers code with LLM assistance. While a set of small-scale user studies have been conducted to primarily build a qualitative understanding of how programmers use LLM assistance, they are typically restricted to evaluations on one model and one form of LLM support, primarily relying on commercial tools like Github Copilot or ChatGPT [Barke et al., 2023, Mozannar et al., 2024, Vaithilingam et al., 2022, Ross et al., 2023, Liang et al., 2023, Peng et al., 2023]. To enable evaluations of a broader set of LLMs and lower the barrier to conducting these studies, we introduce an online evaluation platform, RealHumanEval</p>
<p>(Figure 1). The platform consists of a code editor where programmers can solve coding tasks with two common forms of LLM assistance: programmers can either ask questions to the LLM through a chat window or receive code completion suggestions through an autocomplete system inside the editor. The interface also supports executing and testing code and logging telemetry which can be used to compute productivity metrics, including time to complete a task or number of tasks completed, and preference metrics, including average acceptance rates of suggestions and the likelihood of copying code from chat responses.</p>
<p>Using RealHumanEval, we conduct a user study with 243 participants to understand the effect of a model's benchmark performance and the form of LLM assistance on downstream productivity metrics. Each participant was assigned to one of seven conditions: a control condition with no LLM support, three conditions with autocomplete support from either CodeLlama-7b [Rozière et al., 2023], CodeLlama-34b [Rozière et al., 2023], or GPT-3.5-turbo-instruct[Brown et al., 2020], and finally three conditions where the editor is equipped with a chat window powered by the chat variants of the previous models in addition to GPT-4o [OpenAI, 2022]. We deliberately select model families with increasingly higher benchmark performance and consider model pairs within each family with similar benchmark performance to understand the effect of autocomplete versus chat assistance. Through the study, we collected a dataset of interactions on 888 coding total tasks, where 5204 autocomplete suggestions were shown and 1055 chat messages were sent.</p>
<p>Overall, we find that improving a model's base performance on existing coding benchmarks leads to gains in human productivity, particularly in the time spent completing tasks. These trends were present across both chat and autocomplete interactions, validating the potential "generalizability" of benchmarks to more realistic contexts. However, we observe that gaps in benchmark versus human performance are not necessarily proportional, suggesting that further gains in benchmark performance do not necessarily translate into equivalent gains in human productivity. We also investigated whether human preference metrics, such as the average acceptance rate of suggestions and the likelihood of copying code from chat responses, are aligned with productivity metrics. While these preference metrics are readily available in real deployments of LLM systems compared to task completion time and thus can be attractive proxy metrics [Ziegler et al., 2022], we find that they are only correlated with programmer perceptions of LLM helpfulness but not necessarily with actual programmer performance. The dissimilar findings between benchmarking and human preference metrics highlight the importance of careful evaluation to disentangle which metrics are indicative of downstream performance.</p>
<p>In summary, our contributions are as follows:</p>
<ol>
<li>An open-source platform RealHumanEval to encourage more human-centric evaluations of code LLMs. Code is available at ${ }^{1}$</li>
<li>An evaluation of 7 code LLMs of varying performance using RealHumanEval to provide insights into the alignment and discrepancies between benchmark performance and human preferences with downstream user productivity. Our findings emphasize the importance of studying how programmers interact with code LLMs through user studies to identify nuances in programmer-LLM interactions.</li>
<li>We release the dataset of interactions collected from this study to guide the development of better coding assistants. Datasets are available at ${ }^{2}$.
<sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<h1>2 Related Work</h1>
<p>Table 1: A comparison of our study against prior studies understanding programmer-LLM interactions in terms of the number of participants, models, types of LLM interaction, and tasks. Note that Cui et al. [2024] was a field experiment and thus not a controlled user study with a fixed number of tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Study</th>
<th style="text-align: center;"># participants</th>
<th style="text-align: center;"># models</th>
<th style="text-align: center;">Autocomplete?</th>
<th style="text-align: center;">Chat?</th>
<th style="text-align: center;"># tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vaithilingam et al. [Vaithilingam et al., 2022]</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Peng et al. [Peng et al., 2023]</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Barke et al. [Barke et al., 2023]</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Prather et al. [Prather et al., 2023]</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Mozannar et al. [Mozannar et al., 2024]</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Vasconcelos et al. [Vasconcelos et al., 2023]</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Cui et al. [Cui et al., 2024]</td>
<td style="text-align: center;">1974</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">*</td>
</tr>
<tr>
<td style="text-align: center;">Ross et al. [Ross et al., 2023]</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Chopra et al. [Chopra et al., 2023]</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Gu et al. [Gu et al., 2024]</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Kazemitabaar et al. [Kazemitabaar et al., 2023]</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;">Nam et al. [Nam et al., 2024]</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">243</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">17</td>
</tr>
</tbody>
</table>
<p>Coding Benchmarks. Benchmarks are essential for tracking the progress of LLMs, and coding benchmarks are a key piece [Achiam et al., 2023, Laskar et al., 2023, Zan et al., 2023, Hou et al., 2023]. Moreover, the coding ability of an LLM can be informative of its reasoning abilities [Madaan et al., 2022]; thus, performance on coding benchmark is of broader interest. While HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021] are the most commonly used coding benchmarks, many extensions and further benchmarks have been proposed [Lu et al., 2021, Nijkamp et al., 2023, Zhu et al., 2022, Liu et al., 2023, Jimenez et al., 2023, Khan et al., 2023, Yang et al., 2024, Yan et al., 2023], we highlight a few: EvalPlus extends HumanEval's test cases [Liu et al., 2023], MultiPL-E [Cassano et al., 2023] to other languages, ReCode with robustness checks [Wang et al., 2023], HUMANEVALPACK [Muennighoff et al., 2023] with code repair and explanation tasks, and buggy-HumanEval [Dinh et al., 2023] with bugs in the reference code. Relatedly, the DS-1000 [Lai et al., 2023] benchmark evaluates models' abilities on data science problems that require using external libraries. More involved evaluations include the multi-turn program evaluation benchmark [Nijkamp et al., 2023] and SWE-bench [Jimenez et al., 2023], which requires the LLM to resolve GitHub issues. While existing benchmarks evaluate a diverse set of LLM behaviors across models, these benchmarks do not, however, include a programmer-in-the-loop, as there would be in a real-world setup. Our evaluation complements this existing line of work by conducting a user study, where programmers put these behaviors to the test in realistic scenarios.</p>
<p>Preference Metrics. Instead of relying solely on coding benchmarks' pass@k metrics, which consider only the functional correctness of LLM-generated code, recent work has advocated for incorporating human preferences, which may better reflect how LLM code could be useful to a programmer without necessarily being functionally correct [Dibia et al., 2023]. Preferences are generally collected after a single turn (e.g., after a single LLM response or suggestion) and thus can be collected at scale [Bird et al., 2022, Chiang et al., 2024] or even simulated with LLMs [Dubois et al., 2023, Zheng et al., 2023]. Given that preferences are only a form of intermediate feedback, in this study, we evaluate whether human preferences provide a signal for downstream productivity gains when coding with LLMs.</p>
<p>Programmer-LLM Interaction. Prior work conducting user studies where programmers code with LLM</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We introduce RealHumanEval, an online evaluation platform for LLM-assisted coding. The platform consists of (a) a customizable task description, (b) the code editor which shows autocomplete suggestions in grey, and (c) the chat assistant. Above the editor, users can check their task progress and the amount of time left, reset the editor, change the editor theme, and view study instructions. Below the editor, they can run and submit their code.</p>
<p>assistance has primarily focused on two forms of LLM support, autocomplete suggestions [Vaithilingam et al., 2022, Peng et al., 2023, Barke et al., 2023, Prather et al., 2023, Mozannar et al., 2024, Vasconcelos et al., 2023, Cui et al., 2024] and chat dialogue [Ross et al., 2023, Chopra et al., 2023, Kazemitabaar et al., 2023, Gu et al., 2024, Nam et al., 2024]. While these studies have made progress in understanding programmer-LLM interactions, all studies only consider one LLM—often Copilot or ChatGPT—and one form of LLM supporter either autocomplete or chat, making it difficult to compare outcomes and metrics across models and across forms of support. In Table 1, we compare the aspects of our study with prior works that have conducted user studies where programmers code with LLM support. To our knowledge, ours is the first study to consider models of varying performance capabilities and multiple forms of support. Additionally, we note that the majority of studies have similar participant profiles as ours (i.e., students with some programming experience and industry professions), though a few focus exclusively on novice programmers [Kazemitabaar et al., 2023, Prather et al., 2023]. Finally, multiple studies have limited scope in terms of the number and types of coding tasks that are considered (e.g., focusing on one minesweeper game [Prather et al., 2023] or simple plotting tasks [Ross et al., 2023]), which differ from the breadth of tasks that have been evaluated in benchmarks and are We contribute a web platform RealHumanEval to enable ease of human-centric evaluation of more models and forms of support. Beyond applications of coding assistance, our study contributes to the broader literature studying human interactions with LLMs [Lee et al., 2023, Collins et al., 2023, Lee et al., 2022, Dang et al., 2022, Jakesch et al., 2023, Köpf et al., 2023, Jo et al., 2023, Brynjolfsson et al., 2023].</p>
<h1>3 RealHumanEval</h1>
<p>We introduce RealHumanEval, a web-based platform to conduct human-centric evaluation of LLMs for programming through the workflow shown in Figure 1. We created RealHumanEval to facilitate large-scale studies of programmers coding with LLMs, eliminating the need for participants to perform any additional installation of a bespoke IDE or study-specific extension or to have access to special hardware to serve study-specific models.</p>
<p>Interface. As shown in Figure 2, RealHumanEval incorporates many basic features of common code editors and the functionality of programming interview sites such as LeetCode. Given a coding task that consists of a natural language description, partial code (e.g., a function signature), and unit tests that evaluate the task, RealHumanEval allows the programmer to write code with assistance from an LLM to complete the task. The platform has a panel that displays the natural language description of a task, as shown in Figure 2(a), alongside partial code to solve the task. Participants then write their code for the task in the code editor and can test their code with a button that checks the code against test cases and runs their code directly. The editor displays any errors, if available, and whether the code passes the unit test. Once the programmer completes the task, a new task can be loaded into the interface. For our user study, we only use a single code editor file, however, RealHumanEval can support multiple-file projects.</p>
<p>Forms of LLM Assistance. RealHumanEval supports two forms of LLM assistance: autocomplete-based and chat-based. Examples of autocomplete and chat assistants include GitHub's Copilot [Github, 2022], Replit's Ghostwriter [replit, 2023], Amazon CodeWhisperer [Amazon, 2022], and ChatGPT [OpenAI, 2022]. In autocomplete-based assistance, the programmer writes code in an editor, and the LLM displays a code suggestion inline, which is greyed out as shown in Figure 2(b). The LLM is assumed to be able to fill in code given a suffix and prefix. A suggestion, based on the current code body in the editor, appears whenever the programmer pauses typing for more than two seconds or when the programmer requests a suggestion by pressing a hotkey. The programmer can accept the suggestion by pressing the tab key or reject it by pressing escape or continuing to type.</p>
<p>In chat-based assistance, the programmer writes code in an editor and has access to a side chat window where the programmer can ask questions and get responses from the LLM, as illustrated in Figure 2(c). The LLM is assumed to be a chat model. The programmer can copy and paste code from the LLM's responses into the editor. Currently, the interface supports any LLM invoked via an online API. Further information on the implementation of both forms of assistance is in Appendix A and Appendix C.</p>
<p>Telemetry logging. RealHumanEval logs all user behavior, including interactions with LLM support. For each autocomplete suggestion, we log the following tuple $\left{\left(P_{i}, S_{i}\right), R_{i}, A_{i}\right}<em i="i">{i=1}^{n}$ where $\left(P</em>}, S_{i}\right)$ is the prefix and suffix of the code based on cursor position at the time of suggestion $i, R_{i}$ is the LLM suggestion, and $A_{i}$ is a binary variable indicating whether the suggestion was accepted. All the logs are stored in a dataset $\mathcal{D<em i="i">{\text {ac }}$. For chat-assistance, we log for each user message the following tuple $\left{X</em>\right}}, M_{i}, R_{i}, C_{i<em i="i">{i=1}^{n}$ where $X</em>$. Moreover, every 15 seconds, the interface saves the entire code the user has written.}$ is the code at the time of message $i, M_{i}$ is the user message (including prior chat history), $R_{i}$ is the response from the LLM for the message, and $C_{i}$ is the number of times code was copied from the LLM's response. All the logs are stored in a dataset $\mathcal{D}_{\text {chat }</p>
<p>Metrics. From the telemetry logs, RealHumanEval provides multiple metrics to analyze programmer behaviors: the number of tasks completed (completion is measured by whether the submitted code passes a set of private test cases), time to task success (measured in seconds), acceptance rate (fraction of suggestions</p>
<p>shown that are accepted, for autocomplete), and number of chat code copies (counting when user copies code from LLM response, for chat) among other metrics.</p>
<h1>4 Study Design</h1>
<p>Using RealHumanEval, we conducted a user study to evaluate (1) the impact of LLM assistance on programmer performance as a function of the LLM's performance on static benchmarks and (2) whether human preference metrics correlate with programmer productivity metrics.</p>
<p>Overview. For the entire duration of the study, participants are randomly assigned either to a control group, where they experienced the no LLM condition, or to the LLM-assisted group, where they experienced the autocomplete or chat support condition. For autocomplete-based support, the window in Figure 2(c) is hidden. For chat-based support, no autocomplete suggestions are shown in Figure 2(b). Participants are only assigned to one condition to minimize context switching, given the relatively short duration of the study. The study was conducted asynchronously using the RealHumanEval platform; participants were told not to use any outside resources (e.g., Google), and cannot paste any text originating outside the app into the editor. Specific instructions are in Appendix A. The first problem was a simple task (i.e., compute the sum and product of a list) for participants to familiarize themselves with the interface. Participants are given 35 minutes to complete as many tasks as possible. If 10 minutes pass and the participant has not completed the task, a button appears to provide the option to skip the task.</p>
<p>Tasks. We designed 17 coding tasks for the platform that can be categorized into three categories: (a) algorithmic problems from HumanEval (e.g., solve interview-style coding), (b) data manipulation problems (e.g., wrangle input dataframe into desired output), and (c) editing and augmenting code tasks (e.g., fill in provided code scaffold to achieve desired behavior). While the set of tasks does not evaluate all types of coding problems exhaustively, they do capture tasks of varying difficulty and solutions of varying length, as well as the use of different programming skills, leading to varying opportunities to benefit from LLM support. We chose 17 tasks to build diversity across tasks while being able to collect enough samples per task. We ensured that no LLM model considered in the study, in addition to GPT-4, could solve all tasks perfectly, so that programmers would not simply accept all LLM suggestions and that each task could be solved in under 20 minutes by an experienced programmer (validated through pilots with the authors and volunteer participants), to ensure that these were reasonable questions to consider for a user study. These 17 tasks are distributed into five sets, where each set consists of a different mix of task types in varying orders but shares the first two tasks. Each participant is randomly assigned to one of these sets. The LLMs are not aware of the task descriptions unless the programmer types them in the editor or chat window; this is to simulate the real world where the task description represents the programmer's hidden true intent. We provide examples of the coding tasks in Appendix B and in full in the supplementary materials.</p>
<p>Conditions. For the autocomplete conditions, we chose base LLM models that naturally generate next-word predictions, whereas the "chatty" variants of the base models are employed for the chat conditions. To evaluate the effect of LLM capabilities, we selected three types of models that demonstrate clear gaps in performance on existing benchmarks (as shown in Figure 11). In total, we selected 7 LLMs for our study: 4 from the Code Llama family [Rozière et al., 2023] (CodeLlama-7b, CodeLlama-7b-instruct, CodeLlama-34b, CodeLlama-34b-instruct), along with three models from the</p>
<p>GPT series [Brown et al., 2020] (GPT-3.5-turbo, GPT-3.5-turbo-instruct and GPT-4o). To avoid confusion, we refer to the autocomplete conditions by the base name of the model: CodeLlama-7b, CodeLlama-34b and GPT-3.5 (refers to GPT-3.5-turbo-instruct); and the chat conditions by the base name of the model with chat: CodeLlama-7b (chat) (refers to CodeLlama-7b- instruct), CodeLlama-34b (chat) (refers to CodeLlama-34b- instruct), GPT-3.5 (chat) (refers to GPT-3.5-turbo) and GPT-4o (chat). Specific choices of parameters, system prompts, and other considerations are provided in Appendix C.</p>
<p>Participants. We recruited 263 total participants from university mailing lists and social media to capture a range of coding experiences. We verified that participants were above 18 years of age, resided in the United States, and correctly completed a simple Python screening question. Out of the 263 participants, we filtered out those who did not complete any task or did not write code for a period of 15 minutes during the study to arrive at 243 final participants. Of the 243 participants, $35 \%$ identify as Female. In terms of occupation, $80 \%$ are Undergraduate or Graduate Students studying computer science, $11 \%$ work in Software Development and $7 \%$ work in AI. While a majority of our participants were students, only $35 \%$ of participants had less than 2 years of professional programming experience. We ensured that participants were roughly equally distributed across experimental conditions based on programming experience. $11 \%$ had never used any form of AI for coding while $66 \%$ of participants use AI at least once a week for coding. Participants were provided with a $\$ 15$ Amazon gift card as compensation. This study was approved by institutional IRB review.</p>
<p>User study metrics. To quantify the benefits of LLM assistance on the number of tasks completed and time to task success, we report the gap between each condition where some form of LLM assistance was provided and the control no LLM condition, which we denoted as $\Delta$. For example, for time to task success, $\Delta&lt;0$ for LLM support indicates that participants took less time to complete tasks with the LLM. In addition to the quantitative metrics, we also ask post-study questions to obtain participants' subjective measures of their interactions with the LLM: we ask participants to rate the helpfulness of the LLM on a scale of $[1,10]$ and to describe how the LLM support provided (if any) was helpful and how it could be improved. We also measure two preference metrics, suggestion acceptance rate and percentage of chat code copies.</p>
<h1>5 Results</h1>
<p>We report results for data collected from 243 participants split across the seven conditions; since condition assignment is random ${ }^{3}$, each condition has around 25 to 35 participants (except for No LLM, which has 39 participants). Participants completed a total of 888 coding tasks (mean of 3.6 tasks per person) on average in 358 seconds ( $\mathrm{std}=153$ seconds), were shown 5204 autocomplete suggestions ( $\left|\mathcal{D}<em _chat="{chat" _text="\text">{\text {ac }}\right|$ ), with an average $11.3 \%$ acceptance rate, and received 1055 messages from the chat LLMs ( $\left|\mathcal{D}</em>\right|$ ), with $35.8 \%$ of messages having at least one copy event. In the following analyses, we conduct ordinary least squares regressions with Benjamini-Hochberg correction and use a significance level of 0.05 . A more in-depth analysis of both datasets and results is in Appendix D.}</p>
<p>Providing LLM assistance reduces the amount of time spent coding. To measure the productivity gains of LLM assistance to programmers, we look at two metrics: the amount of time spent coding (in seconds) and the number of tasks completed. We first distill our observations for each metric by comparing performance</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Difference in task completion time (in seconds) comparing LLMs to the No LLM condition.
(b) Difference in number of tasks completed compared to the No LLM condition.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(c) Average task completion time (in seconds) by condition.
(d) Average number of tasks completed by condition.</p>
<p>Figure 3: We measure the effect of LLM support on user study performance on mean task duration in seconds (a,c) and number of tasks completed across model type (b,d). In (a) and (b), we compute $\Delta$, the difference between each model type-aggregating conditions corresponding to the same model type, e.g., Codellama7b and Codellama7b (chat) - and the No LLM condition for each metric. In (c) and (d), we break down the same metrics for each of the seven conditions and mark the percentage improvement over the No LLM condition. We observe that better LLM support can improve task completion time, but not necessarily increase the number of tasks completed. Error bars denote standard errors - the standard deviation divided by the square root of the sample size (i.e., across participants), where each participant contributes a single data point.
for each model type (i.e., combining autocomplete and chat models) against the No LLM condition. ${ }^{4}$ As shown in Figure 3(a), we find that compared to the No LLM setting where participants spent an average of 400 seconds per task, GPT-3.5, CodeLlama-34b, and GPT-4 models reduce the amount of time spent per task by an average of 78,64 , and 60 seconds respectively ( $p=0.04, p=0.12$, and $p=0.10$ ). In contrast, CodeLlama-7b models slightly increase the average time spent on a task by 10 seconds. However, we do not observe statistical differences across any of the conditions in the number of tasks completed, as shown in Figure 3(b), meaning no form of LLM support allowed programmers to solve more problems than they otherwise would have on their own. We hypothesize that benefits in task completion were not observed because of the short duration of the user study ( 35 minutes) and the amount of time it takes to complete each task, though we do observe an increase in the number of tasks attempted.</p>
<p>We now consider how our observations using RealHumanEval implicate the broader code LLM evaluation landscape, specifically the use of (1) static benchmarks and (2) human preference metrics.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Percentage of chat messages copied for chat conditions.
(b) Percentage of autocom- (c) Rating of LLM helpfulness across both plete suggestions accepted. autocomplete and chat conditions.</p>
<p>Figure 4: Measuring participant preferences of different models by the amount of interaction with chat (a) or autocomplete systems (b), with standard error. We find that preference judgments align with the reported helpfulness of the LLM assistant post-study (c); however, these preferences do not necessarily align with their actual task performance.
(1) Are LLM performance on static benchmarks informative of user productivity with LLM assistance? We find that improvements in model-specific evaluations on benchmarks tends to also improve human performance on both productivity metrics in the user study (i.e., CodeLlama-7b models led to the least number of tasks completed, while GPT-3.5 models led to the most). Interestingly, this trend holds even when considering metrics with chat and autocomplete separately, in Figure 3(c-d). However, significant gaps in benchmark performance result in relatively indistinguishable differences in terms of human performance. For instance, CodeLlama-34b (chat) is $19 \%$ better over CodeLlama-7b (chat) models on HumanEval, and participants are $22.8 \%$ ( $95 \%$ CI $[2.8,38.7]$ ) faster on average to complete a task with 34 b vs 7 b . Yet, GPT-3.5 (chat) model outperforms CodeLlama-34b (chat) by $85 \%$ on HumanEval, and yet participants equipped with GPT-3.5 (chat) models are only $8.3 \%$ ( $95 \%$ CI [-11.2, 24.6]) faster than those with CodeLlama-34b (chat). Surprisingly, we also find no statistically significant difference between GPT-40 (chat) and GPT-3.5 (chat) in terms of task completion time. While we do not necessarily expect performance gaps to be consistent, this finding suggests that, after a certain point, additional gains on static benchmarks may not translate to practical utility.
(2) Do human preferences align with productivity? We also consider programmer preferences for the LLM assistant's suggestions on autocomplete and chat: the average suggestion acceptance rate and the average copies-per-response respectively. While GPT-40, GPT-3.5, and CodeLlama-34b models reduced the amount of time spent coding over CodeLlama-7b, we do not find the same trends reflected in human preferences. As shown in Figure 4(a), we find that suggestions from CodeLlama-34b are less likely to be accepted at $5 \%$ compared to $15 \%$ and $9 \%$ for GPT-3.5 and CodeLlama-7b ( $p&lt;0.001$ and $p=0.19$ ). The same ordering occurs for the percentage of chat messages copied ( $27 \%$ versus $35 \%$ and $29 \%$, though not significant) in Figure 4(b). By analyzing the participants' qualitative responses, discussed in Section 6, we identify potential factors that may have contributed to these preferences, including a perceived lack of context in CodeLlama-34b suggestions and a slight increase in latency in CodeLlama-34b (chat) responses. These results suggest that various external factors that might be difficult to anticipate a priori can easily affect human preferences even if they do not impact downstream productivity.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Average task duration with and without LLM assistance with standard error by task category.</p>
<h1>5.1 Additional User Study Observations</h1>
<p>Findings on the effect of the form of LLM support and task type further illustrate the importance of evaluation with humans in the loop.</p>
<p>Chat support is perceived to be more helpful than autocomplete support. Even though autocomplete and chat variants obtained similar performance on static benchmarks and participant performance in both conditions conditioned on a model type was relatively similar, we observe that chat models are rated by participants in the post-study questions as significantly more helpful than autocomplete models $(p&lt;0.001)$, as shown in Figure 4(c). Again, we observe that CodeLlama-34b models tend to be rated as less helpful ( 3.3 out of 10), than the other two models ( 4.19 and 5.09 out of 10 for CodeLlama-7b and GPT-3.5). To no surprise, GPT-40 (chat) is rated as the most helpful with 6.9 out of 10 followed closely by GPT-3.5 (chat) with 6.3 out of 10 .</p>
<p>The benefits of LLM assistance can vary by task type. We also analyze the time spent on each task category, comparing when participants have access to LLM assistance versus the control condition. As shown in Figure 5, we find suggestive evidence that LLM assistance was particularly effective in reducing the time programmers needed to solve data manipulation tasks, by $26.3 \%$, and slightly less so for problems that required editing and augmenting existing code, by $17.1 \%$. In contrast, we found that LLMs were unhelpful on algorithmic problems, increasing the amount of time spent by $11.4 \%$. A breakdown by individual task is in Appendix D.</p>
<p>Alternative Measures of Autocomplete Suggestion Quality. The fraction of suggestions accepted by programmers in the autocomplete conditions is a myopic measure of the interaction between the programmer and the LLM. Programmers often accept suggestions to see them with code styling and then delete them promptly or verify them at a later time [Mozannar et al., 2024]. On the other hand, programmers may reject suggestions inadvertently. A measure that is less myopic than a fraction of accepted suggestions, is the persistence of an accepted suggestion in a programmer's code. In Figure 6, we track each accepted suggestion over time to see if it remains in the programmer's code. We find that suggestions from GPT-3.5 and CodeLlama34b persist more frequently compared to suggestions from CodeLlama7b confirming our productivity assessments.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: For each accepted suggestion across the three models for autocomplete conditions, we track after 15 seconds, 30 s , and 60 s whether the accepted suggestion was still found exactly in the user's code. We track the fraction of the accepted suggestions that persisted across the three time points.</p>
<h1>6 Design Opportunities</h1>
<p>To understand the design opportunities around improving the coding assistance provided through RealHumanEval, we analyzed a post-study question on how coding assistants could be improved. Answers to the question were collected in free response format and were optional, though it was answered by the majority of participants. We summarize participant suggestions into general comments that could apply to both types of interactions and identify autocomplete- and chat-specific suggestions. ${ }^{5}$</p>
<p>Both autocomplete and chat models need improved context. A theme that spanned both types of interactions and model types was the perceived lack of context that the LLM had about the general task when providing either suggestions or chat responses (example shown in Figure 7). While one might expect that a more performant model might mitigate these concerns, we do not observe a significant decrease in mentions regarding this issue for GPT-3.5 models compared to both CodeLlama-7b and CodeLlama-34b models. In general, it may not be obvious how to concisely specify the full "context"-recall that we intentionally considered a set-up where the LLM is unaware of task $T$ to mimic realistic constraints - but the development of new interfaces to facilitate context specification and mechanisms to prompt for additional task-specific information could improve LLM generations. Additionally, further baseline checks can be implemented to minimize concerns mentioned by participants (e.g., ensuring that the LLM responses are provided in the correct programming language, beyond prompting-based approaches implemented in our study). We note that issues surrounding context control have also been highlighted in prior work [Chopra et al., 2023, Barke et al., 2023].</p>
<p>Autocomplete-specific suggestions. We highlight the three most commonly mentioned avenues of improvement across all three model types. (1) Minimize suggestion frequency: Participants noted that the frequency of suggestions appearing in the code editor could disrupt their train of thought. To address this issue, it may be preferable to allow participants to turn off the LLM model when they are brainstorming the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Examples from of helpful and unhelpful chat and autocomplete interactions from the user study. While these examples showcase how LLM assistance can improve programmer productivity (e.g., by providing actionable responses and generating test cases), they also highlight how programmer-LLM interactions can be improved. We discuss design opportunities collected from post-task participant responses in Section 6 and provide more examples in Appendix E.
next steps or to modify the LLM to detect when participants may not need as frequent suggestions based on their current coding behavior. Moreover, we observe quantitatively that participants are between $3-10 \times$ more likely to accept an assistant's suggestion if they requested it, as shown in Figure 12. (2) Dynamic suggestion length: A common issue with autocomplete interactions noted by participants was the presence of "incomplete variable definitions or function implementations" and "fragmented code" (e.g., Figure 20 (left)). As this behavior is a product of the fixed length of LLM generations, autocomplete assistants can be improved by ensuring the suggestion is complete before terminating generation. (3) More concise suggestions: Finally, participants also noted that code completions could be more concise, as "it was overwhelming" and "large chunks of code... start deviating from the task question" (e.g., Figure 20 (right)). It is an open question to determine the appropriate length for how much code to generate in a context-aware manner.</p>
<p>Chat-specific suggestions. There were three common suggestions shared across models. (1) Responses should focus on code, rather than explanation: It is well known that chat LLMs tend to generate verbose responses, which could be detrimental when used as programming assistants. An example of a lengthy response is in Figure 22. In particular, participants noted the additional time required to read large blocks of texts and suggested to "get rid of all explanations and stick to code only, unless the user specifies they want explanations." Additionally, when focusing on code, participants suggested that the chat assistant could anticipate alternative implementations (2) Improved dialogue experience: First, instead of making assumptions</p>
<p>about potentially ambiguous points in a programmer's question (e.g., as in Figure 21), a participant suggested that the LLM "could ask clarifying questions or provide multiple suggestions." Additionally, in particular for CodeLlama-7b, participants asked for better consistency across multiple chat messages (e.g., "It wasn't able to refer back to previous messages that I had sent when answering a question."). (3) Better integration with code editor: Currently, the burden is on the programmer to appropriately prompt the chat assistant with questions and then to integrate chat suggestions into the code body in the editor. This onus can be reduced by more readily incorporating "the code and the most recent error, if any, as well as the test case that generated it in the context for the assistant" and "autocorrect code" based on its suggestions.</p>
<p>Why was CodeLlama-34b less preferred by users? Based on participants' survey responses, we identify two potential reasons that might qualitatively explain why CodeLlama-34b was less preferred for both autocomplete and chat. For autocomplete, the lack of context was a particularly prevalent issue in responses for CodeLlama-34b, mentioned by $54 \%$ of responses, as compared to $32 \%$ and $28 \%$ of CodeLlama-7b and GPT-3.5 responses respectively. In particular, participants noted that the generated suggestions were often irrelevant to the prior code and in the wrong programming language. We show examples of rejected suggestions that illustrate a lack of context from participants who interacted with the CodeLlama-34b model in Figure 23. For chat, while there were no exceptional concerns about lack of context, CodeLlama-34b had the most mentions of latency as a point of improvement ( 6 mentions as compared to only 2 and 1 mentions for CodeLlama-7b and GPT-3.5 respectively). For example, one participant noted that "the responses are slow so sometimes it was faster to go off of my memory even if I wasn't sure if it would work." Indeed, we found that CodeLlama-34b response time (about 10 seconds) was on average twice as slow as either CodeLlama-7b or GPT-3.5 (about 5 seconds). We note that this slight delay did not significantly impact any participant's performance metrics.</p>
<h1>6.1 Opportunities to use data</h1>
<p>Simulating programmer-LLM interaction. The data collected in our study presents an opportunity to build and evaluate simulation environments that mimic how programmers write code with an LLM. Essentially, the simulator could be used to more efficiently replicate the results of RealHumanEval and evaluate a wider set of models. However, despite initial work on simulating programmer-LLM interaction [Mozannar et al., 2023], building a useful simulator requires significant training and validation. Our dataset provides training data for both chat and autocomplete interactions: The dataset of interactions with the chat models $\mathcal{D}<em _ac="{ac" _text="\text">{\text {chat }}$ allows us to simulate the queries programmers make to the chat assistant given the code they have currently written. The dataset of interactions with the autocomplete models $\mathcal{D}</em>$ can allow us to simulate finer-grain interactions with LLM suggestions such as verifying and editing suggestions, among other activities outlined in [Mozannar et al., 2023]. To validate a proposed simulator, one should test whether it faithfully replicates the trends observed in RealHumanEval before it can be used as an evaluation benchmark.}</p>
<p>Optimizing suggestions from human feedback. In addition to using the human feedback data to simulate the interaction, one can use it to fine-tune the models. For instance, the dataset of interactions with autocomplete models $\mathcal{D}_{\text {ac }}$ reveals which suggestions programmers accept and which they reject, which can be used to update the LLM and generate suggestions that maximize the probability of being accepted by the programmer. Moreover, the dataset also captures how accepted suggestions were edited over time, which can be used to generate suggestions that are more likely to persist in the programmer's code. Finally, an</p>
<p>LLM that is not instruction-tuned usually requires specifying a maximum generation length parameter to stop the generation of a code suggestion. In our autocomplete implementation, we intentionally randomized the maximum suggestion length of the generated suggestion to be between the range $[10,120]$ with a mean token length of 64 . This design decision can provide yet another signal about when the LLM should stop generating code.</p>
<h1>7 Discussion</h1>
<p>In this work, we introduce RealHumanEval, a human-centric evaluation platform for code LLMs, and conduct a user study using the platform to measure programmer productivity assisted by different LLMs. We believe RealHumanEval can be adopted to evaluate newly released LLM models in a more meaningful way and become a standard for evaluation. To enable this, our interface is designed to be easily repurposed for future user studies and evaluations by the community and extended to evaluate new ways of interacting with LLMs for programming.</p>
<p>Limitations. Firstly, we acknowledge that a set of 17 coding tasks does not span the entire set of tasks a professional programmer might encounter in their work and may limit the generalizability of our evaluations of the 7 models. We encourage future work to leverage RealHumanEval to conduct further studies with a more extensive set of tasks and with more models. We included the seven models chosen to be representative of different scales of LLMs, notably we did not evaluate models from the Claude series Anthropic [2024] but we expect GPT-4o to have relatively similar performance. Our goal is not to benchmark all available LLMs, but to look at trends between human productivity and LLM performance. Furthermore, our work includes more studied models than prior work with human (Table 1). Second, the coding tasks we used are of short duration, while real-world programming tasks can take hours to months. This presents a trade-off in study design: short tasks allow us to evaluate with more participants and models in a shorter period but give us a less clear signal compared to longer-term tasks. Third, RealHumanEval does not fully replicate all functionality existing products such as GitHub Copilot may have so the study may underestimate exact productivity benefits. Such products are complex systems comprising more than a single LLM, where many details are hidden and thus not easily replicable. We release RealHumanEval to enable others to build more functionality in an open-source manner.</p>
<p>Societal implications. While our evaluations focused on productivity metrics, there are additional metrics of interest that may be important to measure when studying programmer interactions with LLM support. On the programmer side, further evaluations are needed to understand whether programmers appropriately rely on LLM support [Spiess et al., 2024] and whether LLM support leads to potential de-skilling [Bommasani et al., 2021]. Further, our metrics do not consider potential safety concerns, where LLMs may generate harmful or insecure code [Pearce et al., 2022, Perry et al., 2023].</p>
<h2>Acknowledgments</h2>
<p>We thank Wayne Chi, Katie Collins, Hunter Lang, Christina Ma, and Shannon Shen for providing feedback on the manuscript. HM is thankful for the support of the MIT-IBM Watson AI Lab. AT was supported in part by the National Science Foundation grants IIS1705121, IIS1838017, IIS2046613, IIS2112471, and funding from</p>
<p>Meta, Morgan Stanley, Amazon, and Google. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies.</p>
<h1>References</h1>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Amazon. Ml-powered coding companion - amazon codewhisperer, 2022. URL https://aws.amazon.com/ codewhisperer/.</p>
<p>Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic.com/news/ claude-3-family.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Shraddha Barke, Michael B James, and Nadia Polikarpova. Grounded copilot: How programmers interact with code-generating models. Proceedings of the ACM on Programming Languages, 7(OOPSLA1):85-111, 2023 .</p>
<p>Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and Idan Gazit. Taking flight with copilot: Early insights and opportunities of ai-powered pair-programming tools. Queue, 20(6):35-57, 2022.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Erik Brynjolfsson, Danielle Li, and Lindsey R Raymond. Generative ai at work. Technical report, National Bureau of Economic Research, 2023.</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. Multipl-e: a scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 2023 .</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.</p>
<p>Bhavya Chopra, Ananya Singha, Anna Fariha, Sumit Gulwani, Chris Parnin, Ashish Tiwari, and Austin Z Henley. Conversational challenges in ai-powered data science: Obstacles, needs, and design opportunities. arXiv preprint arXiv:2310.16164, 2023.</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694, 2023.</p>
<p>Kevin Zheyuan Cui, Mert Demirer, Sonia Jaffe, Leon Musolff, Sida Peng, and Tobias Salz. The productivity effects of generative ai: Evidence from a field experiment with github copilot. 2024.</p>
<p>Hai Dang, Karim Benharrak, Florian Lehmann, and Daniel Buschek. Beyond text generation: Supporting writers with continuous automatic text summaries. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 1-13, 2022.</p>
<p>Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh, Han Liu, and Saleema Amershi. Aligning offline metrics and human judgments of value for code generation models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8516-8528, 2023.</p>
<p>Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis. Large language models of code fail at completing code with potential bugs. Advances in Neural Information Processing Systems, 36, 2023.</p>
<p>Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2023.</p>
<p>Github. Github copilot - your ai pair programmer, 2022. URL https://github.com/features/copilot.
Ken Gu, Ruoxi Shang, Tim Althoff, Chenglong Wang, and Steven M Drucker. How do analysts understand and verify ai-assisted data analyses? In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 1-22, 2024.</p>
<p>Sandra G Hart. Nasa-task load index (nasa-tlx); 20 years later. In Proceedings of the human factors and ergonomics society annual meeting, volume 50, pages 904-908. Sage publications Sage CA: Los Angeles, CA, 2006 .</p>
<p>Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.</p>
<p>Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman. Co-writing with opinionated language models affects users' views. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-15, 2023.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Eunkyung Jo, Daniel A Epstein, Hyunhoon Jung, and Young-Ho Kim. Understanding the benefits and challenges of deploying conversational ai leveraging large language models for public health intervention. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-16, 2023.</p>
<p>Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J Ericson, David Weintrop, and Tovi Grossman. Studying the effect of ai code generators on supporting novice learners in introductory programming. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-23, 2023.</p>
<p>Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. arXiv preprint arXiv:2303.03004, 2023.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2023.</p>
<p>Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 18319-18345. PMLR, 2023.</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. In Findings of the Association for Computational Linguistics: ACL 2023, pages 431-469, 2023.</p>
<p>Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In CHI Conference on Human Factors in Computing Systems, pages $1-19,2022$.</p>
<p>Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines GerardUrsin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E Wang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani, Michael S. Bernstein, and Percy Liang. Evaluating human-language model interaction. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https: //openreview.net/forum?id=hjDYJUn911.</p>
<p>Jenny T Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, and Thomas Zimmermann. Can gpt-4 replicate empirical software engineering research? arXiv preprint arXiv:2310.01727, 2023 .</p>
<p>Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2023.</p>
<p>Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU.</p>
<p>CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=61E4dQXaUcb.</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384-1403, 2022.</p>
<p>Hussein Mozannar, Valerie Chen, Dennis Wei, Prasanna Sattigeri, Manish Nagireddy, Subhro Das, Ameet Talwalkar, and David Sontag. Simulating iterative human-ai interaction in programming with llms. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.</p>
<p>Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. Reading between the lines: Modeling user behavior and costs in ai-assisted programming. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 1-16, 2024.</p>
<p>Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 1-13, 2024.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=iaYcJKpY2B_.</p>
<p>OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL https://openai.com/blog/ chatgpt/.</p>
<p>Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. Asleep at the keyboard? assessing the security of github copilot's code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pages 754-768. IEEE, 2022.</p>
<p>Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer productivity: Evidence from github copilot. arXiv preprint arXiv:2302.06590, 2023.</p>
<p>Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. Do users write more insecure code with ai assistants? In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pages 2785-2799, 2023.</p>
<p>James Prather, Brent N. Reeves, Paul Denny, Brett A. Becker, Juho Leinonen, Andrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, and Eddie Antonio Santos. "it's weird that it knows what i want": Usability and interactions with copilot for novice programmers. ACM Trans. Comput.-Hum. Interact., 31(1), nov 2023. ISSN 1073-0516. doi: 10.1145/3617367. URL https://doi.org/10.1145/3617367.
replit. Meet ghostwriter, your partner in code., 2023. URL https://replit.com/site/ghostwriter.</p>
<p>Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. The programmer's assistant: Conversational interaction with a large language model for software development. In Proceedings of the 28th International Conference on Intelligent User Interfaces, pages 491-514, 2023.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. An analysis of the automatic bug fixing performance of chatgpt. arXiv preprint arXiv:2301.08653, 2023.</p>
<p>Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Susmit Jha, Prem Devanbu, and Toufique Ahmed. Quality and trust in llm-generated code. arXiv preprint arXiv:2402.02047, 2024 .</p>
<p>Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1-7, 2022.</p>
<p>Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q Vera Liao, and Jennifer Wortman Vaughan. Generation probabilities are not enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code completions. arXiv preprint arXiv:2302.07248, 2023.</p>
<p>Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, et al. Recode: Robustness evaluation of code generation models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13818-13843, 2023.</p>
<p>Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Shuiguang Deng, et al. Codescope: An execution-based multilingual multitask multidimensional benchmark for evaluating llms on code understanding and generation. arXiv preprint arXiv:2311.08588, 2023 .</p>
<p>John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. Large language models meet nl2code: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7443-7464, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-ajudge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao.</p>
<p>Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni, and Chandan K. Reddy. Xlcost: A benchmark dataset for cross-lingual code intelligence, 2022. URL https://arxiv.org/abs/2206.08474.</p>
<p>Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. Productivity assessment of neural code completion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 21-29, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We omit the obvious, blanket suggestion for replacing the assistant with a better LLM, as model-only performance is one of the independent variables in our experiment and a more performant model would undoubtedly improve the assistance provided.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
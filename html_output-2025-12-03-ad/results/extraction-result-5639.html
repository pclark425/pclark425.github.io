<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5639 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5639</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5639</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-6613062</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1410.8149v1.pdf" target="_blank">Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling</a></p>
                <p><strong>Paper Abstract:</strong> Dictionaries are often developed using tools that save to Extensible Markup Language (XML)-based standards. These standards often allow high-level repeating elements to represent lexical entries, and utilize descendants of these repeating elements to represent the structure within each lexical entry, in the form of an XML tree. In many cases, dictionaries are published that have errors and inconsistencies that are expensive to find manually. This paper discusses a method for dictionary writers to quickly audit structural regularity across entries in a dictionary by using statistical language modeling. The approach learns the patterns of XML nodes that could occur within an XML tree, and then calculates the probability of each XML tree in the dictionary against these patterns to look for entries that diverge from the norm.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5639.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5639.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-for-XML-structure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language modeling for detecting structural irregularity in electronic dictionaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's primary method linearizes XML lexical-entry trees into sequences of opening tag tokens ("tag sentences"), trains n-gram language models on those sequences, and uses log-probability and perplexity scores to rank entries by anomalousness for human review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n-gram language models (2-, 3-, 4-gram)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Count-based n-gram statistical language models (2-gram, 3-gram, 4-gram) trained on flattened XML tag sequences (tag sentences). Models were built/evaluated with the SRILM toolkit using standard n-gram statistics and backoff (Good-Turing/Katz backoff); some experiments run without smoothing for direct comparison across n values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Score-based ranking: compute log probability (LOGPROB) and average perplexity per word (PPW) and PPW with end tags (PPWET) for each flattened XML tree, then sort entries (ascending LOGPROB or descending PPW/PPWET) and present top-R entries as candidate anomalies (precision@R reported).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured hierarchical XML trees flattened into token sequences of opening tags ("tag sentences") — i.e., structured/sequence data derived from lexicon XML.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Structural irregularities in dictionary XML trees (nodes in unusual positions, incorrect nesting/parent-child relationships, missing subcomponents such as translations, typographical/OCR-induced structural errors) — semantic/structural anomalies rather than XML syntax/schema violations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Bilingual Urdu–English dictionary (Qureshi, 2003) — 44,237 lexical entries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision@R (R ∈ {15,30,50,100,500,1000}) using ranked LOGPROB, PPW, PPWET. Best overall: 2-gram model ranked by ascending LOGPROB — averaged over the six R values: ENTRY 92% precision, SENSE 95%, FORM 96%. Evaluating top 50 anomalies: ENTRY 96%, SENSE 98%, FORM 100%. Detailed per-tier/per-R tables reported for 2-,3-,4-gram models and different scoring metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared different n-gram orders and scoring metrics (2-,3-,4-gram; LOGPROB, PPW, PPWET); 2-gram LOGPROB performed best. The paper contrasts LM-based detection with traditional XML validation tools qualitatively: schema validators detect well-formedness/validity but not uncommon/illogical but valid structures; no numerical comparison to other anomaly-detection algorithms (e.g., clustering, statistical outlier detectors) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training and testing performed on the same dataset (no held-out cleaner corpus); gold labels limited to edits captured by Dictionary Manipulation Language (DML) so some true errors may be unlabeled (affects recall and evaluation). Flattening loses hierarchical information (authors note structure-aware approaches may improve accuracy). Node-level localization not implemented (per-word perplexity is available but not fully evaluated against exact corrected nodes). Method requires human review; iterative re-training and bootstrapping of cleaner models proposed but not evaluated.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5639.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5639.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XiaWong2006</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anomaly Detecting within Dynamic Chinese Chat Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that applied trigram language models to detect anomalous lexical items in chat transcripts by comparing trigram entropy to typical newspaper-derived entropy, marking high-entropy trigrams as anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Anomaly Detecting within Dynamic Chinese Chat Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>trigram language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Trigram statistical language models: trained on standard Chinese newspaper corpora to derive typical trigram entropy statistics, and trained on a hybrid corpus including chat transcripts to evaluate anomalies via elevated entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Entropy-based scoring: compute trigram entropy for words and POS; mark trigrams with higher entropy than newspaper-derived averages as anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Linear natural-language chat transcripts (token sequences) — both word tokens and POS-tag sequences were considered.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalies in language use (chat-speak) such as nonstandard words, dialectal forms, misspellings, emoticons, or dynamic lexicon items; both lexical anomalies and POS-based anomalies were tested.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Standard Chinese newspaper corpora and Chinese Internet chat transcripts (specific corpora not named in this paper's mention)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F-score reported: best conditions—words F=0.85, POS tags F=0.70.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No direct comparison to classical structural-anomaly detection methods; authors note words were better indicators than POS tags. The work motivated the paper's POS/structure analogy but did not qualify the types of anomalies flagged.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors did not characterize the flagged anomalies (e.g., dialect vs. misspelling). Chat lexicon is dynamic, so supervised models can obsolesce; POS condition less discriminative than words for this task.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5639.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5639.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jabbari2010</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Statistical Model of Lexical Context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PhD thesis work that examined detection of anomalous words using language models and bag-of-words approaches; the LM flagged words as anomalies when their probability in context was below an expected threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Statistical Model of Lexical Context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>language modeling (contextual probability) approach</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probabilistic model that computes probability of a word given its context and compares that to an expected probability of not having the word; details in thesis (bag-of-words and combined models also evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Scoring-based: mark a word as anomalous if P(word | context) < threshold (interpreted as less likely than expected absence). Evaluated individually and combined with bag-of-words features.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Linear word sequences (natural language text); applied to word-level anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous words in context (applications: real-word spelling error detection, word-sense disambiguation).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall F-score of 0.71 for the language-model-only approach; the bag-of-words model and a combined model outperformed the LM-alone approach.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Language model underperformed relative to the bag-of-words baseline and the combined model in the author's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LM-alone performed worse than alternative/contextual models; POS tags were not examined in this thesis (less directly comparable to structure-of-tags scenarios).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5639.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5639.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SRILM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SRI Language Modeling Toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used toolkit for building and evaluating n-gram language models that provides tools for computing n-gram counts, smoothing (Good-Turing, Katz backoff), and scoring (perplexity/log-probability).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SRILM -An Extensible Language Modeling Toolkit.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SRILM n-gram models (toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Software toolkit (command-line and C++ libraries) that computes n-gram statistics and builds ARPA-format models; supports Good-Turing and Katz backoff smoothing by default and provides perplexity/log-prob scoring utilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Provides n-gram model training and scoring utilities used to compute log-probabilities and perplexities for flattened XML tag sequences; anomaly detection is performed by ranking these scores.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Token sequence corpora (here: flattened XML tag sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Not an anomaly detector itself, but used to produce scores that reveal structural anomalies (as per the primary method).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SRILM is focused on n-gram models and does not natively model hierarchical structure; model quality depends on smoothing choices and data partitioning; toolkit use in this paper relied on same-corpus training/testing decisions which affect applicability of smoothing.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Anomaly Detecting within Dynamic Chinese Chat Text <em>(Rating: 2)</em></li>
                <li>A Statistical Model of Lexical Context <em>(Rating: 2)</em></li>
                <li>SRILM -An Extensible Language Modeling Toolkit. <em>(Rating: 1)</em></li>
                <li>Fixing Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5639",
    "paper_id": "paper-6613062",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "LM-for-XML-structure",
            "name_full": "Language modeling for detecting structural irregularity in electronic dictionaries",
            "brief_description": "This paper's primary method linearizes XML lexical-entry trees into sequences of opening tag tokens (\"tag sentences\"), trains n-gram language models on those sequences, and uses log-probability and perplexity scores to rank entries by anomalousness for human review.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "n-gram language models (2-, 3-, 4-gram)",
            "model_description": "Count-based n-gram statistical language models (2-gram, 3-gram, 4-gram) trained on flattened XML tag sequences (tag sentences). Models were built/evaluated with the SRILM toolkit using standard n-gram statistics and backoff (Good-Turing/Katz backoff); some experiments run without smoothing for direct comparison across n values.",
            "model_size": null,
            "anomaly_detection_method": "Score-based ranking: compute log probability (LOGPROB) and average perplexity per word (PPW) and PPW with end tags (PPWET) for each flattened XML tree, then sort entries (ascending LOGPROB or descending PPW/PPWET) and present top-R entries as candidate anomalies (precision@R reported).",
            "data_type": "Structured hierarchical XML trees flattened into token sequences of opening tags (\"tag sentences\") — i.e., structured/sequence data derived from lexicon XML.",
            "anomaly_type": "Structural irregularities in dictionary XML trees (nodes in unusual positions, incorrect nesting/parent-child relationships, missing subcomponents such as translations, typographical/OCR-induced structural errors) — semantic/structural anomalies rather than XML syntax/schema violations.",
            "dataset_name": "Bilingual Urdu–English dictionary (Qureshi, 2003) — 44,237 lexical entries",
            "performance_metrics": "Precision@R (R ∈ {15,30,50,100,500,1000}) using ranked LOGPROB, PPW, PPWET. Best overall: 2-gram model ranked by ascending LOGPROB — averaged over the six R values: ENTRY 92% precision, SENSE 95%, FORM 96%. Evaluating top 50 anomalies: ENTRY 96%, SENSE 98%, FORM 100%. Detailed per-tier/per-R tables reported for 2-,3-,4-gram models and different scoring metrics.",
            "baseline_comparison": "Compared different n-gram orders and scoring metrics (2-,3-,4-gram; LOGPROB, PPW, PPWET); 2-gram LOGPROB performed best. The paper contrasts LM-based detection with traditional XML validation tools qualitatively: schema validators detect well-formedness/validity but not uncommon/illogical but valid structures; no numerical comparison to other anomaly-detection algorithms (e.g., clustering, statistical outlier detectors) is provided.",
            "limitations_or_failure_cases": "Training and testing performed on the same dataset (no held-out cleaner corpus); gold labels limited to edits captured by Dictionary Manipulation Language (DML) so some true errors may be unlabeled (affects recall and evaluation). Flattening loses hierarchical information (authors note structure-aware approaches may improve accuracy). Node-level localization not implemented (per-word perplexity is available but not fully evaluated against exact corrected nodes). Method requires human review; iterative re-training and bootstrapping of cleaner models proposed but not evaluated.",
            "uuid": "e5639.0"
        },
        {
            "name_short": "XiaWong2006",
            "name_full": "Anomaly Detecting within Dynamic Chinese Chat Text",
            "brief_description": "Related work that applied trigram language models to detect anomalous lexical items in chat transcripts by comparing trigram entropy to typical newspaper-derived entropy, marking high-entropy trigrams as anomalies.",
            "citation_title": "Anomaly Detecting within Dynamic Chinese Chat Text",
            "mention_or_use": "mention",
            "model_name": "trigram language model",
            "model_description": "Trigram statistical language models: trained on standard Chinese newspaper corpora to derive typical trigram entropy statistics, and trained on a hybrid corpus including chat transcripts to evaluate anomalies via elevated entropy.",
            "model_size": null,
            "anomaly_detection_method": "Entropy-based scoring: compute trigram entropy for words and POS; mark trigrams with higher entropy than newspaper-derived averages as anomalous.",
            "data_type": "Linear natural-language chat transcripts (token sequences) — both word tokens and POS-tag sequences were considered.",
            "anomaly_type": "Anomalies in language use (chat-speak) such as nonstandard words, dialectal forms, misspellings, emoticons, or dynamic lexicon items; both lexical anomalies and POS-based anomalies were tested.",
            "dataset_name": "Standard Chinese newspaper corpora and Chinese Internet chat transcripts (specific corpora not named in this paper's mention)",
            "performance_metrics": "F-score reported: best conditions—words F=0.85, POS tags F=0.70.",
            "baseline_comparison": "No direct comparison to classical structural-anomaly detection methods; authors note words were better indicators than POS tags. The work motivated the paper's POS/structure analogy but did not qualify the types of anomalies flagged.",
            "limitations_or_failure_cases": "Authors did not characterize the flagged anomalies (e.g., dialect vs. misspelling). Chat lexicon is dynamic, so supervised models can obsolesce; POS condition less discriminative than words for this task.",
            "uuid": "e5639.1"
        },
        {
            "name_short": "Jabbari2010",
            "name_full": "A Statistical Model of Lexical Context",
            "brief_description": "PhD thesis work that examined detection of anomalous words using language models and bag-of-words approaches; the LM flagged words as anomalies when their probability in context was below an expected threshold.",
            "citation_title": "A Statistical Model of Lexical Context",
            "mention_or_use": "mention",
            "model_name": "language modeling (contextual probability) approach",
            "model_description": "Probabilistic model that computes probability of a word given its context and compares that to an expected probability of not having the word; details in thesis (bag-of-words and combined models also evaluated).",
            "model_size": null,
            "anomaly_detection_method": "Scoring-based: mark a word as anomalous if P(word | context) &lt; threshold (interpreted as less likely than expected absence). Evaluated individually and combined with bag-of-words features.",
            "data_type": "Linear word sequences (natural language text); applied to word-level anomaly detection.",
            "anomaly_type": "Anomalous words in context (applications: real-word spelling error detection, word-sense disambiguation).",
            "dataset_name": null,
            "performance_metrics": "Reported overall F-score of 0.71 for the language-model-only approach; the bag-of-words model and a combined model outperformed the LM-alone approach.",
            "baseline_comparison": "Language model underperformed relative to the bag-of-words baseline and the combined model in the author's experiments.",
            "limitations_or_failure_cases": "LM-alone performed worse than alternative/contextual models; POS tags were not examined in this thesis (less directly comparable to structure-of-tags scenarios).",
            "uuid": "e5639.2"
        },
        {
            "name_short": "SRILM",
            "name_full": "SRI Language Modeling Toolkit",
            "brief_description": "A widely used toolkit for building and evaluating n-gram language models that provides tools for computing n-gram counts, smoothing (Good-Turing, Katz backoff), and scoring (perplexity/log-probability).",
            "citation_title": "SRILM -An Extensible Language Modeling Toolkit.",
            "mention_or_use": "use",
            "model_name": "SRILM n-gram models (toolkit)",
            "model_description": "Software toolkit (command-line and C++ libraries) that computes n-gram statistics and builds ARPA-format models; supports Good-Turing and Katz backoff smoothing by default and provides perplexity/log-prob scoring utilities.",
            "model_size": null,
            "anomaly_detection_method": "Provides n-gram model training and scoring utilities used to compute log-probabilities and perplexities for flattened XML tag sequences; anomaly detection is performed by ranking these scores.",
            "data_type": "Token sequence corpora (here: flattened XML tag sentences).",
            "anomaly_type": "Not an anomaly detector itself, but used to produce scores that reveal structural anomalies (as per the primary method).",
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "SRILM is focused on n-gram models and does not natively model hierarchical structure; model quality depends on smoothing choices and data partitioning; toolkit use in this paper relied on same-corpus training/testing decisions which affect applicability of smoothing.",
            "uuid": "e5639.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Anomaly Detecting within Dynamic Chinese Chat Text",
            "rating": 2,
            "sanitized_title": "anomaly_detecting_within_dynamic_chinese_chat_text"
        },
        {
            "paper_title": "A Statistical Model of Lexical Context",
            "rating": 2,
            "sanitized_title": "a_statistical_model_of_lexical_context"
        },
        {
            "paper_title": "SRILM -An Extensible Language Modeling Toolkit.",
            "rating": 1,
            "sanitized_title": "srilm_an_extensible_language_modeling_toolkit"
        },
        {
            "paper_title": "Fixing Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language",
            "rating": 1,
            "sanitized_title": "fixing_errors_in_digital_lexicographic_resources_using_a_dictionary_manipulation_language"
        }
    ],
    "cost": 0.0102545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling</p>
<p>Paul Rodrigues 
University of Maryland College Park
MD</p>
<p>David Zajic dmzajic@umd.edu 
University of Maryland College Park
MD</p>
<p>David Doermann doermann@umd.edu 
University of Maryland College Park
MD</p>
<p>Michael Bloodgood 
University of Maryland College Park
MD</p>
<p>Peng Ye pengyu@umd.edu 
University of Maryland College Park
MD</p>
<p>Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling
anomaly detectionerror correctiondictionaries
Dictionaries are often developed using tools that save to Extensible Markup Language (XML)-based standards. These standards often allow high-level repeating elements to represent lexical entries, and utilize descendants of these repeating elements to represent the structure within each lexical entry, in the form of an XML tree. In many cases, dictionaries are published that have errors and inconsistencies that are expensive to find manually. This paper discusses a method for dictionary writers to quickly audit structural regularity across entries in a dictionary by using statistical language modeling. The approach learns the patterns of XML nodes that could occur within an XML tree, and then calculates the probability of each XML tree in the dictionary against these patterns to look for entries that diverge from the norm.</p>
<p>Introduction</p>
<p>Many dictionaries today are developed using tools that save to Extensible Markup Language (XML)-based standards, such as the Lexical Markup Framework (LMF) (Francopoulo et al., 2007), the Lexicon Interchange FormaT (LIFT) (Hosken, 2009), or the Text Encoding Initiative (TEI) (Burnard &amp; Bauman, 2007). Often, these standards allow high-level repeating elements to represent lexical entries and utilize descendants of these repeating elements to represent the structure of each lexical entry, in the form of an XML tree.</p>
<p>This paper presents a method to audit the structural regularity across all the entries in a dictionary, automatically. This approach uses statistical language modeling (LM), a technique commonly used in natural language processing, to learn the linear combinations of XML nodes that could occur within a lexical entry, and then evaluates each of these lexical entries against the learned patterns, looking for entries that diverge from the norm.</p>
<p>Technical users of XML often utilize tools to check the well-formedness of an XML document, or to determine the validity of a document as applied to a particular data schema. These help catch certain types of errors, such as syntax, or data relationship errors.</p>
<p>With many dictionary schemas, however, the structure within entries can vary from entry to entry. This structural permissiveness can allow a dictionary writer to introduce or underspecify ambiguous relationships, or to accidently place a node underneath an incorrect parent node in the entry's XML tree. These kinds of errors may be valid XML and may conform to the data specification, so they will not be caught by traditional XML tools, but they are semantically incorrect.</p>
<p>The LM technique described here linearizes the lexicon structure, ignoring the underlying text, converting the opening tags in XML into tokens, and then considering the string of tokens representing a lexical entry to be a sentence. A probabilistic language model is learned from these example sentences, and then that model is evaluated against each lexical entry in the corpus. Nodes that are in unusual positions produce a high perplexity, identifying possible anomaly points.</p>
<p>XML</p>
<p>Extensible Markup Language (XML), a text format used to store hierarchical data electronically, is often described by a data modeling definition such as a Document Type Definition (DTD), XML Schema (Gao, 2011), or RELAX NG (ISO, 2008. These data modeling definition documents use a regular language to define the data permissible in the XML document. Tools are available to validate, or ensure strict compliance of an XML document, to the data modeling definition. These tools result in a Boolean decision as to whether the data conforms to the specification, and are unable to alert the user to structurally valid, but illogical or rare structures that one may wish to investigate.</p>
<p>Structural Errors in Dictionaries</p>
<p>Dictionary Creation</p>
<p>Dictionaries are often the product of long-term research projects, or large-scale projects created quickly with multiple collaborators. Without strict conformance to a recording standard, entries can drift in style across time or between collaborators. Additionally, dictionaries can be large and complex, leaving them expensive to edit. Whether due to cost or deadlines, dictionaries are published that have errors and inconsistencies. </p>
<p>Dictionary Digitization</p>
<p>The process of digitizing dictionaries from a printed book by optical character recognition (OCR), or manually keying in content, can cause additional structural errors to be introduced. Typically in print dictionaries, typefaces, text size, text position, and unreserved symbols are used in combination to indicate the structure of a lexical entry and the scope of linguistic operators (such as English words and or or). Typographical errors that occur in the original print dictionary, misinterpretation by the OCR system, operator ambiguity, or typist error during the digitization stage can alter the intended structure of the dictionary. These errors can result in incorrect marking of subcomponents within a lexical entry or incorrectly understanding scope within the language examples. In bilingual dictionaries, translations may be forgotten (Figure 1), and languages may be mixed with no delineation (Figure 2). </p>
<p>Anomaly Detection Using Language Modeling</p>
<p>While language modeling is not a common approach for structural anomaly detection, it has been employed to detect anomalies in language use. Language models in natural language processing are commonly used to model linear combinations of word tokens or part-of-speech types. Lexicon XML structure is similar to the latter, in that the node names and attributes within the XML are chosen from a small closed class. Xia &amp; Wong (2006) used language modeling to tag lexemes in Chinese-language Internet chat transcripts as either standard Chinese, or anomalous. The authors noted that chat speak has a dynamic lexicon, and training corpora for supervised systems in this domain can obsolesce.</p>
<p>The authors trained trigram language models on standard Chinese newspaper corpora in order to induce typical values for trigram entropy on words and parts of speech in the standard language. They then learned a language model on a hybrid corpus consisting of newspaper corpora, and a chat transcript corpus. With the typical entropy values known from the newspaper corpus, they evaluated a language sample with the hybrid model. When a trigram had higher entropy than the standard average, they marked that trigram as an anomaly. The authors found words to be a better indicator of anomaly than POS tags, reaching an F-score of 0.85 for words and 0.70 for POS tags in their best conditions.</p>
<p>The authors do not qualify the data that is flagged anomalous. It would be interesting to know if this data is dialectal Chinese, misspelled words, bad grammar, emoticons, or lexemes unique to Chinese Internet chat. The POS tag condition is more comparable to our scenario, as both their POS categories and our structure description language have a small vocabulary.</p>
<p>Jabbari (2010) was interested in detection of anomalous words in the context of the words around them, which has practical applications including real-word spelling error detection and word sense disambiguation. The author examined an approach using bags of words, one using language models, and then a combination of the two. The language modeling approach marked a word as an anomaly if the probability of the word in that context was less than the expected probability of not having that word in the context. The language modeling system received an overall F-score of 0.71. It performed less well than the author's bag of words model, and the combined model. The author did not look at parts of speech, which is more relevant to our task.</p>
<p>LM Anomaly Detection for Flattened Structures</p>
<p>In the previous section, we showed how language modeling has been used to detect anomalies in a linear string of tokens. This section explains how to convert the XML tree into a linear string of tokens, and how this is used to build a language model.</p>
<p>The input to the language model is determined by specifying a repeating node in the XML file that contains child trees to be examined. Each of these repeating tree structures is traversed depth-first, and the element names and attributes of children are recorded to a buffer as a tag that identifies that element. In XML, a depth-first search is a linear scan of each node within a tree. At the end of each repeating node the buffer contains a single layer of whitespace-separated tags corresponding to a flattened representation of the tree. We call this a tag sentence. The tag sentences for all the repeating nodes form a corpus of tag sentences.</p>
<p>This corpus can then be used to train a statistical language model. For our experiments, we used the SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002). SRILM includes command line programs and C++ libraries to calculate n-gram statistics for a language, and to measure the perplexity of a text sample to those statistics. SRILM reads and writes to a standard ARPA (Advanced Research Projects Agency) file format for n-gram models. There are other language modeling toolkits available. Typically, the differences between applications are in the speed of the evaluation, the size of the model created, or the statistical smoothing algorithms included for estimating low-occurrence combinations. In our case, speed and model size is not much of an issue, but estimating combinations of low token occurrence is. Since we are training and testing on the same dataset, advanced smoothing algorithms would not add any benefit. SRILM runs with Good-Turing and Katz back-off by default.</p>
<p>Evaluation</p>
<p>Dictionary and Evaluation Data</p>
<p>We perform our evaluations on a bilingual Urdu-English dictionary of 44,237 lexical entries (Qureshi, 2003). This dictionary has been edited by a team of linguists and computer scientists to remove errors using a change-tracking system we refer to as Dictionary Manipulation Language (DML) (Zajic et al., 2011). DML provides a number of benefits for dictionary editing, but the core advantage for this application is that DML can be used to mark every error discovered in the original source dictionary. From the change log, we can create a list of trees we know to be errorful that can be used for evaluating our automatic systems.</p>
<p>Tree Tiers</p>
<p>The entries in Qureshi (2003) can contain multiple senses, each of which can contain multiple word forms. An entry can also contain word forms directly. These are high-level structures within each entry that can vary significantly. In order to isolate where the errors occur within the entry, we partition some of the structure, performing evaluations on ENTRY, SENSE, and FORM nodes separately. For the ENTRY evaluation, the highest-level SENSE and FORM branches were collapsed into single nodes, with their descendants pruned. For the SENSE trees, descendent SENSE and FORM branches were collapsed. No branches were collapsed in the trees for FORM evaluation. We call the ENTRY, FORM, and SENSE trees tiers.</p>
<p>Our three tiers are listed in Table 1, showing the number of occurrences in the dictionary, as well as the number of nodes of that tree tier that had a hand-made correction within the tree. </p>
<p>n-gram Models</p>
<p>For each of these tiers, content and closing tags were removed, and the trees flattened to form a tag sentence. These three corpora were used to train 2-, 3-and 4-gram language models, without smoothing.  This language model serves to provide prototype trees for comparison, storing which tags can co-occur with which others, and what the likelihood of that co-occurrence will be. Table 2 lists the three tiers, the count of unique tokens (XML descendants) under that tier, and the number of unique n-grams created by the linear combination of those tokens.</p>
<p>Applying the models</p>
<p>Each tag sentence from the dictionary is then evaluated with this language model, producing statistical measurements for each flattened tree structure: log probability of the sentence (LOGPROB), average perplexity per word (PPW), and average perplexity per word with end tags (PPWET). LOGPROB and PPWET both evaluate trees against n-grams that contain START OF SENTENCE and END OF SENTENCE tags. This helps model differences between tokens that appear initial or final in the tag sentence.</p>
<p>We rank these measurements to force the trees into a decreasing order of anomalousness. For LOGPROB, the trees are sorted in ascending order, and for both PPW and PPWET, the trees are sorted in descending order.</p>
<p>For evaluation, we provide precision at the top R anomalies, where R can be {15, 30, 50, 100, 500, or 1000}. A hit occurs where a tree in the top R of our list has shown up in our errorful tree list. Precision at Rank is defined as the number of hits divided by R.</p>
<p>4-gram Results</p>
<p>Out of the three n-gram lengths evaluated, 4-grams performed the worst overall. The average of the six precisions at rank scores for each tree tier and each language model measurement were lower than those for both 3-and 2-grams. Several trials in the group did reach the best scores for their Tier-R combination, but these are matched in the 2-and 3-gram models. Results can be seen in Tables 3, 4, and 5.   </p>
<p>3-gram Results</p>
<p>The 3-gram language model performed well, capturing the best average Tier / R trials for FORM with the PPW measurement. The results can be found in Tables 6, 7, and 8.   </p>
<p>2-gram Results</p>
<p>2-gram language models results can be found in Tables 9, 10, and 11. This length n-gram performed the best, with the best average Tier / R trial for ENTRY and for SENSE using the LOGPROB measurement. This measurement, shown in Table 11, has the largest number of Tier/R trials with the highest precision.   </p>
<p>Other-grams</p>
<p>Unigram, 5-gram, and 6-gram models were also evaluated according to their LOGPROB. 5-and 6-gram models performed at a lower accuracy for nearly all R and tree levels. Unigram evaluations were inconclusive. Accuracy was slightly higher for some R, but some were far lower.</p>
<p>Conclusions</p>
<p>We presented a statistical error detection technique for dictionary structure that uses language modeling to rank anomalous dictionary trees for human review. To create the language model, we split the dictionary into three tiers-ENTRY, FORM, and SENSE, and flatten each to form a tag sentence. We create 2-, 3-, and 4-gram language models based on this flattened structure, and evaluate against the original dictionary using Perplexity Per Word (PPW), Perplexity Per Word with End Tags (PPWET), and log probability (LOGPROB). These measurements were ranked, and we presented Precision-at-Rank for all trials.</p>
<p>We found the highest precision Tier/R trials to be spread across several n-gram length language models, and several language model measurements. In general, we find that the best overall configuration is a 2-gram language model, which ranks the trees by ascending log probability. Averaging our six precisions for this metric, the system reached 92% precision on ENTRY error detection, 95% on SENSE, and 96% on FORM.</p>
<p>Evaluating the top 50 anomalies, we reached 96% precision on ENTRY, 98% on SENSE, and 100% on FORM.</p>
<p>Comments</p>
<p>Though a large amount of man-hours were dedicated to the eradication of errors in our copy of the dictionary, we can make no assumption that we have found all of the errors present, and some of the trees that have not been marked bad, may indeed be errorful. Evaluation of our system, given this scenario, provides some difficulty.</p>
<p>We have a small number of known-bad trees from the original source dictionary. The large remainder of trees is of questionable character, but are probably good. We cannot make large-scale automatic judgments on the questionable trees, but we can make sure the known-bad trees are ranked highly in our system. Actual precision should be considered at least the numbers reported. Unfortunately, without known-good trees, it is difficult to provide reliable recall measurements.</p>
<p>Future Work</p>
<p>Iterative language model improvement</p>
<p>As each error in a dictionary is corrected, the language model created from that dictionary improves. An iterative approach, having a linguist examine a small R, correcting the trees, and then rerunning the model, may be the most efficient use of a linguist's time.</p>
<p>Bootstrapping a cleaner model</p>
<p>With DML, we can find a small percentage of trees that are guaranteed to have had human review at some level. These trees are more likely to be correct than the completely untouched trees, and a corpus of the trees from the final dictionary could be used to create a higher quality language model to compare against the source dictionary.</p>
<p>Node-level anomaly detection</p>
<p>Evaluation of a language model on a tag sentence outputs a probability at each word. It would be interesting to show whether the peaks of perplexity correspond to the precise errors corrected in our dictionary.</p>
<p>Related systems</p>
<p>The language modeling approach is the first in a series of experiments examining anomaly detection on dictionary structure. We have several other frameworks currently under development, and expect approaches that harness structure, instead of flattening structure, will perform with higher accuracy. Additionally, we are planning work on a graphical tool to enable dictionary editors to interact with these anomaly detection systems, and plan to research how these systems can incorporate automatic error correction with assistance of an editor.</p>
<p>Acknowledgements</p>
<p>We would like to thank Mona Madgavkar for answering questions about the dictionary we were using for evaluations, as well as the other linguists that have contributed to this dictionary's correction. Additional thanks to C. Anton Rytting, Mike Maxwell, and two anonymous reviewers for their comments on this work.</p>
<p>This material is based upon work supported, in whole or in part, with funding from the United States Government. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the University of Maryland, College Park and/or any agency or entity of the United States Government. Nothing in this report is intended to be and shall not be treated or construed as an endorsement or recommendation by the University of Maryland, United States Government, or the authors of the product, process, or service that is the subject of this report. No one may use any information contained or based on this report in advertisements or promotional materials related to any company product, process, or service or in support of other commercial purposes.</p>
<p>Figure 1 :
1Missing Orthography. Scan fromQureshi (2003).</p>
<p>Figure 2 :
2No signal between Dhivehi pronunciation and English meanings. Scan fromReynolds (2003).</p>
<p>Table 2 :
2Unique Token and n-gram grammar counts at each tree level.</p>
<p>Table 4 :
4Descending PPW 4-grams </p>
<p>Tier / R 15 30 50 
100 
500 
1000 AVG 
ENTRY .87 .93 .94 
.90 
.80 
.76 
.87 
FORM 
.80 .90 .92 
.95 
.98 
.78 
.89 
SENSE .93 .93 .96 
.91 
.85 
.81 
.90 </p>
<p>Table 5 :
5Ascending LOGPROB 4-grams</p>
<p>Table 6 :
6Descending PPWET 3-grams </p>
<p>Tier / R 15 30 
50 100 500 1000 AVG 
ENTRY .93 .73 .72 .69 
.69 
.74 
.75 
FORM .97 .98 .99 .99 
.99 
.99 
.99 
SENSE .93 .93 .92 .91 
.64 
.59 
.82 </p>
<p>Table 7 :
7Descending PPW 3-grams </p>
<p>Tier / R 15 30 
50 
100 
500 1000 AVG 
ENTRY .87 .93 .94 
.93 
.86 
.84 
.90 
FORM .87 .93 .94 
.95 
.98 
.78 
.91 
SENSE .93 .97 .98 
.94 
.91 
.87 
.93 </p>
<p>Table 8: Ascending LOGPROB 3-grams </p>
<p>Table 10 :
10Descending PPW 2-grams </p>
<p>Tier / R 15 30 
50 
100 
500 1000 AVG 
ENTRY .87 .93 .96 
.93 
.91 
.90 
.92 
FORM 1.0 1.0 1.0 
.97 
.98 
.78 
.96 
SENSE .93 .97 .98 
.96 
.96 
.91 
.95 </p>
<p>Table 11 :
11Ascending LOGPROB 2-grams</p>
<p>Lexical Markup Framework: ISO standard for semantic information in NLP lexicons. GLDV (Gesellschaft für linguistische Datenverarbeitung). G Francopoulo, N Bel, M George, N Calzolari, M Monachini, M Pet, C Soria, L Burnard, S Bauman, Tubingen HoskenP5: Guidelines for Electronic Text Encoding and Interchange. Text Encoding InitiativeFrancopoulo G., Bel N., George M., Calzolari N., Monachini M., Pet M., Soria C. (2007). Lexical Markup Framework: ISO standard for semantic information in NLP lexicons. GLDV (Gesellschaft für linguistische Datenverarbeitung), Tubingen Hosken, M. (2009) Lexicon Interchange Format. Version 0.13 DRAFT. Retrieved from http://code.google.com/p/lift-standard/ Sept. 30, 2011. Burnard, L., &amp; Bauman, S. (2007). P5: Guidelines for Electronic Text Encoding and Interchange. Text Encoding Initiative. Retrieved from http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ Sept, 30, 2011.</p>
<p>W3C XML Schema Definition Language (XSD) 1.1 Part 1: Structures. W3C Candidate Recommendation 21. S Gao, C M Sperberg-Mcqueen, H S Thompson, Gao, S., Sperberg-McQueen, C.M., Thompson, H.S. (2011). W3C XML Schema Definition Language (XSD) 1.1 Part 1: Structures. W3C Candidate Recommendation 21 July 2011. Retrieved from http://www.w3.org/TR/xmlschema11-1/ Sept. 30, 2011.</p>
<p>A Statistical Model of Lexical Context. S Jabbari, University of SheffieldPh.D ThesisJabbari, S. (2010). A Statistical Model of Lexical Context. Ph.D Thesis. University of Sheffield. Jan. 2010.</p>
<p>Information technology --Document Schema Definition Language (DSDL) --Part 2: Regular-grammar-based validation --RELAX NG. Iso, ISO/IEC 19757. ISO. (2008). ISO/IEC 19757-2:2008 Information technology --Document Schema Definition Language (DSDL) --Part 2: Regular-grammar-based validation --RELAX NG. ISO.</p>
<p>Standard 21 st Century Dictionary. B A Qureshi, Abdul Haq, Educational Publishing HouseDelhi, IndiaQureshi, B.A., Abdul Haq. (2003). Standard 21 st Century Dictionary. Educational Publishing House, Delhi, India.</p>
<p>A Maldivian Dictionary. RoutledgeCurzon. C Reynolds, New YorkReynolds, C. (2003). A Maldivian Dictionary. RoutledgeCurzon. New York.</p>
<p>SRILM -An Extensible Language Modeling Toolkit. A Stolcke, Proceedings of the International Conference on Spoken Language Processing. the International Conference on Spoken Language ProcessingDenver, ColoradoStolcke, A. (2003). SRILM -An Extensible Language Modeling Toolkit. Proceedings of the International Conference on Spoken Language Processing. Denver, Colorado. September 2002</p>
<p>Fixing Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language. Y &amp; Xia, K-F Wong, D Zajic, M Maxwell, D Doermann, P Rodrigues, M Bloodgood, Proceedings of the 11th Conference of the European Chapter of the Association for Computational LinguisticsWorkshop On New Text Wikis And Blogs And Other Dynamic Text Sources. the 11th Conference of the European Chapter of the Association for Computational LinguisticsWorkshop On New Text Wikis And Blogs And Other Dynamic Text SourcesTrento, ItalyProceedings of the Conference on Electronic Lexicography in the 21 st CenturyXia, Y &amp; Wong, K-F. (2006). Anomaly Detecting within Dynamic Chinese Chat Text. Proceedings of the 11th Conference of the European Chapter of the Association for Computational LinguisticsWorkshop On New Text Wikis And Blogs And Other Dynamic Text Sources. Trento, Italy. April 4, 2006. Zajic, D., Maxwell, M., Doermann, D., Rodrigues, P., Bloodgood, M. (2011). Fixing Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language. Proceedings of the Conference on Electronic Lexicography in the 21 st Century.</p>            </div>
        </div>

    </div>
</body>
</html>
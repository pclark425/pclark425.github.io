<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4368 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4368</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4368</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-280536896</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.03962v1.pdf" target="_blank">Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers</a></p>
                <p><strong>Paper Abstract:</strong> The growing volume of scientific literature makes it challenging for scientists to move from a list of papers to a synthesized understanding of a topic. Because of the constant influx of new papers on a daily basis, even if a scientist identifies a promising set of papers, they still face the tedious task of individually reading through dozens of titles and abstracts to make sense of occasionally conflicting findings. To address this critical bottleneck in the research workflow, we introduce a summarization feature to BIP! Finder, a scholarly search engine that ranks literature based on distinct impact aspects like popularity and influence. Our approach enables users to generate two types of summaries from top-ranked search results: a concise summary for an instantaneous at-a-glance comprehension and a more comprehensive literature review-style summary for greater, better-organized comprehension. This ability dynamically leverages BIP! Finder's already existing impact-based ranking and filtering features to generate context-sensitive, synthesized narratives that can significantly accelerate literature discovery and comprehension.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4368.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4368.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIP! Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIP! Finder Summarization Functionality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated, on-the-fly, document-grounded summarization feature added to the BIP! Finder scholarly search engine that uses LLMs to generate concise or literature-review-style summaries of an impact-ranked, user-curated set of papers with mandatory numeric citations and strict grounding to provided titles/abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BIP! Finder Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three-tier architecture: (1) BIP! Finder front-end collects user query and curated list of articles (IDs, titles, abstracts) and sends a POST to the Summarization API; (2) Summarization API selects a system prompt based on the number of articles, constructs the request payload (titles+abstracts, citation rules, grounding constraints) and mediates communication; (3) LLM service (external API or locally hosted model) performs document-grounded, abstractive generation and returns a structured summary and references. The API is a standalone microservice (public source available) compatible with OpenAI-style APIs and inference servers (TGI, vLLM). The system enforces rules in prompts: mandatory numeric citations for all claims, prohibition of any information not contained in provided titles/abstracts, and a required narrative structure (intro, thematic grouping, conclusion) for literature-review mode.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>DeepSeek V3 (currently used); backend compatible with other OpenAI-style models or locally hosted models via TGI/vLLM</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Document-grounded retrieval-then-generation (RAG-inspired): the system consumes the provided list of article metadata (titles and abstracts) rather than querying external corpora; grounding enforced by prompt rules to forbid inventing facts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Abstractive multi-document summarization with dual-mode generation: (a) concise single-paragraph synthesis for 1-5 articles; (b) literature-review-style multi-paragraph syntheses for 6-20 articles that thematically group findings and synthesize trends; uses prompt-engineered narrative structure and required inline numeric citations to link claims to source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on 1-20 articles (concise: 1-5; literature-review-style: 6-20)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature (integrated into BIP! Finder search; topic depends on user query and curated results)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concise summaries (single paragraph), literature review-style multi-paragraph summaries, both with numeric citations and a references list</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Context-aware, impact-ranked input sets (using BIP! Finder's impact indicators) enable more meaningful, coherent syntheses than summarizing arbitrary retrieved documents; strict grounding prompts and mandatory citations reduce hallucination risk and make outputs navigable back to source literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of summaries depends critically on the relevance and coherence of the user-curated source set; potential LLM hallucination risk (mitigated by forbidding non-sourced content in prompts) and reliance on only titles/abstracts (not full texts) which can limit factual completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Mode selection is explicitly tied to the number of input articles (1-5 vs 6-20). No detailed empirical scaling analysis with larger numbers of papers or model sizes is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4368.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4368.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-then-generation question-answering system that first retrieves relevant papers and then generates cited answers grounded in the retrieved content, presented as an inspiration for document-grounded LLM workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core-gpt: Combining open access research and large language models for credible, trustworthy question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a question-answering pipeline that retrieves relevant papers and uses an LLM to generate a cited, grounded answer based on the retrieved documents; presented as an exemplar of the RAG paradigm applied to scholarly QA.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented approach (retrieve relevant papers, then generate answers grounded in retrieved content); implicit use of retrieval for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generates cited answers by synthesizing information from retrieved documents (RAG-style answer composition).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-access scholarly literature / question-answering over research papers</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Cited question-answering responses (document-grounded answers)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as a key inspiration for document-grounded LLM applications in scholarly QA, illustrating retrieval+generation with explicit citations to improve trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4368.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4368.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLMs: LLMs for literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive pipeline that targets literature review generation by combining retrieval with plan-based (multi-step) text generation to produce literature-review-style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllms, llms for literature review: Are we there yet?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLMs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a pipeline that includes retrieval of relevant literature followed by a plan-based generation stage where the model constructs a multi-step plan and produces a literature-review-style synthesis; referenced as covering retrieval and plan-based text generation for literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval followed by plan-based generation (the paper states a comprehensive pipeline including retrieval and planning for review generation).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Plan-based text generation for literature review synthesis (multi-step generation to create structured review text).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review generation across scientific domains (general-purpose pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature reviews / review-style summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Represents a comprehensive example of end-to-end literature review pipelines using LLMs, emphasizing retrieval + planning to structure multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4368.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4368.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructSumm-2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating a structured summary of numerous academic papers: Dataset and method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work introducing a dataset and method for generating structured summaries from many academic papers, relevant to multi-document summarization and automated literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating a structured summary of numerous academic papers: Dataset and method.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generating a structured summary of numerous academic papers</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a work that provides both a dataset and a method aimed at producing structured summaries for numerous academic papers; specifics of architecture or LLM usage are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Structured-summary generation (exact technical approach not described in the citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic paper summarization (general across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured summaries of many academic papers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4368.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4368.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CS-PaperSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CS-PaperSum: A large-scale dataset of AI-generated summaries for scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced dataset effort (CS-PaperSum) that compiles large numbers of AI-generated summaries for scientific papers and is relevant for training/evaluating multi-document or paper-level summarization systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cs-papersum: A large-scale dataset of ai-generated summaries for scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CS-PaperSum</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a large-scale dataset of AI-generated summaries for scientific papers (paper provides dataset; method details are in the cited work and not described here).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>AI-generated summaries for scientific papers (computer science focus implied by name)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>AI-generated paper summaries (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4368.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4368.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Keywordsâ†’StructuredSumm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>From keywords to structured summaries: Streamlining scholarly information access</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced method/paper that proposes transforming keywords into structured summaries to streamline scholarly information access, relevant to automated summarization pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From keywords to structured summaries: Streamlining scholarly information access.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>From keywords to structured summaries</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a work that addresses producing structured summaries from keyword inputs to ease scholarly access; the citing paper does not detail the architecture or model choices of this work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Keyword-driven structured summarization (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scholarly summarization / information access</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured summaries derived from keywords</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Core-gpt: Combining open access research and large language models for credible, trustworthy question answering <em>(Rating: 2)</em></li>
                <li>Litllms, llms for literature review: Are we there yet?. <em>(Rating: 2)</em></li>
                <li>Generating a structured summary of numerous academic papers: Dataset and method. <em>(Rating: 2)</em></li>
                <li>Cs-papersum: A large-scale dataset of ai-generated summaries for scientific papers. <em>(Rating: 1)</em></li>
                <li>From keywords to structured summaries: Streamlining scholarly information access. <em>(Rating: 1)</em></li>
                <li>Ai and generative ai for research discovery and summarization. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4368",
    "paper_id": "paper-280536896",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "BIP! Summarization",
            "name_full": "BIP! Finder Summarization Functionality",
            "brief_description": "An integrated, on-the-fly, document-grounded summarization feature added to the BIP! Finder scholarly search engine that uses LLMs to generate concise or literature-review-style summaries of an impact-ranked, user-curated set of papers with mandatory numeric citations and strict grounding to provided titles/abstracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BIP! Finder Summarization",
            "system_description": "Three-tier architecture: (1) BIP! Finder front-end collects user query and curated list of articles (IDs, titles, abstracts) and sends a POST to the Summarization API; (2) Summarization API selects a system prompt based on the number of articles, constructs the request payload (titles+abstracts, citation rules, grounding constraints) and mediates communication; (3) LLM service (external API or locally hosted model) performs document-grounded, abstractive generation and returns a structured summary and references. The API is a standalone microservice (public source available) compatible with OpenAI-style APIs and inference servers (TGI, vLLM). The system enforces rules in prompts: mandatory numeric citations for all claims, prohibition of any information not contained in provided titles/abstracts, and a required narrative structure (intro, thematic grouping, conclusion) for literature-review mode.",
            "llm_model_used": "DeepSeek V3 (currently used); backend compatible with other OpenAI-style models or locally hosted models via TGI/vLLM",
            "extraction_technique": "Document-grounded retrieval-then-generation (RAG-inspired): the system consumes the provided list of article metadata (titles and abstracts) rather than querying external corpora; grounding enforced by prompt rules to forbid inventing facts.",
            "synthesis_technique": "Abstractive multi-document summarization with dual-mode generation: (a) concise single-paragraph synthesis for 1-5 articles; (b) literature-review-style multi-paragraph syntheses for 6-20 articles that thematically group findings and synthesize trends; uses prompt-engineered narrative structure and required inline numeric citations to link claims to source documents.",
            "number_of_papers": "Operates on 1-20 articles (concise: 1-5; literature-review-style: 6-20)",
            "domain_or_topic": "General scientific literature (integrated into BIP! Finder search; topic depends on user query and curated results)",
            "output_type": "Concise summaries (single paragraph), literature review-style multi-paragraph summaries, both with numeric citations and a references list",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Context-aware, impact-ranked input sets (using BIP! Finder's impact indicators) enable more meaningful, coherent syntheses than summarizing arbitrary retrieved documents; strict grounding prompts and mandatory citations reduce hallucination risk and make outputs navigable back to source literature.",
            "limitations_challenges": "Quality of summaries depends critically on the relevance and coherence of the user-curated source set; potential LLM hallucination risk (mitigated by forbidding non-sourced content in prompts) and reliance on only titles/abstracts (not full texts) which can limit factual completeness.",
            "scaling_behavior": "Mode selection is explicitly tied to the number of input articles (1-5 vs 6-20). No detailed empirical scaling analysis with larger numbers of papers or model sizes is reported.",
            "uuid": "e4368.0",
            "source_info": {
                "paper_title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "CORE-GPT",
            "name_full": "CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering",
            "brief_description": "A retrieval-then-generation question-answering system that first retrieves relevant papers and then generates cited answers grounded in the retrieved content, presented as an inspiration for document-grounded LLM workflows.",
            "citation_title": "Core-gpt: Combining open access research and large language models for credible, trustworthy question answering",
            "mention_or_use": "mention",
            "system_name": "CORE-GPT",
            "system_description": "Described as a question-answering pipeline that retrieves relevant papers and uses an LLM to generate a cited, grounded answer based on the retrieved documents; presented as an exemplar of the RAG paradigm applied to scholarly QA.",
            "llm_model_used": null,
            "extraction_technique": "Retrieval-augmented approach (retrieve relevant papers, then generate answers grounded in retrieved content); implicit use of retrieval for grounding.",
            "synthesis_technique": "Generates cited answers by synthesizing information from retrieved documents (RAG-style answer composition).",
            "number_of_papers": null,
            "domain_or_topic": "Open-access scholarly literature / question-answering over research papers",
            "output_type": "Cited question-answering responses (document-grounded answers)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Serves as a key inspiration for document-grounded LLM applications in scholarly QA, illustrating retrieval+generation with explicit citations to improve trustworthiness.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4368.1",
            "source_info": {
                "paper_title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "LitLLMs",
            "name_full": "LitLLMs: LLMs for literature review",
            "brief_description": "A comprehensive pipeline that targets literature review generation by combining retrieval with plan-based (multi-step) text generation to produce literature-review-style outputs.",
            "citation_title": "Litllms, llms for literature review: Are we there yet?.",
            "mention_or_use": "mention",
            "system_name": "LitLLMs",
            "system_description": "Described as a pipeline that includes retrieval of relevant literature followed by a plan-based generation stage where the model constructs a multi-step plan and produces a literature-review-style synthesis; referenced as covering retrieval and plan-based text generation for literature reviews.",
            "llm_model_used": null,
            "extraction_technique": "Retrieval followed by plan-based generation (the paper states a comprehensive pipeline including retrieval and planning for review generation).",
            "synthesis_technique": "Plan-based text generation for literature review synthesis (multi-step generation to create structured review text).",
            "number_of_papers": null,
            "domain_or_topic": "Literature review generation across scientific domains (general-purpose pipeline)",
            "output_type": "Literature reviews / review-style summaries",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Represents a comprehensive example of end-to-end literature review pipelines using LLMs, emphasizing retrieval + planning to structure multi-document synthesis.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4368.2",
            "source_info": {
                "paper_title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "StructSumm-2023",
            "name_full": "Generating a structured summary of numerous academic papers: Dataset and method",
            "brief_description": "A referenced work introducing a dataset and method for generating structured summaries from many academic papers, relevant to multi-document summarization and automated literature synthesis.",
            "citation_title": "Generating a structured summary of numerous academic papers: Dataset and method.",
            "mention_or_use": "mention",
            "system_name": "Generating a structured summary of numerous academic papers",
            "system_description": "Referenced as a work that provides both a dataset and a method aimed at producing structured summaries for numerous academic papers; specifics of architecture or LLM usage are not detailed in this paper.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Structured-summary generation (exact technical approach not described in the citing paper).",
            "number_of_papers": null,
            "domain_or_topic": "Academic paper summarization (general across domains)",
            "output_type": "Structured summaries of many academic papers",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4368.3",
            "source_info": {
                "paper_title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "CS-PaperSum",
            "name_full": "CS-PaperSum: A large-scale dataset of AI-generated summaries for scientific papers",
            "brief_description": "A referenced dataset effort (CS-PaperSum) that compiles large numbers of AI-generated summaries for scientific papers and is relevant for training/evaluating multi-document or paper-level summarization systems.",
            "citation_title": "Cs-papersum: A large-scale dataset of ai-generated summaries for scientific papers.",
            "mention_or_use": "mention",
            "system_name": "CS-PaperSum",
            "system_description": "Referenced as a large-scale dataset of AI-generated summaries for scientific papers (paper provides dataset; method details are in the cited work and not described here).",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "AI-generated summaries for scientific papers (computer science focus implied by name)",
            "output_type": "AI-generated paper summaries (dataset)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4368.4",
            "source_info": {
                "paper_title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Keywordsâ†’StructuredSumm",
            "name_full": "From keywords to structured summaries: Streamlining scholarly information access",
            "brief_description": "A referenced method/paper that proposes transforming keywords into structured summaries to streamline scholarly information access, relevant to automated summarization pipelines.",
            "citation_title": "From keywords to structured summaries: Streamlining scholarly information access.",
            "mention_or_use": "mention",
            "system_name": "From keywords to structured summaries",
            "system_description": "Cited as a work that addresses producing structured summaries from keyword inputs to ease scholarly access; the citing paper does not detail the architecture or model choices of this work.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Keyword-driven structured summarization (details not provided in this paper).",
            "number_of_papers": null,
            "domain_or_topic": "Scholarly summarization / information access",
            "output_type": "Structured summaries derived from keywords",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4368.5",
            "source_info": {
                "paper_title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Core-gpt: Combining open access research and large language models for credible, trustworthy question answering",
            "rating": 2,
            "sanitized_title": "coregpt_combining_open_access_research_and_large_language_models_for_credible_trustworthy_question_answering"
        },
        {
            "paper_title": "Litllms, llms for literature review: Are we there yet?.",
            "rating": 2,
            "sanitized_title": "litllms_llms_for_literature_review_are_we_there_yet"
        },
        {
            "paper_title": "Generating a structured summary of numerous academic papers: Dataset and method.",
            "rating": 2,
            "sanitized_title": "generating_a_structured_summary_of_numerous_academic_papers_dataset_and_method"
        },
        {
            "paper_title": "Cs-papersum: A large-scale dataset of ai-generated summaries for scientific papers.",
            "rating": 1,
            "sanitized_title": "cspapersum_a_largescale_dataset_of_aigenerated_summaries_for_scientific_papers"
        },
        {
            "paper_title": "From keywords to structured summaries: Streamlining scholarly information access.",
            "rating": 1,
            "sanitized_title": "from_keywords_to_structured_summaries_streamlining_scholarly_information_access"
        },
        {
            "paper_title": "Ai and generative ai for research discovery and summarization.",
            "rating": 1,
            "sanitized_title": "ai_and_generative_ai_for_research_discovery_and_summarization"
        }
    ],
    "cost": 0.01094675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers</p>
<p>Paris Koloveas 
IMSI
Athena RC
AthensGreece</p>
<p>University of the Peloponnese
TripolisGreece</p>
<p>Serafeim Chatzopoulos 
IMSI
Athena RC
AthensGreece</p>
<p>Dionysis Diamantis 
IMSI
Athena RC
AthensGreece</p>
<p>Christos Tryfonopoulos 
University of the Peloponnese
TripolisGreece</p>
<p>Thanasis Vergoulis 
IMSI
Athena RC
AthensGreece</p>
<p>Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers
0783859945F20505DC2D6855E647A607Multi-Document SummarizationScientific LiteratureLarge Language ModelsLiterature Review GenerationDocument-Grounded Generation
The growing volume of scientific literature makes it challenging for scientists to move from a list of papers to a synthesized understanding of a topic.Because of the constant influx of new papers on a daily basis, even if a scientist identifies a promising set of papers, they still face the tedious task of individually reading through dozens of titles and abstracts to make sense of occasionally conflicting findings.To address this critical bottleneck in the research workflow, we introduce a summarization feature to BIP! Finder, a scholarly search engine that ranks literature based on distinct impact aspects like popularity and influence.Our approach enables users to generate two types of summaries from top-ranked search results: a concise summary for an instantaneous at-aglance comprehension and a more comprehensive literature review-style summary for greater, better-organized comprehension.This ability dynamically leverages BIP! Finder's already existing impact-based ranking and filtering features to generate context-sensitive, synthesized narratives that can significantly accelerate literature discovery and comprehension.</p>
<p>Introduction</p>
<p>The growth rate of scientific publications continues to accelerate, driven by an expanding global research community and the intense pressure to "publish or perish".This trend has been amplified by the emergence of AI-assisted writing tools that facilitate generating publishable content [1,2].Consequently, identifying the most valuable and relevant articles for a given research query has become a challenging task.Academic search engines like Google Scholar 1 and Semantic Scholar2 attempt to mitigate this issue by combining keyword relevance with citation-based impact measures to rank results, helping researchers prioritize their reading.However, these systems often rely on simplistic impact measures, like raw citation counts, which are prone to manipulation, biased against recent articles, and treat impact as a single dimension, overlooking the different aspects of scientific impact [3].BIP! Finder [4] addresses this issue by incorporating multiple impact indicators that capture distinctly different aspects of a publication's impact, such as its overall impact (also known as 'influence') and its current attention (also known as 'popularity').As a scientific search engine, BIP! Finder enables users to rank results based on the most relevant impact indicator for their specific use case, offering a customizable and refined approach to scientific discovery.However, researchers still struggle to synthesize the core findings related to a particular question of interest from a list of prioritized articles; they should manually read abstracts to connect ideas, identify trends, and grasp the overall narrative.While BIP! Finder aids users prioritize what to read, this step remains demanding and time-consuming.</p>
<p>ISWC'25: International Semantic Web Conference, November 2-6, 2025, Nara, Japan pkoloveas@athenarc.gr(P.Koloveas); schatz@athenarc.gr(S.Chatzopoulos); dionysis.diamantis@athenarc.gr (D.Diamantis); trifon@uop.gr(C.Tryfonopoulos); vergoulis@athenarc.gr(T.Vergoulis) 0000-0003-2376-089X (P.Koloveas); 0000-0003-1714-5225 (S.Chatzopoulos); 0009-0002-3272-1294 (D.Diamantis); 0000-0003-0640-9088 (C.Tryfonopoulos); 0000-0003-0555-4128 (T.Vergoulis) This paper introduces a significant AI-assisted extension to BIP! Finder that addresses this gap.We present an on-the-fly summarization functionality powered by Large Language Models (LLMs), through a Retrieval-Augmented Generation (RAG)-inspired document-grounded approach, that enables users to generate structured, cited summaries of top-ranked articles directly within the search interface.Our main contributions can be summarized as follows:</p>
<p>â€¢ Integrated Summarization: We integrate our summarization functionality into the scientific search workflow, leveraging BIP! Finder's existing ranking and filtering capabilities.â€¢ Dual-Mode Synthesis: The system automatically selects between two summary types: a concise overview (for rapid understanding) and a more extensive literature review-style summary (for in-depth analysis).â€¢ Context-Aware Generation: By building upon BIP! Finder's eyword-based results, ranked by different impact indicators appropriate to the desired use case, the generated summaries are not of random related articles but of a coherently prioritized set, providing a more meaningful synthesis.We should note that the aforementioned summarization functionality is readily available to the registered users of BIP! Finder3 within its main search user interface. 4</p>
<p>Related Work</p>
<p>Automating the synthesis of scientific literature is a rapidly advancing field, largely driven by LLMs [5,6].While early research focused on extractive methods that select salient sentences from source documents [7], our system is fully abstractive, generating new, coherent prose, that synthesizes information, rather than merely extracting it.Similarly, recent work has focused on generating structured summaries by filling predefined templates with key information like "methodology" or "key takeaways" [8,9,10].Our approach complements this by producing unstructured, narrative summaries, which offers a more flexible and readable synthesis that is not constrained by a fixed schema.</p>
<p>Our system is best understood within the RAG paradigm, where an LLM is grounded with a trusted corpus.CORE-GPT [11], which served as a key inspiration for our work, exemplifies this as a questionanswering system that first retrieves relevant papers and then generates a cited answer based on the retrieved content.LitLLMs [12] explores a comprehensive pipeline for literature review generation, including retrieval and plan-based text generation.Our work implements a similar philosophy but with a crucial distinction in its integration: unlike systems that programmatically generate search queries or retrieve papers for a given abstract, our summarization is performed on a set of articles that has already been intelligently filtered and ranked by the user based on multi-faceted impact criteria.Finally, in contrast to manually curated knowledge exploration tools like TL;DR PROGRESS [13], our system is fully automated and general-purpose, capable of summarizing articles on any topic in real-time.</p>
<p>System Overview</p>
<p>The new summarization functionality is architecturally decoupled from the main BIP! Finder front-end to ensure modularity and scalability.The overall system architecture, depicted in Figure 1, illustrates the interaction between the user-facing application, our new backend service, and the LLM service.</p>
<p>The process starts when the user submits a summarization request from the BIP! Finder Web UI.The front-end sends a POST request to our Summarization API, with a payload containing the user's query as well as the list of selected articles (including their IDs, titles, and abstracts).Based on this, the Summarization API automatically determines the appropriate system prompt based on the number of articles and constructs a request for the LLM service; this can be either an external LLM accessed through an API or a locally deployed model.Our backend then calls the LLM (currently DeepSeek  V3 [14] is used) sending the appropriate data and the selected prompt.Then, the LLM processes the information and returns the generated, structured summary to our Summarization API, which finally relays it back to the BIP! Finder Web UI to be presented to the user.</p>
<p>This three-tier architecture allows us to separate concerns: the BIP! Finder front-end handles user interaction and data presentation, the Summarization API manages logic and communication with the LLM service, which provides the core natural language generation capability.</p>
<p>The source code for the Summarization API is publicly available on GitHub 5 .The API is architected as a standalone microservice, allowing it to be deployed and used independently of the BIP! Finder frontend; any system can utilize this service by following the request examples provided 6 .Furthermore, the service is designed for flexibility in its choice of LLM.While our current implementation leverages the DeepSeek V3 model, the backend is compatible with any LLM that adheres to the OpenAI API standard.This allows for easy substitution with other commercial models or even locally hosted open-source models served through inference servers like TGI 7 or vLLM [15].</p>
<p>Summarization Functionality</p>
<p>The primary goal of our new feature is to accelerate knowledge synthesis.It achieves this by providing on-demand, structured summaries tailored to the user's immediate needs.The functionality is governed by two carefully engineered system prompts8 that guide an LLM to produce high-quality academic text.</p>
<p>The system offers two modes of summarization, automatically selected based on the number of articles the user chooses to include in the summary:</p>
<p>â€¢ Concise Summary (1-5 Articles): When a user requests a summary for a small set of topranked articles, the system generates a tightly focused, single-paragraph summary.The goal is to provide a quick, digestible overview of the key contributions and themes.This is powered by our conscise system prompt.â€¢ Literature Review-Style Summary (6-20 Articles): For users wanting a deeper understanding of a topic, the system can generate a multi-paragraph summary resembling a literature review.This mode is invoked when more than 6 articles are selected.Our lit-review system prompt directs the model to structure the output into 3-4 paragraphs, starting with a contextual introduction, thematically grouping findings, and concluding with a synthesis of major trends.The quality and structure of the generated summaries are ensured through meticulous prompt engineering.Rather than sending a generic instruction, our prompts enforce a set of rules designed to produce text that is useful in an academic context.Key instructions common to both prompts include:</p>
<p>â€¢ Verifiability through Citation: The most critical instruction is the mandatory citation for every claim using a numeric format.This transforms the summary from a simple block of text into a navigable index of the source literature, allowing researchers to immediately trace a statement back to the paper it originated from.</p>
<p>â€¢ Grounded Generation: By strictly forbidding the use of any information not explicitly present in the provided titles and abstracts, we mitigate the risk of LLM "hallucinations" or fabrication.This ensures the summary is a faithful synthesis of the provided source material.â€¢ Enforced Narrative Structure: Both prompts dictate a specific narrative flow.For instance, the lit-review prompt explicitly requires an introductory paragraph that sets the context, body paragraphs that thematically group studies, and a concluding paragraph that synthesizes findings.This moves the output beyond a simple list of facts to a coherent, analytical text that highlights relationships, trends, and knowledge gaps between the papers.â€¢ Scholarly Tone and Style: The prompts guide the LLM to adopt formal academic language, encouraging it to compare and contrast methodologies and identify consensus or contradictions, which are key elements of a genuine literature review.</p>
<p>User Interface</p>
<p>The summarization functionality is integrated into BIP!Finder's search interface to support a natural user workflow, where a researcher first identifies a set of source articles before synthesizing their content.The entire process and its corresponding interface elements are visualized in Figure 2. A user's interaction is designed to first shape the list of articles from which the summary will be generated.The quality of any summary is directly dependent on the relevance of its source material, so the interface provides a suite of tools for this essential curation process.For instance, a user can apply filters from the sidebar to narrow the results by a specific publication date range, focus only on certain artifact types like Publications or Datasets, or isolate papers belonging to one or more research topics (Figure 2, A).In parallel, they can use the "Ordering" dropdown menu (Figure 2, B) to rank these filtered results not just chronologically, but by distinct aspects of scientific impact, such as long-term Influence or current Popularity.This combination of granular filtering and sophisticated impact-based ranking allows a researcher to precisely define a high-quality, thematically coherent set of articles before proceeding to the summarization step.</p>
<p>Once the user is satisfied with the curated list, they can generate a synthesis.The initial summarization can be triggered by the "Summarize top results" button (Figure 2, C), which uses the top 5 articles by default.For more fine-grained control, a set of controls is presented directly above the summary output area.Here, the user can adjust the number of articles to include (from 1 to 20) and click the "Summarize" button to produce a new summary based on the specified number (Figure 2, D).This single adjustment directly controls the summarization mode: selecting 1-5 articles will always produce the concise summary, while selecting 6 or more articles automatically triggers the generation of the more extensive, literature review-style summary.To facilitate the use of this generated text in the researcher's own writing or note-taking tools, a "Copy to clipboard" button is also provided within the summary box (Figure 2, E), allowing for one-click transfer of the summary.</p>
<p>Demonstration Scenarios</p>
<p>To illustrate the capabilities of the new summarization functionality, we present two demonstration scenarios that showcase its practical applications in academic research.</p>
<p>Scenario 1: Getting a Quick Gist of a Trending Topic</p>
<p>An experienced researcher wants to quickly understand the latest developments in "artificial intelligence in agriculture".1.The user enters the query "artificial intelligence agriculture" into the search box.2. They leave the ordering on the default, "Popularity", to see the most recent, trending works.</p>
<ol>
<li>Instead of reading through the abstracts of the top papers one by one, they click the "Summarize top results" button.4. The system generates a concise, single-paragraph summary of the top 5 most popular articles, displayed in a text box.The summary synthesizes the key approaches and findings, with citations linking back to the articles in the results list.This allows them to grasp the current state of research in under a minute.</li>
</ol>
<p>Scenario 2: Building a Foundation for a Literature Review</p>
<p>A new PhD student needs to start a literature review on "citation ranking" and wants to identify foundational and key contributing papers.1.The user searches for "citation ranking".2. To find the most foundational works, they change the ranking criterion from "Popularity" to "Influence".This re-orders the results to prioritize seminal papers with long-term impact.3.They also use the filters on the left to narrow the results to the topic "Artificial intelligence" to exclude irrelevant papers.4. They want a more detailed overview, so they use the controls above the summary to set the number of results to 10 and click "Summarize".5.The system generates a multi-paragraph, literature review-style summary.The summary introduces the core problem, thematically groups different approaches to citation ranking found in the papers, compares their methodologies, and provides a synthesized conclusion, with all claims properly cited.This provides them with a solid, structured starting point for their detailed review.</p>
<p>Conclusions</p>
<p>We have presented the summarization functionality implemented in the BIP! Finder scientific search engine.By using on-the-fly, dual-mode summarization in combination with state-of-the-art, impactbased ranking, our system enables researchers to accelerate the knowledge discovery process.It complements the gap between the determination of pertinent literature and synthesizing its main concepts.With this addition, we show that context-aware summarization of influence or popularityranked articles is well-positioned to be a solution for both overview and in-depth literature research.</p>
<p>g., External API, Local LLM Server) {summary, references}</p>
<p>Figure 1 :
1
Figure 1: System Architecture of the BIP! Finder Summarization Functionality.</p>
<p>Figure 2 :
2
Figure 2: BIP! Finder Web UI with Summarization. A. Filters for narrowing results.B. Dropdown for ordering articles by impact indicators.C. Button to generate a summary of top 5 articles.D. Options to regenerate summary with user-defined article count.E. Button to copy summary and references to system clipboard.</p>
<p>https://scholar.google.com/
https://www.semanticscholar.org/
https://bip.imsi.athenarc.gr/
For review purposes, an anonymous test account is available: Username: test_user, Password: aRdIckiNOrte.
https://github.com/athenarc/bip-scientific-summarization
https://github.com/athenarc/scientific-summarization-api/blob/main/curl_example.md
https://github.com/huggingface/text-generation-inference
https://github.com/athenarc/scientific-summarization-api/blob/main/system_prompts.yaml
AcknowledgmentsThis work has received funding from the EU's Horizon Europe framework programme as part of the SciLake project (GA: 101058573).
The scholarly footprint of chatgpt: a bibliometric analysis of the early outbreak phase. L Mou, H Calvo, Z Wu, D Ã˜ Madsen, E Silva, D Madsen, F Farhat, E S Silva, H Hassani, S S Sohail, Y Himeur, M A Alam, A Zafar, Frontiers in Artificial Intelligence. 62024</p>
<p>A A Alzaabi, A Alamri, H Albalushi, R Aljabri, A K Aalabdulsalam, Chatgpt applications in academic research: A review of benefits, concerns, and recommendations. 2023</p>
<p>Impact-based ranking of scientific publications: A survey and experimental evaluation. I Kanellos, T Vergoulis, D Sacharidis, T Dalamagas, Y Vassiliou, 10.1109/TKDE.2019.2941206doi:10.1109/TKDE.2019.2941206IEEE Trans. Knowl. Data Eng. 332021</p>
<p>Bip! finder: Facilitating scientific literature search by exploiting impact-based ranking. T Vergoulis, S Chatzopoulos, I Kanellos, P Deligiannis, C Tryfonopoulos, T Dalamagas, 10.1145/3357384.3357850doi:10.1145/3357384.3357850Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM '19. the 28th ACM International Conference on Information and Knowledge Management, CIKM '19New York, NY, USAAssociation for Computing Machinery2019</p>
<p>M Glickman, Y Zhang, arXiv:2401.06795Ai and generative ai for research discovery and summarization. 2024arXiv preprint</p>
<p>Z Luo, Z Yang, Z Xu, W Yang, X Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>Scientific article summarization using citation-context and article's discourse structure. A Cohan, N Goharian, arXiv:1704.066192017arXiv preprint</p>
<p>S Liu, J Cao, R Yang, Z Wen, arXiv:2302.04580Generating a structured summary of numerous academic papers: Dataset and method. 2023arXiv preprint</p>
<p>J Liu, A Vats, Z He, arXiv:2502.20582Cs-papersum: A large-scale dataset of ai-generated summaries for scientific papers. 2025arXiv preprint</p>
<p>M Shamsabadi, J Souza, arXiv:2402.14622From keywords to structured summaries: Streamlining scholarly information access. 2024arXiv preprint</p>
<p>Core-gpt: Combining open access research and large language models for credible, trustworthy question answering. D Pride, M Cancellieri, P Knoth, International Conference on Theory and Practice of Digital Libraries. Springer2023</p>
<p>S Agarwal, G Sahu, A Puri, I H Laradji, K D Dvijotham, J Stanley, L Charlin, C , arXiv:2412.15249Litllms, llms for literature review: Are we there yet?. 2024arXiv preprint</p>
<p>S Syed, K Al-Khatib, M Potthast, arXiv:2402.06913Tl; dr progress: Multi-faceted literature exploration in text summarization. 2024arXiv preprint</p>
<p>Deepseek-Ai , A Liu, B Feng, B Xue, B Wang, B Wu, . . , Z Pan, arXiv:2412.19437Deepseek-v3 technical report. 2025</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>            </div>
        </div>

    </div>
</body>
</html>
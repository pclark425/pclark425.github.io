<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-246 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-246</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-246</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-272762520</p>
                <p><strong>Paper Title:</strong> Scaffolding learning: From specific to generic with large language models</p>
                <p><strong>Paper Abstract:</strong> Large language models such as ChatGPT have been shown to excel in solving complex math problems. However, they cannot solve basic arithmetic problems such as 758*639 = 484,362. This makes us ponder if LLMs have been trained to solve math and science problems in the right way. When a student learns math at school, she or he starts with arithmetic, then moves to word problems, polynomials, and calculus. Each skill she or he acquires will be used in the next stage to solve more advanced problems. In this paper we propose Scaffolding Learning for LLMs, which imitates how a student learns a subject in a step-by-step manner. For example, we first train an LLM to perform highly specific operations such as multiplication and division, and then apply such “skills” in a more generic task such as solving word problems. This is related to Curriculum Training, which trains a model on tasks following a specific order, such as training on easy tasks first and then gradually increases the difficulty. Our proposed approach goes from specific tasks to generic ones, which can be considered as a special case of Curriculum Training. Our empirical studies show that when an LLM has “mastered” a specific skill, only a small amount of training is required to teach it to apply the skill to a more generic application.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e246.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e246.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-6.9B arithmetic finetune</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-6.9B fine-tuned for arithmetic operations (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Pythia-6.9B model fine-tuned on procedurally expanded, digit-tokenized training data to learn addition, subtraction, multi-digit multiplication and long division via explicit stepwise algorithmic sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multi-digit multiplication (digit-by-digit/partial-products), long division (long-division algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>operands sampled log-uniform in [10, 10000] (up to 4+ digit integers); training included many multi-digit cases (e.g., 3-4 digit multiplication and division)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on a 450K-example arithmetic dataset with explicit procedural representations: digits spaced as tokens, row-wise partial-product decomposition for multiplication, and expanded long-division steps; no chain-of-thought prompting — direct supervised fine-tuning on algorithmic sequences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Arithmetic evaluation: accuracy increased from ~75% after 100K training examples to ~94% after 450K training examples (evaluation set size 500). Word-problem transfer: after arithmetic finetune, fine-tuning on word problems achieved ~70% accuracy with 100 examples and nearly 90% accuracy with <1,000 examples (SVAMP-derived set; train split 1,892, eval 325).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal representational analysis reported; authors provide procedural engineering that encourages the model to learn algorithmic arithmetic: (1) spacing digits to make each digit a token (avoids BPE chunking), (2) representing multiplication as explicit partial-product rows with 'shifted' notation so model must learn single-digit multiplication table, zero-shifting, and addition of partial products, and (3) representing long division as repeated subtraction of largest single-digit multiples of the divisor (so the model must 'guess' quotient digits). The claimed mechanism is algorithmic learning from explicit stepwise supervision rather than memorization of whole products.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance improves with more arithmetic training examples: reported growth from 75% (100K examples) to 94% (450K examples). Transfer to word problems requires far fewer examples once arithmetic is mastered (tens to a few hundred).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Pretrained LLMs (without this fine-tuning) frequently fail on multi-digit multiplication and division (examples given: 758*639, 7518*639). The paper does not report remaining failure types for the fine-tuned Pythia beyond aggregate accuracy, but emphasizes that many training examples are required to reach high arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to: (a) directly fine-tuning the same pre-trained Pythia-6.9B on word problems (no prior arithmetic finetune) — direct finetune had ~10% accuracy after one epoch on same word-problem training set; (b) GPT-3.5 and GPT-4 as external baselines — reported as having much lower accuracy on the same arithmetic evaluation (no exact numbers reported for GPTs except specific incorrect examples).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Supervised fine-tuning on explicit, stepwise digit-level algorithmic sequences lets a 6.9B LLM reliably learn multi-digit arithmetic (94% at 450K examples) and enables rapid low-data transfer (tens–hundreds of examples) to word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaffolding learning: From specific to generic with large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e246.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e246.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaffolding Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scaffolding Learning (specific-to-generic curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy introduced in this paper that first teaches an LLM a narrowly scoped procedural skill (e.g., arithmetic algorithms) and then fine-tunes the model to apply that skill to broader, language-rich tasks (e.g., word problems).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to Pythia-6.9B in experiments (generalizable to other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>primarily addition, subtraction, multi-digit multiplication and division as the specific skills scaffolded</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>same as the arithmetic dataset used for scaffolding: operands sampled in [10,10000], multi-digit integer arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>two-stage fine-tuning: (1) large-scale fine-tuning on procedural arithmetic dataset (digitized and expanded operations), then (2) small-scale fine-tuning on target generic tasks (word problems) where arithmetic steps are expanded to the same procedural form</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using scaffolding, the model reached ~70% word-problem accuracy with 100 training examples and nearly 90% with <1,000; direct finetuning (no scaffolding) on the same word-problem data produced only ~10% accuracy after one epoch. For derivatives (analogy), scaffolding reached ~85% second-derivative accuracy with ~1,000 examples vs ~2,000 for direct training.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors argue scaffolding works because LLMs already possess language understanding and, when explicitly taught underlying algorithmic skills via stepwise supervision, can rapidly learn to combine those skills with language with very few examples; the mechanism emphasized is acquisition of procedural competence (algorithmic steps) via supervised sequences rather than emergent spontaneous algorithm induction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Scaffolding reduces required target-task examples dramatically: once the specific skill is learned at scale, generic-task performance scales quickly with only tens-to-hundreds of target examples. Training the specific skill itself requires large numbers of examples for high accuracy (hundreds of thousands for arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not suitable when a reusable sub-skill cannot be synthesized into large-scale generated training data; also does not solve issues of formalizing real-world tasks (the method requires that the target tasks can be expressed to call the learned skill).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared scaffolding (specific->generic) vs direct fine-tuning (generic only) on same datasets; scaffolding strongly outperformed direct fine-tuning for word problems and for second-derivative learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Teaching LLMs narrow algorithmic skills first (with large synthetic data) then fine-tuning on small amounts of target data enables rapid, low-data transfer to language-rich tasks that require those skills.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaffolding learning: From specific to generic with large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e246.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e246.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT baselines (GPT-3.5 / GPT-4 / GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 and GPT-4 (including GPT-4o) baseline behavior on arithmetic (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf GPT models (3.5 and 4 / 4o) are reported to frequently fail on basic multi-digit arithmetic examples in this paper and benchmark worse than the fine-tuned Pythia model on the paper's arithmetic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; GPT-4 (GPT-4o mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, multiplication and multi-digit multiplication examples cited explicitly (e.g., 758*639, 7518*639); general arithmetic and word-problem solving referenced</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>examples include 3-4 digit multiplications (e.g., 758*639, 7518*639); general grade-school arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot API queries (authors submitted arithmetic problems to OpenAI API without the scaffolding fine-tune); some qualitative prompting examples shown (e.g., decomposition/distributive property), but baseline is essentially off-the-shelf model responses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>No full numeric accuracy reported; paper states GPT-3.5 and GPT-4 accuracies are 'much lower' than the fine-tuned Pythia (94%), and presents concrete incorrect outputs: GPT-4o answered 758*639 incorrectly (returned a product for 758*239) and computed 7518*639 as 4,803,902 instead of 4,804,002.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Failures on multi-digit multiplication and arithmetic requiring exact digit-level calculation; examples show incorrect decomposition/multiplication or arithmetic carry errors leading to wrong final products (systematic arithmetic mistakes rather than comprehension failures).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively against the fine-tuned Pythia-6.9B arithmetic model and direct-finetuned Pythia: GPTs performed worse on the paper's arithmetic/evaluation examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Off-the-shelf GPT models can be unreliable on exact multi-digit integer arithmetic; explicit procedural fine-tuning markedly improves exact-arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaffolding learning: From specific to generic with large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e246.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e246.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digit-tokenization & procedural decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit-tokenization and explicit procedural decomposition for arithmetic teaching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing and representation method used by the authors: insert spaces between digits so each digit is a token, and convert vertical arithmetic (multiplication/division) into explicit sequence-of-steps (partial products with 'shifted' notation, long-division subtraction steps).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used with Pythia-6.9B experiments (general technique applicable to other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multi-digit multiplication (partial-products), long division (digit-guessing/subtractive steps)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>designed for multi-digit integers up to 4+ digits (operands up to 10000 in generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Data engineering: space-digit tokenization to avoid BPE splitting, explicit stepwise sequences representing multiplication rows (single-digit multiplications, 'shifted' zeros, sum of partial products) and expanded long-division procedures that show remainder computation and 'guessing' of largest single-digit multiples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Not reported as isolated metric, but this representation is the core of the fine-tuning dataset that produced the reported arithmetic performance (75% @ 100K -> 94% @ 450K).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Design forces the model to learn primitive algorithmic building blocks (single-digit multiplication table, zero-shifting, digit-level addition, comparison and remainder operations) by providing explicit procedural traces; authors argue this combats BPE fragmentation and makes arithmetic algorithm learning tractable for an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If digits are not tokenized (i.e., standard BPE on multi-digit tokens), the authors claim the model struggles to align operands and learn consistent multiplication tables; long multiplication represented right-to-left vs left-to-right choice may affect naturalness but authors used left-to-right rows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicitly compared to naive tokenization where numbers are chunked by BPE; authors claim their representation is easier for the LLM to learn algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Making digits atomic tokens and exposing algorithmic steps in training data is an effective representation trick that enables an LLM to learn multi-digit arithmetic procedures from supervised examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaffolding learning: From specific to generic with large language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Analyzing Mathematical Reasoning Abilities of Neural Models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Are NLP Models really able to Solve Simple Math Word Problems? <em>(Rating: 2)</em></li>
                <li>Deep Learning for Symbolic Mathematics <em>(Rating: 1)</em></li>
                <li>Measuring Mathematical Problem Solving With the MATH Dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-246",
    "paper_id": "paper-272762520",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Pythia-6.9B arithmetic finetune",
            "name_full": "Pythia-6.9B fine-tuned for arithmetic operations (this paper)",
            "brief_description": "Open-source Pythia-6.9B model fine-tuned on procedurally expanded, digit-tokenized training data to learn addition, subtraction, multi-digit multiplication and long division via explicit stepwise algorithmic sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pythia-6.9B",
            "model_size": "6.9B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multi-digit multiplication (digit-by-digit/partial-products), long division (long-division algorithm)",
            "number_range_or_complexity": "operands sampled log-uniform in [10, 10000] (up to 4+ digit integers); training included many multi-digit cases (e.g., 3-4 digit multiplication and division)",
            "method_or_intervention": "fine-tuning on a 450K-example arithmetic dataset with explicit procedural representations: digits spaced as tokens, row-wise partial-product decomposition for multiplication, and expanded long-division steps; no chain-of-thought prompting — direct supervised fine-tuning on algorithmic sequences",
            "performance_result": "Arithmetic evaluation: accuracy increased from ~75% after 100K training examples to ~94% after 450K training examples (evaluation set size 500). Word-problem transfer: after arithmetic finetune, fine-tuning on word problems achieved ~70% accuracy with 100 examples and nearly 90% accuracy with &lt;1,000 examples (SVAMP-derived set; train split 1,892, eval 325).",
            "mechanistic_insight": "No internal representational analysis reported; authors provide procedural engineering that encourages the model to learn algorithmic arithmetic: (1) spacing digits to make each digit a token (avoids BPE chunking), (2) representing multiplication as explicit partial-product rows with 'shifted' notation so model must learn single-digit multiplication table, zero-shifting, and addition of partial products, and (3) representing long division as repeated subtraction of largest single-digit multiples of the divisor (so the model must 'guess' quotient digits). The claimed mechanism is algorithmic learning from explicit stepwise supervision rather than memorization of whole products.",
            "performance_scaling": "Performance improves with more arithmetic training examples: reported growth from 75% (100K examples) to 94% (450K examples). Transfer to word problems requires far fewer examples once arithmetic is mastered (tens to a few hundred).",
            "failure_modes": "Pretrained LLMs (without this fine-tuning) frequently fail on multi-digit multiplication and division (examples given: 758*639, 7518*639). The paper does not report remaining failure types for the fine-tuned Pythia beyond aggregate accuracy, but emphasizes that many training examples are required to reach high arithmetic accuracy.",
            "comparison_baseline": "Compared to: (a) directly fine-tuning the same pre-trained Pythia-6.9B on word problems (no prior arithmetic finetune) — direct finetune had ~10% accuracy after one epoch on same word-problem training set; (b) GPT-3.5 and GPT-4 as external baselines — reported as having much lower accuracy on the same arithmetic evaluation (no exact numbers reported for GPTs except specific incorrect examples).",
            "key_finding": "Supervised fine-tuning on explicit, stepwise digit-level algorithmic sequences lets a 6.9B LLM reliably learn multi-digit arithmetic (94% at 450K examples) and enables rapid low-data transfer (tens–hundreds of examples) to word problems.",
            "uuid": "e246.0",
            "source_info": {
                "paper_title": "Scaffolding learning: From specific to generic with large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Scaffolding Learning",
            "name_full": "Scaffolding Learning (specific-to-generic curriculum)",
            "brief_description": "A training strategy introduced in this paper that first teaches an LLM a narrowly scoped procedural skill (e.g., arithmetic algorithms) and then fine-tunes the model to apply that skill to broader, language-rich tasks (e.g., word problems).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to Pythia-6.9B in experiments (generalizable to other LLMs)",
            "model_size": "6.9B",
            "model_architecture": null,
            "arithmetic_operation_type": "primarily addition, subtraction, multi-digit multiplication and division as the specific skills scaffolded",
            "number_range_or_complexity": "same as the arithmetic dataset used for scaffolding: operands sampled in [10,10000], multi-digit integer arithmetic",
            "method_or_intervention": "two-stage fine-tuning: (1) large-scale fine-tuning on procedural arithmetic dataset (digitized and expanded operations), then (2) small-scale fine-tuning on target generic tasks (word problems) where arithmetic steps are expanded to the same procedural form",
            "performance_result": "Using scaffolding, the model reached ~70% word-problem accuracy with 100 training examples and nearly 90% with &lt;1,000; direct finetuning (no scaffolding) on the same word-problem data produced only ~10% accuracy after one epoch. For derivatives (analogy), scaffolding reached ~85% second-derivative accuracy with ~1,000 examples vs ~2,000 for direct training.",
            "mechanistic_insight": "Authors argue scaffolding works because LLMs already possess language understanding and, when explicitly taught underlying algorithmic skills via stepwise supervision, can rapidly learn to combine those skills with language with very few examples; the mechanism emphasized is acquisition of procedural competence (algorithmic steps) via supervised sequences rather than emergent spontaneous algorithm induction.",
            "performance_scaling": "Scaffolding reduces required target-task examples dramatically: once the specific skill is learned at scale, generic-task performance scales quickly with only tens-to-hundreds of target examples. Training the specific skill itself requires large numbers of examples for high accuracy (hundreds of thousands for arithmetic).",
            "failure_modes": "Not suitable when a reusable sub-skill cannot be synthesized into large-scale generated training data; also does not solve issues of formalizing real-world tasks (the method requires that the target tasks can be expressed to call the learned skill).",
            "comparison_baseline": "Compared scaffolding (specific-&gt;generic) vs direct fine-tuning (generic only) on same datasets; scaffolding strongly outperformed direct fine-tuning for word problems and for second-derivative learning.",
            "key_finding": "Teaching LLMs narrow algorithmic skills first (with large synthetic data) then fine-tuning on small amounts of target data enables rapid, low-data transfer to language-rich tasks that require those skills.",
            "uuid": "e246.1",
            "source_info": {
                "paper_title": "Scaffolding learning: From specific to generic with large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT baselines (GPT-3.5 / GPT-4 / GPT-4o)",
            "name_full": "GPT-3.5 and GPT-4 (including GPT-4o) baseline behavior on arithmetic (as reported)",
            "brief_description": "Off-the-shelf GPT models (3.5 and 4 / 4o) are reported to frequently fail on basic multi-digit arithmetic examples in this paper and benchmark worse than the fine-tuned Pythia model on the paper's arithmetic evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; GPT-4 (GPT-4o mentioned)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, multiplication and multi-digit multiplication examples cited explicitly (e.g., 758*639, 7518*639); general arithmetic and word-problem solving referenced",
            "number_range_or_complexity": "examples include 3-4 digit multiplications (e.g., 758*639, 7518*639); general grade-school arithmetic",
            "method_or_intervention": "zero-shot API queries (authors submitted arithmetic problems to OpenAI API without the scaffolding fine-tune); some qualitative prompting examples shown (e.g., decomposition/distributive property), but baseline is essentially off-the-shelf model responses",
            "performance_result": "No full numeric accuracy reported; paper states GPT-3.5 and GPT-4 accuracies are 'much lower' than the fine-tuned Pythia (94%), and presents concrete incorrect outputs: GPT-4o answered 758*639 incorrectly (returned a product for 758*239) and computed 7518*639 as 4,803,902 instead of 4,804,002.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": "Failures on multi-digit multiplication and arithmetic requiring exact digit-level calculation; examples show incorrect decomposition/multiplication or arithmetic carry errors leading to wrong final products (systematic arithmetic mistakes rather than comprehension failures).",
            "comparison_baseline": "Compared qualitatively against the fine-tuned Pythia-6.9B arithmetic model and direct-finetuned Pythia: GPTs performed worse on the paper's arithmetic/evaluation examples.",
            "key_finding": "Off-the-shelf GPT models can be unreliable on exact multi-digit integer arithmetic; explicit procedural fine-tuning markedly improves exact-arithmetic performance.",
            "uuid": "e246.2",
            "source_info": {
                "paper_title": "Scaffolding learning: From specific to generic with large language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Digit-tokenization & procedural decomposition",
            "name_full": "Digit-tokenization and explicit procedural decomposition for arithmetic teaching",
            "brief_description": "A preprocessing and representation method used by the authors: insert spaces between digits so each digit is a token, and convert vertical arithmetic (multiplication/division) into explicit sequence-of-steps (partial products with 'shifted' notation, long-division subtraction steps).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "used with Pythia-6.9B experiments (general technique applicable to other LLMs)",
            "model_size": "6.9B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multi-digit multiplication (partial-products), long division (digit-guessing/subtractive steps)",
            "number_range_or_complexity": "designed for multi-digit integers up to 4+ digits (operands up to 10000 in generation)",
            "method_or_intervention": "Data engineering: space-digit tokenization to avoid BPE splitting, explicit stepwise sequences representing multiplication rows (single-digit multiplications, 'shifted' zeros, sum of partial products) and expanded long-division procedures that show remainder computation and 'guessing' of largest single-digit multiples.",
            "performance_result": "Not reported as isolated metric, but this representation is the core of the fine-tuning dataset that produced the reported arithmetic performance (75% @ 100K -&gt; 94% @ 450K).",
            "mechanistic_insight": "Design forces the model to learn primitive algorithmic building blocks (single-digit multiplication table, zero-shifting, digit-level addition, comparison and remainder operations) by providing explicit procedural traces; authors argue this combats BPE fragmentation and makes arithmetic algorithm learning tractable for an LLM.",
            "performance_scaling": null,
            "failure_modes": "If digits are not tokenized (i.e., standard BPE on multi-digit tokens), the authors claim the model struggles to align operands and learn consistent multiplication tables; long multiplication represented right-to-left vs left-to-right choice may affect naturalness but authors used left-to-right rows.",
            "comparison_baseline": "Implicitly compared to naive tokenization where numbers are chunked by BPE; authors claim their representation is easier for the LLM to learn algorithmic arithmetic.",
            "key_finding": "Making digits atomic tokens and exposing algorithmic steps in training data is an effective representation trick that enables an LLM to learn multi-digit arithmetic procedures from supervised examples.",
            "uuid": "e246.3",
            "source_info": {
                "paper_title": "Scaffolding learning: From specific to generic with large language models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Analyzing Mathematical Reasoning Abilities of Neural Models",
            "rating": 2,
            "sanitized_title": "analyzing_mathematical_reasoning_abilities_of_neural_models"
        },
        {
            "paper_title": "Chain-of-thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Are NLP Models really able to Solve Simple Math Word Problems?",
            "rating": 2,
            "sanitized_title": "are_nlp_models_really_able_to_solve_simple_math_word_problems"
        },
        {
            "paper_title": "Deep Learning for Symbolic Mathematics",
            "rating": 1,
            "sanitized_title": "deep_learning_for_symbolic_mathematics"
        },
        {
            "paper_title": "Measuring Mathematical Problem Solving With the MATH Dataset",
            "rating": 1,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        }
    ],
    "cost": 0.012574499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scaffolding learning: From specific to generic with large language models
September 20, 2024</p>
<p>David S Yin 
Lynbrook High School
San JoseCAUnited States of America</p>
<p>Xiaoxin Yin 0009-0009-5707-8558
Airbnb, San FranciscoCAUnited States of America</p>
<p>Universiteit Gent
BELGIUM</p>
<p>Scaffolding learning: From specific to generic with large language models
September 20, 2024D14A425DF212B5664937E02B0EE9EC6110.1371/journal.pone.0310409Received: June 20, 2024 Accepted: August 31, 2024
Large language models such as ChatGPT have been shown to excel in solving complex math problems.However, they cannot solve basic arithmetic problems such as 758*639 = 484,362.This makes us ponder if LLMs have been trained to solve math and science problems in the right way.When a student learns math at school, she or he starts with arithmetic, then moves to word problems, polynomials, and calculus.Each skill she or he acquires will be used in the next stage to solve more advanced problems.In this paper we propose Scaffolding Learning for LLMs, which imitates how a student learns a subject in a step-by-step manner.For example, we first train an LLM to perform highly specific operations such as multiplication and division, and then apply such "skills" in a more generic task such as solving word problems.This is related to Curriculum Training, which trains a model on tasks following a specific order, such as training on easy tasks first and then gradually increases the difficulty.Our proposed approach goes from specific tasks to generic ones, which can be considered as a special case of Curriculum Training.Our empirical studies show that when an LLM has "mastered" a specific skill, only a small amount of training is required to teach it to apply the skill to a more generic application.</p>
<p>Introduction</p>
<p>In the past few years there have been various studies showing that large language models (LLMs) can be trained to solve various types of math problems, including grade school word problems and generic math problems from arithmetic to calculus.Today's most powerful models such as GPT-4 and Bard can even solve some complex problems in Math Olympiads.However, even the most powerful LLMs often fail to solve some basic math problems in grade schools, such as multiplication and division with three or more digits.Here are some examples (Tested on July 25, 2024.GPT-4 uses Python code to answer arithmetic questions.In order to test the language model, one has to use OpenAI's API.)</p>
<p>Ex. 1 758 * 639 (correct answer is 484362) GPT-4o: The product of 758 and 239 is 181,162.(Incorrect)</p>
<p>We can see that GPT-4o fails to produce the right answer, probably because it has not seen this problem in its training data.Thus we try to teach it calculate step by step with an example.Here is our prompt: From the above examples, we can see that GPT-4o cannot perform some basic math operations such as multiplication of several digits.This will prevent it from solving more complex math problems involving such operations, just like how a student unable to do multiplication will likely get bad grades in math tests.It is surprising to us that such a powerful model which can solve advanced math problems such as differential equations fails on such basic operations.This makes us ponder whether LLMs have been trained to learn math in the right way.When children learn math at school, they start with arithmetic, which enables them to perform basic math operations such as multiplication and division.Then they learn how to solve equations, and how to solve word problems.All the arithmetic skills they acquired before enable them to solve more advanced problems correctly.</p>
<p>Here is an example word problem: "There are 346 employees in a company and the average compensation per person was 4,328 dollars in the last month.What was the total compensation in this company last month?".This is an easy problem for a sixth-grader, who has the necessary arithmetic skills.But this is extremely hard for ChatGPT because it has not learned how to perform multiplication.All it can do is try to remember previous results, which is not very useful in this situation.</p>
<p>In contrast to how a student learns math, LLMs have been trained in a very different way in previous works.As in [1], LLMs were trained with math problems of all levels, together with solutions.LLMs trained in this way successfully solved many hard math problems, including some Math Olympiad problems that most high school students cannot solve.However, they constantly fail in basic math operations that a middle schooler can solve.</p>
<p>This makes us think if we should train LLMs in a way that is more similar to how we teach students.In this study we first train our LLM to perform basic math operations, just like how an elementary school teacher would teach her students.Then we train our LLM how to use such basic operations in word problems, again similar to what a teacher would do.We found that after learning the basic math operations, our LLM quickly learns how to apply such operations in word problems, which enables it to solve these problems with very high accuracy.This is only an example that illustrates how we can teach LLMs in a level-by-level manner, which is often called "Scaffolding Learning" in education.Math is just one of the many subjects on which a student would learn level-by-level.In kinetics, a student first learns the force analysis and Newton's laws, before she or he is asked to solve kinetics problems.In calculus, a student first learns how to compute first derivatives, and then how to compute second derivatives and integrals, before they are asked to solve real-world problems.Although we only focus on basic math in this study, the same methodology can be applied to every field where a student learns in gradual advancements.This is related to Curriculum Learning [2,3], which trains a model on tasks following a meaningful order.The most common order is from easy to difficult, i.e., first trains a model on an easy task and then gradually increases the difficulty of tasks.For example, in [2] the authors trained a model to classify shapes, which starts from the easiest task of classifying single simple shapes, and gradually increases complexity by varying positions, rotating, adding multiple shapes, etc. Curriculum Learning has been widely used in many applications such as object detection [4], face detection [5], and machine translation [6].Our proposed method, Scaffolding Learning, can be considered as a special case of Curriculum Learning.Scaffolding Learning goes from specific to generic, by first training a model on specific tasks such as multiplication and division, and then fine-tuning it to accomplish more generic tasks such as solving word problems.</p>
<p>Both Scaffolding Learning and Curriculum Learning can be considered as special cases of the Pre-training and Fine-tuning Paradigm.In the past decade many large models are first trained on a generic dataset (such as ImageNet [7] or CommonCrawl), and then fine-tuned on different specific tasks to solve specific problems.What Scaffolding Learning does is analogical to a student acquiring a specific skill in one course (e.g., arithmetic skill) and then learning to apply the skill on more generic problems (e.g., word problems).</p>
<p>Here are our main contributions in this paper:</p>
<p>• We propose Scaffolding Learning for LLMs, which first train an LLM on a specific problem, and then train it on a more generic problem using the specific "skill" learned.</p>
<p>• We show that Scaffolding Learning is highly effective, as an LLM can quickly learn to apply an acquired "skill" to a more generic problem.</p>
<p>• We propose a way to teach an LLM to perform arithmetic operations (addition, subtraction, multiplication and division) of many digits.</p>
<p>Related work</p>
<p>Before the era of deep learning, researches have attempted to solve generic math problems (such as word problems) in various ways.In [8] the authors categorize every important word in a problem into either a variable holder(container), a variable(quantity), an item(entity), or a change in the quantity of a variable(state transition).With this information, the problem can be solved by using it to create an equation.</p>
<p>Word problem</p>
<p>JohnðcontainerÞ has 3ðquantityÞ applesðentityÞ: Adam gaveðstate transitionÞ himðcontainerÞ 2ðquantityÞ applesðentityÞ: How many applesðentityÞ does JohnðcontainerÞ have now?
Equation 3 þ 2 ¼ 5
Another method of interpreting a word problem is equation mapping [9], which first maps each number in the problem to a variable, then uses a model to classify the problem into several equation templates.</p>
<p>In recent years deep learning has been applied to a variety of math problems, such as grade school math problems [9][10][11], symbolic regression which infers the formula behind a curve [12,13], and differential equations [14].</p>
<p>After GPT gained popularity in recent years, it has been applied to solve math problems with Chain-of-Thought [15], Tree-of-thoughts [16], and Graph-of-thoughts [17], which construct the prompt with different structures and are shown to outperform simple prompts.In many such studies [18,19] GPT has been used to solve various reasoning problems, especially math word problems.This illustrates the math capabilities and reasoning power of LLMs.</p>
<p>Scaffolding Learning vs. Curriculum Learning</p>
<p>Curriculum Learning was proposed in 2009 [2,3], which trains a model on tasks following a meaningful order.The most common order is from easy to difficult, which first trains a model on easy tasks and then gradually increases the difficulty of tasks.For example, in [2] the authors trained a model to classify handwritten digits, which starts from clean digits and then moves to noisy ones and distorted ones.Curriculum Learning has been widely used in many applications such as object detection [4], face detection [5], and machine translation [6].</p>
<p>Unlike Curriculum Learning which usually goes from easy tasks to difficult ones, Scaffolding Learning goes from specific to generic, by first training a model on specific tasks and then fine-tuning it to perform more generic tasks.For example, in order to teach an LLM to solve elementary school math problems, we first fine-tune an LLM to learn how to perform arithmetic operations, and then it only takes a very small number of examples (e.g., a few hundred) to fine-tune it to solve word problems containing arithmetic operations, which the original model could not solve.</p>
<p>Scaffolding Learning vs. pre-training.</p>
<p>Scaffolding learning is used in nearly every education system because it is intuitive and effective.It allows a student to use her understanding of simple and specific concepts to understand more complex and general concepts.In this paper we apply the same philosophy to train deep learning models.</p>
<p>Scaffolding learning for deep learning shares some similarity with the pre-training &amp; finetuning paradigm, such as the many studies on specific tasks using fine-tuned BERT [20].A model could be pre-trained on a huge dataset such as ImageNet [7] or CommonCrawl, in order to give it a generic understanding of the domain.Then the model can be fine-tuned on specific tasks such as detecting inappropriate languages.</p>
<p>Unlike pre-training &amp; fine-tuning which goes from generic to specifc, Scaffolding learning goes from specific to generic.It first trains the model on a specific task like arithmetic operations, then uses it to solve a broader, more generic task like word problems.Just like a student acquires one or a few specific skills in each course at school, and then learns to apply such skills on more generic problems, we are trying to train LLMs in that manner.</p>
<p>Scaffolding learning allows us to "teach" an LLM to perform a specific task that it was not able to do.Afterwards the LLM can combine its language understanding capability with the newly acquired skill to solve more generic problems.</p>
<p>Model training</p>
<p>We choose to use the Pythia-6.9B[21] as our base model due to its high performance [22] and open-source nature.In the following subsections we describe how we create the training data for two scaffolding learning tasks:</p>
<p>• Train an LLM to perform arithmetic operations, and then to solve word problems using the "arithmetic skills".</p>
<p>• Train an LLM to derive the first derivative of a function, and then to derive second derivatives using the "first-derivative skill".</p>
<p>Arithmetic operations</p>
<p>Our dataset is comprised of two parts.The first is a set problems of additions, subtractions, multiplications and divisions, with 100K problems of each type.Each operand is randomly sampled from a log-uniform distribution in range [10,10000].In subtraction and division the second operand is smaller than the first.Most LLMs (including GPT-4) fail to calculate basic Arithmetic problems such as 758*639.Therefore, we try to teach our model to do the calculation like a human, in a digit-by-digit manner.For multiplication, we will be using a method that splits the problem into several single-digit sub-problems.For division we will use a process that splits the dividend into several chunks of digits that are easier to divide.</p>
<p>For these to be effective, we split each number into individual digits by inserting spaces between digits.Otherwise BPE (Byte-Pair Encoding) [23] would break a number into chunks with various lengths, and it is difficult for an LLM to even remember a multiplication table of such chunks.For example, when asked to solve the question 1009 + 8432, if BPE is used and the original question is tokenized into "100", "9", "+", "84", "32", the model will probably get trouble aligning the two operands.</p>
<p>In this subsection we will describe how we convert a vertical arrangement of an arithmetic operation into a sequence of tokens that can be learned by an LLM.We will start from addition and subtraction and then move to multiplication and division.</p>
<p>3.1.1Addition and subtraction.For an addition problem, we simply break it into digits, so that each digit is a token for an LLM (as shown below).</p>
<p>Example question
3712 þ 2401 Example answer 3712 þ 2401 ¼ 3 7 1 2 þ 2 4 0 1 ¼ 6 1 1 3
This is slightly different from an addition vertical as we do not indicate a carry.This is because the condition that a carry happens is rather simple (i.e., the digits sum to ten or above), and an LLM can easily learn that.Subtractions are handled similarly.</p>
<p>3.1.2Multiplication.Much more details are required for multiplication.If we simply use "7 5 9 * 8 4 2 = 6 3 9 0 7 8" as training data, the model will have a hard time figuring out how the answer is generated, and probably fails in a similar way as GPT does.</p>
<p>Example of Vertical Multiplication
759 �842 1518 3036 6072 639078
Therefore, we try to convert the multiplication vertical into a sequence.Just like the vertical above, we first convert each row into a sequence.For example, the first row indicates "759 × 2 = 1518", and we represent it with the following expression:
½ ð 7 * 2 ¼ 1 4 Þ shifted 2 ¼ 1 4 0 0 þ ð 5 * 2 ¼ 1 0 Þ shifted 1 ¼ 1 0 0 þð 9 * 2 ¼ 1 8 Þ ¼ 1 5 1 8 � ¼ 1 5 1 8
This above row represents a multiplication between a 3-digit number and a single digit."shifted x" means that the result should be shifted left by x digits.For example, "18 shifted 2" is 1800.In order to learn the above operation, an LLM would need to learn the following:</p>
<p>• A multiplication table for 1 through 9</p>
<p>• Shifting a number to the left by adding zeros at the end • Adding multiple numbers represented by digits, such as "1 4 0 0 + 1 0 0 + 1 8 = 1 5 1 8"</p>
<p>We convert each row in a multiplication vertical as above, and we use different types of brackets ("&lt;&gt;", "[]", "{}", "()") to indicate the start and end of a scope.In the above vertical, the second and third row needs to be shifted, as follows:
½ ð 7 * 4 ¼ 2 8 Þ shifted 2 ¼ 2 8 0 0 þ ð 5 * 4 ¼ 2 0 Þ shifted 1 ¼ 2 0 0 þð 9 * 4 ¼ 3 6 Þ ¼ 3 0 3 6 � shifted 1 ¼ 3 0 3 6 0
One difference between our representation and a typical multiplication vertical is that we calculate from left to right, instead of from right to left.In the above example, we first calculate 759 × 8 = 6072 (which then becomes 607200), and then 759 × 4 = 3036, and finally 759 × 2 = 1518.This is more consistent with the original order of the number, and looks more natural in the form of a sequence.Here is our full sequence for the above multiplication problem:
&lt; ½ ð 7 * 8 ¼ 5 6 Þ shifted 2 ¼ 5 6 0 0 þ ð5 * 8 ¼ 4 0 Þ shifted 1 ¼ 4 0 0 þ ð9 * 8 ¼ 7 2 Þ ¼ 6 0 7 2 � shifted 2 ¼ 6 0 7 2 0 0 &gt; þ &lt; ½ ð7 * 4 ¼ 2 8 Þ shifted 2 ¼ 2 8 0 0 þ ð5 * 4 ¼ 2 0 Þ shifted 1 ¼ 2 0 0 þ ð9 * 4 ¼ 3 6 Þ ¼ 3 0 3 6 � shifted 1 ¼ 3 0 3 6 0 &gt; þ½ ð7 * 2 ¼ 1 4 Þ shifted 2 ¼ 1 4 0 0 þ ð5 * 2 ¼ 1 0 Þ shifted 1 ¼ 1 0 0 þ ð9 * 2 ¼ 1 8 Þ ¼ 1 5 1 8 � ¼ 6 0 7 2 0 0 þ 3 0 3 6 0 þ 1 5 1 8 ¼ 6 3 7 5 6 0 þ 1 5 1 8 ¼ 6 3 9 0 7 8</p>
<p>Division.</p>
<p>A different approach is used for division.We teach the model to do long division, which constantly subtracts the largest possible single-digit multiple of the divisor from the first few digits of the dividend.For example, for the division problem 615/47, our first step is to subtract (47 * 1) * 10 = 470 from the dividend and add 1 * 10 = 10 to the result.Here we do not break "47" into individual digits and then run the digit-wise multiplication as in Section 3.1.2,because that would require too many tokens for each multiplication and possibly exceed a model's token limit.Instead, we rely on the model's ability to "guess" the first digit of the quotient, which is the largest single-digit multiple of the divisor that is no larger than the first few digits of the dividend.</p>
<p>Example of Long Division</p>
<p>13R4 47Þ615 47 145 141 4</p>
<p>As one can see, the model would need to learn the following tasks to perform a division correctly: (1) The overall process of division, (2) whether a number is smaller than another, and</p>
<p>(3) "guess" the largest single-digit multiple of the divisor that is no larger than the first few digits of the dividend.We will be focusing on making it understandable for the model.Below is an example of an expanded division operation.</p>
<p>Example question</p>
<p>615=47</p>
<p>Example answer
615=47 ¼ 6 1 5=4 7 ¼ &lt; ½ f ð 61=47 #because 1 * 47&61j1; remainder ¼ 61 À 1 * 47 ¼ 61 À 47 ¼ 14 Þ shifted 1 ¼ 10 g þ f ð 14 shifted 1 þ 5 ¼ 145 =47 because 3 * 47&145j3; remainder ¼ 145 À 3 * 47 ¼ 145 À 141 ¼ 4 Þ ¼ 3 g � ¼ 10 þ 3 ¼ 13 &gt;¼ 13 remainder 4
We use single-character symbols for separators and operators, in order to lower the number of tokens needed.As an example, the "&amp;" token represents "&lt;=" (which is considered to be two tokens by our model).</p>
<p>Word problems</p>
<p>We use SVAMP [24] as our word problem dataset, which consists of 7,000 grade school word problems.The constants in each problem of this dataset can also be customized, allowing us to easily change the difficulty of problems.</p>
<p>In order to teach a model to apply the arithmetic operations learned in Section 3.1 to solve word problems, we expand the math operation in a word problem using the methods in Section 3.1, which makes our model apply its "arithmetic skills" in solving the word problems.For example, this is a word problems with expanded math operation:</p>
<p>Original question</p>
<p>Sam has 615 emails: He takes 194 seconds to read each email: He sends one email every 503 seconds: How long will it take him to read all emails?</p>
<p>Modified question and answer</p>
<p>Sam has 615 emails: He takes 194 seconds to read each email: He sends one email every some seconds: How long will it take him to read all emails?
615 * 194 ¼ h ½ ð 6 * 1 ¼ 6 Þ shifted 2 ¼ 600 þ ð 1 * 1 ¼ 1 Þ shifted 1 ¼ 10 þ ð 5 * 1 ¼ 5 Þ ¼ 615 � shifted 2 ¼ 61500 i þ h ½ ð 6 * 9 ¼ 54 Þ shifted 2 ¼ 5400 þ ð 1 * 9 ¼ 9 Þ shifted 1 ¼ 9 0 þ ð 5 * 9 ¼ 45 Þ ¼ 5535 � shifted 1 ¼ 55350 i þ ½ ð 6 * 4 ¼ 24 Þ shifted 2 ¼ 2400 þ ð 1 * 4 ¼ 4 Þ shifted 1 ¼ 40 þ ð 5 * 4 ¼ 20 Þ ¼ 2460 � ¼ 61500 þ 55350 þ 2460 ¼ 116850þ 2460 ¼ 119310 ðNote :
We have removed the spaces between digits for better readability:Þ</p>
<p>In this study we focus on integer operations, in order to simulate how an elementary school student learns math.Therefore, all constants in our word problems are rounded up and then verified for correctness.All problems that become invalid are filtered out.2,217 problems remain correct and are kept.</p>
<p>When customizing the constants in each word problem, each original constant is first multiplied by a random number sampled from a log-uniform distribution of range [1000, 10000], and then constantly halved until it is less than 10000.This gives us a diverse range of constants that are usually consistent with the originals.</p>
<p>Derivatives</p>
<p>In addition to arithmetic operations and word problems, we also test Scaffolding Learning in computing the first and second derivatives of polynomial functions.It is shown that after an LLM is trained to compute the first derivatives, it only requires a very small number of training examples to teach it to compute second derivatives.</p>
<p>We use the derivatives section of DeepMind's mathematics dataset [25] to automatically generate functions and their derivatives.When customizing the constants in the functions, we use values randomly sampled from a log-uniform distribution of range [1,100].</p>
<p>The training data for first derivatives simply include the functions and their derivatives.For second derivatives, we include a very short rationale explaining that it is the first derivative done twice, as shown in the following example:
Question What is the second derivative of 62 * k * * 2 þ 43k À 9? Answer Derivative of 62 * k * * 2 þ 43k À 9 ¼ 124k þ 43: Derivative of 124k þ 43 ¼ 124</p>
<p>Experiments</p>
<p>We used the Pythia-6.9Bmodel [21] due to its high performance and open-source nature.The model was pre-trained on the Pile dataset [26] and can be downloaded on hugging face (https://huggingface.co/EleutherAI/pythia-6.9b).Example code of fine-tuning this model can be found at github (https://github.com/databrickslabs/dolly).</p>
<p>All our experiments were done using a computer with two NVIDIA A6000 GPUs, an Intel i7-12700K CPU, with Ubuntu 18.04 and Pytorch 2.0.0.When training models, we used Deep-Speed [27] to utilize multiple GPUs, and ZeRO [28] stage 3.</p>
<p>Arithmetic operations</p>
<p>To train our LLM to perform arithmetic operations, we generate examples using the method described in Section 3.1, and fine-tuned the pre-trained Pythia-6.9Bmodel with the examples.An example is never fed to our model twice, which eliminates the risk of overfitting.The training set contains 450K examples, with equal numbers of addition, subtraction, multiplication, and division.The evaluation set contains 500 examples generated in the same way.</p>
<p>Word problems</p>
<p>After fine-tuning the Pythia model to perform arithmetic operations, we fine-tune it to solve word problems as described in Section 3.2, where the training data contain the same expanded procedure of arithmetic operations as in Section 3.1.We want to see that after our model has mastered arithmetic operations, how quickly it can learn to apply such operations to solve word problems.We use a modified version of the SVAMP dataset (see Section 3.2 for details).After removing invalid cases from SVAMP dataset (such as those becoming invalid after rounding), we have 2,217 problems left.We randomly split them into a training set of 1,892 problems (85% of the dataset), and an evaluation set of 325 problems.</p>
<p>In order to test the effectiveness of Scaffolding Learning, we compare two approaches: (1) Scaffolding Learning, which uses the model fine-tuned on the arithmetic dataset (as in Section This experiment shows that Scaffolding Learning is highly effective.Just like a student who has mastered arithmetic skills can learn to apply such skills to solve word problems with a very small number of examples, an LLM can do that same.When an LLM has been trained with hundreds of thousands of examples to perform arithmetic operations, it only takes tens or a few hundred examples to learn how to use its arithmetic skills to solve word problems.This is also because the LLM has always been able to understand natural language, just like a student.</p>
<p>As a summary, when a model is equipped with both language skills and arithmetic skills, it only takes tens or a few hundred examples to learn how to combine them to solve word problems.</p>
<p>Derivatives</p>
<p>We also use Scaffolding Learning to compute high-order derivatives with a very similar procedure to word problems: The LLM is first fine-tuned to compute first derivatives.Then it can learn to compute second derivatives with a very small number of examples.We use the derivatives section of DeepMind's mathematics data generator to generate training and testing data, as described in Section 3.</p>
<p>Discussions</p>
<p>Pre-training and fine-tuning has become the paradigm in creating Neural Networks and especially LLMs, which usually pre-trains on generic datasets (such as CommonCrawls and Ima-geNet) and then fine-tunes on specific tasks (such as question answering and face detection).Curriculum Learning is a special case of pre-training and fine-tuning, as it pre-trains a model on a meaningful order.The most common order is to train on an easy version of the task first, and then keeps fine-tuning the model on tasks with gradually increased difficulties.We propose Scaffolding Learning, which is a special case of Curriculum Learning.It first finetunes a base model on a specific task, in order to learn a particular "skill" such as arithmetic calculations.Then the model is fine-tuned to apply this skill on more generic problems.</p>
<p>In order to measure the effectiveness of Scaffolding Learning, we test how quickly a model can learn to apply an acquired skill to a new scenario.For example, we first fine-tune an LLM to enable it to perform arithmetic calculations, and then test how quickly it can learn to use its arithmetic skills to solve word problems.It turns out that an LLM can learn to apply its newly acquired skill with a surprising small number of examples (e.g., a few hundred).While if we directly fine-tune the LLM to solve word problems without first teaching it to perform arithmetic operations, the LLM could not learn anything with thousands of examples.</p>
<p>Why is it useful?</p>
<p>In many real-world applications in science and engineering, the training examples are generated by humans and exist in limited quantities.For example, in physics problems one often need to use calculus or statistics to solve a real-world problem (such as calculating the orbits of celestial objects).It is often challenging to directly train an LLM to solve such problems, because of the lack of sufficient training data.On the other hand, it is often possible to generate infinitely many examples for the underlying mathematical or technical problem.For example, one can generate infinitely many problems for calculus, with randomized equations and math tools such as Matlab or SymPy.</p>
<p>Scaffolding Learning can help us train baseline models that master various skills for the underlying problems, such as calculus, polynomial operations, and matrix operations.One can usually generate infinite training data for such tasks, and train an LLM to master such skills.Then the model can be fine-tuned to apply such skills to solve real-world problems, which often need to be created manually from real-world scenarios.Since only a small number of examples are needed for the second step, Scaffolding Learning can greatly reduce the manual efforts required, and increase the model's accuracy on the real-world problems.</p>
<p>Possible limitations</p>
<p>The first limitation of Scaffolding Learning is that it is only suitable for tasks where one can generate many training examples for an important sub-problem.For example, it is challenging to apply it on algorithm problems, as it is non-trivial to create many algorithm problems with solutions.</p>
<p>The second limitation is that, after the model is trained to master the underlying skill, it still needs to be fine-tuned to learn how to apply this skill on the original task.While this can be straightforward for word problems, formalizing a problem is sometimes a most challenging part in solving it.For example, in game theory and equilibrium theory, it is more challenging to formalize the problems mathematically than to solve them.Scaffolding Learning cannot assist with formalizing problems, which usually requires common sense.</p>
<p>Despite the above limitations, Scaffolding Learning can still be applied to many applications where training data is limited, but one can generate many training cases for the underlying problem.It is our future work to study how Scaffolding Learning can be combined with common sense, potentially expanding its applicability to a broader range of tasks.</p>
<p>Conclusions</p>
<p>In this paper we present Scaffolding Learning, in which a large language model is first trained on a specific task (such as arithmetic operations), and then fine-tuned on a more generic task (such as word problems).This is analogical to how a student learns at school, by first mastering arithmetic skills and then learning to combine such skills with her language skills and common sense to solve real-life problems.</p>
<p>Our experiments show that Scaffolding Learning can be successfully applied to training large language models.When a model has been pre-trained to understand language and finetuned to master a specific skill (e.g., arithmetic operations), it only takes a very small amount of training for it to apply the skills on more generic tasks such as solving word problems.This would enable us to train a large language models to learn various specific skills, and then combine those skills with its language capability and common sense in many applications.</p>
<p>Ex. 2
2
Prompt: Example Question: 237 * 453 Example Answer: Decompose 453 = 400 + 50 + 3. Therefore, 237 * 453 = 237 * 400 + 237 * 50 + 237 * 3 = 94800 + 11850 + 711 = 107361 Question: 7518 * 639(correct answer 4804002) GPT-4o: To compute the product 7518 × 639, we can use the distributive property, breaking down 639: 639 = 600 + 30 + 9 So, 7518 × 639 = 7518 × (600 + 30 + 9) = 7518 × 600 + 7518 × 30 + 7518 × 9 Now we compute each term separately: 1. 7518 × 600 : 7518 × 600 = 7518 × 6 × 100 = 45108 × 100 = 4510800 2. 7518 × 30 : 7518 × 30 = 7518 × 3 × 10 = 22554 × 10 = 225540 3. 7518 × 9 : 7518 × 9 = 67662 Adding these together: 4510800 + 225540 + 67662 = 4803902 Thus, 7518 × 639 = 4803902</p>
<p>Fig 1A shows the evaluation accuracy vs. training examples for arithmetic operations.One can see that it takes hundreds of thousands of training examples for the model to learn how to perform arithmetic operations, with the accuracy growing from 75% with 100K training examples to 94% with 450K examples.We submit each problem to GPT-3.5 (a.k.a.ChatGPT) and GPT-4 using OpenAI's API.As shown in Fig 1A, their accuracies are much lower than our trained model.</p>
<p>4.1), and fine-tunes it on word problem training set.(2) Directly fine-tuning the open-source pre-trained Pythia-6.9bmodel on the same word problem training set.The results are shown in Fig 1B.We can see that when the model has acquired "arithmetic skills", it achieves 70% accuracy with only 100 training examples, and achieves almost 90% accuracy with less than 1,000 examples.In contrast, the directly fine-tuned model only has about 10% accuracy after one epoch of training, even the training data contains the full procedure of each arithmetic operation as described in Section 3.1.</p>
<p>Fig 1 .
1
Fig 1.The effectiveness of Scaffolding Learning in applying arithmetic skills to word problems.The error bars represent standard deviations.https://doi.org/10.1371/journal.pone.0310409.g001</p>
<p>3 .
3
We first fine-tune a Pythia-6.9bmodel to compute first derivatives on a training set of 17,605 examples, and evaluate it on a test set of 1,784 examples.As shown in Fig 2A, the model achieves an accuracy of 85% with 125K training examples.We then apply Scaffolding Learning, i.e., fine-tuning the above model to compute second derivatives on a training set of 10,000 second derivative examples generated by (Saxton et al. 2019).Again we compare it with directly fine-tuning Pythia model, on an test set of 1,081 examples.As shown in Fig 2B, Scaffolding Learning achieves higher accuracy after each number of training examples.To achieve about 85% of accuracy, it only requires 1,000 examples for Scaffolding Learning, and it takes 2,000 examples for direct training.This is a much smaller gap compared with arithmetic operations and word problems (in Fig 1B).This is probably because Pythia model has been trained on math dataset which includes polynomials and derivatives.</p>
<p>Fig 2 .
2
Fig 2. The effectiveness of Scaffolding Learning in applying the skill of computing first derivatives to compute second derivatives.The error bars represent standard deviations.https://doi.org/10.1371/journal.pone.0310409.g002</p>
<p>PLOS ONE | https://doi.org/10.1371/journal.pone.0310409September 20, 2024<br />
The datasets generated and/or analysed during the current study are available in the github repository at https:// github.com/xiaoxin-yin/automath/blob/main/misc/scaffold-learning-data.zip.The author(s) received no specific funding for this work.Author ContributionsConceptualization: Xiaoxin Yin.
Deep Neural Solver for Math Word Problems. Yan Wang, Shuming Shi, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagenAssociation for Computer Linguistics2017</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, Weston J , Proceedings of ICML'09. ICML'092009</p>
<p>Sebe Nicu: Curriculum Learning: A Survey. Soviany Petru, Ionescu Radu Tudor, Rota Paolo, 10.1007/s11263-022-01611-xInternational Journal of Computer Vision. 13062022</p>
<p>Training region-based object detectors with online hard example mining. A Shrivastava, A Gupta, R Girshick, Proceedings of CVPR'16. CVPR'162016</p>
<p>Active self-paced learning for cost-effective and progressive face identification. L Lin, K Wang, D Meng, W Zuo, L Zhang, 10.1109/TPAMI.2017.265245928092522IEEE Transactions on Pattern Analysis and Machine Intelligence. 4012017</p>
<p>Norm-based curriculum learning for neural machine translation. X Liu, H Lai, D F Wong, L S Chao, Proceedings of ACL'20. ACL'202020</p>
<p>Zelnik-Manor Lihi: ImageNet-21K Pretraining for the Masses. Ridnik Tal, Ben-Baruch Emanuel, Noy Asaf, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1. the Neural Information Processing Systems Track on Datasets and Benchmarks 12021</p>
<p>Classification Problem Solving. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushma, Proceedings of the Fourth National Conference on Artificial Intelligence. the Fourth National Conference on Artificial IntelligenceAustin2014</p>
<p>Phil Blunsom: Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. Wang Ling, Dani Yogatam, Chris Dyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouverAssociation for Computational Linguistics2017</p>
<p>Measuring Mathematical Problem Solving With the MATH Dataset. 28 Jul 2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen Heewoo Jun, Lukasz Kaiser, Matthias Plappert, arXivTraining Verifiers to Solve Math Word Problems. 2016</p>
<p>Guillaume Lample, Charton , Proceedings of the 2020 International Conference on Learning Representations. Virtual Conference. the 2020 International Conference on Learning Representations. Virtual Conference2020Franc ¸ois: Deep Learning for Symbolic Mathematics</p>
<p>Deep Symbolic Regression: Recovering Mathematical Expressions from Data via Riskseeking Policy Gradients. K Brenden, Mikel Petersen, T Landajuela, Claudio P Nathan Mundhenk, Soo K Santiago, Joanne T Kim, Kim, Proceedings of the 2021 International Conference on Learning Representations. Virtual Conference. the 2021 International Conference on Learning Representations. Virtual Conference2020</p>
<p>George Em Karniadakis: DeepXDE: A deep learning library for solving differential equations. Lu Lu, Meng Xuhui, Mao Zhiping, 10.1137/19M1274067SIAM Review. 2021</p>
<p>Chain-of-thought Prompting Elicits Reasoning in Large Language Models. Wei Jason, Wang Xuezhi, Schuurmans Dale, Bosma Maarten, Xia Fei, Chi Ed, Proceedings of the 2022 Conference on Neural Information Processing Systems. the 2022 Conference on Neural Information Processing SystemsNew Orleans2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Yao Shunyu, Yu Dian, Zhao Jeffrey, Shafran Izhak, Griffiths Tom, Cao Yuan, Advances in Neural Information Processing Systems. 362024</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, Proceedings of the 2023 International Conference on Learning Representations. the 2023 International Conference on Learning RepresentationsKigali2023</p>
<p>Text and Patterns: For Effective Chain of Thought. Aman Madaan, Amir Yazdanbakhsh, 2022It Takes Two to Tango. arXiv</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterMinneapolisAssociation for Computer Linguistics2019</p>
<p>Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. Biderman Stella, Schoelkopf Hailey, Anthony Quentin, Bradley Herbie, O'brien Kyle, Hallahan Eric, Proceedings of Machine Learning Research. Machine Learning ResearchPMLR2023</p>
<p>. Llm Leaderboard, 28 Jul 2023</p>
<p>Neural Machine Translation of Rare Words with Subword Units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlinAssociation for Computer Linguistics20161</p>
<p>Are NLP Models really able to Solve Simple Math Word Problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computer Linguistics2021</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, arXivPushmeet Kohli: Analysing Mathematical Reasoning Abilities of Neural Models. 2019</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, arXiv:2101.00027The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020</p>
<p>Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. arXiv. 2022</p>
<p>Yuxiong He: ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, International Conference for High Performance Computing, Networking, Storage and Analysis. DenverIEEE2019</p>            </div>
        </div>

    </div>
</body>
</html>
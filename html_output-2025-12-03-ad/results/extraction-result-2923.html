<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2923 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2923</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2923</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-279118505</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.01839v2.pdf" target="_blank">Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research</a></p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) transition from static tools to fully agentic systems, their potential for transforming social science research has become increasingly evident. This paper introduces a structured framework for understanding the diverse applications of LLM-based agents, ranging from simple data processors to complex, multi-agent systems capable of simulating emergent social dynamics. By mapping this developmental continuum across six levels, the paper clarifies the technical and methodological boundaries between different agentic architectures, providing a comprehensive overview of current capabilities and future potential. It highlights how lower-tier systems streamline conventional tasks like text classification and data annotation, while higher-tier systems enable novel forms of inquiry, including the study of group dynamics, norm formation, and large-scale social processes. However, these advancements also introduce significant challenges, including issues of reproducibility, ethical oversight, and the risk of emergent biases. The paper critically examines these concerns, emphasizing the need for robust validation protocols, interdisciplinary collaboration, and standardized evaluation metrics. It argues that while LLM-based agents hold transformative potential for the social sciences, realizing this promise will require careful, context-sensitive deployment and ongoing methodological refinement. The paper concludes with a call for future research that balances technical innovation with ethical responsibility, encouraging the development of agentic systems that not only replicate but also extend the frontiers of social science, offering new insights into the complexities of human behavior.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2923.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2923.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agent platform (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform of LLM-driven agents in a 2D simulated environment where agents maintain memory streams, plan daily routines, converse, and exhibit emergent social behaviors such as information diffusion and organizing events.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents are LLM-powered entities that maintain persistent memory streams and personas, plan multi-step routines, observe and act within a 2D environment, and interact via natural language; memory informs planning and social interactions to produce emergent behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Generative Agents 2D interactive environment</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A small-scale 2D simulated environment where agents autonomously plan daily activities, converse with each other and the environment, spread information, form groups, and perform social actions (e.g., host events, hold elections), emphasizing long-range social coordination rather than single-step text puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / persistent memory streams</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Agents maintain persistent memory streams recording past observations, events, conversations and notable facts about other agents (persona-relevant entries); the paper references implementations using vector databases, prompt histories, or structured knowledge graphs to persist and index memories across interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicitly described as recency- and relevance-weighted retrieval (semantic similarity / relevance scoring over memory entries using vector indices or prompt-history lookup).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past observations, conversational exchanges, notable events (who met whom, commitments, social facts), agent-specific persona details and facts used for planning and social inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Persistent memory streams enable coherent personas, allow agents to recall prior interactions and produce emergent social dynamics (information diffusion, cooperative gatherings, rudimentary political behavior) that would not arise from stateless agents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper notes general limitations for agentic systems including struggles with uncertainty and asymmetric information; specific memory failure modes (capacity limits, retrieval errors) are not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2923.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2923.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAMEL / Inception prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAMEL framework and inception prompting (Li et al., 2023a; Xu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent prompting architecture that embeds structured roles and shared goals in agent context (inception prompting) to facilitate coherent, multi-turn collaboration in communication games such as Werewolf.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CAMEL / inception prompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting and orchestration approach where each agent is given a structured role, task specification, and shared goals embedded in context (inception prompts); agents iteratively exchange messages under this structured context to coordinate multi-turn behavior and avoid role flipping or conversational loops.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Werewolf (communication / social deduction game)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A multiplayer communication and social-deduction game requiring agents to infer others' intents, maintain roles and hidden states across multiple turns, form trust, and coordinate deceptive or cooperative strategies through language.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>session-based prompt memory / working memory (role & conversation context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory is realized primarily as structured prompt context and message history: role descriptions, shared goals, and iterative message logs are embedded in the agent context to preserve role-consistent behavior across rounds; the paper links this to 'inception prompting' rather than an external persistent memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval is via inclusion in prompt context and recency in message history (recency/relevance-based within prompt windows); the architecture reduces role flipping by keeping role/state in the agent's context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Role assignment, shared goals, recent dialogue turns and inferred beliefs about other players (contained in prompt context / conversation history).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported qualitatively to improve task completion efficiency and coherence in multi-turn collaboration; no numerical success rates given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Embedding role and goal information directly into agent context (inception prompting) helps maintain coherent multi-turn behavior, prevents role flipping and conversational loops, and increases efficiency in collaborative/communication games such as Werewolf.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No quantitative limits reported here; general concerns include susceptibility to convergence on homogeneous behavior and limits imposed by prompt window size and stochastic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2923.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2923.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PsychoGAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PsychoGAT: Interactive Fiction Games with LLM Agents (Yang et al., 2024a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses multi-agent LLMs to transform self-report psychological scales into interactive fiction games, leveraging role-playing, memory, and dynamic dialogue to improve psychometric assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PsychoGAT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-agent setup where LLM agents play roles within interactive fiction to elicit and assess psychological constructs; agents retain role-consistent memory and use dynamic dialogue to adapt assessments over time, combining role-play and memory to produce richer measures than static rating scales.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Interactive fiction assessment games (custom assessment games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Turn-based interactive fiction games designed to surface psychological constructs (depression, personality, cognitive distortions) via naturalistic dialogue and role-play, rather than conventional fixed-response questionnaires.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persona and dialogue memory / persistent session memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Agents integrate role-playing memory and dynamic dialogue history to maintain coherent assessments; the paper describes using memory to preserve persona and prior exchanges across the game session, drawing on the general architectures discussed (prompt histories / persistent memory streams / possible vector stores).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval via conversation history and persona entries embedded in prompts; implied use of recency and relevance to select salient prior interactions for ongoing dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Role/persona attributes, prior responses, dialogue context, prior decisions or disclosures made during the interactive fiction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as achieving strong psychometric validity and improved engagement relative to static rating scales, but no numeric performance metrics are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Integrating role-playing and session memory into interactive assessment games yields richer, context-sensitive measures with strong psychometric validity and higher participant engagement compared to static scales.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Quantitative memory failure modes not provided here; general risks include representational bias and lack of standardized evaluation metrics for such adaptive assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 2)</em></li>
                <li>CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society <em>(Rating: 2)</em></li>
                <li>A Survey on the Memory Mechanism of Large Language Model based Agents <em>(Rating: 2)</em></li>
                <li>Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View <em>(Rating: 1)</em></li>
                <li>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf <em>(Rating: 1)</em></li>
                <li>GenSim: A General Social Simulation Platform with Large Language Model based Agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2923",
    "paper_id": "paper-279118505",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agent platform (Park et al., 2023)",
            "brief_description": "A platform of LLM-driven agents in a 2D simulated environment where agents maintain memory streams, plan daily routines, converse, and exhibit emergent social behaviors such as information diffusion and organizing events.",
            "citation_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al., 2023)",
            "agent_description": "Agents are LLM-powered entities that maintain persistent memory streams and personas, plan multi-step routines, observe and act within a 2D environment, and interact via natural language; memory informs planning and social interactions to produce emergent behaviors.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Generative Agents 2D interactive environment",
            "text_game_description": "A small-scale 2D simulated environment where agents autonomously plan daily activities, converse with each other and the environment, spread information, form groups, and perform social actions (e.g., host events, hold elections), emphasizing long-range social coordination rather than single-step text puzzles.",
            "uses_memory": true,
            "memory_type": "episodic / persistent memory streams",
            "memory_architecture": "Agents maintain persistent memory streams recording past observations, events, conversations and notable facts about other agents (persona-relevant entries); the paper references implementations using vector databases, prompt histories, or structured knowledge graphs to persist and index memories across interactions.",
            "memory_retrieval_mechanism": "Implicitly described as recency- and relevance-weighted retrieval (semantic similarity / relevance scoring over memory entries using vector indices or prompt-history lookup).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Past observations, conversational exchanges, notable events (who met whom, commitments, social facts), agent-specific persona details and facts used for planning and social inference.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Persistent memory streams enable coherent personas, allow agents to recall prior interactions and produce emergent social dynamics (information diffusion, cooperative gatherings, rudimentary political behavior) that would not arise from stateless agents.",
            "memory_limitations": "Paper notes general limitations for agentic systems including struggles with uncertainty and asymmetric information; specific memory failure modes (capacity limits, retrieval errors) are not quantified in this paper.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2923.0",
            "source_info": {
                "paper_title": "Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "CAMEL / Inception prompting",
            "name_full": "CAMEL framework and inception prompting (Li et al., 2023a; Xu et al., 2024)",
            "brief_description": "A multi-agent prompting architecture that embeds structured roles and shared goals in agent context (inception prompting) to facilitate coherent, multi-turn collaboration in communication games such as Werewolf.",
            "citation_title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society",
            "mention_or_use": "mention",
            "agent_name": "CAMEL / inception prompting",
            "agent_description": "A prompting and orchestration approach where each agent is given a structured role, task specification, and shared goals embedded in context (inception prompts); agents iteratively exchange messages under this structured context to coordinate multi-turn behavior and avoid role flipping or conversational loops.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Werewolf (communication / social deduction game)",
            "text_game_description": "A multiplayer communication and social-deduction game requiring agents to infer others' intents, maintain roles and hidden states across multiple turns, form trust, and coordinate deceptive or cooperative strategies through language.",
            "uses_memory": true,
            "memory_type": "session-based prompt memory / working memory (role & conversation context)",
            "memory_architecture": "Memory is realized primarily as structured prompt context and message history: role descriptions, shared goals, and iterative message logs are embedded in the agent context to preserve role-consistent behavior across rounds; the paper links this to 'inception prompting' rather than an external persistent memory store.",
            "memory_retrieval_mechanism": "Retrieval is via inclusion in prompt context and recency in message history (recency/relevance-based within prompt windows); the architecture reduces role flipping by keeping role/state in the agent's context.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Role assignment, shared goals, recent dialogue turns and inferred beliefs about other players (contained in prompt context / conversation history).",
            "performance_with_memory": "Reported qualitatively to improve task completion efficiency and coherence in multi-turn collaboration; no numerical success rates given in this paper.",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Embedding role and goal information directly into agent context (inception prompting) helps maintain coherent multi-turn behavior, prevents role flipping and conversational loops, and increases efficiency in collaborative/communication games such as Werewolf.",
            "memory_limitations": "No quantitative limits reported here; general concerns include susceptibility to convergence on homogeneous behavior and limits imposed by prompt window size and stochastic generation.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2923.1",
            "source_info": {
                "paper_title": "Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "PsychoGAT",
            "name_full": "PsychoGAT: Interactive Fiction Games with LLM Agents (Yang et al., 2024a)",
            "brief_description": "A framework that uses multi-agent LLMs to transform self-report psychological scales into interactive fiction games, leveraging role-playing, memory, and dynamic dialogue to improve psychometric assessment.",
            "citation_title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents",
            "mention_or_use": "mention",
            "agent_name": "PsychoGAT",
            "agent_description": "A multi-agent setup where LLM agents play roles within interactive fiction to elicit and assess psychological constructs; agents retain role-consistent memory and use dynamic dialogue to adapt assessments over time, combining role-play and memory to produce richer measures than static rating scales.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Interactive fiction assessment games (custom assessment games)",
            "text_game_description": "Turn-based interactive fiction games designed to surface psychological constructs (depression, personality, cognitive distortions) via naturalistic dialogue and role-play, rather than conventional fixed-response questionnaires.",
            "uses_memory": true,
            "memory_type": "persona and dialogue memory / persistent session memory",
            "memory_architecture": "Agents integrate role-playing memory and dynamic dialogue history to maintain coherent assessments; the paper describes using memory to preserve persona and prior exchanges across the game session, drawing on the general architectures discussed (prompt histories / persistent memory streams / possible vector stores).",
            "memory_retrieval_mechanism": "Retrieval via conversation history and persona entries embedded in prompts; implied use of recency and relevance to select salient prior interactions for ongoing dialogue.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Role/persona attributes, prior responses, dialogue context, prior decisions or disclosures made during the interactive fiction.",
            "performance_with_memory": "Reported as achieving strong psychometric validity and improved engagement relative to static rating scales, but no numeric performance metrics are provided in this paper.",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Integrating role-playing and session memory into interactive assessment games yields richer, context-sensitive measures with strong psychometric validity and higher participant engagement compared to static scales.",
            "memory_limitations": "Quantitative memory failure modes not provided here; general risks include representational bias and lack of standardized evaluation metrics for such adaptive assessments.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2923.2",
            "source_info": {
                "paper_title": "Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society",
            "rating": 2,
            "sanitized_title": "camel_communicative_agents_for_mind_exploration_of_large_language_model_society"
        },
        {
            "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
            "rating": 2,
            "sanitized_title": "a_survey_on_the_memory_mechanism_of_large_language_model_based_agents"
        },
        {
            "paper_title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
            "rating": 1,
            "sanitized_title": "exploring_collaboration_mechanisms_for_llm_agents_a_social_psychology_view"
        },
        {
            "paper_title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf",
            "rating": 1,
            "sanitized_title": "exploring_large_language_models_for_communication_games_an_empirical_study_on_werewolf"
        },
        {
            "paper_title": "GenSim: A General Social Simulation Platform with Large Language Model based Agents",
            "rating": 1,
            "sanitized_title": "gensim_a_general_social_simulation_platform_with_large_language_model_based_agents"
        }
    ],
    "cost": 0.01499825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research
23 Oct 2025</p>
<p>Jennifer Haase jennifer.haase@hu-berlin.de 
Weizenbaum Institute and HU
BerlinGermany</p>
<p>Sebastian Pokutta pokutta@zib.de 
TU Berlin
Zuse Institute Berlin
BerlinGermany</p>
<p>Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research
23 Oct 2025ED4499FFEAF69EB2906A84C1E61BE159arXiv:2506.01839v2[cs.MA]Large Language Models (LLMs)Multi-Agent SystemsAgentic AISocial Science SimulationEmergent BehaviorComputational Social ScienceInterdisciplinary ResearchMethodological InnovationSynthetic DataHuman-AI Collaboration
As large language models (LLMs) transition from static tools to fully agentic systems, their potential for transforming social science research has become increasingly evident.This paper introduces a structured framework for understanding the diverse applications of LLM-based agents, ranging from simple data processors to complex, multi-agent systems capable of simulating emergent social dynamics.By mapping this developmental continuum across six levels, the paper clarifies the technical and methodological boundaries between different agentic architectures, providing a comprehensive overview of current capabilities and future potential.It highlights how lower-tier systems streamline conventional tasks like text classification and data annotation, while higher-tier systems enable novel forms of inquiry, including the study of group dynamics, norm formation, and large-scale social processes.However, these advancements also introduce significant challenges, including issues of reproducibility, ethical oversight, and the risk of emergent biases.The paper critically examines these concerns, emphasizing the need for robust validation protocols, interdisciplinary collaboration, and standardized evaluation metrics.It argues that while LLM-based agents hold transformative potential for the social sciences, realizing this promise will require careful, context-sensitive deployment and ongoing methodological refinement.The paper concludes with a call for future research that balances technical innovation with ethical responsibility, encouraging the development of agentic systems that not only replicate but also extend the frontiers of social science, offering new insights into the complexities of human behavior.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have become foundational tools in contemporary social science research, widely applied to tasks such as survey response generation, qualitative analysis, coding, and summarization [Kantor, 2024, Hardy et al., 2023, Demszky et al., 2023].These applications offer notable methodological benefits by replicating specific cognitive functions traditionally performed by human researchers or participants [Ke et al., 2024, Manning et al., 2024].However, they remain largely confined to static and reactive uses, where models act as sophisticated text processors without memory, autonomy, or interactive agency-a limitation that echoes longstanding critiques of agentless computational modeling [Epstein, 1999[Epstein, , 2012]].</p>
<p>We argue that the next major step in computational social science lies in moving beyond these static applications toward a systematic understanding of LLM-based agentic systems [Feng et al., 2025, Grossmann et al., 2023].These systems differ fundamentally from conventional LLM use: they incorporate memory, goal-directed behavior, environmental interaction, and in some cases adaptive learning.Embedded within such architectures, LLMs do not merely mimic isolated cognitive acts-they function as interactive social agents capable of participating in simulations, decision-making, and complex group dynamics [Piao et al., 2025, Wang et al., 2024, Lu et al., 2024a].Building on this foundation, we propose a six-tier framework that captures the increasing complexity and autonomy of LLM-based systems in social science research.This framework spans a conceptual continuum from static tools to fully agentic systems and is structured by functional thresholds-such as memory integration, autonomy, coordination, and learning-that define the degree of agentic behavior.At the foundational level, LLM-as-Tool systems (Level 0) operate as stateless text generators, producing contextually appropriate responses without memory, autonomy, or strategic reasoning.These systems rely solely on prompt inputs to generate outputs, lacking the capacity for long-term planning or adaptive behavior.Despite their limitations, they have proven effective for tasks like content generation, data classification, and text summarization, where immediate, context-free processing suffices [Thapa et al., 2025, Haase and Thim, 2020, Karjus, 2025, Ziems et al., 2024].Moving to LLM-as-Role systems (Level 1), these architectures introduce basic state retention, enabling agents to simulate consistent personas or role-based behavior across multiple interactions.This level adds a layer of memory, allowing agents to maintain contextual awareness over short dialogue sequences, supporting applications like customer service, personalized tutoring, and basic psychological profiling [Wang et al., 2025d, Sun et al., 2024].However, these systems still lack the goal-directed autonomy required for independent decision-making.At Agent-like LLM (Level 2), systems gain more structured autonomy, integrating task-oriented reasoning and memory architectures that enable multi-step planning.These agents are capable of decomposing complex tasks, setting intermediate goals, and adjusting their behavior based on task outcomes [Horton, 2023, Argyle et al., 2023].They bridge the gap between static tools and fully autonomous agents, simulating more sophisticated human-like behavior, including context-driven decision-making and reflective judgment.The next tier, LLM-based Agents (Level 3), marks a significant step toward actual agency.These systems integrate comprehensive memory, environment interfaces, and strategic coordination mechanisms.They exhibit proactive behavior, leveraging long-term memory and environmental feedback to refine their actions over time.This level represents a crucial threshold in agentic complexity, where agents transition from passive responders to active decision-makers capable of planning, coordination, and strategic interaction [Manning et al., 2024, Liu et al., 2024].Multi-Agent Systems (Level 4) extend this capability further, integrating multiple, interacting agents within a shared environment.These systems replicate complex social processes, such as negotiation, coalition-building, and organizational decision-making [Zhang et al., 2024b, Feng et al., 2025].They support distributed, collaborative problem-solving, with agents dynamically coordinating their actions based on shared goals and situational awareness.At this stage, agents exhibit emergent group behaviors, reflecting foundational principles from collective intelligence and network theory.Finally, at the highest level, Complex Adaptive Systems (Level 5) encompass large-scale, emergent social dynamics.These architectures consist of numerous interacting agents, each equipped with memory, autonomy, and adaptive learning capabilities.Unlike lower tiers, these systems are characterized by self-organization, norm formation, and systemic adaptation, capturing the unpredictable, emergent properties of real-world social networks [Piao et al., 2025, Wang et al., 2025b].They provide powerful platforms for modeling phenomena like cultural evolution, institutional change, opinion-dynamics and large-scale social movements, pushing the boundaries of agentic LLM applications in computational social science.</p>
<p>These thresholds align with the OODA loop (Observe, Orient, Decide, Act; Boyd 2018, Osinga 2007), providing a decision-theoretic lens to understand how LLM-based agents perceive, reason, and act in dynamic environments.This progression across the six tiers reflects the foundational structure of social science, moving from the study of individual cognition and behavior to the dynamics of small groups, and ultimately to the complex interactions of entire societies and populations.Grounding this structure in functional thresholds and aligning it with the OODA loop enables us to classify existing systems and clarify the developmental logic behind building LLM-based simulations.Through this lens, we present empirical examples and conceptual distinctions that show how such systems can explore social behavior, generate synthetic data, and simulate interactions at scale and ethical boundaries unattainable with human participants alone.By moving beyond narrow prompt-response paradigms toward dynamic, interactive, and adaptive architectures, this paper aims to lay a possible conceptual foundation for the next generation of computational social science.</p>
<p>2 From Tools to Societies: A Framework of LLM-Based Agentic Systems</p>
<p>The integration of LLMs into dynamic, agentic systems presents substantial potential for advancing computational social science.LLMs such as GPT-4 are already widely used for tasks like text generation, qualitative coding, and simulating survey responses [Gao et al., 2024, Wang et al., 2024].However, these applications typically treat  1).They exhibit autonomous reasoning, context-aware memory, goal-directed behavior, and adaptive interaction with complex environments [Epstein, 1999, 2012, Duéñez-Guzmán et al., 2023].These systems maintain persistent internal states and memory, enabling them to draw on prior experiences for informed action [Huang et al., 2024a].Their autonomy is often grounded in predefined or evolving objectives, giving rise to dynamic, self-regulated responses across unfolding conditions [Feng et al., 2025, Park et al., 2023].Between these two extremes lies a conceptual middle ground: LLM-based, agent-like systems.These architectures exceed simple prompt-response configurations by integrating limited capabilities such as session memory, elementary tool use, or task-specific control logic.However, they fall short of full agentic autonomy and adaptability, lacking both sophisticated environmental interaction and long-term planning.The importance of this intermediate stage lies in its role as a stepping stone toward agentic complexity.It reflects a developmental transition in system design that is increasingly visible in practical applications.Recent work also underscores the potential of LLMs to serve as cognitive models, capturing aspects of human reasoning and aligning with foundational theories in cognitive science [Niu et al., 2024].This positions LLMs not only as functional tools but as plausible proxies for human-like agency, reinforcing their value in simulating social phenomena.</p>
<p>Functional Thresholds for LLM-based Agents</p>
<p>To systematically distinguish between these levels of system autonomy and interaction, we propose a continuum structured by a series of functional thresholds.These thresholds-memory integration, autonomy, planning and coordination, and adaptive learning-serve as markers of increasing agentic potential.Importantly, each threshold can be mapped to a deeper level of participation in the OODA loop (Observe, Orient, Decide, Act), a well-established model for adaptive decision-making in dynamic environments [Boyd, 2018, Osinga, 2007].Initially developed by military strategist John Boyd, the OODA loop describes how intelligent agents continuously engage with their environment through a cyclical process: they observe their surroundings, orient themselves by interpreting new information in context, decide on a course of action, and then act-feeding the results back into the next observational phase.This loop is not a linear sequence but a recursive, feedback-driven mechanism central to real-time sense-making, learning, and adaptation.In the context of LLM-based systems, the OODA loop provides a functional scaffold for understanding different levels of agentic behavior (cf. Figure 1).Static LLMs may only participate in the final "act" phase, producing text in response to prompts based on the LLM's innate capabilities.However, as these systems acquire new capabilities, such as memory, goal orientation, planning, and adaptive learning, they progressively engage in more complex phases of the loop-each level adds an OODA element.They begin to "observe" the inputs from the perspective of the given profile, which they can retain thanks to the memory (cf. Figure 1, LLM-as-Role).Thus, they can maintain a coherent persona, including personality or relatively stable opinions and values [Wang et al., 2025d, Huang et al., 2024a].</p>
<p>When adding a planning tool, LLMs can start to orient the output and "behavior" by updating internal states, deciding among competing options, and acting based on multi-step strategies-such tools are agent-like LLM.To become fully agentic, they progressively gain the capacity to use tools for interactive, feedback-driven behaviors (cf. Figure 1, Fully agentic LLM).When such agentic LLMs are chained or combined to more complex systems, they add "learning" from other agents' output as an essential feature of societal, dynamic behavior.Interacting and observing other agents can thus influence the agent's behavior and, especially for Complex Adaptive Systems, agents' interaction lead to adaptive changes in their profiles, just like opinions, and individual behavior changes due to other social actors' opinions or behavior, mimicking fully dynamic social interactions [Park, 2024].The difference between Level 4 and 5-as not visible in Figure 1-is the complexity of agentic systems on Level 5, which leads to emergent behavior based on the agents' interactions.</p>
<p>This spectrum is not merely theoretical.A concrete example can help illustrate the functional distinctions: In an email triage task, a Level 0 system (LLM-as-Tool) might classify or summarize individual messages on demand but lacks any contextual awareness or memory across turns.A Level 1 system (LLM-as-Role) is prompted to simulate a consistent persona-say, an executive assistant-with stylized preferences and tone, responding in a role-consistent way but without memory beyond the current prompt.A Level 2 Agent-like LLM might retain state across sessions, learn user preferences over time, and flag or draft replies based on simple prioritization rules.By Level 3, a Fully agentic LLM could autonomously monitor the email stream, integrate sender context, resolve calendar conflicts, and initiate follow-ups-without requiring user prompting.At Level 4, multiple such agents could coordinate inbox management for different roles within an organization (e.g., assistant, PR, legal), negotiating task allocation and sharing knowledge.Finally, at Level 5, entire ecosystems of agents could simulate broader organizational communication flows, emergent work norms, or institutional adaptation under varying external pressures.</p>
<p>Architectural Components for LLM-based Agents</p>
<p>Agentic LLM systems typically incorporate a set of architectural components that enable them to operate beyond reactive text generation.These components evolve across the six levels described in Table 1, supporting increasingly autonomous, interactive, and emergent behavior.</p>
<p>At the lower levels (0-2), systems range from stateless LLM tools to agent-like entities capable of maintaining session memory and executing goal-directed tasks.Here, architectural requirements include prompt engineering, session-based memory buffers, and basic control logic.At Level 3, fully agentic LLMs incorporate persistent memory stores, strategic planning capabilities, environmental interfaces, and tool-use APIs, allowing them to perceive and interact with digital environments.These capabilities support full engagement with the OODA loop-enabling the agent to observe, orient, decide, and act in context-sensitive ways.At Level 4, multi-agent systems require additional infrastructure for inter-agent communication, task division, negotiation, and shared goal alignment.Architectures must support parallelism, role differentiation, and message-passing protocols to coordinate multiple agents within a shared task environment.At Level 5, complex adaptive systems add yet another layer: mechanisms for learning, feedback loops, and emergent behavior.These require architectures capable of tracking population-level patterns, adapting strategies over time, and supporting norm formation or diffusion through decentralized interactions.</p>
<p>The construction of these systems typically follows modular design patterns widely discussed in recent system surveys [Wang et al., 2024, Gao et al., 2024].A central LLM often serves as the cognitive core, orchestrating inputs and outputs across supporting modules; this is not strictly necessary, especially in fully-decentralized setups.Tool-use APIs extend the LLM's reach beyond language (and often vision in multi-modal LLMs), enabling direct manipulation of software environments (e.g., web browsing, data querying, programming, or GUI control).Memory is implemented through vector databases, prompt histories, or structured knowledge graphs, enabling contextual continuity across interactions [Zhang et al., 2024c].Planning capabilities are scaffolded by frameworks such as ReAct [Yao et al., 2023], AutoGPT [Firat and Kuleli, 2023], and LangChain [Kok-Shun et al., 2023], which provide structures for multi-step reasoning, goal decomposition, and iterative feedback integration [Huang et al., 2024b].At higher levels, orchestration layers coordinate perception, planning, execution, and learning cycles across multiple agents, supporting emergent behavior and adaptive simulation within complex social environments.</p>
<p>As LLM systems evolve along this continuum, they acquire the ability not only to execute predefined tasks but also to generate strategies, evaluate outcomes, and modify behavior in response to environmental changes.This is particularly evident at higher levels of the continuum-Levels 4 and 5-where multiple agentic entities operate in coordinated fashion.In such multi-agent configurations, coordination mechanisms become essential: agents must communicate, negotiate, align goals, and resolve conflicts.According to Wang et al. [2024], these coordination mechanisms can take various structural forms, ranging from centralized planning (via a root controller) to fully decentralized interaction paradigms where agents dynamically exchange messages, assign subtasks, or vote on decisions.Some systems even adopt federated structures, mirroring principles of institutional governance and self-organization.Thus, these multi-agent configurations mirror established principles in distributed cognition, where knowledge and problem-solving are not confined to an individual but are distributed across multiple interacting agents and artifacts [Zhang and Patel, 2006].Moreover, the dynamics observed in these systems can and have been studied via complexity theory and studies of collective intelligence, which emphasize emergence, feedback loops, and adaptive restructuring [Epstein, 2012, Duéñez-Guzmán et al., 2023].Architecturally and behaviorally, LLM-based agentic systems are increasingly capable of supporting simulations that model not only individual reasoning but also organizational behavior, norm evolution, and systemic adaptation.</p>
<p>By clarifying these developmental stages and their architectural dependencies-each aligned with OODA loop phases-the framework that we propose contributes to a more nuanced understanding of agentic potential in LLM-based systems.These distinctions are foundational not only for guiding technical implementation but also for shaping how researchers theorize, validate, and ethically engage with increasingly autonomous computational agents in the social sciences.</p>
<p>Relevance for Social Sciences</p>
<p>Recent scholarship has begun to systematically explore the implications of LLMs for the social sciences, both as methodological tools and as objects of inquiry.A foundational entry point is provided by Valdenegro [2023], who demystifies LLM architectures for social scientists and emphasizes the importance of epistemological caution.LLMs, he argues, are not reasoning agents but pattern-completion engines, trained on extensive textual corpora to predict plausible next tokens.This framing positions LLMs as probabilistic simulators rather than interpretable decision-makers, underscoring the need for methodological transparency and restraint in their use.However, Valdenegro [2023] also highlights the practical affordances of LLMs in the social sciences: they enable low-cost data augmentation, scalable qualitative analysis, and novel forms of simulation-based inquiry.This dual potential, as both powerful analytical tools and ethically complex objects of study, has accelerated the integration of LLMs into social science research.In parallel, scholars like Filippas et al. [2024] and Aher et al. [2023] emphasize that LLMs can serve as proxies for human reasoning in computational experiments.They argue that LLMs, despite their non-human cognitive architecture, capture many aspects of human reasoning due to their extensive training on vast, diverse text corpora.This implicit computational model of human language and behavior allows LLMs to approximate social cognition, making them powerful stand-ins for human participants in certain experimental contexts.As Rossi et al. [2024] notes, "since LLMs are trained on massive amounts of online data, the data will be able to capture fine details of the social system and of the several populations in it" (p.153), positioning them as potentially invaluable tools for studying human social dynamics at scale.</p>
<p>Lvl</p>
<p>The rapid evolution of LLM-based agentic systems-ranging from stateless text generators (Level 0) to fully autonomous, self-organizing systems (Level 5)-has already begun to reshape how social phenomena can be modeled, simulated, and interpreted.These systems are not merely replacing traditional computational methods but are enabling qualitatively new forms of socio-technical modeling.By embedding memory, task-specific reasoning, and adaptive behavior into LLMs, researchers have created multi-agent systems capable of maintaining state, acting autonomously, coordinating with other agents, and interacting with digital environments [Wang et al., 2024].This progression reflects a fundamental shift in how social phenomena can be studied: from isolated, context-free text processing to dynamic, context-rich simulations of human behavior and interaction.</p>
<p>Crucially, the examples provided in this paper, summarized in Table 2, are not intended as an exhaustive or definitive list.Instead, they illustrate the underlying principles of each agentic tier, serving as both proofof-concept demonstrations and inspiration for further empirical work.As the field evolves, we anticipate that many more examples will emerge, reflecting the ongoing innovation in LLM-based social modeling.To provide a clearer understanding of this progression, the following subsections explicitly describe each tier, moving from the foundational LLM-as-Tool systems, through increasingly complex configurations that incorporate memory, autonomy, and multi-agent interactions, up to fully adaptive, self-organizing systems capable of simulating societal dynamics.These examples not only validate the proposed framework but also reveal the accelerating relevance of LLM agents for the design and interpretation of computational social science.</p>
<p>Level 0: LLM-as-Tool</p>
<p>At the foundational level, LLMs serve as stateless tools for generating, summarizing, and transforming text.While they do not exhibit agentic properties such as memory, autonomy, or environmental awareness, their capacity to produce high-quality, contextually appropriate language makes them valuable instruments in social science workflows.Most directly, they function as productivity aids: researchers increasingly rely on LLMs to generate survey items, synthesize literature, and create stimulus materials for experiments [Ke et al., 2024, Demszky et al., 2023].Their ability to generate plausible, well-structured text at scale supports not only experimental design but also the development of instructional or exploratory materials in educational and research settings [Thapa et al., 2025].Beyond automation, LLMs are also adopted as cognitive partners, tools for ideation and structured brainstorming.Researchers have begun to use LLMs in early-stage project scoping, conceptual modeling, and creative hypothesis development.These "thinking companions" can provide useful analogies, suggest alternative framings, or assist in exploring conceptual distinctions [Boers et al., 2025].Even without memory or reasoning capabilities, their probabilistic synthesis of prior language data makes them powerful tools for lateral thinking and divergent idea generation [Haase andHanel, 2023, Haase et al., 2025a].As an example of text generation, the CollabStory framework demonstrates how multiple LLMs can be chained together to co-author fictional narratives collaboratively.This approach reveals critical methodological challenges for authorship attribution and narrative coherence, as the combined outputs of distinct LLMs can exhibit complex, hard-to-disentangle writing patterns.While not truly agentic, these systems stretch the boundaries of what constitutes authorship and collaborative creativity in AI-generated content [Venkatraman et al., 2025].</p>
<p>Another area of impact lies in qualitative coding and text classification.Zero-shot prompting of LLMs has been shown to produce taxonomic labeling and free-form explanations that rival, and occasionally surpass, those of human coders in terms of clarity and interpretability [Ziems et al., 2024].For instance, implicit motive coding, traditionally labor-intensive, can be reliably automated with LLMs, achieving accuracy on par with expert annotators while reducing processing time by over 99% [Nilsson et al., 2025].These advances position LLMs as scalable alternatives for coding open-ended responses or applying theory-based classifications to large textual datasets.LLMs are also becoming central to literature review processes.They help researchers extract, summarize, and organize insights from expansive textual corpora, a task traditionally requiring significant manual effort.In mixed-methods designs, LLMs are increasingly used to support "quantitizing" workflows-that is, converting qualitative insights into structured forms suitable for statistical analysis [Karjus, 2025, Organisciak et al., 2023].In psychology and related disciplines, LLMs now assist in hypothesis generation, experimental design, and methodological instruction [Ke et al., 2024, Demszky et al., 2023].While LLMs are best understood as probabilistic text generators rather than reasoning agents, their integration into research pipelines marks a pivotal shift.They augment the researcher's ideational capacity, reduce manual load, and introduce new possibilities for how inquiry is initiated and iterated in social science.This level, though minimal in agentic complexity, lays the groundwork for more interactive and autonomous systems examined in higher levels of the continuum.</p>
<p>Level 1: LLMs as Role-Taker</p>
<p>At Level 1 of the agentic continuum, LLMs transition from general-purpose tools to entities capable of maintaining pre-defined roles or personas.These systems remain stateless and externally controlled, but are prompted to exhibit consistent behavioral or psychological patterns, such as personality traits, preferences, or affective dispositions.This involves "conditioning" the model through carefully structured prompts or system messages, often supplemented by role descriptions, trait parameters, or simulated contexts.Unlike Level 0 tools, these LLMs are not just language generators; they are simulations of particular kinds of people or profiles, meant to behave coherently across tasks in line with their assigned characteristics.For the social sciences, this opens new opportunities for scalable, repeatable simulations of human-like behavior under controlled conditions.LLMs can serve as synthetic participants in experiments that examine individual differences, group dynamics, or the effects of psychological traits on decision-making.Researchers can explore how simulated agents with distinct personalities respond to the same scenarios, or how demographic conditioning (e.g., gender, political affiliation, or cultural background) influences language-based responses.These persona-driven applications allow for finer control in experimental setups and offer cost-effective ways to model individual-level variability, especially when studying sensitive or ethically difficult issues.</p>
<p>Concrete use cases already demonstrate this potential.A growing body of work focuses on simulating stable personality traits, like the Big Five dimensions.LLMs have been prompted to adopt personality profiles and exhibit trait-consistent behavior with high internal coherence and convergent validity compared to human self-report data [Huang et al., 2024a, Wang et al., 2025d].Similar approaches explore how emotion simulation can influence decision-making: prompting an LLM with affective framing alters its behavioral outputs in line with human emotional responses [Mozikov et al., 2024].Other studies extend role simulation to psychometric modeling.LLMs have been evaluated for their ability to express coherent patterns across psychological dimensions like motivation, affect, and decision-making tendencies, showing potential for systematic, repeatable behavioral outputs in simulated settings [Li et al., 2024b].This opens the door for using LLMs in tasks traditionally reserved for human participants, such as theory testing or applying interventions in experimental psychology and sociology.</p>
<p>In applied domains, LLMs are increasingly used to simulate user profiles and decision-making behavior.For example, persona-based agents have been designed to mimic MovieLens or Amazon-Book users, incorporating memory and emotional modeling to evaluate how personalized recommendations are interpreted or filtered [Zhang et al., 2024a].These user simulators provide insight into the dynamics of algorithmic influence, filter bubbles, and preference formation.However, caution is warranted.Some findings show that even when LLMs are prompted to represent distinct demographic or political groups, the resulting responses often reflect generic or homogenized viewpoints.This is partially attributed to the Reinforcement Learning from Human Feedback (RLHF) process, which tends to smooth out controversial or sensitive positions, thus leading to reduced representational fidelity for underrepresented or marginalized groups [Santurkar et al., 2023, Rossi et al., 2024].Although demographic conditioning (e.g., "random silicon sampling") can generate subgroup-like distributions [Sun et al., 2024], there remains an epistemic and ethical risk in treating these simulations as proxies for real-world populations.</p>
<p>In sum, Level 1 systems simulate role-specific behavior without autonomous goal selection or memory.Their primary affordance lies in modeling consistent, human-like responses for controlled experimental or design settings-especially in psychology, marketing research, and human-AI interaction studies.</p>
<p>Level 2: Agent-like LLM</p>
<p>At Level 2, LLMs move beyond simple role simulation toward more autonomous, structured task performance.These systems exhibit "agent-like behavior"; thus they can decompose tasks, make context-sensitive decisions, and access memory or external tools when necessary.While they lack full autonomy or environment interactivity, they are increasingly used as self-contained agents capable of acting purposefully within constrained scenarios.Architecturally, these LLMs may operate with internal memory buffers, tool integration, and iterative reasoning chains (e.g., chain-of-thought or self-reflection prompts).They can also exhibit planning behaviors, simulate deliberation, and interact with APIs or local documents when embedded in systems with expanded action spaces [Wang et al., 2024].</p>
<p>In the social sciences, these agent-like systems serve a vital function: they can emulate human decision-making and support scalable experimentation without requiring continuous oversight.Instead of merely responding statically, they begin to act as semi-autonomous evaluators, actively interpreting, quantifying, and shaping human-like constructs.Their applications span simulation of behavioral experiments, synthetic survey responses, and even the generation or evaluation of psychological or sociological constructs.</p>
<p>Experiment Replication</p>
<p>One prominent application of agent-like LLMs is the replication of classic experiments.Researchers have shown that these systems can simulate behavioral game-theoretic interactions by adopting specific preferences and strategies, replicating known experimental effects such as altruism or fairness [Filippas et al., 2024, Horton, 2023].In marketing research, LLM agents have replicated over 130 media effect studies with a high degree of correspondence to human results, offering a scalable pathway to validate empirical findings and test generalizability [Yeykelis et al., 2024].Likewise, LLMs have demonstrated strong forecasting capabilities, matching or exceeding expert predictions in social and behavioral science domains [Hewitt et al., 2024, Lippert et al., 2024].</p>
<p>These use cases underscore a key epistemic shift: LLMs are not just tools for supporting human analysis but can serve as experimental subjects or forecasters in their own right, enabling low-cost replication and hypothesis testing.Still, caution remains essential; while models may match behavioral outcomes on average, they often miss interaction effects or contextual nuances that humans intuitively grasp.</p>
<p>Survey Response Simulation</p>
<p>LLMs are also used to simulate human survey responses across various domains-from consumer behavior to political science [Aher et al., 2023, Argyle et al., 2023].Through demographic conditioning or persona steering (cf.Level 1), they can mimic subgroup-specific attitudes and predict, for example, voting behavior [von der Heyde et al., 2023].Yet, while these approaches show promise, limitations persist: LLMs trained with RLHF tend to produce overly sanitized, bias-suppressed outputs, leading to underrepresentation of real-world variance in attitudes or social biases [Santurkar et al., 2023, Tjuatja et al., 2024].This calls into question their validity as stand-ins for marginalized or complex social groups [Rossi et al., 2024].</p>
<p>Other studies focus on building psychologically plausible agents, combining LLMs with stance detection and cognitive architectures to simulate adaptive human behavior within agent-based models, aiming to overcome the abstraction limitations of traditional agent-based models [Mitsopoulos et al., 2023].For example, Yang et al. [2024b] introduce LLM-Measure, a prompting framework that enables LLMs to generate high-quality survey items aligned with psychometric standards, demonstrating that LLM-generated items can match or exceed human-written ones in validity, consistency, and representativeness across diverse psychological constructs.Additionally, LLMs have been used to generate "silicon samples" for early-stage research, offering realistic but ethically unencumbered approximations of consumer behavior [Argyle et al., 2023, Sarstedt et al., 2024].</p>
<p>Measurement and Assessment</p>
<p>Beyond simulating human input, agent-like LLMs are increasingly used to assess and quantify psychological and sociological constructs.Recent work demonstrates how LLMs can serve as embedded evaluators within structured research pipelines: for instance, LLMs have been assigned the roles of coders and analysts in experimental setups to interpret qualitative inputs, simulate inter-rater discussion, and generate hypothesis-driven insights-mirroring key steps of empirical inquiry in psychology and the social sciences [Brickman et al., 2025].These agentic roles extend beyond static annotation by enabling dynamic role-taking, turn-based deliberation, and methodologically grounded prompt chaining.</p>
<p>A related approach is the emerging field of "LLM-as-a-judge", which explores whether LLMs can replace human evaluators in specialized contexts like software engineering.For example, recent empirical studies have evaluated the reliability of LLM-based judgment systems for code review and quality assessment.These systems have demonstrated near-human performance in assessing software artifacts, achieving strong alignment with expert human scores while reducing the need for costly manual evaluation [Wang et al., 2025c].These findings suggest that LLM-based evaluators can approximate human judgment in complex, high-stakes evaluation tasks, providing a foundation for more autonomous, agent-like assessment systems.</p>
<p>Level 3: LLM-based Agents</p>
<p>At this level, LLMs transition from static tools or role-players into fully operational agents capable of planning, memory retention, and interaction with external systems or environments.These agents no longer respond in isolation; they observe, orient, decide, and act (OODA, Boyd 2018, Osinga 2007) in ways that echo foundational agent-based modeling principles.Classical agent-based models aimed to capture how micro-level behaviors could generate emergent macro-level patterns, thus often abstracting cognition or decision-making rules [Epstein, 1999[Epstein, , 2012]].In contrast, LLM-powered agents now make it possible to embed sophisticated language-based reasoning, self-reflection, and interactive planning directly within simulated entities [Park et al., 2023, Lin et al., 2023].</p>
<p>Such agents typically incorporate memory streams, persona consistency, planning modules, and action spaces (e.g., tool use, API calls, web access), enabling meaningful adaptation over time.These capabilities bring LLM-based simulations closer to modeling the dynamic, feedback-driven nature of social environments.However, current implementations also face significant challenges.While they excel in controlled, omniscient simulations, they often struggle with uncertainty and asymmetric information-a hallmark of real-world social interaction [Zhou et al., 2024].Moreover, their adaptability in distributed networks remains limited compared to human actors, particularly in environments requiring cooperation or trust [Han et al., 2024].Nevertheless, the emerging field of LLM-based agent simulations shows immense promise.It is rapidly expanding across domains such as computational social science, digital experimentation, and complex system design [Gao et al., 2024].</p>
<p>Finally, it is important to note that this level marks a conceptual turning point in the continuum: agents become capable of autonomous planning and social reasoning, but typically operate in relative isolation or only loosely coordinated teams.The following subsections illustrate key use cases in social science research.</p>
<p>Problem Solving Agents</p>
<p>LLM-based agents are increasingly designed for creative and autonomous problem-solving in both text-based and embodied environments.One prominent example involves agents operating within a Minecraft simulation, where an imagination module, powered by an LLM, generates multiple candidate responses before acting [Zhang et al., 2023].Given abstract prompts like "build a bridge", these agents evaluate diverse structural ideas internally before committing to action.Their performance surpassed baseline agents by demonstrating flexible, goal-aligned creativity that was not hardcoded but emergent from internal planning processes [Zhang et al., 2023].This work introduced a general architecture for embedding imagination in agents and offered new ways of evaluating creativity in open-ended tasks, including novel metrics based on GPT-4.</p>
<p>Another strategy relies on enhancing reasoning through collaboration across multiple LLMs.Instead of using a single model in isolation, multiple prompts or even multiple models are combined sequentially to overcome individual limitations in logic or knowledge coverage.This technique, while not yet a full agentic system, reflects the growing trend of modularizing problem-solving across distributed reasoning paths, suggesting new architectures for collective cognition [Liu et al., 2024].</p>
<p>Autonomous Experimentation</p>
<p>Beyond creative tasks, agentic LLMs are now entering domains traditionally reserved for human researchers.In high-stakes scientific experimentation, LLM-based agents have been developed with planning, memory, and access to scientific tools or external data sources.The following examples are not proper cases for social science, however, as other research fields seem more advanced in incorporating agents for research, we post these as a blueprint or inspiration for more social adaptations.One example presents a chemistry-focused agent capable of autonomously designing and evaluating experimental protocols, drawing on external tools via natural language interfaces.Here, LLMs are not just embedded within simulations, but orchestrate complex workflows in real time [Boiko et al., 2023].Another, particularly illustrative, example is provided by the PaperBench framework, which simulates a team of agents, each assuming a distinct scientific role (e.g., PhD student, reviewer, PI), to autonomously reproduce recent machine learning papers using publicly available resources [Starace et al., 2025].While its domain is technical, the system exemplifies how coordinated agentic reasoning, role-based task allocation, and iterative planning can be used to assess scientific validity.In a broader sense, PaperBench demonstrates how agentic LLM systems can serve not only as individual experimenters but as autonomous evaluators and replicators of complex research processes, offering an important conceptual and methodological reference for future applications in computational social science.</p>
<p>A striking example of agentic performance beyond typical task automation comes from DeepMind's Alpha-Geometry system, which autonomously solved geometry problems from the International Mathematical Olympiad (IMO) at a silver medal level [AlphaProof and AlphaGeometry teams, 2024].Combining a neural language model with symbolic deduction capabilities, AlphaGeometry reasoned through abstract, multi-step proofs and did so in a way that matched human expert performance.This underscores the potential of hybrid agentic systems to engage in genuine epistemic innovation across domains-including those, like mathematics, long considered the pinnacle of human reasoning.It also highlights the emerging ability of LLM-based agents to autonomously contribute to knowledge generation in highly structured, logic-driven problem spaces.</p>
<p>In the realm of social science, agentic LLMs are being tasked with hypothesis testing and causal inference.By embedding structural causal models into LLM-based agents, researchers have demonstrated the feasibility of automated theory-building and simulation-based testing of social mechanisms.These systems can generate hypotheses, simulate plausible social scenarios, and interpret results-all without direct human instruction at every step [Manning et al., 2024].Such capabilities move LLMs beyond mere assistance roles into positions of epistemic contribution, expanding the potential for theory-driven inquiry through generative computation.</p>
<p>Level 4: Multi-Agent Systems</p>
<p>At this stage of development, LLMs are no longer acting alone.Instead, they are embedded into multi-agent systems, where multiple LLM-based agents interact, communicate, negotiate, and coordinate with one another, either as equals or through hierarchical structures.These agents often hold differentiated roles, personas, or goals, and collectively simulate dynamic group behavior.The architectural shift toward agent societies marks a critical leap: it allows for the modeling of collaborative problem solving, scientific co-creation, emergent deliberation, and distributed decision-making-phenomena at the heart of social complexity.</p>
<p>This level brings the coordination logic of agentic AI into full view.From mimicking human research teams and simulating debates to playing communication games or constructing fictional narratives, multi-agent LLM systems unlock entirely new modes of generative social simulation.However, this level also introduces distinct methodological and epistemic risks: agents may converge prematurely due to shared training biases, amplify misinformation, or fail to exhibit genuine behavioral diversity [Estornell andLiu, 2024, Flamino et al., 2025].Still, the promise is profound: Level 4 systems provide a living laboratory for studying social dynamics-scalable, repeatable, and manipulable in ways human-only teams cannot match.</p>
<p>Research Team</p>
<p>Multi-agent systems are increasingly being deployed to emulate the collaborative workflows of human researchers.For example, the multi-agent system proposed by Sankaranarayanan et al. [2025] automates thematic analysis by orchestrating multiple LLM sub-agents to mirror the traditionally manual, iterative steps of qualitative data coding.This approach allows for autonomous data preprocessing, codebook development, consensus building, and final theme generation, significantly enhancing the transparency and scalability of TA processes.The system also incorporates mechanisms for managing ambiguity and ensuring consistency across agent outputs, aligning closely with human expert evaluations in initial benchmarks.Similarly, Google's Gemini 2.0 powers the "AI Co-Scientist", a system capable of autonomously formulating, testing, and refining hypotheses in biomedical domains.This system illustrates the viability of LLM-based multi-agent architectures for dynamic scientific discovery, demonstrating that agentic LLM systems can move beyond static task execution to adaptive, hypothesis-driven inquiry [Gottweis andNatarajan, 2025, Gottweis et al., 2025].</p>
<p>A further development of this concept is embodied by Sakana AI's AI Scientist [Lu et al., 2024a], a multi-agent framework designed to emulate the division of labor, collaboration, and role diversity within real-world research teams.The system orchestrates heterogeneous agents-ranging from hypothesis generators and experimental designers to result analysts and synthesis agents-across iterative research loops.Notably, the AI Scientist incorporates principles of diversity and redundancy, enabling agents with different architectures and learning biases to challenge and refine one another's outputs.This design mirrors the epistemic pluralism often found in productive human teams and leads to improved robustness and creativity in scientific problem-solving.By coordinating diverse LLM-based agents in structured yet flexible workflows, Sakana's system demonstrates how agentic teams can replicate and even enhance human-style collective intelligence in research settings.</p>
<p>Scientific Exploration</p>
<p>LLM-based multi-agent systems are also being used to explore scientific discovery processes themselves.These systems can simulate full research pipelines-from literature review to hypothesis generation, experimental design, and evaluation-offering a new paradigm for automating or augmenting scientific workflows.One line of work proposes a systematic framework for integrating LLMs into qualitative research processes, enabling mixed-methods analysis and cross-linguistic hypothesis exploration.These systems function less as autonomous researchers and more as collaborative agents augmenting human capabilities, reinforcing the view that LLMs should complement, not replace, domain experts [Karjus, 2025].In a broader case study of LLM deployment across the entire research cycle, another study examines real-world practices and institutional constraints when integrating LLMs into social science workflows.The findings emphasize both the epistemic affordances and governance challenges of using generative models for scientific knowledge production [Rask and Shimizu, 2024].</p>
<p>Debating Team</p>
<p>LLM-based agents are increasingly used in simulated deliberative contexts, offering new tools for studying group dynamics, argumentation, and decision-making.Studies show that when tasked with debating issues or reaching consensus, agents can emulate classic social phenomena such as conformity, polarization, and groupthink.For instance, Estornell and Liu [2024] demonstrate that even in controlled debate simulations, agents develop distinct conversational strategies and coordinate in ways that mirror human group behavior.However, they may also converge on biased outputs due to homogeneous training data and architectural similarities, which can limit argumentative diversity.</p>
<p>A recent experimental study by Flamino et al. [2025] examined how LLM agents perform in mixed human-AI debates.In a structured opinion consensus game, agents powered by GPT-4 and Llama 2 interacted anonymously with human participants to discuss topics like climate-conscious diets.The agents stayed consistently on-topic, increased deliberative structure, and helped improve the overall productivity of discussions.Yet, they were less persuasive than human peers-humans were six times more likely to influence one another's opinions.Agents also changed their own views more frequently, suggesting a flexible but less assertive engagement style.While rated as less confident and convincing, their contributions were goal-oriented and constructive, supporting the deliberative process.These findings highlight both the promise and current limitations of LLM agents in modeling social interaction.</p>
<p>Collaborative Task Solving</p>
<p>In more applied contexts, LLM agents are being used to simulate collaborative task environments where social coordination, negotiation, and goal-aligned interaction are critical.For instance, the MetaAgents framework introduces a multi-agent setup that evaluates coordination strategies in job fair simulations, demonstrating that agent groups can autonomously assign roles, negotiate outcomes, and simulate realistic professional interactions [Li et al., 2023b].Moreover, recent work has shown that agent differentiation by personality traits and cognitive styles can replicate human-like collaboration dynamics in domains like chess, debate, and collective decisionmaking.These multi-agent societies display emergent behaviors such as conformity, leadership, and conflict resolution, closely aligning with foundational social psychology concepts [Zhang et al., 2024b].</p>
<p>To enable such complex interactions, advanced prompting techniques like inception prompting have been introduced, which embed structured roles and shared goals into the agents' context to facilitate coherent, multiturn collaboration.This approach has been applied to complex communication games like Werewolf, where agents must infer the intentions of others, develop trust, and navigate high-stakes social interactions without parameter fine-tuning [Xu et al., 2024].Further, the CAMEL framework by Li et al. [2023a] uses inception prompting to facilitate autonomous cooperation in multi-agent systems.This approach relies on predefined role assignments, structured task specification, and iterative message passing to ensure agents stay on task and avoid role flipping or conversational loops, significantly enhancing task completion efficiency.Such frameworks provide valuable testbeds for studying group dynamics, decision-making, and leadership emergence in synthetic social environments [Feng et al., 2025].</p>
<p>Psychological Assessment</p>
<p>Multi-agent systems are increasingly being applied to psychological measurement contexts, and recent work has emphasized the potential of LLMs to move beyond conventional rating scales entirely.Rather than relying on fixed-response formats, these systems aim to quantify psychological constructs directly from free-form, natural language.This shift addresses a critical limitation of traditional assessments: the reduction of complex psychological states to a fixed set of numerical ratings.For example, open-ended language responses capture more nuanced, context-rich information about mental states, significantly outperforming conventional rating scales in measures like self-information and context sensitivity.LLMs can achieve near-theoretical upper limits of accuracy in aligning with human-rated scales, while also providing deeper insights into psychological constructs through dynamic, personalized language interpretation [Kjell et al., 2024].Further, the PsychoGAT framework leverages LLM agents to transform traditional self-report scales into interactive fiction games, providing a more engaging and personalized assessment experience for constructs like depression, personality, and cognitive distortions.By integrating role-playing, memory, and dynamic dialogue, this approach achieves strong psychometric validity while enhancing user engagement through game mechanics, improving participant satisfaction and immersion [Yang et al., 2024b].These approaches open new pathways for assessing mental health in more ecologically valid, context-sensitive ways.</p>
<p>Learning Communities and Education</p>
<p>Finally, multi-agent systems are increasingly explored as collective intelligence frameworks for educational contexts.These systems envision networks of LLM agents that can learn from experience, exchange information across nodes, and collectively build a shared knowledge base.This approach aligns with foundational concepts in complexity science, which emphasize decentralized learning, self-organization, and emergent behavior [Soltoggio et al., 2024].Individual LLM-based agent systems show their ability to take on specific roles in the overall learning process (e.g., like an error-detector, Xu et al. 2025, learning-content creator, Elkins et al. 2024, and automated answer scoring, Li et al. 2025a).Chu et al. [2025] extend this perspective by examining the potential of multi-agent systems for automated tutoring and adaptive instruction.It highlights how LLM agents can collaboratively simulate classroom interactions, integrate feedback loops, and perform real-time performance assessments, creating adaptive, student-centered learning environments.</p>
<p>As a concrete example, the SimClass framework demonstrates how multi-agent LLM systems can replicate classroom dynamics, enabling real-time adaptation to student needs and facilitating personalized instruction through role-based agents [Zhang et al., 2024d].This approach leverages multi-agent architectures to model the varied cognitive and emotional states of students, offering a scalable platform for studying educational processes at both individual and group levels.In a similar vein, the EduPlanner system uses a multi-agent approach to automate the entire instructional design cycle, including content generation, evaluation, and optimization [Zhang et al., 2025].EduPlanner employs a Skill-Tree structure to represent students' prior knowledge and learning progress, dynamically adjusting lesson plans based on real-time performance feedback.This allows for the generation of personalized educational content, integrating multiple agents for instructional assessment, optimization, and error analysis [Zhang et al., 2025].The system's iterative, adversarial collaboration among evaluator, optimizer, and analyst agents mimics human teacher-student interactions, reinforcing the concept of collective learning within artificial educational ecosystems.Together, these projects highlight the potential of multi-agent LLM systems to revolutionize educational practice, transforming classrooms into adaptive, learner-centered environments that scale with student needs.</p>
<p>Level 5: Complex Adaptive Systems</p>
<p>At Level 5, LLM-based agent systems extend beyond small-scale collaboration or structured task execution to model entire societies, ecosystems, or complex adaptive systems.These platforms aim to capture the emergent, largescale dynamics of social systems by integrating hundreds or even thousands of interacting agents, each equipped with memory, autonomous decision-making, and social learning capabilities.Unlike the more constrained, role-based interactions of Level 4, Level 5 agents operate within fluid, adaptive networks where emergent phenomena-such as social norms, power structures, and collective behaviors-emerge spontaneously from the decentralized interactions of individual agents.This transition reflects a fundamental shift from deterministic simulation to adaptive, emergent modeling, enabling researchers to study not just predefined rules but the self-organizing principles that govern macro-social phenomena.Here, the focus shifts from explicit, top-down control to the bottom-up processes that shape the formation of norms, collective intelligence, and long-term social adaptation, capturing the rich, often unpredictable dynamics of real-world societies.</p>
<p>Human-Like Social Network Behavior</p>
<p>Simulating human-like social network behavior is a critical step toward understanding complex social dynamics.Unlike isolated, single-agent simulations, social network systems aim to replicate the intricate web of human interactions at both individual and population levels.These systems capture not only the direct actions of agents but also the emergent phenomena that arise from their interactions, such as information diffusion, collective sentiment shifts, and social contagion.</p>
<p>One pioneering approach is the Generative Agent platform, which models small-scale, LLM-driven societies within 2D game environments [Park et al., 2023].Here, agents autonomously plan their daily routines, engage in conversations, and adapt their behaviors based on prior interactions.These agents exhibit complex emergent behaviors like spontaneous information diffusion, cooperative group dynamics, and even basic forms of political organization, such as hosting mayoral elections and organizing social gatherings [Park et al., 2023].The system demonstrates the potential for LLMs to simulate realistic, context-sensitive social interactions, providing a foundation for more sophisticated social network modeling.Scaling up this concept, GenSim introduces a platform for simulating up to 100,000 agents, incorporating error correction and adaptive learning to manage the complexity of massive agent interactions.The system abstracts agent profiles, multi-agent scheduling, and environmental setups to provide a flexible framework for large-scale, realistic social simulations.This approach significantly advances the computational feasibility of modeling large-scale social networks, enabling the study of emergent phenomena at unprecedented scales [Tang et al., 2024].GenSim supports real-time interaction among thousands of agents, facilitating the study of macro-level social dynamics, from market behavior to political polarization.</p>
<p>Further extending this concept, the S3 system introduces a framework for simulating social networks with agents that model emotions, attitudes, and interpersonal interactions [Gao et al., 2023].It captures emergent phenomena such as collective sentiment shifts and social contagion by integrating fine-tuned LLM agents capable of perceiving and responding to their informational environment.The system has been tested with real-world social network data, demonstrating its ability to replicate complex social phenomena like the spread of political attitudes or the evolution of public opinion.This framework highlights the importance of modeling not just the actions of individual agents but also the emergent properties that arise from their collective interactions, providing valuable insights for both theoretical and applied social science research.</p>
<p>Emergent Social Dynamics</p>
<p>Emergent social dynamics in complex adaptive systems capture the spontaneous formation of cooperation, conflict, and collective order.These dynamics arise from the repeated interactions of individual agents, which collectively produce higher-order social structures.The study of such emergent phenomena draws on foundational theories like Social Contract Theory (SCT, Dai et al. 2024) and Social Exchange Theory (SET, Wang et al. 2025b), which provide conceptual blueprints for understanding how micro-level exchanges can aggregate into macro-level social order.</p>
<p>For instance, the Artificial Leviathan framework explores the emergence of cooperative norms and governance structures from a "state of nature" characterized by conflict and self-interest.In this model, agents engage in strategic decision-making, alliance formation, and resource management, gradually evolving from isolated, competitive behavior to organized, cooperative societies [Dai et al., 2024].This approach closely aligns with Hobbes's original vision of the social contract, where rational self-interest drives the formation of the collective order.Similarly, the SUVA framework systematically analyzes LLM agents' socially grounded decision-making based on their textual outputs, revealing tendencies toward fairness, reciprocity, and cooperative behavior without explicit priming [Leng and Yuan, 2024].This approach demonstrates that even without pre-defined social scripts, LLM agents can exhibit prosocial tendencies, aligning their actions based on context rather than rigid, pre-programmed responses.</p>
<p>Homans' SET provides a micro-level foundation for understanding how individual interactions can lead to collective social phenomena.Recent work has adapted SET to LLM-based agents, modeling the six core propositions of SET-success, stimulus, value, deprivation-satiation, aggression-approval, and rationality-within a controlled, multi-agent society.These systems demonstrate the emergence of cooperation, reciprocity, and social norms without explicit hardcoding of behaviors, capturing more nuanced human-like exchanges through cognitive and affective components, such as affinity scores and social value orientations [Wang et al., 2025b].Expanding this concept further, the AgentSociety framework introduces a large-scale simulation environment for modeling complex, real-world social phenomena.It supports the exploration of topics like polarization, misinformation spread, and economic policy impacts, providing a concrete testbed for social science research.This approach emphasizes scalability and empirical validation, offering a powerful tool for investigating emergent social dynamics in digitally mediated environments [Piao et al., 2025].</p>
<p>Collaboration and Competitive Dynamics</p>
<p>Multi-agent systems at this level are not only designed to study cooperative behavior but also to explore the competitive dynamics that emerge in complex social and economic environments.These systems capture the interplay between collaboration and competition, revealing how agents adapt to both supportive and adversarial conditions over time.</p>
<p>One prominent example is the GOVernance of the Commons SIMulation (GOVSIM) platform.It is specifically designed to test LLM agents' capacity for sustainable cooperation in common resource dilemmas.Inspired by economic theories of cooperation and the management of shared resources, GOVSIM simulates scenarios like fisheries, pastures, and pollution, where agents must balance short-term gains with long-term sustainability.This approach highlights both the potential and current limitations of LLM-based systems for modeling real-world governance challenges, providing insights into how collective strategies emerge and stabilize in resource-limited environments [Piatti et al., 2024].</p>
<p>In parallel, the CompeteAI framework introduces a novel approach for modeling competitive dynamics among LLM-based agents.This system simulates a virtual economy where agents, such as restaurant owners, compete to attract customers by dynamically adjusting their strategies based on market conditions and competitor behavior.The agents exhibit emergent market phenomena, including price wars, brand loyalty, and customer segmentation, closely mirroring real-world competitive pressures in economic environments.This framework emphasizes the importance of long-term strategic reasoning and adaptive decision-making, highlighting how local agent interactions can lead to complex, emergent market behaviors at the system level [Zhao et al., 2024].</p>
<p>Opinion Dynamics</p>
<p>Modeling opinion dynamics is a cornerstone of social science, capturing how collective beliefs, attitudes, and preferences evolve over time within populations.LLM-based agent systems offer powerful tools for simulating these processes, enabling researchers to study phenomena like consensus formation, polarization, and the spread of misinformation in complex social networks.</p>
<p>Recent work by Chuang et al. [2024] introduces a multi-agent framework for simulating opinion dynamics in large-scale social systems.This approach highlights the inherent biases of LLMs towards producing accurate information, which often drives simulated agents toward consensus around scientifically validated positions.However, by introducing specific prompts that activate confirmation bias, the authors were able to reproduce key social phenomena like ideological clustering and opinion fragmentation, providing a more realistic model of human social interactions.Building on this, Li et al. [2024a] extended the study of opinion dynamics by integrating political polarization mechanisms into LLM-based social simulation platforms.They demonstrated that LLM agents could replicate known polarization effects, such as ideological clustering and opinion reinforcement, when exposed to biased or segmented information environments.Their work underscores the importance of realistic social dynamics, including homophily and network topology, for capturing the full complexity of opinion evolution in digital and physical societies.</p>
<p>Additionally, Cisneros-Velarde [2024] explored the underlying principles that shape opinion dynamics within populations of interacting LLMs.They identified several key biases, including a preference for equity-consensus, caution in opinion shifts, and sensitivity to ethical considerations, which collectively influence the distribution of opinions over time.Their findings emphasize the need to account for these biases when designing multi-agent systems for opinion modeling, as they can significantly impact the emergent properties of such systems.Similarly, Piao et al. [2025] introduced the AgentSociety platform, a large-scale social simulator capable of modeling the complex interplay between social influence, polarization, and collective behavior in populations exceeding 10,000 agents.Here, LLM-based multi-agent systems are being used to simulate the long-term effects of public policies and external shocks by studying the impact of polarization, the spread of inflammatory messages, and the effects of natural disasters on collective behavior.These systems offer a powerful testbed for evaluating policy outcomes, social resilience, and institutional change.</p>
<p>Psychological and Health Dynamics</p>
<p>LLM-based agents are emerging as powerful tools for modeling the complex interplay of psychological and health dynamics, capturing the multifactorial influences of socio-environmental factors on mental health.These systems offer the potential to model interactions at multiple levels-from individual psychological processes to broader community dynamics and societal influences-providing insights that are often ethically challenging to obtain through human experimentation.</p>
<p>For instance, Kambeitz and Meyer-Lindenberg [2025] highlight how generative agents can simulate humanlike behavior in virtual environments to investigate the effects of environmental and social determinants on mental health.These agents can replicate complex social interactions, model adverse life events, and capture the nuanced effects of urban stressors, such as social deprivation, pollution, and lack of green spaces, on psychological well-being.Importantly, these models can simulate emergent phenomena, such as resilience, which depends on both individual traits and supportive social networks, thus offering a more realistic representation of mental health dynamics than traditional methods.In a similar vein, Wu et al. [2024] propose a multi-agent framework specifically designed to integrate diverse psychological theories, including cognitive-behavioral, psychodynamic, and humanistic approaches, to generate more empathetic responses by professionals.Their framework leverages multiple LLMs acting as psychologists, each representing a different therapeutic perspective, and a decision-making agent to select the most contextually appropriate responses.This approach allows for a more nuanced understanding of psychological dynamics by incorporating multiple interpretive frameworks and iterative feedback processes, effectively mirroring real-world therapeutic interactions.</p>
<p>Discussion</p>
<p>This paper set out to provide a conceptual scaffold for understanding and categorizing LLM-based agents in the social sciences.By introducing a six-tier developmental continuum, we offer a framework that connects technical advancements with core social science interests-from modeling individual cognition to simulating emergent societal phenomena.The goal was not to catalog all existing systems but to distill the structural differences that matter most for empirical, theoretical, and ethical engagement with agentic AI.</p>
<p>Even though many of the studies cited are still in early stages or preprint form, the rapid pace of publication and prototyping indicates a vibrant, self-reflective research landscape.Notably, this paper presents only a sample of current efforts; further examples are already being developed across social science domains.As the boundaries between technical design and social application continue to blur, the need for interdisciplinary research becomes paramount [Vladova et al., 2024].Initial use cases already suggest that LLM-based agents could reshape long-standing methodological constraints in the social sciences.They promise a new kind of flexibility-one that enables systematic experimentation at scale, dynamic simulation of collective behavior, and the iterative refinement of theory in silico.Rather than simply mimicking human responses, these systems offer opportunities to construct new epistemic instruments: agents that help test, challenge, and expand our understanding of complex social dynamics.</p>
<p>Importantly, while some tiers, especially Levels 0 to 2, are already being widely implemented for automation and efficiency, higher levels that support coordination and emergence remain underexplored but highly promising.Particularly, Level 3 systems appear relatively scarce, likely due to the overhead of maintaining coherent individual agents compared to the greater expressiveness and scalability of multi-agent coordination.Still, as capabilities grow, so too does the opportunity to model not just isolated behaviors but the generative mechanisms of social systems themselves.As this field progresses, it brings with it not only technical innovation but also a strong tradition of critical reflection.Many recent publications have already engaged deeply with issues of bias, validity, and epistemological caution, which is an encouraging sign for the responsible evolution of agentic AI in social science.The following sections take up this balance in more detail, examining both the methodological affordances and the challenges that come with embedding LLM agents into empirical research and simulation.</p>
<p>Core Potentials and Methodological Considerations</p>
<p>The recent proliferation of LLM-based agent research opens unprecedented avenues for the social sciences.These systems offer dual utility: at lower levels (0-2), they serve as scalable instruments for automating routine research tasks; at higher tiers (3-5), they become tools for generating and testing theories of social interaction, coordination, and emergence.At the foundational levels, LLMs enhance efficiency and consistency in established methods.Stateless tools (Level 0) are already widely used for classification, summarization, and synthetic data generation [Valdenegro, 2023].Role-based systems (Level 1) enable contextualized outputs across longer interactions, supporting more coherent and reproducible workflows [Wang et al., 2024].Level 2 systems extend this further by incorporating task autonomy and memory, which allow agents to simulate goal-driven behavior and manage multi-step analyses or experimental routines [Hewitt et al., 2024].</p>
<p>From Level 3 onward, LLM agents begin to offer epistemic value beyond automation.Fully agentic systems (Level 3) are increasingly used to model specific cognitive profiles, simulate participant roles, or replicate known psychological mechanisms [Li et al., 2025b, Starace et al., 2025].These agents support targeted experimentation and allow for controlled replications of empirical effects, strengthening trust in their validity [Karjus, 2025].Multi-agent systems (Level 4) model social interactions by distributing capabilities across multiple autonomous entities.These configurations enable the study of inter-agent negotiation, group decision-making, norm formation, and institutional dynamics [Borghoff et al., 2025, Yang et al., 2024b].Particularly in political, organizational, or economic contexts, such systems facilitate experiments that would be logistically or ethically infeasible in real-world settings.</p>
<p>At Level 5, complex adaptive systems simulate population-scale phenomena.These models integrate thousands of interacting agents and allow for the exploration of emergent macro-social patterns, such as the spread of misinformation, social tipping points, or cultural diffusion [Hammond et al., 2025, Park et al., 2023].The ability to generate plausible yet novel outcomes-rather than merely reproduce existing ones-underscores the knowledgegenerating potential of agentic simulations [Bail, 2024, Grossmann et al., 2023].Anecdotally, the sophistication of current agentic systems is perhaps best illustrated by their deployment in complex gaming environments that require long-term strategic planning, resource management, and adaptive learning.Major technology companies have increasingly turned to Pokémon-a game requiring the training and strategic deployment of creatures in competitive battles-as a benchmark for testing advanced agentic capabilities.Anthropic's Claude 3.7 Sonnet achieved full game completion in February 2025 [Peter, 2025], followed by Google AI's Gemini 2.5 in May of the same year [Schwartz, 2025].These achievements demonstrate the capacity of contemporary LLM-based agents to engage in complex, multi-step decision-making processes that mirror the kind of strategic reasoning and adaptive behavior central to social coordination and collective problem-solving.</p>
<p>Across these tiers, LLM-based systems offer significant methodological gains.They enable scalable A/B testing, real-time iteration, longitudinal tracking without attrition, and high-throughput hypothesis evaluation.When validated against empirical benchmarks, these systems do not just replicate but extend the reach of social science methods, allowing researchers to model, test, and refine complex theoretical frameworks at a resolution previously inaccessible.</p>
<p>Critical Reflections</p>
<p>While the potential of LLM-based agents is considerable, their integration into social science raises serious methodological and ethical challenges.Chief among these is reproducibility.LLM outputs can vary across identical prompts due to stochastic generation and prompt sensitivity, especially in emergent, multi-agent settings where small changes can cascade into radically different outcomes [Atil et al., 2024, Lu et al., 2024b].This complicates both experimental replication and the trustworthiness of emergent findings.The complexity of multi-agent systems also introduces new failure modes.As Chan et al. [2023b] and Hammond et al. [2025] show, agents may exhibit unintended behaviors such as conflict, miscoordination, or collusion, particularly when incentives or representations diverge.These behaviors are not merely technical bugs but epistemic distortions, especially if they arise from architectural choices rather than domain-relevant social processes.</p>
<p>Representation bias remains a critical issue.LLMs trained on predominantly Western, Anglophone corpora tend to underrepresent minority perspectives and may reinforce dominant cultural norms [Hou andHuang, 2025, Santurkar et al., 2023].This becomes especially problematic when agents are used as proxies for diverse human populations in public opinion research, policy modeling, or global simulations [Wang et al., 2025a, Qu andWang, 2024].Synthetic participants risk mischaracterizing the very populations they intend to model, introducing systemic distortions into research conclusions.In addition, LLMs lack many core elements of human cognition: genuine understanding, emotional depth, and meta-cognitive awareness [Rossi et al., 2024, Roberts et al., 2024].Even when simulations are plausible at a surface level, they may fail to capture the mechanisms that drive real-world behavior.The assumption that linguistic mimicry equates to psychological realism must therefore be treated with caution.</p>
<p>Finally, the field suffers from a lack of methodological standardization.There is little consensus on how to evaluate agentic behavior, validate emergent phenomena, or benchmark performance across tasks [Ke et al., 2024, Lu et al., 2024b].Further, especially for complex adaptive systems, there is no baseline to what to compare emergent phenomena to.While this lack of standardization may be understandable given that the field is relatively new and emerging, methodological standards need not await decades of development-they can be established by building upon the extensive methodological learning curves and validation frameworks that social science has already developed over its long history.This hinders scientific progress, limits comparability across studies, and raises barriers to reproducibility.Without shared standards, the promise of LLM agents risks being diluted by inconsistent or poorly justified findings.</p>
<p>Taken together, these challenges should not be mistaken as disqualifying the use of agentic systems in social science.Rather, they highlight the necessity of cautious, rigorous, and interdisciplinary development.Like traditional social science methods, which have long grappled with the tension between representing complex social realities and simplifying them to identify causal mechanisms [Mwita, 2022, Hofman et al., 2021], the use of LLM-based agents requires a similar balance.The limitations of agentic systems-be they related to reproducibility, bias, or representational fidelity-mirror long-standing critiques of social science itself, which is often accused of being too reductive or too speculative.Yet, despite these critiques, decades of careful, cumulative, and reflective research have demonstrated the field's ability to produce meaningful insights [Mille et al., 2022].Similarly, when embedded within robust research designs, subjected to critical validation, and aligned with a culture of transparency and replication, LLM-based agents can offer significant epistemic value [Freese and Peterson, 2017].With appropriate safeguards and theoretically informed constraints, they are not just technical tools, but potential contributors to a cumulative science of social behavior.</p>
<p>Future Research Directions</p>
<p>The rapid development of LLM-based agents presents a wide range of opportunities for advancing social science research.However, realizing this potential will require both technical innovation and interdisciplinary collaboration.The following directions highlight key areas where future work can contribute to the maturation of this field:</p>
<p>Advancing Agent Capabilities.Future research should focus on enhancing the cognitive and emotional sophistication of LLM-based agents.This includes integrating memory mechanisms, affective reasoning, and goal-oriented planning to create more human-like, context-sensitive agents capable of long-term stable interaction.Advanced architectures, such as Centaurian systems, which combine the strengths of human cognition and machine processing, offer promising paths forward [Borghoff et al., 2025].Additionally, further exploration is needed to understand how agentic systems can simulate complex social processes, including identity formation, social learning, and collective problem-solving, without falling into the traps of oversimplification common for traditional agentic systems [Lowe et al., 2017].</p>
<p>Methodological Innovations.Given the inherent complexity of multi-agent systems, robust validation techniques are essential.This includes the development of standardized evaluation metrics, reproducibility protocols, and benchmarking strategies that capture both the intended behaviors and the unintended emergent properties of these systems.Recent work has highlighted the need for more consistent and interpretable outputs in LLM-based simulations, particularly in light of known stability issues [Atil et al., 2024].As these systems move from experimental to applied settings, the ability to ensure reliability, transparency, and ethical consistency will become increasingly critical.This also includes addressing the inherent biases in language models, which can propagate into social simulations if left unchecked [Ke et al., 2024].</p>
<p>Expanding Application Domains.The flexibility of LLM-based agents offers unique opportunities for extending their use beyond traditional computational settings.Future research should explore cross-cultural, multilingual, and real-world decision-making contexts to capture the full diversity of human social behavior.This could involve integrating LLMs into real-time, high-stakes decision-making processes, such as crisis response, policy simulation, or economic forecasting, where their ability to rapidly process and synthesize large volumes of data can provide significant advantages [Murakami et al., 2002].Additionally, multi-agent systems present a valuable opportunity for studying the dynamics of global phenomena, such as misinformation spread, political polarization, and international collaboration, under controlled but realistic conditions.</p>
<p>Human-AI Collaboration and Hybrid Systems.As agentic systems become more capable, their integration into human workflows across all stages of the research process will require careful consideration.This includes not only technical design but also the creation of ethical guidelines, transparency standards, and training protocols that ensure humans remain in control of critical decision-making processes (e.g., for ChatGPT and scientific authorship see Guidelines are urgently needed [2023]).Recent research has highlighted the need for hybrid systems that effectively integrate human intuition with machine-scale processing, creating a balanced division of cognitive labor that leverages the strengths of both [Borghoff et al., 2025, Haase andPokutta, 2024].This approach could significantly enhance the practical applicability of agentic systems, transforming them from experimental tools into robust components of real-world decision support systems.</p>
<p>Fundamental Theoretical Questions.Finally, researchers should address the deeper theoretical implications of using LLMs as proxies for human behavior.This includes examining the philosophical and ethical dimensions of substituting human participants with synthetic agents, as well as the epistemological risks associated with treating LLM outputs as stand-ins for human cognition [Zhou et al., 2024, Larooij andTörnberg, 2025].Critical reflections are needed to ensure that these systems genuinely advance social science rather than merely replicate its methods and challenges at scale.This requires ongoing dialogue between computer scientists, social scientists, ethicists, and policymakers to establish clear guidelines for the responsible use of agentic systems in research and practice.</p>
<p>Conclusion</p>
<p>This paper proposes a structured framework for integrating LLM-based agents into social science research, outlining a six-tier model that spans from simple stateless tools to fully adaptive multi-agent systems.This continuum offers a conceptual and practical foundation for both researchers and system designers to assess the capabilities, requirements, and epistemic roles of LLM agents in social inquiry.At the lower tiers, agentic systems offer clear methodological benefits: streamlining repetitive tasks, improving reproducibility, and enabling scalable text analysis.At the higher levels, however, the value of LLM agents shifts from efficiency toward epistemic innovation.Here, multi-agent architectures simulate emergent social behavior, test theoretical propositions at scale, and model phenomena, such as norm formation, conflict dynamics, or policy effects, that are difficult, if not impossible, to study through traditional empirical means.These systems, when properly constrained and evaluated, offer genuinely novel ways of generating social scientific insight.</p>
<p>Still, this promise is not without its caveats.The challenges of reproducibility, representational bias, and epistemological overreach demand careful, interdisciplinary scrutiny.Yet, these issues are not unique to agentic systems-they echo long-standing tensions in the social sciences themselves.As with any simplification of complex human phenomena, the power of LLM-based agents lies not in perfect replication but in their capacity to isolate, model, and explore meaningful mechanisms under controlled conditions.When embedded within transparent research designs, aligned with cumulative scientific standards, and subjected to replication and critique, agentic systems can meaningfully contribute to social scientific knowledge.</p>
<p>Looking forward, key priorities for future work include expanding agent capabilities (e.g., memory, affective reasoning, long-term goal adaptation), refining standards for robustness and validation, and applying these systems across a broader range of cultural and institutional contexts.In parallel, their integration into human workflows must be guided by ethical design principles and supported by interdisciplinary collaboration.Together, these efforts chart a path toward a more integrated, responsible, and impactful role for LLM-based agents in the social sciences.Rather than replacing traditional methods, these systems augment and extend them-opening up new ways of seeing, modeling, and understanding the complex dynamics that shape human societies.</p>
<p>Figure 1 :
1
Figure 1: Design Architecture of the Levels of Agentic LLM Systems</p>
<p>Table 2 :
2
Li et al. [2024a]24]ng et al. [2024],Li et al. [2024a], Cisneros-Velarde [2024], Piao et al. [2025] Psychological and health dynamics Kambeitz and Meyer-Lindenberg [2025], Wu et al. [2024] Examples of LLM-Based Systems Across Agentic Levels in Social Science Research
SystemSocial Science FocusScientific SourcesTypeText, idea, material generationGao et al. [2024], Ke et al. [2024], Venkatraman0LLM-as-ToolQualitative codinget al. [2025] Ziems et al. [2024], Nilsson et al. [2025], Ke et al.[2024], Demszky et al. [2023]Data analysisOrganisciak et al. [2023], Ke et al. [2024], Haaseet al. [2025b]Literature reviewKarjus [2025], Ke et al. [2024]1LLM-as-RolePersona simulation (Big Five, etc.) Emotional behavior simulationWang et al. [2025d], Huang et al. [2024a], Li et al. [2024b], Sun et al. [2024] Mozikov et al. [2024]Human preferences simulationZhang et al. [2024a], Santurkar et al. [2023], Rossiet al. [2024]2Agent-like LLMExperiment replicationFilippas et al. [2024], Horton [2023], Yeykelis et al. [2024], Hewitt et al. [2024], Lippert et al. [2024], Mozikov et al. [2024], Zhang et al. [2024a]Survey response simulationAher et al. [2023], Argyle et al. [2023], Tjuatja et al.[2024], von der Heyde et al. [2023]Measurement and assessmentWang et al. [2025c], Brickman et al. [2025]3LLM-basedProblem solving Autonomous experimentationZhang et al. [2023], Liu et al. [2024] Boiko et al. [2023], Manning et al. [2024], StaraceAgentset al. [2025], Yamada et al. [2025]Research teamSankaranarayanan et al. [2025], Gottweis and4Multi-Agent SystemsScientific exploration Debating teams Collaborative task-solvingNatarajan [2025], Gottweis et al. [2025] Karjus [2025], Rask and Shimizu [2024] Estornell and Liu [2024], Flamino et al. [2025] Li et al. [2023b], Phelps and Russell [2023], Zhanget al. [2024b], Feng et al. [2025], Li et al. [2023a],Xu et al. [2024]Psychological assessmentYang et al. [2024a], Kjell et al. [2024]Learning communitiesChu et al. [2025], Soltoggio et al. [2024], Zhanget al. [2024d, 2025]Human-like social network behaviorPark et al.Complex5AdaptiveSystem
Zhao et al. [2024]4]023],Lu et al. [2024b]Emergent social dynamicsLeng and Yuan [2024], Wu et al. [2024], Chen et al. [2023], Dai et al. [2024], Yuan et al. [2024], Wang et al. [2025b], Demszky et al. [2023] Collaboration and competitive dynamics Chen et al. [2023], Chan et al. [2023a], Piatti et al. [2024],Zhao et al. [2024]</p>
<p>AcknowledgmentsResearch reported in this paper was partially supported by the Deutsche Forschungsgemeinschaft (DFG) through the DFG Cluster of Excellence MATH+ (grant number EXC-2046/1, project ID 390685689), and by the German Federal Ministry of Education and Research (BMBF), grant number 16DII133 (Weizenbaum-Institute).</p>
<p>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. Gati Aher, Rosa I Arriaga, Adam Tauman, Kalai , July 2023</p>
<p>AlphaProof and AlphaGeometry teams. AI achieves silver-medal standard solving International Mathematical Olympiad problems. July 2024</p>
<p>Out of One, Many: Using Language Models to Simulate Human Samples. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, David Wingate, 10.1017/pan.2023.2Political Analysis. 1047-1987313July 2023</p>
<p>LLM Stability: A detailed analysis with some surprises. Berk Atil, Alexa Chittams, Liseng Fu, Ferhan Ture, Lixinyu Xu, Breck Baldwin, September 2024</p>
<p>Can Generative AI improve social science?. Christopher A Bail, 10.1073/pnas.2314021121Proceedings of the National Academy of Sciences. 12121e2314021121May 2024</p>
<p>Exploring cognitive strategies in human-AI interaction: ChatGPT's role in creative tasks. Jelle Boers, Terra Etty, Martine Baars, Kim Van Boekhoven, 10.1016/j.yjoc.2025.100095Journal of Creativity. 2713-3745351100095April 2025</p>
<p>Emergent autonomous scientific research capabilities of large language models. A Daniil, Robert Boiko, Gabe Macknight, Gomes, April 2023</p>
<p>Human-Artificial Interaction in the Age of Agentic AI: A System-Theoretical Approach. M Uwe, Paolo Borghoff, Remo Bottoni, Pareschi, February 2025</p>
<p>A Discourse on Winning and Losing. John R Boyd, Air University Press Maxwell Air. 4002018</p>
<p>Large Language Models for Psychological Assessment: A Comprehensive Overview. Jocelyn Brickman, Mehak Gupta, Joshua R Oltmanns, Advances in Methods and Practices in Psychological Science. 2025</p>
<p>Towards the Scalable Evaluation of Cooperativeness in Language Models. Alan Chan, Maxime Riché, Jesse Clifton, March 2023a</p>
<p>Harms from Increasingly Agentic Algorithmic Systems. Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, Dmitrii Krasheninnikov, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian Weller, David Krueger, Tegan Maharaj, 10.1145/3593013.3594033Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT '23. the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT '23New York, NY, USAAssociation for Computing MachineryJune 2023b</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848202326arXiv preprint</p>
<p>Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S Yu, Qingsong Wen, LLM Agents for Education: Advances and Applications. March 2025</p>
<p>Simulating Opinion Dynamics with Networks of LLM-based Agents. Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T Rogers, April 2024</p>
<p>On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models. Pedro Cisneros-Velarde, September 2024</p>
<p>Gordon Dai, Weijia Zhang, Jinhan Li, Siqi Yang, Chidera Onochie Lbe, Srihas Rao, Arthur Caetano, Misha Sra, Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory. July 2024</p>
<p>Using large language models in psychology. Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, Michaela Jones, Danielle Krettek-Cobb, Leslie Lai, Nirel Jonesmitchell, Desmond C Ong, Carol S Dweck, James J Gross, James W Pennebaker, 10.1038/s44159-023-00241-5Nature Reviews Psychology. 2731-0574211November 2023</p>
<p>A social path to human-like artificial intelligence. A Edgar, Suzanne Duéñez-Guzmán, Jane X Sadedin, Kevin R Wang, Joel Z Mckee, Leibo, 10.1038/s42256-023-00754-xNature Machine Intelligence. 2522-5839511November 2023</p>
<p>How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes. Sabina Elkins, Ekaterina Kochmar, Jackie C K Cheung, Iulian Serban, 10.1609/aaai.v38i21.30353Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMarch 202438</p>
<p>Agent-based computational models and generative social science. Joshua M Epstein, 10.1002/(SICI)1099-0526(199905/06)4:5&lt;41::AID-CPLX9&gt;3.0.CO;2-FComplexity. 1076-278745May 1999</p>
<p>Joshua M Epstein, Generative Social Science: Studies in Agent-Based Computational Modeling. Princeton University Press2012</p>
<p>Multi-LLM Debate: Framework, Principals, and Interventions. Andrew Estornell, Yang Liu, 38th Conference on Neural Information Processing Systems Proceedings. 2024</p>
<p>. Shangbin Feng, Wenxuan Ding, Alisa Liu, Zifeng Wang, Weijia Shi, Yike Wang, Zejiang Shen, Xiaochuang Han, Hunter Lang, Chen-Yu Lee, Tomas Pfister, Yejin Choi, Yulia Tsvetkov, When One LLM Drools. February 2025Multi-LLM Collaboration Rules</p>
<p>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?. Apostolos Filippas, John J Horton, Benjamin S Manning, 10.1145/3670865.3673513Proceedings of the 25th ACM Conference on Economics and Computation. the 25th ACM Conference on Economics and ComputationNew Haven CT USAACMJuly 2024</p>
<p>What if GPT4 Became Autonomous: The Auto-GPT Project and Use Cases. Mehmet Firat, Saniye Kuleli, 10.57020/ject.1297961Journal of Emerging Computer Technologies. 2757-8267312023</p>
<p>Limits of Large Language Models in Debating Humans. James Flamino, Mohammed Shahid Modi, Boleslaw K Szymanski, Brendan Cross, Colton Mikolajczyk, February 2025</p>
<p>Replication in Social Science. Jeremy Freese, David Peterson, 10.1146/annurev-soc-060116-053450Annual Review of Sociology. 0360-0572432017. July 2017</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, Social-network Simulation System with Large Language Model-Empowered Agents. July 20233</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li, 10.1057/s41599-024-03611-3Humanities and Social Sciences Communications. 2662-9992111September 2024</p>
<p>Accelerating scientific breakthroughs with an AI co-scientist. Juraj Gottweis, Vivek Natarajan, ; Wei-Hung, Alexander Weng, Tao Daryin, Anil Tu, Petar Palepu, Artiom Sirkovic, Felix Myaskovsky, Keran Weissenberger, Ryutaro Rong, Khaled Tanno, Dan Saab, Jacob Popovici, Fan Blum, Katherine Zhang, Avinatan Chou, Burak Hassidim, Amin Gokturk, Pushmeet Vahdat, Yossi Kohli, Andrew Matias, Kavita Carroll, ; Kulkarni, R D Tiago, Costa, Gary José R Penadés, Yunhan Peltz, Annalisa Xu, Alan Pawlosky, Vivek Karthikesalingam, Natarajan, Nenad Tomasev, Vikram Dhillon, Eeshit Dhaval VaishnavFebruary 2025. 2025Byron Lee,Juraj Gottweis. Towards an AI co-scientist</p>
<p>AI and the transformation of social science research. Igor Grossmann, Matthew Feinberg, Dawn C Parker, Nicholas A Christakis, Philip E Tetlock, William A Cunningham, 10.1126/science.adi1778Science. 0036-80753806650June 2023</p>
<p>Guidelines are urgently needed. The AI writing on the wall. 10.1038/s42256-023-00613-9Nature Machine Intelligence. 2522-583951January 2023</p>
<p>Artificial muses: Generative artificial intelligence chatbots have risen to human-level creativity. Jennifer Haase, H P Paul, Hanel, 10.1016/j.yjoc.2023.100066Journal of Creativity. 2713-37453332023</p>
<p>Human-AI Co-Creativity: Exploring Synergies Across Levels of Creative Collaboration. Jennifer Haase, Sebastian Pokutta, November 2024</p>
<p>An approach to model forgetting. Jennifer Haase, Christof Thim, 10.30844/aistes.v4i1.17AIS Transactions on Enterprise Systems. 1867-7134412020</p>
<p>Has the Creativity of Large-Language Models peaked? An analysis of inter-and intra-LLM variability. Jennifer Haase, H P Paul, Sebastian Hanel, Pokutta, April 2025a</p>
<p>S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment. Jennifer Haase, H P Paul, Sebastian Hanel, Pokutta, May 2025b</p>
<p>Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan Mclean, Chandler Smith, Wolfram Barfuss, Jakob Foerster, Tomáš Gavenčiak, Anh The, Edward Han, Vojtěch Hughes, Jan Kovařík, Joel Z Kulveit, Caspar Leibo, Christian Oesterheld, Nisarg Schroeder De Witt, Michael Shah, Paolo Wellman, Theodor Bova, Carson Cimpeanu, Quentin Ezell, Matija Feuillade-Montixi, Esben Franklin, Igor Kran, Max Krawczuk, Niklas Lamparth, Alexander Lauffer, Sumeet Meinke, Motwani, Georgios Piliouras, and Iyad Rahwan. Multi-Agent Risks from Advanced AI. Anka Reuel, Vincent Conitzer, Michael Dennis, Iason Gabriel, Adam Gleave, Gillian Hadfield, Nika Haghtalab, Atoosa Kasirzadeh, Sébastien Krier, Kate Larson, Joel Lehman, David C. ParkesFebruary 2025</p>
<p>Static network structure cannot stabilize cooperation among Large Language Model agents. Jin Han, Balaraju Battu, Ivan Romić, Talal Rahwan, Petter Holme, November 2024</p>
<p>Large language models meet cognitive science: LLMs as tools, models, and participants. Mathew Hardy, Ilia Sucholutsky, Bill Thompson, Tom Griffiths, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society452023</p>
<p>Predicting Results of Social Science Experiments Using Large Language Models. Luke Hewitt, Ashwini Ashokkumar, Isaias Ghezae, Robb Willer, 2024</p>
<p>Integrating explanation and prediction in computational social science. Jake M Hofman, Duncan J Watts, Susan Athey, Filiz Garip, Thomas L Griffiths, Jon Kleinberg, Helen Margetts, Sendhil Mullainathan, Matthew J Salganik, Simine Vazire, Nature. 59578662021</p>
<p>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?. John J Horton, April 2023</p>
<p>Natural language processing for social science research: A comprehensive review. Yuxin Hou, Junming Huang, 10.1177/2057150X241306780Chinese Journal of Sociology. 2057-150X111January 2025</p>
<p>Designing LLM-Agents with Personalities: A Psychometric Approach. Muhua Huang, Xijuan Zhang, Christopher Soto, James Evans, October 2024a</p>
<p>Understanding the planning of LLM agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, February 2024b</p>
<p>Modelling the impact of environmental and social determinants on mental health using generative agents. Joseph Kambeitz, Andreas Meyer-Lindenberg, 10.1038/s41746-024-01422-zDigital Medicine. 2398-635281January 2025</p>
<p>Best practices for implementing ChatGPT, large language models, and artificial intelligence in qualitative and survey-based research. Jonathan Kantor, 10.1016/j.jdin.2023.10.001JAAD International. 2666-328714March 2024</p>
<p>Machine-assisted quantitizing designs: Augmenting humanities and social sciences with artificial intelligence. Andres Karjus, 10.1057/s41599-025-04503-wHumanities and Social Sciences Communications. 2662-9992121February 2025</p>
<p>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. Song Luoma Ke, Peng Tong, Kaiping Cheng, Peng, 2024</p>
<p>Beyond rating scales: With targeted evaluation, large language models are poised for psychological assessment. N E Oscar, Katarina Kjell, H Andrew Kjell, Schwartz, 10.1016/j.psychres.2023.115667Psychiatry Research. 0165-1781333115667March 2024</p>
<p>Intertwining Two Artificial Minds: Chaining GPT and RoBERTa for Emotion Detection. Brice Valentin Kok-Shun, Johnny Chan, Gabrielle Peko, 2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE). 2023</p>
<p>Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations. Maik Larooij, Petter Törnberg, April 2025</p>
<p>Do LLM Agents Exhibit Social Behavior?. Yan Leng, Yuan Yuan, October 2024</p>
<p>CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. December 2023a36</p>
<p>Jiazheng Li, Artem Bobrov, David West, Cesare Aloisi, Yulan He, 10.1609/aaai.v39i28.35358Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApril 2025a39</p>
<p>. Lincan Li, Jiaqi Li, Catherine Chen, Fred Gui, Hongjia Yang, Chenxiao Yu, Zhengguang Wang, Jianing Cai, Junlong , Aaron Zhou, Bolin Shen, Alex Qian, Weixin Chen, Zhongkai Xue, Lichao Sun, Lifang He, Hanjie Chen, Kaize Ding, Zijian Du, Fangzhou Mu, Jiaxin Pei, Jieyu Zhao, Swabha Swayamdipta, Willie Neiswanger, Hua Wei, Xiyang Hu, Shixiang Zhu, Tianlong Chen, Yingzhou Lu, Yang Shi, Lianhui Qin, Tianfan Fu, Zhengzhong Tu, Yuzhe Yang, Jaemin Yoo, Jiaheng Zhang, Ryan Rossi, Liang Zhan, Liang Zhao, Emilio Ferrara, Yan Liu, Furong Huang, Xiangliang Zhang, Lawrence Rothenberg, Shuiwang Ji, Philip S Yu, Yue Zhao, Yushun Dong, December 2024aPolitical-LLM: Large Language Models in Political Science</p>
<p>Lisa Xiang, Neil Li, Daniel D Chowdhury, Tatsunori Johnson, Percy Hashimoto, Sarah Liang, Jacob Schwettmann, Steinhardt, Eliciting Language Model Behaviors with Investigator Agents. February 2025b</p>
<p>MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents. Yuan Li, Yixuan Zhang, Lichao Sun, October 2023b</p>
<p>Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models. Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun, June 2024b</p>
<p>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, August 2023</p>
<p>Can large language models help predict results from a complex behavioural science study?. Steffen Lippert, Anna Dreber, Magnus Johannesson, Warren Tierney, Wilson Cyrus-Lai, Eric Luis Uhlmann, Thomas Pfeiffer, 10.1098/rsos.240682Royal Society Open Science. 119240682September 2024</p>
<p>Two Heads are Better than One: Zero-shot Cognitive Reasoning via Multi-LLM Knowledge Fusion. Liang Liu, Dong Zhang, Shoushan Li, Guodong Zhou, Erik Cambria, 10.1145/3627673.3679744Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM '24. the 33rd ACM International Conference on Information and Knowledge Management, CIKM '24New York, NY, USAAssociation for Computing MachineryOctober 2024</p>
<p>Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. Ryan Lowe, Y I Wu, Aviv Tamar, Jean Harb, Advances in Neural Information Processing Systems. Curran Associates, Inc201730OpenAI Pieter Abbeel, and Igor Mordatch</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, sep 2024a</p>
<p>LLMs and generative agent-based models for complex systems research. Yikang Lu, Alberto Aleta, Chunpeng Du, Lei Shi, Yamir Moreno, 10.1016/j.plrev.2024.10.013Physics of Life Reviews. 1571-064551December 2024b</p>
<p>Automated Social Science: Language Models as Scientist and Subjects. Benjamin S Manning, Kehang Zhu, John J Horton, April 2024</p>
<p>Effects of continuous and discontinuous non-relevant stimulus on creativity. Charles Mille, Olivier Christmann, Sylvain Fleury, Simon Richir, 10.1080/14626268.2022.2082486Digital Creativity. 1462-62683322022</p>
<p>Psychologically-Valid Generative Agents: A Novel Approach to Agent-Based Modeling in Social Sciences. Konstantinos Mitsopoulos, Ritwik Bose, Brodie Mather, Archna Bhatia, Kevin Gluck, Bonnie Dorr, Christian Lebiere, Peter Pirolli, 10.1609/aaaiss.v2i1.27698Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20232</p>
<p>The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games. Mikhail Mozikov, Nikita Severin, Valeria Bodishtianu, Maria Glushanina, Mikhail Baklashkin, Andrey V Savchenko, Ilya Makarov, June 2024</p>
<p>Multi-agent simulation for crisis management. Y Murakami, K Minami, T Kawasoe, T Ishida, 10.1109/KMN.2002.1115175Proceedings. IEEE Workshop on Knowledge Media Networking. IEEE Workshop on Knowledge Media NetworkingJuly 2002</p>
<p>Strengths and weaknesses of qualitative research in social science studies. Kelvin M Mwita, International Journal of Research in Business and Social Science. 1162022</p>
<p>Automatic implicit motive codings are at least as accurate as humans' and 99% faster. J Malte August Håkan Nilsson, Adithya V Runge, Carl Ganesan, N G Viggo, Nikita Lövenstierne, Oscar N E Soni, Kjell, 10.1037/pspp0000544Journal of Personality and Social Psychology. 1939-13152025</p>
<p>Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li, Yichao Lawrence Kq Yan, Caitlyn Zhang, Cheng Heqi Yin, Tianyang Fei, Yunze Wang, Silin Wang, Ming Chen, Liu, Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges. December 2024</p>
<p>Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models. Peter Organisciak, Selcuk Acar, Denis Dumas, Kelly Berthiaume, 10.1016/j.tsc.2023.101356Thinking Skills and Creativity. September 202349101356</p>
<p>Osinga Frans, Science, Strategy and War: The Strategic Theory of John Boyd. Routledge2007</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, 10.1145/3586183.3606763Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23. the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23New York, NY, USAAssociation for Computing MachineryOctober 2023</p>
<p>Enhancing Anomaly Detection in Financial Markets with an LLM-based Multi-Agent Framework. Taejin Park, March 2024</p>
<p>Anthropic's Claude AI is playing Pokémon. | The Verge. Jay Peter, February 2025</p>
<p>The machine psychology of cooperation: Can GPT models operationalise prompts for altruism, cooperation, competitiveness, and selfishness in economic games. Steve Phelps, Yvan I Russell, Journal of Physics: Complexity. 2023</p>
<p>Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing , Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, Yong Li, AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society. February 2025</p>
<p>Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents. Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, Mrinmaya Sachan, Rada Mihalcea, Advances in Neural Information Processing Systems. December 202437</p>
<p>Performance and biases of Large Language Models in public opinion simulation. Yao Qu, Jue Wang, 10.1057/s41599-024-03609-xHumanities and Social Sciences Communications. 2662-9992111August 2024</p>
<p>Beyond the Average: Exploring the Potential and Challenges of Large Language Models in Social Science Research. Mikko Rask, Koki Shimizu, 10.1109/ACDSA59508.2024.104673412024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA). February 2024</p>
<p>Artificial intelligence and qualitative research: The promise and perils of large language model (LLM) 'assistance'. Critical Perspectives on Accounting. John Roberts, Max Baker, Jane Andrew, 10.1016/j.cpa.2024.102722March 202499102722</p>
<p>The Problems of LLM-generated Data in. Luca Rossi, Katherine Harrison, Irina Shklovski, Social Science Research. Sociologica. 1822024</p>
<p>Automating Thematic Analysis with Multi-Agent LLM Systems. Sreecharan Sankaranarayanan, Conrad Borchers, Sebastian Simon, Elham Tajik, Amine Ataş, Berkan Celik, Francesco Balzan, Bahar Shahrokhian, March 2025</p>
<p>Whose Opinions Do Language Models Reflect?. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLRJuly 2023</p>
<p>Using large language models to generate silicon samples in consumer and marketing research: Challenges, opportunities, and guidelines. Marko Sarstedt, Susanne J Adler, Lea Rau, Bernd Schmitt, 10.1002/mar.21982Psychology &amp; Marketing. 1520-67934162024</p>
<p>Google's Gemini AI Is now a Pokémon Master. Hal Eric, Schwartz, May 2025</p>
<p>A collective AI via lifelong learning and sharing at the edge. Andrea Soltoggio, Eseoghene Ben-Iwhiwhu, Vladimir Braverman, Eric Eaton, Benjamin Epstein, Yunhao Ge, Lucy Halperin, Jonathan How, Laurent Itti, Michael A Jacobs, Pavan Kantharaju, Long Le, Steven Lee, Xinran Liu, T Sildomar, David Monteiro, Saptarshi Musliner, Priyadarshini Nath, Christos Panda, Hamed Peridis, Vishwa Pirsiavash, Kaushik Parekh, Shahaf Roy, Shperberg, T Hava, Peter Siegelmann, Kyle Stone, Jingfeng Vedder, Lin Wu, Guangyao Yang, Soheil Zheng, Kolouri, 10.1038/s42256-024-00800-2Nature Machine Intelligence. 2522-583963March 2024</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Chan Jun Shern, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. PaperBench: Evaluating AI's Ability to Replicate AI Research. 2025</p>
<p>Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information. Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J Jansen, Jang Hyun, Kim , February 2024</p>
<p>Jiakai Tang, Heyang Gao, Xuchen Pan, Lei Wang, Haoran Tan, Dawei Gao, Yushuo Chen, Xu Chen, Yankai Lin, Yaliang Li, Bolin Ding, Jingren Zhou, Jun Wang, Ji-Rong Wen, GenSim: A General Social Simulation Platform with Large Language Model based Agents. October 2024</p>
<p>Large language models (LLM) in computational social science: Prospects, current state, and challenges. Social Network Analysis and Mining. Surendrabikram Thapa, Shuvam Shiwakoti, Bikram Siddhant, Surabhi Shah, Hariram Adhikari, Mehwish Veeramani, Usman Nasim, Naseem, 10.1007/s13278-025-01428-9March 2025154</p>
<p>Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design. Lindia Tjuatja, Valerie Chen, Tongshuang Wu, Ameet Talwalkwar, Graham Neubig, 10.1162/tacl_a_00685Transactions of the Association for Computational Linguistics. 2307-387X12September 2024</p>
<p>A LLM digest for social scientist. Daniel Valdenegro, 2023</p>
<p>CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis. Saranya Venkatraman, Nafis Irtiza Tripto, Dongwon Lee, 2025</p>
<p>Why, with whom, and how to conduct interdisciplinary research? A review from a researcher's perspective. Gergana Vladova, Jennifer Haase, Sascha Friesike, 10.1093/scipol/scae070Science and Public Policy. November 2024</p>
<p>Assessing Bias in LLM-Generated Synthetic Datasets: The Case of German Voter Behavior. Anna-Carolina Leah Von Der Heyde, Alexander Haensch, Wenz, 2023</p>
<p>Large language models that replace human participants can harmfully misportray and flatten identity groups. Angelina Wang, Jamie Morgenstern, John P Dickerson, 10.1038/s42256-025-00986-zNature Machine Intelligence. 2522-583973March 2025a</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, 10.1007/s11704-024-40231-1Frontiers of Computer Science. 2095-2236186186345March 2024</p>
<p>Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents. Lei Wang, Zheqing Zhang, Xu Chen, February 2025b</p>
<p>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering. Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, Xin Xia, April 2025c</p>
<p>Evaluating the ability of large language models to emulate personality. Yilei Wang, Jiabao Zhao, Deniz S Ones, Liang He, Xin Xu, 10.1038/s41598-024-84109-5Scientific Reports. 2045-2322151519January 2025d</p>
<p>LLM-Based Empathetic Response Through Psychologist-Agent Debate. Yijie Wu, Shi Feng, Ming Wang, Daling Wang, Yifei Zhang, 10.1007/978-981-97-7232-2_14Web and Big Data. Wenjie Zhang, Anthony Tung, Zhonglong Zheng, Zhengyi Yang, Xiaoyang Wang, Hongjie Guo, SingaporeSpringer Nature2024</p>
<p>AI-Driven Virtual Teacher for Enhanced Educational Efficiency: Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction. Tianlong Xu, Yifan Zhang, Zhendong Chu, Shen Wang, Qingsong Wen, 10.1609/aaai.v39i28.35144Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApril 202539</p>
<p>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, May 2024</p>
<p>Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. April 2025</p>
<p>PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents. Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang, August 2024a</p>
<p>LLM-Measure: Generating Valid, Consistent, and Reproducible Text-Based Measures for Social Science Research. Yi Yang, Hanyu Duan, Jiaxin Liu, Kar Yan, Tam , September 2024b</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, March 2023</p>
<p>Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings. Leo Yeykelis, Kaavya Pichai, James J Cummings, Byron Reeves, August 2024</p>
<p>Measuring Social Norms of Large Language Models. Ye Yuan, Kexin Tang, Jianhao Shen, Ming Zhang, Chenguang Wang, May</p>
<p>On Generative Agents in Recommendation. An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, Tat-Seng Chua, 10.1145/3626772.3657844Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24New York, NY, USAAssociation for Computing MachineryJuly 2024a</p>
<p>Creative Agents: Empowering Agents with Imagination for Creative Tasks. Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, Zongqing Lu, December 2023</p>
<p>Distributed cognition, representation, and affordance. Jiajie Zhang, L Vimla, Patel, 10.1075/pc.14.2.12zhaPragmatics &amp; Cognition. 0929-0907142January 2006</p>
<p>Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View. Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin Deng, May 2024b</p>
<p>EduPlanner: LLM-Based Multiagent Systems for Customized and Intelligent Instructional Design. Xueqiao Zhang, Chao Zhang, Jianwen Sun, Jun Xiao, Yi Yang, Yawei Luo, 10.1109/TLT.2025.3561332IEEE Transactions on Learning Technologies. 1939-1382182025</p>
<p>A Survey on the Memory Mechanism of Large Language Model based Agents. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, April 2024c</p>
<p>Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhanxin Hao, Jianxiao Jiang, Jie Cao, Huiqin Liu, Zhiyuan Liu, Lei Hou, Juanzi Li, Simulating Classroom Education with LLM-Empowered Agents. November 2024d</p>
<p>CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents. Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, June 2024</p>
<p>Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs. Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap, October 2024</p>
<p>Can large language models transform computational social science? Computational Linguistics. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, 202450</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8896 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8896</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8896</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-1805fa1f126cd66dceb9287a398e4f1f3111b1ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1805fa1f126cd66dceb9287a398e4f1f3111b1ad" target="_blank">Parallel distributed processing</a></p>
                <p><strong>Paper Venue:</strong> Psyke &amp; Logos</p>
                <p><strong>Paper TL;DR:</strong> The notion that PDP approaches provide a sub-symbolic account of cognitive processes, in contrast to theclassical symbolic view, is examined and speculation concerning the explanatory power of PDP systems at the cognitive level of functioning is suggested.</p>
                <p><strong>Paper Abstract:</strong> After briefly reviewing the appealing psychological properties of PDP systems, an introduction to their historical roots and basic computational mechanisms are provided. A variety of network architectures are described including one-layered perceptrons, backpropagation networks, Boltzmann machines and recurrent systems. Three PDP simulations are analysed: First, a model that purports to learn the past tense of English verbs; Second, a constraint satisfaction network which is able to interpret the alternative configurations of a Necker cube; Finally, a recurrent network which is able to decipher membership of grammatical classes from word-order information. The notion that PDP approaches provide a sub-symbolic account of cognitive processes, in contrast to theclassical symbolic view, is examined. The article concludes with brief speculation concerning the explanatory power of PDP systems at the cognitive level of functioning.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8896.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8896.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributed representation (PDP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Distributed (Distributed) Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Knowledge is represented as patterns of graded activity distributed over many simple processing units; concepts correspond to global states or activation patterns rather than single local symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>distributed representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are encoded as multi-dimensional patterns of activation across many units; no single unit uniquely denotes a concept. Functional operations are implemented by transforming these activation patterns via weighted connections and network dynamics (e.g., attractor states in an energy landscape).</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>past-tense learning and generalisation, graceful degradation in damaged networks, classification (Exclusive OR), perceptual disambiguation (Necker cube), word-order/lexical class discovery</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PDP networks learn mappings and generalise to novel inputs (e.g., past-tense mappings produce U-shaped learning curves similar to child data), show graceful degradation under damage (robustness), hidden units can extract problem-relevant dimensions (e.g., kinship dimensions), and recurrent distributed states capture sequential structure (Elman). Quantitatively, Rumelhart & McClelland's simulation produced U-shaped curves for irregular verbs; networks solved non-linear mappings (Exclusive OR) with hidden layers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted with symbolic/localist (rule-based) formats: PDP rejects discrete rule-execution and single-node concept tokens, arguing distributed formats naturally capture graded behaviour, robustness, and emergent prototypes; symbolic accounts are characterised as more nativist and brittle in the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Criticisms include sensitivity to input assumptions (e.g., Pinker & Prince argued the Rumelhart & McClelland U-shaped effect was an artifact), difficulties in interpreting specific distributed encodings and decoding outputs, and early perceptron limitations for non-linear problems (necessitating hidden units/backprop).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Distributed representations functionally support generalisation, context-sensitivity, graceful degradation, and emergent prototype-like representations; they provide a micro-structural, learning-based alternative to symbolic rule systems for conceptual knowledge and cognitive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8896.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8896.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic / Localist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic (Localist) Representation / Rule-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts are represented by discrete symbols or dedicated nodes and processed by explicit rules; knowledge is stored as structured symbolic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>symbolic (localist) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts correspond to identifiable symbolic tokens or dedicated units (localist nodes); cognitive operations are performed by explicit rule application and symbol manipulation rather than by distributed transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / localist</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>explanation of categorical behaviour in language (rules for past tense), claims about compositional syntactic processing, and accounts of serial order problems historically (e.g., transformational grammar, formal symbolic grammars)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper argues symbolic approaches elegantly capture rule-like aspects of language and have historically explained serial-order and rule-like phenomena, but they are criticised for failing to account for graded errors, robustness to damage, and emergent generalisation shown by PDP models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Presented as the main alternative to distributed formats: symbolic/localist favored by classical cognitive theories for explicit rule-structures; the paper contends PDP can subsume many phenomena without explicit rule invocation, though symbolic accounts remain strong for some procedural, compositional descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Symbolic accounts are described as less able to capture fine-grained graded behaviour, robustness to noise and damage, and learning-driven emergent structure; PDP proponents argue symbolic rules are convenient approximations rather than micro-level explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>If conceptual knowledge were strictly symbolic, explanations would rely on innate rule inventories; the paper implies that a functional explanation in terms of learned distributed patterns offers a more parsimonious account for many phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8896.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8896.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototypes (emergent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototypical representations (emergent in PDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prototypical category representations emerge naturally from learning in distributed networks as central attractor states reflecting frequently co-occurring features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>prototype representation (emergent)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Category knowledge is functionally represented by attractor states or common activation patterns that serve as prototypes; items are judged by similarity to these emergent central patterns rather than by membership rules.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based / distributed / prototype</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>categorisation, generalisation to novel items, capturing typicality and graded category behaviour</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PDP networks naturally form prototypical representations during learning and judge new inputs by similarity to these representations; this property is used to explain graded, non-categorical behaviour in tasks across simulations described.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Prototype-like distributed representations are contrasted with categorical rule-based accounts; PDP prototypes explain graded and similarity-driven phenomena that symbolic categorical rules struggle to capture.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>The paper notes that not all human categorical phenomena are purely prototype-based and that emergent prototypes depend on input statistics and architecture; certain rule-like regularities may appear when networks behave categorically under particular conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Prototypes implemented as attractor states provide a functional mechanism for typicality effects, graded category membership, and generalisation without explicit rule storage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8896.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8896.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constraint-satisfaction / Frames</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constraint-satisfaction representations (frames/scripts as energy-landscape attractors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Knowledge is functionally represented as mutually constraining hypotheses/units whose interactions define an energy (goodness) landscape where stable attractors correspond to coherent interpretations or frame configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>constraint-satisfaction / frame/script (attractor) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual/interpretive structures are implemented as networks of mutually excitatory and inhibitory units that encode constraints; the global pattern of activation that maximizes constraint satisfaction (an attractor in the energy landscape) corresponds to a coherent frame or script interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / dynamic / frame-like</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>perceptual disambiguation (Necker cube), interpretation under contextual constraints, multi-cue integration, script/frame activation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Constraint networks produce multiple stable states corresponding to alternative interpretations (e.g., the two standard Necker cube interpretations and local minima for impossible interpretations); hill-climbing on goodness/energy landscapes explains spontaneous selection and switching between interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>These frame-like distributed representations implement frame/script behaviour without symbolic frame slots or a supervising executive; they are favored for explaining context-sensitive, interactive constraint resolution compared to stepwise symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Constraint nets can become trapped in local minima (suboptimal interpretations); Boltzmann learning/simulated annealing is invoked to avoid local minima, indicating limitations in pure deterministic relaxation dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, frames/scripts can be realized as attractor configurations in distributed networks, providing a unified dynamic account of context-dependent interpretation and non-categorical, graded responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8896.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8896.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wickelfeature encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wickelfeature / Wickelphone distributed phonological encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific distributed encoding that represents phonological inputs as sets of local trigram-like phonetic feature units (Wickelfeatures) for mapping stems to inflected forms in a network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Wickelfeature (feature-based distributed encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Phonological forms are functionally encoded as vectors of overlapping local phonetic feature triples (Wickelphones/Wickelfeatures); these distributed feature vectors serve as input/output codes for mapping present to past tense forms.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based / distributed</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>past-tense learning and decoding (Rumelhart & McClelland simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used in Rumelhart & McClelland's past-tense simulation to allow a single-layer associator to map stems to past forms; the simulation replicated U-shaped developmental curves and classes of errors observed in children to some extent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Wickelfeatures are a distributed, sublexical encoding intended to replace symbolic morpheme-based encodings; critics (Pinker & Prince) challenge whether this encoding and the model's results genuinely supplant rule-based explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Criticised for decoding difficulties and questionable adequacy as a phonological representation; Pinker & Prince argue that some observed effects in the simulation are artifacts of training regimes or input assumptions rather than the encoding per se.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, sublexical distributed feature encodings can support mapping and generalisation without explicit rule representations, but their psychological plausibility and decoding are controversial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8896.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8896.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hidden-unit emergent features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hidden-unit emergent internal representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representations that emerge in the internal (hidden) layers of multi-layer networks, capturing salient problem dimensions (e.g., gender, generation in kinship) or category structure without explicit supervision to encode those features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>hidden-unit emergent representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Intermediate (hidden) units form distributed codes that functionally capture statistical regularities and salient dimensions of the input domain; these internal vectors can be clustered or read out to reveal conceptual structure.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / emergent feature-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>kinship representation, category discovery, lexical class clustering from word-order (Elman), feature extraction and compression</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hidden units self-organise to reflect problem structure (Hinton's kinship example), and Elman's recurrent network produced hidden-unit similarity structures that cluster into grammatical/semantic classes (nouns/verbs, animate/inanimate distinctions) even though words were arbitrary symbols in input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Hidden-unit emergent codes provide a middle ground between raw distributed patterns and symbolic tokens: they enable extraction of interpretable dimensions without pre-specified symbolic categories, contrasting with explicit symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Emergent representations can be difficult to interpret; similarity-based clusters may not correspond to semantic meaning (Elman notes semantics are illusory here because inputs were random), and representational content depends on task and input statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, hidden-layer representations implement abstraction and dimensional reduction that support categorisation, prediction, and the emergence of proto-structured conceptual knowledge from sequential and distributional cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8896.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8896.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context-unit / recurrent state</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual / Recurrent state representations (context units)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Temporal/contextual representations implemented by recurrent connections or dedicated context units that store previous hidden states, enabling representation of serial order and sequence-dependent conceptual structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>contextual / recurrent state representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>The network maintains a functional memory of prior states by copying previous hidden activations into context units (or via recurrent loops); current processing therefore depends on both input and an embedded representation of preceding context, enabling temporally extended conceptual encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / dynamic / temporal</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>word-order prediction, discovery of lexical classes from distributional context, capture of serial-order dependencies in language</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Elman's recurrent network using context units learned to predict next-word distributions and produced hidden-state vectors that, when clustered, revealed grammatical classes; the recurrent/contextual representation enabled sensitivity to long-range dependencies absent in simple Markov models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Recurrent/contextual distributed states extend static distributed encodings by embedding temporal structure, contrasting with symbolic sequential rules but providing a mechanism for serial-order representation without explicit grammar rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Predictive performance was limited (networks failed to perfectly memorise sequences), and the emergent structural knowledge did not equate to a full grammatical description; context-based representations depend strongly on architecture and training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, context/recurrent states provide a mechanism by which distributional and sequential information can be converted into stable internal codes that support category discovery, prediction, and sequence-sensitive conceptual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel distributed processing', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On Learning the Past Tenses of English Verbs <em>(Rating: 2)</em></li>
                <li>Learning representations by back-propagating errors <em>(Rating: 2)</em></li>
                <li>A learning algorithm for Boltzmann machines <em>(Rating: 2)</em></li>
                <li>Neural networks and physical systems with emergent collective computational abilities <em>(Rating: 2)</em></li>
                <li>Finding Structure in Time <em>(Rating: 2)</em></li>
                <li>On language and connectionism: Analysis of Rumelhart and McClelland's model of the past tense <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8896",
    "paper_id": "paper-1805fa1f126cd66dceb9287a398e4f1f3111b1ad",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Distributed representation (PDP)",
            "name_full": "Parallel Distributed (Distributed) Representation",
            "brief_description": "Knowledge is represented as patterns of graded activity distributed over many simple processing units; concepts correspond to global states or activation patterns rather than single local symbols.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "distributed representation",
            "representational_format_description": "Concepts are encoded as multi-dimensional patterns of activation across many units; no single unit uniquely denotes a concept. Functional operations are implemented by transforming these activation patterns via weighted connections and network dynamics (e.g., attractor states in an energy landscape).",
            "format_type": "distributed / connectionist",
            "cognitive_task_or_phenomenon": "past-tense learning and generalisation, graceful degradation in damaged networks, classification (Exclusive OR), perceptual disambiguation (Necker cube), word-order/lexical class discovery",
            "key_findings": "PDP networks learn mappings and generalise to novel inputs (e.g., past-tense mappings produce U-shaped learning curves similar to child data), show graceful degradation under damage (robustness), hidden units can extract problem-relevant dimensions (e.g., kinship dimensions), and recurrent distributed states capture sequential structure (Elman). Quantitatively, Rumelhart & McClelland's simulation produced U-shaped curves for irregular verbs; networks solved non-linear mappings (Exclusive OR) with hidden layers.",
            "comparison_with_other_formats": "Contrasted with symbolic/localist (rule-based) formats: PDP rejects discrete rule-execution and single-node concept tokens, arguing distributed formats naturally capture graded behaviour, robustness, and emergent prototypes; symbolic accounts are characterised as more nativist and brittle in the paper's discussion.",
            "limitations_or_counter_evidence": "Criticisms include sensitivity to input assumptions (e.g., Pinker & Prince argued the Rumelhart & McClelland U-shaped effect was an artifact), difficulties in interpreting specific distributed encodings and decoding outputs, and early perceptron limitations for non-linear problems (necessitating hidden units/backprop).",
            "theoretical_claims_or_implications": "Distributed representations functionally support generalisation, context-sensitivity, graceful degradation, and emergent prototype-like representations; they provide a micro-structural, learning-based alternative to symbolic rule systems for conceptual knowledge and cognitive performance.",
            "uuid": "e8896.0",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Symbolic / Localist",
            "name_full": "Symbolic (Localist) Representation / Rule-based representation",
            "brief_description": "Concepts are represented by discrete symbols or dedicated nodes and processed by explicit rules; knowledge is stored as structured symbolic expressions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "symbolic (localist) representation",
            "representational_format_description": "Concepts correspond to identifiable symbolic tokens or dedicated units (localist nodes); cognitive operations are performed by explicit rule application and symbol manipulation rather than by distributed transformations.",
            "format_type": "symbolic / localist",
            "cognitive_task_or_phenomenon": "explanation of categorical behaviour in language (rules for past tense), claims about compositional syntactic processing, and accounts of serial order problems historically (e.g., transformational grammar, formal symbolic grammars)",
            "key_findings": "Paper argues symbolic approaches elegantly capture rule-like aspects of language and have historically explained serial-order and rule-like phenomena, but they are criticised for failing to account for graded errors, robustness to damage, and emergent generalisation shown by PDP models.",
            "comparison_with_other_formats": "Presented as the main alternative to distributed formats: symbolic/localist favored by classical cognitive theories for explicit rule-structures; the paper contends PDP can subsume many phenomena without explicit rule invocation, though symbolic accounts remain strong for some procedural, compositional descriptions.",
            "limitations_or_counter_evidence": "Symbolic accounts are described as less able to capture fine-grained graded behaviour, robustness to noise and damage, and learning-driven emergent structure; PDP proponents argue symbolic rules are convenient approximations rather than micro-level explanations.",
            "theoretical_claims_or_implications": "If conceptual knowledge were strictly symbolic, explanations would rely on innate rule inventories; the paper implies that a functional explanation in terms of learned distributed patterns offers a more parsimonious account for many phenomena.",
            "uuid": "e8896.1",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Prototypes (emergent)",
            "name_full": "Prototypical representations (emergent in PDP)",
            "brief_description": "Prototypical category representations emerge naturally from learning in distributed networks as central attractor states reflecting frequently co-occurring features.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "prototype representation (emergent)",
            "representational_format_description": "Category knowledge is functionally represented by attractor states or common activation patterns that serve as prototypes; items are judged by similarity to these emergent central patterns rather than by membership rules.",
            "format_type": "feature-based / distributed / prototype",
            "cognitive_task_or_phenomenon": "categorisation, generalisation to novel items, capturing typicality and graded category behaviour",
            "key_findings": "PDP networks naturally form prototypical representations during learning and judge new inputs by similarity to these representations; this property is used to explain graded, non-categorical behaviour in tasks across simulations described.",
            "comparison_with_other_formats": "Prototype-like distributed representations are contrasted with categorical rule-based accounts; PDP prototypes explain graded and similarity-driven phenomena that symbolic categorical rules struggle to capture.",
            "limitations_or_counter_evidence": "The paper notes that not all human categorical phenomena are purely prototype-based and that emergent prototypes depend on input statistics and architecture; certain rule-like regularities may appear when networks behave categorically under particular conditions.",
            "theoretical_claims_or_implications": "Prototypes implemented as attractor states provide a functional mechanism for typicality effects, graded category membership, and generalisation without explicit rule storage.",
            "uuid": "e8896.2",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Constraint-satisfaction / Frames",
            "name_full": "Constraint-satisfaction representations (frames/scripts as energy-landscape attractors)",
            "brief_description": "Knowledge is functionally represented as mutually constraining hypotheses/units whose interactions define an energy (goodness) landscape where stable attractors correspond to coherent interpretations or frame configurations.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "constraint-satisfaction / frame/script (attractor) representation",
            "representational_format_description": "Conceptual/interpretive structures are implemented as networks of mutually excitatory and inhibitory units that encode constraints; the global pattern of activation that maximizes constraint satisfaction (an attractor in the energy landscape) corresponds to a coherent frame or script interpretation.",
            "format_type": "distributed / dynamic / frame-like",
            "cognitive_task_or_phenomenon": "perceptual disambiguation (Necker cube), interpretation under contextual constraints, multi-cue integration, script/frame activation",
            "key_findings": "Constraint networks produce multiple stable states corresponding to alternative interpretations (e.g., the two standard Necker cube interpretations and local minima for impossible interpretations); hill-climbing on goodness/energy landscapes explains spontaneous selection and switching between interpretations.",
            "comparison_with_other_formats": "These frame-like distributed representations implement frame/script behaviour without symbolic frame slots or a supervising executive; they are favored for explaining context-sensitive, interactive constraint resolution compared to stepwise symbolic inference.",
            "limitations_or_counter_evidence": "Constraint nets can become trapped in local minima (suboptimal interpretations); Boltzmann learning/simulated annealing is invoked to avoid local minima, indicating limitations in pure deterministic relaxation dynamics.",
            "theoretical_claims_or_implications": "Functionally, frames/scripts can be realized as attractor configurations in distributed networks, providing a unified dynamic account of context-dependent interpretation and non-categorical, graded responses.",
            "uuid": "e8896.3",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Wickelfeature encoding",
            "name_full": "Wickelfeature / Wickelphone distributed phonological encoding",
            "brief_description": "A specific distributed encoding that represents phonological inputs as sets of local trigram-like phonetic feature units (Wickelfeatures) for mapping stems to inflected forms in a network.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Wickelfeature (feature-based distributed encoding)",
            "representational_format_description": "Phonological forms are functionally encoded as vectors of overlapping local phonetic feature triples (Wickelphones/Wickelfeatures); these distributed feature vectors serve as input/output codes for mapping present to past tense forms.",
            "format_type": "feature-based / distributed",
            "cognitive_task_or_phenomenon": "past-tense learning and decoding (Rumelhart & McClelland simulation)",
            "key_findings": "Used in Rumelhart & McClelland's past-tense simulation to allow a single-layer associator to map stems to past forms; the simulation replicated U-shaped developmental curves and classes of errors observed in children to some extent.",
            "comparison_with_other_formats": "Wickelfeatures are a distributed, sublexical encoding intended to replace symbolic morpheme-based encodings; critics (Pinker & Prince) challenge whether this encoding and the model's results genuinely supplant rule-based explanations.",
            "limitations_or_counter_evidence": "Criticised for decoding difficulties and questionable adequacy as a phonological representation; Pinker & Prince argue that some observed effects in the simulation are artifacts of training regimes or input assumptions rather than the encoding per se.",
            "theoretical_claims_or_implications": "Functionally, sublexical distributed feature encodings can support mapping and generalisation without explicit rule representations, but their psychological plausibility and decoding are controversial.",
            "uuid": "e8896.4",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Hidden-unit emergent features",
            "name_full": "Hidden-unit emergent internal representations",
            "brief_description": "Representations that emerge in the internal (hidden) layers of multi-layer networks, capturing salient problem dimensions (e.g., gender, generation in kinship) or category structure without explicit supervision to encode those features.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "hidden-unit emergent representation",
            "representational_format_description": "Intermediate (hidden) units form distributed codes that functionally capture statistical regularities and salient dimensions of the input domain; these internal vectors can be clustered or read out to reveal conceptual structure.",
            "format_type": "distributed / emergent feature-based",
            "cognitive_task_or_phenomenon": "kinship representation, category discovery, lexical class clustering from word-order (Elman), feature extraction and compression",
            "key_findings": "Hidden units self-organise to reflect problem structure (Hinton's kinship example), and Elman's recurrent network produced hidden-unit similarity structures that cluster into grammatical/semantic classes (nouns/verbs, animate/inanimate distinctions) even though words were arbitrary symbols in input.",
            "comparison_with_other_formats": "Hidden-unit emergent codes provide a middle ground between raw distributed patterns and symbolic tokens: they enable extraction of interpretable dimensions without pre-specified symbolic categories, contrasting with explicit symbolic representations.",
            "limitations_or_counter_evidence": "Emergent representations can be difficult to interpret; similarity-based clusters may not correspond to semantic meaning (Elman notes semantics are illusory here because inputs were random), and representational content depends on task and input statistics.",
            "theoretical_claims_or_implications": "Functionally, hidden-layer representations implement abstraction and dimensional reduction that support categorisation, prediction, and the emergence of proto-structured conceptual knowledge from sequential and distributional cues.",
            "uuid": "e8896.5",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Context-unit / recurrent state",
            "name_full": "Contextual / Recurrent state representations (context units)",
            "brief_description": "Temporal/contextual representations implemented by recurrent connections or dedicated context units that store previous hidden states, enabling representation of serial order and sequence-dependent conceptual structure.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "contextual / recurrent state representation",
            "representational_format_description": "The network maintains a functional memory of prior states by copying previous hidden activations into context units (or via recurrent loops); current processing therefore depends on both input and an embedded representation of preceding context, enabling temporally extended conceptual encodings.",
            "format_type": "distributed / dynamic / temporal",
            "cognitive_task_or_phenomenon": "word-order prediction, discovery of lexical classes from distributional context, capture of serial-order dependencies in language",
            "key_findings": "Elman's recurrent network using context units learned to predict next-word distributions and produced hidden-state vectors that, when clustered, revealed grammatical classes; the recurrent/contextual representation enabled sensitivity to long-range dependencies absent in simple Markov models.",
            "comparison_with_other_formats": "Recurrent/contextual distributed states extend static distributed encodings by embedding temporal structure, contrasting with symbolic sequential rules but providing a mechanism for serial-order representation without explicit grammar rules.",
            "limitations_or_counter_evidence": "Predictive performance was limited (networks failed to perfectly memorise sequences), and the emergent structural knowledge did not equate to a full grammatical description; context-based representations depend strongly on architecture and training regime.",
            "theoretical_claims_or_implications": "Functionally, context/recurrent states provide a mechanism by which distributional and sequential information can be converted into stable internal codes that support category discovery, prediction, and sequence-sensitive conceptual knowledge.",
            "uuid": "e8896.6",
            "source_info": {
                "paper_title": "Parallel distributed processing",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On Learning the Past Tenses of English Verbs",
            "rating": 2
        },
        {
            "paper_title": "Learning representations by back-propagating errors",
            "rating": 2
        },
        {
            "paper_title": "A learning algorithm for Boltzmann machines",
            "rating": 2
        },
        {
            "paper_title": "Neural networks and physical systems with emergent collective computational abilities",
            "rating": 2
        },
        {
            "paper_title": "Finding Structure in Time",
            "rating": 2
        },
        {
            "paper_title": "On language and connectionism: Analysis of Rumelhart and McClelland's model of the past tense",
            "rating": 2
        }
    ],
    "cost": 0.013804249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PARALLEL DISTRIBUTED PROCESSING ${ }^{1}$</h1>
<p>Kim Plunkett</p>
<h2>What is PDP?</h2>
<p>Parallel Distributed Processing (PDP) is a recent development in cognitive science modelling research that assumes that individual components of human information processing are highly interactive and that knowledge of events, concepts and language is represented diffusely in the cognitive system. This distributed feature distinguishes PDP from other connectionist models which make use of localist representations, e.g. a single node in a network standing for an entire concept. The PDP approach developed out of the explicit concern of some cognitive scientists that cognitive models should not be restricted to describing aspects of human behaviour that are idealised abstractions of competence. Rather, a unified model of cognition, straddling the traditional competence/performance distinction, has emerged as a worthwhile and realistic research endeavour. Cognitive scientists, working from a PDP perspective, have begun to construct models of human cognition that show promise that their ambitious goals might be achieved. To date, successes range from simulations at the low end of cognition that model speech perception or interpret the different configurations of a necker cube to higher order cognitive skills like sentence processing or language learning. All these models emphasise the context sensitivity of cognitive processes and they all share the assumption that complex behaviours can emerge from the interaction of relatively simple constituents and their environment.</p>
<p>The characteristics of these systems include; a tendency to account for the details of human behaviour within a single framework that does not require ad hoc assumptions to account for apparent exceptional behaviours; a robustness in performance to distortions in the input stimuli or knowledge base itself; a capacity to learn or organise representations of the environment to which they are exposed - prototypical representations emerge as a natural outcome of the learning process; flexibility in responding in an appropriate manner to situations never experienced before. All these characteristics can be attributed to the parallel, distributed architecture of the systems used to implement the models. Typically, PDP models differ from traditional symbolic accounts of human behaviour in their rejection of the need for rule-based processes. Instead, PDP models offer micro-structural accounts. Rule-based accounts are considered by connectionists as convenient</p>
<p>approximations to a system that is considerably more complex. For example, it is argued that categorial rule-systems cannot capture the fine-grained structure of human behaviour in a natural way.</p>
<p>A PDP system's potential for learning is rooted in its high sensitivity to variations in patterns of stimulation and to the structure of the environment to which it is exposed. By manipulating the pattern of connections between its component parts, a network is able to exploit new patterns of stimulation from the environment to create new input/output functions and hence demonstrate new behaviour. All this is achieved with a minimum amount of pre-wiring, i.e. the network organises itself often constructing subtle internal representations of the environment that it is processing. For example, the construction of prototypical representations of environments to which it has never been exposed is a powerful property of such a system. In contrast, many current cognitivist models of learning are highly nativist in their approach, typically relegating learning to the process of choosing between a pre-defined set of symbolic parameters that are innately given.</p>
<p>Graceful degradation, i.e. robustness of behaviour in the face of inadequate stimulation or internal damage, is achieved as a result of the distributed representation of knowledge. Many nodes in a network contribute to the representation of any given fact or proposition. One cannot point to the local representation of a concept as one can in a conventional semantic network. The global distribution of activity in the network has to be considered when evaluating its knowledge state. PDP networks are robust in that the overall pattern of activity in a network often remains stable in the face of pertubation. Similarly, the propensity of PDP networks to respond appropriately to new environments reflects the conservative, assimilative nature of their global representations. New patterns of stimulation are judged against old experiences. Networks modify their behaviour by accommodating to the parallel influence of new sources of information that are simultaneously impinging on the system. It is helpful to view a network as a multi-dimensional state space or energy landscape. The precise contours of the landscape vary with the environment in which the network finds itself. Adaptation in the network can be likened to the process of gradient descent. Final behaviour is determined by the parameters corresponding to the lowest level or valley in the energy landscape. Since the landscape varies from one environment to another, output behaviour will accommodate accordingly.</p>
<h1>Historical Roots</h1>
<p>PDP models demonstrate many characteristics that are desirable in simulations of human cognition. Though individual properties can be found in other approaches, it is rare to find a single system embracing so many important features. Proponents of the PDP approach regard the parsimony of</p>
<p>their models as heralding a new era in the study of cognition. However, Parallel Distributed Processing builds upon a long tradition of scientific endeavour which dates back at least as far as the British Empiricist David Hume. Many of the ideas embodied in the PDP approach can be found in writings of William James.</p>
<p>The amount of activity at any given point in the brain-cortex is the sum of the tendencies of all other points to discharge into it, such tendencies being proportional (1) to the number of times the excitement of each other point may have accompanied that of the point in question; (2) to the intensity of such excite. ments; and (3) to the absence of any rival point functionally disconnected with the first point, into which the discharges might be diverted. (James, 1892, p. 265).</p>
<p>Theoretical developments that laid the foundations for many of today's models were already underway in the forties and fifties (Hebb, 1949; McCulloch \&amp; Pitts, 1943; Rosenblatt, 1959). Yet PDP has emerged as a new perspective only within the last six or seven years (Hinton \&amp; Anderson, 1981; Rumelhart \&amp; McClelland, 1986a). To understand the relationship between PDP and earlier, related approaches, it is necessary to review some of the core concepts of PDP. This brief review will also serve as an introduction to the more sophisticated simulations described below.</p>
<p>The heart of a parallel distributed processing system is a neural network. A neural network consists of a collection of units connected to each other by a set of pathways. Figure one illustrates an example of a simple network designed to solve the logical OR problem (see below).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure One</p>
<p>The circles represent the units or constitutive nodes of the network and the solid lines indicate the pattern of connectivity between the units. Units take on a variety of activation values that can depend on some function of the net input to a unit from other units, the previous state of the unit itself and input from the external environment. The activation value of a unit determines its</p>
<p>output. A variety of functions of the activation value are typically used to determine the output from a unit: A squashing function constrains the output within certain limits: A threshold function determines which of a limited set of outputs will be produced: A linear function simply passes on the activation value itself. Networks may be homogeneous in that all units use the same output function or they may be heterogeneous in the output functions used. Note that it is unusual to find homogeneous nets of linear units. The range of tasks that such networks can perform is limited and they tend to be unstable (activation values have a potential to explode to enormously high values in linear systems). On the other hand, the input function that maps the net input to a unit onto its activation value is typically linear.</p>
<p>Units communicate with each other by passing their output values to the other units in the system with which they are connected. Connections are almost always unidirectional. The input/output functions of the units, together with their pattern of connectivity, define the architecture of the network. Units can excite or inhibit each other. Each connection may have a positive or negative real value that determines the degree and direction of influence of the source unit on the target unit. Target units may receive inputs from a large number of source units. The strength of the connection between two units is called the weight of the connection. The product of the output value of the source unit and the weight value between the source unit and the target unit determines the contribution of the source unit to the net input to the target unit. If the product is negative then the source unit inhibits the target unit. If the product is positive then the source unit excites the target unit. Some systems treat excitatory input and inhibitory input independently from one another (Grossberg, 1980). However, all the models described in this paper treat the two types of input homogeneously. A zero weight between two units is functionally equivalent to a lack of connection between those units in most systems.</p>
<p>The overall behaviour of the network is determined by the set of weights that define the pattern of connectivity of the system, and by the input of the system. The set of weight values embodies the knowledge of the system with respect to a given set of environmental stimuli. In figure one, the numbers on the solid lines represent the weights of the connections between the various pairs of units. The system consists of two inputs and a single output unit. The input units are linear and simply pass on the values received from the environment to the output unit. The output unit is a linear threshold unit i.e. if the activation value reaches or exceeds a given value, in this case  1 , then it outputs  1 , otherwise it outputs $ 0$ . Logical OR can be represented by the following truth table:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LOGICALOR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LOGICALAND</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">input</td>
<td style="text-align: center;">output</td>
<td style="text-align: center;">input</td>
<td style="text-align: center;">output</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">00</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">01</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>A similar network to that depicted in figure one can represent the boolean function Logical AND. The only change that needs to be made is that the threshold of the output unit must be changed to $ 2$ .</p>
<p>Networks like these have been investigated since the forties (McCulloch and Pitts, 1943) and are typical, though simplified, versions of the neural nets that originally sparked off interest in the area. Larger nets with greater numbers of input and output units can represent more complicated input/ output algorithms. However, their mode of operation is essentially identical to the network described above; input patterns are mapped onto a set of output units via weighted connections. The activation values of the output units, together with their threshold values, determine the resultant pattern of output activity. It is also possible to make networks like these learn, i.e. given an input and a desired output it is possible to manipulate the value of the weights automatically so that the required input/output function is achieved. The learning algorithm originally used by Rosenblatt (1962) is called the Perceptron Learning Rule. According to this rule, the weights between two units are modified if the desired output differs from the actual output. The desired output is determined by a teacher signal. The activation value is determined by the propagation of the input signal through the network. The actual output on each output unit is then compared to the target specified for that unit in the teacher signal. If there is a mismatch between these two values, then all the weighted connections feeding into the given output unit are adjusted according to the following rule:</p>
<p>$$
\Delta w_{i}=\left(t_{p}-o_{p}\right) i_{p i}=\bar{\partial}<em i="i" p="p">{p} i</em>
$$</p>
<p>where $\Delta w_{i}$ specifies the change in weight on the connection from input unit to output unit $p, t_{p}$ specifies the teacher signal for output unit $p, o_{p}$ specifies the actual signal on output unit $p$, and $i_{p i}$ is the output from input unit $i$. The change in threshold on the output unit is given by the following rule:</p>
<p>$$
\Delta \hat{\theta}=-\left(t_{p}-o_{p}\right)=-\bar{\partial}_{p}
$$</p>
<p>This procedure is guaranteed to find a set of weights that produces the correct input/output mappings, provided such a set of weights exists. The Perceptron Learning procedure can be applied to a surprisingly wide range of pro-</p>
<p>blems and is still used in many influential models. However, as Minsky and Papert (1969) pointed out, perceptrons are still quite limited in the class of input/output mappings they can learn.</p>
<p>In particular, perceptrons are unable to solve the Exclusive OR problem. The truth table for Exclusive OR is given below:</p>
<p>EXCLUSIVEOR</p>
<table>
<thead>
<tr>
<th style="text-align: center;">input</th>
<th style="text-align: center;">output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Exclusive OR demands a non-linear partitioning of the problem space (figure two).
<img alt="img-1.jpeg" src="img-1.jpeg" />
<img alt="img-2.jpeg" src="img-2.jpeg" />
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure Two
Geometric representations of the three problems</p>
<p>It is possible for networks to learn a non-linear partitioning only when there is an intermediate level of inhibitory units between the input and output units. The Perceptron Learning rule provides only for the adjustment of weights directly connecting the input and output units. Therefore, perceptrons cannot perform non-linear classifications. Since it is known that the class of problems to which Exclusive OR belongs is common in computer science, the demise of neural net models based on the Perceptron Learning procedure followed swiftly on the publication of Minsky and Papert's book. Like spreading excitation in semantic nets, neural network research in general went out of fashion. For many years, the dominant symbolic approach to computational psychology reigned supreme.</p>
<p>Though neural net research receded into the background in the late sixties and seventies, the effects of Minsky and Papert's critique were as much so-</p>
<p>ciological as scientific (Papert, 1988). Perceptrons represent only a small class of possible network architectures and learning procedures and it was to this small class of networks alone that Minsky and Papert's comments were addressed. However, Minsky and Papert's critique was mistakenly interpreted as applying to neural nets in general and funding for most neural net projects dried up. Nevertheless, during the dark years, a small group of researchers continued to investigate the properties of different neural nets and extend their domain of application. J.A. Anderson (1977), Grossburg (1976) and Kohonen (1977) are notable contributors amongst this group. Many of the principles and insights embodied in PDP research today are due to the sustained efforts of this small group of researchers.</p>
<p>As some cognitive scientists became increasingly dissatisfied with the achievements of the traditional symbolic approach to computational psychology, it was recognised that neural nets possessed precisely the kind of properties which traditional symbolic models seem to lack. Furthermore, cognitive scientists began to recognise that various mathematical tools could help extend the generality of the Perceptron Learning procedure to a greater variety of problems, including Exclusive OR. For example, Rumelhart, Hinton and Williams (1986) describe a learning algorithm called Back Propagation (also known as the Generalised Delta-rule), which specifies a procedure for manipulating the weights in a multi-layered network. Back propagation can be used to solve the Exclusive OR problem. Figure three depicts an example of a network that implements the Exclusive OR truth table above.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure Three
A network for solving Exclusive OR</p>
<p>Notice that the network in figure three contains a layer of units between the two input units and the single output unit. Any units in a network that are not exposed to the external environment but only to other units in the network are called hidden units. Back propagation ${ }^{2}$ provides a method for adjusting the threshold values of these hidden units as well as the weighted connections between hidden units and visible units (visible units receive input directly from the environment or send output directly to the environment).</p>
<p>Recent work with networks using the back propagation algorithm has shown that hidden units often organise themselves in a way that reflects the structure of the problem at hand. For example, Hinton (1986) describes a back propagation network designed to learn kinship relationships. Hidden units in this network organise themselves to represent the salient dimensions in kinship relationships, such as gender and generation. Sometimes, the representations discovered by hidden units parallel human interpretations of a problem. At other times, the hidden units discover partitions of problem space not obvious to humans. For example, a back propagation network discovered a novel solution to the Exclusive OR problem. Hidden units have also been used to filter out the redundancies in an input signal, compressing the information for later retrieval (Ackley, Hinton \&amp; Sejnowski, 1985). The ability of hidden units to extract and represent regularities of the environment to which they are exposed has triggered a controversial discussion of the nature of representation in neural nets. Hidden units have no referential function, and yet they seem to share some properties with the symbolic entities embodied in traditional rule-based accounts of cognition.</p>
<p>Another important refinement in neural network architectures has been the development of the Boltzmann machine. The Boltzmann machine belongs to a class of constraints satisfaction networks that are capable of finding solutions to problems which require the simultaneous fulfilment of a selected set of criteria. For example, the process of language comprehension may be reviewed as a constraint satisfaction problem in which the various component parts of a sentence must be integrated to obtain a coherent interpretation (McClelland \&amp; Kawamoto, 1986). These criteria may be mutually supportive or they may be in competition with each other. In the former case, interactive constraint satisfaction networks quickly converge on a best solution. However, if the criteria are in competition with each other, a network may possess a variety of final stable states. Some of these states (often called local minima) may underestimate the potential of the network to solve the problem at hand. Boltzmann learning is a technique for avoiding these local minima (see figure four) and finding the best possible solution to the problem in a given network.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure Four
Local minimum (A) in an energy landscape
B</p>
<p>Boltzmann learning is a stochastic process inspired by statistical mechanics. Just as misaligned crystals or metal can be realigned by a gradual process of heating and cooling called annealing, so can a network be made to explore the energy landscape of a given problem space until it finds a best possible solution. This process is called simulated annealing. As we shall see in the next section, constraint satisfaction networks are particularly useful for simulating psychological phenomena that involve the disambiguation of situations that demand interpretation under the control of contextual determinants. In contrast to feed-forward, back propagation networks, constraint satisfaction networks are typically more highly interconnected. Such networks need not have an obvious layered-structure and many units may be visible to the environment.</p>
<p>More recently, network architectures have been introduced that attempt to integrate time as a dynamic dimension in the representations embodied within a network. These networks involve the use of recurrent connections (units that feed back on themselves). Recurrent connections permit a network to maintain an image of its previous states. Since these recurrent connections can themselves be manipulated by a variety of learning algorithms, such networks can develop a capacity to predict or control future events. For example, Jordan (1986) describes a system which attempts to deal with the classical problem of serial order in behaviour (Lashley, 1951) through the parallel, distributed implementation of a planning structure. Similarly, Elman (1988) shows how a recurrent network can be used to capture some syntactic properties of sentences without the explicit specification of grammatical rules. The problem of serial order in behaviour was impressively resolved by symbolic accounts of cognition (Miller, Galanter \&amp; Pribram, 1960). Connectionist accounts of this problem are needed if PDP is to be regarded as a serious alternative to the classical symbolic approach. In the final section of this article, we will return to a comparative evaluation of connectionist and symbolic contributions to our understanding of cognition. In the next section, we turn to a presentation of some concrete examples of connectionist simulations of psychological phenomena.</p>
<h1>Connectionist simulations</h1>
<p>In this section, I shall briefly review three connectionist simulations. Each simulation uses a different network architecture and addresses a different type of psychological phenomenon. The first model describes a simulation of the learning of past tense forms of verbs by young children. The heart of this model is a simple perceptron learning system. The second model shows how a constraint satisfaction network can be used to simulate the different interpretations of a knecker cube. Finally, a recurrent network that is able to recover lexical classes from word-order is described.</p>
<h1>Learning the past tense of verbs</h1>
<p>A founding assumption of research in child language is that children, like adults, use language productively. That is, after the initial phases of learning, language usage cannot simply result from mimicking what is heard in the input, but rather children acquire the ability to generate syntactic and morphological combinations that they could never have heard before (Ervin, 1964; Berko, 1958). From most current perspectives, linguistic knowledge is framed in terms of general principles, i.e. rules, which govern the productive and (sometimes) interestingly innovative usage of language. The goal for the acquisitionist has been to work through the various domains of language, outlining how and when children come to master the systems of rules governing the production and comprehension of novel forms and utterances. However, current perspectives also acknowledge that certain pockets of linguistic knowledge do not appear to be rule-governed in the same sense. For example, about 150 or so of the most frequently used verbs in English fall outside the domain of the regular past tense rule, add-ed to the stem. Irregular, or strong verbs (Pinker and Prince, 1988), do not form their past tenses by applying a suffixation process, but rather are memorised as separate lexical entries. These verbs can be grouped into three general categories according to the relationship they exhibit to their present tense form: ${ }^{3}$ (a) identity mapping (or no marking - doing nothing to the stem, e.g., hit $--&gt;$ hit); (b) vowel change (changing the vowel, e.g., come $--&gt;$ came); (c) arbitrary (there is no obvious structural relationship between the present and past tense form, e.g., go $--&gt;$ went).</p>
<p>Across acquisition, the fact that two different types of verbs coexist in the lexicon, one group using a general rule and others not, sometimes presents problems for the language learner. In both naturalistic and experimental contexts, children frequently exploit the regularities of the past tense system, applying a general rule to the irregular exceptions to the rule (e.g., Bybee and Slobin, 1982; Kuczaj, 1977). Children will produce errors such as "goed or sitted in which the regular add-ed ending is applied to verb stems which, in the adult grammar, are not subject to this procedure. In addition, several researchers have noted that the time course in the acquisition of these rules (and their exceptions) suggests that children will get worse in their production of forms before they get better. That is, children make mistakes on some types of past tense forms (e.g., comed) after they have been using the forms correctly (e.g., came). After some extended period of overapplication of the general rule to irregular verbs, children reorganise their lexical categorisations and settle into the correct set or irregular stems and corresponding past tense forms, using both regular and irregular verbs in the past tense. This characterisation of the acquisition of morphological regularities (and irregularities) has been described as a U-shaped developmental course in which children pass through two stages before attaining adult com-</p>
<p>petence in handling the past tense in English (Pinker \&amp; Prince, 1988).
Because these erroneous forms are present only infrequently in the input to children, their timely avoidance of exceptions has been taken as the most convincing piece of behavioural evidence that language learning involves the process of recognising and organising linguistic knowledge into a coherent system of general rules. Why else would children produce erroneous forms such as the overgeneralisations of the add-ed rule? Acquisiton, then, involves the construction of a system of generalised statements about the structure of the lexicon, and the accompanying lists of exceptions to those general rules. Traditionally, the notion of a rule has provided linguists and psychologists with an elegant means to neatly package what children and adults know about the intricacies and complexities of language while at the same time accounting for productive language use. As noted recently by Pinker and Prince (1988),
{language} researchers who could agree on little else have all agreed on one thing: that linguistic knowledge is couched in the form of rules and principles (pg. 74).</p>
<p>Indeed, invoking rules serves to elevate language learning above the level of rote imitation and allows linguists to factor a complex phenomena into simpler components that feed representations into one another (Ibid. pg. 84).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure Five</p>
<p>Recently, work within the PDP perspective has promoted reevaluations of the constructs and processes guiding language acquisition. In an attempt to illustrate the applicability of parallel systems to traditionally rule-based domains, Rumelhart \&amp; McClelland (1986b) set out to capture several of the facts of the acquisition of the English past tense; in particular, that children overgeneralise the add-ed suffix to irregular verbs and that development of this system proceeds along a U-shaped course. The goal of this work was to suggest how an account of language processing and acquisition might be able to avoid the reliance on symbol-manipulating, rule-governed processes, while still capturing these phenomena of acquisition. Rumelhart and McClelland's past tense simulation contains three major components. (See figure five).</p>
<p>First, an encoding devide takes the present tense stems of English verbs, symbolised as binary characters, and converts each stem into a distributed representation of context sensitive phonological features called Wickelfeatures. Output from the encoding device consists of a vector of activation across the set of output units ( 460 in all). ${ }^{4}$ Secondly, a single-layered, pattern association network maps the set of Wickelfeatures, which it takes as input, onto a set of output units. The activity on these output units constitute the Wickelfeature representation of the past tense form of the verb that was originally presented to the simulator in its present tense form. The task of the pattern association network is to learn to map correctly input to output vectors through adjusting the set of weights which connect the input and output units. The adjustment of the weights is achieved by using the error signal obtained from comparing the actual output of the network with the desired output stipulated by a teacher signal. The weights connecting the input and output units of the network are then adjusted using the Perceptron Learning rule. This second component of the simulator is entirely responsible for the learning that is required to map present tense stems of verbs onto their corresponding past tense forms. This mechanism, then, can be seen to be the foundation for the overgeneralisations reported by Rumelhart \&amp; McClelland. The third component of the simulator takes as its input the vector representing the activity of the output units of the pattern associator. Its function is to generate the set of Wickelphones that best fit the output vector description. In principle, the decoder provides a Wickelphone description of the past tense form of the verb that was originally provided in the Wickelphone representation of the present tense stem to the encoder. Several researchers as well as Rumelhart \&amp; McClelland themselves have acknowledged several difficulties with this type of decoding process (Pinker \&amp; Prince, 1988). The usefulness of Wickelfeatures as a technique for encoding linguistic information in networks of this types is not crucial for the issues discussed in this paper, and the reader is referred to the original source for details.</p>
<p>The results of the Rumelhart \&amp; McClelland simulation are important for several reasons. First, within some reasonable limits, the learning curves and overgeneralisations created by the simulation resemble many of the errors and stages of development that children are reported to make in the acquisition of past tense verb forms.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure Six
The percentage of correct features for regular and irregular high-frequency verbs as a function of trials</p>
<p>Figure six shows the U-shaped dip for irregular verbs during the early stages of learning. This regression represents the stage of learning in which irregular verbs are treated as though they are regulars. Even more impressively, Rumelhart \&amp; McClelland's simulation provides distinct learning curves for the different classes of irregulars which closely map the types and relative timing of errors made by young children. For example Kuczaj (1977) reports that past tense errors of the form ated occur later in development than errors which simply add-ed to the present tense stem (eated). Rumelhart \&amp; McClelland's simulator is also delayed in producing these former types of error.</p>
<p>The excitement engendered in the cognitive science community by Rumelhart \&amp; McClelland's simulation was partly to do with the accuracy with which it seemed to be able to mimic young children's behaviour. More importantly, however, the simulation showed how one might construct a developmental model that could acquire linguistic knowledge without assuming much nativist baggage. In particular, the simulation does not rely on rules in any obvious way. Furthermore, the simulation embodied an account of the process of morphological reorganisation which is assumed to be crucial to</p>
<p>the achievement of mature linguistic skills (Bowerman, 1982). The model achieves this bt playing off the learning properties of the pattern association network against the structural relationships implicit in the information it must process, i.e. the regularities of English verb morphology.</p>
<p>The past tense simulation has been severely criticised. In addition to their criticism of the Wickelfeature representation used by Rumelhart \&amp; McClelland, Pinker \&amp; Prince (1988) question the input assumptions of the simulation and point out that the U -shaped developmental curve followed by the simulator is an artifact of the discontinuity in vocabulary size and structure to which the model is exposed. For example, the performance dip for irregular verbs (figure six) occurs just after the proportion of regular to irregular transformations in the training set has been greatly increased. However, Plunkett \&amp; Marchman (in press) show that U-shaped learning can be achieved with continuous input to a back propagation network, provided certain realistic assumptions are made about the relative frequencies of the different classes of verbs. Furthermore, the Plunkett \&amp; Marchman simulation provides information as to the conditions under which such a network acts as if it is learning a set of rules and the conditions when behaviour is less categorical. Despite the qualms of classical symbol theorists, the PDP approach still holds out the promise of an alternative developmental account for acquisition and the potential for a new approach to language processing.</p>
<h1>Interpreting the Necker cube</h1>
<p>Constructivist accounts of human perception often cite the shifting interpretations of the necker cube as testifying to the top-down nature of cognitive processes. The necker cube can be interpreted in different ways because we are able to project distinct orientation models onto one and the same stimulus set. On this view, the interpretation of a necker cube is contingent upon the construction of an internal representation. More recently (Feldman, 1981 and Rumelhart, Smolensky, McClelland \&amp; Hinton, 1986), connectionists have shown how the orientation of a necker cube can be computed through the interaction of mental models and bottom-up visual information in a constraint satisfaction network. Figure 7 provides a summary of some of the constraints involved in perceiving the necker cube.</p>
<p>The bottom part of the figure illustrates the necker cube itself whilst the top part of the figure illustrates two interconnected networks, each representing alternative interpretations of the necker cube. Each unit in the network represents a hypothesis about the correct interpretation of a vertex of a necker cube. For example, the unit in the lower left-hand part of the network represents the hypothesis that the lower left-hand vertex of the drawing is a front lower left (FLL) vertex. The upper right-hand unit of the network represents</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure Seven
the hypothesis that the upper right-hand vertex of the necker cube represents a front upper right (FUR) vertex. Notice that both these interpretations are inconsistent. Normally, only one of those vertices can occupy concurrently the frontal plane of the cube at a time. Figure 7 also illustrates the different types of constraints operating in the network. Since each vertex can have only one interpretation, alternative interpretations of the same vertex are connected by inhibitory weights (BUL - - -&gt; FUL). Similarly, since the same interpretation cannot be given to more than one vertex, units representing the same interpretation are mutually inhibiting (BLL - - -&gt; BLL). Thirdly, units that represent locally consistent interpretations are mutually exciting (FUL - - -&gt; FLL). Without these excitatory and inhibitory constraints, it is extremely unlikely that the network will find a solution corresponding to the correct interpretation of the necker cube (all the units in one sub-network turned on and all the units in the other sub-network turned off). In fact, without any constraints whatsoever, there are in principle $2^{16}$ possible configurations of the network. However, once constraints are introduced to the system, the likehood of many of these states occurring is considerably reduced. Hopfield (1982) has shown that the behaviour of constraint satisfaction networks can be characterised as a process of hill-climbing ${ }^{5}$ on a goodness contour. Since many of the units in a constraint satisfaction</p>
<p>network, like the one depicted in Figure 7 above, are in competition with each other, it is possible to evaluate any given configuration of the network as having a particular goodness of fit. Goodness of fit depends essentially on the extent to which each unit satisfies the constraints imposed upon it by other units, the likehood of an individual unit turning on itself (bias), and finally direct evidence from the input that suggests a given unit should turn on. We can summarise the goodness of fit in a constraint satisfaction network for all units in the network in the following equation:</p>
<p>Goodness $=\Sigma_{\mathrm{ij}} \mathrm{w}<em _mathrm_i="\mathrm{i">{\mathrm{ij}} \mathrm{a}</em>}} \mathrm{a<em _mathrm_i="\mathrm{i">{\mathrm{j}}+\Sigma</em>$ input $}<em _mathrm_i="\mathrm{i">{\mathrm{i}} \mathrm{a}</em>$ bias $}}+\Sigma_{\mathrm{i}<em _mathrm_i="\mathrm{i">{\mathrm{i}} \mathrm{a}</em>$
where $\mathrm{w}_{\mathrm{ij}}$ represents the weight connecting unit j to unit i , and $\gg \mathrm{a}$ " represents the activation of a given unit.}</p>
<p>Given this equation and an algorithm that stipulates an updating procedure for the units in a network, one can describe an energy landscape or goodness contour that corresponds to the various configurations of the network. In this way, local computational operations, in which each unit adjusts its activation up or down on the basis of its net input, serve to allow the network to converge towards states that maximise a global measure of goodness or degree of constraint satisfaction. For example, assume that a network like that depicted in figure 7 , with the stipulated set of excitatory and inhibitory connections, is provided with a set of randomly assigned biases on each of the 16 units. A bias determines the probability that a given unit will turn on or off. If the network is allowed to run for a succession of discrete times steps, the activation values of each of the units will adjust themselves in such a way as to relax into a stable global state or maximum goodness of fit for the network.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure Eight</p>
<p>Figure 8 depicts a goodness contour for the possible configurations of the necker cube network. We may interpret this picture as a visual conceptualisation of the possible states of the network. The low point $(0,0)$ corner, corresponds to the start state in which no units are turned on. The peaks on the left and right correspond to the standard interpretations of the necker cube. These goodness peaks are the states to which the network will most often be attracted in the relaxation process. The choice between interpretations is determined by the position on the goodness contour at which the network starts and the particular sequence of updates that is chosen for the units in the network. Thus, it is possible to push the network to a particular interpretation of the necker cube by giving a large bias to one or more of the units. For example, notice that the goodness contour in figure 8 contains a number of smaller peaks. These peaks represent impossible interpretations of the necker cube such as that depicted in figure 9 in which two surfaces are interpreted as being foremost.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure Nine
Three interpretations of the necker cube</p>
<p>Since these peaks represent local maxima on the goodness contour, the hillclimbing process halts and the network remains in this stable, though impossible, state.</p>
<p>Dynamic accounts of the interpretative process show us how a single ambiguous source of information can be resolved into a definite single solution. The goodness or energy landscape provides a global picture of the characteristics of any given constraint satisfaction network. The process of hillclimbing describes the process of change and conflict resolution within the network. Note how the energy landscape is molded by reference to stable states of the network. We may conceptualise these stable states, visualised as peaks in the landscape, as default configurations which the network will move towards in the absence of any conflicting information. Thus, constraint satisfaction networks can be seen to implement representational entities often referred to in the literature as frames (Minsky, 1975) or scripts (Schank \&amp; Abelson, 1977). The mutual constraints within a network and the environment (external input) in which the network finds itself interact to mold a dy-</p>
<p>namic, context sensitive energy landscape. Global maxima in this landscape correspond to prototypical resolutions of the constraint satisfaction problem. Local maxima correspond to intermediate solutions, reflecting competition between constraints. However, local maxima need not represent impossible solutions as in the case of the necker cube. They may equally well represent unorthodox but acceptable configurations of a given frame or script. In this fashion, the continuous nature of the energy landscape provides a foundation for the non-categorial forms of behaviour typical of these networks. Furthermore, stable configurations are achieved on the basis of local computations. No symbolic executive supervises the relaxation process.</p>
<h1>Discovering lexical classes from word-order</h1>
<p>The determination of word-order in an utterance is known to reflect a variety of constraints such as syntactic structure, selectional restrictions, subcategorisation and discourse considerations. Traditionally, psycholinguistic accounts of language production and comprehension have invoked symbolic processing systems to express the abstract structural relationships between words in an utterance. For example, in the sixties, psycholinguistics was dominated by the view that language processing involved some psychological implementation of transformational grammar (Fodor, Bever \&amp; Garrett, 1974). Although this approach turned out to be incorrect, the procedurally oriented theories (e.g. Miller \&amp; Johnson-Laird, 1976) which took over, are still symbolically based. Indeed, it has been argued (Fodor \&amp; Pylyshyn, 1988) that a symbolic level of representation is a necessary foundation for psycholinguistic processing. On this view, neural nets are capable of capturing only the most trivial structural relationships found between the words in an utterance. Neural nets fail in precisely the same way that Markov grammars failed to provide an account of linguistic structure earlier this century. As a first attempt to answer this challenge, Elman (1988) used a recurrent network to simulate word-order prediction. As we shall see, his network was able to assign lexical items to their correct grammatical category and to predict appropriate category orderings in the output from the network.</p>
<p>Earlier in this paper, we saw that recurrent networks can maintain an image of their previous states and so develop the capacity to predict future events. The architecture of the recurrent network used by Elman is shown in figure 10. Notice that figure 10, like the network for solving Exclusive OR, contains a layer of hidden units. Thus, the network possesses the capacity to extract regularities from the input patterns and construct internal representations thereof. In addition to the layer of hidden units, Elman's network contains a set of context units. The hidden units and the context units are equal in number. However, the context units can only communicate with the hidden layer. The weights cconnecting the hidden units to the context units are fixed</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure Ten
and constitute a one-to-one mapping. In effect, on every time-step the context units establish a copy of the previous states of the hidden units. The context units are connected to the hidden units in a one-to-many mapping. The dotted line connecting the context units to the hidden units indicates that the weights are adjustable. The context units display an image of the previous states of the hidden units to the hidden units themselves. In this way, Elman's recurrent network goes beyond the finite state Markov grammar. The recurrent connections, through the context units, provide the system with an indefinitely embedded representation of previous states of the network. The context units act as a contextual memory for the network.</p>
<p>The task which Elman gives the network is fairly straightforward. A word is presented to the input units. In response, the network must predict which word, taken from a previously constructed list, will be presented next to the input units. The network shows that it can predict the next word in the list by generating the word on the output units. During the training phase, a teacher signal provides feedback to the network. Errors are propagated backwards through the network using the Generalised Delta rule. The previously</p>
<p>constructed list consists of sequences of sentences generated by a simple phrase structure grammar. The list contains 10,000 two- or three-word sentences. Each word is represented by a unique random binary string. Thus, if the first sentence presented to the network is woman smash plate, the first two responses of the network should be smash and plate. Elman trained the network for five complete passes through the data set. He discovered that the absolute level of predictive performance was not very good, indicating that the network had failed to memorise the sequence of words. However, after the training phase, the connections in the network were frozen and the individual words used in the grammar presented to the input units,
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>one at a time. Instead of recording the output from the network, Elman recorded the activity across the hidden units (represented as a vector) for each unique word in each of its sentential contexts. The hidden unit activations produced by a given word (in all its contexts) were averaged to yield a single 50 -bit vector for each of the 35 unique words in the input stream. These internal representations were then subjected to a hierarchical clustering analysis.</p>
<p>Figure 11 shows the resulting tree; this tree reflects the similarity structure of the internal representations of these lexical items as perceived by the network. Lexical items which have similar properties are grouped together lower in the tree, and clusters of similar words which resemble other classes are connected higher in the tree. It is clear that the similarity structure depicted in the tree diagram reflects our human intuitions about grammatical similarity between the words which the network knows about. Thus, verbs are grouped together on one branch of the tree whilst nouns have been grouped together on another branch. Although the network has been told nothing about the semantics of the nouns presented, the cluster analysis reveals that the network has discovered appropriate semantic classifications of the words. For example, inanimate objects are distinguished from animate objects and humans are distinguished from small animals. Elman notes, that this hierachy is implicit in the similarity structure of the representations, and not an explicit function of the architecture. The network does not have available any of the symbolic apparatus of semantic networks or tree structures. (pg. 20). He also goes on to point out that the apparent semantic knowledge of the network is an illusory. The network has no knowledge of the meaning of words since each word is represented by a random binary string. It is simply the case that the network is able to classify the different words on the basis of their very similar behaviour with regard to serial order. However, one can imagine real language learners making use of the cues provided by word-order to make intelligent guesses about the meanings of novel words. This simulation suggests how distributional information in the input might be exploited by the learner without couching this knowledge in terms of explicit rules.</p>
<p>Elman admits that the structural relationships implicitly represented in the network in this simulation do not reflect a full-blown grammar of English! However, in a later set of simulations and using a similar network architecture, Elman addresses the problem of pronominal reference. For example, consider the following sentences:
a) If $\mathrm{Leo}<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ wants, $\mathrm{he}</em>$ will attend the meeting.
b) If he wants, $\mathrm{Leo}}<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ will attend the meeting.
c) $\mathrm{Leo}</em>}}$ will attend the meeting if he $\mathrm{e<em _mathrm_i="\mathrm{i">{\mathrm{i}}$ wants.
d) $\mathrm{He}</em>$ wants.}}$ will attend the meeting if $\mathrm{Leo}_{\mathrm{j}</p>            </div>
        </div>

    </div>
</body>
</html>
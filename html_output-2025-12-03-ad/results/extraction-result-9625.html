<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9625 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9625</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9625</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-272524543</p>
                <p><strong>Paper Title:</strong> The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review</p>
                <p><strong>Paper Abstract:</strong> Abstract Objectives This study aims to summarize the usage of large language models (LLMs) in the process of creating a scientific review by looking at the methodological papers that describe the use of LLMs in review automation and the review papers that mention they were made with the support of LLMs. Materials and Methods The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model. ChatGPT and Scite.ai were used in cleaning the data, generating the code for figures, and drafting the manuscript. Results Of the 3788 articles retrieved, 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n = 126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n = 26, 15.1%) were actual reviews that acknowledged LLM usage. Most citations focused on the automation of a particular stage of review, such as Searching for publications (n = 60, 34.9%) and Data extraction (n = 54, 31.4%). When comparing the pooled performance of GPT-based and BERT-based models, the former was better in data extraction with a mean precision of 83.0% (SD = 10.4) and a recall of 86.0% (SD = 9.8). Discussion and Conclusion Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. Despite limitations, such as lower accuracy of extraction for numeric data, we anticipate that LLMs will soon change the way scientific reviews are conducted.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9625.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9625.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o Covidence plugin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o integrated via Microsoft Azure (Covidence add-on)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An application of OpenAI's GPT-4o model used programmatically as a Covidence add-on to assist abstract screening, full-text screening, and data extraction in this systematic review; the add-on ran the model three times per item and used majority voting, with human calibration and spot-check validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (OpenAI) via Azure</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary generative large language model (OpenAI GPT-4o) accessed through Microsoft Azure OpenAI Service; used as an inference API to process article abstracts and full texts for screening and to extract structured data via prompt-based templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Search results from PubMed, Scopus, Dimensions, and Google Scholar (title/abstract queries) uploaded to Covidence; initial retrieval 3,788 studies, 3,341 after duplicate removal; screening and extraction were applied across this project's workflow with final included set of 172 studies for extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>3788</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Systematic review of the use of large language models in review automation; screening/extraction prompts were designed to identify whether papers used LLMs for review automation and to chart fields such as model type, stage automated, and performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based extraction pipeline integrated into Covidence: (1) human experts defined extraction schema and calibrated prompts on sample sets; (2) the add-on submitted abstracts/full texts to GPT-4o with tailored prompts (three distinct prompts per stage); (3) model inference was executed three times per item and final decision/values taken by majority vote; (4) outputs with <80% measured precision were manually validated and corrected by humans; (5) extracted data cleaned using ChatGPT and cross-checked with Google NotebookLM.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured extraction table (fields per citation), aggregated frequency counts, figures (plots/maps), and drafted manuscript sections (Introduction ~40%, Results ~90%, Discussion ~30%)</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example extraction fields produced: {author, year, country, review type, review stage automated, LLM type, reported performance metrics (accuracy/precision/recall/F1), sample size used to compute metrics, evaluation description, reported time savings, funding source, whether paper was a review or methods paper}. (Exact extracted table available in Supplementary File S1 / OSF.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human evaluation and calibration: two human reviewers calibrated on sample sets and compared consensus to LLM votes; LLM extraction precision measured by a single human reviewer; categories with precision <80% assigned to manual reviewer for validation; benchmarks and performance reported in supplementary tables (Tables S1-S4).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM-assisted workflow enabled screening and extraction across the corpus; overall the authors report high usefulness though some extraction categories had lower accuracy. The paper reports aggregate performance across reviewed studies (not only the plugin): GPT-based models had mean data-extraction precision ≈83.07% (SD 10.43) and recall ≈85.99% (SD 9.82); specific precision for the project's extraction tasks varied and categories with <80% precision were manually checked.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scalable automation of time-consuming review phases (screening, extraction, drafting); domain-agnostic prompts; majority-vote strategy to reduce nondeterminism; human-in-the-loop calibration and manual validation for low-precision categories; integration into existing review platform (Covidence) enabling automation of clicks/notes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model size and training data not reported; hallucination risk (especially for generating citations/searching); lower accuracy on numeric data extraction and some extraction categories; some extraction fields had relatively low precision and required human correction; reliance on authors' disclosure of LLM use for included studies (possible underreporting); computational costs and API costs (model expense) noted but not exhaustively quantified in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated/generated false citations when used for searching (noted particularly in earlier GPT versions); certain numeric outcomes and fine-grained performance metrics had lower extraction accuracy (<80%) requiring manual correction; nondeterminism required running inference multiple times and majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9625.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9625.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT/ChatGPT (aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT family / ChatGPT (aggregated use across reviewed literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-based models (including ChatGPT and various GPT API versions) were the dominant architecture mentioned in papers about review automation and evidence synthesis, applied to tasks such as searching, screening, data extraction, summarization, and drafting manuscripts across multiple studies surveyed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT / ChatGPT (various versions referenced, including GPT-3.5, GPT-4, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative pretrained transformer models from OpenAI (ChatGPT interface and API variants) used via desktop apps, APIs, and platform integrations for a range of literature-review tasks; specifics (fine-tuning, retrieval augmentation) vary by study and often are not fully reported in the reviewed papers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Varied by study; individual projects used corpora ranging from small sets of abstracts/full texts to larger search returns; the review aggregated 172 included studies that reported on LLM-assisted review automation but individual corpus sizes differed and are not uniformly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Tasks typically framed around automating stages of systematic/scoping reviews (searching for publications, title/abstract screening, full-text screening, data extraction, evidence synthesis/summarization, drafting manuscripts). Specific prompts and topic queries were developed per project, often PICO-style for clinical topics or domain-specific instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Across studies, methods included prompt engineering, zero-shot or few-shot prompting, some retrieval-augmented approaches (where external documents were provided/context-windowing), majority vote or repeated-inference strategies, and hybrid human-AI supervised workflows; explicit chain-of-thought or formal theory-distillation pipelines were rarely detailed in the reviewed reports.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Outputs reported include prioritized screening lists, extracted structured data, narrative summaries/evidence syntheses, draft manuscript text, tables and figures, and sometimes knowledge maps or PICO frames.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Reported outputs included automated extraction of PICO elements, tabulated clinical outcomes for meta-analysis, narrative evidence summaries, and draft review sections — example metrics: GPT-based data extraction precision mean 83.07% and recall mean 85.99% across papers that reported these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluations reported heterogeneously across studies: common approaches included comparison against human-labeled gold standards (accuracy, precision, recall, F1), bench-marking on test sets, and qualitative human expert appraisal. The systematic review synthesized these reported metrics when available.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The review aggregated reported metrics showing GPT-based models performed better than BERT-based models on data extraction (mean precision ≈83.07%, recall ≈85.99%), but had lower mean accuracy for title/abstract screening compared to BERT (GPT mean accuracy ≈77.34 vs BERT ≈80.87). Many studies nonetheless reported positive or mixed opinions about LLM use.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High performance on complex extraction tasks in multiple reports; flexibility across domains and tasks; low barrier to entry through APIs and desktop apps; ability to draft narrative synthesis and code snippets for figures (as used in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High hallucination rates in tasks such as citation generation and searching; variable transparency about prompt engineering and system setup across studies; cost and compute considerations for large API-based models; lower accuracy for numeric extraction and some specific fields; heterogenous and sometimes qualitative evaluations across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Documented hallucinations (fabricated citations/metrics), decreased accuracy for numeric outcome extraction, and inconsistent reproducibility due to nondeterminism and differing prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9625.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9625.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-based models (aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based masked-language models (BERT family) used for review automation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-family models and derivatives (SciBERT, domain variants) were commonly used for screening/classification and some extraction tasks in pre-LLM and LLM-era automation studies and are compared in the review against GPT-style models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (and BERT-derived models such as SciBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-based transformer architectures pretrained with masked-language modeling objectives; typically fine-tuned for classification/extraction tasks in systematic-review pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Varied across cited studies; commonly trained/fine-tuned on human-labeled title/abstract or full-text datasets specific to the review topic; exact corpus sizes not aggregated in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Used mainly for title/abstract screening (classification) and some information extraction tasks across domain-specific corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Primarily supervised fine-tuning on labeled examples, active learning approaches and prioritization/ranking strategies; typical pipelines included embedding + classifier, relevance ranking, and active learning loops rather than generative summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Classification decisions for screening, prioritized citation lists, extracted entities/PICO elements in structured formats.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Reported aggregated metrics: BERT models had higher mean title/abstract screening accuracy (≈80.87) and precision (≈65.6) but lower precision for data extraction compared with GPT in the studies that reported those comparisons (data-extraction precision mean ≈61.06).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Standard supervised evaluation against human-labeled test sets reported as accuracy, precision, recall, and F1 in individual methodological papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across reviewed papers, BERT-based models tended to have stronger performance for screening precision/accuracy in some contexts, but lower performance than GPT-based models on complex extraction tasks according to the pooled means reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Good performance on classification/screening tasks; relatively easier to fine-tune locally; lower hallucination risk since not generative by default.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Often required labeled training data per project (limited portability), less flexible for open-ended summarization or drafting, and sometimes lower extraction precision for complex multi-field extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor transferability across domains without labeled data; high setup/annotation costs for fine-tuning; degraded performance when project-specific labeled data were scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9625.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9625.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM: a toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced toolkit (Agarwal et al., 2024) aiming to assist scientific literature review tasks with LLMs; cited in this review as an example of methodological work on LLM-assisted literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LitLLM: a toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LitLLM (toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in citation list as a toolkit for literature review using LLMs; the reviewed paper cites it as an example but does not provide architecture or training details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this review; likely varies per use-case in the cited toolkit paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Toolkit intended to support scientific literature review workflows (specific topic queries dependent on user tasks); details not given in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified in this review; likely includes LLM-driven summarization and extraction utilities but the present paper does not report the toolkit's internal methods.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this review (presumed: summaries, extracted metadata, prioritization/ranking outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not specified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Cited as an example of tooling work in the literature on LLM-assisted reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No details available in this systematic review's text; users are referred to the original Agarwal et al. citation for specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9625.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9625.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRISMA-DFLLM / Susnjak</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models (Susnjak)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodological proposal (Susnjak, 2023 and related 2024 work) to extend PRISMA reporting to reviews that use domain-finetuned LLMs, recommending transparency and domain-specific fine-tuning strategies for automated reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Domain-finetuned LLMs (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proposal for using domain-specific fine-tuning of large language models to support systematic-review automation and corresponding reporting standards; the review cites Susnjak's advocacy for domain tuning and reporting guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this review; concept implies use of domain-specific corpora for fine-tuning LLMs prior to review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Guidance-level topic: conducting systematic reviews with domain-finetuned LLMs and reporting such methods transparently.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Conceptual guidance for fine-tuning LLMs on domain corpora and integrating them into review pipelines; exact methods are not detailed in this systematic review's text.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this review (intended outputs: more accurate, domain-aware extraction and synthesis, and standardized reporting).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not reported here; the entry in this review is a methodological citation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Addresses transparency and domain specificity; proposes standardization for LLM-assisted reviews which could improve reproducibility and trust.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The review notes the idea but does not report empirical outcomes from Susnjak's proposals; further evaluation required in original sources.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9625.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9625.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oami et al. (data-extraction protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited protocol (Oami et al., 2024) for prospectively studying LLM accuracy and reliability in data extraction for systematic reviews; identified by the authors as relevant literature on LLMs applied to extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Protocol paper planning to evaluate LLMs for automated data extraction; this systematic review references it as an example but does not provide model specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this review; the protocol itself likely defines a planned corpus and evaluation sets but those details are not included here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Planned prospective evaluation of LLMs for extracting data for systematic reviews (focus on accuracy and reliability).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Planned study (protocol) — specifics are in the protocol paper; the review only cites the protocol as relevant work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this review (expected outputs: empirical performance measures and evaluation of extraction reliability).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Protocol intends prospective human-vs-LLM comparison with accuracy/reliability metrics; this systematic review did not provide the protocol's detailed evaluation plan.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not available in this review (protocol stage at time of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Represents an effort to prospectively quantify LLM extraction accuracy and reliability — addressing a key evaluation gap noted by the review authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No empirical results reported here; referred to as ongoing/planned work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not applicable in the protocol citation within this review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LitLLM: a toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>Automating research synthesis with domain-specific large language model fine-tuning <em>(Rating: 2)</em></li>
                <li>PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models <em>(Rating: 2)</em></li>
                <li>Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study <em>(Rating: 2)</em></li>
                <li>Zero-shot generative large language models for systematic review screening automation <em>(Rating: 1)</em></li>
                <li>Data extraction for evidence synthesis using a large language model: a proof-of-concept study <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9625",
    "paper_id": "paper-272524543",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "GPT-4o Covidence plugin",
            "name_full": "OpenAI GPT-4o integrated via Microsoft Azure (Covidence add-on)",
            "brief_description": "An application of OpenAI's GPT-4o model used programmatically as a Covidence add-on to assist abstract screening, full-text screening, and data extraction in this systematic review; the add-on ran the model three times per item and used majority voting, with human calibration and spot-check validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (OpenAI) via Azure",
            "model_description": "Proprietary generative large language model (OpenAI GPT-4o) accessed through Microsoft Azure OpenAI Service; used as an inference API to process article abstracts and full texts for screening and to extract structured data via prompt-based templates.",
            "model_size": null,
            "input_corpus_description": "Search results from PubMed, Scopus, Dimensions, and Google Scholar (title/abstract queries) uploaded to Covidence; initial retrieval 3,788 studies, 3,341 after duplicate removal; screening and extraction were applied across this project's workflow with final included set of 172 studies for extraction.",
            "input_corpus_size": 3788,
            "topic_query_description": "Systematic review of the use of large language models in review automation; screening/extraction prompts were designed to identify whether papers used LLMs for review automation and to chart fields such as model type, stage automated, and performance metrics.",
            "distillation_method": "Prompt-based extraction pipeline integrated into Covidence: (1) human experts defined extraction schema and calibrated prompts on sample sets; (2) the add-on submitted abstracts/full texts to GPT-4o with tailored prompts (three distinct prompts per stage); (3) model inference was executed three times per item and final decision/values taken by majority vote; (4) outputs with &lt;80% measured precision were manually validated and corrected by humans; (5) extracted data cleaned using ChatGPT and cross-checked with Google NotebookLM.",
            "output_type": "Structured extraction table (fields per citation), aggregated frequency counts, figures (plots/maps), and drafted manuscript sections (Introduction ~40%, Results ~90%, Discussion ~30%)",
            "output_example": "Example extraction fields produced: {author, year, country, review type, review stage automated, LLM type, reported performance metrics (accuracy/precision/recall/F1), sample size used to compute metrics, evaluation description, reported time savings, funding source, whether paper was a review or methods paper}. (Exact extracted table available in Supplementary File S1 / OSF.)",
            "evaluation_method": "Human evaluation and calibration: two human reviewers calibrated on sample sets and compared consensus to LLM votes; LLM extraction precision measured by a single human reviewer; categories with precision &lt;80% assigned to manual reviewer for validation; benchmarks and performance reported in supplementary tables (Tables S1-S4).",
            "evaluation_results": "LLM-assisted workflow enabled screening and extraction across the corpus; overall the authors report high usefulness though some extraction categories had lower accuracy. The paper reports aggregate performance across reviewed studies (not only the plugin): GPT-based models had mean data-extraction precision ≈83.07% (SD 10.43) and recall ≈85.99% (SD 9.82); specific precision for the project's extraction tasks varied and categories with &lt;80% precision were manually checked.",
            "strengths": "Scalable automation of time-consuming review phases (screening, extraction, drafting); domain-agnostic prompts; majority-vote strategy to reduce nondeterminism; human-in-the-loop calibration and manual validation for low-precision categories; integration into existing review platform (Covidence) enabling automation of clicks/notes.",
            "limitations": "Model size and training data not reported; hallucination risk (especially for generating citations/searching); lower accuracy on numeric data extraction and some extraction categories; some extraction fields had relatively low precision and required human correction; reliance on authors' disclosure of LLM use for included studies (possible underreporting); computational costs and API costs (model expense) noted but not exhaustively quantified in main text.",
            "failure_cases": "Hallucinated/generated false citations when used for searching (noted particularly in earlier GPT versions); certain numeric outcomes and fine-grained performance metrics had lower extraction accuracy (&lt;80%) requiring manual correction; nondeterminism required running inference multiple times and majority voting.",
            "uuid": "e9625.0",
            "source_info": {
                "paper_title": "The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT/ChatGPT (aggregated)",
            "name_full": "GPT family / ChatGPT (aggregated use across reviewed literature)",
            "brief_description": "GPT-based models (including ChatGPT and various GPT API versions) were the dominant architecture mentioned in papers about review automation and evidence synthesis, applied to tasks such as searching, screening, data extraction, summarization, and drafting manuscripts across multiple studies surveyed in this review.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT / ChatGPT (various versions referenced, including GPT-3.5, GPT-4, GPT-4o)",
            "model_description": "Generative pretrained transformer models from OpenAI (ChatGPT interface and API variants) used via desktop apps, APIs, and platform integrations for a range of literature-review tasks; specifics (fine-tuning, retrieval augmentation) vary by study and often are not fully reported in the reviewed papers.",
            "model_size": null,
            "input_corpus_description": "Varied by study; individual projects used corpora ranging from small sets of abstracts/full texts to larger search returns; the review aggregated 172 included studies that reported on LLM-assisted review automation but individual corpus sizes differed and are not uniformly reported.",
            "input_corpus_size": null,
            "topic_query_description": "Tasks typically framed around automating stages of systematic/scoping reviews (searching for publications, title/abstract screening, full-text screening, data extraction, evidence synthesis/summarization, drafting manuscripts). Specific prompts and topic queries were developed per project, often PICO-style for clinical topics or domain-specific instructions.",
            "distillation_method": "Across studies, methods included prompt engineering, zero-shot or few-shot prompting, some retrieval-augmented approaches (where external documents were provided/context-windowing), majority vote or repeated-inference strategies, and hybrid human-AI supervised workflows; explicit chain-of-thought or formal theory-distillation pipelines were rarely detailed in the reviewed reports.",
            "output_type": "Outputs reported include prioritized screening lists, extracted structured data, narrative summaries/evidence syntheses, draft manuscript text, tables and figures, and sometimes knowledge maps or PICO frames.",
            "output_example": "Reported outputs included automated extraction of PICO elements, tabulated clinical outcomes for meta-analysis, narrative evidence summaries, and draft review sections — example metrics: GPT-based data extraction precision mean 83.07% and recall mean 85.99% across papers that reported these metrics.",
            "evaluation_method": "Evaluations reported heterogeneously across studies: common approaches included comparison against human-labeled gold standards (accuracy, precision, recall, F1), bench-marking on test sets, and qualitative human expert appraisal. The systematic review synthesized these reported metrics when available.",
            "evaluation_results": "The review aggregated reported metrics showing GPT-based models performed better than BERT-based models on data extraction (mean precision ≈83.07%, recall ≈85.99%), but had lower mean accuracy for title/abstract screening compared to BERT (GPT mean accuracy ≈77.34 vs BERT ≈80.87). Many studies nonetheless reported positive or mixed opinions about LLM use.",
            "strengths": "High performance on complex extraction tasks in multiple reports; flexibility across domains and tasks; low barrier to entry through APIs and desktop apps; ability to draft narrative synthesis and code snippets for figures (as used in this review).",
            "limitations": "High hallucination rates in tasks such as citation generation and searching; variable transparency about prompt engineering and system setup across studies; cost and compute considerations for large API-based models; lower accuracy for numeric extraction and some specific fields; heterogenous and sometimes qualitative evaluations across studies.",
            "failure_cases": "Documented hallucinations (fabricated citations/metrics), decreased accuracy for numeric outcome extraction, and inconsistent reproducibility due to nondeterminism and differing prompt engineering.",
            "uuid": "e9625.1",
            "source_info": {
                "paper_title": "The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "BERT-based models (aggregated)",
            "name_full": "Transformer-based masked-language models (BERT family) used for review automation",
            "brief_description": "BERT-family models and derivatives (SciBERT, domain variants) were commonly used for screening/classification and some extraction tasks in pre-LLM and LLM-era automation studies and are compared in the review against GPT-style models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BERT (and BERT-derived models such as SciBERT)",
            "model_description": "Encoder-based transformer architectures pretrained with masked-language modeling objectives; typically fine-tuned for classification/extraction tasks in systematic-review pipelines.",
            "model_size": null,
            "input_corpus_description": "Varied across cited studies; commonly trained/fine-tuned on human-labeled title/abstract or full-text datasets specific to the review topic; exact corpus sizes not aggregated in this review.",
            "input_corpus_size": null,
            "topic_query_description": "Used mainly for title/abstract screening (classification) and some information extraction tasks across domain-specific corpora.",
            "distillation_method": "Primarily supervised fine-tuning on labeled examples, active learning approaches and prioritization/ranking strategies; typical pipelines included embedding + classifier, relevance ranking, and active learning loops rather than generative summarization.",
            "output_type": "Classification decisions for screening, prioritized citation lists, extracted entities/PICO elements in structured formats.",
            "output_example": "Reported aggregated metrics: BERT models had higher mean title/abstract screening accuracy (≈80.87) and precision (≈65.6) but lower precision for data extraction compared with GPT in the studies that reported those comparisons (data-extraction precision mean ≈61.06).",
            "evaluation_method": "Standard supervised evaluation against human-labeled test sets reported as accuracy, precision, recall, and F1 in individual methodological papers.",
            "evaluation_results": "Across reviewed papers, BERT-based models tended to have stronger performance for screening precision/accuracy in some contexts, but lower performance than GPT-based models on complex extraction tasks according to the pooled means reported in this review.",
            "strengths": "Good performance on classification/screening tasks; relatively easier to fine-tune locally; lower hallucination risk since not generative by default.",
            "limitations": "Often required labeled training data per project (limited portability), less flexible for open-ended summarization or drafting, and sometimes lower extraction precision for complex multi-field extraction tasks.",
            "failure_cases": "Poor transferability across domains without labeled data; high setup/annotation costs for fine-tuning; degraded performance when project-specific labeled data were scarce.",
            "uuid": "e9625.2",
            "source_info": {
                "paper_title": "The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM: a toolkit for scientific literature review",
            "brief_description": "A referenced toolkit (Agarwal et al., 2024) aiming to assist scientific literature review tasks with LLMs; cited in this review as an example of methodological work on LLM-assisted literature review.",
            "citation_title": "LitLLM: a toolkit for scientific literature review",
            "mention_or_use": "mention",
            "model_name": "LitLLM (toolkit)",
            "model_description": "Described in citation list as a toolkit for literature review using LLMs; the reviewed paper cites it as an example but does not provide architecture or training details.",
            "model_size": null,
            "input_corpus_description": "Not specified in this review; likely varies per use-case in the cited toolkit paper.",
            "input_corpus_size": null,
            "topic_query_description": "Toolkit intended to support scientific literature review workflows (specific topic queries dependent on user tasks); details not given in this review.",
            "distillation_method": "Not specified in this review; likely includes LLM-driven summarization and extraction utilities but the present paper does not report the toolkit's internal methods.",
            "output_type": "Not specified in this review (presumed: summaries, extracted metadata, prioritization/ranking outputs).",
            "output_example": null,
            "evaluation_method": "Not specified in this review.",
            "evaluation_results": "Not specified in this review.",
            "strengths": "Cited as an example of tooling work in the literature on LLM-assisted reviews.",
            "limitations": "No details available in this systematic review's text; users are referred to the original Agarwal et al. citation for specifics.",
            "failure_cases": "Not reported in this paper.",
            "uuid": "e9625.3",
            "source_info": {
                "paper_title": "The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "PRISMA-DFLLM / Susnjak",
            "name_full": "PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models (Susnjak)",
            "brief_description": "A methodological proposal (Susnjak, 2023 and related 2024 work) to extend PRISMA reporting to reviews that use domain-finetuned LLMs, recommending transparency and domain-specific fine-tuning strategies for automated reviews.",
            "citation_title": "PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models",
            "mention_or_use": "mention",
            "model_name": "Domain-finetuned LLMs (conceptual)",
            "model_description": "Proposal for using domain-specific fine-tuning of large language models to support systematic-review automation and corresponding reporting standards; the review cites Susnjak's advocacy for domain tuning and reporting guidance.",
            "model_size": null,
            "input_corpus_description": "Not specified in this review; concept implies use of domain-specific corpora for fine-tuning LLMs prior to review tasks.",
            "input_corpus_size": null,
            "topic_query_description": "Guidance-level topic: conducting systematic reviews with domain-finetuned LLMs and reporting such methods transparently.",
            "distillation_method": "Conceptual guidance for fine-tuning LLMs on domain corpora and integrating them into review pipelines; exact methods are not detailed in this systematic review's text.",
            "output_type": "Not specified in this review (intended outputs: more accurate, domain-aware extraction and synthesis, and standardized reporting).",
            "output_example": null,
            "evaluation_method": "Not reported here; the entry in this review is a methodological citation.",
            "evaluation_results": "Not reported in this review.",
            "strengths": "Addresses transparency and domain specificity; proposes standardization for LLM-assisted reviews which could improve reproducibility and trust.",
            "limitations": "The review notes the idea but does not report empirical outcomes from Susnjak's proposals; further evaluation required in original sources.",
            "failure_cases": "Not reported in this review.",
            "uuid": "e9625.4",
            "source_info": {
                "paper_title": "The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Oami et al. (data-extraction protocol)",
            "name_full": "Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study",
            "brief_description": "A cited protocol (Oami et al., 2024) for prospectively studying LLM accuracy and reliability in data extraction for systematic reviews; identified by the authors as relevant literature on LLMs applied to extraction tasks.",
            "citation_title": "Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Protocol paper planning to evaluate LLMs for automated data extraction; this systematic review references it as an example but does not provide model specifics.",
            "model_size": null,
            "input_corpus_description": "Not specified in this review; the protocol itself likely defines a planned corpus and evaluation sets but those details are not included here.",
            "input_corpus_size": null,
            "topic_query_description": "Planned prospective evaluation of LLMs for extracting data for systematic reviews (focus on accuracy and reliability).",
            "distillation_method": "Planned study (protocol) — specifics are in the protocol paper; the review only cites the protocol as relevant work.",
            "output_type": "Not specified in this review (expected outputs: empirical performance measures and evaluation of extraction reliability).",
            "output_example": null,
            "evaluation_method": "Protocol intends prospective human-vs-LLM comparison with accuracy/reliability metrics; this systematic review did not provide the protocol's detailed evaluation plan.",
            "evaluation_results": "Not available in this review (protocol stage at time of this paper).",
            "strengths": "Represents an effort to prospectively quantify LLM extraction accuracy and reliability — addressing a key evaluation gap noted by the review authors.",
            "limitations": "No empirical results reported here; referred to as ongoing/planned work.",
            "failure_cases": "Not applicable in the protocol citation within this review.",
            "uuid": "e9625.5",
            "source_info": {
                "paper_title": "The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LitLLM: a toolkit for scientific literature review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Automating research synthesis with domain-specific large language model fine-tuning",
            "rating": 2,
            "sanitized_title": "automating_research_synthesis_with_domainspecific_large_language_model_finetuning"
        },
        {
            "paper_title": "PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models",
            "rating": 2,
            "sanitized_title": "prismadfllm_an_extension_of_prisma_for_systematic_literature_reviews_using_domainspecific_finetuned_large_language_models"
        },
        {
            "paper_title": "Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study",
            "rating": 2,
            "sanitized_title": "accuracy_and_reliability_of_data_extraction_for_systematic_reviews_using_large_language_models_a_protocol_for_a_prospective_study"
        },
        {
            "paper_title": "Zero-shot generative large language models for systematic review screening automation",
            "rating": 1,
            "sanitized_title": "zeroshot_generative_large_language_models_for_systematic_review_screening_automation"
        },
        {
            "paper_title": "Data extraction for evidence synthesis using a large language model: a proof-of-concept study",
            "rating": 2,
            "sanitized_title": "data_extraction_for_evidence_synthesis_using_a_large_language_model_a_proofofconcept_study"
        }
    ],
    "cost": 0.0173185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review
7 May 2025</p>
<p>PhDDmitry Scherbakov 0009-0005-7274-0934
Biomedical Informatics Center
Department of Public Health Sciences
Medical University of South Carolina (MUSC)
29403CharlestonSCUnited States</p>
<p>PhDNina Hubig 
Biomedical Informatics Center
Department of Public Health Sciences
Medical University of South Carolina (MUSC)
29403CharlestonSCUnited States</p>
<p>Interdisciplinary Transformation University
OG</p>
<p>A-4040LinzAustria</p>
<p>PhDVinita Jansari 
School of Computing
Clemson University
29634CharlestonSCUnited States</p>
<p>MScAlexander Bakumenko 
School of Computing
Clemson University
29634CharlestonSCUnited States</p>
<p>MD, MS �Leslie A Lenert lenert@musc.edu 0000-0002-9680-5094
Biomedical Informatics Center
Department of Public Health Sciences
Medical University of South Carolina (MUSC)
29403CharlestonSCUnited States</p>
<p>Biomedical Informatics Center
Department of Public Health Sciences
Medical University of South Carolina (MUSC)
22 WestEdge Street, Suite 200, Room WG21329403CharlestonSCUnited States</p>
<p>The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review
7 May 20255AFE65A1C31A62375D8729C95A38675C10.1093/jamia/ocaf063Received: February 12, 2025; Revised: April 2, 2025; Editorial Decision: April 6, 2025; Accepted: April 11, 2025large language modelsreview automationsystematic reviewscoping reviewCovidence
Objectives: This study aims to summarize the usage of large language models (LLMs) in the process of creating a scientific review by looking at the methodological papers that describe the use of LLMs in review automation and the review papers that mention they were made with the support of LLMs.Materials and Methods:The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers.Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model.ChatGPT and Scite.ai were used in cleaning the data, generating the code for figures, and drafting the manuscript.Results: Of the 3788 articles retrieved, 172 studies were deemed eligible for the final review.ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n ¼ 126, 73.2%).A significant number of review automation projects were found, but only a limited number of papers (n ¼ 26, 15.1%) were actual reviews that acknowledged LLM usage.Most citations focused on the automation of a particular stage of review, such as Searching for publications (n ¼ 60, 34.9%) and Data extraction (n ¼ 54, 31.4%).When comparing the pooled performance of GPT-based and BERT-based models, the former was better in data extraction with a mean precision of 83.0% (SD ¼ 10.4) and a recall of 86.0% (SD ¼ 9.8).Discussion and Conclusion:Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs.Despite limitations, such as lower accuracy of extraction for numeric data, we anticipate that LLMs will soon change the way scientific reviews are conducted.</p>
<p>Introduction</p>
<p>The abundance of scientific information available can be overwhelming, posing a challenge for researchers to navigate relevant data.Consequently, the number of scoping and systematic reviews helping scientists synthesize the evidence has increased significantly over the years.Toh and Lee noted an exponential rise in the number of scoping reviews, with 2665 published in 2020 alone, compared with fewer than 10 reviews published annually before 2009. 1 The same trend is observed in systematic reviews and meta-analyses.For example, in cardiology, over 2400 meta-analyses were published in 2019, quadrupling the number reported in 2012. 2 The completion of a review requires substantial resources 3 ; furthermore, there is often unpredictable uncertainty in the amount of resources required. 4The time to complete a single systematic review varies, but authors typically give estimates in months and even years. 5Screening automation platforms, such as Covidence, 6 facilitate systematic and scoping reviews by streamlining established guidelines and checklists, such as the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) and Population, Intervention, Comparison, and Outcome (PICO) to ensure transparency and rigor in the review process. 7The use of such platforms may reduce the time needed to complete reviews by providing tools that automate key tasks, such as removing duplicate references and generating flow charts of the screening process, visual extraction designers, and workflows for several independent reviewers.</p>
<p>Although, for example, Covidence includes features to reduce the time to complete screening, such as key term highlighting and embedded natural language processing (NLP) algorithm, 8 it primarily organizes the significant manual work that is still needed from human reviewers, such as screening and extraction.Each of these steps typically involves 2 independent analysts, with a third optional human expert supervising the process and resolving the disagreements.</p>
<p>Even with 2 reviewers resolving disagreements through discussion, as many as 3% of relevant citations are missed, and if only a single reviewer is used (for example, in rapid reviews), as many as 13% of relevant publications can be missed. 9The relatively weak performance of humans in screening relevant articles has led some investigators to develop natural language processing tools 10-13 to automate screening.A recent statement by the National Institute for Health and Care Excellence (NICE) highlights the potential and accompanying risks of artificial intelligence (AI) in the systematic review process automation. 14Large language models (LLMs) have recently emerged as some of the most powerful NLP tools across different ranges of tasks, [15][16][17] which are reviewed in this publication.</p>
<p>While LLMs signify a notable advancement, earlier methodologies rooted in machine learning (ML) and natural language processing (NLP) laid the groundwork for assisted reviewing and annotation. 18Prior methods to automate systematic reviews primarily focused on text classification and data extraction, aiming to decrease manual review burden and improve review efficiency. 10,19n title and abstract screening, ML techniques such as Support Vector Machines (SVM), Naive Bayes, and Logistic Regression trained on human-labeled data were prevalent, with tools such as Abstrackr and EPPI-Reviewer demonstrating workload reductions of approximately 40-50% while maintaining high recall (eg, 95% or higher). 20Some studies indicated potential workload reductions up to 88-98% when using text mining as a second screener. 21These tools typically prioritized citations by relevance probability, enabling reviewers to efficiently manage screening tasks by assessing highly relevant citations first.Active learning, a key approach, involved the machine learning from reviewer decisions on a subset of citations, strategically selecting the most informative citations for subsequent review, thus enhancing classifier accuracy with less human effort.Crowdsourcing, notably via platforms such as Cochrane Crowd, also contributed to eligibility assessment automation. 22or data extraction, earlier approaches relied extensively on rule-based systems, such as MedEx, which extracted specific clinical data from texts with reasonable accuracy. 10,18achine learning methods were explored but faced constraints, including limited availability of annotated datasets for training and validation. 19,23For instance, RobotReviewer achieved automated risk of bias assessment with accuracy close to human performance, effectively identifying supporting textual evidence for bias judgments. 23re-LLM literature search strategies employed predominantly optimized Boolean queries, with emerging text mining approaches reducing manual screening workloads by approximately 30-70%, though sometimes at the cost of a 5% recall loss. 21However, early text mining methods required considerable setup time, specialized technical expertise, and extensive collaboration or training for research teams. 21Machine learning-based review automation systems faced significant dependence on high-quality labeled datasets, necessitating substantial manual annotation efforts, complicated further by limited public availability of clinical data due to privacy regulations like HIPAA. 10,18,22,23Additionally, these systems exhibited poor transferability, with models specifically tailored to individual systematic reviews, leading to repeated resource-intensive training processes for each new project. 10,21,22Rule-based systems, despite interpretability benefits, also lacked generalizability due to reliance on handcrafted, domain-specific rules developed collaboratively by experts and physicians. 18Furthermore, systematic bias, particularly automation bias-where reliance on automated systems could lead to overlooked errors-posed an ongoing challenge to their adoption. 24Thus, while pre-LLM text mining approaches showed potential for automating systematic review processes, their adoption was hindered by extensive initial investments, labeled data dependencies, limited model portability, systematic biases, and requisite domain-specific expertise. 10,18,21,24These challenges underscored the advantages offered by advancements such as LLMs.</p>
<p>In summary, previous efforts leveraging NLP and ML in systematic review processes achieved significant efficiencies in screening and extraction stages, though adoption was constrained by concerns about bias, limited annotated datasets, and accuracy in highly specific tasks.LLMs have since qualitatively expanded these capabilities, building upon foundational work done by earlier NLP and ML systems.</p>
<p>In this systematic review, we evaluated the use of LLMs to assist with several components of the review process.The review aims to (1) summarize the current state-of-the-art research projects using LLMs to automate the review process, (2) look at the range of review types and review stages that are being automated, and (3) assess the performance of LLMs used for automation.This review aims to cover both, the review papers created with various degrees of LLM support, and methodological papers describing review automation with LLMs.As detailed below, we used LLM to assist with several key aspects of our review.</p>
<p>Methods</p>
<p>The study's research plan was formulated by the author team and the review was registered in the Open Science Framework (OSF) database. 25The results are reported using the checklist provided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA 2020, checklist is provided in Supplementary File S2). 26 We decided that to be included in the review, citations had to be either reviews created with LLMs (and LLM usage was disclosed by authors) or methodological papers centered around the usage of LLMs in the automation of different phases of a systematic review.Only English-language journal publications, including conference abstracts and review publications that used LLMs in their creation, were considered.</p>
<p>Publications were excluded if they:</p>
<p>� Did not use some kind of LLM (eg, ChatGPT, Mistral, GPT-3.5, and BERT); � Did not describe the automation of any stage of the review process; � The paper was a review article itself that did not use LLM to conduct the review; � The full text of the article could not be retrieved or was not published in English.</p>
<p>The initial search was conducted by a human reviewer (D.S.) in June 2024 using title and abstract fields in PubMed, Scopus, and Dimensions 27 databases and with default filters in Google Scholar.All publications were searched from the time of inception.Table 1 presents the search strategy for the databases.</p>
<p>All citations were then uploaded to Covidence.Covidence was used as a review protocol to track the progress of the study.The screening and extraction process took place in Covidence with the help of the LLM plugin for Covidence that our team developed.This plugin is used during the screening and extraction phases.The process of using the LLM plugin for screening and extraction is shown in Figure 1.</p>
<p>The developed add-on works by interacting with the Covidence platform programmatically via an intermediary software solution that was created in Python and R. The solution passes content between Covidence and the LLM OpenAI GPT-4o model provided by Microsoft Azure cloud service. 28nce the LLM generates the response, a script automates actions in Covidence, such as clicking the Include/Exclude buttons or leaving notes.</p>
<p>The review process involved 3 stages that were automated by the Covidence add-on: abstract screening, full-text screening, and extraction.In each stage, 2 human reviewers were calibrated by screening a sample to refine the inclusion criteria and extraction categories.They then created and tested prompts for the LLM.LLM inference was programmed to run inference 3 times to determine the final decision (eg, "include" or "exclude") based on the majority vote.Three prompts per phase are detailed in Table S1.</p>
<p>For the screening phases, a human-LLM consensus was reached through the process of using 2 human reviewers who first agreed and reached a human consensus on the subset of 100 abstracts (30 full-texts for the full-text screening phase), and then by comparing the results of their consensus against LLM votes, establishing a new human-LLM consensus (for instance, LLM can reveal false positives or false negatives in human consensus).LLM extraction precision was measured by a single human reviewer, and for categories with low precision (&lt;80%), a manual reviewer was assigned to validate and correct LLM outputs.Benchmarks are provided in Tables S2-S4.</p>
<p>The data charting form for extraction was designed by human experts (D.S., V.J., A.B., L.L., and N.H.) and adopted into the LLM prompt to collect the following primary information:</p>
<p>� Author, year, title; � Country and/or US state of the study; � What types of reviews were automated; � Stage of review automated in the research project; � LLM type used; � Performance metrics reported by authors during each stage of the review.In particular, accuracy, precision, recall, specificity, and F1 were extracted; if other metrics were used instead, they were grouped under the "Other metrics" category; if no metrics were reported, a "Not mentioned/qualitative" value was assigned.</p>
<p>� Number of samples (full-texts or abstracts) that authors used to compute their performance metrics; � Brief information on how performance metrics were calculated; � Brief information on reported timesaving; � What was the general opinion of the study team on the usage of LLMs in review automation (positive, negative, or mixed) with a citation to support this viewpoint; � Sources of the funding of the research project (public, private, mixed, or unknown); � Is the paper an actual review that used LLMs or a methods paper?</p>
<p>Due to the diverse nature of publications and study designs, bias and quality assessment were not performed.</p>
<p>An LLM tool by Google NotebookLM (version from August 2024), 29 along with a manual review (D.S., V.J., and A.B.), was used to cross-check the extraction results for the fields where the precision of extraction was low (&lt;0.8) during the benchmark.ChatGPT (4o model) 30 was used to clean the extraction data: the case was formatted, duplicates that were not identified automatically were removed, and similar entries were renamed to a common name.The data were manually fed into the chat window by a human reviewer (D.S.).Scite.ai(version from August 2024) 31 was used to draft parts of the introduction and discussion sections, whereas ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce Figures 3-5.Frequency count was the main method to synthesize the results, and column plots were used to present the frequencies.A map figure was used to synthesize location data.To compare performance metrics, boxplots were used to display the median, interquartile range (IQR), whiskers (within 1.5 × IQR), and outliers (outside 1.5 × IQR) of the most frequently mentioned model types.However, mean and standard deviation were used when comparing and reporting performance metrics.Only studies reporting the metrics we specified in the extraction form were used in the comparison.Studies with other numeric metrics or qualitative metrics were not compared.</p>
<p>ChatGPT was used to draft the text of the results section, which was then corrected by our team where needed.As a result, approximately 40% of the Introduction, 90% of the Results, and 30% of the Discussion section were generated by different types of LLMs.Human experts edited and verified the final LLM-generated draft of the manuscript.</p>
<p>Additionally, we report the time savings and the computational costs in Supplementary File S1.We used our time measurements and reference data from experienced reviewers to calculate the time savings. 32</p>
<p>Results</p>
<p>Figure 2 outlines the PRISMA article selection process for this study.Initially, 3788 studies were identified across several databases: PubMed (n ¼ 2174), Scopus (n ¼ 1207), (("large language models" OR "large language model" OR "LLM" OR "LLMs" OR "ChatGPT" OR "GPT-3" OR "GPT-4" OR "LLaMA" OR "Mistral" OR "Mixtral" OR "BARD" OR "BERT" OR "Claude" OR "PaLM" OR "Gemini" OR "Copilot") AND ("systematic review � " OR "scoping review � " OR "literature review � " OR "narrative review � " OR "umbrella review � " OR "rapid review � " OR "integrative review � " OR "evidence synthesis" OR "meta-analysis"))</p>
<p>Dimensions (n ¼ 356), and Google Scholar (n ¼ 48), along with 3 additional studies from citation searching.Following the removal of 447 duplicates (1 manually and 446 by Covidence), 3341 studies remained for the screening phase.</p>
<p>During the title and abstract screening process, 3041 studies were excluded, leaving 300 studies for retrieval and fulltext eligibility assessment.Out of these 300 studies, 128 were excluded for various reasons, with the most common being "The paper does not describe the automation of any stage of the review process" (n ¼ 88).A total of 172 studies were included in the final review.</p>
<p>Figure 3 shows the geographic distribution of studies across 43 countries.Most citations are from the United States (n ¼ 60, 34.9%), followed by Australia (n ¼ 14, 8.14%), the United Kingdom and China (n ¼ 13, 7.6%), and Germany (n ¼ 11, 6.4%).Other notable contributors include Canada (n ¼ 7, 4.1%) and India (n ¼ 6, 3.5%).Austria, Ireland, Italy, the Netherlands, and South Korea each contributed 4 studies (2.3%), while countries like New Zealand, France, Japan, and others provided 3 (1.7%).The rest contributed 1-2 studies.</p>
<p>In the United States, 47 studies had state-level data.Tennessee, New York, and Massachusetts led with 5 citations each (10.6%), followed by California (n ¼ 4, 8.5%).North Carolina and Ohio contributed 3 studies (6.4%), while several other states provided 2 (4.3%) or 1 (2.1%) citations.</p>
<p>Figure 4A and Table 2 show the types of reviews discussed in automation papers.The most frequently mentioned type is "Systematic Review" (n ¼ 118, 68.6%), followed by "Literature/Narrative Review" (n ¼ 37, 21.5%) and "Meta-Analysis" (n ¼ 19, 11.0%).The remaining categories include "Scoping Review" (n ¼ 8, 4.7%), "Other/Non-specific" (n ¼ 14, 8.1%), and "Rapid Review" (n ¼ 6, 3.5%)."Umbrella Review" has a smaller representation with 2 mentions (1.2%).</p>
<p>Figure 4B and Table 2 illustrate the stages of review discussed in automation papers.The most frequently mentioned stage is "Searching for publications" (n ¼ 60, 34.9%), followed by "Data extraction" (n ¼ 54, 31.4%) and "Evidence synthesis/summarization" (n ¼ 32, 18.6%).Other categories with notable mentions include "Title and abstract screening" (n ¼ 43, 25.0%), "Drafting a publication" (n ¼ 22, 12.8%), "Full-text screening" (n ¼ 14, 8.1%), "Quality and bias assessment" (n ¼ 12, 7.0%), "Publication classification" (n ¼ 10, 5.8%), "Other stages" (n ¼ 6, 3.5%), and "Code and plots generation" (n ¼ 4, 2.3%).</p>
<p>The most frequently mentioned AI model is GPT/ ChatGPT, with 126 occurrences (73.3%), showing its widespread use (Figure 5).BERT-based models are also notable with 32 mentions (18.6%).LLaMA/Alpaca models have 8 mentions (4.7%), followed by Google Bard/Gemini with 5 (2.9%) and Claude models with 7 (4.1%).Other models like BART (n ¼ 3, 1.7%) and Mistral (n ¼ 4, 2.3%) are less frequent.Several models, including Bing and XLNet, have 2 mentions each (1.2%), while many others are mentioned just once (0.6%).</p>
<p>Of the 172 citations, 79 (45.9%) reported common metrics like Accuracy, Precision/Recall, and F1, while 36 (20.9%) used less common metrics, such as G-score and Jaccard similarity.The remaining 57 publications (33.1%) relied on qualitative assessments.Figure 6 shows the performance metrics for GPT-and BERT-based models.Based on the comparison of mean values across reviewed papers, GPT models had lower accuracy in title/abstract screening (M ¼ 77.34, SD ¼ 13.06) compared to BERT models (M ¼ 80.87, SD ¼ 11.81).However, GPT models performed better in data extraction, with precision (M ¼ 83.07, SD ¼ 10.43) and recall (M ¼ 85.99, SD ¼ 9.82), while BERT models had lower precision (M ¼ 61.06, SD ¼ 31.26) and similar recall (M ¼ 80.03, SD ¼ 10.09).In title/abstract screening, BERT models had higher precision (M ¼ 65.6, SD ¼ 17.65) but lower recall (M ¼ 72.93, SD ¼ 23.95) than GPT models (precision M ¼ 63.2, SD ¼ 24.34; recall M ¼ 80.42, SD ¼ 23.31).</p>
<p>The majority of the reviewed publications were papers describing how LLM could be used to automate a certain phase of the review (n ¼ 146, 84.9%).Only 26 (15.1%) papers were actual reviews conducted with some help from LLM tools.Most authors were positive about the usage of LLMs in reviews (n ¼ 120, 69.8%), with 43 citations (25.0%) containing mixed or cautious views on LLM usage.Only 9 (5.2%) study teams had negative experiences with LLM usage.More than half of the studies had public funding reported (n ¼ 97, 56.4%).</p>
<p>Table S5 in presents the complete extraction table with all extracted categories across 172 citations.</p>
<p>Discussion</p>
<p>Our LLM-assisted systematic review revealed a significant number of research projects related to review automation with LLM.Despite finding a significant number of projects using LLMs to automate some stages of the review process, only a few papers focused on the full cycle of review automation. 53,54There might be perceived publication barriers; for example, journals have recently started to ask about LLMgenerated content, although we do not have information on whether this leads to changes in the reviewing process.A growing number of LLM-generated papers will probably   eventually change how the review is conducted (reviewers might be assisted by LLMs, or the review paper format could eventually be replaced by online real-time information retrieval).The strength of the present review includes the large-scale (over 3000 abstracts screened and 172 full-text publications eligible for extraction) automation of different stages of review, including drafting the manuscript sections and plot generation.Only a few citations focused on the automation of the full cycle of review, while most focused only on specific areas like extraction or screening, including our previous systematic review where GPT-3.5 was used with LDA-based topic modeling for validation of findings made by human reviewers. 100In contrast, the LLM-based method that we applied in this work demonstrated its direct applicability by facilitating the automation of the abstract and full-text screening, data extraction, as well as knowledge synthesis stages, with the discussed constraints.Furthermore, our method is domain-agnostic; thus, it can be integrated into large-scale review projects across different domains.The implications of such automation include reducing human workload and improving the overall efficiency of systematic reviews.Furthermore, such tools in their more mature form will require less expertise from human reviewers, which could contribute to the democratization of the systematic and scoping review process, with the potential to add features related to meta-analysis into the process.</p>
<p>GPT-based LLMs were the most dominant type of LLMs and the ones that seemed to yield remarkable results in the data extraction, arguably the most complex and timeconsuming stage of any review.The relatively low result in searching for publications can be attributed to high hallucination rates, when these models are prompted to generate scientific citations on the research topic, this was especially noticeable in earlier versions of GPT-based models.</p>
<p>At this moment, there are few restrictions on the type of information users can load into ChatGPT, and published papers are unlikely to contain any sensitive information, making ChatGPT, with its high-performing model, developed desktop application and API, an obvious choice.Usage of these expensive models shows overall a significant reduction in cost to complete a review. 205At the same time, smaller models, such as BERT, Llama, or Mistral, can be run and fine-tuned locally at much lower costs; we expect to see more automation projects with this LLM in the future. 206</p>
<p>Limitations</p>
<p>We used calibrated LLMs as reviewers in this project.Some extraction categories, such as performance metrics, had  Journal of the American Medical Informatics Association, 2025, Vol.32, No. 6 relatively lower accuracy.Therefore, the results of this extraction category should be taken with caution.Nevertheless, in this review, LLMs achieved remarkable results in accuracy, making it possible to delegate time-consuming phases of review to LLMs.Studies generally recommend a single-reviewer approach in some cases, such as rapid reviews. 207However, we believe that the LLM approach could substitute human reviewers, and human effort should be redirected to supervision of the review process.Future research should focus on improving LLM performance metrics, particularly precision and recall in lower-accuracy extraction categories.Additionally, integrating and evaluating different LLMs, possibly in combination with other AI models, should be explored to enhance performance.The short-and long-term impacts of these integrations on review quality, along with ethical considerations, must also be assessed to maintain research credibility and trust.</p>
<p>It is important to mention that we relied on the disclosure of LLM usage by the authors of reviewed publications, and this study did not use any type of automatic LLM usage detection; thus, we could have missed publications, especially potential reviews, that could have been created with LLM support.</p>
<p>Conclusion</p>
<p>The use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis.LLMs are likely to significantly reduce the time needed for reviews while producing similar or higher-quality data in greater quantities than manual reviews do.Research shows it is becoming increasingly difficult to distinguish between LLM-generated and human-written texts, 208 and the presence of LLM-generated text in scientific publications is growing exponentially. 209To promote transparency and proper acknowledgment, researchers are encouraged to openly disclose their use of LLMs in academic papers, providing information on the prompts employed and the sections of text affected. 210espite early successes, few systematic reviews using LLMs were identified in our review.Although still in its early stages, AI-assisted reviews are already yielding impressive results, with growing interest as researchers develop semi-automated pipelines.However, generating trustworthy and useful AIdriven reviews still presents both technological and ethical challenges, particularly for quantitative meta-analyses comparing treatment effects.However, the conduct of more simple systematic reviews, such as scoping reviews, appears to be well within the capabilities of current or near-future AI methods.</p>
<p>supported by grant T15 LM013977, Biomedical Informatics and Data Science for Health Equity Research (SC BID-S4Health).This publication was supported in part by a Smartstate Chair endowment.The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p>
<p>Figure 1 .
1
Figure 1.LLM workflow added into Covidence for screening and extraction.</p>
<p>Figure 2 .
2
Figure 2. Flow diagram of the systematic review process.</p>
<p>Figure 3 .
3
Figure 3. (A) Publications by country of origin.(B) Publications by state in the United States.</p>
<p>Figure 4 .
4
Figure 4. (A) Types of automated review.(B) Which stages of review are automated in the paper.</p>
<p>Figure 5 .
5
Figure 5. LLM types proposed for automation (models mentioned in 2 or more studies shown).</p>
<p>Table 1 .
1
Search strategy.</p>
<p>Table 2 .
2
Summary table of reviewed citations.Journal of the American Medical InformaticsAssociation, 2025, Vol.32, No. 6
CitationsReview stageM/RLLMReviewAgarwal, 2024 33 ; Anghelescu, 2023 34 ; Ber-DraftingMGPTSys., N, Umbrella,senev, 2024 35 ; Dossantos, 2023 36 ; Jenko,Other, M2024 37 ; Khlaif, 2023 38 ; Li, 2024 39 ; Liv-berber, 2023 40 ; Lozano, 2024 41 ; Najafali,2023 42 ; Semrl, 2023 43 ; Teperikidis,2024 44 ; Wang, 2024 45 ; Wu, 2023 46 ; Yun,2023 47 ; Zhao, 2024 48Yun, 2023 47DraftingMOtherSys.Huang, 2023 49 ; Lamov� sek, 2023 50 ; Liu,DraftingRGPTSys., Umbrella, Other, N2023 51 ; Pedroso-Roussado, 2023 52 ; Scho-pow, 2023 53 ; Teperikidis, 2023 54Ahmed, 2023 55 ; Marshalova, 2023 56 ;ExtractionMBERTOther, Scop., Sys., M, NMutinda, 2022 ; Panayi, 2023 58 ; Scells,2023 59 ; Shinde, 2022 60 ; Wang, 2022 61 ;Whitton, 2023 62 ; Yazi, 2021 63Oami, 2024 64 ; Prasad, 2024 65 ; Ye, 2024 66ExtractionMBard/GeminiSys., NGartlehner, 2024 ; Oami, 2024 64ExtractionMClaudeSys.Ahmed, 2023 55 ; Aronson, 2023 68 ; Flaherty,ExtractionMGPTSys., M, Other,2024 69 ; Gue, 2024 70 ; Kartchner, 2023 71 ;N, UmbrellaKhraisha, 2024 ; Kılıc¸, 2023 73 ; Lozano,2024 41 ; Mahmoudi, 2024 74 ; Mahuli,2023 75 ; Miao, 2023 76 ; Oami, 2024 64 ;Prasad, 2024 65 ; Reason, 2024 77 ; Schmidt,2024 78 ; Serajeh, 2024 79 ; Shah-Moham-madi, 2024 80 ; Susnjak, 2023 81 ; Susnjak,2024 82 ; Tang, 2024 83 ; Tao, 2024 84 ;Teperikidis, 2024 44 ; Tovar, 2023 85 ; Uit-tenhove, 2024 86 ; Urrutia, 2023 87 ; Wang,2024 88 ; Yun, 2024 89 ; Zamani, 2024 90 ;Zhao, 2024 48Ghosh, 2024 91 ; Serajeh, 2024 79 ; Tovar,ExtractionMLlamaM, Other, N, Sys.2023 85 ; Yun, 2024 89Susnjak, 2024 82 ; Tsai, 2024 92 ; Yun, 2024 89ExtractionMMistralSys., MHossain, 2024 93 ; Jain, 2024 94 ; Sami, 2024 95ExtractionMNon-specificSys., NGrokhowsky, ; Yun, 2024 89ExtractionMOtherM, RSun, 2024 97 ; White, 2023 98ExtractionRClaudeSys.Janes, 2022 99 ; Liu, 51 ; Noe-Steinm€ uller,ExtractionRGPTSys., Umbrella, Other2024 100 ; Pattyn, 101 ; Schopow,2023 53 ; Teperikidis, 2023 54Beheshti, 2023 102 ; Sun, 2024 97ExtractionROtherSys.Ambalavanan, 2020 103 ; Martenot, 2022 104Full-textMBERTM, Sys., NYe, 2024 66Full-textMBard/GeminiN, Sys.Aronson, 2023 68 ; Khraisha, 2024 72 ; Lozano,Full-textMGPTSys., Other2024 41 ; Susnjak, 81Tsai, 2024 92Full-textMMistralSys.Hossain, 2024 93 ; Sami, 2024 95Full-textMNon-specificSys.Guo, 2023 105Full-textMOtherSys.Liu, 2023 51 ; Schopow, 2023 53 ; Teperikidis,Full-textRGPTSys., Umbrella, Other2023 54Scells, 2023 59OtherMBERTSys.Atkinson, 2023 106 ; Demir, 2024 107 ; Giunti,OtherMGPTOther, M, Sys., N2024 108 ; Kılıc¸, 2023 73 ; Najafali, 2023 42 ;Qureshi, 2023 ; Whang, 2024 110 ; Zhao,2024 48Abd-Alrazaq,OtherRBERTOtherKhadhraoui, 2022 112 ; Liang, 2023 113 ;Publication classificationMBERTSys.Likhareva, 2024 114Alshami, 2023 115 ; Guler, 2023 116 ; Lam,Publication classificationMGPTN, Sys.2024 117Grokhowsky, ; Platt, 2023 118 ; Raja,Publication classificationMOtherR, Sys.2024 119Twinomurinzi, 2023 120Publication classificationRGPTScop.Wang, 2022 121Quality/bias assessmentMBERTSys.Lai, 2024 122 ; Woelfle, 2024 123Quality/bias assessmentMClaudeM, Sys.Barsby, 2024 124 ; Chern, 2023 125 ; Hasan,Quality/bias assessmentMGPTM, Sys., N, Umbrella2024 126 ; Lai, 2024 122 ; Mahuli, 2023 75 ;Pitre, 2023 127 ; Roberts, 2023 128 ; Srivas-tava, 2023 129 ; Teperikidis, 2024 44 ;Treviño-Juarez, 2024 130 ; Woelfle, 2024 123(continued)</p>
<p>Table 2 .
2
(continued)
CitationsReview stageM/RLLMReviewWoelfle, 2024 123Quality/bias assessmentMMistralM, Sys.Alchokr, 2022 131 ; Lu, 2021 132 ; Tang,SearchingMBERTN, Scop., Sys.2023 133Aiumtrakul, 2023 134 ; Chelli,SearchingMBard/GeminiSys.Agarwal, 2024 33 ; Aiumtrakul, 2023 134 ;SearchingMGPTSys., N, Other, Scop.,Anghelescu, 2023 34 ; Antu, ;R, M, UmbrellaChelli, 2024 135 ; Choueka, ;Demir, 2024 107 ; D� ıaz, 2023 138 ; Dossan-tos, 2023 36 ; Flaherty, 2024 69 ; Goldfarb,2024 139 ; Gupta, 2023 140 ; Gupta, 2023 141 ;Gwon, 2024 142 ; Herbst, 2023 143 ; Jafari,2024 144 ; Kim, 2024 145 ; Kılıc¸, 2023 73 ; Li,2024 146 ; Liu, 2023 147 ; Lozano, 41 ;Maniaci, 2024 148 ; Najafali, 2023 42 ; Qure-shi, 2023 109 ; Roy, 2024 149 ; Ruksakulpi-wat, 2024 150 ; Sanii, 2024 151 ; Semrl,2023 43 ; Singh, 2023 152 ; Spillias, 2023 153 ;Suppadungsuk, 2023 154 ; Susnjak, 2023 81 ;Teperikidis, 2024 44 ; Tovar, 2023 87 ;Wang, 2023 155 ; Yan, 2024 156 ; Zamani,2024 90 ; Zhao, 2024 48 ; Zhu, 2023 157 ;Zimmermann, 2024 158Tovar, 2023 85SearchingMLlamaNTsai, 2024 92SearchingMMistralSys.Hossain, 2024 93 ; Jain, 2024 94 ; Sami, 2024 95SearchingMNon-specificSys., NAiumtrakul, 2023 134 ; Guo, 2023 105 ; Gwon,SearchingMOtherSys., N2024 142 ; Sanii, 2024 151 ; Zhu, 2023 157Anghelescu, 2023 159 ; Cambaz, 2024 160 ;SearchingRGPTR, Sys., Other, Scop.,Haltaufderheide, 2024 161 ; Liu, 2023 51 ;Umbrella, M, NPattyn, 2023 101 ; Ruksakulpiwat, 2023 162 ;Sallam, 2023 163 ; Schopow, 2023 53 ; Srivas-tava, 2023 164 ; Teperikidis, ; Zhao,2024 165Beheshti, 2023 102 ; Cambaz, 2024 160SearchingROtherSys.Lan, 2024 166 ; Lu, 2021 132 ; Shinde, 2022 60 ;SynthesisMBERTSys., NTeslyuk, 2020 167Anghelescu, 2023 34 ; Antu, 2023 136 ; Atkin-SynthesisMGPTSys., M, Other,son, 2023 106 ; Aydın, 2022 168 ; Blasingame,N, Umbrella2024 169 ; Chaker, 2024 170 ; Dossantos,2023 36 ; Jenko, 2024 37 ; Kim, 2024 145 ;LamHoai, 2023 171 ; Li, 2024 146 ; Lozano,2024 41 ; Qureshi, 2023 109 ; Susnjak,2023 81 ; Susnjak, 2024 82 ; Tang, 172 ;Teperikidis, 2024 44 ; Wang, 2024 45 ; Yan,2023 173 ; Zhao, 2024 48Susnjak, 2024 82SynthesisMMistralSys.Yu, 2022 174SynthesisMOtherSys.Lamov� sek, 2023 50 ; Li, 2024 175 ; Liu, 2023 51 ;SynthesisRGPTN, Sys., OtherNoe-Steinm€ uller, 2024 100 ; Pedroso-Rous-sado, 2023 52 ; Rajjoub, 2024 176 ; Temsah,2023 177Ambalavanan, 2020 178 ; Aum, 2021 179 ;Title/abstractMBERTSys., Other, NEdwards, 2024 180 ; Hasny, ; Kats,2023 182 , Mao, 2024 183 ; Martenot,2022 104 ; Ng, 2023 184 ; Qin, 2021 185 ;Wang, 2022 186Ye, 2024 66Title/abstractMBard/GeminiN, Sys.Castillo-Segura, 2023 187Title/abstractMClaudeSys.Akinseloyin, 2023 188 ; Ali, 2024 ; Cai,Title/abstractMGPTSys., Scop., R, M,2023 190 ; Castillo-Segura, 2023 187 ; Guo,Other, Umbrella2024 191 ; Huotala, 2024 192 ; Issaiy,2024 193 ; Kataoka, 2023 194 ; Khraisha,2024 72 ; Kılıc¸, 2023 73 ; Li, 2024 195 ; Loz-ano, 2024 41 ; Robinson, 2023 ; Susnjak;2023 81 ; Syriani, 2023 197 ; Teperikidis,2024 44 ; Tran, 2024 198 ; Urrutia, 87 ;Wang, 2023 199 ; Wang, 2024 200 ; Wilkins,2023 201 ; Yang, 2024 202 ; Zamani, 2024 90(continued)</p>
<p>Table 2 .
2
(continued)
CitationsReview stageM/RLLMReviewLi, 2024 195 ; Robinson, 2023 196 ; Wang,Title/abstractMLlamaM, Sys.2023 199 ; Wang, 2024 200Tsai, 2024 92Title/abstractMMistralSys.Hossain, 2024 93 ; Sami, 2024 95Title/abstractMNon-specificSys.Castillo-Segura, 2023 187 ; Li, 2024 195 ; Rob-Title/abstractMOtherM, Sys.inson, 2023 196Buchlak, 2022 203 ; Buchlak, 2022 204Title/abstractRBERTSys.White, 2023 98Title/abstractRClaudeSys.Liu, 2023 51 ; Schopow, 2023 53 ; Teperikidis,Title/abstractRGPTSys., Umbrella, Other2023 54Buchlak, 2022 203 ; Buchlak, 2022 204Title/abstractROtherSys.
R, Review paper; M, Methods paper; Title/abstract , Title or abstract screening; Full-text, Full-text screening; Extraction, Data extraction; Searching, Publication searching; Synthesis, Evidence Synthesis; Sys., Systematic review; Scop., Scoping review; M, Meta-analysis; N, Narrative/literature review; GPT, GPT/ChatGPT-based models; BERT, BERT-based models; Llama, Llama/Alpaca-based models.</p>
<p>Journal of the American Medical InformaticsAssociation, 2025, Vol. 32, No. 6<br />
Journal of the American Medical Informatics Association, 2025, Vol. 32, No. 6
AcknowledgmentsScreening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model as described in the Methods section.An LLM tool by Google NotebookLM (version from August 2024) was used to cross-check the extraction results for the fields where the precision of extraction was low during the benchmark.ChatGPT (4o model) was used to clean the extraction data (case formatting, removal of duplicates, and standardization of names).Scite.ai(version from August 2024) was used to draft parts of the introduction and discussion sections, whereas ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce Figures3-5.Data availabilityThe data underlying this article are available in the article, supplementary materials, and in OSF registration record (https://doi.org/10.17605/OSF.IO/EJKSY).FundingThis publication was supported, in part, by the National Center for Advancing Translational Sciences of the National Institutes of Health under Grant Number UL1 TR001450.D.S. wasAuthor contributionsDmitrySupplementary materialSupplementary material is available at Journal of the American Medical Informatics Association online.Conflicts of interestThe authors have no competing interests to declare.
Statistical note: using scoping and systematic reviews. T S Toh, J H Lee, Pediatr Crit Care Med. 222021</p>
<p>Quality assessment of published systematic reviews in high impact cardiology journals: revisiting the evidence pyramid. A I Abushouk, I Yunusa, A O Elmehrath, Front Cardiovasc Med. 86715692021</p>
<p>The roles of adolescents' emotional problems and social media addiction on their self-esteem. I H Acar, G Avcılar, G Yazıcı, S Bostancı, 10.1007/s12144-020-01174-5Curr Psychol. 412020</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. R Borah, A W Brown, P L Capers, BMJ Open. 7e0125452017</p>
<p>Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. Z Munn, Mdj Peters, C Stern, BMC Med Res Methodol. 182018</p>
<p>. L Kellermeyer, B Harnke, S Knight, Rayyan Covidence, Jmla, 2018106580</p>
<p>Myoclonus and cerebellar ataxia associated with COVID-19: a case report and systematic review. J L Chan, K A Murphy, J R Sarna, J Neurol. 2682021</p>
<p>Machine learning-the game changer for trustworthy evidence. August, 14, 2024</p>
<p>Single-reviewer abstract screening missed 13 percent of relevant studies: a crowdbased, randomized controlled trial. G Gartlehner, L Affengruber, V Titscher, J Clin Epidemiol. 1212020</p>
<p>Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. I J Marshall, B C Wallace, Syst Rev. 82019</p>
<p>Large language model evaluation via multi ai agents: preliminary results. Z Rasheed, M Waseem, P Syst€ A K, Abrahamsson, arXiv:2404.010232024arXiv, preprintpreprint: not peer reviewed</p>
<p>Zero-shot generative large language models for systematic review screening automation. S Wang, H Scells, S Zhuang, M Potthast, B Koopman, G Zuccon, European Conference on Information Retrieval. Springer2024</p>
<p>Natural language processing-guided meta-analysis and structure factor database extraction from glass literature. M Zaki, S R Namireddy, T Pittie, J Non-Crystalline Solids: X. 151001032022</p>
<p>Use of AI in evidence generation: NICE position statement. April 24, 2025National Institute for Health and Care Excellence</p>
<p>ChatGPT-a new milestone in the field of education. Z Liu, ACE. 352024</p>
<p>The potential applications and challenges of ChatGPT in the medical field. Y Mu, D He, Int J Gen Med. 172024</p>
<p>What if the devil is my guardian angel: ChatGPT as a case study of using Chatbots in education. A Tlili, B Shehata, M A Adarkwah, Smart Learn Environ. 10152023</p>
<p>Clinical information extraction applications: a literature review. Y Wang, L Wang, M Rastegar-Mojarad, J Biomed Inform. 772018</p>
<p>Automating data extraction in systematic reviews: a systematic review. S R Jonnalagadda, P Goyal, M D Huffman, Syst Rev. 42015</p>
<p>Machine learning for screening prioritization in systematic reviews: comparative performance of Abstrackr and EPPI-Reviewer. A Y Tsou, J R Treadwell, E Erinoff, Syst Rev. 92020</p>
<p>Using text mining for study identification in systematic reviews: a systematic review of current approaches. A O'mara-Eves, J Thomas, J Mcnaught, Syst Rev. 42015</p>
<p>Living Systematic Review Network, et al. Living systematic reviews: 2. Combining human and machine effort. J Thomas, A Noel-Storr, I Marshall, J Clin Epidemiol. 912017</p>
<p>RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials. I J Marshall, J Kuiper, B C Wallace, J Am Med Inform Assoc. 232016</p>
<p>Automation bias: a systematic review of frequency, effect mediators, and mitigators. K Goddard, A Roudsari, J C Wyatt, J Am Med Inform Assoc. 192012</p>
<p>Large language models in scoping and systematic reviews automation: an automated systematic review. D Scherbakov, protocol registration</p>
<p>. Accessed, 10.17605/OSF.IO/EJKSYApril 24, 2025</p>
<p>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, BMJ. 372712021</p>
<p>. Dimensions. April 24, 2025</p>
<p>Azure OpenAI Service. 2025 Mar 20Microsoft</p>
<p>. Accessed, April 24, 2025</p>
<p>. Google, Notebooklm, 2025 Mar 20</p>
<p>. Accessed, April 24, 2025</p>
<p>Hello GPT-4o. Openai, 2025. 2025 Mar 20</p>
<p>. Accessed, April 24, 2025</p>
<p>. Accessed, April 24, 2025</p>
<p>Predicting the time needed for environmental systematic reviews and systematic maps. N R Haddaway, M J Westgate, Conserv Biol. 332019</p>
<p>LitLLM: a toolkit for scientific literature review. S Agarwal, I H Laradji, L Charlin, C Pal, 2024arXiv, preprint arXivnot peer reviewed</p>
<p>To be or not to be" . . . in academic research. The human mind's analytical rigor and capacity to discriminate between AI bots' truths and hallucinations. A Anghelescu, I Ciobanu, C Munteanu, Balneo PRM Res J. 146142023</p>
<p>Replicating a high-impact scientific publication using systems of large language models. D Bersenev, A Yachie-Kinoshita, S K Palaniappan, bioRxiv 2024.04.08.5886142024</p>
<p>Eyes on AI: ChatGPT's transformative potential impact on ophthalmology. J Dossantos, An J Javan, R , Cureus. 15e407652023</p>
<p>An evaluation of AI generated literature reviews in musculoskeletal radiology. N Jenko, S Ariyaratne, L Jeys, Surgeon. 222024</p>
<p>The potential and concerns of using AI in scientific research: ChatGPT performance evaluation. Z N Khlaif, A Mousa, M K Hattab, JMIR Med Educ. 9e470492023</p>
<p>X Li, J Ouyang, Explaining relationships among research papers. arXiv, preprint arXiv. 2024not peer reviewed</p>
<p>Toward non-human-centered design: designing an academic article with ChatGPT. T Livberber, EPI. 322023</p>
<p>ai: an opensource retrieval-augmented large language model system for answering medical questions using scientific literature. A Lozano, S L Fleming, C-C Chiang, Pac Symp Biocomput. 292024</p>
<p>Truth or lies? The pitfalls and limitations of ChatGPT in systematic review creation. D Najafali, J M Camacho, E Reiche, Aesthet Surg J. 432023</p>
<p>AI language models in human reproduction research: exploring ChatGPT's potential to assist academic writing. N Semrl, S Feigl, N Taumberger, Hum Reprod. 382023</p>
<p>Prompting ChatGPT to perform an umbrella review. E Teperikidis, A Boulmpou, C Papadopoulos, Acta Cardiol. 792024</p>
<p>When young scholars cooperate with LLMs in academic tasks: the influence of individual differences and task complexities. J Wang, C Huang, S Yan, W Xie, D He, Int J Hum-Comput Interact. 2024</p>
<p>Addition of dexamethasone to prolong peripheral nerve blocks: a ChatGPT-created narrative review. C L Wu, B Cho, R Gabriel, Reg Anesth Pain Med. 492023</p>
<p>Appraising the potential uses and harms of LLMs for medical systematic reviews. H S Yun, I J Marshall, T A Trikalinos, B C Wallace, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>Potential to transform words to watts with large language models in battery research. S Zhao, S Chen, J Zhou, Cell Rep Phys Sci. 51018442024</p>
<p>The role of ChatGPT in scientific communication: writing better scientific review articles. J Huang, M Tan, Am J Cancer Res. 132023</p>
<p>Analysis of research on artificial intelligence in public administration. N Lamov� Sek, CEPAR. 212023</p>
<p>How good is ChatGPT for medication evidence synthesis? Stud Health Technol Inform. H Liu, Y Peng, C Weng, 2023302</p>
<p>Investigating the limitations of fashion research methods in applying a sustainable design practice: a systematic review. C Pedroso-Roussado, Preprints.org 202310.0250.v22023</p>
<p>Applications of the natural language processing tool ChatGPT in clinical practice: comparative study and augmented systematic review. N Schopow, G Osterhoff, D Baur, JMIR Med Inform. 11e489332023</p>
<p>Does the longterm administration of proton pump inhibitors increase the risk of adverse cardiovascular outcomes? A ChatGPT powered umbrella review. E Teperikidis, A Boulmpou, V Potoupni, Acta Cardiol. 782023</p>
<p>Reimagining open data ecosystems: a practical approach using AI, CI, and Knowledge Graphs. U Ahmed, BIR Workshops. 2023</p>
<p>Automatic aspect extraction from scientific texts. A Marshalova, E Bruches, T Batura, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>Automatic data extraction to support meta-analysis statistical analysis: a case study on breast cancer. F W Mutinda, K Liew, S Yada, BMC Med Inform Decis Mak. 221582022</p>
<p>Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews. A Panayi, K Ward, A Benhadji-Schaff, Syst Rev. 121872023</p>
<p>Smooth operators for effective systematic review queries. H Scells, F Schlatt, M Potthast, 10.1145/3539618.35917682023. April 24, 2025</p>
<p>Extractive-abstractive approach for multi-document summarization of scientific articles for literature review. K Shinde, T Roy, Ghosal An, T , 2022. April 24, 2025</p>
<p>PICO entity extraction for preclinical animal literature. Q Wang, J Liao, M Lapata, Syst Rev. 112092022</p>
<p>Automated tabulation of clinical trial results: a joint entity and relation extraction approach with transformer-based language representations. J Whitton, A Hunter, Artif Intell Med. 1441026612023</p>
<p>Towards automated detection of contradictory research claims in medical literature using deep learning approach. F S Yazi, W T Vong, V Raman, Phh Then, M J Lunia, 10.1109/CAMP51653.2021.94980612021. April 24, 2025</p>
<p>Accuracy and reliability of data extraction for systematic reviews using large language models: a protocol for a prospective study. T Oami, Y Okada, T-A Nakada, medRxiv 2024.05.22.243077402024</p>
<p>Towards development of automated knowledge maps and databases for materials engineering using large language models. D Prasad, M Pimpude, A Alankar, 2024arXiv, preprint arXivnot peer reviewed</p>
<p>A hybrid semi-automated workflow for systematic and literature review processes with large language model analysis. A Ye, A Maiti, M Schmidt, Future Internet. 161672024</p>
<p>Data extraction for evidence synthesis using a large language model: a proof-ofconcept study. G Gartlehner, L Kahwati, R Hilscher, Res Synth Methods. 152024</p>
<p>Preparing to integrate generative pretrained transformer series 4 models into genetic variant assessment workflows: assessing performance, drift, and nondeterminism characteristics relative to classifying functional evidence in literature. S J Aronson, K Machini, J Shin, arXiv2023preprint arXiv, not peer reviewed</p>
<p>Beyond plagiarism: ChatGPT as the Vanguard of technological revolution in research and citation. H B Flaherty, J Yurch, Res Social Work Pract. 342024</p>
<p>Evaluating the OpenAI's GPT-3.5 Turbo's performance in extracting information from scientific articles on diabetic retinopathy. Ccy Gue, Nda Rahim, W Rojas-Carabali, Syst Rev. 131352024</p>
<p>Zero-shot information extraction for clinical meta-analysis using large language models. D Kartchner, I Al-Hussaini, O Kronick, S Ramalingam, C Mitchell, 2023. April 24, 2025</p>
<p>Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. Q Khraisha, S Put, J Kappenberg, Res Synth Methods. 152024</p>
<p>A semiautomated solution approach recommender for a given use case: a case study for AI/ML in oncology via Scopus and OpenAI. D K Kılıc¸, A E Vasegaard, A Desoeuvres, P Nielsen, arXiv2023preprint arXiv, not peer reviewed</p>
<p>A critical assessment of large language models for systematic reviews: utilizing ChatGPT for complex data extraction. H Mahmoudi, D Chang, H Lee, N Ghaffarzadegan, M S Jalali, SSRN J. 2024</p>
<p>Application ChatGPT in conducting systematic reviews and meta-analyses. S A Mahuli, A Rai, A V Mahuli, Br Dent J. 2352023</p>
<p>Mining topic structure of AI algorithmic literature. H Miao, X Yu, H Wu, 10.1109/IEIR59294.2023.103912532023. April 24, 2025</p>
<p>Artificial intelligence to automate network meta-analyses: four case studies to evaluate the potential application of large language models. T Reason, E Benbow, J Langham, Pharmacoecon Open. 82024</p>
<p>Exploring the use of a large language model for data extraction in systematic reviews: a rapid feasibility study. arXiv, preprint arXiv, not peer reviewed. L Schmidt, K Hair, S Graziozi, Journal of the American Medical Informatics Association. 3262024. 1082. 2025</p>
<p>N T Serajeh, I Mohammadi, V Fuccella, De Rosa, M , LLMs in HCI data work: bridging the gap between information retrieval and responsible research practices. arXiv, preprint arXiv. 2024not peer reviewed</p>
<p>Large language model-based architecture for automatic outcome data extraction to support meta-analysis. F Shah-Mohammadi, J Finkelstein, 10.1109/CCWC60891.2024.104278292024. April 24, 2025</p>
<p>PRISMA-DFLLM: an extension of PRISMA for systematic literature reviews using domain-specific finetuned large language models. T Susnjak, arXiv:2306.149052023arXiv, preprintnot peer reviewed</p>
<p>Automating research synthesis with domain-specific large language model fine-tuning. T Susnjak, P Hwang, N H Reyes, arXiv:2404.086802024arXiv, preprintnot peer reviewed</p>
<p>Large language model in medical information extraction from titles and abstracts with prompt engineering strategies: a comparative study of GPT-3.5 and GPT-4. Y Tang, Z Xiao, X Li, medRxiv 2024.03.20.243045722024</p>
<p>GPT-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet. K Tao, Z A Osman, P L Tzou, BMC Med Res Methodol. 241392024</p>
<p>D A Tovar, AI literature review suite. arXiv, preprint arXiv, not peer reviewed. 2023</p>
<p>Large language models in psychology: application in the context of a systematic literature review. K Uittenhove, P Martinelli, A Roquet, PsyArXiv, preprint PsyArXiv. 2024not peer reviewed</p>
<p>Deep natural language feature learning for interpretable prediction. F Urrutia, C Buc, V Barriere, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>MetaMate: large language model to the rescue of automated data extraction for educational systematic reviews and meta-analyses. X Wang, G Luo, EdArXiv, preprint EdArXiv. 2024not peer reviewed</p>
<p>Automatically extracting numerical results from randomized controlled trials with large language models. H S Yun, D Pogrebitskiy, I J Marshall, B C Wallace, 2024arXiv, preprint arXivnot peer reviewed</p>
<p>Generative AI-the end of systematic reviews in PhD projects?. S Zamani, R Sinha, ACM Inroads. 152024</p>
<p>AlpaPICO: extraction of PICO frames from clinical trial documents using LLMs. Methods. M Ghosh, S Mukherjee, A Ganguly, 2024226</p>
<p>Comparative analysis of automatic literature review using Mistral large language model and human reviewers. H-C Tsai, Y-F Huang, C-W Kuo, 10.21203/rs.3.rs-4022248/v1Research Square. 2024. April 24, 2025</p>
<p>Using ChatGPT and other forms of generative AI in systematic reviews: challenges and opportunities. M M Hossain, J Med Imaging Radiat Sci. 552024</p>
<p>Sci-Space literature review: harnessing AI for effortless scientific discovery. S Jain, A Kumar, T Roy, K Shinde, G Vignesh, R Tondulkar, 2024</p>
<p>System for systematic literature review using multiple AI agents: concept and an empirical evaluation. A M Sami, Z Rasheed, K-K Kemell, arXiv2024preprint arXiv, not peer reviewed</p>
<p>Reducing knowledge synthesis workload time using a text-mining algorithm for research location and subtopic extraction from geographically dependent research publications. N Grokhowsky, 10.21203/rs.3.rs-3129370/v12023. April 24, 2025Research Square</p>
<p>How good are large language models for automated data extraction from randomized trials?. Z Sun, R Zhang, S A Doi, medRxiv 2024.02. 20.243030832024</p>
<p>Sample size in quantitative instrument-based studies published in Scopus up to 2022: an artificial intelligence aided systematic review. M White, Acta Psychol (Amst). 2411040952023</p>
<p>Open tracing tools: overview and critical comparison. A Janes, X Li, V Lenarduzzi, J Syst Softw. 2041117932023</p>
<p>Defining suffering in pain: a systematic review on pain-related suffering using natural language processing. Noe-Steinm€ Uller, N Scherbakov, D Zhuravlyova, A , Pain. 1652024</p>
<p>Preliminary structured literature review results using ChatGPT: towards a pragmatic framework for product managers at software startups. F Pattyn, 2023 IEEE 31st International Requirements Engineering Conference Workshops (REW). IEEE2023</p>
<p>Transitioning drivers from linear to circular economic models: evidence of entrepreneurship in emerging nations. M Beheshti, Amoozad Mahdiraji, H Rocha-Lona, L , Manage Decis. 622024</p>
<p>Cascade neural ensemble for identifying scientifically sound articles. A K Ambalavanan, M Devarakonda, 2020arXiv, preprint arXivnot peer reviewed</p>
<p>LiSA: an assisted literature search pipeline for detecting serious adverse drug events with deep learning. V Martenot, V Masdeu, J Cupe, BMC Med Inform Decis Mak. 223382022</p>
<p>SciMine: an efficient systematic prioritization model based on richer semantic information. F Guo, Y Luo, L Yang, Y Zhang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>ChatGPT and computational-based research: benefits, drawbacks, and machine learning applications. C F Atkinson, Discov Artif Intell. 3422023</p>
<p>Enhancing systematic reviews in orthodontics: a comparative examination of GPT-3.5 and GPT-4 for generating PICO-based queries with tailored prompts and configurations. G B Demir, Y S€ Uk€ Ut, G S Duran, Eur J Orthod. 46e0112024</p>
<p>Cocreating an automated mHealth apps systematic review process with generative AI: design science research approach. G Giunti, C P Doherty, JMIR Med Educ. 10e489492024</p>
<p>Are ChatGPT and large language models "the answer" to bringing us closer to systematic review automation? Syst Rev. R Qureshi, D Shaughnessy, Kar Gill, 20231272</p>
<p>ChatGPT for editors: enhancing efficiency and effectiveness. Y Whang, Sci Editing. 112024</p>
<p>Machine learningbased approach for identifying research gaps: COVID-19 as a case study. A Abd-Alrazaq, A J Nashwan, Z Shah, JMIR Form Res. 8e494112024</p>
<p>Survey of BERTbase models for scientific text classification: COVID-19 case study. M Khadhraoui, H Bellaaj, M B Ammar, Appl Sci. 1228912022</p>
<p>Sentiment analysis for software quality assessment. F Liang, F Hou, S Farshidi, S Jansen, CEUR Workshop Proceedings. 3567</p>
<p>Empowering interdisciplinary research with BERT-based models: an approach through SciBERT-CNN with topic modeling. D Likhareva, H Sankaran, S Thiyagarajan, 2024arXiv, preprint arXivnot peer reviewed</p>
<p>Harnessing the power of ChatGPT for automating systematic review process: methodology, case study, limitations, and future directions. A Alshami, M Elsayed, E Ali, Systems. 113512023</p>
<p>Artificial intelligence research in business and management: a literature review leveraging machine learning and large language models. N Guler, S Kirshner, R Vidgen, SSRN J. 2023</p>
<p>Concept induction: analyzing unstructured text with high-level concepts using LLooM. M S Lam, J Teoh, J A Landay, J Heer, M S Bernstein, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Effectiveness of generative artificial intelligence for scientific content analysis. M Platt, D Platt, 10.1109/AICT59525.2023.103131672023. April 24, 2025</p>
<p>Automated category and trend analysis of scientific articles on ophthalmology using large language models: development and usability study. H Raja, A Munawar, N Mylonas, JMIR Form Res. 8e524622024</p>
<p>ChatGPT in scholarly discourse: sentiments and an inflection point. H Twinomurinzi, S Gumbo, 10.1007/978-3-031-39652-6_172023. April 24, 2025</p>
<p>Risk of bias assessment in preclinical literature using natural language processing. Q Wang, J Liao, M Lapata, Res Synth Methods. 132022</p>
<p>Assessing the risk of bias in randomized clinical trials with large language models. H Lai, L Ge, M Sun, JAMA Netw Open. 7e24126872024</p>
<p>Benchmarking human-AI collaboration for common evidence appraisal tools. T Woelfle, J Hirt, P Janiaud, L Kappos, Jpa Ioannidis, L G Hemkens, medRxiv 2024.04.21.243061372024</p>
<p>Pilot study on large language models for risk-of-bias assessments in systematic reviews: A(I) new type of bias?. J Barsby, S Hume, H A Lemmey, J Cutteridge, R Lee, K D Bera, BMJ Evid Based Med. 302025</p>
<p>FacTool: factuality detection in generative AI-a tool augmented framework for multitask and multi-domain scenarios. I C Chern, S Chern, S Chen, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>Integrating large language models in systematic reviews: a framework and case study using ROBINS-I for risk of bias assessment. B Hasan, S Saadi, N S Rajjoub, BMJ Evid Based Med. 292024</p>
<p>ChatGPT for assessing risk of bias of randomized trials using the RoB 2.0 tool: a methods study. T Pitre, T Jassal, J R Talukdar, M Shahab, M Ling, D Zeraatkar, medRxiv 2023.11.19.232987272024</p>
<p>Comparative study of ChatGPT and human evaluators on the assessment of medical literature according to recognised reporting standards. R H Roberts, S R Ali, H A Hutchings, BMJ Health Care Inform. 30e1008302023</p>
<p>A day in the life of ChatGPT as an academic reviewer: investigating the potential of large language model for scientific literature review. M Srivastava, 2023OSF Preprints</p>
<p>Assessing risk of bias using ChatGPT-4 and Cochrane ROB2 Tool. Trevino-Juarez As, Med Sci Educ. 342024</p>
<p>Supporting systematic literature reviews using deep-learning-based language models. R Alchokr, M Borkar, M Thotadarya, S Saake, G Leich, T , 10.1145/3528588.35286582022. April 24, 2025</p>
<p>Revealing opinions for COVID-19 questions using a context retriever, opinion aggregator, and questionanswering model: model development study. Z H Lu, J X Wang, X Li, J Med Internet Res. 23e228602021</p>
<p>Guidance for clinical evaluation under the medical device regulation through automated scoping searches. F-Sk-B Tang, M Bukowski, T Schmitz-Rode, Appl Sci. 1376392023</p>
<p>Navigating the landscape of personalized medicine: the relevance of ChatGPT, BingChat, and Bard AI in Nephrology literature searches. N Aiumtrakul, C Thongprayoon, S Suppadungsuk, J Pers Med. 1314572023</p>
<p>Hallucination rates and reference accuracy of ChatGPT and Bard for systematic reviews: comparative analysis. M Chelli, J Descamps, Lavou� E V, J Med Internet Res. 26e531642024</p>
<p>Using LLM (large language model) to improve efficiency in literature review for undergraduate research. S A Antu, H Chen, C K Richards, LLM@ AIED. Tokyo, Japan. CEUR-WS.org2023</p>
<p>D Choueka, A L Tabakin, D F Shalom, ChatGPT in urogynecology research: novel or not? Urogynecology (Phila). 2022</p>
<p>Inquiry frameworks for research question scoping in DSR: a realization for ChatGPT. O D� Iaz, X Garmendia, J P Contell, J Pereira, 10.1007/978-3-031-32808-4_192023. April 24, 2025</p>
<p>Barriers and suggested solutions to nursing participation in research: a systematic review with NLP Tools (Preprint). N Goldfarb, N Tal, I-C Cohen, 2024JMIR Preprints</p>
<p>Utilization of ChatGPT for plastic surgery research: friend or foe?. R Gupta, I Herzog, J Weisberger, J Plast Reconstr Aesthet Surg. 802023</p>
<p>Expanding cosmetic plastic surgery research with ChatGPT. R Gupta, J B Park, C Bisht, Aesthet Surg J. 432023</p>
<p>The use of generative AI for scientific literature searches for systematic reviews: ChatGPT and Microsoft Bing AI performance evaluation. Y N Gwon, J H Kim, H S Chung, JMIR Med Inform. 12e511872024</p>
<p>Accelerating literature screening for systematic literature reviews with large language models-development, application, and first evaluation of a solution. P Herbst, H Baars, 2023. April 24, 2025</p>
<p>Streamlining the selection phase of systematic literature reviews (SLRs) using AI-enabled GPT-4 assistant API. Sma Jafari, 2024arXiv, preprint arXivnot peer reviewed</p>
<p>J Kim, J-S Lee, H Kim, T Lee, Systematic review on healthcare systems engineering utilizing ChatGPT. arXiv, preprint arXiv. 2024not peer reviewed</p>
<p>RefAI: a GPT-powered retrieval-augmented generative tool for biomedical literature recommendation and summarization. Y Li, J Zhao, M Li, J Am Med Inform Assoc. 312024</p>
<p>CoQuest: exploring research question co-creation with an LLM-based agent. Y Liu, S Chen, H Cheng, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>Is generative pre-trained transformer artificial intelligence (Chat-GPT) a reliable tool for guidelines synthesis? A preliminary evaluation for biologic CRSwNP therapy. A Maniaci, A M Saibene, C Calvo-Henriquez, Eur Arch Otorhinolaryngol. 2812024</p>
<p>GEAR-Up: generative AI and external knowledge-based retrieval upgrading scholarly article searches for systematic reviews. K Roy, V Khandelwal, V Vera, H Surana, H Heckman, A Sheth, 10.1609/aaai.v38i21.305772024. April 24, 2025</p>
<p>Assessing the efficacy of ChatGPT versus human researchers in identifying relevant studies on mHealth interventions for improving medication adherence in patients with ischemic stroke when conducting systematic reviews: comparative analysis. S Ruksakulpiwat, L Phianhasin, C Benjasirisan, JMIR Mhealth Uhealth. 12e515262024</p>
<p>Utility of artificial intelligence in orthopedic surgery literature review: a comparative pilot study. R Y Sanii, J K Kasto, W B Wines, Orthopedics. 472024</p>
<p>ChatGPT as a tool for conducting literature review for dry eye disease. S Singh, S Watson, Clin Exp Ophthalmol. 512023</p>
<p>Human-AI collaboration to identify literature for evidence synthesis. S Spillias, 10.21203/rs.3.rs-3099291/v1Research Square. 2023. April 24, 2025</p>
<p>Examining the validity of ChatGPT in identifying relevant nephrology literature: findings and implications. S Suppadungsuk, C Thongprayoon, P Krisanapan, J Clin Med. 1255502023</p>
<p>Can ChatGPT write a good Boolean query for systematic review literature search?. S Wang, H Scells, B Koopman, G Zuccon, arXiv2023preprint arXiv. not peer reviewed</p>
<p>Leveraging generative AI to prioritize drug repurposing candidates for Alzheimer's disease with real-world clinical validation. C Yan, M E Grabowska, A L Dickson, NPJ Digit Med. 7462024</p>
<p>Hierarchical catalogue generation for literature review: a benchmark. arXiv, preprint arXiv, not peer reviewed. K Zhu, X Feng, X Feng, Y Wu, B Qin, Journal of the American Medical Informatics Association. 3262023. 1084. 2025</p>
<p>Leveraging large language models for literature review tasks-a case study using ChatGPT. R Zimmermann, M Staab, M Nasseri, P Brandtner, 10.1007/978-3-031-48858-0_252024. April 24, 2025</p>
<p>PRISMA systematic literature review, including with Meta-Analysis vs Chatbot/GPT (AI) regarding current scientific data on the main effects of the calf blood deproteinized hemoderivative medicine (Actovegin) in ischemic stroke. A Anghelescu, F C Firan, G Onose, Biomedicines. 1116232023</p>
<p>Use of AI-driven code generation models in teaching and learning programming: a systematic literature review. D Cambaz, X Zhang, 10.1145/3626252.36309582024. April 24, 2025</p>
<p>The ethics of ChatGPT in medicine and healthcare: a systematic review on Large Language Models (LLMs). J Haltaufderheide, R Ranisch, NPJ Digit Med. 71832024</p>
<p>Using ChatGPT in medical research: current status and future directions. S Ruksakulpiwat, A Kumar, A Ajibade, J Multidiscip Healthc. 162023</p>
<p>ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. M Sallam, Healthcare (Basel). 118872023</p>
<p>A rapid scoping review and conceptual analysis of the educational metaverse in the Global South: sociotechnical perspectives. A Srivastava, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>P Zhao, X Zhang, J Cao, M M Cheng, J Yang, X Li, A literature review of literature reviews in pattern analysis and machine intelligence. arXiv, preprint arXiv. 2024not peer reviewed</p>
<p>Automatic categorization of self-acknowledged limitations in randomized controlled trial publications. M Lan, M Cheng, L Hoang, J Biomed Inform. 1521046282024</p>
<p>The concept of system for automated scientific literature reviews generation. A Teslyuk, 10.1007/978-3-030-50420-5_322020. April 24, 2025</p>
<p>OpenAI ChatGPT generated literature review: digital twin in healthcare. € O Aydın, E Karaarslan, Emerging Computer Technologies. € Aydın, Elsevier2</p>
<p>Evaluating a large language model's ability to answer clinicians' requests for evidence summaries. M N Blasingame, T Y Koonce, A M Williams, medRxiv 2024.05.01.243066912024</p>
<p>Easing the burden on caregivers -applications of artificial intelligence for physicians and caregivers of children with cleft lip and palate. Cleft Palate Craniofac J. S C Chaker, Y-C Hung, M Saad, 10.1177/105566562312235962024</p>
<p>Comparing meta-analyses with ChatGPT in the evaluation of the effectiveness and tolerance of systemic therapies in moderate-to-severe plaque psoriasis. Lam Hoai, X L Simonart, T , J Clin Med. 1254102023</p>
<p>Evaluating large language models on medical evidence summarization. L Tang, Z Sun, B Idnay, NPJ Digit Med. 61582023</p>
<p>Leveraging generative AI to prioritize drug repurposing candidates: validating identified candidates for Alzheimer's disease in real-world clinical datasets. C Yan, M E Grabowska, A L Dickson, medRxiv 2023.07.07.232923882023</p>
<p>Evaluating pre-trained language models on multidocument summarization for literature reviews. B Yu, Proceedings of the Third Workshop on Scholarly Document Processing. the Third Workshop on Scholarly Document Processing2022</p>
<p>Y Li, L Chen, A Liu, K Yu, L Wen, ChatCite: LLM agent with human workflow guidance for comparative literature summary. arXiv, preprint arXiv. 2024not peer reviewed</p>
<p>ChatGPT and its role in the decision-making for the diagnosis and treatment of lumbar spinal stenosis: a comparative analysis and narrative review. R Rajjoub, J S Arroyave, B Zaidat, Global Spine J. 142024</p>
<p>Overview of early ChatGPT's presence in medical literature: insights from a hybrid literature review by ChatGPT and human experts. O Temsah, S A Khan, Y Chaiah, Cureus. 15e372812023</p>
<p>Using the contextual language model BERT for multi-criteria classification of scientific articles. A K Ambalavanan, M V Devarakonda, J Biomed Inform. 1121035782020</p>
<p>srBERT: automatic article classification model for systematic review using. S Aum, S Choe, BERT. Syst Rev. 102852021</p>
<p>ADVISE: accelerating the creation of evidence syntheses for global development using natural language processing-supported human-AI collaboration. K M Edwards, B Song, J Porciello, J Mech Desig. 1462024</p>
<p>BERT for complex systematic review screening to support the future of medical research. M Hasny, A P Vasile, M Gianni, 10.1007/978-3-031-34344-5_212023. April 24, 2025</p>
<p>Relevance feedback strategies for recall-oriented neural information retrieval. T Kats, P Van Der Putten, J Scholtes, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>X Mao, B Koopman, G Zuccon, A reproducibility study of goldilocks: just-right tuning of BERT for TAR. arXiv, preprint arXiv. 2024not peer reviewed</p>
<p>Semi-automating abstract screening with a natural language model pretrained on biomedical literature. Sh-X Ng, K L Teow, G Y Ang, Syst Rev. 121722023</p>
<p>Natural language processing was effective in assisting rapid title and abstract screening when updating systematic reviews. X Qin, J Liu, Y Wang, J Clin Epidemiol. 1332021</p>
<p>S Wang, H Scells, B Koopman, Neural rankers for effective screening prioritisation in medical systematic review literature search. arXiv, preprint arXiv. 2022not peer reviewed</p>
<p>Leveraging the potential of generative AI to accelerate systematic literature reviews: an example in the area of educational technology. P Castillo-Segura, C Alario-Hoyos, C D Kloos, Fernandez Panadero, C , 10.1109/WEEF-GEDC59520.2023.103440982023. April 24, 2025</p>
<p>A novel question-answering framework for automated abstract screening using large language models. O Akinseloyin, X Jiang, V Palade, medRxiv 2023.12.17.233001022024</p>
<p>Can machine learning help accelerate article screening for systematic reviews? Yes, when article separability in embedding space is high. F Ali, EdArXiv, preprint EdArXiv. 2024not peer reviewed</p>
<p>Utilizing ChatGPT to select literature for meta-analysis shows workload reduction while maintaining a similar recall level as manual curation. X Cai, Y Geng, Y Du, medRxiv 2023.09.06.232950722023</p>
<p>Automated paper screening for clinical reviews using large language models: data analysis study. E Guo, M Gupta, J Deng, J Med Internet Res. 26e489962024</p>
<p>M€ antyl€ a M. 2024. The promise and challenges of using LLMs to accelerate the screening process of systematic reviews. A Huotala, M Kuutila, P Ralph, arXivpreprint arXiv, not peer reviewed</p>
<p>Methodological insights into ChatGPT's screening performance in systematic reviews. M Issaiy, H Ghanaati, S Kolahi, BMC Med Res Methodol. 24782024</p>
<p>Development of meta-prompts for large language models to screen titles and abstracts for diagnostic test accuracy reviews. Y Kataoka, R So, M Banno, medRxiv 2023.10.31.232978182023</p>
<p>Evaluating the effectiveness of large language models in abstract screening: a comparative analysis. M Li, J Sun, X Tan, Syst Rev. 132192024</p>
<p>Bio-SIEVE: exploring instruction tuning large language models for systematic review automation. arXiv, preprint arXiv, not peer reviewed. A Robinson, W Thorne, B P Wu, 10.48550/arxiv.2308.066102023. April 24, 2025</p>
<p>Assessing the ability of ChatGPT to screen articles for systematic reviews. E Syriani, I David, G Kumar, 10.48550/arxiv.2307.064642023. April 24, 2025arXiv, preprint arXiv</p>
<p>Sensitivity and specificity of using GPT-3.5 Turbo models for title and abstract screening in systematic reviews and meta-analyses. V-T Tran, G Gartlehner, S Yaacoub, Ann Intern Med. 1772024</p>
<p>Generating natural language queries for more effective systematic review screening prioritisation. S Wang, H Scells, B Koopman, M Potthast, G Zuccon, Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region2023</p>
<p>Zero-shot generative large language models for systematic review screening automation. S Wang, H Scells, S Zhuang, M Potthast, B Koopman, G Zuccon, European Conference on Information Retrieval. Springer2024</p>
<p>Automated title and abstract screening for scoping reviews using the GPT-4 large language model. D Wilkins, 2023arXiv, preprint arXivnot peer reviewed</p>
<p>Automating biomedical literature review for rapid drug discovery: leveraging GPT-4 to expedite pandemic response. J Yang, K C Walker, A A Bekar-Cesaretli, Int J Med Inform. 1891055002024</p>
<p>Natural language processing applications in the clinical neurosciences: a machine learning augmented systematic review. Q D Buchlak, N Esmaili, C Bennett, Acta Neurochir Suppl. 1342022</p>
<p>Clinical outcomes associated with robotic and computer-navigated total knee arthroplasty: a machine learning-augmented systematic review. Q D Buchlak, J Clair, N Esmaili, Eur J Orthop Surg Traumatol. 322022</p>
<p>Development of prompt templates for large language model-driven screening in systematic reviews. C Cao, J Sang, R Arora, Ann Intern Med. 1782025</p>
<p>Interacting with the artificial intelligence (AI) language model ChatGPT: a synopsis of Earth observation and remote sensing in archaeology. A Agapiou, V Lysandrou, Heritage. 62023</p>
<p>Single screening versus conventional double screening for study selection in systematic reviews: a methodological systematic review. S Waffenschmidt, M Knelangen, W Sieben, BMC Med Res Methodol. 191322019</p>
<p>Detecting LLMgenerated text in computing education: comparative study for ChatGPT cases. M S Orenstrakh, O Karnalim, C A Suarez, M Liut, 2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC). IEEE2024</p>
<p>Mapping the increasing use of LLMS in scientific papers. W Liang, Y Zhang, Z Wu, arXiv:2404.012682024arXiv, preprintnot peer reviewed</p>
<p>The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. M Hosseini, D B Resnik, K L Holmes, Res Ethics. 192023</p>            </div>
        </div>

    </div>
</body>
</html>
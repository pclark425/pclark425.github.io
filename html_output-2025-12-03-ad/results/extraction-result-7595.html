<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7595 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7595</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7595</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-f57afb6c8addfc7a32f9be5916a374a542d1a026</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f57afb6c8addfc7a32f9be5916a374a542d1a026" target="_blank">ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work presents ECG-QA, the first QA dataset specifically designed for ECG analysis, which comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility.</p>
                <p><strong>Paper Abstract:</strong> Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the development of intelligent QA systems capable of assisting clinicians in ECG interpretations. Dataset URL: https://github.com/Jwoo5/ecg-qa</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7595.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7595.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (zero-shot, SE-WRN prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (zero-shot) prompted with SE-WRN-generated ECG interpretations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (legacy version used in experiments) was given textual ECG interpretations produced by an upstream ECG classifier (SE-WRN) along with the question and a constrained set of answer options; the model was prompted zero-shot to select an answer from the given options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM by OpenAI (legacy version referenced as gpt-4-0314 in the paper); used in zero-shot setting to map textual ECG interpretations + question -> answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECG-QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Question-answering over ECGs where ECG signals are first turned into text interpretations by an ECG classifier (SE-WRN) and then answered by an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt that includes SE-WRN-generated ECG interpretation lines (attribute:score), the question, and an explicit 'Options:' list limiting allowable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot (no few-shot examples), SE-WRN outputs thresholded at 0.5 to include attributes; the prompt lists interpretations per lead (or overall) with numeric scores, provides candidate answer options for one-to-one evaluation, and instructs 'Only answer based on the given Options without any explanation.' 10% of the test set randomly sampled (due to API quota); single-run evaluation (no seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match accuracy (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>S-Verify: 71.0% EM; S-Choose: 48.1% EM; S-Query: 35.7% EM; CC-Verify: 54.9% EM; CC-Query: 13.0% EM; CI-Verify: 68.8% EM; CI-Query: 2.53% EM (values from sampled 10% test set reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>M^3AE (best QA baseline) on same sampled splits: S-Verify 76.0% EM; S-Choose 58.2% EM; S-Query 40.0% EM; CC-Verify 74.7% EM; CC-Query 21.2% EM; CI-Verify 75.2% EM; CI-Query 4.36% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>S-Verify: -5.0% absolute (GPT-4 vs M^3AE); S-Choose: -10.1% abs; S-Query: -4.3% abs; CC-Verify: -19.8% abs; CC-Query: -8.2% abs; CI-Verify: -6.4% abs; CI-Query: -1.83% abs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>LLM received SE-WRN outputs filtered by threshold 0.5; prompts included explicit Options; zero-shot; 10% of ECG-QA test sampled; one run per model due to API quota; SE-WRN used as upstream interpretation model.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7595.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7595.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo (zero-shot, SE-WRN prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI gpt-3.5-turbo (zero-shot) prompted with SE-WRN-generated ECG interpretations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>gpt-3.5-turbo (legacy) was used similarly to GPT-4: given SE-WRN textual ECG interpretations plus the question and constrained options in a zero-shot prompt, instructed to answer only from the supplied options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's gpt-3.5-turbo family LLM (legacy version gpt-3.5-turbo-0301 referenced); used zero-shot with classifier-derived ECG text inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECG-QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer ECG interpretation QA by reading classifier-produced textual descriptions and then selecting from given options.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt containing SE-WRN attribute:score lines, the question, and explicit candidate Options; forced to choose from Options only.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot, threshold=0.5 on SE-WRN outputs to select attributes to display; candidate answer options enumerated in prompt; instructed to output only the option string without explanation; 10% sampled test set; single-run.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match accuracy (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>S-Verify: 69.3% EM; S-Choose: 36.1% EM; S-Query: 31.1% EM; CC-Verify: 58.2% EM; CC-Query: 10.5% EM; CI-Verify: 64.1% EM; CI-Query: 1.32% EM (sampled 10% test).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>M^3AE baseline (sampled same splits): S-Verify 76.0% EM; S-Choose 58.2% EM; S-Query 40.0% EM; CC-Verify 74.7% EM; CC-Query 21.2% EM; CI-Verify 75.2% EM; CI-Query 4.36% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>S-Verify: -6.7% abs vs M^3AE; S-Choose: -22.1% abs; S-Query: -8.9% abs; CC-Verify: -16.5% abs; CC-Query: -10.7% abs; CI-Verify: -11.1% abs; CI-Query: -3.04% abs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts with SE-WRN outputs (threshold 0.5); prompts constrain output to given Options; 10% test sampling; single evaluation run due to API quota.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7595.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7595.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003 (zero-shot, SE-WRN prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-davinci-003 (zero-shot) prompted with SE-WRN-generated ECG interpretations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>text-davinci-003 was evaluated in the same pipeline: SE-WRN produced textual ECG interpretations which were passed into zero-shot prompts that enumerated answer options and asked the model to pick one.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Legacy OpenAI instruction-following model (text-davinci-003); used zero-shot with SE-WRN-derived textual ECG summaries and constrained answer options.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECG-QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Respond to ECG-related QA using classifier-derived textual descriptions of ECG signals; answers restricted to enumerated options.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt with classifier outputs (attribute:score), question text, and explicit Options list; instruction to reply only with one option.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot (no few-shot), SE-WRN outputs filtered at 0.5, Options provided in the prompt to restrict allowed outputs; tested on 10% of test set; single-run.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match accuracy (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>S-Verify: 75.0% EM; S-Choose: 37.8% EM; S-Query: 36.0% EM; CC-Verify: 56.3% EM; CC-Query: 15.4% EM; CI-Verify: 71.5% EM; CI-Query: 1.40% EM (sampled 10% test).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>M^3AE baseline (sampled same splits): S-Verify 76.0% EM; S-Choose 58.2% EM; S-Query 40.0% EM; CC-Verify 74.7% EM; CC-Query 21.2% EM; CI-Verify 75.2% EM; CI-Query 4.36% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>S-Verify: -1.0% abs vs M^3AE; S-Choose: -20.4% abs; S-Query: -4.0% abs; CC-Verify: -18.4% abs; CC-Query: -5.8% abs; CI-Verify: -3.7% abs; CI-Query: -2.96% abs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>SE-WRN outputs thresholded at 0.5 converted to human-readable lines; prompts include Options; zero-shot; 10% test sample; single trial.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7595.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7595.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Question Type (Verify vs Choose vs Query)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Problem presentation: question-type formatting (Verify = yes/no/not sure; Choose = select between two options; Query = open-ended retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper categorizes QA prompts by question-type (Verify/Choose/Query) and demonstrates that format (type of question) substantially affects model performance: Verify (yes/no) questions are systematically easier than Choose (binary choice) and Query (open-ended) across QA baselines and LLM setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M^3AE, MedViLL, Fusion Transformer (QA baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-modal QA baselines (pretrained multimodal masked/vision-language methods: M^3AE and MedViLL adapted to ECGs, and a Fusion Transformer) evaluated on the same dataset but with different question-type formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECG-QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering clinically-grounded ECG questions presented in one of three question formats requiring different reasoning: Verify (yes/no/not sure), Choose (pick one of two options), Query (retrieve attribute(s)).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Question-type (format) distinction: Verify = constrained categorical (yes/no/not sure), Choose = selection from two named options, Query = free-form selection from a larger set of possible attributes (converted to multi-label outputs for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Verify questions typically have 2–3 answers; Choose has 4 possible joint outcomes but conceptually a binary choice; Query requires returning one or multiple attributes (open-ended within constrained answer set). The dataset enforces different sampling strategies and candidate-answer restrictions per type (e.g., AUROC computed only among valid options for a question).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match accuracy (EM) and AUROC (both reported in paper; EM emphasizes full-answer correctness, AUROC captures ranking capability among valid options).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative M^3AE results (Table 2 & Table 4): Single-Verify EM 74.0% (AUROC 0.781); Single-Choose EM 57.1% (AUROC 0.850); Single-Query EM 41.0% (AUROC 0.836). Across models Verify > Choose > Query in EM, indicating simpler formats (yes/no) yield higher EM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>per Q-type majority prior (majority class): S-Verify 67.7% EM; S-Choose 31.2% EM; S-Query 23.2% EM — shows models improve over naive priors but margin varies by question type.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Relative differences (approx): For M^3AE, S-Verify vs S-Query: +33.0% relative improvement in EM (74.0% vs 41.0%); S-Choose sits between (57.1% EM). AUROC differences smaller but EM shows large sensitivity to question format.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All QA baselines trained and evaluated on ECG-QA; evaluation metrics computed per question type; AUROC calculated only among valid answer candidates per question to avoid overestimation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>95% confidence intervals reported across 3 random seeds for some baseline metrics (paper notes CI reporting), but specific p-values for format comparisons not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7595.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7595.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt lexical variation (paraphrases)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphrase-based lexical variation in question phrasing (template paraphrases)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The dataset includes multiple paraphrases per question template (machine-generated by ChatGPT then manually curated) and ensures that paraphrases in the test split are not present in training, explicitly testing how lexical variations in prompts affect generalization of QA models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (dataset / prompt construction artefact)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paraphrase generation pipeline: for each template, ChatGPT produced 20 candidate paraphrases which were manually filtered and edited; 7 paraphrases used for training/validation and 3 reserved for test to evaluate lexical generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECG-QA (lexical-robustness evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether QA models generalize to unseen lexical variants of the question templates (i.e., prompt paraphrases) by withholding certain paraphrases from training and using them in test.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question templates with paraphrase variability; training uses a disjoint set of paraphrase realizations compared to test.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt variation / lexical form</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For each template ChatGPT generated 20 candidates; manual curation ensured medical terms preserved; final selection: 7 paraphrases for training/validation and 3 for test (ensuring paraphrase-level generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not directly reported as a single delta in the paper (authors state they withheld test paraphrases to evaluate generalizability), models evaluated on test paraphrases as part of overall EM/AUROC metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Paraphrase candidates generated by ChatGPT then manually curated; paraphrase splits controlled so test paraphrases were unseen during training; no isolated ablation numbers for paraphrase impact reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7595.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7595.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt constraint: enumerated Options</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of enumerated candidate answer options in prompts to restrict LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When querying LLMs, prompts explicitly included valid answer options and instructed the model to answer only from these options; this was done to enable exact-match evaluation and one-to-one mapping between LLM responses and ground-truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs in experiments (GPT-4, gpt-3.5-turbo, text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs were constrained at prompt-time by enumerating candidate answer options (e.g., 'Options: lead I, lead II, ...' or 'Options: yes, no, not sure') and instructing the model to choose only from those.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECG-QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selecting the correct answer from an enumerated list (constrained output vocabulary) given classifier-derived ECG text and a natural-language question.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt augmented with explicit 'Options:' list; output constrained to listed tokens to facilitate exact-match scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / output constraint</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts varied by question: for lead-specific queries the prompt enumerated all 12 leads; for Verify questions 'Options: yes, no, not sure' were provided; instruction 'Only answer based on the given Options without any explanation.'</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match accuracy (EM) reported for LLMs on sampled test set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Used together with SE-WRN outputs; zero-shot; one-run; 10% sampled test set; designed to force discrete outputs for automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chatcad: Interactive computeraided diagnosis on medical image using large language models <em>(Rating: 2)</em></li>
                <li>Multimodal masked autoencoders for medical vision-and-language pre-training <em>(Rating: 2)</em></li>
                <li>Multi-modal understanding and generation for medical images and text via vision-language pre-training <em>(Rating: 2)</em></li>
                <li>VQA: Visual question answering <em>(Rating: 1)</em></li>
                <li>GQA: A new dataset for real-world visual reasoning and compositional question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7595",
    "paper_id": "paper-f57afb6c8addfc7a32f9be5916a374a542d1a026",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "GPT-4 (zero-shot, SE-WRN prompts)",
            "name_full": "OpenAI GPT-4 (zero-shot) prompted with SE-WRN-generated ECG interpretations",
            "brief_description": "GPT-4 (legacy version used in experiments) was given textual ECG interpretations produced by an upstream ECG classifier (SE-WRN) along with the question and a constrained set of answer options; the model was prompted zero-shot to select an answer from the given options.",
            "citation_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based LLM by OpenAI (legacy version referenced as gpt-4-0314 in the paper); used in zero-shot setting to map textual ECG interpretations + question -&gt; answer.",
            "model_size": null,
            "task_name": "ECG-QA",
            "task_description": "Question-answering over ECGs where ECG signals are first turned into text interpretations by an ECG classifier (SE-WRN) and then answered by an LLM.",
            "problem_format": "Zero-shot natural-language prompt that includes SE-WRN-generated ECG interpretation lines (attribute:score), the question, and an explicit 'Options:' list limiting allowable outputs.",
            "format_category": "prompt style",
            "format_details": "Zero-shot (no few-shot examples), SE-WRN outputs thresholded at 0.5 to include attributes; the prompt lists interpretations per lead (or overall) with numeric scores, provides candidate answer options for one-to-one evaluation, and instructs 'Only answer based on the given Options without any explanation.' 10% of the test set randomly sampled (due to API quota); single-run evaluation (no seeds).",
            "performance_metric": "Exact Match accuracy (EM)",
            "performance_value": "S-Verify: 71.0% EM; S-Choose: 48.1% EM; S-Query: 35.7% EM; CC-Verify: 54.9% EM; CC-Query: 13.0% EM; CI-Verify: 68.8% EM; CI-Query: 2.53% EM (values from sampled 10% test set reported in Table 5).",
            "baseline_performance": "M^3AE (best QA baseline) on same sampled splits: S-Verify 76.0% EM; S-Choose 58.2% EM; S-Query 40.0% EM; CC-Verify 74.7% EM; CC-Query 21.2% EM; CI-Verify 75.2% EM; CI-Query 4.36% EM.",
            "performance_change": "S-Verify: -5.0% absolute (GPT-4 vs M^3AE); S-Choose: -10.1% abs; S-Query: -4.3% abs; CC-Verify: -19.8% abs; CC-Query: -8.2% abs; CI-Verify: -6.4% abs; CI-Query: -1.83% abs.",
            "experimental_setting": "LLM received SE-WRN outputs filtered by threshold 0.5; prompts included explicit Options; zero-shot; 10% of ECG-QA test sampled; one run per model due to API quota; SE-WRN used as upstream interpretation model.",
            "statistical_significance": null,
            "uuid": "e7595.0",
            "source_info": {
                "paper_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "gpt-3.5-turbo (zero-shot, SE-WRN prompts)",
            "name_full": "OpenAI gpt-3.5-turbo (zero-shot) prompted with SE-WRN-generated ECG interpretations",
            "brief_description": "gpt-3.5-turbo (legacy) was used similarly to GPT-4: given SE-WRN textual ECG interpretations plus the question and constrained options in a zero-shot prompt, instructed to answer only from the supplied options.",
            "citation_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "OpenAI's gpt-3.5-turbo family LLM (legacy version gpt-3.5-turbo-0301 referenced); used zero-shot with classifier-derived ECG text inputs.",
            "model_size": null,
            "task_name": "ECG-QA",
            "task_description": "Answer ECG interpretation QA by reading classifier-produced textual descriptions and then selecting from given options.",
            "problem_format": "Zero-shot natural-language prompt containing SE-WRN attribute:score lines, the question, and explicit candidate Options; forced to choose from Options only.",
            "format_category": "prompt style",
            "format_details": "Zero-shot, threshold=0.5 on SE-WRN outputs to select attributes to display; candidate answer options enumerated in prompt; instructed to output only the option string without explanation; 10% sampled test set; single-run.",
            "performance_metric": "Exact Match accuracy (EM)",
            "performance_value": "S-Verify: 69.3% EM; S-Choose: 36.1% EM; S-Query: 31.1% EM; CC-Verify: 58.2% EM; CC-Query: 10.5% EM; CI-Verify: 64.1% EM; CI-Query: 1.32% EM (sampled 10% test).",
            "baseline_performance": "M^3AE baseline (sampled same splits): S-Verify 76.0% EM; S-Choose 58.2% EM; S-Query 40.0% EM; CC-Verify 74.7% EM; CC-Query 21.2% EM; CI-Verify 75.2% EM; CI-Query 4.36% EM.",
            "performance_change": "S-Verify: -6.7% abs vs M^3AE; S-Choose: -22.1% abs; S-Query: -8.9% abs; CC-Verify: -16.5% abs; CC-Query: -10.7% abs; CI-Verify: -11.1% abs; CI-Query: -3.04% abs.",
            "experimental_setting": "Zero-shot prompts with SE-WRN outputs (threshold 0.5); prompts constrain output to given Options; 10% test sampling; single evaluation run due to API quota.",
            "statistical_significance": null,
            "uuid": "e7595.1",
            "source_info": {
                "paper_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "text-davinci-003 (zero-shot, SE-WRN prompts)",
            "name_full": "OpenAI text-davinci-003 (zero-shot) prompted with SE-WRN-generated ECG interpretations",
            "brief_description": "text-davinci-003 was evaluated in the same pipeline: SE-WRN produced textual ECG interpretations which were passed into zero-shot prompts that enumerated answer options and asked the model to pick one.",
            "citation_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "Legacy OpenAI instruction-following model (text-davinci-003); used zero-shot with SE-WRN-derived textual ECG summaries and constrained answer options.",
            "model_size": null,
            "task_name": "ECG-QA",
            "task_description": "Respond to ECG-related QA using classifier-derived textual descriptions of ECG signals; answers restricted to enumerated options.",
            "problem_format": "Zero-shot natural-language prompt with classifier outputs (attribute:score), question text, and explicit Options list; instruction to reply only with one option.",
            "format_category": "prompt style",
            "format_details": "Zero-shot (no few-shot), SE-WRN outputs filtered at 0.5, Options provided in the prompt to restrict allowed outputs; tested on 10% of test set; single-run.",
            "performance_metric": "Exact Match accuracy (EM)",
            "performance_value": "S-Verify: 75.0% EM; S-Choose: 37.8% EM; S-Query: 36.0% EM; CC-Verify: 56.3% EM; CC-Query: 15.4% EM; CI-Verify: 71.5% EM; CI-Query: 1.40% EM (sampled 10% test).",
            "baseline_performance": "M^3AE baseline (sampled same splits): S-Verify 76.0% EM; S-Choose 58.2% EM; S-Query 40.0% EM; CC-Verify 74.7% EM; CC-Query 21.2% EM; CI-Verify 75.2% EM; CI-Query 4.36% EM.",
            "performance_change": "S-Verify: -1.0% abs vs M^3AE; S-Choose: -20.4% abs; S-Query: -4.0% abs; CC-Verify: -18.4% abs; CC-Query: -5.8% abs; CI-Verify: -3.7% abs; CI-Query: -2.96% abs.",
            "experimental_setting": "SE-WRN outputs thresholded at 0.5 converted to human-readable lines; prompts include Options; zero-shot; 10% test sample; single trial.",
            "statistical_significance": null,
            "uuid": "e7595.2",
            "source_info": {
                "paper_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Question Type (Verify vs Choose vs Query)",
            "name_full": "Problem presentation: question-type formatting (Verify = yes/no/not sure; Choose = select between two options; Query = open-ended retrieval)",
            "brief_description": "The paper categorizes QA prompts by question-type (Verify/Choose/Query) and demonstrates that format (type of question) substantially affects model performance: Verify (yes/no) questions are systematically easier than Choose (binary choice) and Query (open-ended) across QA baselines and LLM setups.",
            "citation_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
            "mention_or_use": "use",
            "model_name": "M^3AE, MedViLL, Fusion Transformer (QA baselines)",
            "model_description": "Multi-modal QA baselines (pretrained multimodal masked/vision-language methods: M^3AE and MedViLL adapted to ECGs, and a Fusion Transformer) evaluated on the same dataset but with different question-type formats.",
            "model_size": null,
            "task_name": "ECG-QA",
            "task_description": "Answering clinically-grounded ECG questions presented in one of three question formats requiring different reasoning: Verify (yes/no/not sure), Choose (pick one of two options), Query (retrieve attribute(s)).",
            "problem_format": "Question-type (format) distinction: Verify = constrained categorical (yes/no/not sure), Choose = selection from two named options, Query = free-form selection from a larger set of possible attributes (converted to multi-label outputs for evaluation).",
            "format_category": "question type",
            "format_details": "Verify questions typically have 2–3 answers; Choose has 4 possible joint outcomes but conceptually a binary choice; Query requires returning one or multiple attributes (open-ended within constrained answer set). The dataset enforces different sampling strategies and candidate-answer restrictions per type (e.g., AUROC computed only among valid options for a question).",
            "performance_metric": "Exact Match accuracy (EM) and AUROC (both reported in paper; EM emphasizes full-answer correctness, AUROC captures ranking capability among valid options).",
            "performance_value": "Representative M^3AE results (Table 2 & Table 4): Single-Verify EM 74.0% (AUROC 0.781); Single-Choose EM 57.1% (AUROC 0.850); Single-Query EM 41.0% (AUROC 0.836). Across models Verify &gt; Choose &gt; Query in EM, indicating simpler formats (yes/no) yield higher EM performance.",
            "baseline_performance": "per Q-type majority prior (majority class): S-Verify 67.7% EM; S-Choose 31.2% EM; S-Query 23.2% EM — shows models improve over naive priors but margin varies by question type.",
            "performance_change": "Relative differences (approx): For M^3AE, S-Verify vs S-Query: +33.0% relative improvement in EM (74.0% vs 41.0%); S-Choose sits between (57.1% EM). AUROC differences smaller but EM shows large sensitivity to question format.",
            "experimental_setting": "All QA baselines trained and evaluated on ECG-QA; evaluation metrics computed per question type; AUROC calculated only among valid answer candidates per question to avoid overestimation.",
            "statistical_significance": "95% confidence intervals reported across 3 random seeds for some baseline metrics (paper notes CI reporting), but specific p-values for format comparisons not reported.",
            "uuid": "e7595.3",
            "source_info": {
                "paper_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Prompt lexical variation (paraphrases)",
            "name_full": "Paraphrase-based lexical variation in question phrasing (template paraphrases)",
            "brief_description": "The dataset includes multiple paraphrases per question template (machine-generated by ChatGPT then manually curated) and ensures that paraphrases in the test split are not present in training, explicitly testing how lexical variations in prompts affect generalization of QA models.",
            "citation_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
            "mention_or_use": "use",
            "model_name": "N/A (dataset / prompt construction artefact)",
            "model_description": "Paraphrase generation pipeline: for each template, ChatGPT produced 20 candidate paraphrases which were manually filtered and edited; 7 paraphrases used for training/validation and 3 reserved for test to evaluate lexical generalization.",
            "model_size": null,
            "task_name": "ECG-QA (lexical-robustness evaluation)",
            "task_description": "Assess whether QA models generalize to unseen lexical variants of the question templates (i.e., prompt paraphrases) by withholding certain paraphrases from training and using them in test.",
            "problem_format": "Natural-language question templates with paraphrase variability; training uses a disjoint set of paraphrase realizations compared to test.",
            "format_category": "prompt variation / lexical form",
            "format_details": "For each template ChatGPT generated 20 candidates; manual curation ensured medical terms preserved; final selection: 7 paraphrases for training/validation and 3 for test (ensuring paraphrase-level generalization).",
            "performance_metric": "Not directly reported as a single delta in the paper (authors state they withheld test paraphrases to evaluate generalizability), models evaluated on test paraphrases as part of overall EM/AUROC metrics.",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Paraphrase candidates generated by ChatGPT then manually curated; paraphrase splits controlled so test paraphrases were unseen during training; no isolated ablation numbers for paraphrase impact reported.",
            "statistical_significance": null,
            "uuid": "e7595.4",
            "source_info": {
                "paper_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Prompt constraint: enumerated Options",
            "name_full": "Use of enumerated candidate answer options in prompts to restrict LLM outputs",
            "brief_description": "When querying LLMs, prompts explicitly included valid answer options and instructed the model to answer only from these options; this was done to enable exact-match evaluation and one-to-one mapping between LLM responses and ground-truth labels.",
            "citation_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
            "mention_or_use": "use",
            "model_name": "LLMs in experiments (GPT-4, gpt-3.5-turbo, text-davinci-003)",
            "model_description": "LLMs were constrained at prompt-time by enumerating candidate answer options (e.g., 'Options: lead I, lead II, ...' or 'Options: yes, no, not sure') and instructing the model to choose only from those.",
            "model_size": null,
            "task_name": "ECG-QA",
            "task_description": "Selecting the correct answer from an enumerated list (constrained output vocabulary) given classifier-derived ECG text and a natural-language question.",
            "problem_format": "Natural-language prompt augmented with explicit 'Options:' list; output constrained to listed tokens to facilitate exact-match scoring.",
            "format_category": "prompt style / output constraint",
            "format_details": "Prompts varied by question: for lead-specific queries the prompt enumerated all 12 leads; for Verify questions 'Options: yes, no, not sure' were provided; instruction 'Only answer based on the given Options without any explanation.'",
            "performance_metric": "Exact Match accuracy (EM) reported for LLMs on sampled test set.",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Used together with SE-WRN outputs; zero-shot; one-run; 10% sampled test set; designed to force discrete outputs for automated scoring.",
            "statistical_significance": null,
            "uuid": "e7595.5",
            "source_info": {
                "paper_title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chatcad: Interactive computeraided diagnosis on medical image using large language models",
            "rating": 2
        },
        {
            "paper_title": "Multimodal masked autoencoders for medical vision-and-language pre-training",
            "rating": 2
        },
        {
            "paper_title": "Multi-modal understanding and generation for medical images and text via vision-language pre-training",
            "rating": 2
        },
        {
            "paper_title": "VQA: Visual question answering",
            "rating": 1
        },
        {
            "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "rating": 1
        }
    ],
    "cost": 0.0196495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram</h1>
<p>Jungwoo Oh ${ }^{1}$, Gyubok Lee ${ }^{1}$, Seongsu Bae ${ }^{1}$, Joon-myoung Kwon ${ }^{2}$, Edward Choi ${ }^{1}$<br>KAIST, Daejeon ${ }^{1}$ Medical AI Inc., Seoul ${ }^{2}$<br>{ojw0123, gyubok.lee, seongsu, edwardchoi}@kaist.ac.kr ${ }^{1}$<br>cto@medicalai.com ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the development of intelligent QA systems capable of assisting clinicians in ECG interpretations.</p>
<h2>1 Introduction</h2>
<p>In recent years, significant advancements in natural language processing have revolutionized the field of question answering (QA) in a wide range of domains. Previous works have demonstrated the great potential of QA systems in various domains, where they have been combined with different modalities such as images [1, 32, 15, 8] or tables with images [24, 13]. Concurrently, QA systems have also been explored in the healthcare domain, including visual QA with chest X-ray [11, 14], clinical-note-based QA [20], and QA over structured electronic health record (EHR) data [28, 12]. These pioneering efforts have successfully bridged the gap between general-domain QA and the medical field, unlocking new possibilities to improve healthcare outcomes and enhance medical decision-making processes.
Despite this remarkable progress, there is a noticeable absence of datasets that combine electrocardiogram (ECG) data with question answering. As a fundamental diagnostic tool in cardiology, ECG provides critical insights into the electrical activity of the heart and plays an important role in detecting various cardiac conditions [3, 22, 33]. Consequently, integrating ECG data with QA systems holds tremendous potential to improve the interpretation of cardiac data, leading to more accurate diagnoses and personalized treatment plans.
To this end, we present ECG-QA ${ }^{1}$, a novel QA dataset that incorporates ECG data for question answering tasks. To the best of our knowledge, ECG-QA is the first dataset that combines QA and ECG, opening up new avenues for integrating multi-modal machine learning with cardiac healthcare.
The main contributions of this work are threefold:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample question-answer pairs in ECG-QA. (a) Questions to a single ECG with various types of attributes. (b) Comparison questions between two different ECGs. Refer to Section 3.1.1 for more details about each attribute type.</p>
<ul>
<li>We propose the ECG-QA dataset, a diverse collection of questions focused on ECG interpretation and analysis (See Figure 1). This dataset introduces the novel concept of incorporating question answering into the realm of ECG analysis, making it a valuable resource for developing and evaluating QA systems in the context of cardiology.</li>
<li>To cover more complex yet clinically critical questions, we include questions that require comparative analysis of two ECGs (See Figure 1 (b)). This inclusion brings a new degree of complexity, as addressing these comparison questions extends beyond the conventional scope of ECG analysis using machine learning. By incorporating these types of questions, we not only address the real-world needs of medical professionals but also broaden the potential applications of machine learning in ECG analysis.</li>
<li>We provide a benchmark for QA models, including recent large language models (LLMs), on the ECG-QA dataset, promoting further research and encouraging the development of novel methods to leverage ECG signals for question answering tasks. We believe that ECG-QA will serve as a valuable resource in advancing machine learning applications in cardiology and improving medical decision-making processes.</li>
</ul>
<h1>2 Related works</h1>
<p>Medical QA datasets QA systems have been extensively explored in the healthcare domain, catering to the specific needs and challenges of medical data. However, most existing medical QA datasets are primarily based on clinical texts, medical images, or structured EHR tables. For example, Pampari et al. [20] proposed the emrQA dataset, consisting of question-answer pairs derived from unstructured clinical notes. In addition, Kovaleva et al. [11] and Liu et al. [14] proposed datasets for visual QA using X-ray images, aiming to investigate intelligent interactive systems for radiology. Meanwhile, in the field of QA over structured EHR data, Wang et al. [28] and Lee et al. [12] have developed datasets called MIMICSQL and EHRSQL respectively, which consist of questions and their corresponding SQL queries. While these healthcare QA systems demonstrate the potential of leveraging medical data with QA to improve healthcare outcomes, there is currently no dedicated QA dataset specifically designed for ECG data despite its widespread use in diagnosing cardiovascular conditions and monitoring patients' heart health.</p>
<p>Electrocardiogram Previous studies in the field of ECG have predominantly focused on using ECG data for diagnostic purposes such as identifying various heart diseases. For instance, Nejedly et al. [18] proposed an ensemble of residual networks with attention modules to classify cardiac diseases using ECGs, which won first place in the PhysioNet/Computing in Cardiology Challenge 2021 [21]. At the same challenge, Han et al. [6] achieved second place by utilizing SE-WRN [31], which is a</p>
<p>Table 1: Sample template questions for different question \&amp; attribute types in ECG-QA.</p>
<table>
<thead>
<tr>
<th>Question type</th>
<th>Attribute type</th>
<th>Example template question</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single-Verify</td>
<td>SCP Code</td>
<td>Does this ECG show symptoms of non-specific ST changes?</td>
</tr>
<tr>
<td></td>
<td>Noise</td>
<td>Does this ECG show baseline drift in lead I?</td>
</tr>
<tr>
<td></td>
<td>Stage of infarction</td>
<td>Does this ECG show early stage of myocardial infarction?</td>
</tr>
<tr>
<td></td>
<td>Extra systole</td>
<td>Does this ECG show ventricular extrasystoles?</td>
</tr>
<tr>
<td></td>
<td>Heart axis</td>
<td>Does this ECG show left axis deviation?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>Does the RR interval of this ECG fall within the normal range?</td>
</tr>
<tr>
<td>Single-Choose</td>
<td>SCP Code</td>
<td>Which symptom does this ECG show, conduction disturbance or hypertrophy?</td>
</tr>
<tr>
<td></td>
<td>Noise</td>
<td>Which noise does this ECG show, baseline drift or static noise?</td>
</tr>
<tr>
<td></td>
<td>Stage of infarction</td>
<td>Which stage of infarction is this ECG at, early stage of myocardial infarction or late stage of myocardial infarction?</td>
</tr>
<tr>
<td></td>
<td>Extra systole</td>
<td>Which kind of extra systoles does this ECG show, ventricular extrasystoles or supraventricular extrasystoles?</td>
</tr>
<tr>
<td></td>
<td>Heart axis</td>
<td>Which cardiac axis does this ECG show, left axis deviation or right axis deviation?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>Which range does the RR interval of this ECG fall in, below the normal range or within the normal range?</td>
</tr>
<tr>
<td>Single-Query</td>
<td>SCP Code</td>
<td>What form-related symptoms does this ECG show?</td>
</tr>
<tr>
<td></td>
<td>Noise</td>
<td>What kind of noises does this ECG show in lead I?</td>
</tr>
<tr>
<td></td>
<td>Stage of infarction</td>
<td>What stage of infarction is this ECG at?</td>
</tr>
<tr>
<td></td>
<td>Extra systole</td>
<td>What kind of extra systoles does this ECG show?</td>
</tr>
<tr>
<td></td>
<td>Heart axis</td>
<td>What direction is this ECG deviated to?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>What range does the RR interval of this ECG fall in?</td>
</tr>
<tr>
<td>Comparison-Consecutive-Verify</td>
<td>SCP Code</td>
<td>Compared to the previous tracing, has left ventricular hypertrophy been resolved in the recent tracing?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>Compared to the previous tracing, has the PR interval of the recent tracing become normal?</td>
</tr>
<tr>
<td>Comparison-Consecutive-Query</td>
<td>SCP Code</td>
<td>What symptoms have been resolved in the recent tracing as compared to the previous one?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>What numeric features of the recent tracing now have become normal compared to the previous one?</td>
</tr>
<tr>
<td>Comparison-Irrelevant-Verify</td>
<td>SCP Code</td>
<td>Compared to the first ECG, has atrial fibrillation been newly detected in the second ECG?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>Compared to the first ECG, has the $\mathbf{P}$ duration of the second ECG changed to an abnormal value?</td>
</tr>
<tr>
<td>Comparison-Irrelevant-Query</td>
<td>SCP Code</td>
<td>What symptoms still remain in the second ECG as compared to the first ECG?</td>
</tr>
<tr>
<td></td>
<td>Numeric feature</td>
<td>What numeric features of the second ECG are now considered abnormal values as compared to the first ECG?</td>
</tr>
</tbody>
</table>
<p>combination of wide residual network [30] and squeeze-and-excitation modules [7]. Furthermore, several works [4, 10, 19] studied self-supervised learning with ECGs to improve performances on the cardiac arrhythmia classification task. These works concentrate on classifying diagnoses based on a single ECG, and do not consider the significance of comparing two ECGs, despite its importance in clinical contexts. For example, by detecting resolved symptoms after some treatments, the medical practitioners can assess the effectiveness of the treatments and evaluate the progress of the patient's condition. To reflect this clinical reality, we have included questions that involve the comparison of two ECGs within the ECG-QA dataset, making our dataset unique and valuable.</p>
<h1>3 Dataset Construction</h1>
<p>We constructed the ECG-QA dataset upon the PTB-XL dataset [26] ${ }^{2}$, which offers comprehensive metadata regarding ECGs annotated by expert cardiologists. This metadata covers a wide range of information including ECG reports, diagnostic statements, diagnosis likelihoods, and signal-specific properties. To ensure the high quality of our dataset, we performed additional filtering on the original</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualization of the ECG-QA sample generation pipeline. The numbers in the sampling stage (d) stand for the ECG IDs in the PTB-XL dataset. In the sampling process, we also convert the template questions into pre-defined paraphrases for each sample.</p>
<p>PTB-XL dataset. Specifically, we selected ECGs that were marked with a validated_by_human tag set to True, which indicates the validation by a human cardiologist, and excluded ECGs that had empty reports. As a result, the ECG-QA dataset was constructed using 16,054 samples of 10-second ECGs from the PTB-XL dataset. In addition, we split the samples into training and test sets according to a 8:2 ratio based on their patient IDs before generating QA samples to prevent the overlapping of ECGs between training and test sets. We again split the training samples into training and validation sets by a 9:1 ratio, yielding 7.2:0.8:2.0 training-validation-test distribution.</p>
<h1>3.1 Question template</h1>
<p>To generate QA samples, we start by creating the question templates to collect questions, answers, and their corresponding ECGs. Because the questions are fully derived from these templates, it is important to define templates that are not only diverse but also clinically meaningful. To achieve this goal, we extracted the relevant attributes from the PTB-XL metadata to determine the content of the questions (i.e., attribute types) and categorized the questions into several question types. Then, we combined these types to construct the template questions, and additionally generated template paraphrases to add lexical diversity to our dataset. As a result, we defined a total of 70 templates as shown in Supplementary A.2, and we also provide the example questions derived from these templates in Table 1. All the processes of designing question templates have been validated by a board-certified medical expert from the Department of Critical Care and Emergency Medicine in terms of clinical utility. The detailed processes of each step are described in the following subsections, as well as visualized in Figure 2 (a) and (b).</p>
<h3>3.1.1 Attribute type extraction</h3>
<p>SCP code The PTB-XL dataset provides SCP codes for each ECG sample, consisting of 71 different ECG symptoms that adhere to the SCP-ECG v0.4 that preceded the current SCP-ECG standard [23]. These attributes are composed of form-related (e.g., inverted T-waves), rhythm-related (e.g., sinus arrhythmia), and diagnostic symptoms (e.g., non-specific ischemic) along with additional 5 superclasses for diagnostic labels. Given that detecting cardiac symptoms is a primary objective in many ECG studies [18, 6, 4, 10, 19], we included questions that inquire about various ECG symptoms in the ECG-QA dataset. To ensure the dataset quality, we excluded attributes with a low number of positive ECG samples in the test split, such as $\mathrm{W}^{\mathrm{PW}}$ (wolf-parkinson-white syndrome), resulting in a final selection of 64 attributes, including the 5 superclasses. Furthermore, we developed a regular expression parser to extract the grounded lead position of form-related symptoms from the ECG reports (See Supplementary B.1). This enables us to include questions in the ECG-QA dataset that</p>
<p>specifically address the leads in which symptoms are detected (e.g., Does this ECG show symptoms of inverted T-waves in lead I?), making the ECG-QA dataset more comprehensive.</p>
<p>Noise Considering that ECG measurements involve placing electrodes on specific body surfaces, it is inevitable to encounter various signal interferences such as baseline drift, which can be caused by patient movement or machine issues. Therefore, it becomes crucial to differentiate these interferences from the original ECG signals during analysis. To reflect this aspect in the ECG-QA dataset, we leveraged the signal noise information available in the PTB-XL metadata. This information is provided as a string indicating the specific lead positions where each noise is detected (e.g., "v1-v6" or "i-iii"). We parsed these strings to identify the exact lead positions associated with four different types of noises: Baseline drift, Static noise, Burst noise, and Electrode problems.</p>
<p>Stage of infarction Since identifying the stage of myocardial infarction (MI) helps healthcare professionals determine the most appropriate management strategies by assessing the risk profile for the patient, we also have considered the stage of infarction as an important attribute in the ECG-QA dataset. In the PTB-XL dataset, it distinguishes the stage of MI into six levels including intermediate stages: "I," "I-II," "II," "II-III," "III," and "Unknown." In addition, because there are two fields indicating the stage of infarction (infarction_stadium1 and infarction_stadium2), the statements could be potentially multiple. For the sake of simplicity, we simplified the stages into 4 levels by regarding the intermediate stages ("I-II" and "II-III") as their "lower" stage ("I" and "II"). Then, we used the second statement if there are multiple entries at a time. After defining an additional stage called "None" for those who do not have MI, we could derive five attributes for the stage of infarction.</p>
<p>Extra systole Since extra systoles can be a sign of underlying cardiac conditions or abnormalities, it is also important to detect them to evaluate the patient's heart health. To address the presence of extra systoles in the ECG-QA dataset, we utilized the relevant annotations provided in the PTB-XL metadata, which includes information about the occurrence of different types of extra systoles: Extrasystoles, Ventricular extrasystoles, and Supraventricular extrasystoles.</p>
<p>Heart axis The heart axis provides valuable information about the direction of the heart's electrical activity during each cardiac cycle. In the ECG-QA dataset, we have considered the heart axis as another crucial attribute since it is an important parameter that can help in diagnosing certain cardiac conditions. Although the PTB-XL metadata includes heart axis information, we did not utilize it because it does not specify the actual numerical values of the heart axis. Instead, we manually calculated the heart axis degrees by employing an external tool, NeuroKit2 [16] ${ }^{3}$. Then, we classified them into four categories following the conventional standards: Normal heart axis, Left axis deviation, Right axis deviation, and Extreme axis deviation.</p>
<p>Numeric feature The ECG-QA dataset also incorporates numeric features that provide further insights into the cardiac signals. Similar to the heart axis, we used NeuroKit2 to calculate the numeric values for these features since the PTB-XL dataset does not explicitly provide such information. Specifically, we extracted the locations of P, Q, R, S, and T waves for each beat present in lead II, and computed six different numeric features: $R R$ interval, $P$ duration, $P R$ interval, $Q R S$ duration, $Q T$ interval, and $Q T$ corrected. Given that a 10 -second ECG recording typically contains multiple beats and thus multiple numeric values for each feature, we represented each feature using its median value. This approach helps to minimize the impact of abnormal contractions, such as ventricular premature contractions, on the calculated values. Additionally, we categorized each numeric value as below, within, or above the normal range, where the normal range criteria are described in Supplementary B.2, derived from a previous study [27]. These ranges serve as a useful reference for assessing the numerical measurements and identifying potential abnormalities in the cardiac signals.</p>
<h1>3.1.2 Question type definition</h1>
<p>We have defined two different types of questions. The first type pertains to the type of ECGs associated with a question, which can be categorized as follows: 1) Single, which refers to questions involving a single ECG; 2) Comparison-Consecutive, which involves comparison questions between two</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Test performances for different question types. We also provide 95% confidence interval across 3 random seeds. The best performances for each question type are highlighted with boldface.</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>per Q-type</th>
<th>M ${ }^{2} A E^{1}$ [2]</th>
<th></th>
<th>MedViLL ${ }^{1}$ [17]</th>
<th></th>
<th>Fusion Transf.</th>
<th></th>
<th>Blind Transf.</th>
<th></th>
<th>Deaf Transf.</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>majority</td>
<td>EM Acc.</td>
<td>AUROC</td>
<td>EM Acc.</td>
<td>AUROC</td>
<td>EM Acc.</td>
<td>AUROC</td>
<td>EM Acc.</td>
<td>AUROC</td>
<td>EM Acc.</td>
<td>AUROC</td>
</tr>
<tr>
<td>S-Verify</td>
<td>67.7</td>
<td>$\mathbf{7 4 . 0}_{1,01.4}$</td>
<td>$0.781_{0.0,002}$</td>
<td>$73.9_{0,0.3}$</td>
<td>$\mathbf{0 . 7 6 8}_{1,0,02.1}$</td>
<td>$72.1_{0,0.2}$</td>
<td>$0.729_{0.0,004}$</td>
<td>$67.7_{0,0.0}$</td>
<td>$0.629_{0.0,009}$</td>
<td>$67.3_{0,0.2}$</td>
<td>$0.615_{0.0,002}$</td>
</tr>
<tr>
<td>S-Choose</td>
<td>31.2</td>
<td>$\mathbf{5 7 . 1}_{1,01.8}$</td>
<td>$\mathbf{0 . 8 5 0}_{1,0,002}$</td>
<td>$54.1_{0,0.8}$</td>
<td>$0.839_{0.0,001}$</td>
<td>$46.4_{0,0.4}$</td>
<td>$0.797_{0.0,007}$</td>
<td>$31.0_{0,0.1}$</td>
<td>$0.529_{0.0,006}$</td>
<td>$31.4_{0,0.0}$</td>
<td>$0.786_{0.0,011}$</td>
</tr>
<tr>
<td>S-Query</td>
<td>23.2</td>
<td>$\mathbf{4 1 . 0}_{1,01.5}$</td>
<td>$\mathbf{0 . 8 3 6}_{1,0,002}$</td>
<td>$40.4_{0,0.8}$</td>
<td>$0.831_{0.0,004}$</td>
<td>$37.4_{0,0.8}$</td>
<td>$0.799_{0.0,011}$</td>
<td>$34.8_{0,0.0}$</td>
<td>$0.549_{0.0,009}$</td>
<td>$27.0_{0,0.1}$</td>
<td>$0.754_{0.0,002}$</td>
</tr>
<tr>
<td>CC-Verify</td>
<td>62.8</td>
<td>$\mathbf{7 5 . 5}_{1,01.2}$</td>
<td>$\mathbf{0 . 7 9 2}_{1,0,002}$</td>
<td>$74.3_{0,2.6}$</td>
<td>$0.778_{0.0,047}$</td>
<td>$71.9_{0,0.6}$</td>
<td>$0.769_{0.0,003}$</td>
<td>$65.7_{0,0.8}$</td>
<td>$0.610_{0.0,001}$</td>
<td>$59.5_{0,0.6}$</td>
<td>$0.510_{0.0,009}$</td>
</tr>
<tr>
<td>CC-Query</td>
<td>16.9</td>
<td>$20.1_{11.8}$</td>
<td>$0.808_{0.0,003}$</td>
<td>$\mathbf{2 2 . 0}_{1,1.2}$</td>
<td>$\mathbf{0 . 8 1 6}_{1,0,002}$</td>
<td>$18.4_{1,1.3}$</td>
<td>$0.781_{0.0,003}$</td>
<td>$16.9_{0,0.0}$</td>
<td>$0.568_{0.0,023}$</td>
<td>$16.9_{0,0.1}$</td>
<td>$0.693_{0.0,012}$</td>
</tr>
<tr>
<td>CI-Verify</td>
<td>66.1</td>
<td>$75.3_{0,0.0}$</td>
<td>$0.769_{0.0,010}$</td>
<td>$\mathbf{7 7 . 5}_{1,1.8}$</td>
<td>$\mathbf{0 . 8 2 3}_{1,0,023}$</td>
<td>$68.1_{0,0.6}$</td>
<td>$0.723_{0.0,010}$</td>
<td>$66.2_{0,0.1}$</td>
<td>$0.508_{0.0,004}$</td>
<td>$61.1_{0,0.5}$</td>
<td>$0.505_{0.0,004}$</td>
</tr>
<tr>
<td>CI-Query</td>
<td>1.10</td>
<td>$\mathbf{4 . 1 9}_{1,01.2}$</td>
<td>$0.741_{0.0,009}$</td>
<td>$3.30_{0,0.4}$</td>
<td>$\mathbf{0 . 7 5 8}_{1,0,002}$</td>
<td>$2.19_{0,0.1}$</td>
<td>$0.704_{0.0,003}$</td>
<td>$0.95_{0,0.1}$</td>
<td>$0.527_{0.0,009}$</td>
<td>$1.11_{0,0.0}$</td>
<td>$0.632_{0.0,016}$</td>
</tr>
</tbody>
</table>
<p>consecutive ECGs from the same patient; and 3) Comparison-Irrelevant, which involves comparison questions between two irrelevant ECGs from different patients. Although Comparison-Irrelevant questions may not seem realistic in a clinical setting, we included these questions since they can help to reinforce a machine's comprehension ability and be utilized for model evaluation when comparing two different ECGs. In addition, inspired by GQA [8], the second type of question refers to the main function it should perform. These can be categorized as follows: 1) Verify, which corresponds to yes/no questions; 2) Choose, which applies to questions where the selection is made from two given options; and 3) Query, which are open-ended questions that seek to retrieve specific attributes.
By combining these two types, we could derive a total of 9 possible question types. However, we did not include the combinations of Comparison and Choose types since it seemed unnatural to select from two given options when comparing different ECGs. Similarly, with regards to the attribute types, we only considered SCP code and Numeric feature for comparison questions because these two attribute types are providing the most informative features when comparing two ECGs.</p>
<h1>3.1.3 Paraphrase generation</h1>
<p>To enhance the lexical diversity of the ECG-QA dataset, we manually curated paraphrases for each question template based on the machine-generated candidates by utilizing OpenAI's ChatGPT. We ensured that the questions in the test split were not included in the training set to evaluate the generalizability of the QA models on different lexical variations. The detailed procedure for generating paraphrases and its results are presented in Supplementary A.3.</p>
<h3>3.2 QA sample collection</h3>
<p>As shown in Figure 2 (b) and (c), we collected questions by plugging the corresponding attributes into the placeholder that existed in the question templates. For example, a question template "Does this ECG show symptoms of \${scp_code}?" can be transformed into "Does this ECG show symptoms of conduction disturbance?" We further gathered the corresponding answers for each question and paired them to create (question, answer) pairs. Then, for each (question, answer) pair, we again randomly sampled the corresponding ECGs from the candidate ECGs. In each split, the candidate ECGs can be 1) all the single ECGs for Single questions; 2) all the $\left(\mathrm{ECG}<em 2="2">{1}, \mathrm{ECG}</em>}\right)$ pairs where $\mathrm{ECG<em 2="2">{1}$ and $\mathrm{ECG}</em>}$ are the consecutive ECGs from the same patient for Comparison-Consecutive questions; and 3) all the $\left(\mathrm{ECG<em 2="2">{1}, \mathrm{ECG}</em>}\right)$ combinations where $\mathrm{ECG<em 2="2">{1}$ and $\mathrm{ECG}</em>$ have different patient IDs for Comparison-Irrelevant questions. After we finally replaced the template question with randomly selected paraphrases that matched the corresponding question's template, the process of collecting QA samples was complete. The detailed sampling strategies for different question types are described in Supplementary B.3.
After all these processes, the ECG-QA dataset consists of 267,539 training samples, 64,663 validation samples, and 82,146 test samples, which cover various types of attributes and questions. More detailed statistics of the dataset are described in Supplementary B.4.</p>
<h2>4 Experiments</h2>
<p>Task formulation We formulate QA task as a multi-label classification over all possible answer options that exist in the ECG-QA dataset. The answer labels are composed of 88 attributes from the six attribute sets, 12 lead positions (i.e., lead I - lead V6), and 3 answers for Verify questions (yes, no, not sure), leading to a total of 103 answers. Note that we processed "None" answer as an empty label.</p>
<p>Baselines We implemented the following QA baselines: $\mathrm{M}^{3} \mathrm{AE}^{\dagger}$ [2], MedViLL ${ }^{\dagger}$ [17], Fusion Transformer, Blind Transformer (seeing questions only), and Deaf Transformer (seeing ECGs only). Because the original implementations of $\mathrm{M}^{3} \mathrm{AE}$ and MedViLL were intended to pre-train images with texts, we modified them to be applied to ECGs instead of images and pre-trained them using ECG data, as marked with ${ }^{\dagger}$. Additionally, similar to the per Q-type prior in VQA [1], we include a prior model, per Q-type majority, which outputs only the most frequent answer for each question type in the test split. More details about each model implementation including training hyperparameters are described in Supplementary C.1.1.</p>
<h1>4.1 Evaluation metrics</h1>
<p>Exact match accuracy To calculate the exact match accuracy, we applied a threshold value of 0.5 to each score in the model's output vector, $\hat{\mathbf{y}} \in \mathbb{R}^{103}$, which gives a multi-hot vector of length 103. Then, we compare the output vector with the ground truth answer vector. If the two vectors are exactly the same, we assign a score of 1 ; otherwise, we assign a score of 0 . To obtain the overall accuracy, we sum the scores for all the test questions and divide the aggregated score by the total number of questions, yielding the percentage of questions that were answered exactly.</p>
<p>AUROC While the exact match accuracy is a useful metric, it may not fully capture the model's performance since it does not consider partial credits, especially in the case of Query questions that require consideration of much more attributes. To provide a more comprehensive evaluation, we employ the area under ROC curve (AUROC) as another metric. When calculating AUROC, we adopt a cautious approach by only considering the "valid" answer candidates for each question. This approach aims to prevent overestimation, as the model might naturally assign lower scores to "invalid" answer options. For example, in a question like "Which noise does this ECG show, baseline drift or static noise?", we exclusively consider the scores of the answer options baseline drift and static noise. We collect scores for each answer option over all the samples and compute macro-averaged AUROC among the answer options.</p>
<h3>4.2 Upper bound experiments</h3>
<p>In the field of clinical medicine, even experienced medical practitioners cannot be entirely certain when making crucial decisions, such as diagnosing a patient's condition. Similarly, the ECG-QA dataset can also suffer from this inherent uncertainty even though we extracted attributes from the existing annotations made by expert cardiologists. As $100 \%$ accuracy is unlikely to attain due to the inherent uncertainty, we aim to estimate the upper bound performance a model can achieve with our dataset, and use it as a reference when evaluating the model performance.
Within our dataset, we speculate that questions of the Single-Verify type necessitate basic perceptual abilities while other question types can be solved by logically combining these perceptual abilities. For example, Single-Choose questions can be answered by verifying the presence of each attribute in the given two options, and similarly, Single-Query questions can be solved by verifying the presence of each element within the specified attribute set. Consequently, we hypothesize that achieving high performance on the whole ECG-QA dataset is unlikely without a high level of perceptual ability. Based on this hypothesis, we can estimate the upper bound performances for the whole ECG-QA dataset by measuring the upper bound performances of the Single-Verify samples. To this end, we designed the following experiments.
We convert all the Single QA samples (Single-Verify, Single-Choose, Single-Query) in the training set into the format that ECG classification models can process, and train the classification models using the converted training samples. Similarly, after converting the Single-Verify samples in the test set, we estimate the upper bound performance by measuring performances on the converted Single-Verify test samples. Then, we compare this upper bound with Single-Verify performances of QA models to show how much the QA baselines can be improved in terms of their perceptual ability. The detailed process of converting QA samples into ECG classification format is described in Supplementary C.1.2.
For these experiments, we employ powerful ECG classification models classifying all the individual attributes present in the Single-Verify samples. The models we use include a Transformer-based model pre-trained with the W2V+CMSC+RLM [19] method, Resnet with Attention [18], and SE-</p>
<p>Table 3: Macro-averaged test performances of upper bound models over all attributes for SingleVerify questions</p>
<table>
<thead>
<tr>
<th>Upper bound Model</th>
<th>Acc.</th>
<th>AUROC</th>
</tr>
</thead>
<tbody>
<tr>
<td>W2V+CMSC+RLM [19]</td>
<td>$83.0_{ \pm 0.4}$</td>
<td>$0.864_{ \pm 0.003}$</td>
</tr>
<tr>
<td>Resnet-Attention [18]</td>
<td>$82.6_{ \pm 0.3}$</td>
<td>$0.875_{ \pm 0.002}$</td>
</tr>
<tr>
<td>SE-WRN [6]</td>
<td>$83.1_{ \pm 0.3}$</td>
<td>$0.883_{ \pm 0.002}$</td>
</tr>
<tr>
<td>MAX</td>
<td>$85.4_{ \pm 0.4}$</td>
<td>$0.907_{ \pm 0.002}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Macro-averaged test performances of QA models over all attributes for SingleVerify questions.</p>
<table>
<thead>
<tr>
<th>QA Model</th>
<th>Acc.</th>
<th>AUROC</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathrm{M}^{3} \mathrm{AE}^{\dagger}$ [2]</td>
<td>$80.8_{ \pm 0.3}$</td>
<td>$0.808_{ \pm 0.006}$</td>
</tr>
<tr>
<td>MedViLL ${ }^{\dagger}$ [17]</td>
<td>$79.8_{ \pm 0.3}$</td>
<td>$0.809_{ \pm 0.005}$</td>
</tr>
<tr>
<td>Fusion Transf.</td>
<td>$76.4_{ \pm 0.6}$</td>
<td>$0.764_{ \pm 0.010}$</td>
</tr>
</tbody>
</table>
<p>WRN [6]. In addition, to present the maximized upper bound, we derive another model that takes only the maximum score among the three models for each attribute, which is denoted as MAX. Detailed model implementations and training configurations are presented in Supplementary C.1.2.</p>
<h1>4.3 Modeling with LLMs</h1>
<p>As for one of the future research directions with our dataset, we further investigated the possibility of leveraging LLMs for ECG-QA. Inspired by ChatCAD [29], for each QA sample, we transformed ECGs into text descriptions using the output from the trained upper bound model (SE-WRN) and forwarded them to several OpenAI's GPT models (gpt-4, gpt-3.5-turbo, text-davinci-003) ${ }^{4}$ along with the corresponding question. Due to the restricted quota of OpenAI's api usage policy, we randomly sampled 10\% from the ECG-QA test set and conducted experiments only once for each LLM model. The detailed processes including prompts that we used are described in Supplementary C.1.3.</p>
<h3>4.4 Results</h3>
<p>QA results The baseline results are presented in Table 2. We also report test performances for different attribute types in Supplementary C.2. As expected, Blind and Deaf Transformer exhibit poor performance while other models all achieve higher scores compared to the prior model (per Q-type majority), indicating that our dataset cannot be solved by solely seeing each question and ECG separately. Furthermore, among the top three models ( $\mathrm{M}^{3} \mathrm{AE}^{\dagger}$, MedViLL ${ }^{\dagger}$, and Fusion Transformer), the pre-trained models ( $\mathrm{M}^{3} \mathrm{AE}^{\dagger}$ and MedViLL ${ }^{\dagger}$ ) outperform Fusion Transformer, which demonstrates the potential advantages of utilizing novel multi-modal pre-training methods for our dataset. Additionally, the lower performance of Choose or Query questions compared to Verify questions suggests that the primary challenges in our dataset lie on a model's ability to learn logical and set operations based on the basic perceptual abilities that can be acquired from Verify questions.</p>
<p>Upper bound results The results of the upper bound experiments are reported in Table 3 and 4. When we compare SE-WRN, which is the best upper bound model, with the best QA model, $\mathrm{M}^{3} \mathrm{AE}^{\dagger}$, we can see that the perceptual ability of baseline models can be improved by $2.3 \% \mathrm{p}$ and $7.5 \% \mathrm{p}$ in terms of EM accuracy and AUROC, respectively. Moreover, when comparing with MAX, which is expected to show a higher upper bound, the differences are increased to $4.6 \%$ p in EM accuracy and $9.9 \%$ p in AUROC. We believe these upper bound results can serve as a useful yardstick for assessing the basic perceptual ability required for more complicated questions such as Choose or Query.</p>
<p>LLM modeling results The results of the experiments with LLMs are presented in Table 5. Interestingly, the performance of all the GPT models did not surpass that of the QA baseline model. We speculate that this is due to two primary reasons: 1) the upper bound model (i.e. SE-WRN) fails to accurately extract necessary information, and 2) some questions were too complicated to be answered with a zero-shot prompt. Since LLMs fully rely on the ECG classification model for interpreting the ECGs, their performance inevitably depends on the capabilities of the ECG classification model. However, we cannot guarantee that SE-WRN is such a perfect model that always outputs accurate interpretations, because it has been trained with a limited set of QA training set to measure only the upper bound of the perceptual ability. Therefore, we expect significant performance gains if we have</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Test EM accuracies for different question types. Note that we randomly sampled $\mathbf{1 0 \%}$ from the ECG-QA test set for each question type to test the models due to the restricted quota of OpenAI's api usage policy. The best performance for each question type are highlighted with boldface.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">per Q-type</th>
<th style="text-align: center;">SE-WRN</th>
<th style="text-align: center;">SE-WRN</th>
<th style="text-align: center;">SE-WRN</th>
<th style="text-align: center;">$\mathrm{M}^{3} \mathrm{AE}^{\dagger}$ [2]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">majority</td>
<td style="text-align: center;">+ gpt-4</td>
<td style="text-align: center;">+ gpt-3.5-turbo</td>
<td style="text-align: center;">+ text-davinci-003</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">S-Verify</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">76.0 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">S-Choose</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">58.2 $\pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">S-Query</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">40.0 $\pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">CC-Verify</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">74.7 $\pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">CC-Query</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">21.2 $\pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">CI-Verify</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">75.2 $\pm 1.7$</td>
</tr>
<tr>
<td style="text-align: center;">CI-Query</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">4.36 $\pm 0.7$</td>
</tr>
</tbody>
</table>
<p>S: Single, CC: Comparison-Consecutive, CI: Comparison-Irrelevant
a strong classification model that can extract all the existing information from an ECG, and fine-tune LLMs with our QA training set (i.e., applying instruction learning to LLMs).</p>
<h1>5 Conclusion</h1>
<p>In this work, we present ECG-QA, the first QA dataset that incorporates ECG data for question answering tasks. Our dataset is designed to ensure clinical relevance and has been validated by an ECG expert. We created carefully designed question templates, which leverage clinically meaningful attributes extracted from the PTB-XL dataset, to generate a diverse collection of questions, including those that require the comparison of two ECGs. We believe that our dataset has the potential to significantly advance the field of ECG question answering research and contribute to the improvement of clinical practice in analyzing ECG data.</p>
<p>As for the future research directions with ECG-QA, one of the promising avenues is the exploration of multi-modal LLMs that can simultaneously process both ECG signals and natural language. While there is extensive work on LLMs that combine vision and language, there has been limited research on models that integrate signal processing with natural language. We believe our dataset can serve as an excellent testbed for such models.</p>
<h2>6 Limitation</h2>
<p>Despite our best efforts to create the current version of the dataset, there are some limitations as follows.</p>
<p>Small number of ECGs Due to the limited number of ECGs available in the original dataset (PTB-XL), our dataset was constructed using a relatively small number of ECGs ( $\sim 16 \mathrm{k}$ ), which leads that questions involving too rare symptoms (e.g., Wolf-Parkinson-White syndrome) could not be included. To provide more diverse combinations of ECGs and questions by incorporating questions regarding very rare attributes, we are planning to employ another dataset that is larger than the PTB-XL dataset such as MIMIC-IV-ECG [5], which is planned to be released in late 2023.</p>
<p>Upper-bound of the dataset As mentioned in Section 4.2, given the intricacies of the medical field, even medical experts cannot provide $100 \%$ accurate diagnoses for all questions. Thus, the upperbound of the dataset itself is not expected to be $100 \%$. To address this, we conducted experiments demonstrating the estimated upper-bound for each question type and attribute.</p>
<p>Old version of SCP-ECG standard Despite SCP-ECG v3.0 being the latest version, the metadata of the original dataset, PTB-XL, follows the SCP-ECG v0.4 standard. Consequently, in ECG-QA, we were constrained to categorize various symptoms based on the SCP-ECG v0.4 standard. However, after investigating how the SCP codes in SCP-ECG v3.0 are categorized, we found that there is only a little difference between SCP-ECG v0.4 and v3.0 regarding the SCP codes used in PTB-XL. Among the SCP codes used in PTB-XL, only one SCP code (BIGU, bigeminal pattern - unknown origin,</p>
<p>SV or Ventricular) has a different representation, which has changed to "SVBIG" (supraventricular bigeminy BIGU bigeminal pattern - unkown origin, SV or Ventricular) in SCP-ECG v3.0. The rest of the SCP codes have maintained their codes and definitions intact in SCP-ECG v3.0. Therefore, we believe that the impact of the differences between the two versions will not be significant in the ECG-QA dataset.</p>
<p>Automatic generation of paraphrases Although the paraphrases were manually curated, the initial candidates were automatically generated by ChatGPT, which may not be an optimal strategy. We expect that paraphrases could have been more diverse if we had involved medical practitioners in manually generating paraphrases.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>This work was supported by Institute of Information \&amp; communications Technology Planning \&amp; Evaluation (IITP) grant (No.2019-0-00075), National Research Foundation of Korea (NRF) grant (NRF-2020H1D3A2A03100945), and Korea Medical Device Development Fund grant (Project Number: 1711138160, KMDF_PR_20200901_0097), funded by the Korea government (MSIT, MOTIE, MOHW, MFDS).</p>
<h1>References</h1>
<p>[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015.
[2] Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, and Tsung-Hui Chang. Multimodal masked autoencoders for medical vision-and-language pre-training. In Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Singapore, September 18-22, 2022, Proceedings, Part V, pages 679-689. Springer, 2022.
[3] W Bruce Fye. A history of the origin, evolution, and impact of electrocardiography. The American journal of cardiology, 73(13):937-949, 1994.
[4] Bryan Gopal, Ryan Han, Gautham Raghupathi, Andrew Ng, Geoff Tison, and Pranav Rajpurkar. 3kg: contrastive learning of 12-lead electrocardiograms using physiologically-inspired augmentations. In Machine Learning for Health, pages 156-167. PMLR, 2021.
[5] Brian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody, Chrystinne Fernandes, Nathaniel Greenbaum, Seth Berkowitz, Dana Moukheiber, Parastou Eslami, et al. Mimic-iv-ecg-diagnostic electrocardiogram matched subset.
[6] Hyeongrok Han, Seongjae Park, Seonwoo Min, Hyun-Soo Choi, Eunji Kim, Hyunki Kim, Sangha Park, Jinkook Kim, Junsang Park, Junho An, et al. Towards high generalization performance on electrocardiogram classification. In 2021 Computing in Cardiology (CinC), volume 48, pages 1-4. IEEE, 2021.
[7] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132-7141, 2018.
[8] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709, 2019.
[9] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[10] Dani Kiyasseh, Tingting Zhu, and David A Clifton. Clocs: Contrastive learning of cardiac signals across space, time, and patients. In International Conference on Machine Learning, pages 5606-5615. PMLR, 2021.
[11] Olga Kovaleva, Chaitanya Shivade, Satyananda Kashyap, Karina Kanjaria, Joy Wu, Deddeh Ballah, Adam Coy, Alexandros Karargyris, Yufan Guo, David Beymer Beymer, Anna Rumshisky, and Vandana Mukherjee Mukherjee. Towards visual dialog for radiology. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pages 60-69, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.bionlp-1.6. URL https://aclanthology.org/2020.bionlp-1. 6 .
[12] Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, and Edward Choi. Ehrsql: A practical text-to-sql benchmark for electronic health records. Advances in Neural Information Processing Systems, 35:15589-15601, 2022.
[13] Yongqi Li, Wenjie Li, and Liqiang Nie. MMCoQA: Conversational question answering over text, tables, and images. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4220-4231, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.290. URL https://aclanthology.org/2022.acl-long. 290.
[14] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650-1654. IEEE, 2021.
[15] Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, and Jianyong Wang. R-vqa: learning visual relation facts with semantic attention for visual question answering. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 1880-1889, 2018.
[16] Dominique Makowski, Tam Pham, Zen J. Lau, Jan C. Brammer, François Lespinasse, Hung Pham, Christopher Schölzel, and S. H. Annabel Chen. NeuroKit2: A python toolbox for neurophysiological signal processing. Behavior Research Methods, 53(4):1689-1696, feb 2021. doi: 10.3758/s13428-020-01516-y. URL https://doi.org/10.3758\%2Fs13428-020-01516-y.</p>
<p>[17] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, and Edward Choi. Multi-modal understanding and generation for medical images and text via vision-language pre-training. IEEE Journal of Biomedical and Health Informatics, 26(12):6070-6080, 2022.
[18] Petr Nejedly, Adam Ivora, Radovan Smisek, Ivo Viscor, Zuzana Koscova, Pavel Jurak, and Filip Plesinger. Classification of ecg using ensemble of residual cnns with attention mechanism. In 2021 Computing in Cardiology (CinC), volume 48, pages 1-4. IEEE, 2021.
[19] Jungwoo Oh, Hyunseung Chung, Joon-myoung Kwon, Dong-gyun Hong, and Edward Choi. Lead-agnostic self-supervised learning for local and global representations of electrocardiogram. In Conference on Health, Inference, and Learning, pages 338-353. PMLR, 2022.
[20] Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. emrqa: A large corpus for question answering on electronic medical records. arXiv preprint arXiv:1809.00732, 2018.
[21] Matthew Reyna, Nadi Sadr, Annie Gu, Erick Andres Perez Alday, Chengyu Liu, Salman Seyedi, Amit Shah, and Gari Clifford. Will two do? varying dimensions in electrocardiography: the PhysioNet/Computing in Cardiology Challenge 2021. Computing in Cardiology 2021, 48:1-4, 2021.
[22] Anna Rosiek and Krzysztof Leksowski. The risk factors and prevention of cardiovascular disease: the importance of electrocardiogram in the diagnosis and treatment of acute coronary syndrome. Therapeutics and clinical risk management, pages 1223-1229, 2016.
[23] Paul Rubel, Danilo Pani, Alois Schloegl, Jocelyne Fayn, Fabio Badilini, Peter W Macfarlane, and Alpo Varri. Scp-ecg v3. 0: An enhanced standard communication protocol for computer-assisted electrocardiography. In 2016 Computing in Cardiology Conference (CinC), pages 309-312. IEEE, 2016.
[24] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint arXiv:2104.06039, 2021.
[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
[26] Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Wojciech Samek, and Tobias Schaeffter. PTBXL, a large publicly available electrocardiography dataset (version 1.0.3). PhysioNet, 2022. doi: https: //doi.org/10.13026/x4td-x982.
[27] Li-ping Wang, Mi Shen, Jia-fei Tong, and Jun Dong. An uncertainty reasoning method for abnormal ecg detection. In 2009 IEEE International Symposium on IT in Medicine \&amp; Education, volume 1, pages 1091-1096. IEEE, 2009.
[28] Ping Wang, Tian Shi, and Chandan K Reddy. Text-to-sql generation for question answering on electronic medical records. In Proceedings of The Web Conference 2020, pages 350-361, 2020.
[29] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computeraided diagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257, 2023.
[30] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
[31] Xian Zhong, Oubo Gong, Wenxin Huang, Lin Li, and Hongxia Xia. Squeeze-and-excitation wide residual networks in image classification. In 2019 IEEE International Conference on Image Processing (ICIP), pages 395-399. IEEE, 2019.
[32] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995-5004, 2016.
[33] Peter J Zimetbaum and Mark E Josephson. Use of the electrocardiogram in acute myocardial infarction. New England Journal of Medicine, 348(10):933-940, 2003.</p>
<h1>Supplementary Material</h1>
<h2>A Full list of templates</h2>
<h2>A. 1 Attribute type descriptions</h2>
<p>Individual attributes belonging to each attribute set used in ECG-QA are described in Table 6. For a detailed description of the attributes in SCP code, which are presented as abbreviations, please refer to the PTB-XL dataset ${ }^{1}$.</p>
<p>Table 6: Individual attributes belonging to each attribute set used in the ECG-QA dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Num.</th>
<th style="text-align: center;">Attributes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SCP code</td>
<td style="text-align: center;">SCP-ECG statements (symptoms)</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">NORM, STTC, MI, HYP, CD, NDT, <br> NST_, DIG, LNGQT, IMI, ASMI, <br> LVH, LAFB, ISC_, IRBBB, 1AVB, <br> IVCD, ISCAL, CRBBB, CLBBB, <br> ILMI, LAO/LAE, AMI, ALMI, <br> ISCIN, INJAS, LMI, ISCIL, <br> LPFB, ISCAS, INJAL, ISCLA, <br> RAO/RAE, ILBBB, IPLMI, ISCAN, <br> IPMI, INJIN, INJLA, PMI, INJIL, <br> ABQRS, PVC, STD_, VCLVH, QWAVE, <br> LOWT, NT_, PAC, LPR, INVT, LVOLT, <br> HVOLT, TAB_, STE_, SR, AFIB, STACH, <br> SARRH, SBRAD, PACE, BIGU, AFLT, SVTAC</td>
</tr>
<tr>
<td style="text-align: center;">Noise</td>
<td style="text-align: center;">Signal artifacts</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Baseline drift, Static noise, Burst noise, Electrode problems</td>
</tr>
<tr>
<td style="text-align: center;">Stage of infarction</td>
<td style="text-align: center;">Stage of myocardial infarction</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">None, Unknown, Early, Middle, Late</td>
</tr>
<tr>
<td style="text-align: center;">Extra systole</td>
<td style="text-align: center;">Extra systoles</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Extrasystoles, <br> Ventricular extrasystoles, <br> Supraventricular extrasystoles</td>
</tr>
<tr>
<td style="text-align: center;">Heart axis</td>
<td style="text-align: center;">Direction of heart axis</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Left axis deviation, Right axis deviation Extreme axis deviation, Normal heart axis</td>
</tr>
<tr>
<td style="text-align: center;">Numeric feature</td>
<td style="text-align: center;">Numeric features</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">RR interval, P duration, PR interval, QRS duration, QT interval, QT corrected</td>
</tr>
</tbody>
</table>
<h2>A. 2 Question templates</h2>
<p>A total of 70 question templates is reported in Table 19.</p>
<h2>A. 3 Paraphrases</h2>
<p>The overall procedure of generating paraphrases is as follows:</p>
<ol>
<li>20 candidate paraphrases per template question were automatically generated by OpenAI's ChatGPT with the following prompt:</li>
</ol>
<p>Please provide 20 paraphrases for this question. The paraphrased sentences should keep the placeholder which is marked with $}$. The paraphrases should entail the original sentence.
\${question_template}
2. Based on the machine-generated paraphrases, we manually refined them to ensure the high quality of paraphrases. Specifically, we filtered out the paraphrases that deviated too much</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>from the original question and manually revised them if the specific medical term we were targeting had changed.
3. Then, we randomly selected seven paraphrases for training and validation splits, and three paraphrases for the test split.</p>
<p>As a result, the final paraphrases for each question template are presented in Table 20.</p>
<h1>B Dataset construction details</h1>
<h2>B. 1 Regular expressions for parsing lead positions of form-related SCP codes</h2>
<p>To utilize grounded lead information of form-related SCP codes, we defined a parser based on regular expressions, which can be shown in Table 16, 17, and 18. During the parsing process, we utilized the Google Cloud Translation API to translate the ECG reports into English, as the majority of the original reports were written in German.</p>
<h2>B. 2 Numeric feature extraction</h2>
<p>To extract numeric values including heart axis degrees, we utilized NeuroKit2 [16]. The specific method to extract these attributes are described in the following paragraphs.</p>
<p>Heart axis To calculate heart axis degrees, we extracted the magnitude of R peaks from the lead I and lead aVF for each heartbeat existed in the ECG. Then, we computed heart axis degrees by the following equation:</p>
<p>$$
x=\frac{1}{N} \sum_{k=1}^{N} \arctan \frac{R_{a V F}^{(k)}}{R_{I}^{(k)}}
$$</p>
<p>where $N$ is the number of heartbeats in the ECG, and $R_{l}^{k}$ is the magnitude value of the $k$-th R peak in the lead $l$. To ensure accurate calculations, we did not process the samples with any noises in lead I or lead aVF and restricted them not to be sampled from the relevant questions regarding the heart axis. Based on the calculated heart axis degrees, we categorized them into four standard classes as follows:</p>
<p>$$
\text { (Heart axis) }=\left{\begin{array}{lll}
\text { Normal } &amp; \text { if } &amp; -30 \leq x&lt;90 \
\text { LAD } &amp; \text { if } &amp; -90 \leq x&lt;-30 \
\text { RAD } &amp; \text { if } &amp; 90 \leq x \leq 180 \
\text { EAD } &amp; \text { if } &amp; -180 \leq x \leq-90
\end{array}\right.
$$</p>
<p>Numeric feature For the numeric features such as $R R$ interval or $P R$ interval, we extracted the locations of $\mathrm{P}, \mathrm{Q}, \mathrm{R}, \mathrm{S}$, and T waves for each beat present in lead II, and computed the following six numeric features:</p>
<ul>
<li>RR interval: The interval seconds between consecutive R peaks.</li>
<li>P duration: The interval seconds between P onset and P offset.</li>
<li>PR interval: The interval seconds between P onset and R onset.</li>
<li>QRS duration: The interval seconds between Q peak and S peak.</li>
<li>QT interval: The interval seconds between Q peak and T offset.</li>
<li>QT corrected: $\frac{\text { QT interval }}{\sqrt{\text { RR interval }}}$</li>
</ul>
<p>Similar to Heart axis, we did not process neither the samples with any noises in lead II nor the samples that have less than seven R peaks detected in lead II. Again, we categorized the calculated values into below, within, or above the normal range where the normal range criteria are presented in Table 7.</p>
<p>Table 7: Normal range criteria for numeric features.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Numeric Feature</th>
<th style="text-align: center;">Below the normal range</th>
<th style="text-align: center;">Within the normal range</th>
<th style="text-align: center;">Above the normal range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RR interval</td>
<td style="text-align: center;">$x&lt;0.6$</td>
<td style="text-align: center;">$0.6 \leq x \leq 1.0$</td>
<td style="text-align: center;">$1.0&lt;x$</td>
</tr>
<tr>
<td style="text-align: center;">P duration</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$x \leq 0.12$</td>
<td style="text-align: center;">$0.12&lt;x$</td>
</tr>
<tr>
<td style="text-align: center;">PR interval</td>
<td style="text-align: center;">$x&lt;0.12$</td>
<td style="text-align: center;">$0.12 \leq x \leq 0.2$</td>
<td style="text-align: center;">$0.2&lt;x$</td>
</tr>
<tr>
<td style="text-align: center;">QRS duration</td>
<td style="text-align: center;">$x&lt;0.06$</td>
<td style="text-align: center;">$0.06 \leq x \leq 0.11$</td>
<td style="text-align: center;">$0.11&lt;x$</td>
</tr>
<tr>
<td style="text-align: center;">QT interval</td>
<td style="text-align: center;">$x&lt;0.33$</td>
<td style="text-align: center;">$0.33 \leq x \leq 0.43$</td>
<td style="text-align: center;">$0.43&lt;x$</td>
</tr>
<tr>
<td style="text-align: center;">QT corrected</td>
<td style="text-align: center;">$x&lt;0.33$</td>
<td style="text-align: center;">$0.33 \leq x \leq 0.45$</td>
<td style="text-align: center;">$0.45&lt;x$</td>
</tr>
</tbody>
</table>
<h1>B. 3 Sampling strategy</h1>
<p>During the sampling process, we sampled more negative samples than positive samples to reflect the clinical reality where normal (negative) cases are much more frequent than abnormal (positive) cases. The sampling size for each question is presented in Table 8. To avoid excessively unbalanced sampling, we set a limit on the number of negative samples if there are too few positive samples, ensuring it does not exceed five times the number of positive samples. For example, if there are 30 positive samples for Single-Verify question in the training set, we sample 150 negative samples for that question instead of 200 negative samples. Different sampling strategies for each question type are described in the following paragraphs.</p>
<p>Table 8: Sampling size for each question. It varies depending on the question type.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question Type</th>
<th style="text-align: center;">Train <br> (Pos / Neg)</th>
<th style="text-align: center;">Validation <br> (Pos / Neg)</th>
<th style="text-align: center;">Test <br> (Pos / Neg)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">S-Verify</td>
<td style="text-align: center;">$100 / 200$</td>
<td style="text-align: center;">$20 / 40$</td>
<td style="text-align: center;">$20 / 40$</td>
</tr>
<tr>
<td style="text-align: center;">S-Choose</td>
<td style="text-align: center;">$10 / 10$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$2 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">S-Query</td>
<td style="text-align: center;">$100 / 200$</td>
<td style="text-align: center;">$50 / 100$</td>
<td style="text-align: center;">$50 / 100$</td>
</tr>
<tr>
<td style="text-align: center;">CC-Verify</td>
<td style="text-align: center;">$50 / 100$</td>
<td style="text-align: center;">$10 / 20$</td>
<td style="text-align: center;">$10 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">CC-Query</td>
<td style="text-align: center;">$50 / 100$</td>
<td style="text-align: center;">$25 / 50$</td>
<td style="text-align: center;">$25 / 50$</td>
</tr>
<tr>
<td style="text-align: center;">CI-Verify</td>
<td style="text-align: center;">$50 / 100$</td>
<td style="text-align: center;">$10 / 20$</td>
<td style="text-align: center;">$10 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">CI-Query</td>
<td style="text-align: center;">$50 / 100$</td>
<td style="text-align: center;">$25 / 50$</td>
<td style="text-align: center;">$25 / 50$</td>
</tr>
</tbody>
</table>
<p>Verify questions Typically there are two answer options (e.g., yes/no), but we included an additional answer option "not sure" especially for diagnostic labels of SCP codes to utilize the likelihood information given by the PTB-XL metadata. Specifically, in the PTB-XL metadata, diagnostic SCP codes are annotated along with their likelihood, indicating the certainty of the diagnoses on a scale of $[0,15,35,50,80,100]$. Considering that the likelihood was derived from keywords in the ECG report and set to zero where no relevant keyword was available (i.e., statements with no adjectives such as "non-specific st-t wave changes" $\rightarrow$ set to 0 likelihood), we classified 0 and 100 as a "certain" diagnosis, and any other value as an "uncertain" diagnosis. Accordingly, when a question asks to verify the presence of a specific diagnostic SCP code in an ECG, the answer is "not sure" if the ECG has been labeled with that SCP code with a likelihood of $[15,35,50,80]$.
Additionally, for questions related to a specific grounded lead position, we defined two types of negative samples: "hard negative" and "soft negative" to add complexity to the dataset. Specifically, "hard negative" samples have the corresponding attribute in other leads but not in the inquired lead, while "soft negative" samples do not have the attribute at all. We sampled at most half of the negative samples from the "hard negative" samples, and the remaining from the "soft negative" samples. By doing so, we intended the QA models to be able to deduce the grounding lead information of specific attributes.</p>
<p>Choose questions Choose questions involve two attributes, and the answer depends on whether each attribute exists in the ECG, which leads to four possible answer options for each question. For these questions, we considered the absence of both attributes in the ECG as a negative sample, and the presence of at least one attribute as a positive sample. We balanced out the negative samples by</p>
<p>adjusting their number to be no more than five times the maximum number of samples among the three positive options.</p>
<p>Query questions Similar to Choose questions, we regarded negative samples for Query questions as the samples that do not have any of the attributes in the inquired attribute set. For example, if a question asks "What kind of noises does this ECG show?", the negative samples are defined as the ECGs that do not have any noises such as Baseline drift or Static noise. Then, as aforementioned, we restricted the number of negative samples to maintain balance.</p>
<p>Table 9: ECG-QA dataset statistics for different question types.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question type</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Validation</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">S-Verify</td>
<td style="text-align: center;">62,554</td>
<td style="text-align: center;">10,718</td>
<td style="text-align: center;">13,081</td>
</tr>
<tr>
<td style="text-align: left;">S-Choose</td>
<td style="text-align: center;">50,015</td>
<td style="text-align: center;">9,085</td>
<td style="text-align: center;">9,855</td>
</tr>
<tr>
<td style="text-align: left;">S-Query</td>
<td style="text-align: center;">46,737</td>
<td style="text-align: center;">11,334</td>
<td style="text-align: center;">18,157</td>
</tr>
<tr>
<td style="text-align: left;">CC-Verify</td>
<td style="text-align: center;">21,173</td>
<td style="text-align: center;">2,721</td>
<td style="text-align: center;">4,230</td>
</tr>
<tr>
<td style="text-align: left;">CC-Query</td>
<td style="text-align: center;">5,128</td>
<td style="text-align: center;">672</td>
<td style="text-align: center;">1,662</td>
</tr>
<tr>
<td style="text-align: left;">CI-Verify</td>
<td style="text-align: center;">39,880</td>
<td style="text-align: center;">7,318</td>
<td style="text-align: center;">7,718</td>
</tr>
<tr>
<td style="text-align: left;">CI-Query</td>
<td style="text-align: center;">42,052</td>
<td style="text-align: center;">22,815</td>
<td style="text-align: center;">27,443</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">267,539</td>
<td style="text-align: center;">64,663</td>
<td style="text-align: center;">82,146</td>
</tr>
</tbody>
</table>
<p>Table 10: ECG-QA dataset statistics for different attribute types.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attribute type</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Validation</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SCP code</td>
<td style="text-align: center;">201,183</td>
<td style="text-align: center;">47,160</td>
<td style="text-align: center;">60,869</td>
</tr>
<tr>
<td style="text-align: left;">Noise</td>
<td style="text-align: center;">26,192</td>
<td style="text-align: center;">6,017</td>
<td style="text-align: center;">7,460</td>
</tr>
<tr>
<td style="text-align: left;">Stage of infarction</td>
<td style="text-align: center;">1,233</td>
<td style="text-align: center;">304</td>
<td style="text-align: center;">364</td>
</tr>
<tr>
<td style="text-align: left;">Extra systole</td>
<td style="text-align: center;">1,777</td>
<td style="text-align: center;">407</td>
<td style="text-align: center;">493</td>
</tr>
<tr>
<td style="text-align: left;">Heart axis</td>
<td style="text-align: center;">1,780</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">440</td>
</tr>
<tr>
<td style="text-align: left;">Numeric feature</td>
<td style="text-align: center;">35,374</td>
<td style="text-align: center;">10,380</td>
<td style="text-align: center;">12,520</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">267,539</td>
<td style="text-align: center;">64,663</td>
<td style="text-align: center;">82,146</td>
</tr>
</tbody>
</table>
<h1>B. 4 Dataset statistics</h1>
<p>ECG-QA dataset statistics for different question types and attribute types are described in Table 9 and 10 .</p>
<h2>C Experimental details</h2>
<h2>C. 1 Implementation details</h2>
<h2>C.1.1 QA Baselines</h2>
<p>The training configurations for QA baselines along with model configurations and resource information are reported in Table 11. We used Adam [9] optimizer for all the models.</p>
<ol>
<li>per Q-type majority: This is a prior model that outputs only the most frequent answer in the test split for each question type.</li>
<li>$\mathbf{M}^{\mathrm{q}} \mathbf{A E}^{\dagger}$ [2]: A multi-modal architecture of ECGs and texts based on Transformer Encoder [25], pre-trained with Masked Language Modeling (MLM), Masked Image Modeling (MIM), and Image Text Matching (ITM) tasks. The architecture comprises three components: 1) separated uni-modal encoders, 2) a multi-modal fusion module, and 3) separated uni-modal decoders for pre-training tasks. Considering the characteristics of the signal data, we employed several 1-d convolutional layers to embed ECGs instead of using a linear layer in the original implementation. Otherwise, we followed the same configurations as the original paper including model architecture and pre-training hyperparameters, except for batch size and learning rate. We used 256 batch size, and $5 \mathrm{e}-5$ learning rate for the pre-training. In the fine-tuning phase, for Comparison questions that need to see two ECGs, we concatenate the two ECGs and forwarded them to the uni-modal encoder (the 1st component) to get ECG embeddings.</li>
<li>MedViLL ${ }^{\dagger}$ [17]: Fusion Transformer (see below) pre-trained with MedViLL [17] methodology. This method implements multi-modal pre-training with ECGs and texts, consisting of MLM and ITM tasks. When pre-training, we followed the same configurations introduced in the original paper such as MLM ratio, except for batch size and learning rate. We used 256 batch size, and $5 \mathrm{e}-5$ learning rate for the pre-training.</li>
<li>Fusion Transformer: Both the questions and (potentially two) ECGs are concatenated and forwarded into the Transformer Encoder after embedding ECGs with several convolutional layers. To encode questions, we tokenize questions to subword tokens following the BERT</li>
</ol>
<p>Table 11: Training, model configurations for QA baselines along with resource information. Some model configurations are not reported if not applicable. For any other configurations that are not reported here, we followed the original paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">$\mathrm{M}^{3} \mathrm{AE}^{1}$ [2]</th>
<th style="text-align: center;">MedViLL $^{1}$ [17]</th>
<th style="text-align: center;">Fusion Transf.</th>
<th style="text-align: center;">Deaf Transf.</th>
<th style="text-align: center;">Blind Transf.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model configurations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Conv layers</td>
<td style="text-align: center;">$[(256,2,2)] \times 4$</td>
<td style="text-align: center;">$[(256,2,2)] \times 4$</td>
<td style="text-align: center;">$[(256,2,2)] \times 4$</td>
<td style="text-align: center;">$[(256,2,2)] \times 4$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Transformer layers</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Hidden dimension</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: center;">Attention heads</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Training configurations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Training step</td>
<td style="text-align: center;">50,000</td>
<td style="text-align: center;">50,000</td>
<td style="text-align: center;">50,000</td>
<td style="text-align: center;">50,000</td>
<td style="text-align: center;">50,000</td>
</tr>
<tr>
<td style="text-align: center;">Local batch size</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">Total batch size</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">Gradient accumulation step</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">LR scheduler</td>
<td style="text-align: center;">$\operatorname{Tri}(0.1,0.4,0.5)$</td>
<td style="text-align: center;">$\operatorname{Tri}(0.1,0.4,0.5)$</td>
<td style="text-align: center;">$\operatorname{Tri}(0.1,0.4,0.5)$</td>
<td style="text-align: center;">$\operatorname{Tri}(0.1,0.4,0.5)$</td>
<td style="text-align: center;">$\operatorname{Tri}(0.1,0.4,0.5)$</td>
</tr>
<tr>
<td style="text-align: center;">Resources</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPU device</td>
<td style="text-align: center;">A6000 $\times 4$</td>
<td style="text-align: center;">RTX 3090 $\times 4$</td>
<td style="text-align: center;">RTX 3090 $\times 4$</td>
<td style="text-align: center;">RTX 3090 $\times 4$</td>
<td style="text-align: center;">RTX 3090 $\times 1$</td>
</tr>
<tr>
<td style="text-align: center;">VRAM</td>
<td style="text-align: center;">48GB $\times 4$</td>
<td style="text-align: center;">24GB $\times 4$</td>
<td style="text-align: center;">24GB $\times 4$</td>
<td style="text-align: center;">24GB $\times 4$</td>
<td style="text-align: center;">24GB $\times 1$</td>
</tr>
<tr>
<td style="text-align: center;">Training time</td>
<td style="text-align: center;">45 hours</td>
<td style="text-align: center;">37 hours</td>
<td style="text-align: center;">37 hours</td>
<td style="text-align: center;">33 hours</td>
<td style="text-align: center;">18 hours</td>
</tr>
</tbody>
</table>
<p>Conv layers: ${($ channel size, kernel size, stride $)} \times \mathrm{N}$
Total batch size: Local batch size $\times$ Gradient accumulation step $\times$ Number of GPU devices
$\operatorname{Tri}(\mathrm{x}, \mathrm{y}, \mathrm{z})$ : warmup ratio ( x ), hold ratio (y), exponential decay ratio (z), final lr decaying scale $=0.05$
A6000: NVIDIA RTX A6000
RTX 3090: NVIDIA GeForce RTX 3090
tokenizer and encode them using the embedding layer of BERT. For Single questions which involve only one ECG, the second ECG is padded by zeros. Then, we separately averagepool each of the modalities, which yields two ECG vectors $\left(\mathbf{v}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \mathbf{v}</em>}} \in \mathbb{R}^{768}\right)$ and a single question vector $\left(\mathbf{v<em _mathbf_2="\mathbf{2">{\mathbf{3}} \in \mathbb{R}^{768}\right)$. For Single questions, $\mathbf{v}</em>}}$ is set to $\mathbf{0}$. We concatenate these vectors and perform multi-label classification task after projecting them onto the final answer space $\left(\left[\mathbf{v<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \mathbf{v}</em>}}, \mathbf{v<em _mathbf_1="\mathbf{1">{\mathbf{3}}\right] \in \mathbb{R}^{2304} \rightarrow \hat{\mathbf{y}} \in \mathbb{R}^{103}\right)$.
5. Deaf Transformer: Only the ECGs are forwarded to the Transformer Encoder after being embedded by several convolutional layers. We average-pool the output vectors for each of ECGs, which results in two ECG vectors $\left(\mathbf{v}</em>}}, \mathbf{v<em _mathbf_2="\mathbf{2">{\mathbf{2}} \in \mathbb{R}^{768}\right)$. Similar to the Fusion Transformer, for Single questions, the second ECG is padded by zeros, and $\mathbf{v}</em>}}$ is set to $\mathbf{0}$. Then, we concatenate two vectors and project them onto the final answer space $\left(\left[\mathbf{v<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \mathbf{v}</em>\right)$.
6. Blind Transformer: Only the questions are forwarded to the Transformer Encoder. Similar to Fusion Transformer, we follow BERT to tokenize and encode questions. We average-pool the output vectors to get a single question vector, and project the vector onto the final answer space $\left(\mathbf{v} \in \mathbb{R}^{768} \rightarrow \hat{\mathbf{y}} \in \mathbb{R}^{103}\right)$.}}\right] \in \mathbb{R}^{1536} \rightarrow\right.$ $\left.\hat{\mathbf{y}} \in \mathbb{R}^{103</p>
<h1>C.1.2 Upper bound models</h1>
<p>The procedure of training upper bound models with QA samples is illustrated in Figure 3. For a fair comparison with QA models, we need to train the upper bound models using the comparable dataset with the ECG-QA dataset, requiring converting QA samples to the format that the upper bound models can process. Accordingly, we convert each Single QA sample pair (Question, ECG, Answer) in the training set into 4-tuples (ECG, Lead, Attribute, Label), and collect all the corresponding (Attribute, Label) pairs for each unique (ECG, Lead) pair as shown in Figure 3 (a). Then, each (ECG, Lead) pair is fed to the upper bound (ECG classification) model along with its labels, and the model is trained by the binary cross entropy (BCE) losses calculated from the corresponding classification heads (See Figure 3 (b)). When training the upper bound model, although the model outputs the scores (i.e., probabilities) for all the possible attributes from the classification heads, we only calculate BCE losses from the corresponding attributes that have been labeled for each (ECG, Lead) pair so that the classification heads for unlabeled attributes are not trained. In addition, if Lead in each (ECG, Lead) pair indicates the specific lead (e.g., Lead I), not an entire ECG, we forward the ECG to the model after zero-padding the other leads.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the procedure for training upper bound models with QA samples. (a) It starts by converting all the Single QA samples into the appropriate format that the upper bound models can process. (b) Then, each unique (ECG, Lead) pair is fed into the upper bound model, and the binary cross entropy losses are calculated from the corresponding labels to train the model. Here, we do not calculate losses from the classification heads for unlabeled attributes, which are marked with blurred boxes in the final layer.</p>
<p>The strategies for converting QA samples into the 4-tuple format for different question types are described in the following paragraphs.</p>
<p>Single-Verify samples These types of QA samples can be directly convertible to the 4-tuple format according to their answers. For example, if we have a QA sample ("Does this ECG show baseline drift in lead I?", $E C G_{A}$, "yes"), then this QA sample is converted to ( $E C G_{A}$, Lead I, Baseline drift, True), which means that " $E C G_{A}$ has Baseline drift in Lead I". On the other hand, if we have another QA sample where the answer is "no", such as ("Does this ECG show baseline drift in Lead I?", $E C G_{B}$, "no"), we can derive ( $E C G_{B}$, Lead I, Baseline drift, False), which means that " $E C G_{B}$ does not have Baseline drift in lead $I$ ".</p>
<p>Single-Choose samples Because there are two relevant attributes in these types of QA samples, we can extract two 4-tuples for each QA sample. For example, if we have a QA sample ("Which noise does this ECG show in Lead I, baseline drift or static noise?", $E C G_{A}$, "baseline drift"), then we convert this QA sample into two 4-tuples: 1) $\left(E C G_{A}\right.$, Lead I, Baseline drift, True) and 2) $\left(E C G_{A}\right.$, Lead I, Static noise, False).</p>
<p>Single-Query samples Similar to Single-Choose samples, we derive 4-tuples for each QA sample as many as the number of the relevant attributes. For example, if we have a QA sample ("What kind of noises does this ECG show in Lead I?", $E C G_{A}$, "baseline drift, burst noise"), then we can derive four 4-tuples: 1) (ECG $<em A="A">{A}$, Lead I, Baseline drift, True), 2) (ECG $</em>$, Lead I, Electrode problems, False).}$, Lead I, Static noise, False), 3) $\left(E C G_{A}\right.$, Lead I, Burst noise, True), and 4) (ECG $_{A</p>
<p>Table 12: Training configurations for upper bound models along with resource information.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>W2V+CMSC+RLM [19]</th>
<th>Resnet+Attention [18]</th>
<th>SE-WRN [6]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training configurations</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Training step</td>
<td>100,000</td>
<td>100,000</td>
<td>100,000</td>
</tr>
<tr>
<td>Batch size</td>
<td>64</td>
<td>128</td>
<td>128</td>
</tr>
<tr>
<td>Learning rate</td>
<td>$5 \mathrm{e}-5$</td>
<td>$1 \mathrm{e}-4$</td>
<td>$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td>LR scheduler</td>
<td>$\operatorname{Tri}(0.1,0.4,0.5)$</td>
<td>$\operatorname{Tri}(0.1,0.4,0.5)$</td>
<td>$\operatorname{Tri}(0.1,0.4,0.5)$</td>
</tr>
<tr>
<td>Resources</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPU device</td>
<td>RTX $3090 \times 1$</td>
<td>RTX $3090 \times 1$</td>
<td>RTX $3090 \times 1$</td>
</tr>
<tr>
<td>VRAM</td>
<td>$24 \mathrm{~GB} \times 1$</td>
<td>$24 \mathrm{~GB} \times 1$</td>
<td>$24 \mathrm{~GB} \times 1$</td>
</tr>
<tr>
<td>Training time</td>
<td>25 hours</td>
<td>7 hours</td>
<td>12 hours</td>
</tr>
</tbody>
</table>
<p>$\operatorname{Tri}(\mathrm{x}, \mathrm{y}, \mathrm{z})$ : warmup ratio ( x ), hold ratio (y), exponential decay ratio (z), final lr decaying scale $=0.05$
RTX 3090: NVIDIA GeForce RTX 3090</p>
<p>The detailed upper bound model implementations are described in the following paragraphs. In addition, training configurations for upper bound models including resource information are presented in Table 12.</p>
<ol>
<li>W2V+CMSC+RLM [19]: A Transformer-based model pre-trained with the W2V+CMSC+ RLM [19] method. This model comprises several convolutional layers to extract features from ECGs, followed by Transformer Encoder to contextualize the features. For the model configurations, we follow the original implementation. Specifically, we employ four convolutional layers, each of which has 256 channels with strides of two and kernel lengths of two, and 12 Transformer Encoder layers with 12 self-attention heads and 768 hidden dimensions.</li>
<li>Resnet-Attention [18]: This model was introduced in PhysioNet/Computing in Cardiology Challenge 2021 [21] (CinC 2021), which won first place in the challenge. It is composed of several residual blocks followed by a multi-head attention layer, and two output layers: 1) a conventional BCE loss layer and 2) another loss layer specifically designed for optimizing the challenge score. Since we do not need to optimize the model to achieve a high challenge score, we omit the second loss layer and use only BCE loss to train the model. For any other model configurations such as the number of residual blocks or dropout rate, we follow the original implementation.</li>
<li>SE-WRN [6]: This model was introduced in the same challenge (CinC 2021), which won second place. It consists of a series of wide residual networks [30] combined with squeeze-and-excitation modules [7]. In the original paper, they additionally use demographic features such as age and sex, but we do not use those features. We follow the original implementation for the model configurations. However, because the original authors did not mention the kernel lengths for each convolutional layer, we selected kernel lengths of 11 after searching for the best options among $3,5,7$, and 11 .</li>
</ol>
<h1>C.1.3 Modeling with LLMs</h1>
<p>The procedure of modeling the ECG-QA dataset with LLMs is visualized in Figure 4. To provide the results of the ECG interpretations to the LLMs, we transform the outputs from the best upper bound model (SE-WRN [6]) into the text descriptions that LLMs can interpret. Specifically, we apply a threshold value of 0.5 to each score in the model's outputs to get only the attributes whose score is more than 0.5 . Then, based on the selected attributes with their scores, we forward the following prompts to LLMs and measure the exact match accuracy. To enable the one-to-one comparison between LLM's answer and GT answer, we induce the LLMs to output the answers only from the valid answer options by giving the candidate options in the prompt for each question.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of modeling with LLMs for the ECG-QA dataset. The ECG is interpreted by the ECG classification model (SE-WRN), which is transformed into the text description for generating the prompt text for the LLMs. When converting the model's output to the text description, we only consider the attributes whose score is more than 0.5 . For the prompts for other types of questions such as questions regarding specific lead positions or comparison questions, refer to Supplementary C.1.3.</p>
<p>Prompt for Single questions that address the specific leads in which attributes are detected. (e.g., "Does this ECG show symptoms of Inverted T-waves in lead 1?")</p>
<p>These are the interpretations of each ECG along with their scores.
Higher score means more certainty about the interpretations.
Interpretation of the ECG in \${lead}:
\${attribute#0}: \${score#0}
$\qquad$
\${attribute#N}: \${score#N}
Question: \${question}
Options: \${options}
Only answer based on the given Options without any explanation.</p>
<p>Prompt for Single questions that require retrieving specific leads in which attributes are detected.
(e.g., "What leads are showing symptoms of inverted T-waves?")</p>
<p>These are the interpretations of each ECG along with their scores.
Higher score means more certainty about the interpretations.
Interpretation of the ECG in lead I:
\${attribute#0}: \${score#0}
$\$ \cdot{$ attribute#N}: \${score#N}
・.
Interpretation of the ECG in lead V6:
\${attribute#0}: \${score#0}
$\$ \cdot{$ attribute#N}: \${score#N}
Question: \${question}
Options: lead I, lead II, lead III, lead aVR, lead aVL, lead aVF, lead V1, lead V2, lead V3, lead V4, lead V5, lead V6</p>
<p>Only answer based on the given Options without any explanation.</p>
<p>Prompt for other Single questions.
(e.g., "Does this ECG show symptoms of non-diagnostic (abnormalities?")</p>
<p>These are the interpretations of each ECG along with their scores.
Higher score means more certainty about the interpretations.
Interpretation of the ECG:
\${attribute#0}: \${score#0}
$\$ \cdot{$ attribute#N}: \${score#N}
Question: \${question}
Options: \${options}
Only answer based on the given Options without any explanation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://physionet.org/content/ptb-xl/1.0.3/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1254 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1254</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1254</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-274192171</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.14499v2.pdf" target="_blank">Understanding World or Predicting Future? A Comprehensive Survey of World Models</a></p>
                <p><strong>Paper Abstract:</strong> The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1254.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1254.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sora</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sora (text-to-video generation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale multimodal text-/image-/video-conditioned video generation model that produces high-quality, temporally consistent videos (up to minute-long) and is discussed as a candidate 'video world model' or world simulator in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sora</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-scale encoder-decoder / transformer-based video generation model accepting text, image, and video inputs and producing temporally coherent video sequences; functions as an implicit visual world simulator rather than an explicit dynamics engine.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>implicit video-generation world model / neural simulator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>long-form video generation; proposed as a general world simulator for visual prediction and simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Qualitative human-preference video judgments, temporal consistency, visual realism; evaluated diagnostically on physical-law benchmarks and out-of-distribution tests (e.g., Physics-IQ, combinatorial/causal tests referenced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Produces high visual quality and minute-long coherent videos in-distribution; reported to frequently fail physical-law consistency and causal-interaction tasks (no numeric fidelity values provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Largely a black-box neural generator; limited causal or mechanistic interpretability; does not expose transparent physical equations or modular causal factors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Failure analysis via diagnostic physical-law benchmarks and out-of-distribution tests (benchmarking and qualitative analysis); no built-in symbolic interpretability reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Described as large-scale and computationally heavy (training/inference cost not specified in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More direct visual-world generation than multi-stage pipelines (per survey), but computationally expensive compared to compact latent simulators; scaling improves in-distribution fidelity but not OOD robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High-quality video synthesis useful for perceptual simulation tasks; limited utility for action-conditioned counterfactual prediction and planning because it poorly models causal intervention effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Excellent for generating realistic perceptual sequences (useful for perception/data augmentation); limited for downstream decision-making/planning since it cannot reliably simulate consequences of alternative actions or physical counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Visual fidelity vs. causal/physical correctness: scaling yields photorealism but not robust rule-based generalization; high compute vs. interactivity/controllability limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Multimodal encoder-decoder plus transformer architecture, diffusion/transformer generative techniques (survey summary); emphasis on large-scale visual pretraining for temporal coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably on physical-law/generalization tests to hybrid approaches that embed physics (e.g., Genesis, PhysGen); superior in raw visual realism vs. classic geometric simulators but weaker at causal fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends hybridization with explicit physics priors or simulators (physics cores, rigid-body simulators) and targeted benchmarks to improve counterfactual generalization and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1254.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3 (latent dynamics world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-space model-based RL/world-model approach that couples a visual encoder with latent dynamics and adds normalization/balancing to scale to many tasks, reported to solve large suites of pixel-based control problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model that encodes high-dimensional observations into a compact latent state and learns dynamics in latent space (RSSM-like); uses representation learning and robust normalization to stabilize training across diverse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (implicit dynamics in learned latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>pixel-based control and reinforcement learning (e.g., many continuous control and simulated tasks, including Minecraft diamond collection as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task success rates and generalization across suite of tasks; model-learning metrics include reconstruction loss and next-step prediction errors (MSE-style) in latent space as standard objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in-survey to solve over 150 tasks without human data/domain tuning (qualitative statement from survey); no single numeric prediction-error reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations are compact but not fully interpretable; some latent dimensions can encode task-relevant features but overall model remains neural and partially opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Implicit inspection of latent dynamics; memory-centric extensions (e.g., Recall-to-Imaging) are used to probe and improve long-horizon reasoning, but no formal symbolic extraction described in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Large models (hundreds of millions of parameters in large agents referenced for TD-MPC2), training cost significant but more sample-efficient than naive model-free approaches; exact GPU/time figures not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample-efficient than model-free RL for many tasks; enables learning without task-specific tuning, suggesting efficiency gains on multi-task suites (no concrete multiplicative factors provided).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong task performance across many pixel-control tasks; claimed successful results on 150+ tasks including complex ones like Minecraft diamond collection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility for long-horizon control where compact latent dynamics suffice; prioritizes task-relevant predictive accuracy over perfect visual reconstruction, which helps policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Compression into latent space improves efficiency and policy learning but can lose fine-grained detail needed for high-fidelity physical simulation; interpretability remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Autoencoder/visual encoder + latent dynamics model (RSSM-like), normalization and balancing improvements, memory-centric modules for long-horizon reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms many model-free baselines in sample efficiency and multi-task robustness; compared to full visual generators, DreamerV3 trades visual fidelity for compact task-relevant representations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey emphasizes compact, smooth latent dynamics and normalization to scale; recommends balancing latent compression (efficiency) with sufficient capacity for task-relevant detail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1254.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Ha & Schmidhuber, 2018 line of work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early neural-network-based implicit world models that learn latent representations from observations (autoencoder + latent dynamics) to support downstream decision-making and planning in MBRL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (Ha & Schmidhuber pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoencoder-latent-state pipeline: an encoder compresses observations into latent variables; a learned latent dynamics model predicts future latents; policies or planners act on imagined rollouts in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / implicit representation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>reinforcement learning and model-based control (simulated tasks, games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Next-step prediction MSE / reconstruction loss on observations and quality of imagined rollouts as judged by downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Demonstrated successful latent imagination supporting policy learning in benchmarks (qualitative); no single numeric fidelity reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent variables are abstract and not directly interpretable as physical parameters; they provide compact task-relevant abstractions rather than explicit, human-interpretable models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructions and imagined trajectories; evaluation via downstream policy results rather than explicit symbolic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally cheaper than pixel-level video generation due to compressed latent rollouts; exact resource figures not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient for planning (latent rollouts) than pixel-space simulation; provides lower sample/trial cost for policy search compared to model-free RL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables effective policy learning in simulated control domains via latent imagination; demonstrated as proof-of-concept for MBRL advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent world models provide sufficient fidelity for many control tasks while greatly reducing computational and sample cost compared to full observation-space simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reduced observation fidelity for efficiency and tractable planning; loss of fine-grained visual/physical detail may limit realism for tasks requiring precise physics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete design: autoencoder for compression, recurrent or feedforward latent dynamics, imagination-based planning (MPC or policy learning in latent space).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pixel-space video generators, world models are more efficient and better integrated with MBRL; compared to explicit physics simulators, they are more data-driven and less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey signals optimality when latent representations capture task-relevant dynamics succinctlyâ€”balance compactness and representational capacity to support downstream planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1254.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JEPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Embedding Predictive Architecture (JEPA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework proposed by LeCun that separates perception and cognition: perception maps sensory data to embeddings and a cognitive module predicts/evaluates future embeddings, embodying a compact world model used for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JEPA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Perception module produces embeddings; cognitive/predictive module models embedding dynamics (latent predictive modeling) enabling simulation of future embeddings for decision-making; inspired by human dual-system (fast/slow) thinking.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent predictive world model / cognitive-style architecture</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general decision-making and planning across modalities (vision, embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality/utility of predicted embeddings for downstream decision-making (implicit metrics rather than pixel-level reconstruction); not specified numerically in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports conceptual advantages in enabling 'System 2' planning but gives no numeric performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Designed for minimal latent representations that filter redundancies; potentially more interpretable at module boundary-level (perception vs. cognition) but still neural and latent-internal.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Architectural modularity (separating perception and cognitive embedding spaces) used as a design for interpretability; no formal interpretability methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Aims for computational efficiency by using compact embeddings instead of full sensory reconstruction; concrete compute numbers not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Argued to be more efficient than pipelines that reconstruct full observations for planning, since planning operates in a lower-dimensional embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Theoretical framework expected to improve deliberative planning and long-horizon reasoning; empirical task metrics not supplied in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Promotes a tradeoff: smaller embeddings reduce compute and speed up planning but risk omitting task-relevant details unless embeddings are carefully designed.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Compression/efficiency vs. completeness of world representation; JEPA emphasizes minimalism to aid efficient planning at possible cost to full-fidelity simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Explicit two-module design (perception -> embeddings, cognition -> predictive modeling); use of latent variables to capture essential state for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to both latent world models (similar) and pixel-space simulators (more efficient); advocated over naive full-reconstruction approaches for sample/compute efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey highlights JEPA's recommendation for minimal sufficient embeddings to support planning (balance compression with preservation of planning-relevant information).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1254.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAIA-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAIA-1 (generative world model for driving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative video-based world model for autonomous driving scenarios that directly generates camera perception data for simulation and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GAIA-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer/diffusion-based generative world model trained on driving datasets to synthesize realistic driving camera frames and scenario rollouts as a world simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>video-generation-based world simulator (diffusion/transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving simulation (camera-space scenario generation and forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Visual realism and temporal consistency of generated driving footage; utility measured by downstream planning/control tasks and human-preference judgments (as discussed in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey states GAIA-1 can generate camera perception data resembling reality in-distribution; no numeric fidelity numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box generative model: high visual fidelity but limited explicit interpretability of underlying dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Evaluated via qualitative realism and downstream utility; no specific interpretability techniques described in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Diffusion/transformer generative costs are high; survey does not provide explicit compute stats.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Offers a direct camera-space simulation alternative to multi-module geometric pipelines, reducing information loss at cost of heavier compute compared to compact geometric simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Useful for generating realistic training/evaluation data and supporting planning in camera-space; suitability for closed-loop planning improved in Drive-WM (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High perceptual fidelity improves realism for perception modules, but without reliable physical/causal fidelity it may be limited for robust closed-loop planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Visual realism and controllability vs. physical interpretability and compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Diffusion/transformer generative backbone trained on driving video datasets; leverages text/image alignment tools (survey mentions CLIP-like control) for controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Better visual realism than geometric trajectory simulators; less physically principled than hybrid physics-coupled models and occupancy/geometry models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests combining generative visual realism with spatial/physics-aware representations (e.g., occupancy grids or explicit physics modules) to serve driving planning tasks robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1254.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Drive-WM / DriveDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Drive-WM and DriveDreamer (driving world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DriveDreamer series and Drive-WM are diffusion/transformer-driven driving world models: DriveDreamer targets realworld-driven video/world models and Drive-WM introduces closed-loop control for planning with the generated world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DriveDreamer / Drive-WM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion/transformer-based video generation models trained on driving data; Drive-WM extends generation with a closed-loop control interface enabling planning over generated rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>video-generation world model with closed-loop planning extension</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving: perception simulation, planning and closed-loop control experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Visual realism, temporal consistency, and closed-loop planning success metrics when used with planning agents; also human-preference judgments and downstream planning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports DriveDreamer generates realistic driving videos; Drive-WM enables closed-loop planning but no quantitative numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predominantly neural and implicit; interpretability limited to qualitative visualization of generated frames and planning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Assessed via visualization of generated sequences and evaluation of planning outcomes; no internal symbolic interpretability described.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High due to diffusion/transformer training and closed-loop inference; exact costs not specified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides more natural camera-space simulation than geometric pipelines but at greater compute; Drive-WM trades extra compute for closed-loop planning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables visual scenario generation and can support planning when integrated with closed-loop control; utility depends on fidelity of generated dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>When visuals are realistic and dynamics consistent, useful for planning research and data augmentation; limited by physical-law and OOD failures for safety-critical planning unless combined with spatial/physics models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adds planning utility at the cost of additional compute and exposure to generative-model physical inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of diffusion/transformer backbones, closed-loop interface for planning over generated frames, and emphasis on resolution/prediction duration improvements in variants like Vista.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to occupancy/geometry-based simulators, video-based models give perceptual realism but worse explicit spatial interpretability; hybrid solutions or occupancy outputs may be preferred for safety-critical planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends combining image-space generation with spatial/occupancy predictions or physics priors for robust closed-loop planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1254.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OccWorld / OccSora</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OccWorld and OccSora (3D occupancy generation models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that predict 3D occupancy grids or 4D occupancy as a world model for autonomous driving, producing spatially grounded future-state representations more aligned with downstream spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OccWorld / OccSora</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion/attention-based models (per survey) that predict future 3D occupancy grids (or 4D occupancy) instead of raw pixels, producing spatially structured representations of future states for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>occupancy-grid world model / spatial world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving and mobile perception (trajectory/occupancy prediction and planning)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Occupancy prediction accuracy (e.g., IoU, occupancy classification metrics), downstream planning/trajectory prediction performance, and temporal consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey states these models better reflect spatial characteristics of traffic scenarios compared to video-only outputs; no numeric occupancy-accuracy values provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>More interpretable/spatially grounded than pixel-space generators because outputs correspond to explicit spatial grids; easier to connect to geometric planners.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Direct inspection of occupancy grids and spatial rendering; evaluation via downstream planning and trajectory metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate-to-high (diffusion/attention backbones), but potentially cheaper for planners since occupancy is lower-dimensional than full video frames; specific compute stats not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient for spatial planning than pixel-based generators (less post-processing for geometry); less visually rich but more utilitarian for motion planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improves usefulness for spatial planning and motion prediction tasks relative to pure video generation; survey claims better reflection of scene geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High task utility for spatial planning because outputs are directly usable by planners; may sacrifice photorealism but gain in spatial correctness and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Sacrifices pixel-level visual realism for spatially structured, planner-friendly representations; potential compute tradeoffs depend on model backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Model outputs are explicit occupancy grids (3D/4D), trained with diffusion/attention or transformer backbones; design prioritizes spatial fidelity over raw image quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Preferable to pixel-space simulators for planning tasks; compared to explicit physics simulators, occupancy models are data-driven and may lack guaranteed physical correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests occupancy prediction is optimal when planning utility and spatial interpretability are priorities; hybridizing with physics priors recommended for improved dynamics fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1254.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhysGen / Genesis / PhysDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-grounded hybrid generators (PhysGen, Genesis, PhysDreamer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid approaches that combine explicit physics simulators or physics cores with neural generative/refinement models (e.g., diffusion refiners) to improve physical plausibility and counterfactual generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PhysGen / Genesis / PhysDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid architectures coupling a fast photo-realistic renderer or physics core (rigid-body simulator, universal physics core) with neural diffusion/refinement components to produce controllable, physically-plausible motion and visuals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid world model (explicit physics + neural refiner)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-to-video generation with physics, robotic simulation and manipulation, physically-plausible video synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Physical-plausibility diagnostics (physics-benchmarks), long-horizon stability, and visual realism; measured by physics-specific benchmarks (e.g., tasks testing gravity/fluid/optics) and standard perceptual metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports hybrid models improve physical plausibility relative to pure data-driven video generators; no concrete numeric improvements provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>More interpretable than black-box generators because embedded physics core exposes mechanistic, first-principles structure; partial transparency in modeled dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Use of explicit simulators and physics engines provides inherently interpretable dynamics; analysis via physics tests and ablation studies discussed in survey context.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Potentially higher per-sample cost due to physics simulation overhead, but may improve sample-efficiency and OOD generalization; exact compute/time not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Hybrid models may be more compute-intensive than pure neural generators but yield better OOD robustness and physical correctness; tradeoff depends on simulation fidelity and neural refiner complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Better at tasks requiring accurate physical interactions and counterfactual reasoning (e.g., robotics physics tests) compared to pure neural video generators.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility when physical correctness and causal reasoning are critical (safety-critical planning, robot interaction); may be overkill for purely perceptual data augmentation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Extra computational and implementation complexity vs. gains in interpretability, physical fidelity, and OOD robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Combine first-principles physics engines with neural diffusion/transformer refiners; enforce physical priors during generation and use rendering pipelines for photorealism.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Superior to pure data-driven video generators on physics-sensitive tasks; less convenient than compact latent models for rapid planning, but more robust for counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends these hybrids when physical laws and counterfactual fidelity are essentialâ€”suggests embedding physics cores or enforcing physics priors as a best-practice direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1254.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniSim / Pandora</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniSim and Pandora (dynamic embodied environment generators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>UniSim dynamically generates robot manipulation video sequences conditioned on spatial movements, commands, and camera parameters; Pandora generalizes dynamic environment generation to human and robot actions across indoor/outdoor scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniSim / Pandora</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal generative pipelines (diffusion/transformer/LLM-conditioned) that synthesize first-person, action-conditioned dynamic video environments for embodied agent training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>dynamic embodied world model / multimodal generator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation simulation, embodied agent training, dynamic environment generation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Action-conditioned video fidelity, temporal coherence, realism of generated interactions, utility measured by downstream policy learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports UniSim and Pandora can generate varied, realistic manipulation and action-driven sequences, enabling embodied agent practice; no numeric fidelity scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Implicit neural models with partial interpretability via conditioned variables (actions, camera parameters); internal dynamics remain black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Conditioned generation allows some interpretability correlating inputs (actions) to generated outcomes; evaluation mainly via downstream agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate-to-high due to multimodal generative backbones; training and generation cost unspecified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More flexible and scalable than hand-crafted static simulators; reduces manual environment creation effort but trades increased generative compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables large-scale generation of embodied experiences helpful for training and fine-tuning embodied agents; supports variety and realism in dynamic scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility for data generation and first-person training where diversity and realism matter; transfer-to-real-world depends on physical and causal fidelity of generated sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generative flexibility and scene diversity vs. potential physical/causal inaccuracies and compute cost; dynamic generation alleviates manual setup burden.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Condition on actions, camera parameters, and text instructions; use multimodal data sources (3D sims, real robot actions, internet media) for richness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More scalable and diverse than static simulators (less manual asset engineering); may be less physically accurate than physics-based simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests dynamic generative environments are optimal for rapid diversity and first-person embodied training, with hybrid physics constraints preferred for sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1254.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1254.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynalang</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynalang (multimodal LLM world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal world model that predicts future text and image representations and learns policies from imagined rollouts; used as an LLM-backboned world model for action/policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dynalang</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns to predict future multimodal (text + image) representations conditioned on actions and uses imagined rollouts in representation space to train actor-critic policies without directly operating in pixel space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>language+vision multimodal latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>embodied control, navigation, planning and policy learning from imagined multimodal rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Accuracy of predicted multimodal representations (e.g., next-image embedding prediction), and downstream policy success rates (actor-critic returns).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Survey reports Dynalang can predict future multimodal representations and support policy learning from imagined rollouts; no numeric performance figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Intermediate representations (text/image embeddings) provide some semantic interpretability; model internals remain neural and not fully transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of predicted representations and policy behaviors; no systematic symbolic extraction reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Potentially lower than pixel-space video generation since it operates in representation space; exact compute numbers not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient for policy learning than pixel-level methods by operating on compact multimodal representations and imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables policy learning using imagined multimodal futures; reportedly effective in supporting actor-critic training purely on generated representations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility for multi-modal decision-making tasks where semantic representations are sufficient; may miss fine-grained physical details needed for precise control.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Semantic compactness and sample efficiency vs. loss of pixel-level fidelity and precise physical simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predicts future text/image embeddings, trains policies on imagined rollouts, uses actor-critic updates driven by multimodal rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More flexible and semantically grounded than pure-visual latent models; less physically explicit than hybrid physics-embedding approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey indicates representation-space imagination is effective when task reward can be inferred from semantic embeddingsâ€”optimal balance favors compact multimodal embeddings for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding World or Predicting Future? A Comprehensive Survey of World Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Video generation models as world simulators <em>(Rating: 2)</em></li>
                <li>How far is video generation from world model: A physical law perspective <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>DreamerV3 <em>(Rating: 2)</em></li>
                <li>GAIA-1: A generative world model for autonomous driving <em>(Rating: 2)</em></li>
                <li>PhysGen: Rigid-body physics-grounded image-to-video generation <em>(Rating: 2)</em></li>
                <li>UniSim <em>(Rating: 2)</em></li>
                <li>DriveDreamer: Towards realworld-driven world models for autonomous driving <em>(Rating: 2)</em></li>
                <li>OccWorld: Learning a 3d occupancy world model for autonomous driving <em>(Rating: 2)</em></li>
                <li>Dynalang <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1254",
    "paper_id": "paper-274192171",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Sora",
            "name_full": "Sora (text-to-video generation model)",
            "brief_description": "A large-scale multimodal text-/image-/video-conditioned video generation model that produces high-quality, temporally consistent videos (up to minute-long) and is discussed as a candidate 'video world model' or world simulator in the survey.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Sora",
            "model_description": "Large-scale encoder-decoder / transformer-based video generation model accepting text, image, and video inputs and producing temporally coherent video sequences; functions as an implicit visual world simulator rather than an explicit dynamics engine.",
            "model_type": "implicit video-generation world model / neural simulator",
            "task_domain": "long-form video generation; proposed as a general world simulator for visual prediction and simulation",
            "fidelity_metric": "Qualitative human-preference video judgments, temporal consistency, visual realism; evaluated diagnostically on physical-law benchmarks and out-of-distribution tests (e.g., Physics-IQ, combinatorial/causal tests referenced in survey)",
            "fidelity_performance": "Produces high visual quality and minute-long coherent videos in-distribution; reported to frequently fail physical-law consistency and causal-interaction tasks (no numeric fidelity values provided in survey).",
            "interpretability_assessment": "Largely a black-box neural generator; limited causal or mechanistic interpretability; does not expose transparent physical equations or modular causal factors.",
            "interpretability_method": "Failure analysis via diagnostic physical-law benchmarks and out-of-distribution tests (benchmarking and qualitative analysis); no built-in symbolic interpretability reported in survey.",
            "computational_cost": "Described as large-scale and computationally heavy (training/inference cost not specified in survey).",
            "efficiency_comparison": "More direct visual-world generation than multi-stage pipelines (per survey), but computationally expensive compared to compact latent simulators; scaling improves in-distribution fidelity but not OOD robustness.",
            "task_performance": "High-quality video synthesis useful for perceptual simulation tasks; limited utility for action-conditioned counterfactual prediction and planning because it poorly models causal intervention effects.",
            "task_utility_analysis": "Excellent for generating realistic perceptual sequences (useful for perception/data augmentation); limited for downstream decision-making/planning since it cannot reliably simulate consequences of alternative actions or physical counterfactuals.",
            "tradeoffs_observed": "Visual fidelity vs. causal/physical correctness: scaling yields photorealism but not robust rule-based generalization; high compute vs. interactivity/controllability limitations.",
            "design_choices": "Multimodal encoder-decoder plus transformer architecture, diffusion/transformer generative techniques (survey summary); emphasis on large-scale visual pretraining for temporal coherence.",
            "comparison_to_alternatives": "Compared unfavorably on physical-law/generalization tests to hybrid approaches that embed physics (e.g., Genesis, PhysGen); superior in raw visual realism vs. classic geometric simulators but weaker at causal fidelity.",
            "optimal_configuration": "Survey recommends hybridization with explicit physics priors or simulators (physics cores, rigid-body simulators) and targeted benchmarks to improve counterfactual generalization and interpretability.",
            "uuid": "e1254.0",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3 (latent dynamics world model)",
            "brief_description": "A latent-space model-based RL/world-model approach that couples a visual encoder with latent dynamics and adds normalization/balancing to scale to many tasks, reported to solve large suites of pixel-based control problems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DreamerV3",
            "model_description": "Latent world model that encodes high-dimensional observations into a compact latent state and learns dynamics in latent space (RSSM-like); uses representation learning and robust normalization to stabilize training across diverse tasks.",
            "model_type": "latent world model (implicit dynamics in learned latent space)",
            "task_domain": "pixel-based control and reinforcement learning (e.g., many continuous control and simulated tasks, including Minecraft diamond collection as cited)",
            "fidelity_metric": "Task success rates and generalization across suite of tasks; model-learning metrics include reconstruction loss and next-step prediction errors (MSE-style) in latent space as standard objectives.",
            "fidelity_performance": "Reported in-survey to solve over 150 tasks without human data/domain tuning (qualitative statement from survey); no single numeric prediction-error reported in survey.",
            "interpretability_assessment": "Latent representations are compact but not fully interpretable; some latent dimensions can encode task-relevant features but overall model remains neural and partially opaque.",
            "interpretability_method": "Implicit inspection of latent dynamics; memory-centric extensions (e.g., Recall-to-Imaging) are used to probe and improve long-horizon reasoning, but no formal symbolic extraction described in survey.",
            "computational_cost": "Large models (hundreds of millions of parameters in large agents referenced for TD-MPC2), training cost significant but more sample-efficient than naive model-free approaches; exact GPU/time figures not provided in survey.",
            "efficiency_comparison": "More sample-efficient than model-free RL for many tasks; enables learning without task-specific tuning, suggesting efficiency gains on multi-task suites (no concrete multiplicative factors provided).",
            "task_performance": "Strong task performance across many pixel-control tasks; claimed successful results on 150+ tasks including complex ones like Minecraft diamond collection.",
            "task_utility_analysis": "High utility for long-horizon control where compact latent dynamics suffice; prioritizes task-relevant predictive accuracy over perfect visual reconstruction, which helps policy learning.",
            "tradeoffs_observed": "Compression into latent space improves efficiency and policy learning but can lose fine-grained detail needed for high-fidelity physical simulation; interpretability remains limited.",
            "design_choices": "Autoencoder/visual encoder + latent dynamics model (RSSM-like), normalization and balancing improvements, memory-centric modules for long-horizon reasoning.",
            "comparison_to_alternatives": "Outperforms many model-free baselines in sample efficiency and multi-task robustness; compared to full visual generators, DreamerV3 trades visual fidelity for compact task-relevant representations.",
            "optimal_configuration": "Survey emphasizes compact, smooth latent dynamics and normalization to scale; recommends balancing latent compression (efficiency) with sufficient capacity for task-relevant detail.",
            "uuid": "e1254.1",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models (Ha & Schmidhuber, 2018 line of work)",
            "brief_description": "Early neural-network-based implicit world models that learn latent representations from observations (autoencoder + latent dynamics) to support downstream decision-making and planning in MBRL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "World Models (Ha & Schmidhuber pipeline)",
            "model_description": "Autoencoder-latent-state pipeline: an encoder compresses observations into latent variables; a learned latent dynamics model predicts future latents; policies or planners act on imagined rollouts in latent space.",
            "model_type": "latent world model / implicit representation",
            "task_domain": "reinforcement learning and model-based control (simulated tasks, games)",
            "fidelity_metric": "Next-step prediction MSE / reconstruction loss on observations and quality of imagined rollouts as judged by downstream policy performance.",
            "fidelity_performance": "Demonstrated successful latent imagination supporting policy learning in benchmarks (qualitative); no single numeric fidelity reported in survey.",
            "interpretability_assessment": "Latent variables are abstract and not directly interpretable as physical parameters; they provide compact task-relevant abstractions rather than explicit, human-interpretable models.",
            "interpretability_method": "Visualization of reconstructions and imagined trajectories; evaluation via downstream policy results rather than explicit symbolic interpretability.",
            "computational_cost": "Computationally cheaper than pixel-level video generation due to compressed latent rollouts; exact resource figures not provided in survey.",
            "efficiency_comparison": "More computationally efficient for planning (latent rollouts) than pixel-space simulation; provides lower sample/trial cost for policy search compared to model-free RL.",
            "task_performance": "Enables effective policy learning in simulated control domains via latent imagination; demonstrated as proof-of-concept for MBRL advantages.",
            "task_utility_analysis": "Latent world models provide sufficient fidelity for many control tasks while greatly reducing computational and sample cost compared to full observation-space simulation.",
            "tradeoffs_observed": "Reduced observation fidelity for efficiency and tractable planning; loss of fine-grained visual/physical detail may limit realism for tasks requiring precise physics.",
            "design_choices": "Discrete design: autoencoder for compression, recurrent or feedforward latent dynamics, imagination-based planning (MPC or policy learning in latent space).",
            "comparison_to_alternatives": "Compared to pixel-space video generators, world models are more efficient and better integrated with MBRL; compared to explicit physics simulators, they are more data-driven and less interpretable.",
            "optimal_configuration": "Survey signals optimality when latent representations capture task-relevant dynamics succinctlyâ€”balance compactness and representational capacity to support downstream planning.",
            "uuid": "e1254.2",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "JEPA",
            "name_full": "Joint Embedding Predictive Architecture (JEPA)",
            "brief_description": "A framework proposed by LeCun that separates perception and cognition: perception maps sensory data to embeddings and a cognitive module predicts/evaluates future embeddings, embodying a compact world model used for planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "JEPA",
            "model_description": "Perception module produces embeddings; cognitive/predictive module models embedding dynamics (latent predictive modeling) enabling simulation of future embeddings for decision-making; inspired by human dual-system (fast/slow) thinking.",
            "model_type": "latent predictive world model / cognitive-style architecture",
            "task_domain": "general decision-making and planning across modalities (vision, embodied tasks)",
            "fidelity_metric": "Quality/utility of predicted embeddings for downstream decision-making (implicit metrics rather than pixel-level reconstruction); not specified numerically in survey.",
            "fidelity_performance": "Survey reports conceptual advantages in enabling 'System 2' planning but gives no numeric performance metrics.",
            "interpretability_assessment": "Designed for minimal latent representations that filter redundancies; potentially more interpretable at module boundary-level (perception vs. cognition) but still neural and latent-internal.",
            "interpretability_method": "Architectural modularity (separating perception and cognitive embedding spaces) used as a design for interpretability; no formal interpretability methods reported.",
            "computational_cost": "Aims for computational efficiency by using compact embeddings instead of full sensory reconstruction; concrete compute numbers not provided in survey.",
            "efficiency_comparison": "Argued to be more efficient than pipelines that reconstruct full observations for planning, since planning operates in a lower-dimensional embedding space.",
            "task_performance": "Theoretical framework expected to improve deliberative planning and long-horizon reasoning; empirical task metrics not supplied in survey.",
            "task_utility_analysis": "Promotes a tradeoff: smaller embeddings reduce compute and speed up planning but risk omitting task-relevant details unless embeddings are carefully designed.",
            "tradeoffs_observed": "Compression/efficiency vs. completeness of world representation; JEPA emphasizes minimalism to aid efficient planning at possible cost to full-fidelity simulation.",
            "design_choices": "Explicit two-module design (perception -&gt; embeddings, cognition -&gt; predictive modeling); use of latent variables to capture essential state for planning.",
            "comparison_to_alternatives": "Compared conceptually to both latent world models (similar) and pixel-space simulators (more efficient); advocated over naive full-reconstruction approaches for sample/compute efficiency.",
            "optimal_configuration": "Survey highlights JEPA's recommendation for minimal sufficient embeddings to support planning (balance compression with preservation of planning-relevant information).",
            "uuid": "e1254.3",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GAIA-1",
            "name_full": "GAIA-1 (generative world model for driving)",
            "brief_description": "A generative video-based world model for autonomous driving scenarios that directly generates camera perception data for simulation and downstream tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GAIA-1",
            "model_description": "Transformer/diffusion-based generative world model trained on driving datasets to synthesize realistic driving camera frames and scenario rollouts as a world simulator.",
            "model_type": "video-generation-based world simulator (diffusion/transformer)",
            "task_domain": "autonomous driving simulation (camera-space scenario generation and forecasting)",
            "fidelity_metric": "Visual realism and temporal consistency of generated driving footage; utility measured by downstream planning/control tasks and human-preference judgments (as discussed in survey).",
            "fidelity_performance": "Survey states GAIA-1 can generate camera perception data resembling reality in-distribution; no numeric fidelity numbers provided.",
            "interpretability_assessment": "Black-box generative model: high visual fidelity but limited explicit interpretability of underlying dynamics.",
            "interpretability_method": "Evaluated via qualitative realism and downstream utility; no specific interpretability techniques described in survey.",
            "computational_cost": "Diffusion/transformer generative costs are high; survey does not provide explicit compute stats.",
            "efficiency_comparison": "Offers a direct camera-space simulation alternative to multi-module geometric pipelines, reducing information loss at cost of heavier compute compared to compact geometric simulators.",
            "task_performance": "Useful for generating realistic training/evaluation data and supporting planning in camera-space; suitability for closed-loop planning improved in Drive-WM (see separate entry).",
            "task_utility_analysis": "High perceptual fidelity improves realism for perception modules, but without reliable physical/causal fidelity it may be limited for robust closed-loop planning.",
            "tradeoffs_observed": "Visual realism and controllability vs. physical interpretability and compute cost.",
            "design_choices": "Diffusion/transformer generative backbone trained on driving video datasets; leverages text/image alignment tools (survey mentions CLIP-like control) for controllability.",
            "comparison_to_alternatives": "Better visual realism than geometric trajectory simulators; less physically principled than hybrid physics-coupled models and occupancy/geometry models.",
            "optimal_configuration": "Survey suggests combining generative visual realism with spatial/physics-aware representations (e.g., occupancy grids or explicit physics modules) to serve driving planning tasks robustly.",
            "uuid": "e1254.4",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Drive-WM / DriveDreamer",
            "name_full": "Drive-WM and DriveDreamer (driving world models)",
            "brief_description": "DriveDreamer series and Drive-WM are diffusion/transformer-driven driving world models: DriveDreamer targets realworld-driven video/world models and Drive-WM introduces closed-loop control for planning with the generated world.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DriveDreamer / Drive-WM",
            "model_description": "Diffusion/transformer-based video generation models trained on driving data; Drive-WM extends generation with a closed-loop control interface enabling planning over generated rollouts.",
            "model_type": "video-generation world model with closed-loop planning extension",
            "task_domain": "autonomous driving: perception simulation, planning and closed-loop control experiments",
            "fidelity_metric": "Visual realism, temporal consistency, and closed-loop planning success metrics when used with planning agents; also human-preference judgments and downstream planning accuracy.",
            "fidelity_performance": "Survey reports DriveDreamer generates realistic driving videos; Drive-WM enables closed-loop planning but no quantitative numbers given.",
            "interpretability_assessment": "Predominantly neural and implicit; interpretability limited to qualitative visualization of generated frames and planning trajectories.",
            "interpretability_method": "Assessed via visualization of generated sequences and evaluation of planning outcomes; no internal symbolic interpretability described.",
            "computational_cost": "High due to diffusion/transformer training and closed-loop inference; exact costs not specified in survey.",
            "efficiency_comparison": "Provides more natural camera-space simulation than geometric pipelines but at greater compute; Drive-WM trades extra compute for closed-loop planning capability.",
            "task_performance": "Enables visual scenario generation and can support planning when integrated with closed-loop control; utility depends on fidelity of generated dynamics.",
            "task_utility_analysis": "When visuals are realistic and dynamics consistent, useful for planning research and data augmentation; limited by physical-law and OOD failures for safety-critical planning unless combined with spatial/physics models.",
            "tradeoffs_observed": "Adds planning utility at the cost of additional compute and exposure to generative-model physical inconsistencies.",
            "design_choices": "Use of diffusion/transformer backbones, closed-loop interface for planning over generated frames, and emphasis on resolution/prediction duration improvements in variants like Vista.",
            "comparison_to_alternatives": "Compared to occupancy/geometry-based simulators, video-based models give perceptual realism but worse explicit spatial interpretability; hybrid solutions or occupancy outputs may be preferred for safety-critical planning.",
            "optimal_configuration": "Survey recommends combining image-space generation with spatial/occupancy predictions or physics priors for robust closed-loop planning.",
            "uuid": "e1254.5",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "OccWorld / OccSora",
            "name_full": "OccWorld and OccSora (3D occupancy generation models)",
            "brief_description": "Models that predict 3D occupancy grids or 4D occupancy as a world model for autonomous driving, producing spatially grounded future-state representations more aligned with downstream spatial reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "OccWorld / OccSora",
            "model_description": "Diffusion/attention-based models (per survey) that predict future 3D occupancy grids (or 4D occupancy) instead of raw pixels, producing spatially structured representations of future states for downstream tasks.",
            "model_type": "occupancy-grid world model / spatial world model",
            "task_domain": "autonomous driving and mobile perception (trajectory/occupancy prediction and planning)",
            "fidelity_metric": "Occupancy prediction accuracy (e.g., IoU, occupancy classification metrics), downstream planning/trajectory prediction performance, and temporal consistency.",
            "fidelity_performance": "Survey states these models better reflect spatial characteristics of traffic scenarios compared to video-only outputs; no numeric occupancy-accuracy values provided in survey.",
            "interpretability_assessment": "More interpretable/spatially grounded than pixel-space generators because outputs correspond to explicit spatial grids; easier to connect to geometric planners.",
            "interpretability_method": "Direct inspection of occupancy grids and spatial rendering; evaluation via downstream planning and trajectory metrics.",
            "computational_cost": "Moderate-to-high (diffusion/attention backbones), but potentially cheaper for planners since occupancy is lower-dimensional than full video frames; specific compute stats not provided.",
            "efficiency_comparison": "More efficient for spatial planning than pixel-based generators (less post-processing for geometry); less visually rich but more utilitarian for motion planning.",
            "task_performance": "Improves usefulness for spatial planning and motion prediction tasks relative to pure video generation; survey claims better reflection of scene geometry.",
            "task_utility_analysis": "High task utility for spatial planning because outputs are directly usable by planners; may sacrifice photorealism but gain in spatial correctness and interpretability.",
            "tradeoffs_observed": "Sacrifices pixel-level visual realism for spatially structured, planner-friendly representations; potential compute tradeoffs depend on model backbone.",
            "design_choices": "Model outputs are explicit occupancy grids (3D/4D), trained with diffusion/attention or transformer backbones; design prioritizes spatial fidelity over raw image quality.",
            "comparison_to_alternatives": "Preferable to pixel-space simulators for planning tasks; compared to explicit physics simulators, occupancy models are data-driven and may lack guaranteed physical correctness.",
            "optimal_configuration": "Survey suggests occupancy prediction is optimal when planning utility and spatial interpretability are priorities; hybridizing with physics priors recommended for improved dynamics fidelity.",
            "uuid": "e1254.6",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "PhysGen / Genesis / PhysDreamer",
            "name_full": "Physics-grounded hybrid generators (PhysGen, Genesis, PhysDreamer)",
            "brief_description": "Hybrid approaches that combine explicit physics simulators or physics cores with neural generative/refinement models (e.g., diffusion refiners) to improve physical plausibility and counterfactual generalization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PhysGen / Genesis / PhysDreamer",
            "model_description": "Hybrid architectures coupling a fast photo-realistic renderer or physics core (rigid-body simulator, universal physics core) with neural diffusion/refinement components to produce controllable, physically-plausible motion and visuals.",
            "model_type": "hybrid world model (explicit physics + neural refiner)",
            "task_domain": "vision-to-video generation with physics, robotic simulation and manipulation, physically-plausible video synthesis",
            "fidelity_metric": "Physical-plausibility diagnostics (physics-benchmarks), long-horizon stability, and visual realism; measured by physics-specific benchmarks (e.g., tasks testing gravity/fluid/optics) and standard perceptual metrics.",
            "fidelity_performance": "Survey reports hybrid models improve physical plausibility relative to pure data-driven video generators; no concrete numeric improvements provided in survey.",
            "interpretability_assessment": "More interpretable than black-box generators because embedded physics core exposes mechanistic, first-principles structure; partial transparency in modeled dynamics.",
            "interpretability_method": "Use of explicit simulators and physics engines provides inherently interpretable dynamics; analysis via physics tests and ablation studies discussed in survey context.",
            "computational_cost": "Potentially higher per-sample cost due to physics simulation overhead, but may improve sample-efficiency and OOD generalization; exact compute/time not reported in survey.",
            "efficiency_comparison": "Hybrid models may be more compute-intensive than pure neural generators but yield better OOD robustness and physical correctness; tradeoff depends on simulation fidelity and neural refiner complexity.",
            "task_performance": "Better at tasks requiring accurate physical interactions and counterfactual reasoning (e.g., robotics physics tests) compared to pure neural video generators.",
            "task_utility_analysis": "High utility when physical correctness and causal reasoning are critical (safety-critical planning, robot interaction); may be overkill for purely perceptual data augmentation tasks.",
            "tradeoffs_observed": "Extra computational and implementation complexity vs. gains in interpretability, physical fidelity, and OOD robustness.",
            "design_choices": "Combine first-principles physics engines with neural diffusion/transformer refiners; enforce physical priors during generation and use rendering pipelines for photorealism.",
            "comparison_to_alternatives": "Superior to pure data-driven video generators on physics-sensitive tasks; less convenient than compact latent models for rapid planning, but more robust for counterfactuals.",
            "optimal_configuration": "Survey recommends these hybrids when physical laws and counterfactual fidelity are essentialâ€”suggests embedding physics cores or enforcing physics priors as a best-practice direction.",
            "uuid": "e1254.7",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "UniSim / Pandora",
            "name_full": "UniSim and Pandora (dynamic embodied environment generators)",
            "brief_description": "UniSim dynamically generates robot manipulation video sequences conditioned on spatial movements, commands, and camera parameters; Pandora generalizes dynamic environment generation to human and robot actions across indoor/outdoor scenes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "UniSim / Pandora",
            "model_description": "Multimodal generative pipelines (diffusion/transformer/LLM-conditioned) that synthesize first-person, action-conditioned dynamic video environments for embodied agent training and evaluation.",
            "model_type": "dynamic embodied world model / multimodal generator",
            "task_domain": "robotic manipulation simulation, embodied agent training, dynamic environment generation",
            "fidelity_metric": "Action-conditioned video fidelity, temporal coherence, realism of generated interactions, utility measured by downstream policy learning performance.",
            "fidelity_performance": "Survey reports UniSim and Pandora can generate varied, realistic manipulation and action-driven sequences, enabling embodied agent practice; no numeric fidelity scores provided.",
            "interpretability_assessment": "Implicit neural models with partial interpretability via conditioned variables (actions, camera parameters); internal dynamics remain black-box.",
            "interpretability_method": "Conditioned generation allows some interpretability correlating inputs (actions) to generated outcomes; evaluation mainly via downstream agent performance.",
            "computational_cost": "Moderate-to-high due to multimodal generative backbones; training and generation cost unspecified in survey.",
            "efficiency_comparison": "More flexible and scalable than hand-crafted static simulators; reduces manual environment creation effort but trades increased generative compute.",
            "task_performance": "Enables large-scale generation of embodied experiences helpful for training and fine-tuning embodied agents; supports variety and realism in dynamic scenarios.",
            "task_utility_analysis": "High utility for data generation and first-person training where diversity and realism matter; transfer-to-real-world depends on physical and causal fidelity of generated sequences.",
            "tradeoffs_observed": "Generative flexibility and scene diversity vs. potential physical/causal inaccuracies and compute cost; dynamic generation alleviates manual setup burden.",
            "design_choices": "Condition on actions, camera parameters, and text instructions; use multimodal data sources (3D sims, real robot actions, internet media) for richness.",
            "comparison_to_alternatives": "More scalable and diverse than static simulators (less manual asset engineering); may be less physically accurate than physics-based simulators.",
            "optimal_configuration": "Survey suggests dynamic generative environments are optimal for rapid diversity and first-person embodied training, with hybrid physics constraints preferred for sim-to-real transfer.",
            "uuid": "e1254.8",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Dynalang",
            "name_full": "Dynalang (multimodal LLM world model)",
            "brief_description": "A multimodal world model that predicts future text and image representations and learns policies from imagined rollouts; used as an LLM-backboned world model for action/policy learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Dynalang",
            "model_description": "Learns to predict future multimodal (text + image) representations conditioned on actions and uses imagined rollouts in representation space to train actor-critic policies without directly operating in pixel space.",
            "model_type": "language+vision multimodal latent world model",
            "task_domain": "embodied control, navigation, planning and policy learning from imagined multimodal rollouts",
            "fidelity_metric": "Accuracy of predicted multimodal representations (e.g., next-image embedding prediction), and downstream policy success rates (actor-critic returns).",
            "fidelity_performance": "Survey reports Dynalang can predict future multimodal representations and support policy learning from imagined rollouts; no numeric performance figures provided.",
            "interpretability_assessment": "Intermediate representations (text/image embeddings) provide some semantic interpretability; model internals remain neural and not fully transparent.",
            "interpretability_method": "Inspection of predicted representations and policy behaviors; no systematic symbolic extraction reported.",
            "computational_cost": "Potentially lower than pixel-space video generation since it operates in representation space; exact compute numbers not reported.",
            "efficiency_comparison": "More efficient for policy learning than pixel-level methods by operating on compact multimodal representations and imagined rollouts.",
            "task_performance": "Enables policy learning using imagined multimodal futures; reportedly effective in supporting actor-critic training purely on generated representations.",
            "task_utility_analysis": "High utility for multi-modal decision-making tasks where semantic representations are sufficient; may miss fine-grained physical details needed for precise control.",
            "tradeoffs_observed": "Semantic compactness and sample efficiency vs. loss of pixel-level fidelity and precise physical simulation.",
            "design_choices": "Predicts future text/image embeddings, trains policies on imagined rollouts, uses actor-critic updates driven by multimodal rollouts.",
            "comparison_to_alternatives": "More flexible and semantically grounded than pure-visual latent models; less physically explicit than hybrid physics-embedding approaches.",
            "optimal_configuration": "Survey indicates representation-space imagination is effective when task reward can be inferred from semantic embeddingsâ€”optimal balance favors compact multimodal embeddings for efficiency.",
            "uuid": "e1254.9",
            "source_info": {
                "paper_title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Video generation models as world simulators",
            "rating": 2,
            "sanitized_title": "video_generation_models_as_world_simulators"
        },
        {
            "paper_title": "How far is video generation from world model: A physical law perspective",
            "rating": 2,
            "sanitized_title": "how_far_is_video_generation_from_world_model_a_physical_law_perspective"
        },
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "DreamerV3",
            "rating": 2
        },
        {
            "paper_title": "GAIA-1: A generative world model for autonomous driving",
            "rating": 2,
            "sanitized_title": "gaia1_a_generative_world_model_for_autonomous_driving"
        },
        {
            "paper_title": "PhysGen: Rigid-body physics-grounded image-to-video generation",
            "rating": 2,
            "sanitized_title": "physgen_rigidbody_physicsgrounded_imagetovideo_generation"
        },
        {
            "paper_title": "UniSim",
            "rating": 2
        },
        {
            "paper_title": "DriveDreamer: Towards realworld-driven world models for autonomous driving",
            "rating": 2,
            "sanitized_title": "drivedreamer_towards_realworlddriven_world_models_for_autonomous_driving"
        },
        {
            "paper_title": "OccWorld: Learning a 3d occupancy world model for autonomous driving",
            "rating": 2,
            "sanitized_title": "occworld_learning_a_3d_occupancy_world_model_for_autonomous_driving"
        },
        {
            "paper_title": "Dynalang",
            "rating": 1
        }
    ],
    "cost": 0.02497675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding World or Predicting Future? A Comprehensive Survey of World Models
25 Jun 2025</p>
<p>Jingtao Ding 
Yunke Zhang 
Yu U Shang 
Yuheng Zhang 
Zefang Zong 
Jie Feng 
Yuan Yuan 
Hongyuan Su 
Nian Li 
Nicholas Sukiennik 
Fengli Xu 
Yong Li liyong07@tsinghua.edu.cn 
Yunke Zhang 
Yuheng Zhang </p>
<p>Department of Electronic Engineering
Beijing National Research Center for Information Science and Technology (BNRist)
Tsinghua University
China</p>
<p>Department of Electronic Engineering
Beijing National Research Center for Information Science and Technology (BNRist)
Tsinghua University
China</p>
<p>Understanding World or Predicting Future? A Comprehensive Survey of World Models
25 Jun 202516A7ADC9BA99881559028CFEED0014AEarXiv:2411.14499v2[cs.CL]CCS Concepts:Computing methodologies â†’ Machine learningArtificial intelligenceModeling and simulation World model, model-based RL, video generation, embodied environment, autonomous driving, robots, social simulacra
The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence.This survey offers a comprehensive review of the literature on world models.Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics.This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making.Initially, we examine the current progress in these two categories.We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects.Finally, we outline key challenges and provide insights into potential future research directions.We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.</p>
<p>INTRODUCTION</p>
<p>The scientific community has long aspired to develop a unified model that can replicate its fundamental dynamics of the world in pursuit of Artificial General Intelligence (AGI) [109].In 2024, the emergence of multimodal large language models (LLMs) and video generation models like Sora [146] has intensified discussions surrounding such World Models.While these models demonstrate an emerging capacity to capture aspects of world knowledge-such as Sora's generated videos, which appear to perfectly adhere to physical laws-questions persist regarding whether they truly qualify as comprehensive world models.Therefore, a systematic review of recent advancements, applications, and future directions in world model research is both timely and essential as we look toward new breakthroughs in the era of artificial intelligence (AI).</p>
<p>The definition of a world model remains a subject of ongoing debate, generally divided into two primary perspectives: understanding the world and predicting the future.As depicted in Figure 1, early work by Ha and Schmidhuber [66] focused on abstracting the external world to gain a deep understanding of its underlying mechanisms.In contrast, LeCun [109] argued that a world model should not only perceive and model the real world but also possess the capacity to envision possible future states to inform decision-making.Video generation models such as Sora represent an approach that concentrates on simulating future world evolution and thus align more closely World Modelsmental model [65] JEPA -perception and reasoning [109] UniSim [236] Sora [146] Applications Autonomous Driving Robots Social Simulacra</p>
<p>World Model
Implicit</p>
<p>2024.5</p>
<p>Smallville [148] 2023.8 2022 2018 2024.5 Vista [54] DayDreamer [220] 2022.12 World Knowledge in LLM [64] 2023.10Frame System Theory [136] 1974 Fig. 1.The overall framework of this survey.We systematically define the essential purpose of a world model as understanding the dynamics of the external world and predicting future scenarios.The timeline illustrates the development of key definitions and applications.</p>
<p>with the predictive aspect of world models.This raises the question of whether a world model should prioritize understanding the present or forecasting future states.In this paper, we provide a comprehensive review of the literature from both perspectives, highlighting key approaches and challenges.</p>
<p>The potential applications of world models span a wide array of fields, each with distinct requirements for understanding and predictive capabilities.In autonomous driving, for example, world models need to perceive road conditions in real-time [198,216] and accurately predict their evolution [143,187,264], with a particular focus on immediate environmental awareness and forecasting of complex trends.For robotics, world models are essential for tasks such as navigation [179], object detection [204], and task planning [69], requiring a precise understanding of external dynamics [51] and the ability to generate interactive and embodied environments [148].In the realm of simulation of virtual social systems, world models must capture and predict more abstract behavioral dynamics, such as social interactions and human decision-making processes.Thus, a comprehensive review of advancements in these capabilities, alongside an exploration of future research directions and trends, is both timely and essential.</p>
<p>Existing surveys on world models can generally be classified into two categories, as shown in Table S1.The first category primarily focuses on describing the application of world models in specific fields such as video processing and generation [24,266], autonomous driving [62,112,231], and agent-based applications [266].The second category [130] concentrates on the technological transitions from multi-modal models, which are capable of processing data across various modalities, to world models.However, these papers often lack a systematic examination of what precisely constitutes a world model and what different real-world applications require from these models.</p>
<p>In this article, we aim to formally define and categorize world models, review recent technical progress, and explore their extensive applications.</p>
<p>The main contributions of this survey can be summarized as follows: (1) We present a novel categorization system for world models structured around two primary functions: constructing implicit representations to understand the mechanism of the external world and predicting future states of the external world.The first category focuses on the development of models that learn and internalize world knowledge to support subsequent decision-making, while the latter emphasizes enhancing predictive and simulative capabilities in the physical world from visual perceptions.(2) Based on this categorization, we classify how various key application areas, including autonomous driving, robots, and social simulacra, emphasize different aspects of world models.(3) We highlight future research directions and trends of world models that can adapt to a broader spectrum of practical applications.</p>
<p>The remainder of this paper is organized as follows.In Section 2, we introduce the background of the world model and propose our categorization system.Section 3 and Section 4 elaborate on the details of current research progress on two categories of world models, respectively.Section 5 covers applications of the world model in three key research fields.Section 6 outlines open problems and future directions of world models.</p>
<p>BACKGROUND AND CATEGORIZATION</p>
<p>In this section, we explore the evolving concepts of world models in the literature and categorize efforts to construct world models into two distinct branches: internal representation and future prediction.</p>
<p>The concept of building an internal model of the world has a long history in AI, dating back to foundational work such as Marvin Minsky's frame representation in the 1960s [136], designed to systematically capture structured knowledge about the world.Ha et al. [65,66] significantly revived and popularized the term "world model" in 2018 by proposing neural-network-based implicit models for learning latent representations.This line of research aligns with the psychological theory of "mental models" [94] 1 , which holds that humans perceive the external world by abstracting it into simplified elements and relationships-an underlying philosophical root reflected alike in both frames and world models.This principle suggests that our descriptions of the world, when viewed from a deep, internal perspective, typically involve constructing an abstract representation that suffices without requiring detailed depiction.Building upon this conceptual framework, the authors introduce an agent model inspired by the human cognitive system, as illustrated in Figure 1.In this pioneering model, the agent receives feedback from the real-world environment, which is then transformed into a series of inputs that train the model.This model is adept at simulating potential outcomes following specific actions within the external environment.Essentially, it creates a mental simulation of potential future world evolutions, with decisions made based on the predicted outcomes of these states.This methodology closely mirrors the Model-based Reinforcement Learning (MBRL) method, where both strategies involve the model generating internal representations of the external world.These representations facilitate navigation through and resolution of various decision-making tasks in the real world.</p>
<p>In the visionary article on the development of autonomous machine intelligence in 2022 [109], Yann LeCun introduced the Joint Embedding Predictive Architecture (JEPA), a framework mirroring the human brain's structure.As illustrated in Figure 1, JEPA comprises a perception module that processes sensory data, followed by a cognitive module that evaluates this information, effectively embodying the world model.This model allows the brain to assess actions and determine the most suitable responses for real-world applications.LeCun's framework is intriguing due to its incorporation of the dual-system concept, mirroring "fast" and "slow" thinking.System 1 involves intuitive, instinctive reactions: quick decisions made without a world model, such as instinctively dodging an oncoming person.In contrast, System 2 employs deliberate, calculated reasoning that considers the future state of the world.It extends beyond immediate sensory input, simulating potential future scenarios, like predicting events in a room over the next ten minutes and adjusting actions accordingly.This level of foresight requires constructing a world model to effectively guide decisions based on the anticipated dynamics and evolution of the environment.In this framework, the world model is essential for understanding and representing the external world.It models the state of the world using latent variables, which capture key information while filtering out redundancies.This approach allows for a highly efficient, minimalistic representation of the world, facilitating optimal decision-making and planning for future scenarios.</p>
<p>The ability of models to capture world knowledge is critical for their effective performance in a wide range of real-world tasks.In the recent wave of works on large language models starting from 2023, several have demonstrated the presence of latent world knowledge.In other words, these models capture intuitive knowledge, including spatial and temporal understanding, which enables them to make predictions about real-world scenarios [64,133].Furthermore, LLMs are capable of modeling the external world through cognitive maps, as indicated by recent research revealing the brain-like structures embedded within them [117].These models can even learn to predict future events based on prior experiences, thereby enhancing their utility and applicability in real-world contexts.</p>
<p>The above world models primarily represent an implicit understanding of the external world.Powered by generative learning like diffusion modeling and model architecture like transformer, recent video generation models (e.g., Sora [146], Keling [104], Gen-2 [29], etc.) take text instructions or real-world visual data as input and output high-quality video frames.Notably, these models demonstrate exceptional modeling capabilities, such as maintaining consistency in 3D video simulations, producing physically plausible outcomes, and simulating digital environments.These capabilities suggest that they not only mimic the appearance of but also model the real-world dynamics within simulation scenarios, focusing on realistically modeling dynamic world changes rather than merely representing static world states.</p>
<p>Whether focusing on learning internal representations of the external world or simulating its operational principles, these concepts coalesce into a shared consensus: the essential purpose of a world model is to understand the dynamics of the world and compute the next state with certainty (or with some guarantee), which empowers the model to extrapolate longer-horizon evolution and to support downstream decision-making and planning.From this perspective, we conduct a thorough examination of recent advancements in world models, analyzing them through the following lenses, as depicted in Figure 1.</p>
<p>â€¢ Implicit representation of the external world (Section 3): This research category constructs a model of environmental change to enable more informed decision-making, ultimately aiming to predict the evolution of future states.It fosters an implicit comprehension by transforming external realities into a model that represents these elements as latent variables.Furthermore, with the advent of large language models (LLMs), efforts previously concentrated on traditional decision-making tasks have been significantly enhanced by the detailed descriptive power of these models regarding world knowledge.We further focus on the integration of world knowledge into existing models.â€¢ Future predictions of the external world (Section 4): We initially explore generative models that simulate the external world, primarily using visual video data.These works emphasize the realness of generated videos that mirror future states of the physical world.</p>
<p>As recent advancements shift focus toward developing a truly interactive physical world, we further investigate the transition from visual to spatial representations and from video to embodiment.This includes comprehensive coverage of studies related to the generation of embodied environments that mirror the external world.â€¢ Applications of world models (Section 5): World models have a wide range of applications across various fields, including autonomous driving, robotics, and social simulacra.We explore how the integration of world models in these domains advances both theoretical research and practical implementations, emphasizing their transformative potential in real-world applications.</p>
<p>IMPLICIT REPRESENTATION OF THE EXTERNAL WORLD</p>
<p>This section examines how world models enable informed decision-making by representing the environment as latent variables.Section 3.1 focuses on the world models in model-based RL (MBRL), while Section 3.2 explores the integration of world knowledge into advanced AI models, especially LLMs, enhancing real-world task performance.</p>
<p>World Model in Decision-Making</p>
<p>In decision-making tasks, understanding the environment is the major task in setting a foundation for optimized policy generation.As such, the world model in decision-making should include a comprehensive understanding of the environment.It enables us to take hypothetical actions without affecting the real environment, facilitating a low trial-and-error cost.In literature, research on how to learn and utilize the world model was initially proposed in the field of model-based RL.Furthermore, recent progress on LLM and MLLM also provide comprehensive backbones for world model construction.With language serving as a more general representation, language-based world models can be adapted to more generalized tasks.The two schemes of leveraging world models in decision-making tasks are shown in Figure 2.  World Model Learning.To learn an accurate world model, the most straightforward approach is to leverage the mean squared prediction error on each one-step transitions [89,90,108,129,163],
min ðœƒ E ð‘  â€² âˆ¼ð‘€ * (â€¢ |ð‘ ,ð‘Ž) [||ð‘  â€² âˆ’ ð‘€ ðœƒ (ð‘ , ð‘Ž)|| 2 2 ],(1)
where  * is the real transition dynamics used to collect trajectory data and   is the parameterized transition to learn.Apart from directly utilizing the deterministic transition model, Chua et al. [27] further model the aleatoric uncertainty with the probabilistic transition model.The objective is to minimize the KL divergence between the transition models,
min ðœƒ E ð‘  â€² âˆ¼ð‘€ * (â€¢ |ð‘ ,ð‘Ž) [ð‘™ð‘œð‘”( ð‘€ * (ð‘  â€² |ð‘ , ð‘Ž) ð‘€ ðœƒ (ð‘  â€² |ð‘ , ð‘Ž) )].(2)
In both settings, the phase of the world model learning task can be transformed into a supervised learning task.The learning labels are the trajectories derived from real interaction environments, also called the simulation data [128].</p>
<p>For high-dimensional environments, representation learning is essential for effective worldmodel training in MBRL.Early work by Ha and Schmidhuber [65] reconstructs images through an autoencoder-latent-state pipeline, whereas Hafner et al. [68,70] couple a visual encoder with latent dynamics to master pixel-based control tasks.Their latest iteration, DreamerV3 [71], adds robust normalization and balancing techniques, solving over 150 tasks-including diamond collection in Minecraft-without human data or domain-specific tuning.Memory-centric extensions such as Recall-to-Imaging by Samsami et al. [171] further enhance long-horizon reasoning.A complementary trend is unified model learning via next-token prediction with transformer architectures, as shown by Janner et al. [90] and expanded by Schubert et al. [177].Further, Georgiev et al. [55] train a large off-policy multi-task world model whose smooth latent dynamics enable efficient per-task policy learning with first-order gradients, achieving strong scalability and performance without online planning.Recent work by Jonathan Richens et al. [167] further reinforces the necessity of world models, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment, with the world model emerging from the agent's policy.This insight aligns with the ongoing trend of incorporating predictive modeling into reinforcement learning to handle more complex and goal-oriented tasks.</p>
<p>Policy Generation with World Model.With an ideally optimized world model, one most straightforward way to generate a corresponding policy is model predictive control (MPC) [102].MPC plans an optimized sequence of actions given the model as follows: max
ð‘Ž ð‘¡ :ð‘¡ +ðœ E ð‘  ð‘¡ â€² +1 âˆ¼ð‘ (ð‘  ð‘¡ â€² +1 |ð‘  ð‘¡ â€² ,ð‘Ž ð‘¡ â€² ) [ ð‘¡ +ðœ âˆ‘ï¸ ð‘¡ â€² =ð‘¡ ð‘Ÿ (ð‘  ð‘¡ â€² , ð‘Ž ð‘¡ â€² )],(3)
where  denotes the planning horizon.Nagabandi et al. [141] adopt a simple Monte Carlo method to sample action sequences.Rather than sampling actions uniformly, Chua et al. [27] propose a new probabilistic algorithm that ensembles with trajectory sampling.Further literature also improves the optimization efficiency by leveraging the world model usage [68,79,208,245].Hansen et al. [72] introduced an improved model-based RL algorithm called TD-MPC2 that integrates trajectory optimization within the latent space of a learned implicit world model.It achieves strong performance across diverse continuous control tasks and demonstrates scalability by training large agents with hundreds of millions of parameters across multiple domains.</p>
<p>Another popular approach to generating world model policies is the Monte Carlo Tree Search (MCTS).By maintaining a search tree where each node refers to a state evaluated by a predefined value function, actions will be chosen such that the agent can be processed to a state with a higher value.AlphaGo and AlphaGo Zero are two significant applications using MCTS in discrete action space [189,190].Moerland et al. [138] extended MCTS to solve decision problems in continuous action space.Oh et al. [144] proposed a value prediction network that applies MCTS to the learned model to search for actions based on value and reward predictions.</p>
<p>3.1.2World model with language backbone.The rapid growth of language models, especially LLM and MLLM, benefits development in many related applications.With language serving as a universal representation backbone, language-based world models have shown their potential in many decision-making tasks.</p>
<p>Direct Action Generation via LLM World Models.LLM is capable of directly generating actions in decision-making tasks based on corresponding constructed world models.For example, in the navigation scenarios, Yang et al. [234] transfer pre-trained text-to-video models to domainspecific tasks for robot control, successfully annotating robot manipulation with text instructions as LLM outputs.Zhou et al. [263] further learn a compositional world model by factorizing the video generation process.Such a method enables a strong few-shot transfer ability to unseen tasks.</p>
<p>Besides training or fine-tuning specialized language-based world models, LLMs and MLLMs can be directly deployed to understand the world environment in decision-making tasks.For example, Long et al. [126] propose a multi-expert scheme to handle visual language navigation tasks.They construct a standardized discussion process where eight LLM-based experts participate to generate the final movement decision.An abstract world model is constructed from the discussion and further imagination (of future states) of the experts to support action generation.Zhao et al. [255] further combine LLMs and open-vocabulary detection to construct the relationship between multi-modal signals and key information in navigation.They propose an omni-graph to capture the structure of the local space as the world model for the navigation task.Meanwhile, Yang et al. [239] utilize an LLM-based imaginative assistant to infer the global semantic graph as the world model based on the environment perception, and another reflective planner to directly generate actions.</p>
<p>Modular Usage of LLM World Models.Although taking LLM outputs as actions directly is straightforward in application and deployment, the decision quality in such a scheme heavily relies on the reasoning ability of the LLM itself.Although this year has witnessed the large potential of LLM reasoning capability [225], it can be further improved by integrating LLM-based world models as modules with external model-based verifiers or other effective planning algorithms [96].</p>
<p>Guan et al. [61] extract explicit world models by prompting GPT-4 to generate and iteratively refine PDDL domain descriptions, then pair these models with off-the-shelf planners, yielding good planning performance with much less human intervention.Xiang et al. [224] deploys an embodied agent in a world model, the simulator of VirtualHome [156], where the corresponding embodied knowledge is injected into LLMs.To better plan and complete specific goals, they propose a goal-conditioned planning schema where Monte Carlo Tree Search (MCTS) is utilized to search for the true embodied task goal.Lin et al. [119] introduce an agent, Dynalang, which learns a multimodal world model to predict future text and image representations, and which learns to act from imagined model rollouts.The policy learning stage utilizes an actor-critic algorithm purely based on the previously generated multimodal representations.Liu et al. [125] further cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs).LLMs, like the world model, perform in an in-context manner within the actor-critic updates of MDPs.The proposed RAFA framework shows significantly increased performance in multiple complex reasoning tasks and environments, such as ALFWorld [188].KoLA [242] 2024 ICLR Language Benchmark EWOK [86] 2024 arxiv Language Benchmark Geometry of Concepts [117] 2024 arxiv Language Analysis</p>
<p>Knowledge of Global Physcial World</p>
<p>Space&amp;Time [64] 2024 ICLR Language Analysis GeoLLM [133] 2024 ICLR Language Learning GeoLLM-Bias [132] 2024 ICML Language Learning GPT4GEO [169] 2023 NeurIPS(FMDM) Language Benchmark CityGPT [42] 2024 arxiv Language Learning CityBench [44] 2024 arxiv Language&amp;Vision Benchmark</p>
<p>Knowledge of Local Physical World</p>
<p>Predictive [59] 2024 NMI Vision Learning Emergent [93] 2024 ICML Language Learning E2WM [224] 2023 NeurIPS Language Learning Dynalang [119] 2024 ICML Language&amp;Vision Learning</p>
<p>Knowledge of Human Society</p>
<p>Testing ToM [194] 2024 NHB Language Benchmark High-order ToM [195] 2024 arxiv Language Benchmark COKE [219] 2024 ACL Language Learning MuMA-ToM [185] 2024 ACL Language&amp;Vision Benchmark SimToM [215] 2024 ACL Language Learning</p>
<p>World Knowledge Learned by Models The Knowledge of Local Physical World</p>
<p>The Knowledge of Human Society</p>
<p>The Knowledge of Global Physical World Common Sense and General Knoweldge Fig. 3. World knowledge in large language models for world model.</p>
<p>World Knowledge Learned by Models</p>
<p>After pretraining on large-scale web text and books [145,201], large language models attain extensive knowledge about the real world and common sense relevant to daily life.This embedded knowledge is considered crucial for their remarkable ability to generalize and perform effectively in real-world tasks.For instance, researchers leverage the common sense of large language models for task planning [257], robot control [82], and image understanding [123].Furthermore, Li et al. [117] discover brain-like structures of world knowledge embedded in the high-dimensional vectors that represent the universe of concepts in large language models.Also, Li et al. [111] demonstrate that language models partially converge towards representations isomorphic to those of vision models.</p>
<p>Unlike common sense and general knowledge, we focus on world knowledge within large language models from the perspective of a world model.As shown in Figure 3, based on objects and spatial scope, the world knowledge in the large language models can be categorized into three parts: 1) knowledge of the global physical world; 2) knowledge of the local physical world; and 3) knowledge of human society.We summarize recent works in Table 1.</p>
<p>Knowledge of the Global Physical World.</p>
<p>We first introduce research focused on analyzing and understanding the knowledge of the global physical world.Gurnee et al. [64] present the first evidence that large language models genuinely acquire spatial and temporal knowledge of the world, rather than merely collecting superficial statistics.They identify distinct "spatial neurons" and "temporal neurons" in LLama2 [201], suggesting that the model learns linear representations of space and time across multiple scales.Distinct from previous observations focused on embedding space, Manvi et al. [132,133] develop effective prompts about textual address to extract intuitive real-world knowledge about the geospatial space and successfully improve the performance of the model in various downstream geospatial prediction tasks.</p>
<p>While large language models do acquire some implicit knowledge of the real world [64,117], the quality of this knowledge remains questionable [42,169].For example, Feng et al. [42] find that the urban knowledge embedded in large language models is often coarse and inaccurate.To address this, they propose an effective framework to improve the acquisition of urban knowledge of specific cities in large language models.From these works, we can see that although large language models have demonstrated the ability to capture certain aspects of real-world knowledge [64,117,169], it is clear that further efforts are needed to enhance this knowledge to enable broader and more reliable real-world applications [43].</p>
<p>Knowledge of the Local Physical</p>
<p>World.Unlike the knowledge of the global physical world, the local physical world represents the primary environment for human daily life and most realworld tasks.Therefore, understanding and modeling the local physical world is a more critical topic for building a comprehensive world model.We first introduce the concept of the cognitive map [200], which refers to the mental representation that humans form to navigate and understand their environment, including spatial relationships and landmarks.Although initially developed to explain human learning processes, researchers have discovered similar structures in large language models [117] and have leveraged these insights to enhance the efficiency and performance of artificial models in learning and understanding the physical world.</p>
<p>Recent studies explore actively encouraging models to learn abstract knowledge through cognitive map-like processes across various environments.For example, Cornet et al. [59] show that in a simplified Minecraft world, visual predictive coding lets an agent build a spatial cognitive map purely from pixels.Once trained, the latent map encodes its metric distance to any target, enabling accurate rollout of future observations.Lin et al. [119] investigate teaching models to understand the game environments through a world model learning procedure, specifically by predicting the subsequent frame of the environment.In this way, the model can generate better actions in dynamic environments.Moreover, Jin et al. [93] find that language models can learn the emergent representations of program semantics by predicting the next token.</p>
<p>Knowledge of the Human Society.</p>
<p>Beyond the physical world, understanding human society is another crucial aspect of world models.David Premack and Guy Woodruff proposed the Theory of Mind [155], which was later developed to explain how individuals infer the mental states of others around them.Recent works have extensively explored how large language models develop and demonstrate this social world model [173,194].Sap et al. [173] conduct an investigation focusing on evaluating the performance of large language models across various Theory of Mind tasks to determine whether their human-like behaviors reflect genuine comprehension of social rules and implicit knowledge.Strachan et al. [194] conduct a comparative analysis between human and LLM performance on diverse Theory of Mind abilities, such as understanding false beliefs and recognizing irony.While their findings demonstrate the potential of GPT-4 in these tasks, they also identify its limitations, particularly in detecting faux pas.</p>
<p>To address these limitations, researchers propose innovative methods to enhance the abilities of large language models in Theory of Mind for complex real-world applications.Wu et al. [219] introduce COKE, which constructs a knowledge graph to help large language models explicitly using theory in mind through cognitive chains.Additionally, Alex et al. [215] develop SimToM, a two-stage prompting framework, to enhance the performance of large language models in theory of mind tasks.</p>
<p>FUTURE PREDICTION OF THE PHYSICAL WORLD</p>
<p>World Model as Video Generation</p>
<p>The integration of video generation into world models marks a significant leap forward in the field of environment modeling [146].Traditional world models primarily focused on predicting discrete or static future states [66,109].However, by generating video-like simulations that capture continuous spatial and temporal dynamics, world models [146,233] have evolved to address more complex, dynamic environments.This breakthrough in video generation has pushed the capabilities of world models to a new level.</p>
<p>Towards Video World Models.</p>
<p>A video world model is a computational framework designed to simulate and predict the future state of the world by processing past observations and potential actions within a visual context [146].This concept builds on the broader idea of world models, which strive to capture the dynamics of an environment and enable machines to predict how the world will evolve over time.In the case of a video world model, the focus is on generating sequences of visual frames that represent these evolving states.</p>
<p>Sora as a World Simulator.Sora [146] is a large-scale video generation model, which is designed to generate high-quality, temporally consistent video sequences, up to one minute long, based on various input modalities such as text, images, and videos.Sora leverages a combination of powerful neural network architectures, including encoder-decoder frameworks and transformers, to process multimodal inputs and generate visually coherent simulations.Sora's core capabilities lie in its ability to generate videos that align with real-world physical principles, such as the reflection of light on surfaces or the melting of candles.These properties suggest that Sora has the potential to act as a world simulator, predicting future states of the world based on its understanding of the initial conditions and simulation parameters.</p>
<p>Sora's Limitations.However, despite its impressive video generation abilities, Sora has several limitations in terms of understanding and simulating the external world.One key limitation concerns causal reasoning [24,266], wherein the model is limited in simulating dynamic interactions within the environment.Thus, Sora can only passively generate video sequences based on an observed initial state, but cannot actively intervene or predict how changes in actions might alter the course of events.Another limitation is that it still fails to reproduce correct physical laws consistently [97].While Sora can generate visually realistic scenes, it struggles with accurately simulating real-world physics, such as the behavior of objects under different forces, fluid dynamics, or the accurate depiction of light and shadow interactions.</p>
<p>Other Video World Models.Sora has undoubtedly catalyzed a significant wave of research into video world models, inspiring a surge of advancements in this field.Following Sora's success in generating high-quality video sequences, numerous subsequent models have been developed, each aiming to push the boundaries of what video world models can achieve.For example, some approaches have extended video lengths to enable long-form video simulation [77,121,241].In addition to conventional language-guided video generation, more modalities are being integrated, such as images and actions [223,258].Researchers are also shifting their focus from basic video generation, which lacks user control, to interactive simulations that aim to replicate the decision space of the real world and facilitate decision-making [87,218,223,235,237,249].Several studies have worked to enhance the smoothness of action transitions, improve the accuracy of physical laws, and maintain temporal consistency [17,166,229,233].Meanwhile, the concept of world models has evolved beyond imagination and is being applied in various scenario-specific simulations, including natural environments, games, and autonomous driving [12,16,77,121,134,135,209,211,261].Table 2 summarizes the categorization of improvements in video world models across different aspects.</p>
<p>4.1.2Capabilities of Video World Models.Despite the ongoing debate about whether models like Sora can be considered full-fledged world models, there is no doubt that video world models hold tremendous potential for advancing environment simulation and prediction [24,97,266].These models can offer a powerful approach to understanding and interacting with complex environments by generating realistic, dynamic video sequences.To achieve this level of sophistication, this section outlines the key capabilities that video world models must possess to set them apart from traditional video generation models.</p>
<p>Long-Term Predictive Ability.A robust video world model should be capable of making longterm predictions that adhere to the dynamic rules of the environment over an extended period.This capability allows the model to simulate how a scenario evolves, ensuring that the generated video sequences remain consistent with the temporal progression of the real world.Although Sora has achieved the generation of minute-long video sequences with high-quality temporal coherence, it is still far from being able to simulate complex, long-term dynamics found in real-world environments.Recent efforts have explored extending video lengths to capture longer-term dependencies and improve temporal consistency [77,121,241].</p>
<p>Multi-Modal Integration.In addition to language-guided video generation, video world models are increasingly integrating other modalities, such as images and actions, to enhance realism and interactivity [223,258].The integration of multiple modalities allows for richer simulations that better capture the complexity of real-world environments, improving both the accuracy and diversity of generated scenarios.</p>
<p>Interactivity.Another critical capability of video world models is their potential for controllability and interactivity.An ideal model should not only generate realistic simulations but also allow for interaction with the environment.This interactivity involves simulating the consequences of different actions and providing feedback, enabling the model to be used in applications requiring dynamic decision-making.Recent work is focusing on enhancing control over the simulations, allowing for more user-guided exploration of scenarios [218,237].</p>
<p>Diverse Environments.Finally, video world models are being adapted to a variety of scenariospecific simulations, including natural environments, autonomous driving, and gaming.These models are evolving beyond basic video generation to replicate real-world dynamics and support a wide range of applications [16,121,211].</p>
<p>World Model as Embodied Environment</p>
<p>The development of world models for embodied environments is crucial for simulating and predicting how agents interact with and adapt to the external world.Initially, generative models focused on simulating visual aspects of the world, using video data to capture dynamic changes in the Table 2. Overview of recent models in video generation across various categories, which summarizes key models in long-term video generation, multi-modal learning, interactive video generation, temporal consistency, and diverse environment modeling.</p>
<p>Category Model Description Technique</p>
<p>Long-term NUWA-XL [241] "Coarse-to-fine" Diffusion over Diffusion architecture for long video generation.</p>
<p>Diffusion LWM [121] Training large transformers on long video and language sequences.</p>
<p>Transformer GAIA-1 [77] Generative world model predicting driving scenarios for autonomous driving.</p>
<p>Transformer, Diffusion</p>
<p>Multimodal 3D-VLA [258] Integrates 3D perception, reasoning, and action in a world model for embodied AI.</p>
<p>Diffusion</p>
<p>Pandora [223] World-state simulation and real-time control with free-text actions.</p>
<p>LLM</p>
<p>Genie [16] Generative model from text, images, and sketches.Transformer</p>
<p>Interactive UniSim [235] Simulates real-world interactions for vision-language and RL training.</p>
<p>Diffusion, RL</p>
<p>VideoDecision [237] Extends video models to real-world tasks like planning and RL.</p>
<p>Transformer, Diffusion iVideoGPT [218] Combines visual, action, and reward signals for interactive world modeling.</p>
<p>Transformer</p>
<p>PhysDreamer [249] Simulates 3D object dynamics to generate responses to novel interactions.</p>
<p>Diffusion PEEKABOO [87] Enhances interactivity with spatiotemporal control without extra training.</p>
<p>Diffusion Transformer</p>
<p>Consistency</p>
<p>WorldGPT [233] Improves temporal consistency and action smoothness with multimodal learning and refined key frame generation.</p>
<p>Diffusion</p>
<p>DiffDreamer [17] Long-range scene extrapolation with improved consistency.</p>
<p>Diffusion</p>
<p>ConsistI2V [166] Enhances visual consistency in image-to-video generation.</p>
<p>Diffusion</p>
<p>Diverse environments</p>
<p>WorldDreamer [211] World model capturing dynamic elements across diverse scenarios.</p>
<p>Transformer</p>
<p>Genie [16] Unsupervised generative model for action-controllable virtual environments.</p>
<p>Transformer MUVO [12] Multimodal world model using camera and lidar data.Transformer UniWorld [135] 3D detection and motion prediction in autonomous driving.</p>
<p>Transformer environment.More recently, the focus has shifted towards creating fully interactive and embodied simulations.These models not only represent the visual elements of the world but also incorporate spatial and physical interactions that more accurately reflect real-world dynamics.By integrating spatial representations and transitioning from video-based simulations to immersive, embodied environments, world models can now provide a more comprehensive platform for developing agents capable of interacting with complex real-world environments.</p>
<p>World models as embodied environments can be divided into three categories: indoor, outdoor, and dynamic environments, as shown in Figure 4, and the relevant works are summarized in Table 3.It can be summarized that most current works focus on developing static, existing indoor and outdoor embodied environments.An emerging trend is to predict the dynamic, future world Table 3.Comparison of existing works on world models as embodied environments, including indoor, outdoor, and dynamic environments.In the 'Modality' column, 'V' refers to vision, 'L' refers to lidar, 'T' refers to text, and 'A' refers to audio.In the 'Num of Scenes' column, '-' means no reported data, and 'Arbitrary' means the method can support generating any number of scenes.through generative models producing first-person, dynamic video-based simulation environments.Such environments can offer flexible and realistic feedback for training embodied agents, enabling them to interact with ever-changing environments and improve their generalization ability.</p>
<p>Indoor Environments.</p>
<p>Indoor environments offer controlled, structured scenarios where agents can perform detailed, task-specific actions such as object manipulation, navigation, and real-time interaction with users [18,53,101,150,156,174,183,222].Early works on establishing indoor environments like AI2-THOR [101] and Matterport 3D [18] focus on providing only visual information.These works build indoor environments by providing photorealistic settings where agents can practice visual navigation and engage in interactive tasks that mimic real-life home activities.These environments emphasize the importance of using visual-based reinforcement learning techniques that allow agents to optimize their decision-making based on environmental cues.By simulating real-world tasks like cooking or cleaning, these platforms assess an agent's capacity to generalize learned behaviors across different types of spaces and objects.A line of further works contributes toward expanding the data modalities of the provided environments.Among these, iGibson [183] introduces Lidar observation as additional signal feedback, contributing to more accurate environment perception of agents.AVLEN [150] further supplements audio signals allowing agents to execute tasks such as object manipulation and navigation in household-like settings.The challenge here lies in enabling agents to understand and act on multimodal input including vision, language, and sound within a constrained space.Adding a social dimension, environments like GRUtopia [205] introduce agents to spaces where they must navigate and interact with both objects and NPCs.Here, agents need to understand social dynamics, such as positioning and task sharing, which requires more advanced forms of interaction modeling.The inclusion of social interaction modules in these settings demonstrates how agents can be trained to balance human-like social behaviors with task performance.More recently, with the development of LLMs, some works [21,48,238] seek to provide a flexible environment generation pipeline, supporting the generation of arbitrary indoor environments with language instructions.</p>
<p>Outdoor Environments.</p>
<p>In contrast to indoor environments, creating outdoor environments [41,49,180,205,221] faces greater challenges due to their larger scale and increased variability.Some existing works focus on urban environments, such as MetaUrban [221], where agents are deployed to navigate in large-scale urban environments, where they encounter challenges like dynamically changing traffic, varied building structures, and social interactions with other entities.These tasks often require the use of context-aware navigation algorithms that allow agents to adjust their trajectories and behaviors based on the layout and conditions of the environment.However, the environments in MetaUrban are created by retrieving and organizing 3D assets from existing libraries.Recently, utilizing advanced generative techniques, UrbanWorld [180] significantly enhances the scope of outdoor environments, using 3D generative models to create complex, customizable urban spaces that allow for more diverse urban scenarios.This shift from static asset-based environments to generative ones ensures that agents are exposed to a wider variety of tasks, from navigating unfamiliar street layouts to interacting with new types of objects or structures.In addition to the above real open-world generation works, there are also some virtual open-world platforms like MineDOJO [41] that extend these challenges even further by simulating procedurally generated, sandbox-like environments.These platforms, inspired by the open-ended world of Minecraft, push agents to engage in tasks like resource collection, construction, and survival, demanding continuous exploration and adaptive learning.In such environments, agents are motivated to seek out new information and adapt their behavior to finish given tasks.Training in such environments can help agents learn knowledge across a broad range of tasks and terrains, enabling them to operate effectively in various outdoor environments.</p>
<p>Dynamic Environments.</p>
<p>Dynamic environments mark a significant evolution from traditional, static simulators by utilizing generative models to create flexible, real-time simulations.Unlike predefined environments that require manual adjustments, these models allow for the dynamic creation of a wide variety of scenarios, enabling agents to experience diverse, first-person perspectives.This shift provides agents with richer, more varied training experiences, improving their adaptability and generalization in complex, unpredictable real-world situations.A representative work is UniSim [236], which dynamically generates robot manipulation video sequences based on input conditions like spatial movements, textual commands, and camera parameters.Leveraging multimodal data from 3D simulations, real-world robot actions, and internet media, this system generates varied, realistic environments where agents can practice tasks like object manipulation and navigation.The key advantage of this approach is its flexibility, allowing agents to adapt to various scenarios without the limitations of static physical environments.Pandora [223] expands the dynamic environment generation from robot actions in Unisim to wider domains including human and robot actions in both indoor and outdoor scenes.Another subsequent work, AVID [168] builds on UniSim by conditioning on actions and modifying noise predictions from a pre-trained diffusion model to generate action-driven visual sequences for dynamic environment generation.Beyond the video diffusion-based framework of Unisim, EVA [23] introduces an additional vision-language model for embodied video anticipation, producing more consistent embodied video predictions.As for the generation of open-world dynamic environments, Streetscapes [32] employs autoregressive video diffusion models to simulate urban environments where agents must navigate dynamic challenges like changing weather and traffic.These environments offer consistently coherent, yet flexible, urban settings, exposing agents to real-world-like variability.The core trend in dynamic environments is the use of generative world models that provide scalable, adaptable simulations.This approach significantly reduces the manual effort required for environment setup, allowing agents to train across a diverse range of scenarios quickly.Moreover, the focus on first-person training closely mimics real-world decision-making, enhancing the agents' ability to adapt to evolving situations.These advances are key in developing embodied environments supporting agent learning in complex, dynamic scenarios.Given the above developments, it is evident that world models as embodied environments have made significant advances in simulating and predicting how agents interact with dynamic, realworld scenarios.Current research predominantly focuses on developing indoor, static environments, with notable efforts expanding to large-scale outdoor environments and dynamic simulation environments.A promising direction is to construct dynamic environments, which can provide first-person, action-conditioned future world prediction, enabling agents to better adapt to unseen conditions.These methods are promising to offer flexible, scalable environments for training embodied agents, enhancing their generalization capabilities for real-world tasks.</p>
<p>APPLICATION</p>
<p>Autonomous Driving</p>
<p>In recent years, with the rapid advancement of vision-based generative models [14,75,193] and multimodal large language models [1,122], world models have attracted growing interest in the field of autonomous driving.The modern autonomous driving pipeline is typically divided into four key components: perception, prediction, planning, and control.Among these, the perception and prediction stages correspond to driving scene understanding-i.e., learning an implicit representation of the vehicle's external environment.In parallel, recent surveys [62] highlight the emergence of end-to-end world simulators that learn to simulate realistic driving environments based on multimodal inputs-such as images, point clouds, trajectories, and language-and then generate future states to support downstream tasks like planning and decision-making.These two perspectives align well with our earlier categorization of world models, and in the following, we detail their applications and developments within the autonomous driving domain accordingly.5.1.1Learning Implicit Representations.Autonomous vehicles typically utilize cameras, radar, and lidar to perceive the real world, gathering information through images, video data, and point cloud data.In the initial decision-making paradigm [20,175], models often take perceptual data as input and directly output motion planning results for the autonomous vehicle.Conversely, when humans operate vehicles, they typically observe and predict the current and future states of other traffic participants to determine their own driving strategies [83].Thus, learning the implicit representation of the world through perceptual data and predicting the future states of the surrounding environment is a crucial step in enhancing the decision-making reliability of autonomous vehicles.We consider this process as it manifests in how autonomous vehicles learn a world model in latent space.</p>
<p>End to End Model</p>
<p>As shown in the left half of Figure 5, before the advent of multimodal large models and end-to-end autonomous driving technologies [80], the perception and prediction tasks of autonomous vehicles were typically assigned to distinct modules, each trained on their respective tasks and datasets.The perception module processed data from images, point clouds, and other sources to accomplish tasks such as object detection and map segmentation, projecting the perceived world into an abstract geometric space.Furthermore, the prediction module would typically operate within these geometric spaces to forecast the future states of the surrounding environment, including the trajectories and motions of traffic participants.</p>
<p>The processing of perceptual data is closely tied to the evolution of deep learning technologies, as shown in Table 4. Pointnet [158], introduced in 2017, was the first to employ deep learning methods for processing point cloud data.As convolutional neural networks advanced, perception techniques based on image data, exemplified by YOLOP [216] and MultiNet [198], emerged and excelled in driving scene understanding tasks [74,107,203,262].In recent years, the transformer architecture has gained prominence in natural language processing, and this technology has also been applied to image data understanding.BEVFormer [118] utilizes the attention mechanism to  [158] 2017 Lidar MLP 3D Classification MultiNet [198] 2018 Camera CNN Semantic Segmentation OmniDet [107] 2021 Camera CNN &amp; Attention Multi-task Visual Perception YOLOP [216] 2022 Camera CNN Object Detection BEVFormer [118] 2022 Camera Attention 3D Visual Perception Transfusion [8] 2022 Camera &amp; Lidar Transformer 3D Object Detection Prediction Wayformer [142] 2022 Geometric Space Attention Trajectory Prediction MTR [187] 2022 Geometric Space Transformer Trajectory Prediction QCNet [264] 2023 Geometric Space Transformer Trajectory Prediction HPTR [253] 2023 Geometric Space Transformer Trajectory Prediction Jiang et al. [92] 2023 Geometric Space Diffusion Trajectory Prediction</p>
<p>End to End Scene Understanding UniAD [80] 2023 Camera Transformer Motion Planning TOKEN [199] 2024 Camera MLLM Motion Planning OmniDrive [107] 2024 Camera MLLM Motion Planning</p>
<p>Driving World Simulation</p>
<p>Motion Simulation SUMO [127] 2000 Geometric Space Rule-based Traffic Simulation Metadrive [116] 2022 Geometric Space Data-driven Traffic Simulation Trafficbots [252] 2023 Geometric Space Transformer Traffic Simulation Waymax [63] 2024 Geometric Space Data-driven Traffic Simulation</p>
<p>End to End Sensor Simulation GAIA-1 [78] 2023 Camera Transformer Video Generation DriveDreamer [210] 2023 Camera Diffusion Video Generation Drive-WM [213] 2023 Camera Diffusion Video Generation OccWorld [260] 2023 Occupancy Attention Occupancy Generation OccSora [206] 2024 Occupancy Diffusion Occupancy Generation Vista [54] 2024 Camera Diffusion Video Generation Copilot4D [248] 2024 Lidar Diffusion Point Cloud Generation integrate images from multiple camera angles, constructing an abstract geometric space from a bird's-eye view, and achieving state-of-the-art results in various tasks, including object detection.Additionally, Transfusion [8] enhances perceptual accuracy by fusing lidar and camera data through a cross-attention approach.Building on the perceptual results, a series of techniques such as RNNs [6,98,267], CNNs [25,28,152], and Transformers [84,143,187,264] have been employed to encode historical scene information and predict the future behaviors of traffic participants.</p>
<p>With the emergence and rapid development of multimodal large language models in recent years, many efforts have sought to apply the general scene understanding capabilities of these models to the field of autonomous driving.TOKEN [199] tokenizes the whole traffic scene into object-level knowledge, using the reasoning ability of the language model to handle the long-tail prediction and planning problems, OmniDrive [107] sets up llm-based agents and covers multiple tasks including scene description, counterfactual reasoning and decision making through visual question-answering.4, before the emergence of multimodal large models and vision-based generative models, traffic scenario simulations are often conducted in geometric spaces.The scene data on which these simulations rely is typically collected by the perception modules of autonomous vehicles or constructed manually.These simulations represent future states of the scenario in the form of geometric trajectories [63,116,127,252], which require further modeling and rendering to produce outputs suitable for vehicle perception.The cascading of multiple modules often results in information loss and increases the complexity of simulations, making scenario control more challenging.Furthermore, realistic scene rendering typically requires substantial computational resources, which limits the efficiency of virtual traffic scenario generation.</p>
<p>World Simulators. As shown in Table</p>
<p>Using diffusion-based video generation models as a world model partially addresses the aforementioned issues.By training on large-scale traffic scenario datasets, diffusion models can directly generate camera perception data that closely resembles reality.Additionally, the inherent controllability of diffusion models, combined with text-image alignment methods like CLIP [161], enables users to exert control over scenario generation in a straightforward manner.The GAIA-1 [78] and DriveDreamer series [210,256] are among the first to employ this method for constructing world models.Building on this foundation, Drive-WM [213] introduces closed-loop control for planning tasks, while Vista [54] focuses on improving the resolution of generated results and extending prediction duration.In addition to methods that predict future states in video space, many other works have explored different forms of vehicle perception data.OccWorld [260] and OccSora [206] predict the future state of the world by forecasting 3D occupancy grids, whereas Copilot4D [248] constructs a world model by predicting changes in radar point cloud data.Compared to video data, these types of features better reflect the spatial characteristics of traffic scenarios.</p>
<p>Robots</p>
<p>World models have emerged as a transformative paradigm in robotics, empowering robots to perceive, predict, and act effectively in complex environments.This progress is driven in part by advances in neural architectures [75,202] and learning algorithms [162,178], which enable robots to build implicit representations that capture key aspects of the external world.Complementarily, prediction models [46,47] offer the ability to forecast future environmental states, moving beyond static abstractions to support anticipatory and adaptive behavior.Together, these capabilities make it increasingly feasible for robots to learn directly from real-world interactions.In Table 5, we summarize the core learning tasks involved in constructing world models for robotics, categorized according to the three major perspectives outlined above (typical examples shown in Figure S1).</p>
<p>Learning Implicit Representation.</p>
<p>Traditional robotic tasks (e.g., object grasping) are typically performed in highly structured environments where the critical components are explicitly modeled [38,100], eliminating the need for the robot to independently learn or adapt its understanding of the world.However, when the robot is deployed in unfamiliar environments, especially those in which key features or dynamics have not been explicitly modeled, tasks that were previously successful may fail as the robot struggles to generalize to these unknown features [95,137].Thus, enabling a robot to learn an implicit representation of its environment is a crucial first step toward achieving intelligence.</p>
<p>To help a robot understand the objects in the world, visual models such as convolutional neural networks (CNNs) [57,103,110] and vision transformers (ViT) [34,204] integrate visual characteristics of entities into representations, making it possible for robots to recognize critical objects for tasks.RoboCraft [184] transfers visual observation into particles and captures the structure of the underlying system through a graph neural network.Moreover, other attempts are made for the sensing of physical 3D space.PointNet [157,159] first encodes the unstructured 3D point clouds with asymmetrical functions, capturing the spatial characteristics of the environment.A recent work [59] assembles observations acquired along local exploratory paths into a global representation of the physical space within its latent space, enabling robots to tail and approach specific targets.SpatialLM [197] further advances this direction by processing raw 3D point clouds into structured 3D scene representations with semantic labels, enhancing spatial reasoning for complex tasks in robotics and autonomous driving.With the advancement of language comprehension in LLMs [15,35,201], a novel paradigm for enabling robots to capture task intentions involves describing the task in textual form and then obtaining a textual representation through LLMs [56,81,140,207].BC-Z [88] utilizes language representations as task representations, enhancing the multi-task performance of robots.Text2Motion [120] splits the natural language instruction into task-level and motion-level plans with LLM to handle complex sequential manipulation tasks.</p>
<p>Predicting</p>
<p>Future States of the Environment.Robotic tasks are inherently sequential and longterm, where early decisions significantly influence future outcomes [191].Effective robotic planning  [110] 1998 Image CNN ViT [34] 2020 Image Transformer RoboCraft [184] 2024 Image GNN</p>
<p>3D Representation</p>
<p>PointNet [157] 2017 3D point clouds MLP Predictive Coding [59] 2024 Image ResNet SpatialLM [197] 2025 3D point clouds MLLM Task Representation BC-Z [88] 2022 Text&amp; Video LLM&amp; ResNet Text2Motion [120] 2023 Text LLM Gensim [207] 2023 Text LLM Predicting Future Environment Video Prediction UniPi [36] 2024 Video Diffusion VIPER [39] 2024 Video Transformer GR-2 [19] 2024</p>
<p>Text &amp; Video Transformer IRASim [265] 2024 Trajectory Diffusion</p>
<p>Real-world Planning</p>
<p>Real-World Adaptation</p>
<p>DayDreamer [220] 2023 Video RSSM SWIM [134] 2023 Video Transfer Learning CoSTAR [2] 2021 Multimodal Belief Space Evaluation OpenEQA [131] 2024 Image&amp; Text LLM thus relies heavily on accurately predicting future environmental states, enabling proactive decisionmaking and reducing costly errors.Classic closed-loop algorithms [10,99], which select actions solely based on current observations, are typically short-sighted, risking irreversible mistakes.Additionally, approaches depending on explicit dynamic models crafted from expert knowledge tend to be limited in flexibility and robustness.</p>
<p>A key recent insight is the use of generative video models-particularly those leveraging diffusion [11,22,40,73] and transformer architectures [230,243]-to implicitly learn environmental dynamics directly from visual data.For instance, UniPi [36] frames action prediction explicitly as a video generation problem, conditioning a constrained diffusion model on the current state to visualize future scenarios.Similarly, VIPER [39] employs a pretrained autoregressive transformer to guide robotic actions, effectively leveraging rich representations learned from expert demonstration videos.IRASim [265] leverages diffusion models for trajectory-to-video generation tasks, starting from an initial given frame.Moreover, models like GR-2 [19,217] benefit from the vast scale of internet videos to establish robust priors, subsequently fine-tuning on specific robotics tasks to generate accurate image predictions and action trajectories.</p>
<p>Collectively, these methods demonstrate the promise of generative, vision-centric modeling as a foundation for anticipatory robotic control, significantly enhancing robots' capabilities to reason (to some extent) about future states and improve long-term task performance.</p>
<p>5.2.3</p>
<p>From Simulation to Real World.Deep reinforcement learning has demonstrated remarkable capabilities in robotics, enabling autonomous performance in complex tasks such as stable locomotion [106,192], precise object manipulation [33,244], and intricate activities like tying shoelaces [5].However, its practicality remains significantly limited by low sample efficiency.For instance, training a robot to solve a Rubik's Cube in the real world can require tens of thousands of simulated years [3].Consequently, most robot training is conducted within simulation environments, leveraging distributed training techniques to enhance efficiency [67,170].Unfortunately, due to discrepancies between simulations and real-world conditions, policies trained in simulation often fail when directly transferred to physical robots, particularly in complex or novel environments.</p>
<p>A pivotal recent insight is that world models can effectively bridge this simulation-to-reality gap by learning generalized representations of real-world dynamics.For example, NeBula [2] constructs a structured belief space that enables reasoning and rapid adaptation across diverse AI Town [148] Machine society Stylized facts S3 [51] Social network Predictions Papachristou et al. [147] Social network Stylized facts &amp; Predictions Xu et al. [228] Games Stylized facts EconAgent [114] Macroeconomics Stylized facts SRAP-Agent [91] Resource allocation Stylized facts Project Sid [4] Collective rules (tax) Stylized facts AgentSociety [153] Social life Stylized facts</p>
<p>World Model in Social Siumlacra</p>
<p>Agent-Pro [250] Games Belief Zhang et al. [247] Machine society Reflection &amp; Debate GovSim [154] Resource sharing Cognition AgentGroupChat [60] Group chat Belief &amp; Memory robot morphologies and unstructured environments.DayDreamer [220] further demonstrates the capability of generalized world models, allowing robots to directly learn locomotion in realworld environments within hours, significantly reducing the reliance on extensive simulations.Additionally, SWIM [134] highlights the power of human-video-based learning combined with minimal real-world fine-tuning, enabling task generalization with less than 30 minutes of interaction.These examples illustrate that by building robust, real-world-oriented internal representations, world models substantially narrow the gap between simulation and reality, facilitating rapid adaptation and generalization in robotics.</p>
<p>Social Simulacra</p>
<p>The concept of "social simulacra" was originally introduced as a prototyping technique in [149], aimed at helping designers create a virtual social computing system encompassing many diverse agents.The traditional methods of constructing agents based on expert-defined rules [13,176] or reinforcement learning [259] face issues such as overly simplistic behaviors or a lack of interpretability.However, the emergence of LLMs provides a transformative tool for building more realistic social simulacra, achieving more convincing stylized facts [114] or accurate predictions.Social simulacra can be seen as a form of world model that mirrors real-world social computing systems.From another perspective, the agents within social simulacra also develop implicit representations of the external system; that is, they build an implicit world model that supports the generation of their social behaviors.The relationship between the world model and social simulacra is shown in Figure S2, and the summary of representative works is shown in Table 6.</p>
<p>Building Social Simulacra Mirroring</p>
<p>Real-world Society.In the era of the rapid rise of LLM agents, building realistic social simulation systems becomes more practical.One of the most famous examples of social simulacra is AI Town [148], a world model composed of 25 generative agents, essentially forming a sandbox social environment.In this virtual community, the agents exhibit believable individual behaviors, and at the group level, emergent social behaviors similar to those that might appear in the real world.Along these lines, more and more attempts are being made to replace humans in various social scenarios with LLM agents, in effect forming their own scenariospecific social simulacra.These works have used the simulacra paradigm in such scenarios as social networks and cooperative or competitive games, among others [50].</p>
<p>Recent studies have extended social simulacra by demonstrating how LLM agents can reflect realistic social interactions and collective behaviors, effectively mirroring human societies.A critical insight is that LLM-driven agents, by modeling aspects such as emotions, attitudes, and decision-making patterns, can reproduce intricate social phenomena across multiple contexts.For example, S3 [51] leverages these human-like features to simulate realistic message propagation on social networks, successfully capturing the dynamics of real-world public events.Related studies [147] deepen this understanding by analyzing how network structures emerge organically among LLM agents, comparing these structures directly to those formed in human social networks.Beyond networks, the capacity of LLM agents to embody sophisticated strategic reasoning is highlighted in simulations of social games such as Werewolf [228], where the agent outputs exhibit patterns resembling human strategic behaviors such as deception and confrontation.Furthermore, in economic contexts, works such as EconAgent [114] reveal that collective behaviors emerging from individual economic reasoning by LLM agents can convincingly reproduce macroeconomic trends and regularities.Recently, AgentSociety [153] has further expanded this paradigm by creating a large-scale, LLM-powered societal simulation capable of modeling diverse social phenomena such as polarization and response to public policies, thus providing a versatile platform for computational social science research.These studies collectively illustrate the broad potential of social simulacra, demonstrating that agent-based world models powered by LLMs can capture diverse and realistic social, strategic, and economic interactions.</p>
<p>Agent's Understanding of External</p>
<p>World in Social Simulacra.LLM agents build their memory by storing observations obtained through interactions with the external environment [251], thereby forming implicit representations and basic cognition of the external world, especially in the context of simulating social scenarios.This cognition is stored in a memory bank in textual form for LLM agents to retrieve and use, enabling them to access useful information and fully leverage experiential knowledge from past interactions with the environment when making decisions.</p>
<p>Agent-Pro [250] transforms the memory of its interactions with the external environment (specifically with other agents in interactive tasks) into what are called 'beliefs'.Based on these beliefs, it makes the next decision and updates its behavior strategy.These beliefs represent the agent's social understanding of the environment and other agents within it, relating to Theory of Mind mentioned in Section 3.2.Other works on LLM agents have also adopted similar designs.For example, Zhang et al. [247] introduces mechanisms of reflection and debate from a social psychology view for modeling multi-agent collaboration tasks.A more advanced study is GovSim [154], which explores whether cooperative behaviors aimed at sustainable resource development can emerge within a society composed of LLM agents.In this setup, each agent gathers information about the external world and other agents' behavioral strategies through multi-agent conversations and subsequently forms its own high-level insights, essentially creating an implicit representation of the world model.Another similar application scenario is Interactive Group Chat [60], where human-like behaviors and strategies emerge across four narrative scenarios, including Inheritance Disputes, Law Court Debates, etc.</p>
<p>OPEN PROBLEMS AND FUTURE DIRECTIONS</p>
<p>The recent advance of hyper-realistic generative AI has brought a lot of attention to development of the world model, with particular focus on the multi-modal big models like Sora [146].Despite the rapid innovation, there are also a lot of important open problems that remain to be solved.</p>
<p>Physical Rules and Counterfactual Simulation</p>
<p>A key objective of world models is to capture the causal structure of their environments-especially the underlying physical rules-so they can reason about counterfactuals beyond the data distribution [151].This capacity is crucial for handling rare, mission-critical events (e.g., autonomousdriving corner cases [45]) and for narrowing sim-to-real gaps.Recent progress raises the question of whether large-scale, purely data-driven generative models can acquire such rules from raw visual data alone.While transformer-and diffusion-based video generators such as Sora [146] produce strikingly realistic sequences, studies reveal persistent physical-law failures-e.g., inaccurate gravity, fluid, or thermal dynamics [212].</p>
<p>Hybrid approaches that explicitly embed physics are emerging as promising alternatives.Genesis [7] illustrates this direction by unifying fast, photo-realistic rendering with a re-engineered universal physics core, allowing language-conditioned data generation grounded in first-principles simulation.PhysGen [124] takes a similar stance at the image-to-video level: it couples a rigid-body simulator with a diffusion refiner, enabling controllable, physically plausible motion from a single image.Complementary diagnostic work underscores why such hybrids are needed.Kang et al. [97] show that scaling diffusion video models yields perfect in-distribution fidelity yet breaks down on out-of-distribution or combinatorial tests, indicating "case-based" rather than rule-based generalization.Motamed et al. [139] reach a similar conclusion with the Physics-IQ benchmark: current video generators achieve visual realism but largely fail on tasks requiring understanding of optics, fluid dynamics, or magnetism.</p>
<p>Taken together, the evidence suggests that data-driven scaling alone is insufficient to recover robust physical laws.Integrating explicit simulators-or otherwise enforcing physical priors [186]-remains a promising path toward world models that generalize to unseen counterfactual scenarios while retaining interpretability and transparency.</p>
<p>Enriching the Social Dimension</p>
<p>Simulating the physical elements alone is not sufficient for an advanced world model, since human behavior and social interaction also play a crucial role in many important scenarios [50,58,246].For example, the behavior of urban dwellers is particularly important for building world models of the urban environment [9,226].Previous work shows that the human-like commonsense reasoning capabilities of LLMs provide a unique opportunity to simulate realistic human behavior with generative agents [148].However, designing autonomous agents that can simulate realistic and comprehensive human behavior and social interactions remains an open problem.Recent studies suggest that theories of human behavior patterns and cognitive processes can inform the design of agentic workflows, which in turn enhance the human behavior simulation capabilities of LLMs [148,182], representing an important direction for future research.In addition, the evaluation of the realism of generated human behavior still largely relies on subjective human assessment, which is challenging to scale up to a large-scale world model.Developing a reliable and scalable evaluation scheme will be another future research direction that can enrich the social dimension of the world model.</p>
<p>Bridging Simulation and Reality with Embodied Intelligence</p>
<p>The world model has long been envisioned as a critical step towards developing embodied intelligence [174].It can serve as a powerful simulator that creates comprehensive elements of the environment and models realistic relationships between them.Such an environment can facilitate embodied agents to learn through interaction with a simulated environment, reducing the need for supervision data.To achieve this goal, improving the multi-modal, multi-task, and 3D capacities of generative AI models has become an important research problem for developing general world models for embodied agents.Moreover, closing the simulation-to-reality gaps [76] has been a long-standing research problem for embodied environment simulators, and it is therefore important to transfer the trained embodied intelligence from the simulation environment to the physical world.Collecting more fine-grained sensory data is also a critical step toward this goal, which can be facilitated through the interface of embodied agents.Therefore, an interesting future research direction is to create self-reinforcing loops to harness the synergy power of generative world models and embodied agents.</p>
<p>Simulation Efficiency</p>
<p>Ensuring high simulation efficiency of world models is important for many applications.For example, number of frames per second is a key metric for high quality for learning sophisticated drone manipulating AIs.The popular transformer architecture of most big generative AIs poses a huge challenge for high-speed simulation because its autoregressive nature can only generate one token at a time.Several strategies are proposed to accelerate the inference of large generative models, such as incorporating big and small generative models [181] and distilling big models [182].More holistic solutions include building a simulation platform that optimally schedule LLM requests [232].High computation cost is also a problem for classic physics simulators when they are tasked to simulate large and complex systems.Previous research finds deep learning models like graph neural networks can be used to efficiently approximate physical systems [172].Therefore, an important research direction will be to explore the synergy between smaller deep learning models and big generative AI models.Additionally, the overall improvement from underlying hardware to programming platform and AI models is also needed to achieve substantial speedup.</p>
<p>Ethical and Safety Concerns</p>
<p>Data Privacy.The recent trend of building world models with big generative AIs raises significant concerns of privacy risk, largely due to the massive and often opaque training data [240].Extensive research effort is devoted to assessing the risk of inferring private information with big generative AIs like LLM [115], which could be especially sensitive in the context of video generation models.To be compliant with privacy regulations like GDPR [196], it is important to improve the transparency of the life cycle of generative AIs, helping the public understand how data is collected, stored, and used in these AI models.</p>
<p>Simulating Unsafe Scenario.The incredibly intelligent power of generative AIs makes safeguarding their access a paramount task.Previous studies on LLMs found they can be misled to generate unsafe content with adversarial prompting [85,105].The risk of unsafe use of world models can be even larger.Adversarial users might leverage such techniques to simulate harmful scenarios, reducing the cost of planning illegal and unethical activities.Therefore, an important future research direction is to safeguard the usage of world models.</p>
<p>Accountability.The ability to generate hyper-realistic text, images, and videos has caused severe social problems of spreading misinformation and disinformation.For example, the emergence of deepfake technology gives rise to large-scale misuses that have widespread negative effects on social, economic, and political systems [214].Thus, detecting AI-generated content has been a key research problem in addressing these risks [164].However, this problem is becoming increasingly challenging due to the advance of generative AIs, and it will be even more difficult with the arrival of a world model that can generate consistent, multi-dimensional output.Technology like watermarking could help improve the accountability of world model usage [30].More research attention, as well as legal solutions, are needed to improve the accountability of world model usage.</p>
<p>Benchmark</p>
<p>Benchmarking world models is both necessary and challenging.Because the community pursues divergent goals-learning internal representations vs. predicting future worlds-with heterogeneous technical approaches (e.g., LLM agents, video diffusion) and wide-ranging application domains (autonomous driving, robotics, social simulation), there is no single canonical task or metric.Nevertheless, several recent efforts illustrate how carefully designed testbeds can expose the specific gaps that prevent current models from becoming reliable world simulators.</p>
<p>Video-centric world simulation.Qin et al. propose WorldSimBench [160], a dual evaluation suite combining human-preference video judgments and action-level consistency across three embodied settings (open-ended sandboxes, autonomous driving, and robot manipulation).Complementarily, Duan et al. present WorldScore [37], which decomposes "world generation" into controllability, visual quality, and dynamics across 3,000 camera-specified scenes, enabling head-tohead comparisons of 3D, 4D, and video generators.</p>
<p>Physical and spatial reasoning.PhysBench [26] (10k video-image-text triplets) and UrbanVideo-Bench [254] (5.2k drone clips) demonstrate that current VLMs lack basic physics understanding and urban navigation skills.Xu et al. translate psychometric testing into AI evaluation by defining five Basic Spatial Abilities and diagnosing geometry and rotation weaknesses across 13 VLMs [227].</p>
<p>Embodied decision making.The Embodied Agent Interface (EAI) [113] standardizes four LLMbased modules-goal parsing, sub-goal generation, action sequencing, and transition modeling-and reports fine-grained errors rather than a single success rate.</p>
<p>Despite these advances, benchmarking world models remains an open challenge.Future work should focus on building more diverse and realistic benchmarks to rigorously test generalization capabilities.Additionally, standardizing evaluation protocols will be key to improving comparability and robustness assessments across environments.</p>
<p>CONCLUSION</p>
<p>Understanding the world and predicting the future have been long-standing objectives for scientists developing artificial generative intelligence, underscoring the significance of constructing world models across various domains.This paper presents the first comprehensive survey of world models that systematically explores their two primary functionalities: implicit representations and future predictions of the external world.We provide an extensive summary of existing research on these core functions, with particular emphasis on world models in decision-making, world knowledge learned by models, world models as video generation, and world models as embodied environments.Additionally, we review progress in key applications of world models, including autonomous driving, robotics, and social simulation.Finally, recognizing the unresolved challenges in this rapidly evolving field, we highlight open problems and propose promising research directions with the hope of stimulating further investigation in this burgeoning area.</p>
<p>Fig. 2 .
2
Fig. 2. Two schemes of utilizing world model in decision-making.</p>
<ol>
<li>1 . 1
11
World model in model-based RL.In decision-making, the concept of the world model largely refers to the environment model in MBRL.A decision-making problem is typically formulated as a Markov Decision Process (MDP), denoted with a tuple (, , , , ), where , ,  denotes the state space, action space and the discount factor each.The world model here consists of , the state transition dynamics and , the reward function.Since the reward function is defined in most cases, the key task of MBRL is to learn and utilize the transition dynamics, which can further support policy optimization.</li>
</ol>
<p>Fig. 4 .
4
Fig.4.Classification of world models as interactive embodied environments, including indoor, outdoor and dynamic environments.The modeling of the outside world is evolving from constructing static, current environments to predicting dynamic, future environments.</p>
<p>Fig. 5 .
5
Fig. 5. Application of world model in autonomous driving.</p>
<p>Fig. S2 .
S2
Fig. S2.World model and social simulacra.</p>
<p>Table 1 .
1
Overview of recent works in world knowledge learned by models.
CategoryMethods/ModelYear&amp;VenueModalityContentCommon Sense &amp; General Knowledge</p>
<p>Table 4 .
4
Comparison of existing works in scene understanding and world simulation.
TaskWorkYearData ModalityTechniqueTask DescriptionFaster r-cnn [165]2015CameraCNNObeject DetectionPointnetPerceptionDriving SceneUnderstanding</p>
<p>Table 5 .
5
Core learning tasks involved in constructing world models for robotics.
TaskModelYearInputBackboneVisualCNNRepresentationLearningInnerRepresentation</p>
<p>Table 6 .
6
Representative works of social simulacra.
AdvanceWhat to simulateEffects of world modelWorld Model asSocial Simulacra
ACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: June 20xx.
https://plato.stanford.edu/entries/mental-representation/ ACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: June
20xx.
A RELATED SURVEYTableS1.Comparison with existing surveys.This paper focuses on a comprehensive overview of the systematic definition and the capabilities of world models.Survey Venue and YearMain Focus Deficiency[266]Arxiv, 2024 General world model Limited to discussion on applications[130]Arxiv, 2024 Efficient multimodal models Limited to discussion on techniques[24]Arxiv, 2024 Text-to-video generation Limited scope[62]IEEE T-IV, 2024 Autonomous driving Limited scope[112]Arxiv, 2024 Autonomous driving Limited scope[231]Arxiv, 2024 Autonomous driving Limited scopeB FIGURES AND TABLES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Ali Agha, Kyohei Otsu, Benjamin Morrell, Rohan David D Fan, Angel Thakker, Sung-Kyun Santamaria-Navarro, Amanda Kim, Xianmei Bouman, Jeffrey Lei, Edlund, arXiv:2103.11470Quest for robotic autonomy in challenging environments; team costar at the darpa subterranean challenge. 2021arXiv preprint</p>
<p>Solving rubik's cube with a robot hand. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob Mcgrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, arXiv:1910.071132019arXiv preprint</p>
<p>Project sid: Many-agent simulations toward ai civilization. A L Altera, Andrew Ahn, Nic Becker, Stephanie Carroll, Nico Christie, Manuel Cortes, Arda Demirci, Melissa Du, Frankie Li, Shuying Luo, arXiv:2411.001142024arXiv preprint</p>
<p>Aloha 2: An enhanced low-cost hardware for bimanual teleoperation. Jorge Aldaco, Travis Armstrong, Robert Baruch, Jeff Bingham, Sanky Chan, Kenneth Draper, Debidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer Goodrich, arXiv:2405.022922024arXiv preprint</p>
<p>An lstm network for highway trajectory prediction. Florent AltchÃ©, Arnaud De, La Fortelle, 2017 IEEE 20th international conference on intelligent transportation systems (ITSC). IEEE2017</p>
<p>Genesis: A generative and universal physics engine for robotics and beyond. ACM Comput. Surv. 11December 2024. June 20xxArticle .. Publication date</p>
<p>Transfusion: Robust lidar-camera fusion for 3d object detection with transformers. Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, Chiew-Lan Tai, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Digital twins in city planning. Michael Batty, Nature Computational Science. 432024</p>
<p>Dynamic programming and optimal control: Volume I. Dimitri Bertsekas, Athena scientific. 42012</p>
<p>Zero-shot robotic manipulation with pretrained image-editing diffusion models. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine, arXiv:2310.106392023arXiv preprint</p>
<p>Muvo: A multimodal generative world model for autonomous driving with geometric representations. Daniel Bogdoll, Yitian Yang, Marius ZÃ¶llner, 20232311arXiv e-prints</p>
<p>Heterogeneous beliefs and routes to chaos in a simple asset pricing model. A William, Cars H Brock, Hommes, Journal of Economic dynamics and Control. 228-91998</p>
<p>Video generation models as world simulators. Tim Brooks, Bill Peebles, Connor Holmes, Will Depue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, Aditya Ramesh, 2024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Genie: Generative interactive environments. Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Forty-first International Conference on Machine Learning. 2024</p>
<p>Diffdreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models. Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, Gordon Wetzstein, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV). 2017</p>
<p>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, arXiv:2410.061582024arXiv preprint</p>
<p>Model-free deep reinforcement learning for urban autonomous driving. Jianyu Chen, Bodi Yuan, Masayoshi Tomizuka, 2019</p>
<p>Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, Maosong Sun, Legent, arXiv:2404.18243Open platform for embodied agents. 2024arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Xiaowei Chi, Hengyuan Zhang, Chun-Kai Fan, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-Min Chan, Wei Xue, Wenhan Luo, Shanghang Zhang, arXiv:2410.15461An embodied world model for future video anticipation. 2024arXiv preprint</p>
<p>Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong , Seon Hong, Chaoning Zhang, arXiv:2403.05131Sora as an agi world model? a complete survey on text-to-video generation. 2024arXiv preprint</p>
<p>Predicting motion of vulnerable road users using high-definition maps and efficient convnets. Fang-Chieh Chou, Tsung-Han Lin, Henggang Cui, Vladan Radosavljevic, Thi Nguyen, Tzu-Kuo Huang, Matthew Niedoba, Jeff Schneider, Nemanja Djuric, 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE2020</p>
<p>Physbench: Benchmarking and enhancing vision-language models for physical world understanding. Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang, arXiv:2501.164112025arXiv preprint</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in neural information processing systems. 201831</p>
<p>Multimodal trajectory predictions for autonomous driving using deep convolutional networks. Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, Nemanja Djuric, 2019 international conference on robotics and automation (icra). IEEE2019</p>
<p>A feasibility study on runway gen-2 for generating realistic style images. Yifan Cui, Xinyi Shan, Jeanhun Chung, International Journal of Internet, Broadcasting and Communication. 1612024</p>
<p>Scalable watermarking for identifying large language model outputs. Sumanth Dathathri, Abigail See, Sumedh Ghaisas, Po-Sen Huang, Rob Mcadam, Johannes Welbl, Vandana Bachani, Alex Kaskasoli, Robert Stanforth, Tatiana Matejovicova, Nature. 63480352024</p>
<p>Procthor: Large-scale embodied ai using procedural generation. Matt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, Advances in Neural Information Processing Systems. 202235</p>
<p>Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion. Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, Gordon Wetzstein, ACM SIGGRAPH 2024 Conference Papers. 2024</p>
<p>Multi-robot grasp planning for sequential assembly operations. Mehmet Dogar, Andrew Spielberg, Stuart Baker, Daniela Rus, Autonomous Robots. 432019</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, arXiv:2010.119292020arXiv preprint</p>
<p>Glam: Efficient scaling of language models with mixture-of-experts. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, International Conference on Machine Learning. PMLR2022</p>
<p>Learning universal policies via text-guided video generation. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, Pieter Abbeel, Advances in Neural Information Processing Systems. 202436</p>
<p>Worldscore: A unified evaluation benchmark for world generation. Hong-Xing Haoyi Duan, Sirui Yu, Li Chen, Jiajun Fei-Fei, Wu, arXiv:2504.009832025arXiv preprint</p>
<p>Simultaneous localization and mapping: part i. Hugh Durrant, - Whyte, Tim Bailey, IEEE robotics &amp; automation magazine. 1322006</p>
<p>Video prediction models as rewards for reinforcement learning. Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, Pieter Abbeel, Advances in Neural Information Processing Systems. 362024</p>
<p>Structure and content-guided video synthesis with diffusion models. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Advances in Neural Information Processing Systems. 202235</p>
<p>Citygpt: Empowering urban spatial cognition of large language models. Jie Feng, Yuwei Du, Tianhui Liu, Siqi Guo, Yuming Lin, Yong Li, arXiv:2406.139482024arXiv preprint</p>
<p>A survey of large language model-powered spatial intelligence across scales. Jie Feng, Jinwei Zeng, Qingyue Long, Hongyi Chen, Jie Zhao, Yanxin Xi, Zhilun Zhou, Yuan Yuan, Shengyuan Wang, Qingbin Zeng, arXiv:2504.09848Advances in embodied agents, smart cities, and earth science. 2025arXiv preprint</p>
<p>Jie Feng, Jun Zhang, Junbo Yan, Xin Zhang, Tianjian Ouyang, Tianhui Liu, Yuwei Du, Siqi Guo, Yong Li, Citybench, arXiv:2406.13945Evaluating the capabilities of large language model as world model. 2024arXiv preprint</p>
<p>Dense reinforcement learning for safety validation of autonomous vehicles. Shuo Feng, Haowei Sun, Xintao Yan, Haojie Zhu, Zhengxia Zou, Shengyin Shen, Henry X Liu, Nature. 61579532023</p>
<p>Unsupervised learning for physical interaction through video prediction. Chelsea Finn, Ian Goodfellow, Sergey Levine, Advances in neural information processing systems. 292016</p>
<p>Deep visual foresight for planning robot motion. Chelsea Finn, Sergey Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>
<p>Anyhome: Open-vocabulary generation of structured and textured 3d homes. Rao Fu, Zehao Wen, Zichen Liu, Srinath Sridhar, European Conference on Computer Vision. Springer2025</p>
<p>Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, arXiv:2007.04954A platform for interactive multi-modal physical simulation. 2020arXiv preprint</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li, Humanities and Social Sciences Communications. 1112024</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 20233arXiv preprint</p>
<p>Embodiedcity: A benchmark platform for embodied agent in real-world city environment. Chen Gao, Baining Zhao, Weichen Zhang, Jinzhu Mao, Jun Zhang, Zhiheng Zheng, Fanhang Man, Jianjie Fang, Zile Zhou, Jinqiang Cui, arXiv:2410.096042024arXiv preprint</p>
<p>Alexa arena: A user-centric interactive platform for embodied ai. Qiaozi Gao, Govind Thattai, Suhaila Shakiah, Xiaofeng Gao, Shreyas Pansare, Vasu Sharma, Gaurav Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zhang, Advances in Neural Information Processing Systems. 362024</p>
<p>Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, Hongyang Li, arXiv:2405.17398Vista: A generalizable driving world model with high fidelity and versatile controllability. 2024arXiv preprint</p>
<p>Pwm: Policy learning with multi-task world models. Ignat Georgiev, Varun Giridhar, Nicklas Hansen, Animesh Garg, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Nl2plan: Robust llm-driven planning from minimal text descriptions. Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp, arXiv:2405.042152024arXiv preprint</p>
<p>Rich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2014</p>
<p>Jiahui Gong, Jingtao Ding, Fanjin Meng, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li, arXiv:2505.17631Behavegpt: A foundation model for large-scale user behavior modeling. 2025arXiv preprint</p>
<p>Automated construction of cognitive maps with visual predictive coding. James Gornet, Matt Thomson, Nature Machine Intelligence. 672024</p>
<p>Agent group chat: An interactive group chat simulacra for better eliciting collective emergent behavior. Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, arXiv:2403.134332024arXiv preprint</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>World models for autonomous driving: An initial survey. Yanchen Guan, Haicheng Liao, Zhenning Li, Jia Hu, Runze Yuan, Yunjian Li, Guohui Zhang, Chengzhong Xu, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research. Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang, Xiangyu Chen, Advances in Neural Information Processing Systems. 202436</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Recurrent world models facilitate policy evolution. David Ha, JÃ¼rgen Schmidhuber, Advances in neural information processing systems. 201831</p>
<p>. David Ha, JÃ¼rgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Learning to walk in the real world with minimal human effort. Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, Jie Tan, arXiv:2002.085502020arXiv preprint</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.016032019arXiv preprint</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, International conference on machine learning. PMLR2019</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, arXiv:2010.021932020arXiv preprint</p>
<p>Mastering diverse control tasks through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, Nature. 2025</p>
<p>Td-mpc2: Scalable, robust world models for continuous control. Nicklas Hansen, Hao Su, Xiaolong Wang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Learning an actionable discrete diffusion policy via large-scale actionless video pre-training. Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Mgnet: A unified framework of multigrid and convolutional neural network. Juncai He, Jinchao Xu, Science China Mathematics. 627May 2019</p>
<p>Advances in neural information processing systems. Jonathan Ho, Ajay Jain, Pieter Abbeel, 202033Denoising diffusion probabilistic models</p>
<p>Sim2real in robotics and automation: Applications and challenges. Sebastian HÃ¶fer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, IEEE transactions on automation science and engineering. 1822021</p>
<p>Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado, arXiv:2309.17080Gaia-1: A generative world model for autonomous driving. 2023arXiv preprint</p>
<p>Gaia-1: A generative world model for autonomous driving. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado, 2023</p>
<p>Sequential classification-based optimization for direct policy search. Yi-Qi Hu, Hong Qian, Yang Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Planning-oriented autonomous driving. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li, 2023</p>
<p>Gensim2: Scaling robot data generation with multi-modal and reasoning llms. Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, Lirui Wang, arXiv:2410.036452024arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>A survey on trajectory-prediction methods for autonomous driving. Yanjun Huang, Jiatong Du, Ziru Yang, Zewei Zhou, Lin Zhang, Hong Chen, IEEE Transactions on Intelligent Vehicles. 732022</p>
<p>Multi-modal motion prediction with transformer-based neural network for autonomous driving. Zhiyu Huang, Xiaoyu Mo, Chen Lv, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, arXiv:2312.06674Llama guard: Llm-based input-output safeguard for human-ai conversations. 2023arXiv preprint</p>
<p>Elements of world knowledge (ewok): A cognition-inspired framework for evaluating basic world knowledge in language models. Anna A Ivanova, Aalok Sathe, Benjamin Lipkin, Unnathi Kumar, Setayesh Radkani, Thomas H Clark, Carina Kauf, Jennifer Hu, Gabriel Pramod, Grand, arXiv:2405.096052024arXiv preprint</p>
<p>Peekaboo: Interactive video generation via maskeddiffusion. Yash Jain, Anshul Nasery, Vibhav Vineet, Harkirat Behl, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in neural information processing systems. 201932</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in neural information processing systems. 202134</p>
<p>Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin, arXiv:2410.14152Srap-agent: Simulating and optimizing scarce resource allocation policy with llm-based agent. 2024arXiv preprint</p>
<p>Motiondiffuser: Controllable multi-agent motion prediction using diffusion. Max Chiyu, Andre Jiang, Cheolho Cornman, Ben Park, Yin Sapp, Dragomir Zhou, Anguelov, 2023</p>
<p>Emergent representations of program semantics in language models trained on programs. Charles Jin, Martin Rinard, Forty-first International Conference on Machine Learning. 2024</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Philip Nicholas, Johnson-Laird , 1983Harvard University Press</p>
<p>Uncertainty-aware reinforcement learning for collision avoidance. Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, Sergey Levine, arXiv:1702.011822017arXiv preprint</p>
<p>Position: Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, Anil B Murthy, Forty-first International Conference on Machine Learning. 2024</p>
<p>How far is video generation from world model: A physical law perspective. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng, arXiv:2411.023852024arXiv preprint</p>
<p>Multimodal trajectory predictions for urban environments using geometric relationships between a vehicle and lanes. Atsushi Kawasaki, Akihito Seki, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>A unified approach for motion and force control of robot manipulators: The operational space formulation. Oussama Khatib, IEEE Journal on Robotics and Automation. 311987</p>
<p>A survey on learning-based robotic grasping. Kilian Kleeberger, Richard Bormann, Werner Kraus, Marco F Huber, Current Robotics Reports. 12020</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.054742017arXiv preprint</p>
<p>Model predictive control. Basil Kouvaritakis, Mark Cannon, 2016Springer International Publishing38Switzerland</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. 252012</p>
<p>Next-generation ai creative studio. Kuaishou, Kling Ai, 2025</p>
<p>Certifying llm safety against adversarial prompting. Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, Hima Lakkaraju, arXiv:2309.027052023arXiv preprint</p>
<p>Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik, arXiv:2107.04034Rma: Rapid motor adaptation for legged robots. 2021arXiv preprint</p>
<p>Omnidet: Surround view cameras based multi-task visual perception network for autonomous driving. Ravi Varun, Senthil Kumar, Hazem Yogamani, Ganesh Rashed, Christian Sistu, Isabelle Witt, Stefan Leang, Patrick Milz, MÃ¤der, 2023</p>
<p>Thanard Kurutach, Ignasi Clavera, Yan Duan, arXiv:1802.10592Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. 2018arXiv preprint</p>
<p>A path towards autonomous machine intelligence version 0. Yann Lecun, Open Review. 912022</p>
<p>Gradient-based learning applied to document recognition. Yann Lecun, LÃ©on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 86111998</p>
<p>Do vision and language models share concepts? a vector space alignment study. Jiaang Li, Yova Kementchedjhieva, Constanza Fierro, Anders SÃ¸gaard, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Data-centric evolution in autonomous driving: A comprehensive survey of big data system, data mining, and closed-loop technologies. Lincan Li, Wei Shao, Wei Dong, Yijun Tian, Qiming Zhang, Kaixiang Yang, Wenjie Zhang, arXiv:2401.128882024arXiv preprint</p>
<p>Embodied agent interface: Benchmarking llms for embodied decision making. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, Advances in Neural Information Processing Systems. 202437</p>
<p>Econagent: large language model-empowered agents for simulating macroeconomic activities. Nian Li, Chen Gao, Mingyu Li, Yong Li, Qingmin Liao, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan Hendrycks, Zhangyang Wang, arXiv:2408.12787Llm-pbe: Assessing data privacy in large language models. 2024arXiv preprint</p>
<p>Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, Bolei Zhou, 202245</p>
<p>The geometry of concepts: Sparse autoencoder feature structure. Yuxiao Li, Eric J Michaud, David D Baek, Joshua Engels, Xiaoqing Sun, Max Tegmark, 2024</p>
<p>Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, Jifeng Dai, arXiv:2203.172702022arXiv preprint</p>
<p>Learning to model the world with language. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan, 2024</p>
<p>Text2motion: From natural language instructions to feasible plans. Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, Autonomous Robots. 4782023</p>
<p>World model on million-length video and language with ringattention. Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel, arXiv:2402.082682024arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>Physgen: Rigid-body physics-grounded imageto-video generation. Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, Shenlong Wang, European Conference on Computer Vision. Springer2024</p>
<p>Reason for future, act for now: A principled framework for autonomous llm agents with provable sample efficiency. Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang, 2024</p>
<p>Discuss before moving: Visual language navigation via multiexpert discussions. Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Microscopic traffic simulation using sumo. Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang FlÃ¶tterÃ¶d, Robert Hilbrich, Leonhard LÃ¼cken, Johannes Rummel, Peter Wagner, Evamarie WieÃŸner, The 21st IEEE International Conference on Intelligent Transportation Systems. IEEE2018</p>
<p>A survey on model-based reinforcement learning. Fan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, Yang Yu, Science China Information Sciences. 6721211012024</p>
<p>Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, Tengyu Ma, arXiv:1807.038582018arXiv preprint</p>
<p>From efficient multimodal models to world models: A survey. Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang, arXiv:2407.001182024arXiv preprint</p>
<p>Embodied question answering in the era of foundation models. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Large language models are geographically biased. Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, Stefano Ermon, arXiv:2402.026802024arXiv preprint</p>
<p>Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, Stefano Ermon, Geollm, arXiv:2310.06213Extracting geospatial knowledge from large language models. 2023arXiv preprint</p>
<p>Russell Mendonca, Shikhar Bahl, Deepak Pathak, arXiv:2308.10901Structured world models from human videos. 2023arXiv preprint</p>
<p>Uniworld: Autonomous driving pre-training via world models. Chen Min, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai, arXiv:2308.072342023arXiv preprint</p>
<p>A framework for representing knowledge. Marvin Minsky, 1974</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Joost Thomas M Moerland, Aske Broekens, Catholijn M Plaat, Jonker, arXiv:1805.09613A0c: Alpha zero in continuous action space. 2018arXiv preprint</p>
<p>Do generative video models learn physical principles from watching videos?. Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, Robert Geirhos, arXiv:2501.090382025arXiv preprint</p>
<p>Clarifygpt: Empowering llm-based code generation with intention clarification. Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, Chenxue Wang, Shichao Liu, Qing Wang, arXiv:2310.109962023arXiv preprint</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, Sergey Levine, IEEE international conference on robotics and automation (ICRA). 2018. 2018IEEE</p>
<p>Wayformer: Motion forecasting via simple &amp; efficient attention networks. Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, Benjamin Sapp, 2022</p>
<p>Scene transformer: A unified multi-task model for behavior prediction and planning. Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang, Lewis Hao-Tien, Jeffrey Chiang, Rebecca Ling, Alex Roelofs, Chenxi Bewley, Ashish Liu, Venugopal, arXiv:2106.0841722021arXiv preprint</p>
<p>Advances in neural information processing systems. Junhyuk Oh, Satinder Singh, Honglak Lee, 201730Value prediction network</p>
<p>. OpenAI. Introducing chatgpt. 2025</p>
<p>Openai, Sora, Creating video from text. 2024. 2025</p>
<p>Marios Papachristou, Yuan Yuan, arXiv:2402.10659Network formation and dynamics among multi-llms. 2024arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Avlen: Audio-visual-language embodied navigation in 3d environments. Sudipta Paul, Amit Roy-Chowdhury, Anoop Cherian, Advances in Neural Information Processing Systems. 202235</p>
<p>Causal inference in statistics: An overview. Judea Pearl, 2009</p>
<p>Covernet: Multimodal behavior prediction using trajectory sets. Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton, Oscar Beijbom, Eric M Wolff, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Agentsociety: Large-scale simulation of llm-driven generative agents advances understanding of human behaviors and society. Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing , Yi Wang, Di Zhou, arXiv:2502.086912025arXiv preprint</p>
<p>Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard SchÃ¶lkopf, arXiv:2404.16698Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainability behaviors in a society of llm agents. 2024arXiv preprint</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 141978</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Charles R Qi, Li Yi, Hao Su, Leonidas J Guibas, 2017</p>
<p>Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, Bernard Ghanem, Advances in neural information processing systems. 352022</p>
<p>Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, arXiv:2410.18072Towards video generation models as world simulators. 2024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024</p>
<p>A game theoretic framework for model based reinforcement learning. Aravind Rajeswaran, Igor Mordatch, Vikash Kumar, International conference on machine learning. PMLR2020</p>
<p>Deepfake detection: A systematic literature review. Md Shohel, Rana , Mohammad Nur Nobi, Beddhu Murali, Andrew H Sung, IEEE access. 102022</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in Neural Information Processing Systems. C Cortes, N Lawrence, D Lee, M Sugiyama, R Garnett, Curran Associates, Inc201528</p>
<p>Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, Wenhu Chen, arXiv:2402.04324Consisti2v: Enhancing visual consistency for image-to-video generation. 2024arXiv preprint</p>
<p>Jonathan Richens, David Abel, Alexis Bellot, Tom Everitt, arXiv:2506.01622General agents need world models. 2025arXiv preprint</p>
<p>Avid: Adapting video diffusion models to world models. Marc Rigter, Tarun Gupta, Agrin Hilmkil, Chao Ma, arXiv:2410.128222024arXiv preprint</p>
<p>Jonathan Roberts, Timo LÃ¼ddecke, Sowmen Das, Kai Han, Samuel Albanie, Gpt4geo, arXiv:2306.00020How a language model sees the world's geography. 2023arXiv preprint</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. Nikita Rudin, David Hoeller, Philipp Reist, Marco Hutter, Conference on Robot Learning. PMLR2022</p>
<p>Mastering memory tasks with world models. Reza Mohammad, Artem Samsami, Janarthanan Zholus, Sarath Rajendran, Chandar, arXiv:2403.042532024arXiv preprint</p>
<p>Learning to simulate complex physics with graph networks. Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter Battaglia, International conference on machine learning. PMLR2020</p>
<p>Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi, arXiv:2210.13312Neural theory-of-mind? on the limits of social intelligence in large lms. 2022arXiv preprint</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Driving in dense traffic with model-free reinforcement learning. Mauria Dhruv, Sangjae Saxena, Alireza Bae, Kikuo Nakhaei, Maxim Fujimura, Likhachev, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEEMay 2020</p>
<p>Dynamic models of segregation. C Thomas, Schelling, Journal of mathematical sociology. 121971</p>
<p>Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller, Jost Tobias Springenberg, Arunkumar Byravan, arXiv:2305.10912Leonard Hasenclever, and Nicolas Heess. A generalist dynamics model for control. 2023arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Gnm: A general navigation model to drive any robot. Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, Sergey Levine, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Urbanworld: An urban world model for 3d city generation. Yu Shang, Jiansheng Chen, Hangyu Fan, Jingtao Ding, Jie Feng, Yong Li, arXiv:2407.119652024arXiv preprint</p>
<p>Defint: A default-interventionist framework for efficient reasoning with hybrid large language models. Yu Shang, Yu Li, Fengli Xu, Yong Li, arXiv:2402.025632024arXiv preprint</p>
<p>Beyond imitation: Generating human mobility from context-aware reasoning with large language models. Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng Wang, Yong Li, arXiv:2402.098362024arXiv preprint</p>
<p>igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. Bokui Shen, Fei Xia, Chengshu Li, Roberto MartÃ­n-MartÃ­n, Linxi Fan, Guanzhi Wang, Claudia PÃ©rez-D'arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Robocraft: Learning to see, simulate, and shape elasto-plastic objects in 3d with graph networks. Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, Jiajun Wu, The International Journal of Robotics Research. 4342024</p>
<p>Haojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin, Layla Isik, Yen-Ling Kuo, Tianmin Shu, arXiv:2408.12574Muma-tom: Multi-modal multi-agent theory of mind. 2024arXiv preprint</p>
<p>Learning symbolic models for graph-structured physical mechanism. Hongzhi Shi, Jingtao Ding, Yufan Cao, Li Liu, Yong Li, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Motion transformer with global intention localization and local movement refinement. Shaoshuai Shi, Li Jiang, Dengxin Dai, Bernt Schiele, Advances in Neural Information Processing Systems. 202235</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Structured control for autonomous robots. G Reid, Simmons, IEEE transactions on robotics and automation. 1011994</p>
<p>A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. Laura Smith, Ilya Kostrikov, Sergey Levine, arXiv:2208.078602022arXiv preprint</p>
<p>Yang Song, Jascha Sohl-Dickstein, Abhishek Diederik P Kingma, Stefano Kumar, Ben Ermon, Poole, arXiv:2011.13456Score-based generative modeling through stochastic differential equations. 2020arXiv preprint</p>
<p>Testing theory of mind in large language models and humans. Dalila James Wa Strachan, Giulia Albergo, Oriana Borghini, Eugenio Pansardi, Saurabh Scaliti, Krati Gupta, Alessandro Saxena, Stefano Rufo, Guido Panzeri, Manzi, Nature Human Behaviour. 2024</p>
<p>Llms achieve adult human performance on higher-order theory of mind tasks. Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael Mckibben, Tatenda Kanyere, Alison Lentz, Robin Im Dunbar, arXiv:2405.188702024arXiv preprint</p>
<p>Design principles for the general data protection regulation (gdpr): A formal concept analysis and its evaluation. Damian A Tamburri, Information Systems. 911014692020</p>
<p>Spatiallm: Large language model for spatial understanding. Manycore Research, Team , 2025</p>
<p>Multinet: Real-time joint semantic reasoning for autonomous driving. Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, Raquel Urtasun, 2018</p>
<p>Tokenize the world into object-level knowledge to address long-tail events in autonomous driving. Ran Tian, Boyi Li, Xinshuo Weng, Yuxiao Chen, Edward Schmerling, Yue Wang, Boris Ivanovic, Marco Pavone, 2024</p>
<p>Cognitive maps in rats and men. Edward C Tolman, Psychological review. 5541891948</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Hybridnets: End-to-end perception network. Dat Vu, Bao Ngo, Hung Phan, 2022</p>
<p>Repvit: Revisiting mobile cnn from vit perspective. Ao Wang, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, arXiv:2407.10943Dream general robots in a city at scale. 2024arXiv preprint</p>
<p>Occsora: 4d occupancy generation models as world simulators for autonomous driving. Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, Jiwen Lu, arXiv:2405.203372024arXiv preprint</p>
<p>Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang, Gensim, arXiv:2310.01361Generating robotic simulation tasks via large language models. 2023arXiv preprint</p>
<p>Exploring model-based planning with policy networks. Tingwu Wang, Jimmy Ba, arXiv:1906.086492019arXiv preprint</p>
<p>Drivedreamer: Towards realworld-driven world models for autonomous driving. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, Jiwen Lu, arXiv:2309.097772023arXiv preprint</p>
<p>Drivedreamer: Towards realworld-driven world models for autonomous driving. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, Jiwen Lu, 2023</p>
<p>Worlddreamer: Towards general world models for video generation via predicting masked tokens. Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, Jiwen Lu, arXiv:2401.099852024arXiv preprint</p>
<p>Yi Ru, Wang , Jiafei Duan, Dieter Fox, Siddhartha Srinivasa, arXiv:2310.07018Are large language models capable of physical reasoning?. Newton2023arXiv preprint</p>
<p>Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, Zhaoxiang Zhang, 2023</p>
<p>The emergence of deepfake technology: A review. Mika Westerlund, Technology innovation management review. 9112019</p>
<p>Think twice: Perspective-taking improves large language models' theory-of-mind capabilities. Alex Wilf, Shawn Sihyun, Paul Pu Lee, Louis-Philippe Liang, Morency, 2023</p>
<p>Yolop: You only look once for panoptic driving perception. Dong Wu, Man-Wen Liao, Wei-Tian Zhang, Xing-Gang Wang, Xiang Bai, Wen-Qing Cheng, Wen-Yu Liu, Machine Intelligence Research. 196November 2022</p>
<p>Unleashing large-scale video generative pre-training for visual robot manipulation. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong, arXiv:2312.131392023arXiv preprint</p>
<p>Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long, arXiv:2405.15223ivideogpt: Interactive videogpts are scalable world models. 2024arXiv preprint</p>
<p>Coke: A cognitive knowledge graph for machine theory of mind. Jincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand Sabour, Helen Meng, Minlie Huang, arXiv:2305.053902024arXiv preprint</p>
<p>Daydreamer: World models for physical robot learning. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, Ken Goldberg, Conference on robot learning. PMLR2023</p>
<p>Metaurban: A simulation platform for embodied ai in urban spaces. Wayne Wu, Honglin He, Yiran Wang, Chenda Duan, Jack He, Zhizheng Liu, Quanyi Li, Bolei Zhou, arXiv:2407.087252024arXiv preprint</p>
<p>Sapien: A simulated part-based interactive environment. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, arXiv:2406.09455Towards general world model with natural language actions and video states. 2024arXiv preprint</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, Advances in neural information processing systems. 202436</p>
<p>Towards large reasoning models: A survey of reinforced reasoning with large language models. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, arXiv:2501.096862025arXiv preprint</p>
<p>Urban generative intelligence (ugi): A foundational platform for agents in embodied city environment. Fengli Xu, Jun Zhang, Chen Gao, Jie Feng, Yong Li, arXiv:2312.118132023arXiv preprint</p>
<p>Defining and evaluating visual language models' basic spatial abilities: A perspective from psychometrics. Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li, arXiv:2502.118592025arXiv preprint</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582023arXiv preprint</p>
<p>Temporally consistent transformers for video generation. Wilson Yan, Danijar Hafner, Stephen James, Pieter Abbeel, International Conference on Machine Learning. PMLR2023</p>
<p>Videogpt: Video generation using vq-vae and transformers. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas, arXiv:2104.101572021arXiv preprint</p>
<p>Forging vision foundation models for autonomous driving: Challenges, methodologies, and opportunities. Haiming Xu Yan, Yingjie Zhang, Jingming Cai, Weichao Guo, Bin Qiu, Kaiqiang Gao, Yue Zhou, Huan Zhao, Jiantao Jin, Gao, arXiv:2401.080452024arXiv preprint</p>
<p>Opencity: A scalable platform to simulate urban activities with massive llm agents. Yuwei Yan, Qingbin Zeng, Zhiheng Zheng, Jingzhe Yuan, Jie Feng, Jun Zhang, Fengli Xu, Yong Li, arXiv:2410.212862024arXiv preprint</p>
<p>Worldgpt: a sora-inspired video ai agent as rich world models from text and image inputs. Deshun Yang, Luhui Hu, Yu Tian, Zihao Li, Chris Kelly, Bang Yang, Cindy Yang, Yuexian Zou, arXiv:2403.079442024arXiv preprint</p>
<p>Probabilistic adaptation of text-to-video models. Mengjiao Yang, Yilun Du, Bo Dai, Dale Schuurmans, Joshua B Tenenbaum, Pieter Abbeel, arXiv:2306.018722023arXiv preprint</p>
<p>Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>Learning interactive real-world simulators. Sherry Yang, Yilun Du, Seyed Kamyar, Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, Pieter Abbeel, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, arXiv:2402.17139Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. 2024arXiv preprint</p>
<p>Language guided generation of 3d embodied ai environments. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli Vanderbilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Rila: Reflective and imaginative language agent for zero-shot semantic audio-visual navigation. Zeyuan Yang, Jiageng Liu, Peihao Chen, Anoop Cherian, Tim K Marks, Jonathan Le Roux, Chuang Gan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, Yue Zhang, A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing. 2024100211</p>
<p>Nuwa-xl: Diffusion over diffusion for extremely long video generation. Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, arXiv:2303.123462023arXiv preprint</p>
<p>Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, arXiv:2306.09296Carefully benchmarking world knowledge of large language models. 2023arXiv preprint</p>
<p>Magvit: Masked generative video transformer. Lijun Yu, Yong Cheng, Kihyuk Sohn, JosÃ© Lezama, Han Zhang, Huiwen Chang, Ming-Hsuan Alexander G Hauptmann, Yuan Yang, Irfan Hao, Essa, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Se-resunet: A novel robotic grasp detection method. Sheng Yu, Di-Hua Zhai, Yuanqing Xia, Haoran Wu, Jun Liao, IEEE Robotics and Automation Letters. 722022</p>
<p>Derivative-free optimization via classification. Yang Yu, Hong Qian, Yi-Qi Hu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201630</p>
<p>Learning the complexity of urban mobility with deep generative network. Yuan Yuan, Jingtao Ding, Depeng Jin, Yong Li, PNAS nexus. 45812025</p>
<p>Exploring collaboration mechanisms for llm agents: A social psychology view. Jintian Zhang, Xin Xu, Shumin Deng, arXiv:2310.021242023arXiv preprint</p>
<p>Copilot4d: Learning unsupervised world models for autonomous driving via discrete diffusion. Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel Urtasun, 2024</p>
<p>Physdreamer: Physics-based interaction with 3d objects via video generation. Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, William T Freeman, European Conference on Computer Vision. Springer2025</p>
<p>Agent-pro: Learning to evolve via policy-level reflection and optimization. Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu, arXiv:2402.175742024arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Trafficbots: Towards world models for autonomous driving simulation and motion prediction. Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Real-time motion prediction via heterogeneous polyline transformer with relative pose encoding. Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, Luc Van Gool, 2023</p>
<p>Urbanvideo-bench: Benchmarking vision-language models on embodied intelligence with video data in urban spaces. Baining Zhao, Jianjie Fang, Zichao Dai, Ziyou Wang, Jirong Zha, Weichen Zhang, Chen Gao, Yue Wang, Jinqiang Cui, Xinlei Chen, arXiv:2503.061572025arXiv preprint</p>
<p>Over-nav: Elevating iterative vision-and-language navigation with open-vocabulary detection and structured representation. Ganlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang, 2024</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 202436</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C Parkes, Richard Socher, Science advances. 81826072022</p>
<p>Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, Jiwen Lu, Occworld, arXiv:2311.16038Learning a 3d occupancy world model for autonomous driving. 2023arXiv preprint</p>
<p>Learning a 3d occupancy world model for autonomous driving. Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, Jiwen Lu, Occworld, European Conference on Computer Vision. Springer2025</p>
<p>Matrixvt: Efficient multi-camera to bev transformation for 3d perception. Hongyu Zhou, Zheng Ge, Zeming Li, Xiangyu Zhang, 2022</p>
<p>Robodreamer: Learning compositional world models for robot imagination. Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, Chuang Gan, arXiv:2404.123772024arXiv preprint</p>
<p>Query-centric trajectory prediction. Zikang Zhou, Jianping Wang, Yung-Hui Li, Yu-Kai Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, Tao Kong, Irasim, arXiv:2406.14540Learning interactive real-robot action simulators. 2024arXiv preprint</p>
<p>Is sora a world simulator? a comprehensive survey on general world models and beyond. Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, arXiv:2405.035202024arXiv preprint</p>
<p>Naturalistic driver intention and path prediction using recurrent neural networks. Alex Zyner, Stewart Worrall, Eduardo Nebot, IEEE transactions on intelligent transportation systems. 2142019. June 20xxArticle .. Publication date</p>            </div>
        </div>

    </div>
</body>
</html>
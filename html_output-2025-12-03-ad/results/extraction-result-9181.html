<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9181 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9181</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9181</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-277781050</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.10415v2.pdf" target="_blank">LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9181.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9181.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model used in this work as an automated symbolic/evidence evaluator, a novelty-checker, and as a generator of synthetic terms and natural-language problem specifications across multiple scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI family model (closed-source). The paper uses GPT-4o both as a discovery backbone in some experiments and — critically — as an automated evaluator and novelty-checker; the authors do not provide model parameter counts or training data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>chemistry, biology, physics, material science (benchmark-wide use across LSR-Synth and LSR-Transform problems)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation/evaluation roles: (1) evaluate symbolic equivalence between predicted hypotheses and ground-truth equations (LLM-based symbolic accuracy evaluator), (2) act as a novelty/evaluation oracle to screen synthetic terms and tasks, and (3) generate synthetic candidate terms and natural-language problem descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>As evaluator: agreement with human symbolic-equivalence judgments (binary equivalence decision). Downstream discovery performance measured with Symbolic Accuracy (SA), Accuracy-to-tolerance Acc0.1, and NMSE.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>GPT-4o used as the symbolic-equivalence evaluator agreed with two human evaluators on 123/130 randomly sampled problems = 94.6% agreement (reported validation). As a discovery backbone/participant, systems using GPT-4o variants contributed to best-performing runs (overall best symbolic accuracy reported across systems ≈ 31.5% on LSR-Transform problems), though the paper separates the evaluator role (GPT-4o) from backbone-conditioned discovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Evaluator/backbone performance affected by: dataset type (LSR-Transform vs LSR-Synth), problem representation (unfamiliar mathematical representations cause fragility), memorization vs genuine discovery (memorized textbook equations inflate apparent performance), LLM backbone and architecture, prompt design (prompts used for novelty/equivalence judgments), access to data (data-blind direct prompting performed poorly), task complexity (expression-tree node count), and OOD generalization demands.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human evaluators (two authors) used for validation of evaluator decisions — GPT-4o showed 94.6% agreement with humans on symbolic equivalence; compared to non-LLM symbolic-regression (PySR), LLM-based workflows (which used GPT-4o for some roles) achieved higher symbolic accuracy but PySR was often competitive or superior on purely numeric accuracy (Acc0.1) in several domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4o evaluator has nonzero disagreement with humans (~5.4% of sampled cases); LLM-based generation/evaluation can be brittle to unfamiliar representations (LSR-Transform), may enable memorization/retrieval rather than data-driven discovery on textbook-like problems, and its evaluator role does not replace numeric/ODE solvers for generating datapoints — it assesses symbolic equivalence. The model's judgments can be sensitive to prompt phrasing and representation format.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use LLM-based symbolic evaluators but validate them with human checks; evaluate discovered equations on OOD data to assess generalization; combine LLM evaluators with numerical solvers and domain experts; mitigate memorization by using transformed and synthetic problems (LSR-Transform and LSR-Synth).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9181.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9181.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (OpenAI variant used as a backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source OpenAI model variant used as an LLM backbone for hypothesis generation and refinement in the benchmarked equation-discovery methods; reported to yield the best symbolic-accuracy results among tested backbones in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary, smaller/faster variant of GPT-4o used as a backbone LLM in several discovery pipelines in the experiments; the paper does not disclose parameter counts or training data.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>chemistry, biology, physics, material science (used as the generative backbone for discovery tasks across LSR-Synth and LSR-Transform)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based hypothesis generation and iterative refinement (acting as a text-driven 'simulator' of candidate scientific equations), proposing equation skeletons and natural-language reasoning that the discovery agents turn into symbolic hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream discovery performance measured by Symbolic Accuracy (SA, model-based GPT-4o evaluator), Accuracy-to-tolerance Acc0.1, and NMSE (in-domain and out-of-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>The paper reports that systems using GPT-4o-mini produced the best symbolic accuracy in some settings — the best-performing system across the benchmark reaches only ~31.5% symbolic accuracy (LSR-Transform). Exact backbone-specific numeric breakdowns are not provided per-backbone in tabular form, but the text explicitly states GPT-4o-mini outperforms GPT-3.5-turbo and performs comparably or better than Llama-3.1-8B in symbolic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Backbone choice (GPT-4o-mini vs GPT-3.5-turbo vs Llama), access to data (data-guided refinement improves results vs data-blind prompting), problem type (LSR-Transform easier than LSR-Synth in some numeric metrics but still challenging), OOD test difficulty, expression representation, prompt templates and hyperparameters (1k LLM calls budget standardized), and interplay with the outer search algorithm (LLM-SR, LaSR, SGA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against other LLM backbones (Llama-3.1-8B-Instruct and GPT-3.5-turbo) within the same discovery pipelines — GPT-4o-mini variants consistently outperformed GPT-3.5-turbo on symbolic accuracy; compared to non-LLM baseline PySR, LLM-based pipelines with GPT-4o-mini achieved higher symbolic accuracy but PySR sometimes had superior numeric accuracy (Acc0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite being the best backbone in these experiments, GPT-4o-mini-driven discovery still yields low absolute symbolic-accuracy (~31.5% best), struggles with OOD generalization in many LSR-Synth problems, and is vulnerable to memorization when benchmarks include textbook forms. Performance remains highly problem-dependent and sensitive to how equations are represented.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Select stronger backbones and combine them with data-driven refinement, enforce OOD evaluation, include transformed/synthetic benchmarks to reduce memorization, and integrate domain tools (e.g., numerical solvers, literature retrieval) to improve meaningful discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9181.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9181.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct (Open-source instruct-tuned Llama-3 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruct-tuned LLM (8B parameters) used both as a backbone for discovery experiments and operationally to filter out transformed problems solvable by direct sampling (i.e., used to identify cases not requiring data).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruct-tuned Llama-3 variant with ~8 billion parameters (as named in the paper). Used in experiments as one of three backbone LLMs; also used to detect problems solvable by direct sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>benchmark-wide: physics-derived (Feynman/LSR-Transform) and LSR-Synth domains (chemistry, biology, physics, material science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based generator and analyzer of candidate equations (hypothesis generation/refinement); additionally used to attempt direct-sampling solutions to transformed equations to identify and exclude trivial problems (i.e., a text-based check for whether an LLM can 'solve' a transformed problem without data).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream discovery metrics: Symbolic Accuracy (SA via GPT-4o evaluator), Acc0.1, NMSE; also used operationally as a filter (no numeric accuracy reported for the filter beyond its use).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No direct numeric accuracy for the Llama filter is reported; the paper reports that methods using Llama-3.1-8B often outperform GPT-3.5-turbo in discovery pipelines and that Llama was used to exclude transformed problems that it could solve through direct sampling (operational filter).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and architectural differences (8B open model vs larger closed models), prompt engineering, whether data is available for refinement, complexity and representation of transformed equations, and domain-specific problem difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared within experiments to GPT-4o-mini and GPT-3.5-turbo; Llama-based runs tended to outperform GPT-3.5-turbo and sometimes rival GPT-4o-mini in certain tasks. Compared to PySR (non-LLM baseline), Llama-based LLM methods had higher symbolic accuracy but PySR could be competitive in numeric accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller parameter count relative to closed models can limit reasoning on more complex transformations; direct-sampling capabilities may incorrectly lead to exclusion of some legitimately hard problems if the LLM happens to recite memorized forms; still vulnerable to representation fragility and memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Open-source instruct models can be effective; use them in combination with data-driven refinement and robust filters to avoid trivial memorized solutions; validate filters and exclusion criteria carefully to avoid excluding meaningful discovery cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9181.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9181.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used OpenAI model used here as one of the LLM backbones for hypothesis generation and iterative discovery; reported to underperform GPT-4o-mini and Llama-3.1-8B in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A proprietary OpenAI LLM (earlier generation than GPT-4 family) used as a backbone in several discovery pipelines; paper does not list parameter counts or training data.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>chemistry, biology, physics, material science (used as a backbone across LLM-SRBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based hypothesis generation and refinement (acting as a 'simulator' of candidate symbolic laws), participating in iterative search algorithms (LLM-SR, LaSR, SGA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Symbolic Accuracy (SA by GPT-4o evaluator), Acc0.1, NMSE (in-domain and OOD).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported qualitatively as lower than GPT-4o-mini and Llama-3.1-8B across discovery pipelines; the paper states that systems using GPT-3.5-turbo performed worse overall but does not provide a per-backbone numeric breakdown in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model reasoning capability relative to newer/specialized backbones, susceptibility to memorization vs genuine data-driven discovery, prompt templates, and the specific discovery algorithm used (LLM-SR, LaSR, SGA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4o-mini and Llama-3.1-8B within the same discovery frameworks — GPT-3.5-turbo underperformed those alternatives. Compared to non-LLM baseline PySR, GPT-3.5-driven methods similarly show the LLM advantage in symbolic plausibility but not necessarily in numeric fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower reasoning/representation capability relative to more recent backbones in this benchmark; contributes to lower symbolic accuracy and weaker OOD generalization in tested pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Prefer stronger/backbone models for symbolic-discovery tasks, combine LLM strengths with numerical parameter optimization and OOD evaluation, and use synthetic/transformational benchmarks to avoid inflated memorization-driven results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery <em>(Rating: 2)</em></li>
                <li>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>In-context Symbolic Regression <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific equation discovery via programming with large language models <em>(Rating: 2)</em></li>
                <li>Autoturb: Using large language models for automatic algebraic model discovery of turbulence closure <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9181",
    "paper_id": "paper-277781050",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A closed-source large language model used in this work as an automated symbolic/evidence evaluator, a novelty-checker, and as a generator of synthetic terms and natural-language problem specifications across multiple scientific domains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary OpenAI family model (closed-source). The paper uses GPT-4o both as a discovery backbone in some experiments and — critically — as an automated evaluator and novelty-checker; the authors do not provide model parameter counts or training data in this paper.",
            "scientific_subdomain": "chemistry, biology, physics, material science (benchmark-wide use across LSR-Synth and LSR-Transform problems)",
            "simulation_task": "Text-based simulation/evaluation roles: (1) evaluate symbolic equivalence between predicted hypotheses and ground-truth equations (LLM-based symbolic accuracy evaluator), (2) act as a novelty/evaluation oracle to screen synthetic terms and tasks, and (3) generate synthetic candidate terms and natural-language problem descriptions.",
            "evaluation_metric": "As evaluator: agreement with human symbolic-equivalence judgments (binary equivalence decision). Downstream discovery performance measured with Symbolic Accuracy (SA), Accuracy-to-tolerance Acc0.1, and NMSE.",
            "simulation_accuracy": "GPT-4o used as the symbolic-equivalence evaluator agreed with two human evaluators on 123/130 randomly sampled problems = 94.6% agreement (reported validation). As a discovery backbone/participant, systems using GPT-4o variants contributed to best-performing runs (overall best symbolic accuracy reported across systems ≈ 31.5% on LSR-Transform problems), though the paper separates the evaluator role (GPT-4o) from backbone-conditioned discovery performance.",
            "factors_affecting_accuracy": "Evaluator/backbone performance affected by: dataset type (LSR-Transform vs LSR-Synth), problem representation (unfamiliar mathematical representations cause fragility), memorization vs genuine discovery (memorized textbook equations inflate apparent performance), LLM backbone and architecture, prompt design (prompts used for novelty/equivalence judgments), access to data (data-blind direct prompting performed poorly), task complexity (expression-tree node count), and OOD generalization demands.",
            "comparison_baseline": "Human evaluators (two authors) used for validation of evaluator decisions — GPT-4o showed 94.6% agreement with humans on symbolic equivalence; compared to non-LLM symbolic-regression (PySR), LLM-based workflows (which used GPT-4o for some roles) achieved higher symbolic accuracy but PySR was often competitive or superior on purely numeric accuracy (Acc0.1) in several domains.",
            "limitations_or_failure_cases": "GPT-4o evaluator has nonzero disagreement with humans (~5.4% of sampled cases); LLM-based generation/evaluation can be brittle to unfamiliar representations (LSR-Transform), may enable memorization/retrieval rather than data-driven discovery on textbook-like problems, and its evaluator role does not replace numeric/ODE solvers for generating datapoints — it assesses symbolic equivalence. The model's judgments can be sensitive to prompt phrasing and representation format.",
            "author_recommendations_or_insights": "Use LLM-based symbolic evaluators but validate them with human checks; evaluate discovered equations on OOD data to assess generalization; combine LLM evaluators with numerical solvers and domain experts; mitigate memorization by using transformed and synthetic problems (LSR-Transform and LSR-Synth).",
            "uuid": "e9181.0",
            "source_info": {
                "paper_title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini (OpenAI variant used as a backbone)",
            "brief_description": "A closed-source OpenAI model variant used as an LLM backbone for hypothesis generation and refinement in the benchmarked equation-discovery methods; reported to yield the best symbolic-accuracy results among tested backbones in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "Proprietary, smaller/faster variant of GPT-4o used as a backbone LLM in several discovery pipelines in the experiments; the paper does not disclose parameter counts or training data.",
            "scientific_subdomain": "chemistry, biology, physics, material science (used as the generative backbone for discovery tasks across LSR-Synth and LSR-Transform)",
            "simulation_task": "Text-based hypothesis generation and iterative refinement (acting as a text-driven 'simulator' of candidate scientific equations), proposing equation skeletons and natural-language reasoning that the discovery agents turn into symbolic hypotheses.",
            "evaluation_metric": "Downstream discovery performance measured by Symbolic Accuracy (SA, model-based GPT-4o evaluator), Accuracy-to-tolerance Acc0.1, and NMSE (in-domain and out-of-domain).",
            "simulation_accuracy": "The paper reports that systems using GPT-4o-mini produced the best symbolic accuracy in some settings — the best-performing system across the benchmark reaches only ~31.5% symbolic accuracy (LSR-Transform). Exact backbone-specific numeric breakdowns are not provided per-backbone in tabular form, but the text explicitly states GPT-4o-mini outperforms GPT-3.5-turbo and performs comparably or better than Llama-3.1-8B in symbolic accuracy.",
            "factors_affecting_accuracy": "Backbone choice (GPT-4o-mini vs GPT-3.5-turbo vs Llama), access to data (data-guided refinement improves results vs data-blind prompting), problem type (LSR-Transform easier than LSR-Synth in some numeric metrics but still challenging), OOD test difficulty, expression representation, prompt templates and hyperparameters (1k LLM calls budget standardized), and interplay with the outer search algorithm (LLM-SR, LaSR, SGA).",
            "comparison_baseline": "Compared against other LLM backbones (Llama-3.1-8B-Instruct and GPT-3.5-turbo) within the same discovery pipelines — GPT-4o-mini variants consistently outperformed GPT-3.5-turbo on symbolic accuracy; compared to non-LLM baseline PySR, LLM-based pipelines with GPT-4o-mini achieved higher symbolic accuracy but PySR sometimes had superior numeric accuracy (Acc0.1).",
            "limitations_or_failure_cases": "Despite being the best backbone in these experiments, GPT-4o-mini-driven discovery still yields low absolute symbolic-accuracy (~31.5% best), struggles with OOD generalization in many LSR-Synth problems, and is vulnerable to memorization when benchmarks include textbook forms. Performance remains highly problem-dependent and sensitive to how equations are represented.",
            "author_recommendations_or_insights": "Select stronger backbones and combine them with data-driven refinement, enforce OOD evaluation, include transformed/synthetic benchmarks to reduce memorization, and integrate domain tools (e.g., numerical solvers, literature retrieval) to improve meaningful discovery.",
            "uuid": "e9181.1",
            "source_info": {
                "paper_title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-3.1-8B-Instruct",
            "name_full": "Llama-3.1-8B-Instruct (Open-source instruct-tuned Llama-3 variant)",
            "brief_description": "An open-source instruct-tuned LLM (8B parameters) used both as a backbone for discovery experiments and operationally to filter out transformed problems solvable by direct sampling (i.e., used to identify cases not requiring data).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_description": "Open-source instruct-tuned Llama-3 variant with ~8 billion parameters (as named in the paper). Used in experiments as one of three backbone LLMs; also used to detect problems solvable by direct sampling.",
            "scientific_subdomain": "benchmark-wide: physics-derived (Feynman/LSR-Transform) and LSR-Synth domains (chemistry, biology, physics, material science)",
            "simulation_task": "Text-based generator and analyzer of candidate equations (hypothesis generation/refinement); additionally used to attempt direct-sampling solutions to transformed equations to identify and exclude trivial problems (i.e., a text-based check for whether an LLM can 'solve' a transformed problem without data).",
            "evaluation_metric": "Downstream discovery metrics: Symbolic Accuracy (SA via GPT-4o evaluator), Acc0.1, NMSE; also used operationally as a filter (no numeric accuracy reported for the filter beyond its use).",
            "simulation_accuracy": "No direct numeric accuracy for the Llama filter is reported; the paper reports that methods using Llama-3.1-8B often outperform GPT-3.5-turbo in discovery pipelines and that Llama was used to exclude transformed problems that it could solve through direct sampling (operational filter).",
            "factors_affecting_accuracy": "Model size and architectural differences (8B open model vs larger closed models), prompt engineering, whether data is available for refinement, complexity and representation of transformed equations, and domain-specific problem difficulty.",
            "comparison_baseline": "Compared within experiments to GPT-4o-mini and GPT-3.5-turbo; Llama-based runs tended to outperform GPT-3.5-turbo and sometimes rival GPT-4o-mini in certain tasks. Compared to PySR (non-LLM baseline), Llama-based LLM methods had higher symbolic accuracy but PySR could be competitive in numeric accuracy.",
            "limitations_or_failure_cases": "Smaller parameter count relative to closed models can limit reasoning on more complex transformations; direct-sampling capabilities may incorrectly lead to exclusion of some legitimately hard problems if the LLM happens to recite memorized forms; still vulnerable to representation fragility and memorization.",
            "author_recommendations_or_insights": "Open-source instruct models can be effective; use them in combination with data-driven refinement and robust filters to avoid trivial memorized solutions; validate filters and exclusion criteria carefully to avoid excluding meaningful discovery cases.",
            "uuid": "e9181.2",
            "source_info": {
                "paper_title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A widely used OpenAI model used here as one of the LLM backbones for hypothesis generation and iterative discovery; reported to underperform GPT-4o-mini and Llama-3.1-8B in these experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "A proprietary OpenAI LLM (earlier generation than GPT-4 family) used as a backbone in several discovery pipelines; paper does not list parameter counts or training data.",
            "scientific_subdomain": "chemistry, biology, physics, material science (used as a backbone across LLM-SRBench tasks)",
            "simulation_task": "Text-based hypothesis generation and refinement (acting as a 'simulator' of candidate symbolic laws), participating in iterative search algorithms (LLM-SR, LaSR, SGA).",
            "evaluation_metric": "Symbolic Accuracy (SA by GPT-4o evaluator), Acc0.1, NMSE (in-domain and OOD).",
            "simulation_accuracy": "Reported qualitatively as lower than GPT-4o-mini and Llama-3.1-8B across discovery pipelines; the paper states that systems using GPT-3.5-turbo performed worse overall but does not provide a per-backbone numeric breakdown in the main text.",
            "factors_affecting_accuracy": "Model reasoning capability relative to newer/specialized backbones, susceptibility to memorization vs genuine data-driven discovery, prompt templates, and the specific discovery algorithm used (LLM-SR, LaSR, SGA).",
            "comparison_baseline": "Compared against GPT-4o-mini and Llama-3.1-8B within the same discovery frameworks — GPT-3.5-turbo underperformed those alternatives. Compared to non-LLM baseline PySR, GPT-3.5-driven methods similarly show the LLM advantage in symbolic plausibility but not necessarily in numeric fidelity.",
            "limitations_or_failure_cases": "Lower reasoning/representation capability relative to more recent backbones in this benchmark; contributes to lower symbolic accuracy and weaker OOD generalization in tested pipelines.",
            "author_recommendations_or_insights": "Prefer stronger/backbone models for symbolic-discovery tasks, combine LLM strengths with numerical parameter optimization and OOD evaluation, and use synthetic/transformational benchmarks to avoid inflated memorization-driven results.",
            "uuid": "e9181.3",
            "source_info": {
                "paper_title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery",
            "rating": 2,
            "sanitized_title": "llm_and_simulation_as_bilevel_optimizers_a_new_paradigm_to_advance_physical_scientific_discovery"
        },
        {
            "paper_title": "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 2,
            "sanitized_title": "scienceagentbench_toward_rigorous_assessment_of_language_agents_for_datadriven_scientific_discovery"
        },
        {
            "paper_title": "In-context Symbolic Regression",
            "rating": 2,
            "sanitized_title": "incontext_symbolic_regression"
        },
        {
            "paper_title": "LLM-SR: Scientific equation discovery via programming with large language models",
            "rating": 2,
            "sanitized_title": "llmsr_scientific_equation_discovery_via_programming_with_large_language_models"
        },
        {
            "paper_title": "Autoturb: Using large language models for automatic algebraic model discovery of turbulence closure",
            "rating": 1,
            "sanitized_title": "autoturb_using_large_language_models_for_automatic_algebraic_model_discovery_of_turbulence_closure"
        }
    ],
    "cost": 0.017776499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models
7 Jun 2025</p>
<p>Parshin Shojaee <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#97;&#114;&#115;&#104;&#105;&#110;&#115;&#104;&#111;&#106;&#97;&#101;&#101;&#64;&#118;&#116;&#46;&#101;&#100;&#117;">&#112;&#97;&#114;&#115;&#104;&#105;&#110;&#115;&#104;&#111;&#106;&#97;&#101;&#101;&#64;&#118;&#116;&#46;&#101;&#100;&#117;</a>. 
Ngoc-Hieu Nguyen 
Kazem Meidani 
Carnegie Mellon University</p>
<p>Amir Barati Farimani 
Carnegie Mellon University</p>
<p>Khoa D Doan 
Chandan K Reddy </p>
<p>1 Virginia Tech 2 VinUniversity</p>
<p>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models
7 Jun 20254017970BB46AA6A101A3FA5462B31EE8arXiv:2504.10415v2[cs.CL]
Scientific equation discovery has long been a cornerstone of scientific progress, enabling the derivation of laws governing natural phenomena.Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation.However, it is difficult to assess the true discovery capabilities of these methods because existing benchmarks often use well-known equations.This makes them vulnerable to memorization by LLMs and results in inflated performance metrics that do not reflect genuine discovery.In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization.Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring datadriven reasoning.Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the bestperforming system so far achieves only 31.5% symbolic accuracy.These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.</p>
<p>Introduction</p>
<p>Equation discovery, the process of uncovering symbolic mathematical expressions from observational data, has been a cornerstone of scientific advancement.This task, also known as symbolic regression (SR), goes beyond mere datadriven predictive modeling by seeking interpretable mathematical relations that reveal the underlying mechanisms of natural phenomena.When scientists derive mathematical equations from empirical data, they gain more than just predictive power -they obtain insights into fundamental physical principles, enable extrapolation beyond observed data, and facilitate knowledge transfer across scientific domains (Langley, 1981;Schmidt &amp; Lipson, 2009).</p>
<p>Standard approaches to equation discovery have primarily relied on genetic programming (GP) and evolutionary algorithms (Cranmer, 2023;La Cava et al., 2021), which represent mathematical expressions as trees and navigate the vast space of possible equations through evolutionary search techniques.However, these methods face two fundamental challenges.First, the NP-hard nature of equation discovery (Virgolin &amp; Pissis, 2022) makes their random mutation and crossover operations computationally prohibitive across</p>
<p>Goal / Instruction -Discover the mathematical equation/law that describes [output variable] based on given [input features].</p>
<p>-Use domain-specific knowledge of [the scientific field] and provided data samples to find an equation that is scientifically valid and fits the data well.The benchmark tasks (left) combine scientific context with numerical data.The discovery process (middle) iteratively leverages LLM's scientific knowledge and data-driven reasoning to generate hypotheses for underlying equations.Discovered hypotheses, represented as equation strings, trees, or programs, are then evaluated (right) using multiple metrics including data fidelity, symbolic accuracy, and computational efficiency.</p>
<p>Data</p>
<p>vast search spaces.Second, unlike human scientists who leverage their domain knowledge and expertise to guide hypothesis formation, these approaches are mostly purely data-driven, and isolated from existing scientific knowledge.These limitations have motivated researchers to develop methods that incorporate scientific domain knowledge into the equation discovery process.</p>
<p>Large Language Models (LLMs) have recently emerged as a promising solution to these challenges, offering a new paradigm for scientific equation discovery.LLMs, trained on vast corpora of scientific literature, possess extensive embedded scientific knowledge.This has sparked significant interest in leveraging LLMs for scientific equation discovery, with several recent works demonstrating their potential (Shojaee et al., 2024b;Ma et al., 2024;Grayeli et al., 2024;Merler et al., 2024;Du et al., 2024;Reddy &amp; Shojaee, 2024;Zhang et al., 2024).These LLM-based approaches have shown to enhance the equation hypothesis generation process by incorporating scientific priors, guiding the exploration of equation search spaces more efficiently, and providing interpretable reasoning for the search process.</p>
<p>Despite the promising potential of LLM-based equation discovery methods, their rigorous and robust evaluation still remains an open challenge.The current scientific equation discovery benchmarks are primarily represented by SRBench (La Cava et al., 2021) and SRSD (Matsubara et al., 2022).SRBench incorporates two key data groups for this purpose: the Feynman physics equations (Udrescu &amp; Tegmark, 2020), and Strogatz dynamical systems (La Cava et al., 2016;Strogatz, 2018).A notable extension to this framework is SRSD (Matsubara et al., 2022), which enhances the Feynman benchmark by incorporating physically meaningful sampling ranges for data points.However, these benchmarks exhibit significant limitations for the evaluation of LLM-based methods.Their problems are mostly based on known physics equations from textbooks, which makes them often subject to memorization by LLMs.</p>
<p>As noted by (Shojaee et al., 2024b), LLMs frequently succeed on these common equation discovery benchmarks through simple recitation based on variable names and problem descriptions, rather than the actual process of datadriven discovery and reasoning.Our analysis (shown in Fig. 1) also confirms this finding -the sudden drop in the numeric error curve within the first few iterations and significantly lower symbolic error on Feynman problems indicate memorized solutions rather than a meaningful search towards discovery.To mitigate this issue, (Shojaee et al., 2024b;Ma et al., 2024) have introduced a handful of five custom-crafted problems designed to prevent memorization by manually modifying known physical models.While these efforts represent a step forward, the small scale and limited diversity of these problem sets are insufficient to provide a comprehensive evaluation framework for emerging LLM-based methods in scientific equation discovery.</p>
<p>A more robust and systematic benchmark is needed to enable standardized evaluation and foster the development of innovative methods in this emerging field.</p>
<p>In this paper, we introduce LLM-SRBench, a new benchmark designed to rigorously evaluate the capabilities of LLM-based scientific equation discovery methods.LLM-SRBench addresses the limitations of existing benchmarks by constructing problem sets that avoid trivial recitation while leveraging the scientific priors embedded in LLMs, simulating conditions akin to scientific discovery.The benchmark is structured around two main categories of problems, each targeting distinct aspects of equation discovery.The first category focuses on transforming common scientific problems, such as those from the Feynman equations, into different mathematical representations of the same underlying physical problem.By symbolically altering input-output mappings and generating less common mathematical forms for the same problem, we challenge LLM-based equation discovery to go beyond memorization of the common forms.This approach is motivated by recent findings on the fragility of LLMs' reasoning capabilities to unfamiliar representations of otherwise familiar problems (Mirzadeh et al., 2024;Xie et al., 2024;Wu et al., 2023).</p>
<p>The second category extends the approach introduced by (Shojaee et al., 2024b), which combines known terms in the underlying equation with synthetic, novel terms to create problems that go beyond memorization and demand datadriven reasoning.We expand this idea into a comprehensive set of benchmark problems spanning diverse scientific domains.These problems incorporate carefully designed synthetic terms that are both novel and plausible.We further verify the solvability of the generated equations using numerical solvers, ensuring that the benchmark problems remain grounded in physical feasibility while presenting meaningful challenges for LLM-based discovery methods.• We introduce LLM-SRBench, the first comprehensive benchmark with 239 challenging problems across various scientific domains, designed to evaluate LLM-based scientific equation discovery methods.</p>
<p>• We propose a novel benchmark design through alternative mathematical representations (LSR-Transform) and synthetic, discovery-driven problems (LSR-Synth) to ensure rigorous evaluation of scientific reasoning and discovery capabilities beyond LLM memorization.</p>
<p>• Extensive experiments on state-of-the-art methods reveal performance peaks at 31%, highlighting the benchmark's challenging nature and its potential for future research.</p>
<p>LLM-SRBench</p>
<p>We introduce LLM-SRBench, a novel benchmark designed to evaluate LLM-based methods for data-driven scientific equation discovery.As shown in Fig. 2, in this benchmark, a "data-driven scientific equation discovery" task is defined as follows: Given a task dataset D, the corresponding scientific context C, the objective is to derive a hypothesis h that represents the underlying mathematical relations behind the data with high precision and scientific plausibility.This process resembles the iterative search and refinement undertaken by human scientists, where LLMs act as optimizers, proposing and refining hypotheses based on both scientific knowledge and empirical data.</p>
<p>LSR-Transform</p>
<p>This category is designed to evaluate whether LLM-based methods can discover equations in less common mathematical forms, avoiding reliance on memorization of well-known representations.This approach is motivated by the observation that LLMs often struggle with unfamiliar instantiations of otherwise familiar problems, as highlighted by recent studies on the fragility of LLM reasoning (Mirzadeh et al., 2024;Xie et al., 2024;Wu et al., 2023).By transforming existing benchmark problems into different mathematical representations, we challenge LLMs' capabilities in datadriven scientific equation discovery and reasoning.</p>
<p>We build on the Feynman (Udrescu &amp; Tegmark, 2020) benchmark (current standard benchmark in scientific equation discovery), which consists of 100 physics equations, and systematically transform these equations into alternative mathematical forms (examples in App.A.1).As demonstrated in Fig. 3(a), the transformation process involves seven key steps: 1) Equation Collection: We gather the original mathematical expressions, along with their input and output variables, and scientific problem descriptions from the Feynman benchmark.2) Select Pivot Variable:</p>
<p>For each equation, we choose an input feature to become the new target variable.3) Feature-Target Transformation: We transform the dataset by switching the roles of the selected input feature and the original target variable.4) Symbolic Transformation: Using the SymPy library in Python on the parsed expressions, we solve each equation with respect to the selected input variable, treating it as the new output and the original output variable as an input in the transformed equation.5) Solvability Check:</p>
<p>We retain only those transformations that are analytically solvable, ensuring the feasibility of the resulting equations.6) Dataset Refinement: For the transformed equations with altered data domains (e.g., due to square roots or denominators), we filter the original Feynman dataset to ensure all data points fall within the valid domains of the new equations.7) Problem Reformulation: Using LLM (GPT-4o), we generate a new natural language specification for each transformed problem.During this data generation process, we constrain the transformed equations' complexity (measured by expression tree node count) to the range of original Feynman dataset distribution (full analysis in Fig. 8, App.A.1).This allows us to focus on the semantic aspects of discovery-specifically the interplay between reasoning and memorization of the mathematical forms-rather than conflating performance with the ability to handle syntactically complex and lengthy hypotheses.We also exclude transformed problems that LLM can solve through direct sampling without requiring access to data.</p>
<p>This process yields 111 total transformed equations derived from the 100 original Feynman problems.Each transformed equation shares the same scientific context, problem description, and variables as its original counterpart but presents a less common mathematical form to be discovered.The goal of LSR-Transform is not to discover new equations but to evaluate whether LLM-based systems can validate discoveries from non-trivial, data-driven transformations of known equations.To support scientific knowledge-guided discovery, each task in LSR-Transform is supplemented with a natural language description of the scientific problem and dataset, including variable names and their meanings.These descriptions are absent in the original Feynman benchmark but they are needed for LLM-based scientific equation discovery methods to provide scientific context in prompts for knowledge-guided equation discovery by LLMs.</p>
<p>LSR-Synth</p>
<p>This category is designed to assess whether LLMs can discover equations that incorporate new synthetic terms alongside known terms, requiring scientific as well as data-driven reasoning rather than reliance on memorization.The LSR-Synth dataset is motivated by the approach introduced in (Shojaee et al., 2024b) for the handful of manually designed problems and systematically expands it into a comprehensive set of benchmark problems across diverse scientific domains.By combining known terms with synthetic, novel terms, LLMs are challenged to demonstrate discovery capabilities in unobserved contexts, yet leverage their knowledge in the process.The LSR-Synth dataset spans four scientific domains: chemistry, biology, physics, and material science, focusing on key scientific problems, including reaction kinetics in chemistry, population growth in biology, damped harmonic oscillators in physics, and stress-strain relationships in material science (examples in App.A.2).</p>
<p>The data generation process for LSR-Synth involves multi-ple steps , as illustrated in Fig. 3(b), to ensure the creation of high-quality, challenging benchmark problems: 1) Select Scientific Problem: We select problems from different scientific domains, such as reaction kinetics in chemistry or population dynamics in biology.2) Known Term Generation: Given the problem description, we prompt an LLM (GPT-4o) to generate a list of common and well-known mathematical terms that typically appear in the underlying models.3) Synthetic Term Generation: Similarly, we prompt the LLM to generate a list of diverse novel synthetic terms for a given scientific problem, along with descriptions of the problem and variables.For example, in chemistry reaction kinetics, known terms for reaction rate (dA/dt) based on concentration (A) and time (t) might include first-order (−kA) and second-order kinetics (−kA 2 ) or the exponential decay term −k exp (−k s t), while synthetic terms could represent non-linear high-order saturation, e.g., kA 2 /(1 + βA 4 ), or non-linear quantum tunneling effects, e.g., kA exp (− γ t )/t 2 .4) Solvability Check: After sampling from the generated known and synthetic terms and combining them into a complete mathematical expression, we verify the solvability of these expressions using numerical solvers such as solve ivp in Python.This step ensures that the expressions are feasible, providing a basis for generating datapoints.5) Novelty Check: In the context of each scientific problem and the complete expression, we evaluate the novelty of the new generated task using LLM (GPT-4o) as a novelty evaluator.This step is to verify that the synthetic terms are novel in the provided context and require data-driven reasoning rather than relying on established knowledge to be discovered.6) Datapoint Generation: For expressions that pass the solvability and novelty checks, we generate datapoints using numerical solvers based on the specified initial conditions and parameters.These datapoints are used to create the final task datasets.7) Expert Validation: Finally, the filtered expressions, along with visualizations of their generated datapoints, are cross-checked by two subject matter experts to validate their plausibility.After these filtering steps, we finalize a candidate list of 128 problems across the four domains (36: chemistry; 24: biology; 43: physics; and 25: material science).More detailed analysis of LLM-SRBench datasets are provided in App. A.</p>
<p>Evaluation</p>
<p>Evaluating LLM-based scientific equation discovery methods introduces unique challenges due to the open-ended nature of the task and diverse symbolic representation of hypotheses.A discovered equation can be assessed from two perspectives: (a) data fidelity, which measures how well the equation fits the observed and out-of-domain (OOD) data, and (b) symbolic accuracy, which evaluates the alignment with ground-truth symbolic equation hypotheses.Both perspectives are critical, as equations may exhibit similar symbolic forms but differ numerically, or vice versa.</p>
<p>Data Fidelity.We evaluate data-driven fidelity using two known metrics in equation discovery: (1) Acccuracy to tolerance τ (Acc τ ) (Kamienny et al., 2022;Biggio et al., 2021), and Normalized Mean Squared Error (NMSE).These metrics are computed on both in-domain test data and OOD data (when available) to assess generalization capacity, a crucial requirement for scientific equations.
Acc τ = 1 max 1≤i≤Ntest ŷi − y i y i ≤ τ , NMSE = Ntest i=1 (ŷ i − y i ) 2
Ntest i=1 (y i − ȳ) 2 Symbolic Accuracy.We evaluate symbolic accuracy with a model-based evaluation strategy using GPT-4o as an evaluator (prompt in App.B, Fig. 11).This approach addresses the limitations of current symbolic metrics like recovery rate in symbolic regression (La Cava et al., 2016), which are very sensitive to exact symbolic matches and fail to account for mathematical equivalence, particularly in different hypothesis representations (e.g., equation as strings, expression trees, or Python programs).Here, GPT-4o evaluates mathematical equivalence by comparing the symbolic form of the predicted hypothesis versus the ground-truth equation after removing parameters and constants.The ability of LLMs to recognize semantic equivalence across different representations makes them particularly well-suited for evaluating LLM-based equation discovery methods, which often operate within a more diverse and open-ended hypothesis space.To validate this metric, two authors also independently evaluated symbolic equivalence on 130 sampled problems, finding 94.6% agreement between GPT-4o and human evaluators.App.B provides more details on the evaluation metrics.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>We benchmark state-of-the-art LLM-based scientific equation discovery methods using three LLM backbones: one open-source model (Llama-3.1-8B-Instruct)and two proprietary models (GPT-4o-mini and GPT-3.5-turbo).Each discovery task takes as input the problem description, variables, the corresponding dataset, and an instruction specifying the task.The discovery methods then generate and refine equation hypotheses through LLMs.To ensure fair comparison, we standardize each of the methods to use 1k LLM calls per problem while maintaining their core algorithmic designs and hyperparameter settings.Detailed implementation specifics and prompts of each method are provided in App. C. We  evaluate the following discovery methods:
(%)↑ Acc0.1(%)↑ NMSE↓ SA (%)↑ Acc0.1(%)↑ NMSE↓ SA (%)↑ Acc0.1(%)↑ NMSE↓ SA (%)↑ Acc0.1(%)↑ NMSE↓ SA (%)↑ Acc0.1(%)↑ NMSE↓ Direct Prompting (DataBlind) Llama-3.1-8B-Instruct 3.
LLM-SR (Shojaee et al., 2024b), a program search equation discovery method that generates hypotheses of equation skeleton as Python functions with the main idea of combining LLMs' scientific knowledge with multi-island evolutionary search guided by feedback from data.</p>
<p>LaSR (Grayeli et al., 2024), a concept learning equation discovery method that finds abstract textual concepts of mathematical relations from successful equation hypotheses with LLMs and uses these concepts to evolve new hypotheses through a hybrid approach of evolutionary search (with PySR (Cranmer, 2023)) and LLM-guided search.</p>
<p>SGA (Ma et al., 2024), a bilevel optimization equation discovery method that iteratively combines LLMs for discrete hypothesis generation of scientific laws and physical simulations in PyTorch for continuous parameter optimization with respect to data.</p>
<p>Direct Prompting (DataBlind) serves as a baseline for generating hypotheses purely from contextual information without access to data.By not using data-driven reasoning and refinement in the hypothesis generation, this baseline helps to assess LLMs' memorization of the problem.</p>
<p>Main Results</p>
<p>Our experimental results (Table 1) reveals several key insights into the strengths and limitations of LLM-based scientific equation discovery methods.Overall, performance remains relatively low across both symbolic and numeric metrics, underscoring the fundamental challenges of this task.One key observation is the poor performance of direct prompting method (DataBlind), which only relies on LLMs' knowledge about the problem without access to data for data-driven refinement.This result underscores the necessity of combining LLM reasoning with observational data, as relying solely on prior knowledge proves insufficient for accurate equation discovery across different problems in LLM-SRBench.We observe that on LSR-Transform data group, LaSR achieves the highest numerical accuracy, leading in both Acc 0.1 and NMSE, while LLM-SR with GPT-</p>
<p>Chemistry</p>
<p>Biology Material Science Physics 4o-mini outperforms other methods in symbolic accuracy (∼31%).This comparative advantage inverts in the LSR-Synth material science problems, where LaSR consistently yields better symbolic accuracy and LLM-SR achieves better numerical precision, suggesting that different equation discovery strategies may be better suited to different problems.</p>
<p>Another notable observation is the consistent outperformance of models using GPT-4o-mini and Llama-3.1-8Bcompared to those based on GPT-3.5-turbo.This may be due to improved reasoning architectures or better effectiveness of smaller, less opinionated models in the search and exploration needed for navigating space of possible equations.The lower performance on LSR-Synth compared to LSR-Transform tasks also indicates that the ability to find transformed variants of known problems does not necessarily extend to more challenging scenarios involving novel synthetic terms, where systematic data-driven exploration becomes essential.</p>
<p>Analysis</p>
<p>LSR-Transform vs. Feynman datasets.We analyze the performance gap between Feynman and LSR-Transform datasets across different equation complexity levels, measured by the number of nodes in the corresponding expression tree (La Cava et al., 2021).Fig. 4 shows the aggregated average performance (over all methods and LLM backbones) in terms of both symbolic accuracy (a) and numeric precision (b).It can be observed that even at the same complexity levels, LSR-Transform problems are substantially more challenging for current discovery methods than original Feynman problems.Also, this performance disparity persists even for simpler problems ([0-15] nodes), indicating that the challenging nature of LSR-Transform problems for LLM-based scientific equation discovery methods is not necessarily due to the structural complexity.</p>
<p>Performance on In-domain vs. OOD.Generalization to unseen data is a fundamental requirement for scientific laws and a critical aspect of equation discovery.A correct mathematical model of observations should not only fit observed data but also extrapolate accurately to out-of-domain (OOD) scenarios.However, current equation discovery benchmarks largely overlook this aspect.In this work, we advocate for explicit OOD assessment in scientific equation discovery by introducing held-out OOD test sets in our benchmark.</p>
<p>To systematically evaluate generalization beyond observed data, we generate dedicated OOD test sets for synthetic problems in the LSR-Synth category (see App.A for details on data generation).Fig. 5 provides a comparative analysis of ID vs. OOD results.As expected, all discovery methods exhibit higher NMSE in OOD settings, indicating degraded generalization compared to in-domain data.Among the evaluated methods, LLM-SR achieves the lowest NMSE across both ID and OOD settings, while direct prompting performs the worst.Also, we observe some domain-specific variations in generalization performance: the performance gap between ID and OOD is more pronounced in chemistry and biology problems compared to physics and material science, although the complexity of problems are designed to be similar, as shown in Fig. 10.This suggests that different scientific problems may pose distinct challenges for equation discovery methods, highlighting the need for future research to develop more robust approaches for different scientific disciplines.</p>
<p>OOD generalization and symbolic accuracy.We further analyzed the correlation between our proposed symbolic accuracy metric (Sec.2.3) and data-driven extrapolation performance in OOD settings (averaged over all LSR-Synth domains).As shown in Fig. 6, symbolic accuracy exhibits a strong positive correlation with numerical precision (Acc 0.1 ) on OOD data and a corresponding negative correlation with numerical error (NMSE).This strong correlation observed between symbolic and OOD performance provides two key insights: First, it establishes OOD evaluation as a powerful approach for assessing the discovery of generalizable equations-an aspect often underutilized in symbolic regression research; second, it validates our LLM-based symbolic evaluation approach through its strong alignment with numeric generalization performance.</p>
<p>More detailed experimental results, including both qualitative analyses of discovered equations and quantitative performance comparisons across scientific equation discovery methods and LLMs, are provided in App.D.</p>
<p>Related Work</p>
<p>AI for Scientific Discovery.Recent advancements in AI for science highlight the ability of LLMs to generate scientific hypotheses by leveraging their extensive knowledge and reasoning capabilities (Lu et al., 2024;Ji et al., 2024;Reddy &amp; Shojaee, 2024).LLM agents, when augmented with external tools and scientific simulators, have shown promise in automated scientific data-driven analysis (Majumder et al., 2024a).While recent benchmarks have been developed to evaluate LLMs and agents in hypothesis generation and scientific question answering (Majumder et al., 2024b;Chen et al., 2024), evaluation for equation discovery and symbolic regression-one of the core tasks in scientific discovery-remains yet unexplored.</p>
<p>Symbolic Regression.Symbolic regression approaches fall into three main categories: search-based methods that explore equation spaces via evolutionary algorithms or reinforcement learning (Schmidt &amp; Lipson, 2009;Cranmer, 2023;Petersen et al., 2021;Sun et al., 2023), learning-based methods leveraging pre-trained Transformers on synthetic data (Biggio et al., 2021;Kamienny et al., 2022), and hybrid approaches that guide search using neural priors (Landajuela et al., 2022;Shojaee et al., 2024a;Mundhenk et al., 2021;Meidani et al., 2023).While these methods have advanced the field of automated symbolic function discovery from data, they mostly lack mechanisms to incorporate scientific domain knowledge into the discovery process.</p>
<p>LLMs for Equation Discovery.Recent work has leveraged LLM-based symbolic regression to enhance scientific equation discovery through various approaches leveraging LLMs' knowledge.LLM-SR (Shojaee et al., 2024b) utilizes LLMs' embedded scientific knowledge to generate initial equation hypotheses in the form of Python programming functions, which are then refined through adaptive mutation and crossover operations with LLMs as evolutionary optimizers.In-Context Symbolic Regression (ICSR) (Merler et al., 2024) employs an iterative few-shot learning paradigm over expression candidates, using previously tested successful expressions along with their fitness scores to guide the generation of improved candidates.LaSR (Grayeli et al., 2024) alternates between hypothesis evolution, concept abstraction, and concept iteration phases to build a learned library of scientific concepts for mathematical relations needed to find the equation for a given data.The learned concepts are then used with pure evolutionary search methods (Cranmer, 2023) like PySR (Cranmer, 2023)</p>
<p>Conclusion</p>
<p>We</p>
<p>Impact Statement</p>
<p>The development and future adoption of LLM-SRBench as a benchmark for evaluating LLM-based scientific equation discovery has the potential to significantly impact the field of artificial intelligence for science and scientific discovery.</p>
<p>There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.</p>
<p>Appendix A. Dataset Details</p>
<p>A.1. LSR-Transform</p>
<p>The LSR-Transform is the first category of datasets in LLM-SRBench, designed to evaluate the ability of LLM-based scientific equation discovery methods in less common mathematical forms.This dataset challenges LLM-based discovery methods to avoid reliance on memorization of well-known representations and instead reason through unfamiliar instantiations of familiar problems.This approach is motivated by the observation that LLMs often struggle with unfamiliar instantiations of otherwise familiar problems, as highlighted by recent studies on the fragility of LLM reasoning (Mirzadeh et al., 2024) Find a mathematical expression that represents the total energy () of a harmonic oscillator system, given data on the mass of the object (), the angular frequency of the system (), the natural angular frequency of the system ( ₀ ), and the displacement from the equilibrium position ().</p>
<p>Find a mathematical expression that represents the electric potential (  ) at a point in space due to an electric dipole, given data on the dipole moment (  ), the angle between the dipole axis and the radius vector to the point (), the distance from the dipole to the point (), and the permittivity of free space ( ).</p>
<p>Find a mathematical expression that represents the current () in a semiconductor diode, given data on the saturation current ( 0 ), the elementary charge (), the voltage across the diode (), Boltzmann's constant (  ), and the absolute temperature ().to mass (m) or angular frequency (ω).Similarly, in the electric potential equation V e = 1 4πϵ</p>
<p>Description of Original Problem Original Problem Transformed Problem Examples
p d cos (θ) r 2
is also transformed into p d = 4πϵr 2 Ve cos (θ) , and r = p d cos (θ) 4πϵVe , showcasing how the problem is reformulated to solve for dipole moment (p d ), and distance (r).These transformations introduce less-common mathematical representations that are simple but not trivial for LLMs to find from the problem description and data.By systematically altering the input-output relationships into new analytically solvable symbolic forms, LSR-Transform challenges models to reason through unfamiliar mathematical forms, testing their ability to generalize beyond memorized representations and leverage data-driven reasoning to find new forms.</p>
<p>The transformed expressions generally exhibit higher complexity than the original physical laws in the Feynman benchmark.To maintain our focus on evaluating semantic complexity (reasoning and memorization capabilities) rather than syntactic complexity and lengthy hypotheses, we deliberately filtered out LSR-transform expressions with significantly higher complexities from the dataset.This filtering ensures that the benchmark primarily challenges discovery models' ability to understand and conduct both scientific and data-driven reasoning rather than their capacity to model longer and more complex mathematical expressions.Figure 8 demonstrates the complexity distribution between the original Feynman Benchmark problems versus their transformed counterparts in LSR-Transform.Following (La Cava et al., 2021), the complexity of each hypothesis (i.e., expression) is quantified as the number of nodes in the expression tree representation of the equation.The expression tree is constructed by parsing the equation into its constituent unary and binary operators, variables, and constants.</p>
<p>Finally, we also exclude the transformed problems that LLM (Llama-3.1-8B-Instruct)can solve through direct sampling without requiring access to data.This process creates a dataset of 111 transformed equations, each sharing the same scientific context and variables as its original counterpart but presenting a less common mathematical form.The goal of LSR-Transform is not to discover new equations but to evaluate whether LLM-based systems can guide discoveries from non-trivial, data-driven transformations of known equations.</p>
<p>Details of Filtering Process</p>
<p>This section provides a comprehensive breakdown of the filtering steps applied during the LSR-Transform dataset generation, addressing the apparent reduction from 100 original Feynman problems to 111 transformed equations.The LSR-Transform dataset generation involves multiple filtering stages that significantly reduce the number of candidate problems.Starting from 100 original Feynman problems, the transformation process initially generates 471 candidate equations by selecting different pivot variables for each equation and performing feature-target transformations.This expansion reflects an average of approximately 4.7 transformed candidates per original problem, demonstrating the diversity introduced by considering multiple input variables as potential targets.The first major filtering occurs during the solvability check using SymPy's symbolic solver (Step 5 in Figure 3), which eliminates 53 problems (11.3% of candidates) that cannot be analytically solved for the target variable.These typically include transcendental equations without closed-form solutions, high-degree polynomial equations where symbolic solutions become intractable, and equations involving complex multi-valued functions.After this stage, 418 problems remain.Notably, no equations are eliminated during dataset refinement (Step 6 in Figure 3).This stage focuses solely on filtering individual datapoints to ensure they fall within the valid domains of the transformed equations (e.g., ensuring positive values under square roots, avoiding division by zero), while the equations themselves remain intact.The most significant reduction occurs during complexity filtering, where 307 problems (73.4% of remaining candidates) are eliminated, resulting in the final 111 problems.This filtering serves a crucial purpose: to ensure that the challenging nature of LSR-Transform stems from semantic complexity (reasoning about the scientific problem and unfamiliar mathematical forms) rather than syntactic complexity (handling lengthy expressions).Following La Cava et al. (?), complexity is measured as the number of nodes in the expression tree representation of each equation.Following this definition, we constrain the complexity distribution to match that of the original Feynman benchmark (Figure 8).In other words, transformed equations with complexity significantly exceeding the original Feynman distribution are exclude.These design choices maintains focus on testing reasoning capabilities while preserving analytical tractability and scientific diversity across physics domains.As demonstrated in Figure 8, even after filtering, LSR-Transform problems remain substantially more challenging than original Feynman problems at same levels of complexity.</p>
<p>A.2. LSR-Synth</p>
<p>The LSR-Synth is the second category of datasets in LLM-SRBench which is a collection of synthetic problems designed to benchmark the performance of LLMs in scientific equation discovery.This dataset is particularly focused on generating plausible yet challenging equation discovery problems that span multiple scientific domains, including chemistry, physics, biology, and material science.The problems in LSR-Synth are constructed by combining known terms, which are wellestablished in the scientific literature, with synthetic terms that introduce novel and plausible variations to the equations.</p>
<p>Figure 9 provides examples of problems from the LSR-Synth.These examples demonstrate the dataset's design, which combines well-established mathematical and scientific expressions with novel, domain-specific variations to create challenging models that address the trivial LLM memorization.Each equation is composed of both known and synthetic terms (highlighted in red).Known terms are terms that are commonly found in scientific equations and are well-documented in the literature for that domain and specific problem.For example, terms like −C 0 A(t) and −C 0 A(t) 2 are typical in chemistry reactions as the first-order and second-order kinetics.These terms are included to ensure that the problems remain grounded in the established scientific context, providing a foundation for the LLM-based methods to build upon for equation discovery related to each scientific problem.On the other hand, synthetic terms are introduced to create novel variations in the problems to avoid trivial LLM memorization.For instance, terms like sin ( A(t)) and cos (log (A(t) + 1)) in chemistry reaction kinetics are designed to challenge the LLM-based discovery models by introducing non-linearities and interactions that are not commonly seen in standard models.These terms are critical for testing the ability of LLM-based equation discovery models to generalize beyond memorization of standard known formulations and discover new patterns from data-driven reasoning and refinement.The combination of known and synthetic terms in LSR-Synth creates a dataset that is both challenging and representative of established scientific problems.This approach enables rigorous evaluation of models' capabilities in interpreting and discovering complex scientific equations, striking a balance between domain familiarity and innovative data-driven reasoning.To generate these known and synthetic terms across various domains, we leverage LLM (GPT-4o) by providing problem domain context and descriptions, prompting it to generate candidate terms.These suggested terms and equations are then filtered based on solvability and novelty criteria, followed by domain expert validation.</p>
<p>Figure 10 provides an analysis of the complexity of the problems in the LSR-Synth dataset.Similar to Figure 8, complexity is quantified as the number of nodes in the expression tree.This figure highlights the diverse nature of the LSR-Synth dataset, with complexity levels ranging from simple expressions to highly complex ones.By spanning a wide range of domains (chemistry, physics, biology, and material science) and hypothesis complexities, LSR-Synth serves as a comprehensive dataset for evaluating the capabilities of LLMs in scientific equation discovery.</p>
<p>Once the structure of equations is generated, their parameters (coefficients) are sampled randomly from specified and scientifically valid ranges, and then data are generated through different solution methods depending on the domain.For dynamical systems (chemical reactions, population dynamics, and physical oscillators), we employ numerical integration using SciPy's solve ivp with the RK45 method, while static relationships (material stress-strain) are evaluated directly over predetermined input ranges.For each domain, we generate 5000 evenly spaced samples.In dynamical systems, these samples span the time interval t ∈ [0, 60], while for material stress-strain relationships, the samples cover strain ϵ ∈ [0, 0.6] and temperature T ∈ [273, 573]K.To evaluate out-of-distribution (OOD) generalization, for time-dependent systems, we designate the last 500 time points as the out-of-domain (OOD) test set, with the remaining 4500 points used for in-domain (ID) training and validation.Similarly, for the stress-strain domain, the OOD test set comprises the last 500 points based  on temperature values, maintaining a consistent evaluation framework across all domains.The data generation process incorporates the same quality control criteria used in equation generation.Generated solutions must satisfy: (1) solvability within specified numerical tolerance, (2) meaningful physical behavior (avoiding divergence or constant solutions), and (3) uniqueness compared to existing solutions (using RMSE thresholds).These criteria ensure that the final dataset contains diverse, physically meaningful, and numerically stable solutions suitable for benchmarking equation discovery methods.</p>
<p>B. Evaluation Details</p>
<p>B.1. Data Fidelity</p>
<p>We evaluate the data-driven performance of discovered equations through multiple complementary metrics focusing on both predictive accuracy and generalization capability.The primary metrics include Accuracy to Tolerance (Acc τ ), and Normalized Mean Squared Error (NMSE).The Acc τ metric provides a binary assessment of prediction accuracy based on point-wise relative error.An equation is considered accurate if the maximum relative error across all test tolerance τ .Formally:
Acc τ = 1 max 1≤i≤Ntest ŷi − y i y i ≤ τ
where ŷi represents the predicted value, y i is the true value, and N test is the number of test samples.The indicator function 1(•) returns 1 if the condition is satisfied and 0 otherwise.This metric is particularly useful for cases where maintaining a consistent level of accuracy across all predictions is crucial, as it identifies equations that might have occasional but significant deviations from the true values.NMSE also provides a continuous measure of the overall prediction quality, normalized by the scale of the true values:
NMSE = Ntest i=1 (ŷ i − y i ) 2 Ntest i=1 (y i − ȳi ) 2
This normalization makes the metric scale-invariant, allowing meaningful comparisons across different datasets and equation types.The NMSE ranges from 0 to ∞, where 0 indicates perfect prediction.Unlike Acc τ , NMSE provides a more nuanced view of model performance by considering the magnitude of prediction errors across all test points rather than just their maximum relative error.Beyond standard predictive metrics, we also place particular emphasis on evaluation of out-of-distribution (OOD) generalization, a critical requirement for scientific equations.For datasets in LSR-Synth which have been generated synthetically, we evaluate the discovered hypotheses on held-out OOD test sets to also assess the extrapolation capabilities.The performance gap between in-domain and OOD test sets (∆NMSE and ∆Acc τ ) provides valuable insights into the generalizability of the discovered equations.</p>
<p>B.2. Symbolic Accuracy</p>
<p>We introduce a novel evaluation methodology for equation discovery that leverages LLM (GPT-4o) as an evaluator for assessing mathematical equivalence between predicted and gold equation hypotheses.Traditional metrics in symbolic regression, such as recovery rate (La Cava et al., 2016), exact match, or normalized tree edit distance (Matsubara et al., 2022), often fail to capture the true semantic equivalence of mathematical expressions, especially when dealing with different representation formats or algebraically equivalent forms.Our approach employs GPT-4o as an automated evaluator, capable of analyzing symbolic equivalence across diverse representation formats including equation strings, expression trees, and executable programs.The evaluation process begins by pre-processing the hypotheses by (1) removing additional information (such as natural language comments in the case of programs), and (2) replacing constants with placeholder parameter vectors, focusing solely on logical structure and mathematical relations.To assess the reliability of this LLMbased symbolic evaluation approach for equation discovery, we conducted a human evaluation study.Two of the authors independently assessed mathematical symbolic equivalence on a set of 130 randomly sampled problems.The validation study revealed a 94.6% agreement rate between GPT-4o and human evaluators, where agreement rate is calculated as the percentage of cases where both LLM and human evaluators made the same judgment about the mathematical equivalence between predicted and ground truth equations (123 out of 130).</p>
<p>Figure 11 provides the prompt used for our GPT-4o based evaluation of the mathematical symbolic equivalence between the generated hypothesis (in the form of program or expression) against the ground truth equation.In this setting, the GPT-4o first articulates its mathematical reasoning before making an equivalence binary assessment.</p>
<p>C.2.2. LASR</p>
<p>We use the default prompts from LaSR's (Grayeli et al., 2024) public code repository (https://github.com/trishullab/LibraryAugmentedSymbolicRegression.jl), which includes:</p>
<p>1.The LLMINIT prompt, which is used in an LLM-augmented initialization operation.</p>
<ol>
<li>
<p>LLMMUTATION prompt is used to mutate an expression based on a set of concepts.</p>
</li>
<li>
<p>LLMCROSSOVER prompt is used to construct a new expression from the crossover of two sampled expressions based on a set of concepts.</p>
</li>
<li>
<p>LLM Concept Abstraction prompt in CONCEPTABSTRACTION function, which extracts a natural language concept from current trends of hypotheses at each iteration.</p>
</li>
<li>
<p>LLM Concept Evolution prompt in CONCEPTEVOLUTION function, which creates a new concept that follows a set of ideas in the current library.</p>
</li>
</ol>
<p>In the following, we provide examples of these prompts.</p>
<ol>
<li>LLMINIT prompt.</li>
</ol>
<p><System prompt> You are a helpful assistant that proposes a mathematical expression by following three provided suggestions.</p>
<p>An expression must consist of the following variables: {{variables}}.All constants will be represented with the symbol C. Each expression will only use these operators: {{operators}}.</p>
<p><User prompt> Suggestion 1: {{assump1}} Suggestion 2: {{assump2}} Suggestion 3: {{assump3}}</p>
<p>Propose {{N}} expressions that would be appropriate given the suggestions.Provide short commentary for each of your decisions.End with a JSON list that enumerates the proposed expressions following this format: '''json ["expr1", "expr2", ... "expr{{N}}" ] '''</p>
<ol>
<li>LLMMUTATION prompt.</li>
</ol>
<p><System prompt> You are a helpful assistant that mutates a mathematical expression by following a few provided suggestions.You will be given three suggestions and a single reference expression to mutate.An expression must consist of the following variables: {{variables}}.All constants will be represented with the symbol C. Each expression will only use these operators: {{operators}}.</p>
<p><User prompt> Suggestion 1: {{assump1}} Suggestion 2: {{assump2}} Suggestion 3: {{assump3}} Reference Expression: {{expr}} Propose {{N}} expressions that would be appropriate given the suggestions and references.Provide short commentary for each of your decisions.End with a JSON list that enumerates the proposed expressions following this format: '''json ["expr1", "expr2", ... "expr{{N}}" ] '''</p>
<ol>
<li>LLMCROSSOVER prompt.</li>
</ol>
<p><System prompt> You are a helpful assistant that recombines two mathematical expressions by following a few provided suggestions.You will be given three suggestions and two reference expressions to recombine.An expression must consist of the following variables: {{variables}}.All constants will be represented with the symbol C. Each expression will only use these operators: {{operators}}.Propose {{N}} hypotheses that would be appropriate given the expressions.Provide short commentary for each of your decisions.Do not talk about topics related to the simplicity or complexity of the expressions.I want ideas that are unique and interesting enough to amaze the world's best mathematicians.End with a JSON list that enumerates the proposed hypotheses following this format: '''json ["hyp1", "hyp2", ... "hyp{{N}}" ] '''</p>
<p>LLM Concept Evolution prompt.</p>
<p><System prompt> You are an insightful assistant skilled in logical reasoning and deduction.Your task is to analyze a set of ideas and infer nontrivial conclusions that logically follow from them.The ultimate goal is to uncover underlying principles or properties of the hidden expressions.Focus on providing logical conclusions that are unique, interesting, and profound.</p>
<p><User prompt> Idea 1: {{idea1}} Idea 2: {{idea2}} Idea 3: {{idea3}} Idea 4: {{idea4}} Idea 5: {{idea5}}</p>
<p>Based on these ideas, deduce {{N}} logical conclusions or hypotheses that directly follow from them.Provide a brief explanation for each conclusion, highlighting the logical connections between the ideas.Avoid discussing topics related to the simplicity or complexity of the expressions.Conclude with a JSON list that enumerates the proposed conclusions in the following format:
'''json ["Conclusion 1", "Conclusion 2", ... "Conclusion {{N}}" ] ''' C.2.3. SGA
The following prompts are used in our implementation of SGA (Ma et al., 2024) for scientific equation discovery tasks, following the original implementation SGA's public code repository (https://github.com/PingchuanMa/SGA),which includes:</p>
<p>System prompt for task.</p>
<p>You are an intelligent AI assistant for coding and scientific equation discovery.</p>
<p>You are tasked with discovering mathematical function structures for scientific systems.Follow the user's requirements carefully and make sure you understand them.Keep your answers short and to the point.Do not provide any information that is not requested.Always document your code as comments to explain the reason behind them.Use Markdown to format your solution.</p>
<p>You are very familiar with Python and PyTorch.Do not use any external libraries other than the libraries used in the examples.</p>
<p>Code formatting prompt for scientific equation discovery task.</p>
<h3>PyTorch Tips 1.When working with tensors, always use PyTorch's operators (such as 'torch.exp','torch.cos','torch.sqrt',...) to ensure compatibility and optimal performance.2. In PyTorch, operator input arguments must be tensors, not floats.</h3>
<h3>Code Requirements 1.The only library allowed is PyTorch.Follow the format provided by the user examples.</h3>
<ol>
<li>Annotate the size of the tensor as comment after each tensor operation.For example, # (B, 3, 3).3. Output the code in a single code block "'''python ... '''" with detailed comments in the code block.Do not add any trailing comments before or after the code block.Start this section with "### Code".</li>
</ol>
<p>Context prompt for each scientific problem.</p>
<h3>Context</h3>
<p>The objective is to construct a mathematical expression that accurately maps input variables to a target output based on a provided dataset.The task involves filling in a code block to define a symbolic expression or model that minimizes the difference between predicted and ground-truth outputs.The code block defines a class with two functions: one for parameters within the expression and another for generating or modifying the symbolic structure of the expression.Feedback is provided in the form of metrics measuring the error between the model's predictions and the ground-truth values, as well as guidance on structural improvements to the symbolic expression.</p>
<p>The expression represents {$OUTPUT_VAR_DESC}, given data on {$INPUTS_DESC}.</p>
<p>D. Additional Results and Analysis</p>
<p>Detailed Numeric Accuracy Analysis.While Table 1 presents median Normalized Mean Squared Error for each method-LLM combination across LLM-SRBench datasets, Figure 12 provides a more comprehensive view of error distributions across all samples.These box plots illustrate performance variations across LLM-SRBench datasets from two perspectives: comparing different equation discovery methods with GPT-4o-mini as the LLM backbone, and examining different LLM backbones when using LLM-SR method.The substantial variance in NMSE performance across samples reflects the diverse complexity inherent in our benchmark-stemming from both the varying mathematical transformations in LSR-Transform and the different combinations of known and synthetic terms in LSR-Synth datasets.Notably, the relative difficulty of datasets varies across methods and LLM backbones, suggesting that different methods and LLMs possess distinct capabilities in terms of leveraging domain knowledge, reasoning, and generating novel hypotheses.Symbolic Accuracy and Generalization.For scientific equation discovery methods, both symbolic accuracy and out-ofdomain generalization serve as crucial evaluation metrics, reflecting the methods' ability to uncover true governing equations.Figure 13 examines the relationship between these metrics, plotting symbolic accuracy against both OOD accuracy and OOD NMSE across all method-LLM-domain combinations in LSR-Synth.The strong correlation observed between symbolic and OOD performance yields two important insights: first, it establishes OOD evaluation as a powerful metric for assessing the discovery of generalizable equations, an approach historically underutilized in symbolic regression; second, it validates our LLM-based symbolic evaluation approach through its strong alignment with numeric generalization performance.</p>
<p>Qualitative Analysis of Outputs.To provide deeper insights into the behavior of different discovery methods, Figure 14 illustrates their final discovered hypotheses on a biological population growth problem (BPG0) using Llama-3.1-8B as the LLM backbone.Direct Prompting (Figure 14(a)) generates equations that capture basic population dynamics, demonstrating LLMs' ability to propose scientifically plausible structures.SGA's solution (Figure 14(b)) successfully incorporates one of the common population growth terms while exploring additional structural components.LaSR (Figure 14(c)) discovers an equation structure that combines multiple interaction terms, though it differs from established scientific formulations.LLM-SR (Figure 14(d)) combines both standard population dynamics terms and synthetic components in its solution.These examples demonstrate the diverse approaches methods take in balancing scientific interpretability with mathematical expressiveness when discovering equation structures.</p>
<p>E. Discussion and Future Directions</p>
<p>Our findings from LLM-SRBench reveal several key insights that inform the design of future LLMs for scientific discovery applications.Scientific equation discovery remains a challenging problem for LLMs, requiring a complex interplay of domain knowledge, search capabilities with data-driven feedback, and mathematical manipulation skills.Our results demonstrate that this problem poses significant challenges for LLM-based discovery frameworks across different model architectures, suggesting that current approaches may be fundamentally limited in their ability to perform genuine scientific discovery.</p>
<p>This work questions the current evaluation paradigm for equation discovery in emerging LLM-based techniques.We demonstrate that existing benchmarks for this task are susceptible to memorization and inadequate for evaluating these techniques' true scientific discovery capabilities.Motivated by these limitations, we designed LLM-SRBench to address the memorization issue through two key innovations: synthetic imaginary scenarios (LSR-Synth category) that are not based on existing scientific knowledge and require data-driven discovery tools for solution, and transformed equations (LSR-Transform category) that convert common forms of scientifically known equations into less familiar formulations.The LSR-Synth category targets genuine innovation in LLM-based discovery techniques by eliminating the possibility of recalling memorized equations, while LSR-Transform problems are difficult to recite from memory and require reasoning over hypothesis generation steps, making them suitable candidates for evaluating recently emerging LLM-based scientific discovery agents.While the mathematical transformations in LSR-Transform are algebraically valid, their scientific meaningfulness varies considerably across contexts.Many transformations correspond to legitimate physics problems from the Feynman Lecture Series collection and represent alternative problem formulations with practical significance.For example, in the Harmonic Oscillator Energy problem, the original formulation E = 1 4 m(ω 2 + ω 2 0 )x 2 expresses energy as a function of system parameters, while the transformed version m = 4E (ω 2 +ω 2 0 )x 2 determines the mass required for given energy storage.This transformation maintains scientific meaning by addressing the engineering question of what mass is needed to store a specific amount of energy in an oscillating system, and such inversions are common in engineering design problems where system parameters must be determined to achieve desired performance characteristics.Similarly, the Electric Potential problem transforms from V e = 1 4πϵ p d cos(θ) r 2</p>
<p>(potential at a point due to a dipole) to r = p d cos(θ) 4πϵVe</p>
<p>(distance for a given potential), addressing the practical question of determining measurement distances in electrostatic experiments or sensor design.</p>
<p>However, not all transformations maintain clear physical interpretability.Some result in equations where the target variable appears in complex functional forms that may not correspond to natural physical questions, such as solving for angular frequency in oscillatory systems yielding expressions involving square roots of differences that lack intuitive physical meaning.Additionally, certain transformations may obscure natural causal relationships-transforming from "force causes acceleration" to "acceleration determines force" maintains mathematical validity but may not reflect underlying physical causality.The LSR-Transform category represents a deliberate balance between mathematical rigor and physical meaningfulness by constraining the complexity of transformed problems to match original problems, focusing on semantic rather than syntactic challenges in scientific equation discovery, while maintaining the original scientific context and variable meanings to ensure that underlying physics remains relevant even when mathematical formulation changes.The varying scientific meaningfulness of transformations reflects broader challenges in automated scientific discovery that warrant future investigation.Automated discovery systems must incorporate mechanisms to evaluate not only data-driven correctness but also scientific plausibility and interpretability of generated hypotheses, as mathematical validity alone is insufficient for meaningful scientific contribution.The most effective approach to scientific equation discovery likely involves close collaboration between AI systems, which excel at exploring vast hypothesis spaces, and human domain scientists, who can assess scientific meaningfulness and guide discovery directions based on deep contextual understanding.Future equation discovery methods could improve by incorporating literature retrieval tools to build grounding foundations for scientific context and domain knowledge, helping to prioritize discoveries that are mathematically valid, data-consistent, novel, and scientifically meaningful.The field needs evaluation frameworks that assess not just mathematical correctness but also scientific novelty, interpretability, and practical applicability of discovered equations, moving beyond narrow accuracy metrics toward a more comprehensive understanding of what constitutes valuable scientific discovery in the age of LLMs with their vast scientific knowledge.</p>
<p>F. Comparison with Standard (non-LLM) Symbolic Regression Baselines</p>
<p>To further validate the utility of LLM-SRBench and demonstrate the advantages of LLM-based approaches, we conducted additional experiments comparing LLM-based methods with traditional symbolic regression techniques that do not incorporate domain knowledge.We evaluated PySR (Cranmer, 2023), a state-of-the-art symbolic regression method based on genetic programming, on all LLM-SRBench datasets.PySR operates purely on numerical data points without access to the scientific context, variable descriptions, or domain knowledge that LLM-based methods can leverage in discovery process.We used PySR's default configuration with the same computational budget (equivalent number of evaluations) as the LLM-based methods to ensure fair comparison.Table 3 presents the performance comparison between the best-performing LLM-based method from Table 1 and PySR across all LLM-SRBench datasets.The results reveal several key insights about the complementary strengths and limitations of non-LLM versus LLM-based approaches in equation discovery.</p>
<p>PySR demonstrates competitive and sometimes even better numerical accuracy (Acc 0.1 ) across all datasets.However, PySR consistently shows significantly lower symbolic accuracy, particularly struggling with non-physics domains where it achieves 0% symbolic accuracy on chemistry, biology, and material science datasets.The performance gap is most pronounced in problems that require specialized scientific knowledge.While PySR can fit mathematical patterns in the data, it lacks the scientific intuition to discover equations that align with established physical principles or domain-specific terminology.Interestingly, PySR shows relatively better performance on physics problems, achieving modest symbolic accuracy of 4.54% on LSR-Synth Physics and 8.11% on LSR-Transform (which is based on Feynman physics equations).This suggests that physics problems may contain mathematical patterns that are more aligned with the dictionary design in PySR.So they can be discovered better through the data-driven search pipeline designed in PySR.These findings strengthen the motivation for LLM-based scientific equation discovery and demonstrate that LLM-SRBench successfully captures challenges in equation discovery that traditional symbolic regression methods cannot adequately address through numerical data-driven optimization alone.
CKR5 −kA(t) 2 + k q A(t) log(γt + 1) CKR6 −k (A(t) + k f A(t) 0.33 CKR7 −kA(t) exp(−k s t) + k m sin( A(t)) CKR8 −kA(t) exp(−k s t) + k w cos(log(A(t) + 1)) CKR9 −kA(t) 2 − kA(t) + k t sin(log(A(t) + 1)) CKR10 −k A(t) + k w cos(log(A(t) + 1)) CKR11 −kA(t) 2 + k t sin(log(A(t) + 1)) CKR12 −kA(t) 2 + k m sin( A(t)) CKR13 −kA(t) exp(−k s t) + k t sin(log(A(t) + 1)) CKR14 −kA(t) + k p sin(ωA(t)) CKR15 −k A(t) − kA(t) exp(−k s t) + k p sin(ωA(t)) CKR16 −k A(t) − kA(t) exp(−k s t) + k t sin(log(A(t) + 1)) CKR17 −kA(t) + k f A(t) 0.33 CKR18 −kA(t) exp(−k s t) + k f A(t)
Figure 2 .
2
Figure2.Overview of the LLM-based Scientific Equation Discovery.The benchmark tasks (left) combine scientific context with numerical data.The discovery process (middle) iteratively leverages LLM's scientific knowledge and data-driven reasoning to generate hypotheses for underlying equations.Discovered hypotheses, represented as equation strings, trees, or programs, are then evaluated (right) using multiple metrics including data fidelity, symbolic accuracy, and computational efficiency.</p>
<p>Figure 3 .
3
Figure 3. Data generation pipelines for the two dataset categories in LLM-SRBench.(a) LSR-Transform converts Feynman problems into alternative mathematical forms through symbolic transformation and input-output role switching, and (b) LSR-Synth generates novel discovery-driven problems by combining known scientific terms in the underlying models with synthetic novel terms.Both pipelines include validation steps to ensure solvability and scientific plausibility.</p>
<p>Figure 4 .
4
Figure 4. Performance comparison across equation complexity levels for Feynman and LSR-Transform datasets: (a) symbolic accuracy and (b) numeric precision (Acc0.1)showing considerable performance gap between these two datasets at same complexity levels (averaged over all method-LLM pairs).</p>
<p>Figure 5 .
5
Figure 5. Detailed results of in-domain (ID) and out-of-domain (OOD) performance using Normalized Mean Squared Error across various LSR-Synth scientific domains and LLM-based equation discovery methods (with GPT-4o-mini as LLM backbone).</p>
<p>Figure 6 .
6
Figure 6.Correlation between symbolic accuracy and OOD performance across different equation discovery methods and LLM backbones: (a) symbolic accuracy vs. Acc0.1 showing positive correlation; (b) symbolic accuracy vs. normalized mean squared error showing negative correlation.Results are averaged over all LSR-Synth datasets.</p>
<p>Figure 7 .
7
Figure 7. Examples of how LLM-SRBench (LSR-Transform) problems can be obtained from original Feynman benchmark problems.</p>
<p>Figure 7 Figure 8 .
78
Figure 7 demonstrates the equation transformation process, showing examples of the original Feynman problems (along with their scientific descriptions) and their potential transformed versions.These examples show the dataset's design for altering the mathematical representation of the same problem by analytically solving the equations with respect to different input variables.For instance, the original harmonic oscillator energy equation E = 1 4 m(ω 2 + ω 2 0 )x 2 is transformed into symbolic representation of m = 4E (ω 2 +ω 2 0 )x 2 and ω =</p>
<p>Figure 9 .
9
Figure9.Examples of LLM-SRBench (LSR-Synth) problems with known and synthetic terms across different domains.Each problem presents a target equation as the hypothesis to be discovered which is composed of known terms and synthetic terms (in blue).</p>
<p>hypotheses sampled from LLM for initial prompt No access to data for data-driven refinement Time limit T = 30s per program hypothesis execution, BFGS optimizer from Scipy for parameter optimization of equation skeletons SGA(Ma et al., 2024) PyTorch-based implementation of model and torch.nn.Module class Mean square error loss for data-driven feedback in agentic search Adam optimizer in PyTorch for differential parameter optimization of equation skeletons LaSR (Grayeli et al., * , −, /, ∧, exp, log, sqrt, sin, cos, tan, cosh LLM weights: llm mutate =0.005, llm crossover =0.005, llm gen random =0.005 Top-K = 20 concepts from library Default configuration of PySR for parameter optimization LLM-SR (Shojaee et al., 2024b) Temperature τ = 0.8 Batch size b = 4 equation programs per prompt e = 4 parallel evaluators Time limit T = 30s per program hypothesis, Memory limit M = 2GB m = 10 islands for population diversity through search k = 2 in-context examples per prompt Maximum 10 parameters per equation skeleton BFGS optimizer from Scipy for parameter optimization of equation skeletons # Load data observations inputs, outputs = data['inputs'], data['outputs'] X = inputs # Optimize parameters based on data from scipy.optimize import minimize def loss(params): y_pred = equation( * X, params) return np.mean((y_pred -outputs) ** 2) loss_partial = lambda params: loss(params) result = minimize(loss_partial, [1.0] * MAX_NPARAMS, method='BFGS') # Return evaluation score optimized_params = result.xloss = result.funif np.isnan(loss) or np.isinf(loss): return None else: return -loss 3. Equation example specification as Python programming function.### Function Examples def equation_v0($INPUT_VAR[0], ..., $INPUT_VAR[N], params): """ Mathematical function for {$OUTPUT_VAR_DESC} Args: $INPUT_VAR[0]: A numpy array representing observations of {$INPUT_VAR_DESC[0]}. ... $INPUT_VAR[N]: A numpy array representing observations of {$INPUT_VAR_DESC[N]}. params: Array of numeric constants or parameters to be optimized Return: A numpy array representing {$OUTPUT_VAR_DES} as the result of applying the mathematical function to the inputs.""" # Equation example 1 logic as function body ... def equation_v1($INPUT_VAR[0], ..., $INPUT_VAR[N], params): # Equation example 2 ... ### Function to be completed def equation($INPUT_VAR[0], ..., $INPUT_VAR[N], params):""" Improvement version of equation_v0 and equation_v1 """</p>
<ol>
<li>Separate the code into parameters that can be tuned with differentiable optimization and the symbolic expression represented by PyTorch code.Define them respectively in the 5.The proposed code must strictly follow the structure and function signatures below: '''python import torch import torch.nnas nn class SymbolicEquation(nn.Module): def <strong>init</strong>(self, {$PARAM_INPUTS}): """ Define trainable continuous parameters for differentiable optimization.Tentatively initialize the parameters with the default values in args.step-by-step what the potential problem is in the previous iterations based on the feedback.Think about why the results from previous iterations mismatched with the ground truth.Do not give advice about how to optimize.Focus on the formulation of the scientific equation.Start this section with "### Analysis".Analyze all iterations individually, and start the subsection for each iteration with "#### Iteration N", where N stands for the index.Remember to analyze every iteration in the history.2. Think step-by-step what you need to do in this iteration.Think about what is needed to improve performance.If the analysis suggests specific functional forms or constraints, think about how these will be incorporated into the symbolic equation.Think about how to separate your algorithm into a continuous parameter part and a symbolic expression model part.Describe your plan in pseudo-code, written out in great detail.Remember to update the default values of the trainable parameters based on previous optimizations.Start this section with "### Step-by-Step Plan".</li>
</ol>
<p>Figure 12 .
12
Figure 12.Normalized Mean Squared Error (NMSE) of discovered equations in various domains of LLM-SRBench with respect to (left) different equation discovery methods using GPT-4omini LLM backbone, and (right) different LLM backbones using LLM-SR method</p>
<p>Figure 13 .
13
Figure 13.Symbolic Accuracy versus OOD performance over all domains, methods, and backbone LLM pairs.</p>
<p>Figure 14 .Figure 16 .
1416
Figure 14.Example of output hypotheses from different LLM scientific equation discovery methods for BPG0 problem in LSR-Synth biology domain.</p>
<p>Table 1 .
1
Comparison of different LLM-based scientific equation discovery methods on LLM-SRBench.Performance metrics include symbolic accuracy (SA), numeric precision (Acc0.1),and normalized mean squared error (NMSE).Bold values indicate best performance within each method, and underlined values show best overall performance across discovery methods.
ModelsLSR-TransformChemistryBiologyLSR-SynthPhysicsMaterial ScienceSA</p>
<p>(Udrescu &amp; Tegmark, 2020)g benchmark problems into alternative mathematical representations, LSR-Transform provides a rigorous testbed to evaluate how well LLM-based discovery methods perform in both (1) semantic scientific reasoning, which draws on LLMs' built-in scientific knowledge, and (2) data-driven reasoning, which utilizes experimental feedback for equation discovery.LSR-Transform builds on the Feynman benchmark(Udrescu &amp; Tegmark, 2020), a widely used standard benchmark in scientific equation discovery and symbolic regression.The Feynman benchmark consists of 100 physics equations from Feynman Lecture Series 1 , representing fundamental laws in physics.While the Feynman benchmark has been instrumental in evaluating symbolic regression methods, it primarily tests the ability to recover equations in their standard, well-known forms which are mostly memorized by LLMs.However, real-world scientific equation discovery often involves reasoning about unknown equations based on domain expertise and knowledge from literature as well as empirical data observations.To address this gap, LSR-Transform transforms the original Feynman equations into less common alternative mathematical forms of the same physical problem by switching input-output variables and symbolically solving for the new target variables.
𝐸 =1 4𝑚 𝜔 2 + 𝜔 0 2 𝑥 2𝑚 = 𝜔 =4𝐸 𝜔 2 + 𝜔 0 2 𝑥 2 4𝐸 𝑚𝑥 2 − 𝜔 0 2𝑝 𝑑 =4𝜋𝜖𝑟 2 𝑉 𝑒 cos 𝜃𝑉 𝑒 =1 4𝜋𝜖.𝑝 𝑑 cos 𝜃 𝑟 2𝑟 =𝑝 𝑑 cos 𝜃 4𝜋𝜖𝑉 𝑒𝜃 = arccos(4𝜋𝜖𝑟 2 𝑉 𝑒 𝑝 𝑑)𝑞 =𝑘 𝑏 . 𝑇 𝑉ln( 𝐼 0 𝐼+ 1)𝐼 = 𝐼 0 exp𝑞. 𝑉 𝑘 𝑏 . 𝑇− 1𝐼 0 =exp𝐼 𝑞. 𝑉 𝑘 𝑏 . 𝑇− 1𝑇 =𝑞. 𝑉 𝑘 𝑏 . ln( 𝐼 𝐼 0+ 1)</p>
<p>0 .sin  −  1 .() −  2 .. exp(− () )  0 .sin  −  1 .  3 −  2 .sin   .() −  3 .sin    0 .sin  −  1 .  −  2 .sin   .  +  3 .  2 .  −  4 .().exp −   − 0 .  −  1 .sin   − 0 .  2 −  1 .exp − 2 . +  3 .sin  4 . 
Example of LSR-Synth Problems with Known and Synthetic TermsChemistry:Reaction rate with respect toTime and Concentration−𝐶 0 . 𝐴 𝑡 2 − 𝐶 1 . 𝐴 𝑡 − 𝐶 2 . cos log 𝐴 𝑡 + 1Known Terms Synthetic Terms𝑟 1 −𝑃 𝑡 𝐾 0. 𝑃 𝑡 + 𝑟.𝑃 𝑡 2 𝛼𝑃 𝑡 + 1Biology: Growth rate with respect to𝑟. 𝑃 𝑡 + 𝑟 1 −𝑃 𝑡 𝐾 0. 𝑃 𝑡 + 𝛽𝑃 𝑡 sin(𝜔𝑡)Time and Population size𝑟 1 −𝑃 𝑡 𝐾 0. 𝑃 𝑡 + 𝑟 1 −𝑃 𝑡 𝐾 0. −1 +𝑃 𝑡 𝛼. 𝑃 𝑡 + 𝑟. (1 − exp −𝛾𝑃 𝑡 . 𝑃 𝑡Physics:Acceleration with respect toTime, Displacement, and Velocity𝐶 0 . 1 − 𝐶 1 . 𝑇 − 𝑇 0 . 𝜖 + 𝐶 2 . exp − 𝑇 − 𝑇 02 . 𝜖Material Science:Stress with respect to Strain𝐶 0 . 1 − 𝐶 1 . 𝑇 − 𝑇 0 . 𝜖 − 𝐶 2 𝑇 − 𝑇 0 + 𝐶 3 . 𝑇 − 𝑇 0 . log(𝜖 + 1)and Temperature𝐶 0 . 1 − 𝐶 1 . 𝑇 − 𝑇 0 . 𝜖 + 𝐶 2 . 𝜖 𝐶 3 . exp(−𝐶 4 𝐶 5 . 𝑇) + 𝐶 6 . exp − 𝑇 − 𝑇 02 . 𝜖</p>
<p>Distribution of problem complexity in LLM-SRBench (LSR-Synth) datasets across scientific domains.
0.10 0.12LSR-Synth Domains Chemistry Biology Physics Material ScienceFraction of Dataset0.04 0.06 0.080.020.000-45-910-1415-1920-24 Complexity25-2930-3435-3940-44Figure 10.</p>
<p>Table 2 .
2
Implementation details of LLM-based scientific equation discovery methods.</p>
<p>prompt&gt; You are a helpful assistant that hypothesizes about the underlying assumptions that generated a list of good and bad mathematical expressions in detailed ways.My ultimate goal is to discover what assumptions generated the observed good mathematical expressions and excludes the bad mathematical expressions.Focus more on the good expressions, their mathematical structure, and any relation to physical concepts.Note that capital C represents an arbitrary constant
<User prompt>Suggestion 1: {{assump1}}Suggestion 2: {{assump2}}Suggestion 3: {{assump3}}Reference Expression 1: {{expr1}}Reference Expression 2: {{expr2}}Propose {{N}} expressions that would be appropriate given the suggestions and references. Provide short commentary foreach of your decisions. End with a JSON list that enumerates the proposed expressions following this format:'''json["expr1","expr2",..."expr{{N}}"]'''4. LLM Concept Abstraction prompt.<User prompt>Good Expression 1: {{gexpr1}}Good Expression 2: {{gexpr2}}Good Expression 3: {{gexpr3}}Good Expression 4: {{gexpr4}}Good Expression 5: {{gexpr5}}Bad Expression 1: {{bexpr1}}Bad Expression 2: {{bexpr2}}Bad Expression 3: {{bexpr3}}Bad Expression 4: {{bexpr4}}Bad Expression 5: {{bexpr5}}
&lt;System</p>
<p>Table 3 .
3
Performance comparison between LLM-based methods and state-of-the-art non-LLM symbolic regression baseline PySR on LLM-SRBench.SA = Symbolic Accuracy (%), Acc0.1 = Accuracy to tolerance 0.1 (%).
DatasetLLM-SR (best) LaSR (best) SGA (best)PySR(Metric)SA / Acc 0.1SA / Acc 0.1 SA / Acc 0.1 SA / Acc 0.1LSR-Transform31.53 / 39.6412.61 / 50.45 9.91 / 8.11 8.11 / 56.76LSR-Synth Chemistry11.11 / 66.662.77 / 38.920 / 16.660 / 41.67LSR-Synth Biology25.30 / 58.338.33 / 20.83 4.16 / 12.510 / 25.0LSR-Synth Physics9.91 / 36.369.91 / 31.814.54 / 9.09 4.54 / 29.55LSR-Synth Material Science20.24 / 88.2828.12 / 72.040 / 36.110 / 68.0</p>
<p>Table 4 :
4
LSR-Synth mathematical equations for each scientific domain.
DomainEquation IDEquationChemistry CKR1−kA(t) 2 + k z A(t) 2 /(βA(t) 4 + 1)CKR2−kA(t)
2 − kA(t) + k w cos(log(A(t) + 1))CKR3−kA(t) + k w cos(log(A(t) + 1))Continued on next page</p>
<p>Table 4
4
− kA(t) exp(−k s t) + k w cos(log(A(t) + 1))
-continued from previous pageDomainEquation IDEquationCKR4−kA(t) 2</p>
<p>CKR28−kA(t) 2 − kA(t) exp(−k s t)+ k f A(t) 0.33 CKR29 −kA(t) exp(−k s t) + k z A(t) 2 /(βA(t) 4 + 1) CKR30 −kA(t) − kA(t) exp(−k s t) + k z A(t) 2 /(βA(t) 4 + 1) CKR31 −kA(t) − kA(t) exp(−k s t) + k t sin(log(A(t) + 1)) − P (t)/K 0 )P (t) + rP (t) 0.33 BPG2 rP (t) exp(−γt) + rP (t) 2 /(αP (t) + 1) − P (t)/K 0 )P (t) + rP (t) 2 /(αP (t) + 1) BPG7 −QαP (t) + r(1 − P (t)/K 0 )P (t) + rP (t) 0.33 + rP (t) BPG8 r(−1 + P (t)/α)(1 − P (t)/K 0 )P (t) + r(1 − P (t)/K 0 )P (t) + rP (t) 0.33 BPG9 r(1 − P (t)/K 0 )P (t) + rP (t) 0.33 + rP (t) BPG10 r(−1+P (t)/α)(1−P (t)/K 0 )P (t)+r(1−P (t)/K 0 )P (t)+r(1−exp(−γP (t)))P (t) BPG11 rP (t) 0.33 + rP (t) BPG12 r(1 − P (t)/K 0 )P (t) + rP (t) 0.33 + rP (t) exp(−γt) sin(t) − µ(1 − x(t) 2 )v(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) PO15 F 0 sin(t) − β log(|v(t)| + 1) − β sin(v(t)) − 2βv(t) − µ(1 − x(t) 2 )v(t) Continued on next pageTable 4 -continued from previous page − x(t) 2 )v(t) − ω 2 0 x(t) exp(−|x(t)|) PO43 F 0 sin(t) − αv(t) 3 − β sin(x(t))v(t) − 2βv(t) PO44 F 0 sin(t) − β sin(x(t))v(t) − 2βv(t) − µ(1 − x(t) 2 )v(t) −
Table 4 -continued from previous page Table 4 -continued from previous page0.33 −k A(t) − kA(t) + k w cos(log(A(t) + 1)) −kA(t) CKR32 CKR19 CKR33 −kA(t) − kA(t) exp(−k s t) + k f A(t) 0.33 CKR34 −k A(t) − kA(t) 2 + k t sin(log(A(t) + 1)) CKR35 PO8 −αv(t) 3 − β|v(t)| 0.33 − ω 2 0 x(t) 3 −kA(t) Continued on next page Continued on next page Equation ID Equation BPG1 r(1 BPG3 βP (t) sin(ωt) + rP (t) exp(−γt) BPG4 r(−1 + P (t)/α)(1 − P (t)/K 0 )P (t) + r(1 − exp(−γP (t)))P (t) BPG5 r(1 − P (t)/K 0 )P (t) + rP (t)/(1 + exp(−α(−β + P (t)))) BPG6 r(1 Physics Domain Biology PO1 F 0 sin(t) − β sin(v(t)) − ω 2 0 x(t) 3 − ω 2 0 x(t) exp(−|x(t)|) PO2 F 0 sin(t) − ω 2 0 x(t) − ω 2 0 x(t) exp(−|x(t)|) PO3 −αv(t) 3 − µ(1 − x(t) 2 )v(t) − ω 2 0 x(t) − ω 2 0 x(t) exp(−|x(t)|) PO4 F 0 sin(t) − β sin(v(t)) − 2βv(t) PO5 F 0 sin(t) − αv(t) 3 − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) PO6 −β sin(v(t)) − 2βv(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) 3 − ω 2 0 x(t) PO7 −β log(|v(t)| + 1) − 2βv(t) − ω 2 0 x(t) 3 Domain Equation ID Equation PO9 −β|v(t)| 0.33 − ω 2 0 x(t) 3 PO10 F 0 sin(t) − µ(1 − x(t) 2 )v(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) PO11 F 0 sin(t) − ω 2 0 (γt + 1)x(t) − ω 2 0 x(t) 3 − ω 2 0 x(t) PO12 −β sin(v(t)) − ω 2 0 (γt + 1)x(t) − ω 2 0 x(t) 3 PO13 F 0 sin(t) − αv(t) 3 − β|v(t)| 0.33 − ω 2 0 (γt + 1)x(t) − ω 2 0 x(t) PO14 F 0 PO16 F 0 sin(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) − ω 2 0 x(t) exp(−|x(t)|) PO17 F 0 sin(t) − β sin(x(t))v(t) − β sin(v(t)) − ω 2 0 x(t) 3 PO18 F 0 sin(t) − β sin(x(t))v(t) − 2βv(t) − ω 2 0 x(t) PO19 −β sin(x(t))v(t) − ω 2 0 x(t) PO20 −2βv(t) − ω 2 0 x(t) exp(−|x(t)|) PO21 −αv(t) PO29 −2βv(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) 3 − ω 2 0 x(t) PO30 −µ(1 − x(t) 2 )v(t) − ω 2 0 (γt + 1)x(t) − ω 2 0 x(t) 3 PO31 −αv(t) 3 − β sin(x(t))v(t) − β sin(v(t)) − ω 2 0 x(t) 3 PO32 −ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) 3 PO33 F 0 sin(t) − αv(t) 3 − β exp(−|v(t)|)v(t) − ω 2 0 x(t) 3 PO34 −2βv(t) − µ(1 − v(t) 2 )v(t) − ω 2 0 (γt + 1)x(t) − ω 2 0 x(t) PO35 −2βv(t) − µ(1 − v(t) 2 )v(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) PO36 F 0 sin(t) − β sin(v(t)) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) PO37 F 0 sin(t) − β exp(−|x(t)|)v(t) PO38 F 0 sin(t) − αv(t) 3 − 2βv(t) − ω 2 0 (γt + 1)x(t) PO39 −β sin(v(t)) − µ(1 − x(t) 2 )v(t) − ω 2 0 x(t) exp(−|x(t)|) PO40 Equation ID Equation F Domain PO42 −µ(1
2 + k p sin(ωA(t))CKR20−kA(t) 2 − kA(t) exp(−k s t) + k t sin(log(A(t) + 1))CKR21−kA(t) exp(−k s t) + k p sin(ωA(t))CKR22−kA(t) exp(−k s t) + k q A(t) log(γt + 1)CKR23 −kA(t) 2 − kA(t) exp(−k s t) + k z A(t) 2 /(βA(t) 4 + 1) CKR24 −k A(t) + k p sin(ωA(t)) CKR25 −k A(t) − kA(t) 2 + k f A(t) 0.33 CKR26 −kA(t) + k t sin(log(A(t) + 1)) CKR27 −kA(t) 2 − kA(t) exp(−k s t) + k m sin( A(t)) 2 + k f A(t) 0.33 CKR36 −kA(t) + k q A(t) log(γt + 1) BPG13 βP (t) sin(ωt) + r(1 − P (t)/K 0 )P (t) BPG14 r(−1 + P (t)/α)(1 − P (t)/K 0 )P (t) + rP (t) + rP (t)/(1 + exp(−α(−β + P (t)))) BPG15 r(1 − P (t)/K 0 )P (t) + r(1 − exp(−γP (t)))P (t) + rP (t) exp(−γt) BPG16 rP (t) 0.33 + rP (t) exp(−γt) BPG17 r(−1 + P (t)/α)(1 − P (t)/K 0 )P (t) + rP (t) 0.33 + rP (t) BPG18 r(−1 + P (t)/α)(1 − P (t)/K 0 )P (t) + rP (t) 0.33 BPG19 βP (t) sin(ωt) + r(1 − P (t)/K 0 )P (t) + rP (t) BPG20 r(1 − P (t)/K 0 )P (t) + rP (t)/t α BPG21 r(−1+P (t)/α)(1−P (t)/K 0 )P (t)+r(1−P (t)/K 0 )P (t)+rP (t)/(1+exp(−α(−β+P (t)))) BPG22 r(−1 + P (t)/α)(1 − P (t)/K 0 )P (t) + rP (t)/t α BPG23 r(1 − exp(−γP (t)))P (t) + rP (t) exp(−γt) BPG24 r(1 − P (t)/K 0 )P (t) + r(1 − exp(−γP (t)))P (t) 3 − β log(|v(t)| + 1) − 2βv(t) − µ(1 − v(t) 2 )v(t) − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) PO22 F 0 sin(t) − β sin(x(t))v(t) PO23 −2βv(t) − β exp(−|x(t)|)v(t) − µ(1 − x(t) 2 )v(t) − ω 2 0 x(t) 3 PO24 F 0 sin(t) − β log(|v(t)| + 1) − ω 2 0 x(t) exp(−|x(t)|) PO25 F 0 sin(t) − αv(t) 3 − β log(|v(t)| + 1) PO26 F 0 sin(t) − β sin(v(t)) PO27 F 0 sin(t) − β log(|v(t)| + 1) − 2βv(t) − ω 2 0 x(t) 3 PO28 F 0 sin(t) − αv(t) 3 − 2βv(t) − β exp(−|v(t)|)v(t) 0 sin(t) − αv(t) 3 − β exp(−|x(t)|)v(t) − µ(1 − v(t) 2 )v(t)PO41F 0 sin(t) − β|v(t)| 0.33 − ω 2 0 (γ|v(t)| 0.33 + 1)x(t) − ω 2 0 x(t) 3 − ω 2 0 x(t)</p>
<p>https://space.mit.edu/home/tegmark/aifeynman.html
AcknowledgmentsThis research was partially supported by the U.S. National Science Foundation (NSF) under Grant No. 2416728.Question: Given the ground truth mathematical expression A and the hypothesis B, determine if there exist any constant parameter values that would make the hypothesis equivalent to the given ground truth expression.Let's think step by step.Explain your reasoning and then provide the final answer as:(A): 'sqrt(q1/(Ef<em>epsilon))/(2</em>sqrt(pi))'C. Implementation DetailsFor a comprehensive evaluation, we implement four state-of-the-art LLM-guided scientific equation discovery baselines, each tested on LLM-SRBench datasets with three different LLM backbones: an open-source model (Llama-3.1-8B-Instruct)and two closed-source models (GPT-3.5-turboand GPT-4o-mini).C.1. ParametersTable2presents the key implementation details for each discovery agentic method.We adopt most of the hyperparameters from the original implementation for these methods.We have only changed some hyperparameters in different baselines that affect the number of LLM calls in the search framework.This is to make sure we have a fair comparison across baseline discovery frameworks with same access budget to LLMs.In our experiments, all baseline frameworks have 1k calls to LLMs (per problem) through the discovery process.C.2. PromptsC.2.1. LLM-SRWe use the default prompts from LLM-SR's(Shojaee et al., 2024b)public code repo (https://github.com/deep-symbolic-mathematics/LLM-SR), which includes:1. Instruction prompt.You are a helpful assistant tasked with discovering mathematical function structures for scientific systems.Complete the 'equation' function below, considering the physical meaning and relationships of inputs.Evaluation specification prompt.
Neural symbolic regression that scales. L Biggio, T Bendinelli, A Neitz, A Lucchi, G Parascandolo, Proceedings of the 38th International Conference on Machine Learning. M Meila, T Zhang, the 38th International Conference on Machine LearningPMLRJul 2021139</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.050802024arXiv preprint</p>
<p>Interpretable machine learning for science with pysr and symbolicregression. M Cranmer, arXiv:2305.015822023jl. arXiv preprint</p>
<p>Large language models for automatic equation discovery of nonlinear dynamics. M Du, Y Chen, Z Wang, L Nie, D Zhang, Physics of Fluids. 3692024</p>
<p>Symbolic regression with a learned concept library. A Grayeli, A Sehgal, O Costilla-Reyes, M Cranmer, S Chaudhuri, arXiv:2409.093592024arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. H Ji, Q Wang, D Downey, T Hope, ACL Anthology: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. 20241University of Illinois Urbana-Champaign/CABBI</p>
<p>End-to-end symbolic regression with transformers. P.-A Kamienny, S Ascoli, G Lample, F Charton, Advances in Neural Information Processing Systems. 2022</p>
<p>Inference of compact nonlinear dynamic models by epigenetic local search. Engineering Applications of Artificial Intelligence. La Cava, W Danai, K Spector, L , 10.1016/j.engappai.2016.07201655</p>
<p>URL. </p>
<p>Contemporary symbolic regression methods and their relative performance. La Cava, W Orzechowski, P Burlacu, B De Franca, F Virgolin, M Jin, Y Kommenda, M Moore, J , Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. J Vanschoren, S Yeung, the Neural Information Processing Systems Track on Datasets and Benchmarks20211</p>
<p>A unified framework for deep symbolic regression. M Landajuela, C Lee, J Yang, R Glatt, C P Santiago, I Aravena, T N Mundhenk, G Mulcahy, B K Petersen, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>Data-driven discovery of physical laws. P Langley, Cognitive Science. 511981</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. P Ma, T.-H Wang, M Guo, Z Sun, J B Tenenbaum, D Rus, C Gan, W Matusik, Forty-first International Conference on Machine Learning. 2024</p>
<p>B P Majumder, H Surana, D Agarwal, S Hazra, A Sabharwal, P Clark, arXiv:2402.13610Data-driven discovery with large generative models. 2024aarXiv preprint</p>
<p>B P Majumder, H Surana, D Agarwal, B D Mishra, A Meena, A Prakhar, T Vora, T Khot, A Sabharwal, P Clark, Discoverybench, arXiv:2407.01725Towards data-driven discovery with large language models. 2024barXiv preprint</p>
<p>Rethinking symbolic regression datasets and benchmarks for scientific discovery. Y Matsubara, N Chiba, R Igarashi, T Tatsunori, Y Ushiku, arXiv:2206.105402022arXiv preprint</p>
<p>Snip: Bridging mathematical symbolic and numeric realms with unified pre-training. K Meidani, P Shojaee, C K Reddy, A B Farimani, The Twelfth International Conference on Learning Representations. 2023</p>
<p>In-context symbolic regression: Leveraging large language models for function discovery. M Merler, K Haitsiukevich, N Dainese, P Marttinen, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics20244Student Research Workshop)</p>
<p>I Mirzadeh, K Alizadeh, H Shahrokhi, O Tuzel, S Bengio, M Farajtabar, arXiv:2410.05229Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024arXiv preprint</p>
<p>Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. T N Mundhenk, M Landajuela, R Glatt, C P Santiago, D Faissol, B K Petersen, Advances in Neural Information Processing Systems. A Beygelzimer, Y Dauphin, P Liang, J W Vaughan, 2021</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. B K Petersen, M L Larma, T N Mundhenk, C P Santiago, S K Kim, J T Kim, International Conference on Learning Representations. 2021</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. C K Reddy, P Shojaee, arXiv:2412.114272024arXiv preprint</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, 10.1126/science.1165893Science Advance. 0036-807532459232009</p>
<p>Transformer-based planning for symbolic regression. P Shojaee, K Meidani, A Barati Farimani, C Reddy, Advances in Neural Information Processing Systems. 362024a</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, arXiv:2404.184002024barXiv preprint</p>
<p>Nonlinear dynamics and chaos with student solutions manual: With applications to physics, biology, chemistry, and engineering. S H Strogatz, 2018CRC press</p>
<p>Symbolic physics learner: Discovering governing equations via monte carlo tree search. F Sun, Y Liu, J.-X Wang, H Sun, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Ai feynman: A physicsinspired method for symbolic regression. S.-M Udrescu, M Tegmark, 10.1126/sciadv.aay2631Science Advances. 61626312020</p>
<p>Semantically-based crossover in genetic programming: application to real-valued symbolic regression. N Q Uy, N X Hoai, M O'neill, R I Mckay, E Galván-López, Genetic Programming and Evolvable Machines. 122011</p>
<p>Symbolic regression is NPhard. M Virgolin, S P Pissis, Transactions on Machine Learning Research. 2835-88562022</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>C Xie, Y Huang, C Zhang, D Yu, X Chen, B Y Lin, B Li, B Ghazi, R Kumar, arXiv:2410.23123On memorization of large language models in logical reasoning. 2024arXiv preprint</p>
<p>Autoturb: Using large language models for automatic algebraic model discovery of turbulence closure. Y Zhang, K Zheng, F Liu, Q Zhang, Z Wang, arXiv:2410.106572024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
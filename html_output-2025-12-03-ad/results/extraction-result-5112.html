<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5112 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5112</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5112</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-02186caef0f02305f85ceaf188f2ed8773e39217</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/02186caef0f02305f85ceaf188f2ed8773e39217" target="_blank">A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper Abstract:</strong> A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans’ inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate such human biases, or are they able to overcome them? Focusing on the case of syllogisms—inferences from two simple premises—we show that, within the PaLM 2 family of transformer language models, larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases: they show sensitivity to the (irrelevant) ordering of the variables in the syllogism, and draw confident but incorrect inferences from particular syllogisms (syllogistic fallacies). Overall, we find that language models often mimic the human biases included in their training data, but are able to overcome them in some cases.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5112.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5112.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 family of transformer language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of pretrained transformer language models trained on multilingual web text, books, code, mathematics and conversations; evaluated in four publicly available sizes (XXS, XS, S, L) on deductive syllogistic reasoning using zero-shot chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM 2 Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer autoregressive language models trained with self-supervised objectives on large multi-domain corpora (web documents, books, code, math, conversations). Publicly available in four sizes labelled XXS, XS, S, and L. Models used here are pretrained-only (no task fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XXS, XS, S, L</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Syllogistic reasoning benchmark (classical 64 syllogisms; 27 syllogisms that license non-'nothing follows' conclusions evaluated in detail; 64×30 = 1920 total items with 30 content triples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Deductive syllogistic inference: given two premises relating three terms with quantifiers {all, some, none, some are not} and a particular variable ordering (four figures), determine which of eight possible quantified conclusions (or 'nothing follows') necessarily follows based only on the premises (no world knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought (CoT) generative prompting instructing the model to 'think step by step', with randomized ordering of answer options; generative sampling with rejection sampling to detect explicit conclusion strings; 30 samples per problem, temperature 0.5, max decode length 75 tokens. Alternative evaluation methods tested: multiple-choice scoring via mutual information and binary validity discrimination via mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy increases with model size; the best PaLM 2 model achieves roughly 75% accuracy on the subset of syllogisms with valid non-'nothing follows' conclusions (paper quote: 'only about 75%'), exceeding average human accuracy (~50%) on the same dataset. Performance varies widely by syllogism type: some syllogisms remain very low accuracy even for large models. PaLM 2 models show high correlation with human distributions over responses (PaLM 2 Small had the highest correlation) while also achieving higher-than-human accuracy for many syllogisms. On the 37 syllogisms whose only correct answer is 'nothing follows', PaLM 2 models almost never output 'nothing follows' (accuracy near 0% under the CoT generative method).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Systematic errors persist: (1) strong aversion to responding 'nothing follows' using the generative CoT method, leading to near-zero accuracy on those 37 syllogisms; (2) sensitivity to variable ordering ('figural' effect) even when ordering is logically irrelevant; (3) syllogistic fallacies—high-confidence consistent incorrect answers for particular syllogisms; (4) inconsistent scaling: larger models often perform better but improvements are not monotonic (the largest model sometimes underperforms the second-largest on some metrics); (5) some syllogisms remain very challenging across sizes. Models may be influenced by training data composition (authors hypothesize inclusion of code may help).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to humans: larger PaLM 2 models are more accurate on average (best ~75% vs human ~50%) and sometimes solve syllogisms humans struggle with, but they also replicate several human biases (variable-ordering effects, certain fallacies). Compared to random guessing: substantially above chance for syllogisms with valid conclusions. Compared to Llama 2: PaLM 2 generally achieves higher accuracy and higher correlation with human response distributions. Compared to cognitive model (mReasoner): PaLM 2 behavior projects onto mReasoner-derived principal components, with larger models showing higher scores on a deliberation-like component.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Prompt variants tested: 'stepxstep' (the CoT prompt used in main text) yielded the highest accuracy among tested prompts; variations ('logically', 'empty', 'alt') gave broadly similar patterns. Decoding hyperparameters: small accuracy gains with increased decoding length (50→75→100 tokens) and modest effects across temperatures {0.25,0.5,0.75}. Sampling: used 30 samples per item with rejection of samples that did not contain an exact conclusion string. Evaluation method comparison: generative CoT achieved highest accuracy on syllogisms with valid conclusions; multiple-choice MI and binary discrimination methods performed worse on those syllogisms, but binary discrimination produced substantially more 'nothing follows' responses and achieved higher correlation with humans for that subset. Size analysis: accuracy generally increases with scale but not strictly; projection into mReasoner PCA space shows larger PaLM 2 models have higher PC1 (77% of mReasoner variance) associated with higher SYSTM2 and WEAKEN (deliberative reasoning); controlling for accuracy (setting correct-answer probabilities to zero) preserves the deliberation signature, indicating the effect is not solely due to increased accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5112.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5112.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 family of transformer language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open foundation transformer LMs evaluated here in 7B, 13B and 70B parameter sizes on the same syllogistic reasoning suite using the same zero-shot CoT prompt (preliminary analyses; fewer hyperparameter sweeps).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open foundation and fine-tuned chat models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer autoregressive/foundation models from Meta in multiple parameter counts; here the 7B, 13B and 70B variants were evaluated with zero-shot chain-of-thought prompting; experiments regarded as preliminary due to limited hyperparameter exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Same syllogistic reasoning benchmark as used for PaLM 2 (64 syllogisms × 30 content triples = 1920 items).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Deductive syllogistic inference (see PaLM 2 entry).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought generative prompting (same prompt family as for PaLM 2). Evaluation and sampling were analogous, but with fewer hyperparameter variants explored.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall aggregated accuracy across syllogisms was similar to human accuracy with a modest increase as model size increased. Per-syllogism breakdown revealed discrepancies: Llama 2 struggled with some syllogisms that humans find easy (e.g., 'some A are B, all B are C'), while achieving high accuracy on some syllogisms humans struggle with. Llama 2 displayed a human-like variable ordering (figural) effect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Unlike PaLM 2, Llama 2 models exhibited poorer alignment with human response distributions overall (lower correlation) and inconsistent scaling benefits; they struggle on certain syllogisms humans solve reliably. Projection into the mReasoner PCA space initially suggested increased deliberative signatures with scale, but that effect vanished when controlling for accuracy (i.e., no robust size→deliberation effect after accuracy control).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to PaLM 2: Llama 2 performed worse in aggregate human-correlation and showed more per-syllogism mismatches (struggling on some syllogisms humans find easy). Compared to humans: aggregate accuracy near human level, but distributions differ; lower correlation with human response distributions. Compared to mReasoner projection: weaker or non-robust deliberation signature after controlling for accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Limited hyperparameter exploration prevented exhaustive ablations. Observed modest scaling improvement (7B→70B) in aggregate accuracy, but inconsistent per-syllogism. Projection into mReasoner PC space showed an apparent deliberative trend with size when not controlling for accuracy, but that trend disappeared when controlling for accuracy (set correct-answer probabilities to zero) — unlike PaLM 2, where deliberation signatures persist after that control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5112.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5112.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot chain-of-thought prompting ('Let's think this through, step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that instructs an LM to produce intermediate reasoning steps (a 'chain-of-thought') in a zero-shot setting to elicit more explicit reasoning traces and improve deductive reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering technique: the prompt describes the task and possible conclusions and ends with a reasoning trigger such as 'Let's think this through, step by step' to encourage the model to produce step-by-step reasoning and explicit conclusion strings. Used with generative sampling and a rejection/selection mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Syllogistic reasoning benchmark (paper's primary task).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elicits stepwise reasoning traces to help models produce logically justified conclusions rather than directly emitting a label.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot generative CoT prompt (referred to as 'stepxstep' in the paper) with randomized ordering of listed possible conclusions; sample generation (30 samples, temperature 0.5, max tokens 75), then detect and take the most frequent explicit conclusion string across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Among the elicitation methods tested, generative CoT achieved the highest accuracy on the 27 syllogisms that license explicit valid conclusions (it outperformed multiple-choice MI and binary discrimination methods on those syllogisms). CoT produced poor rates of 'nothing follows' answers (near 0% for PaLM 2 models) under the generative decoding scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Strong reluctance to output 'nothing follows' with the generative CoT formulation used here (severely lowering accuracy on the 37 'nothing follows' syllogisms). Sensitivity to answer ordering in the prompt was controlled but remains a factor in elicitation. CoT performance depends on decoding hyperparameters (temperature, decode length) and sampling strategy; the authors used rejection sampling to ensure detection of explicit conclusion strings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to multiple-choice MI scoring and binary discrimination: CoT yielded higher accuracy on valid-conclusion syllogisms; binary discrimination produced substantially more appropriate 'nothing follows' outputs and better correlation with humans for those cases. Prompt variants ('logically', 'empty', 'alt') were tested; 'stepxstep' was best overall in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Prompt ablation: 'stepxstep' performed best among variants; removing or changing the reasoning trigger reduced accuracy slightly. Decoding hyperparameters: increasing decode length modestly improved accuracy; temperature variations produced only small accuracy changes. The rejection-sampling detection procedure (30 samples) was part of method; alternative scoring (mutual information over answer choices) produced different behavior (see multiple-choice and binary methods).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5112.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5112.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binary discrimination (MI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Validity binary-discrimination via mutual information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative evaluation method that queries the model about the validity of one conclusion at a time and uses mutual information scores between the prompt and 'valid'/'invalid' labels to compute per-conclusion validity probabilities, then normalizes across conclusions to select the model's answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Binary discrimination (valid/invalid MI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluation protocol: for each candidate conclusion, present the premises plus that single conclusion and ask 'Is this conclusion valid given the premises:', then compute mutual information scores for 'valid' vs 'invalid' tokens given that prompt. Convert those scores into probabilities and normalize across candidate conclusions; if none exceed threshold, predict 'nothing follows'.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Syllogistic reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Validity discrimination for each candidate quantified conclusion (binary judgement valid/invalid) and normalization over candidates to select conclusion or 'nothing follows'.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Mutual information scoring for 'valid'/'invalid' given prompt+conclusion, probability normalization across conclusions; a conclusion is selected if its 'valid' probability is highest or 'nothing follows' is selected if no conclusion exceeds 50% valid.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>This method achieved the highest rate of correctly producing 'nothing follows' among the three elicitation methods tested (generative CoT, multiple-choice MI, binary discrimination), and also achieved the highest correlation with human responses for the 'nothing follows' set. However, it achieved lower accuracy on the valid-conclusion syllogisms than the generative CoT method.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower accuracy on syllogisms with explicit valid conclusions compared to the generative CoT approach; requires separate queries for each candidate conclusion (more computationally intensive). Relies on careful mutual information estimation; sensitive to choice of baseline (the authors used unconditional probability).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared against generative CoT and multiple-choice MI: best at eliciting 'nothing follows' and better human-correlation for that subset, but worse on explicit-valid-conclusion syllogisms. Multiple-choice MI performed worst among the three overall.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Binary discrimination consistently elicited more 'nothing follows' responses; authors note this method may be promising to explore further because it aligns better with human behavior on 'nothing follows' cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5112.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5112.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mReasoner (implementation of Mental Models Theory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational implementation of Mental Models Theory used to simulate human syllogistic reasoning; parameterized by LEN, BROAD, SYSTM2, and WEAKEN and used here as an interpretable behavioral space onto which LM behavior was projected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reasoning about properties: A computational theory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Computational cognitive model that constructs and revises small mental models (entities with conjunctive properties) to draw syllogistic conclusions. Key hyperparameters: LEN (expected number of entities), BROAD (sampling from broader vs canonical sets), SYSTM2 (propensity to run System 2 counterexample searches), WEAKEN (tendency to weaken conclusions vs answering 'nothing follows').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Modeling human performance on the syllogistic reasoning benchmark and providing interpretable dimensions (deliberation vs heuristics) to describe LM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generative cognitive simulation of syllogistic reasoning: per-syllogism stochastic simulation producing distributions over the eight candidate conclusions. Used to generate a grid of behaviors (1296 parameter instantiations, 923 completed) that were then reduced via PCA to principal components representing cognitive tendencies (PC1 ~ deliberation).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Instantiate mReasoner across a parameter grid (LEN ∈ {2.0..4.5}, BROAD, SYSTM2, WEAKEN ∈ {0.0..0.9}), run stochastic simulations (100 runs per syllogism per instance), represent each instance as a 216-dim vector (27 valid syllogisms × 8 conclusions), perform probabilistic PCA and project LM and human response distributions into this space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PCA: first principal component (PC1) captured ~77% of behavioral variance across mReasoner instances and loaded heavily on SYSTM2 (and to a lesser extent WEAKEN), which the authors interpret as a deliberative reasoning axis. Mapping shows larger PaLM 2 models have higher PC1 values (more deliberative), and this correlation with SYSTM2 persists even when controlling for accuracy (setting correct-answer probabilities to zero), indicating that larger models' error patterns align more with deliberative strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>mReasoner is one specific cognitive model (Mental Models theory); it is not unique—other cognitive or algorithmic explanations could account for LM behavior. Some mReasoner instantiations timed out and were discarded (923/1296 retained), though discarded instances did not systematically bias the grid. The mapping is interpretative (assumes LM behavior can be projected onto human cognitive dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Used as a cognitive interpretive baseline to compare LM behavior against human-like heuristics and deliberative processes. mReasoner parameter loadings provide interpretable axes (e.g., deliberation) that LMs can be compared to; PaLM 2 exhibits increasing deliberative signature with size while Llama 2's deliberative signature disappears after controlling for accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5112.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5112.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syllogism dataset (Ragni et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>When does a reasoner respond: Nothing follows?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human behavioral dataset of syllogistic reasoning used as the human ground truth: 139 participants each responded to 64 classical syllogisms instantiated with 30 content triples (1920 stimuli); participants chose among eight possible quantified conclusions or 'nothing follows'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When does a reasoner respond: Nothing follows?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ragni et al. (2019) syllogistic dataset</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset for syllogistic reasoning experiments: 64 syllogism types (combining four quantifiers and two variable-orderings per premise) instantiated with 30 triples of minimally associated content words (e.g., hunters, analysts, swimmers), producing 1920 unique problems; human responses were collected after a brief training phase and each participant responded once to each syllogism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Classic syllogistic reasoning (as used in cognitive psychology): determine necessary quantified relations between terms A and C given premises relating A–B and B–C using {A (all), I (some), E (none), O (some are not)} moods and four variable-orderings (figures).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to evaluate both human and model deductions, compute accuracy, entropy, distribution over responses, correlations, and identify syllogistic fallacies (low entropy, low accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Human behavioral baseline collected by Ragni et al.; in this paper the dataset is used to (a) compute human response distributions and accuracies, (b) compare LM output distributions to humans via correlations, (c) identify syllogisms that license 'nothing follows' versus those with valid conclusions, and (d) derive per-syllogism analyses of errors, entropies, variable-ordering effects, and fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Human average accuracy across all syllogisms ~50% (paper reports ~50%); humans rarely answer 'nothing follows' when it is correct, and display systematic biases: variable-ordering (figural) effects, content effects, and particular syllogistic fallacies where most participants choose the same incorrect conclusion with low entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Human responses reflect content effects and systematic biases; dataset focuses on classical syllogisms with artificially minimized semantic associations (30 triples chosen to be minimally associated), which controls for world-knowledge effects but still exhibits non-logical biases in human judgments. The human experimental paradigm differs from LM evaluation paradigms (e.g., training, feedback), complicating direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PaLM: Scaling Language Modeling with Pathways <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Reasoning about properties: A computational theory <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>When does a reasoner respond: Nothing follows? <em>(Rating: 2)</em></li>
                <li>Llama 2: Open foundation and fine-tuned chat models <em>(Rating: 1)</em></li>
                <li>FOLIO: Natural Language Reasoning with First-Order Logic <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5112",
    "paper_id": "paper-02186caef0f02305f85ceaf188f2ed8773e39217",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "PaLM 2",
            "name_full": "PaLM 2 family of transformer language models",
            "brief_description": "A family of pretrained transformer language models trained on multilingual web text, books, code, mathematics and conversations; evaluated in four publicly available sizes (XXS, XS, S, L) on deductive syllogistic reasoning using zero-shot chain-of-thought prompting.",
            "citation_title": "PaLM 2 Technical Report",
            "mention_or_use": "use",
            "model_name": "PaLM 2",
            "model_description": "Transformer autoregressive language models trained with self-supervised objectives on large multi-domain corpora (web documents, books, code, math, conversations). Publicly available in four sizes labelled XXS, XS, S, and L. Models used here are pretrained-only (no task fine-tuning).",
            "model_size": "XXS, XS, S, L",
            "logical_reasoning_task": "Syllogistic reasoning benchmark (classical 64 syllogisms; 27 syllogisms that license non-'nothing follows' conclusions evaluated in detail; 64×30 = 1920 total items with 30 content triples)",
            "task_description": "Deductive syllogistic inference: given two premises relating three terms with quantifiers {all, some, none, some are not} and a particular variable ordering (four figures), determine which of eight possible quantified conclusions (or 'nothing follows') necessarily follows based only on the premises (no world knowledge).",
            "method_or_approach": "Zero-shot chain-of-thought (CoT) generative prompting instructing the model to 'think step by step', with randomized ordering of answer options; generative sampling with rejection sampling to detect explicit conclusion strings; 30 samples per problem, temperature 0.5, max decode length 75 tokens. Alternative evaluation methods tested: multiple-choice scoring via mutual information and binary validity discrimination via mutual information.",
            "performance": "Average accuracy increases with model size; the best PaLM 2 model achieves roughly 75% accuracy on the subset of syllogisms with valid non-'nothing follows' conclusions (paper quote: 'only about 75%'), exceeding average human accuracy (~50%) on the same dataset. Performance varies widely by syllogism type: some syllogisms remain very low accuracy even for large models. PaLM 2 models show high correlation with human distributions over responses (PaLM 2 Small had the highest correlation) while also achieving higher-than-human accuracy for many syllogisms. On the 37 syllogisms whose only correct answer is 'nothing follows', PaLM 2 models almost never output 'nothing follows' (accuracy near 0% under the CoT generative method).",
            "limitations_or_failure_cases": "Systematic errors persist: (1) strong aversion to responding 'nothing follows' using the generative CoT method, leading to near-zero accuracy on those 37 syllogisms; (2) sensitivity to variable ordering ('figural' effect) even when ordering is logically irrelevant; (3) syllogistic fallacies—high-confidence consistent incorrect answers for particular syllogisms; (4) inconsistent scaling: larger models often perform better but improvements are not monotonic (the largest model sometimes underperforms the second-largest on some metrics); (5) some syllogisms remain very challenging across sizes. Models may be influenced by training data composition (authors hypothesize inclusion of code may help).",
            "comparison": "Compared to humans: larger PaLM 2 models are more accurate on average (best ~75% vs human ~50%) and sometimes solve syllogisms humans struggle with, but they also replicate several human biases (variable-ordering effects, certain fallacies). Compared to random guessing: substantially above chance for syllogisms with valid conclusions. Compared to Llama 2: PaLM 2 generally achieves higher accuracy and higher correlation with human response distributions. Compared to cognitive model (mReasoner): PaLM 2 behavior projects onto mReasoner-derived principal components, with larger models showing higher scores on a deliberation-like component.",
            "ablation_or_analysis_results": "Prompt variants tested: 'stepxstep' (the CoT prompt used in main text) yielded the highest accuracy among tested prompts; variations ('logically', 'empty', 'alt') gave broadly similar patterns. Decoding hyperparameters: small accuracy gains with increased decoding length (50→75→100 tokens) and modest effects across temperatures {0.25,0.5,0.75}. Sampling: used 30 samples per item with rejection of samples that did not contain an exact conclusion string. Evaluation method comparison: generative CoT achieved highest accuracy on syllogisms with valid conclusions; multiple-choice MI and binary discrimination methods performed worse on those syllogisms, but binary discrimination produced substantially more 'nothing follows' responses and achieved higher correlation with humans for that subset. Size analysis: accuracy generally increases with scale but not strictly; projection into mReasoner PCA space shows larger PaLM 2 models have higher PC1 (77% of mReasoner variance) associated with higher SYSTM2 and WEAKEN (deliberative reasoning); controlling for accuracy (setting correct-answer probabilities to zero) preserves the deliberation signature, indicating the effect is not solely due to increased accuracy.",
            "uuid": "e5112.0",
            "source_info": {
                "paper_title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Llama 2",
            "name_full": "Llama 2 family of transformer language models",
            "brief_description": "Open foundation transformer LMs evaluated here in 7B, 13B and 70B parameter sizes on the same syllogistic reasoning suite using the same zero-shot CoT prompt (preliminary analyses; fewer hyperparameter sweeps).",
            "citation_title": "Llama 2: Open foundation and fine-tuned chat models",
            "mention_or_use": "use",
            "model_name": "Llama 2",
            "model_description": "Transformer autoregressive/foundation models from Meta in multiple parameter counts; here the 7B, 13B and 70B variants were evaluated with zero-shot chain-of-thought prompting; experiments regarded as preliminary due to limited hyperparameter exploration.",
            "model_size": "7B, 13B, 70B",
            "logical_reasoning_task": "Same syllogistic reasoning benchmark as used for PaLM 2 (64 syllogisms × 30 content triples = 1920 items).",
            "task_description": "Deductive syllogistic inference (see PaLM 2 entry).",
            "method_or_approach": "Zero-shot chain-of-thought generative prompting (same prompt family as for PaLM 2). Evaluation and sampling were analogous, but with fewer hyperparameter variants explored.",
            "performance": "Overall aggregated accuracy across syllogisms was similar to human accuracy with a modest increase as model size increased. Per-syllogism breakdown revealed discrepancies: Llama 2 struggled with some syllogisms that humans find easy (e.g., 'some A are B, all B are C'), while achieving high accuracy on some syllogisms humans struggle with. Llama 2 displayed a human-like variable ordering (figural) effect.",
            "limitations_or_failure_cases": "Unlike PaLM 2, Llama 2 models exhibited poorer alignment with human response distributions overall (lower correlation) and inconsistent scaling benefits; they struggle on certain syllogisms humans solve reliably. Projection into the mReasoner PCA space initially suggested increased deliberative signatures with scale, but that effect vanished when controlling for accuracy (i.e., no robust size→deliberation effect after accuracy control).",
            "comparison": "Compared to PaLM 2: Llama 2 performed worse in aggregate human-correlation and showed more per-syllogism mismatches (struggling on some syllogisms humans find easy). Compared to humans: aggregate accuracy near human level, but distributions differ; lower correlation with human response distributions. Compared to mReasoner projection: weaker or non-robust deliberation signature after controlling for accuracy.",
            "ablation_or_analysis_results": "Limited hyperparameter exploration prevented exhaustive ablations. Observed modest scaling improvement (7B→70B) in aggregate accuracy, but inconsistent per-syllogism. Projection into mReasoner PC space showed an apparent deliberative trend with size when not controlling for accuracy, but that trend disappeared when controlling for accuracy (set correct-answer probabilities to zero) — unlike PaLM 2, where deliberation signatures persist after that control.",
            "uuid": "e5112.1",
            "source_info": {
                "paper_title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Zero-shot CoT",
            "name_full": "Zero-shot chain-of-thought prompting ('Let's think this through, step by step')",
            "brief_description": "A prompting method that instructs an LM to produce intermediate reasoning steps (a 'chain-of-thought') in a zero-shot setting to elicit more explicit reasoning traces and improve deductive reasoning performance.",
            "citation_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "Zero-shot chain-of-thought prompting",
            "model_description": "Prompt engineering technique: the prompt describes the task and possible conclusions and ends with a reasoning trigger such as 'Let's think this through, step by step' to encourage the model to produce step-by-step reasoning and explicit conclusion strings. Used with generative sampling and a rejection/selection mechanism.",
            "model_size": null,
            "logical_reasoning_task": "Syllogistic reasoning benchmark (paper's primary task).",
            "task_description": "Elicits stepwise reasoning traces to help models produce logically justified conclusions rather than directly emitting a label.",
            "method_or_approach": "Zero-shot generative CoT prompt (referred to as 'stepxstep' in the paper) with randomized ordering of listed possible conclusions; sample generation (30 samples, temperature 0.5, max tokens 75), then detect and take the most frequent explicit conclusion string across samples.",
            "performance": "Among the elicitation methods tested, generative CoT achieved the highest accuracy on the 27 syllogisms that license explicit valid conclusions (it outperformed multiple-choice MI and binary discrimination methods on those syllogisms). CoT produced poor rates of 'nothing follows' answers (near 0% for PaLM 2 models) under the generative decoding scheme.",
            "limitations_or_failure_cases": "Strong reluctance to output 'nothing follows' with the generative CoT formulation used here (severely lowering accuracy on the 37 'nothing follows' syllogisms). Sensitivity to answer ordering in the prompt was controlled but remains a factor in elicitation. CoT performance depends on decoding hyperparameters (temperature, decode length) and sampling strategy; the authors used rejection sampling to ensure detection of explicit conclusion strings.",
            "comparison": "Compared to multiple-choice MI scoring and binary discrimination: CoT yielded higher accuracy on valid-conclusion syllogisms; binary discrimination produced substantially more appropriate 'nothing follows' outputs and better correlation with humans for those cases. Prompt variants ('logically', 'empty', 'alt') were tested; 'stepxstep' was best overall in their experiments.",
            "ablation_or_analysis_results": "Prompt ablation: 'stepxstep' performed best among variants; removing or changing the reasoning trigger reduced accuracy slightly. Decoding hyperparameters: increasing decode length modestly improved accuracy; temperature variations produced only small accuracy changes. The rejection-sampling detection procedure (30 samples) was part of method; alternative scoring (mutual information over answer choices) produced different behavior (see multiple-choice and binary methods).",
            "uuid": "e5112.2",
            "source_info": {
                "paper_title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Binary discrimination (MI)",
            "name_full": "Validity binary-discrimination via mutual information",
            "brief_description": "A discriminative evaluation method that queries the model about the validity of one conclusion at a time and uses mutual information scores between the prompt and 'valid'/'invalid' labels to compute per-conclusion validity probabilities, then normalizes across conclusions to select the model's answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Binary discrimination (valid/invalid MI)",
            "model_description": "Evaluation protocol: for each candidate conclusion, present the premises plus that single conclusion and ask 'Is this conclusion valid given the premises:', then compute mutual information scores for 'valid' vs 'invalid' tokens given that prompt. Convert those scores into probabilities and normalize across candidate conclusions; if none exceed threshold, predict 'nothing follows'.",
            "model_size": null,
            "logical_reasoning_task": "Syllogistic reasoning benchmark.",
            "task_description": "Validity discrimination for each candidate quantified conclusion (binary judgement valid/invalid) and normalization over candidates to select conclusion or 'nothing follows'.",
            "method_or_approach": "Mutual information scoring for 'valid'/'invalid' given prompt+conclusion, probability normalization across conclusions; a conclusion is selected if its 'valid' probability is highest or 'nothing follows' is selected if no conclusion exceeds 50% valid.",
            "performance": "This method achieved the highest rate of correctly producing 'nothing follows' among the three elicitation methods tested (generative CoT, multiple-choice MI, binary discrimination), and also achieved the highest correlation with human responses for the 'nothing follows' set. However, it achieved lower accuracy on the valid-conclusion syllogisms than the generative CoT method.",
            "limitations_or_failure_cases": "Lower accuracy on syllogisms with explicit valid conclusions compared to the generative CoT approach; requires separate queries for each candidate conclusion (more computationally intensive). Relies on careful mutual information estimation; sensitive to choice of baseline (the authors used unconditional probability).",
            "comparison": "Compared against generative CoT and multiple-choice MI: best at eliciting 'nothing follows' and better human-correlation for that subset, but worse on explicit-valid-conclusion syllogisms. Multiple-choice MI performed worst among the three overall.",
            "ablation_or_analysis_results": "Binary discrimination consistently elicited more 'nothing follows' responses; authors note this method may be promising to explore further because it aligns better with human behavior on 'nothing follows' cases.",
            "uuid": "e5112.3",
            "source_info": {
                "paper_title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "mReasoner",
            "name_full": "mReasoner (implementation of Mental Models Theory)",
            "brief_description": "A computational implementation of Mental Models Theory used to simulate human syllogistic reasoning; parameterized by LEN, BROAD, SYSTM2, and WEAKEN and used here as an interpretable behavioral space onto which LM behavior was projected.",
            "citation_title": "Reasoning about properties: A computational theory",
            "mention_or_use": "use",
            "model_name": "mReasoner",
            "model_description": "Computational cognitive model that constructs and revises small mental models (entities with conjunctive properties) to draw syllogistic conclusions. Key hyperparameters: LEN (expected number of entities), BROAD (sampling from broader vs canonical sets), SYSTM2 (propensity to run System 2 counterexample searches), WEAKEN (tendency to weaken conclusions vs answering 'nothing follows').",
            "model_size": null,
            "logical_reasoning_task": "Modeling human performance on the syllogistic reasoning benchmark and providing interpretable dimensions (deliberation vs heuristics) to describe LM behavior.",
            "task_description": "Generative cognitive simulation of syllogistic reasoning: per-syllogism stochastic simulation producing distributions over the eight candidate conclusions. Used to generate a grid of behaviors (1296 parameter instantiations, 923 completed) that were then reduced via PCA to principal components representing cognitive tendencies (PC1 ~ deliberation).",
            "method_or_approach": "Instantiate mReasoner across a parameter grid (LEN ∈ {2.0..4.5}, BROAD, SYSTM2, WEAKEN ∈ {0.0..0.9}), run stochastic simulations (100 runs per syllogism per instance), represent each instance as a 216-dim vector (27 valid syllogisms × 8 conclusions), perform probabilistic PCA and project LM and human response distributions into this space.",
            "performance": "PCA: first principal component (PC1) captured ~77% of behavioral variance across mReasoner instances and loaded heavily on SYSTM2 (and to a lesser extent WEAKEN), which the authors interpret as a deliberative reasoning axis. Mapping shows larger PaLM 2 models have higher PC1 values (more deliberative), and this correlation with SYSTM2 persists even when controlling for accuracy (setting correct-answer probabilities to zero), indicating that larger models' error patterns align more with deliberative strategies.",
            "limitations_or_failure_cases": "mReasoner is one specific cognitive model (Mental Models theory); it is not unique—other cognitive or algorithmic explanations could account for LM behavior. Some mReasoner instantiations timed out and were discarded (923/1296 retained), though discarded instances did not systematically bias the grid. The mapping is interpretative (assumes LM behavior can be projected onto human cognitive dimensions).",
            "comparison": "Used as a cognitive interpretive baseline to compare LM behavior against human-like heuristics and deliberative processes. mReasoner parameter loadings provide interpretable axes (e.g., deliberation) that LMs can be compared to; PaLM 2 exhibits increasing deliberative signature with size while Llama 2's deliberative signature disappears after controlling for accuracy.",
            "uuid": "e5112.4",
            "source_info": {
                "paper_title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Syllogism dataset (Ragni et al. 2019)",
            "name_full": "When does a reasoner respond: Nothing follows?",
            "brief_description": "Human behavioral dataset of syllogistic reasoning used as the human ground truth: 139 participants each responded to 64 classical syllogisms instantiated with 30 content triples (1920 stimuli); participants chose among eight possible quantified conclusions or 'nothing follows'.",
            "citation_title": "When does a reasoner respond: Nothing follows?",
            "mention_or_use": "use",
            "model_name": "Ragni et al. (2019) syllogistic dataset",
            "model_description": "Dataset for syllogistic reasoning experiments: 64 syllogism types (combining four quantifiers and two variable-orderings per premise) instantiated with 30 triples of minimally associated content words (e.g., hunters, analysts, swimmers), producing 1920 unique problems; human responses were collected after a brief training phase and each participant responded once to each syllogism.",
            "model_size": null,
            "logical_reasoning_task": "Classic syllogistic reasoning (as used in cognitive psychology): determine necessary quantified relations between terms A and C given premises relating A–B and B–C using {A (all), I (some), E (none), O (some are not)} moods and four variable-orderings (figures).",
            "task_description": "Used to evaluate both human and model deductions, compute accuracy, entropy, distribution over responses, correlations, and identify syllogistic fallacies (low entropy, low accuracy).",
            "method_or_approach": "Human behavioral baseline collected by Ragni et al.; in this paper the dataset is used to (a) compute human response distributions and accuracies, (b) compare LM output distributions to humans via correlations, (c) identify syllogisms that license 'nothing follows' versus those with valid conclusions, and (d) derive per-syllogism analyses of errors, entropies, variable-ordering effects, and fallacies.",
            "performance": "Human average accuracy across all syllogisms ~50% (paper reports ~50%); humans rarely answer 'nothing follows' when it is correct, and display systematic biases: variable-ordering (figural) effects, content effects, and particular syllogistic fallacies where most participants choose the same incorrect conclusion with low entropy.",
            "limitations_or_failure_cases": "Human responses reflect content effects and systematic biases; dataset focuses on classical syllogisms with artificially minimized semantic associations (30 triples chosen to be minimally associated), which controls for world-knowledge effects but still exhibits non-logical biases in human judgments. The human experimental paradigm differs from LM evaluation paradigms (e.g., training, feedback), complicating direct comparisons.",
            "uuid": "e5112.5",
            "source_info": {
                "paper_title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Reasoning about properties: A computational theory",
            "rating": 2
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2
        },
        {
            "paper_title": "When does a reasoner respond: Nothing follows?",
            "rating": 2
        },
        {
            "paper_title": "Llama 2: Open foundation and fine-tuned chat models",
            "rating": 1
        },
        {
            "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "rating": 1
        }
    ],
    "cost": 0.0204945,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models</h1>
<p>Tiwalayo Eisape, ${ }^{\dagger}$ MH Tessler, ${ }^{\ddagger}$ Ishita Dasgupta, ${ }^{\ddagger}$ Fei Sha, ${ }^{\S}$<br>Sjoerd van Steenkiste, ${ }^{\S, <em>}$ Tal Linzen ${ }^{\S, </em>}$<br>Massachusetts Institute of Technology ${ }^{\dagger}$, Google DeepMind ${ }^{\ddagger}$, Google Research ${ }^{\S}$</p>
<h2>Abstract</h2>
<p>A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate such human biases, or are they able to overcome them? Focusing on the case of syllogisms-inferences from two simple premises-we show that, within the PaLM 2 family of transformer language models, larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases: they show sensitivity to the (irrelevant) ordering of the variables in the syllogism, and draw confident but incorrect inferences from particular syllogisms (syllogistic fallacies). Overall, we find that language models often mimic the human biases included in their training data, but are able to overcome them in some cases.</p>
<h2>1 Introduction</h2>
<p>The capacity to reason deductively-that is, to determine which inferences, if any, follow from a given set of premises-is central to rational thought (Newell and Simon, 1972; Laird et al., 1987; Fodor and Pylyshyn, 1988; Griffiths et al., 2010). Despite the importance of this capacity, human reasoning often displays systematic biases (Gigerenzer and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Gaissmaier, 2011; Marcus, 2009; Kahneman, 2013; McClelland et al., 2010). In recent years, language models (LMs) trained with self-supervised objectives have been reported to display a range of capabilities, including the ability to reason (Brown et al., 2020; Chowdhery et al., 2022; Bubeck et al., 2023). Does LMs' logical reasoning follow the rules of logic to a greater extent than humans'? To the extent that LMs' reasoning deviates from normative logic, are their biases similar to humans' (Binz and Schulz, 2023; Dasgupta et al., 2022)?</p>
<p>In this work, we address these questions with a detailed study of a particularly simple caseinferences from pairs of premises, or syllogisms, such as the following:</p>
<div class="codehilite"><pre><span></span><code><span class="k">If</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">bakers</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">artists</span>,
<span class="w">    </span><span class="nv">and</span><span class="w"> </span><span class="nv">some</span><span class="w"> </span><span class="nv">bakers</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">chemists</span>,
</code></pre></div>

<p>then: some artists are chemists.</p>
<p>In a syllogism, each premise relates two terms with one of four quantifiers (traditionally known as "moods"): all, some, none and some are not. Only one term is shared between the premises (bakers in the example above). Inference is required to determine if there is a necessary relationship between the two remaining terms (here, artists and chemists) when the premises in question are true.</p>
<p>When human participants in experiments are asked to make syllogistic inferences, their responses often deviate from the rules of logic; in fact, for some syllogisms the vast majority of participants draw incorrect inferences (Khemlani and Johnson-Laird, 2012). This could pose a challenge to language models (LMs), as they learn from corpora consisting primarily of human-generated texts-texts which, in turn, reflect human beliefs and inferences. Is there sufficient signal in the training corpus to steer LMs away from (often incorrect)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">A:</th>
<th style="text-align: left;">All artists are bakers</th>
<th style="text-align: left;">I:</th>
<th style="text-align: left;">Some artists are bakers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E:</td>
<td style="text-align: left;">No artists are bakers</td>
<td style="text-align: left;">O:</td>
<td style="text-align: left;">Some artists are not bakers</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">1</th>
<th style="text-align: left;">2</th>
<th style="text-align: left;">3</th>
<th style="text-align: left;">4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A-B</td>
<td style="text-align: left;">B-A</td>
<td style="text-align: left;">A-B</td>
<td style="text-align: left;">B-A</td>
</tr>
<tr>
<td style="text-align: left;">B-C</td>
<td style="text-align: left;">C-B</td>
<td style="text-align: left;">C-B</td>
<td style="text-align: left;">B-C</td>
</tr>
</tbody>
</table>
<p>Table 1: Syllogism moods (left) and variable orderings (right).
human inferences and toward a behavior consistent with the normative rules of logic-the behavior that is desirable for most applications?</p>
<p>We address this question in a detailed comparison between the PaLM 2 family of transformer LMs (Google, 2023) and studies from cognitive psychology, as well as a replication with the Llama 2 family of transformer LMs (Touvron et al., 2023). We report the following results:</p>
<ol>
<li>LMs draw correct inferences more often than humans, and larger LMs tend to be more accurate than smaller ones, but the accuracy of even the best performing LM is only about $75 \%$, and scale does not consistently lead to accuracy gains (Section 4.1).</li>
<li>LM errors are systematic, with very low accuracy on particular syllogism types (Section 4.1); the syllogisms that LMs struggle with are a subset of those that humans find difficult (Section 4.2).</li>
<li>Like humans, LMs are sensitive to the ordering of terms in the premises of a syllogism even when it is logically irrelevant (Section 4.2; this pattern is known as the "figural effect" in cognitive psychology; Johnson-Laird and Steedman 1978).</li>
<li>LMs show many of the same syllogistic fallacies, characterized by high confidence and low accuracy, as humans. Larger LMs are somewhat more susceptible to these fallacies than smaller ones (Section 4.2; Khemlani and Johnson-Laird 2017).</li>
<li>Using the Mental Models theory from cognitive psychology, we find quantitative evidence that larger LMs reason more deliberatively than smaller ones (Section 5; Khemlani and JohnsonLaird 2022).</li>
</ol>
<p>Overall, we find that PaLM 2 LMs replicate many of the human biases discovered in psychology studies, consistent with the fact that LMs are
trained on human-generated text. For some syllogisms, however, sufficiently large models overcome those biases and achieve dramatically better accuracy than humans, although their overall accuracy is still far from the perfect logical reasoner.</p>
<h2>2 Background and Related Work</h2>
<h3>2.1 Syllogisms</h3>
<p>Syllogisms are logical arguments consisting of two premises relating three variables, A, B and C (e.g., artists, bakers, and chemists in the example from the introduction). Each premise relates just two of the variables, through one of four quantifiers, often referred to as "moods" (Table 1, left). The variables in each of the premises can be ordered in either of the two directions-e.g., all artists are bakers vs. all bakers are artists-and so there are four possible pairs of orderings (Table 1, right). These orderings are traditionally referred to as "figures", but we will use the more transparent term "variable ordering". Taking the cross product of these building blocks yields 64 possible syllogisms: two premises, each of which can take one of four quantifiers and one of two possible orderings.</p>
<p>Though the premises only relate A and B , or B and C -never A and $\mathrm{C}-27$ of the 64 syllogisms imply a quantified relationship between A and C (e.g., some $A$ are $C$ ). In the remaining 37 syllogisms, no relation between A and C can be deduced; in human experiments, the expected response to these syllogisms is "nothing follows" (see Figure 10 in Appendix A for the full set of valid conclusions for each syllogism).</p>
<h3>2.2 Human Syllogistic Reasoning</h3>
<p>Psychologists, going back to the early 20th century, have found that the conclusions that humans draw from the premises of a syllogism often deviate from logical norms (for a review, see Khemlani and Johnson-Laird 2012). These errors are systematic: some syllogisms are much harder than others, and the incorrect conclusions that participants tend to draw are consistent across participants. For example, from the two premises (1) no artists are bakers and (2) all bakers are chemists, the vast majority of</p>
<p>participants incorrectly conclude that it is the case that no artists are chemists. We analyse such cases in detail in Section 4.2.</p>
<p>In addition to these specific, highly challenging syllogisms, several broader reasoning biases have been documented. When given a syllogistic argument where the variables in the premises are ordered "A-B, B-C", participants show a bias towards conclusions with an A-C ordering, even though reordering the variables in the premises does not affect the conclusions licensed by the syllogism (Johnson-Laird and Steedman, 1978). Participants are also more likely to produce a conclusion when it is true in the real world, independently of whether it follows from the premises ("content effects", Evans et al. 1983).</p>
<p>A number of theories have been proposed to explain human syllogistic reasoning. An influential account that we focus on in this work is the Mental Models Theory (Johnson-Laird and Byrne, 1991). This theory posits that human reasoners construct mental models populated by a small number of entities that instantiate the premises; e.g., to instantiate the premise all artists are bakers, a reasoner might construct a world with three specific artists, all of whom are bakers. These worlds are constructed based on a number of fallible heuristics, and human reasoning errors arise when those heuristics produce incorrect conclusions (see Section 5).</p>
<h3>2.3 Language Models and Reasoning</h3>
<p>LMs trained with self-supervised objectives on large text corpora have been instrumental in achieving high performance on a range of tasks. Some of the tasks in which LMs have shown promise have been referred to as reasoning tasks, including commonsense reasoning, natural language inference, or question answering (e.g., Chowdhery et al. 2022). In this work, we focus more specifically on deductive logical reasoning: drawing conclusions that must, rather than are likely to, be true given the premises, and where the inference is based only on the premises, and does not rely on world knowledge. Unlike work on datasets collected from textbooks or through crowdsourcing, we perform a well-controlled analysis of a simple logical task for which there is a wealth of human data.</p>
<p>Several studies have benchmarked LMs on logical reasoning tasks (Han et al., 2022; BIG-bench collaboration, 2022; Wu et al., 2023a; Betz et al., 2020; Saparov and He, 2022; Saparov et al., 2023; Ye et al., 2023) and examined LM reasoning biases
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The zero-shot chain-of-thought prompt we use to assess LM syllogistic reasoning. The different parts of the prompt are grouped together for illustration purposes only; see also Figure 11 in the Appendix for a purely textual representation of the prompt.
(Dasgupta et al., 2022; Razeghi et al., 2022; Wu et al., 2023b; McCoy et al., 2023). Saparov and He (2022) take a similarly controlled experimental approach to ours (see also Saparov et al. 2023), but they analyze LMs' performance on formal logic rather than problems phrased in natural language as we do, and do not compare their results to humans. The closest study to ours is Dasgupta et al. (2022), which demonstrates content effects in a number of logical reasoning domains, including syllogisms. We extend their approach to study other aspects of syllogistic reasoning.</p>
<h2>3 Methods</h2>
<h3>3.1 Data</h3>
<p>The human behavioral data we use is drawn from Ragni et al. (2019), an online experiment where 139 participants responded once to each of the syllogisms. In each trial, a participant was presented with a syllogism and was instructed to choose among nine options: the eight possible conclusions and "nothing follows". The experimental trials were preceded by a brief training phase where participants were familiarized with the task.</p>
<p>Following Ragni et al. (2019), we generate syllogisms by replacing the abstract terms (A, B, C) in each syllogism with one of 30 content triples</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Accuracy of PaLM 2 models, humans (red), and random guessing (grey). Random guessing accuracy differs by syllogism as some syllogisms have more than one valid conclusion. Syllogisms are partitioned into variable ordering (by row) and ordered by decreasing human accuracy from left to right. The top right inset shows the average accuracy across all syllogisms. Syllogisms are identified with the letters of the moods of the premises (Table 1, left) and the number associated with their variable ordering (Table 1, right).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Correlation between PaLM 2 models' predictions and human predictions. The oracle here is a logically correct reasoner that samples a response at random from all valid responses; the correlation of such an oracle with humans is relatively low as it does not mimic human errors.</p>
<p>chosen such that there is no obvious semantic association between the terms (e.g., one of the triplets included <em>hunters</em>, <em>analysts</em> and <em>swimmers</em>; see Appendix A for the full list). This resulted in 64 × 30 = 1920 unique data points.</p>
<h3>3.2 Models and Inference</h3>
<p>Most of our analyses focus on the PaLM 2 family of LMs, which are publicly available in four sizes (XXS, XS, S, and L; Google 2023). These are transformer LMs trained on a large corpus of multilingual web documents, books, code, mathematics and conversations. We also repeat all of our analyses for the 7B-, 13B- and 70B-parameter versions of the Llama 2 family of transformer models (Touvron et al., 2023). Since, unlike for PaLM 2, we were unable to explore the different hyperparameters of our evaluation method for these models, we regard these results as preliminary and summarize them separately from the PaLM 2 results (Section 4.3 and Appendix E). All of the models we use are pretrained only, without additional finetuning to match human preferences.</p>
<p>Following the emerging standard practice for eliciting reasoning from LMs, we use zero-shot "chain-of-thought" prompting, where the model is instructed to "think step by step" (Kojima et al., 2022; Wei et al., 2022). We speculate that the more explicit reasoning process triggered by such prompts may more closely resemble the behavior of human participants in experiments; for an analysis of alternative prompting strategies that we explored before settling on this one, see Appendix B.1. The prompt we use is illustrated in Figure 1. We randomize the order of the conclusions in the prompt to control for LMs' sensitivity to answer ordering (Pezeshkpour and Hruschka, 2023).</p>
<p>For each of the 1920 reasoning problems, we estimated the distribution over conclusions for each LM with a rejection sampling approach. Samples were rejected if no conclusion was identified via uncased exact string match, and we took the LMs'</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Variable ordering effects in PaLM 2 models and humans. Left: The marginal probabilities of A-C and C-A ordered conclusions. Right: The magnitude of the variable ordering effect (the absolute value of the difference between the C-A probability and the A-C probability).
response to be the conclusion with the highest probability in this distribution. Each distribution was estimated with 30 such proposals generated with a temperature of 0.5 and a maximum decoding length of 75 tokens. See Appendix B for further details and an exploration of the impact of different prompts and decoding parameters.</p>
<h2>4 Results $^{1}$</h2>
<h3>4.1 Do LMs Reason Accurately?</h3>
<p>We first examine the PaLM 2 LMs' behavior on each of the 64 syllogism types separately. In practice the LMs rarely produced the output "nothing follows", which is the correct conclusion for 37 of the syllogisms. We return to this behavior briefly in Section 4.2, but in most of the following analyses, we restrict ourselves to the 27 syllogisms that license conclusions other than "nothing follows" (see Figure 2 for the full pattern of results on each of those 27 syllogisms). We compute the LMs' accuracy for each syllogism by dividing the number of logically valid conclusions produced by the LM by the total number of responses; note that some syllogisms have more than one valid conclusion (as many as four) and so the random baseline in Figure 2 varies by syllogism.</p>
<p>When averaged across all syllogisms, LM accuracy generally improves with scale, with the two largest models exceeding human accuracy. The relationship between scale and accuracy is not unambiguous, however: the largest model has somewhat lower accuracy than the second-largest one. There is considerable variation across syllogisms; for multiple syllogisms, accuracy is very low for all model sizes and can even decrease as model size increases (this is the case, for example, for EA1: no $A$ are $B$, all $B$ are $C$ ).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Do LMs Reason Like Humans?</h3>
<p>Human accuracy averaged across all syllogisms is roughly $50 \%$ (Figure 2; red dashed line); as such, high LM accuracy on this dataset does not necessarily imply humanlike reasoning. A comparison by syllogism type reveals that the syllogisms that PaLM 2 models struggle with are syllogisms that humans also find challenging, but the inverse is not true: multiple syllogisms that are hard for humans are solved correctly by larger models. For example, for the syllogism IE4 (some $B$ are $A$, no $B$ are $C$ ), human accuracy is barely above chance, but PaLM 2 Small and PaLM 2 Large are substantially more accurate.</p>
<p>Comparing the distribution over responses. So far we have focus on the proportion of correct responses. There are eight possible conclusions; is the distribution over all responses, including incorrect ones, similar across humans and PaLM 2 models? To compute the probability distribution over conclusions for each syllogism, we aggregate response counts for each syllogism and normalize them into a probability distribution as in Khemlani and Johnson-Laird (2016). We then correlate the probability estimates from humans with the estimates from PaLM 2 models across the entire dataset (Figure 3; for a by-syllogism breakdown, see Figure 14 in Appendix C). The correlation is fairly high across models, and is highest for PaLM 2 Small.</p>
<p>PaLM 2 Small and Large display both a high correlation with human responses and a higher-than-human accuracy. This suggests that the miscalibration to human data that models accrue due to higher accuracy is offset by a better fit to humans elsewhere in the dataset. The next analyses test this hypothesis, zooming in on two specific biases.</p>
<p>Variable ordering effects. Humans' syllogistic inferences are sensitive to variable ordering, even</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Right: Each syllogism plotted by accuracy (y-axis) and entropy (x-axis) and the regression line relating the two. Dashed lines black lines show the residuals for each of the top three human syllogistic fallacies. Left: The result of correlating PaLM 2's regression residuals with residuals estimated from human data.</p>
<p>when the ordering is logically irrelevant (Johnson-Laird and Steedman, 1978). Specifically, humans produce more conclusions with an A-C variable ordering when reasoning in response to a syllogism presented in ordering 1 (A-B, B-C); and they produce more conclusions with a C-A ordering when presented with a syllogism in ordering 2 (B-A, C-B). We aggregate the human and LM responses across all (A-B, B-C) syllogisms and across all (B-A, C-B) syllogisms separately and normalize the aggregated response counts. All four PaLM 2 models show an ordering effect in the same direction as humans (Figure 4, left). We compute the magnitude of the effect as |<em>P</em>(A-C) - <em>P</em>(C-A)|, where <em>P</em>(A-C) is the probability placed on conclusions with the order A-C. All models display a moderately larger bias than humans (Figure 4, right). We do not find a clear trend in the magnitude of the bias as model size increases; if anything, the largest model shows a slightly weaker bias than the second-largest one.</p>
<p><strong>Syllogistic fallacies.</strong> In general, humans are well-calibrated syllogistic reasoners—their accuracy is inversely correlated with the entropy of their responses (Figure 5; see also Khemlani and Johnson-Laird 2012). In other words, for most syllogisms where humans give incorrect answers, the particular incorrect answers they give vary substantially across individuals and trials. However, there are exceptions to this tendency: in some cases, humans confidently and consistently choose a particular incorrect answer (that is, low entropy coincides with low accuracy). For example, given the syllogism <em>no artists are bakers, all bakers are chemists</em>, humans overwhelmingly respond with the logically invalid conclusion <em>no artists are chemists</em>; the correct conclusion, <em>some chemists are not artists</em>, is</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The proportion of "nothing follows" responses from humans and PaLM 2 models on the 37 syllogisms whose only valid conclusion is "nothing follows" (left) and the syllogisms that license conclusions other than "nothing follows" (right).</p>
<p>produced only 3% of the time, and the distribution over responses elicited from humans for this syllogism has one of the lowest entropies in the Ragni et al. (2019) dataset. We refer to such cases as <em>syllogistic fallacies</em> (Newsome and Johnson-Laird, 2006; Khemlani and Johnson-Laird, 2017).</p>
<p>To identify potential fallacies in LMs, we fit a regression line relating entropy (in nats) and accuracy, and then compute the distance from this line (the residual) for each syllogism (Figure 5, right; for alternative calibration measures, see Guo et al. 2017). The top three human syllogistic fallacies, defined as the top three outliers when plotting accuracy against entropy, are also outliers for the PaLM 2 models. We also correlate the residuals for all 27 syllogisms across humans and LMs, and find that larger models display stronger correlations (Figure 5, left).</p>
<p><strong>LMs avoid responding "nothing follows".</strong> An important divergence from human behavior is that LMs rarely produce the response "nothing follows", even for the 37 syllogisms for which this is the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Schematic of mReasoner deducing an incorrect conclusion before finding counterexamples ("System 2" processes shown in green) and updating to the correct conclusion, "nothing follows".
correct conclusion. Humans are also reluctant to conclude "nothing follows" (Ragni et al., 2019), but the LMs' aversion to this response is much stronger than humans'-we observe accuracies close to $0 \%$ (Figure 6). This issue is particularly severe with the zero-shot chain-of-throught prompting method we use; in Appendix B.3, we describe an evaluation paradigm that can be used to elicit that conclusion, and leave further analysis of this behavior to future work.</p>
<h3>4.3 Llama 2 Results</h3>
<p>Llama 2 models' overall accuracy, when aggregated across all syllogisms, was similar to human accuracy, with a modest increase in accuracy as scale increases (Figure 18 in the Appendix). However, the breakdown by syllogism shows that this pattern masks substantial differences between humans and Llama 2: unlike PaLM 2 models, Llama 2 models struggle with some syllogisms that humans find easy, such as some $A$ are $B$, all $B$ are $C$. Llama 2 models do, however, display a human-like variable ordering effect (Figure 20 in the Appendix). We refer the reader to Appendix E for plots of the results and additional analyses of Llama 2 models.</p>
<h2>5 Interpreting Language Models Using Mental Models Theory</h2>
<p>We next analyze the behavior of PaLM 2 models using the Mental Models theory of human logical reasoning (Johnson-Laird, 1983), which has been
developed over decades to account for human data. The theory takes humans to be resource-limited and simulation-based reasoners (Craik 1967; Lake et al. 2017; Lieder and Griffiths 2019; Johnson-Laird 1983, i.a.), with a potentially high degree of variability across individuals. The implementation we use-mReasoner ${ }^{2}$ (Khemlani and Johnson-Laird, 2022)—captures these aspects of human reasoning with a small set of interpretable hyperparameters that enable it to construct, refine, and draw conclusions from internal mental models of the situations described in a syllogism.</p>
<p>Mental models consist of sets of entities instantiating the premises, where an entity is represented by a conjunction of logical properties. For example, Figure 7 illustrates a mental model corresponding to the syllogism some artists are bakers, some bakers are chemists. This model consists of just three entities, the first of whom is an artist who is also a baker and a chemist, the second is an artist and a baker who may or may not be a chemist (this uncertainty is represented in the figure with a blank space), and so on. The reasoner constructs and maintains its mental model with a set of actions parameterized by four hyperparameters:</p>
<ul>
<li>LEN $(\lambda \in[1, \infty))$ : The number of entities generated by the reasoner is sampled from a Poisson distribution with a mean of LEN.</li>
<li>BROAD $(\epsilon \in[0,1])$ : Determines the set of individuals that mReasoner samples from. There are two possible sets: a broader set of all individuals consistent with the premises, and a smaller, canonical set of individuals consistent with the premises. The canonical sets were determined from human experiments (Khemlani and Johnson-Laird 2022; for an example, see Figure 15 in Appendix D).</li>
<li>SYSTM2 $(\sigma \in[0,1])$ : The reasoner's propensity to reconsider its conclusion and search for counterexamples. Search is conducted by adding an entity to the model, moving a property from one entity to another, or decomposing one entity into two (these strategies are illustrated in Figure 16 in Appendix D).</li>
<li>WEAKEN $(\omega \in[0,1])$ : Determines the model's reaction to finding a counterexample. The reasoner's options in this case are either to respond "nothing follows" or to weaken its response (i.e., amending erroneous global conclusions such as</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Correlations between the four principal components resulting from our analysis and mReasoner's original parameters (top four rows) as well as accuracy (bottom row).
all $A$ are $C$ to weaker particular conclusions such as some $A$ are $C$ ). When WEAKEN is higher, mReasoner is more likely to weaken its response and less likely to answer "nothing follows".</p>
<p>Figure 7 illustrates how mReasoner might process the syllogism some artists are bakers, some bakers are chemists. First, it constructs a mental model, with length governed by LEN and content governed by BROAD, consisting of the entities mentioned above: an artist-baker-chemist, an artistbaker, and an artist. The conclusion some artists are chemists is consistent with this particular model (i.e., the first entity is both an artist and a chemist). This conclusion is not true in every model that is consistent with the premises, and as such it is not logically valid; however, if the reasoner does not trigger a System 2 process, it will (incorrectly) take this conclusion as valid and return it. Alternatively, with probability SYSTM2 mReasoner will scrutinize the conclusion by amending its model in an attempt to find a counterexample. In this case, mReasoner successfully finds a counterexample by breaking the first entity into two entities that are still consistent with the premises but are not consistent with some artists are chemists; consequently, mReasoner corrects its answer to "nothing follows".</p>
<p>Mapping LM predictions onto cognitively meaningful dimensions. Syllogistic reasoning behavior is high-dimensional; in the set of syllogisms and conclusions we consider, there are 27 syllogisms and eight possible responses to each, for a total of 216 dimensions. We instantiate 1296 mReasoner models, one for each point in a parameter grid, and analyze the 923 of them that finished simulations
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Left and right: Projecting PaLM 2 models onto the first two principal components of the feature space resulting from the behavior of simulations using mReasoner. Center: Projecting PaLM 2 models onto the same space when only their errors are taken into account.
before timing out (for the details of the parameter grid, see Table 3 in Appendix D.2). We evaluate each instance on each syllogism and represent the instance as a vector in a 216-dimensional space. Finally, we use PCA to identify the top four principal components in this space.</p>
<p>Characterizing the space of reasoning behaviors described by mReasoner. Although mReasoner is characterized by four parameters, we find a single principal component (PC 1) that captures $77 \%$ of the variance in the model's behavior. This component loads heavily on SYSTM2 and, to a lesser degree, on WEAKEN (Figure 8). Following the terminology of Khemlani and Johnson-Laird (2016), we view this dimension as representing deliberative reasoning. Similarly, PC 2 loads heavily on BROAD. This dimension, however, describes much less of the behavioral variance of mReasoner.</p>
<p>LMs show signatures of deliberative reasoning. We project the 216-dimensional vectors describing the human data as well as the behavior of each of our LMs into the PC space. This allows us to interpret the LMs' behavior, in particular as model size increases, in terms of reasoning strategies (Figure 9). We find that larger LMs behave more like mReasoner instantiations with high SYSTM2 and WEAKEN values, as indicated by the fact that their first principal component is higher; in the terminology of Khemlani and Johnson-Laird (2016), they show a stronger behavioral signature of deliberative reasoning.</p>
<p>Deliberative reasoning is partly dissociable from accuracy. PC 1 is strongly correlated not only with SYSTM2, but also with accuracy. Can the</p>
<p>changes in coordinates assigned to PaLM 2 be explained simply by differences in accuracy? To test this, we repeat our analysis, this time setting the probabilities of the correct answers to 0 for all mReasoner instantiations, LMs and renormalizing (Figure 9, center). In this control analysis, the accuracy of all models is $0 \%$ (by design), but larger models still display more deliberative reasoning. Here the deliberative component (PC 1) has zero correlation with accuracy but a correlation of 0.6 with SYSTEM2; correlations with all other parameters are below 0.15 . This indicates that even the models' errors become more consistent with deliberative reasoning.</p>
<h2>6 Discussion</h2>
<p>Human-like reasoning or accurate reasoning? Because of humans' systematic reasoning errors, syllogistic reasoning is a particularly clear demonstration of the tension between the two central aims of artificial intelligence: human-likeness and accuracy. We hypothesize that for most applications, accuracy is more important than human-likeness; one notable exception is cognitive modeling, where the goal is to better understand human reasoning by developing models that reason like humans. We consider this application to be an important direction for future work.</p>
<p>Why are LMs more accurate than humans? LMs learn from human-generated text, which is likely to reflect human beliefs and biases; it is natural to hypothesize that the language modeling objective would incentivize LMs to replicate those biases. We find only partial support for this hypothesis. While the largest model's responses are indeed slightly more correlated with human responses than the smaller ones, for some syllogisms where humans reason very poorly, the models overcome human biases and reason correctly. One possible explanation for this finding is that the data that PaLM 2 models were trained on includes not only natural language text, but also source code (Chowdhery et al., 2022), which may teach models to reason more effectively. The effect of the composition of the LM's training corpus can be tested in a controlled comparison in the future.</p>
<p>Cognitive science for LM interpretation. We have used cognitive science to shed light on LM reasoning in two ways. First, we used the biases documented in the cognitive psychology literature
as hypotheses for the biases that LMs might acquire. This approach is motivated by the hypothesis that because LMs are trained on human-generated texts, which reflect human biases and beliefs, they will be incentivized to replicate those biases to improve perplexity. We found partial support for this hypothesis: larger LMs were more calibrated to human responses in some cases, in particular in our analysis of the correlation between accuracy and entropy (Section 4.2).</p>
<p>The second and more novel way in which we use cognitive science is in interpreting LM behavior using a computational cognitive model developed to explain human reasoning. Under the assumption that LM reasoning follows the same heuristic strategies as humans do (Section 5)-an assumption which, again, is informed by the fact that LMs learn from text generated by humans-we concluded from this analysis that LMs become more deliberative as their size increases.</p>
<h2>7 Conclusion</h2>
<p>Do LMs learn to reason correctly from selfsupervised learning alone, even though much of their training data was produced by humans, whose reasoning often deviates from normative logic? We have addressed this question through a detailed examination of the syllogistic reasoning behavior of the PaLM 2 family of LMs. We find that the largest LMs make significantly fewer mistakes than humans but still display systematic errors (Section 4.1), and that while their mistakes are only partly aligned with human errors, LMs are susceptible to several qualitative reasoning biases shown by humans (Section 4.2).</p>
<h2>Acknowledgments</h2>
<p>We thank Andrew Lampinen for helpful discussion and Sangeet Khemlani for open-sourcing MReasoner. TE is supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1745302.</p>
<h2>Ethical Considerations and Limitations</h2>
<p>Part of this work's motivation is to extend the understanding of similarities and differences between humans and current LMs, and we hope our work will have broader positive impacts, such as facilitating cognitively informed and ethical model development. Our results are limited, however, with challenges in directly comparing LM and human</p>
<p>behavior, and we comment on specific limitations below.</p>
<p>Eliciting LM reasoning. The space of possible ways to evaluate LMs on paradigms from human experiments is fairly large. One can generate from the model (Aina and Linzen, 2021), as we did; elicit meta-level judgements (Hu and Levy, 2023; Beguš et al., 2023); or simply compare the probabilities assigned by the LM to possible continuations (Linzen et al., 2016; Dasgupta et al., 2022). Evaluations can be done in a zero-shot way, as we did, or in a few-shot way, which may better approximate the training phase used in some human reasoning experiments, such as Ragni et al. (2019); for discussion, see Lampinen (2022). Finally, generative approaches can rely on a large set of possible prompts, and can be used with or without "chain-of-thought" statements encouraging the model to reveal its reasoning process (Kojima et al., 2022). Following preliminary experiments (Appendix B), we focused on zero-shot chain-of-thought; a more systematic evaluation of the different elicitation approaches would be an important direction for future work.</p>
<p>The focus on Mental Models Theory. In Section 5, we used a particular cognitive model, the Mental Models Theory, to interpret LM reasoning behavior. This is not the only possible mechanism that might underlie LM reasoning. Other accounts of human reasoning have argued that people do, in fact, apply normative logic rules (Rips, 1994), perform probabilistic inference with constrained resources (Chater and Oaksford, 1999), or combine probabilistic, heuristic and pragmatic reasoning (Tessler et al., 2022); and it is possible that LMs reason in a way that does not match any of these theories. We leave a systematic comparison of the fit of each of these theories to LM reasoning for future work.</p>
<h2>References</h2>
<p>Laura Aina and Tal Linzen. 2021. The language model understood the prompt was ambiguous: Probing syntactic uncertainty through generation. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 4257, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Gašper Beguš, Thomas Lu, and Zili Wang. 2023. Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks.</p>
<p>Gregor Betz, Christian Voigt, and Kyle Richardson. 2020. Critical Thinking for Language Models.</p>
<p>BIG-bench collaboration. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.</p>
<p>Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4.</p>
<p>Nick Chater and Mike Oaksford. 1999. The probability heuristics model of syllogistic reasoning. Cognitive psychology, 38(2):191-258.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.</p>
<p>Kenneth Craik. 1967. The Nature of Explanation, 1 edition. Cambridge University Press.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie C Y Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning.</p>
<p>J S Evans, J L Barston, and P Pollard. 1983. On the conflict between logic and belief in syllogistic reasoning. Mem. Cognit., 11(3):295-306.</p>
<p>J A Fodor and Z W Pylyshyn. 1988. Connectionism and cognitive architecture: a critical analysis. Cognition, 28(1-2):3-71.</p>
<p>Gerd Gigerenzer and Wolfgang Gaissmaier. 2011. Heuristic decision making. Annu. Rev. Psychol., 62:451-482.</p>
<p>Google. 2023. PaLM 2 Technical Report.
Thomas L Griffiths, Nick Chater, Charles Kemp, Amy Perfors, and Joshua B Tenenbaum. 2010. Probabilistic models of cognition: exploring representations and inductive biases. Trends Cogn. Sci., 14(8):357364.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On Calibration of Modern Neural Networks.</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. 2022. FOLIO: Natural Language Reasoning with First-Order Logic.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jennifer Hu and Roger Levy. 2023. Prompt-based methods may underestimate large language models' linguistic generalizations.</p>
<p>Philip N Johnson-Laird and Mark Steedman. 1978. The psychology of syllogisms. Cogn. Psychol., 10(1):6499.</p>
<p>Philip Nicholas Johnson-Laird. 1983. Mental models: Towards a cognitive science of language, inference, and consciousness. Harvard University Press, Cambridge, MA.</p>
<p>Philip Nicholas Johnson-Laird and Ruth MJ Byrne. 1991. Deduction. Lawrence Erlbaum Associates, Inc.</p>
<p>Daniel Kahneman. 2013. Thinking, Fast and Slow, 1 edition. Farrar, Straus and Giroux.</p>
<p>Sangeet Khemlani and P N Johnson-Laird. 2012. Theories of the syllogism: A meta-analysis. Psychol. Bull., 138(3):427-457.</p>
<p>Sangeet Khemlani and P N Johnson-Laird. 2016. How people differ in syllogistic reasoning. https://modeltheory.org/papers/ 2016syllogisms-indvl-diffs.pdf. Accessed: 2023-6-22.</p>
<p>Sangeet Khemlani and P N Johnson-Laird. 2022. Reasoning about properties: A computational theory. Psychol. Rev., 129(2):289-312.</p>
<p>Sangeet S Khemlani and P N Johnson-Laird. 2017. Illusions in reasoning. Minds Mach., 27(1):11-35.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>John E Laird, Allen Newell, and Paul S Rosenbloom. 1987. SOAR: An architecture for general intelligence. Artif. Intell., 33(1):1-64.</p>
<p>Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behav. Brain Sci., 40:e253.</p>
<p>Andrew Kyle Lampinen. 2022. Can language models handle recursively nested grammatical structures? A case study on comparing models and humans.</p>
<p>Falk Lieder and Thomas L Griffiths. 2019. Resourcerational analysis: Understanding human cognition as the optimal use of limited computational resources. Behav. Brain Sci., 43:e1.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Gary Marcus. 2009. Kluge: The Haphazard Evolution of the Human Mind, first edition edition. Mariner Books.</p>
<p>James L McClelland, Matthew M Botvinick, David C Noelle, David C Plaut, Timothy T Rogers, Mark S Seidenberg, and Linda B Smith. 2010. Letting structure emerge: connectionist and dynamical systems approaches to cognition. Trends Cogn. Sci., 14(8):348356.</p>
<p>Thomas R McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L Griffiths. 2023. Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve.</p>
<p>Allen Newell and Herbert A Simon. 1972. Human problem solving. 920.</p>
<p>Mary R Newsome and P N Johnson-Laird. 2006. How falsity dispels fallacies. Think. Reason., 12(2):214234.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830.</p>
<p>Pouya Pezeshkpour and Estevam Hruschka. 2023. Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.</p>
<p>Marco Ragni, Hannah Dames, Daniel Brand, and Nicolas Riesterer. 2019. When does a reasoner respond: Nothing follows? In CogSci, pages 2640-2546.</p>
<p>Yasaman Razeghi, Robert L Logan, IV, Matt Gardner, and Sameer Singh. 2022. Impact of Pretraining Term Frequencies on Few-Shot Reasoning.</p>
<p>Lance J Rips. 1994. The psychology of proof: Deductive reasoning in human thinking. MIT Press.</p>
<p>Abulhair Saparov and He He. 2022. Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought.</p>
<p>Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, and He He. 2023. Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples.</p>
<p>Michael Henry Tessler, Joshua B Tenenbaum, and Noah D Goodman. 2022. Logic, probability, and pragmatics in syllogistic reasoning. Top. Cogn. Sci.</p>
<p>Michael E Tipping and Christopher M Bishop. 1999. Probabilistic principal component analysis. J. R. Stat. Soc. Series B Stat. Methodol., 61(3):611-622.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models.</p>
<p>Yongkang Wu, Meng Han, Yutao Zhu, Lei Li, Xinyu Zhang, Ruofei Lai, Xiaoguang Li, Yuanhang Ren, Zhicheng Dou, and Zhao Cao. 2023a. Hence, Socrates is mortal: A benchmark for natural language syllogistic reasoning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2347-2367, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023b. Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks.</p>
<p>Mengyu Ye, Tatsuki Kuribayashi, Jun Suzuki, Goro Kobayashi, and Hiroaki Funayama. 2023. Assessing step-by-step reasoning against lexical negation: A case study on syllogism. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14753-14773, Stroudsburg, PA, USA. Association for Computational Linguistics.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Valid conclusions for each syllogism. Conclusion identifiers show the conclusion mood (see Table 1) followed by 'ac' if the first variable in the conclusion is A and the second is C and 'ca' in the opposite case.</p>
<h2>A Further Details on Syllogism Dataset</h2>
<p>Table 2 displays the full list of the content triples used in our experiments. The words in each triple were chosen to have minimal semantic associations with each other.</p>
<h2>B Prompting and Evaluation</h2>
<p>Before settling on the generative chain-of-thought evaluation strategy that we focus on in this paper (described in detail in Section B.1), we explored two additional strategies for eliciting and scoring syllogistic inferences from LMs. First, we explored a multiple-choice approach, where, following the prompt, we computed the mutual information between the prompt and each of the nine possible conclusions (eight valid conclusions plus "nothing follows"; Section B.2); and second, we explored a simplified binary discrimination approach, where, following the prompt and a particular conclusion, we computed the mutual information between the prompt and each of the strings "valid" and "invalid" (Section B.3). Of these three methods, chain-ofthought prompting achieved the highest accuracy generally and had qualitatively similar performance across a range of hyperparameters, so we use it in the main text. That being said, the binary discrimination approach has the highest correlation with humans and is the only method that consistently</p>
<table>
<thead>
<tr>
<th style="text-align: left;">actuaries, sculptors, writers</th>
<th style="text-align: left;">assistants, poets, scientists</th>
<th style="text-align: left;">athletes, assistants, chefs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">chemists, drivers, dancers</td>
<td style="text-align: left;">chemists, workers, painters</td>
<td style="text-align: left;">clerks, butchers, athletes</td>
</tr>
<tr>
<td style="text-align: left;">dancers, bankers, riders</td>
<td style="text-align: left;">doctors, riders, investors</td>
<td style="text-align: left;">drivers, porters, chemists</td>
</tr>
<tr>
<td style="text-align: left;">farmers, surfers, writers</td>
<td style="text-align: left;">gamblers, cleaners, models</td>
<td style="text-align: left;">golfers, cyclists, assistants</td>
</tr>
<tr>
<td style="text-align: left;">hunters, analysts, swimmers</td>
<td style="text-align: left;">joggers, actors, carpenters</td>
<td style="text-align: left;">linguists, cooks, models</td>
</tr>
<tr>
<td style="text-align: left;">linguists, skaters, singers</td>
<td style="text-align: left;">managers, clerks, butchers</td>
<td style="text-align: left;">miners, tellers, poets</td>
</tr>
<tr>
<td style="text-align: left;">models, tailors, florists</td>
<td style="text-align: left;">nurses, scholars, buyers</td>
<td style="text-align: left;">planners, sailors, engineers</td>
</tr>
<tr>
<td style="text-align: left;">riders, agents, waiters</td>
<td style="text-align: left;">riders, novelists, linguists</td>
<td style="text-align: left;">runners, opticians, clerks</td>
</tr>
<tr>
<td style="text-align: left;">scientists, novelists, florists</td>
<td style="text-align: left;">skaters, barbers, cooks</td>
<td style="text-align: left;">students, cashiers, doctors</td>
</tr>
<tr>
<td style="text-align: left;">students, hikers, designers</td>
<td style="text-align: left;">surfers, painters, porters</td>
<td style="text-align: left;">therapists, hikers, opticians</td>
</tr>
</tbody>
</table>
<p>Table 2: The 30 content word triples we use to construct syllogisms (e.g., for the first entry in the table, the variables $\mathrm{A}, \mathrm{B}$ and C in the syllogism are replaced with actuaries, sculptors and writers, respectively). The words in each triple were chosen to be minimally semantically associated with each other.
provides the response "nothing follows" when appropriate, and as such is a promising method to explore in future work. The remainder of this appendix provides additional details about the different elicitation methods and the variations on those methods that we explored. All of the empirical results in this appendix are based on PaLM 2.</p>
<h2>B. 1 Generative Evaluation with a Zero-Shot Chain-of-Thought Prompt</h2>
<p>The zero-shot chain-of-thought approach is illustrated in Figure 1. We first describe the inference task to the model: "Choose the conclusion that necessarily follows from the premises or "nothing follows" if none of the other conclusions logically follow, ". We then define the conclusion space, with the string "the possible conclusions are: " followed by the list of all possible conclusions, including "nothing follows"; the possible conclusions are provided in a randomized order. Next, we provide the two premises for the syllogism being queried in the format: "Premise 1: PREMISE1, Premise 2: PREMISE2, ". Finally, we add the string "Let's think this through, step by step", which is intended to instruct the LM to produce a reasoning trace. We then generate from the LM, and determine for each of the conclusions whether they appear in the text generated by the LM. The conclusion that was detected most often, across content triples and samples, is taken to be the answer produced by the model.</p>
<h2>B.1.1 Robustness to Prompt and Decoding Hyperparameters</h2>
<p>The analyses presented in the main text are based on a decoding process in which we sequentially
generate 75 tokens from the LM, with a temperature of 0.5 , and take 30 such samples for each combination of syllogism type and content triple. Due to compute limitations, we are unable to conduct a systematic exploration of different variations on these hyperparameters for all model sizes; as such, we focus on PaLM 2 XS. As in the main text, we only report accuracy for the 27 syllogisms that have valid conclusions, and exclude the syllogisms for which "nothing follows" is the correct response.</p>
<p>Prompts. In addition to the prompt we used in the main text, which we refer to as stepxstep, we consider three variations on this prompt (Figure 11):</p>
<ol>
<li>logically: The same as stepxstep, except the zero-shot reasoning trigger "Let's think this through, step by step" is replaced by "Think logically" (like stepxstep, this prompt is inspired by a prompt from Kojima et al. 2022).</li>
<li>empty: This prompt does not include any zeroshot reasoning trigger (that is, "Let's think this through, step by step" is replaced with the empty string).</li>
<li>alt: We created this prompt in an attempt to mitigate the LMs' reluctace to produce "nothing follows"; here the possibility of a "nothing follows" response is highlighted closer to the end of the prompt and in a more verbose way. This prompt also encourages the model to use the exact wording included in the prompt, and replaces "Let's think this through, step by step" with the slight variation "Let's think step by step".</li>
</ol>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Variations on the prompt we used for the generative elicitation method; the prompt used in the main text is stepxstep.</p>
<p>In the experiments varying the prompt, we hold the decoding temperature at 0.5 and the maximum number of decoded tokens at 75 . We find that the prompt variants show broadly similar patterns (Figure 12), though stepxstep achieves moderately higher accuracy than the other prompts.</p>
<p>Decoding hyperparameters Next, we hold the stepxstep prompt used in the main paper constant, and independently vary decoding length and temperature. First, we use the temperatures ${0.25,0.5,0.75}$, holding the decoding length at 75. Second, we vary the number of tokens decoded between 50,75 and 100 , keeping the temperature at 0.5 . Here, we observe a slight increase in accuracy as the number of decoded tokens increases, which is expected (Figure 12).</p>
<h2>B. 2 Multiple-Choice Discriminative Evaluation</h2>
<p>In this approach to evaluating LM reasoning, we replace the generative evaluation with a discriminative scoring of each of the possible conclusions. The prompt is very similar: we remove the zeroshot chain-of-thought trigger from stepxstep and replace it with "The conclusion that necessarily follows is: ", then feed the prompt to the models and score each of the conclusions. To normalize for the idiosyncratic features of each conclusion,
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Accuracy for the chain-of-thought prompting method, with different prompts, temperatures and number of decoding steps.
such as its length and prior probability, we use the mutual information between the prompt (p) and the conclusion (c) as the score (Holtzman et al., 2021):</p>
<p>$$
\operatorname{MI}(\mathrm{c} ; \mathrm{p})=\log P(\mathrm{c} \mid \mathrm{p})-\log P\left(\mathrm{c} \mid{ }^{" "}\right)
$$</p>
<p>We then renormalise these scores to compute a distribution over the conclusions (indexed by $i$ ):</p>
<p>$$
P\left(\mathrm{c}<em i="i">{i}\right)=\frac{\exp \left(\mathrm{MI}\left(\mathrm{c}</em>
$$} ; \mathrm{p}\right)\right)}{\sum_{j} \exp \left(\mathrm{MI}\left(\mathrm{c}_{j} ; \mathrm{p}\right)\right)</p>
<p>and take the conclusion with the highest $P\left(\right.$ concludon $\left._{i}\right)$ to be the LM's prediction for a given combination of syllogism and content triple. Results obtained using this method are shown in Figure 13.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Comparison across reasoning elicitation methods with PaLM 2 models: the CoT generation method used in the main text (generative), as well as the binary and multiple-choice methods. We show accuracy among syllogisms with a valid conclusion (top), correlation with humans (middle), and accuracy among syllogisms where the correct response is "nothing follows" (bottom). The accuracy of the generative method is highest on the valid syllogisms, but the binary discrimination method achieves markedly higher accuracy on the "nothing follows" syllogisms. Both outperform the multiple-choice method substantially.</p>
<h2>B. 3 Simplified Binary Evaluation</h2>
<p>While the multiple-choice format is most similar to the paradigm used in human experiments, it poses a significantly harder task than simple binary discrimination (Dasgupta et al., 2022), which may be more sensitive. In the validity discrimination evaluation method, we present the LM with the prompt "Is this conclusion valid given the premises:" followed by the premises and a single conclusion (we refer to the concatenation of the prompt and conclusion ${ }<em i="i">{i}$ as prompt $</em>$ below). We do this for all eight possible conclusions (omitting "nothing follows"). We, again, use the mutual information to score and compute the binary probability of "valid" as:</p>
<p>$$
\begin{aligned}
&amp; P\left(\text { "valid" } \mid \mathrm{c}<em i="i">{i}\right)= \
&amp; \frac{\exp \left(\mathrm{MI}\left(\text { "valid" } ; \mathrm{p}</em>}\right)\right)}{\exp \left(\mathrm{MI}\left(\text { "valid" } ; \mathrm{p<em i="i">{i}\right)\right)+\exp \left(\mathrm{MI}\left(\text { "invalid" } ; \mathrm{p}</em>
\end{aligned}
$$}\right)\right)</p>
<p>We compute discrete conclusion decisions by normalizing $P$ ("valid") for each conclusion into a probability distribution:</p>
<p>$$
P\left(\mathrm{c}<em i="i">{i}\right)=\frac{P\left(\text { "valid" } \mid \mathrm{c}</em>
$$}\right)}{\sum_{j} P\left(\text { "valid" } \mid \mathrm{c}_{j}\right)</p>
<p>and taking the conclusion with the largest probability according to Equation 3 to be the LM's selected conclusion for a syllogism (the conclusion most likely to be valid according to the LM). In this approach, the LM's prediction is taken to be "nothing follows" if $P$ ("valid"|conclusion) does not exceed $50 \%$ for any of the conclusions. We note that this method is the only one that successfully elicits "nothing follows" conclusions for a substantial proportion of the syllogisms (Figure 13).</p>
<h2>C By-Syllogism Correlations with Human Responses</h2>
<p>Figures 14 provides correlations between LMs and humans at the individual syllogism level. While larger LMs are generally more human-like, we observe a diversity of relationships between model scale and human-likeness, including cases such as IE2 where larger models are in fact less correlated with humans.</p>
<h2>D Mental Models Simulations: Additional Details</h2>
<h2>D. 1 Model details</h2>
<p>This section provides additional details on mReasoner. Figure 15 shows an example of the "canoni-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LEN</th>
<th style="text-align: center;">2.0</th>
<th style="text-align: center;">2.5</th>
<th style="text-align: center;">3.0</th>
<th style="text-align: center;">3.5</th>
<th style="text-align: center;">4.0</th>
<th style="text-align: center;">4.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BROAD</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">SYSTM2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">WEAKEN</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
</tr>
</tbody>
</table>
<p>Table 3: Parameter grid used to instantiate our mReasoner models.
cal sets" that mReasoner uses to heuristically sample entities, and Figure 16 illustrates the subroutines used to revise mental models.</p>
<h2>D. 2 mReasoner instantiations</h2>
<p>We instantiate one mReasoner model for every parameter vector in the grid shown in Table 3. This resulted in a total of 1,296 models. As the model's reasoning process is stochastic, we evaluate each model 100 times for each syllogism to estimate the distribution over responses. Due to resource constraints, we discarded models that did not finish these 100 iterations in 60 seconds, leaving us with 923 models spaced relatively evenly over the grid (i.e., this timeout criterion did not systemtically favor some hyperparameter values).</p>
<p>Each of the 923 models is represented by a 216dimension vector, with eight possible conclusions for each of the 27 valid syllogisms $(27 \times 8=216)$. We perform a standard PCA-the probabilistic PCA of Tipping and Bishop (1999) on the centered dataset, using scikit-learn (Pedregosa et al., 2011)—on these 923 vectors.</p>
<h2>E Llama 2: Additional Results and Plots</h2>
<p>As we mentioned in Section 4.3, while the overall accuracy of Llama 2 models is similar to that of humans, this aggregate pattern masks large discrepancies with humans, and in particular poor accuracy on some syllogisms where humans rarely make mistakes, as well as high accuracy on syllogisms that humans struggle with. Consequently, these models exhibit a substantially lower correlation with human behavior across the board (Figure 21) than do PaLM 2 models (cf. Figure 14). Llama 2 models also demonstrate a slight decrease in correlation with human behavior as a function of model size in our analysis of the correlation between accuracy and entropy in syllogistic fallacies (Figure 19; cf. Figure 5 for PaLM 2 models).</p>
<p>We also repeated the mReasoner analysis (described in Section 5) to analyze Llama 2 models' behavior. Although the Llama 2 models, like</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Correlation between PaLM 2 models' distribution over responses and the probabilities derived from normalizing human responses, broken down by syllogism. Syllogisms are partitioned into variable ordering type (by row) and ordered by decreasing human accuracy from left to right. Chance performance (dashed grey line) reflects random guessing. The top right inset shows correlation across the entire dataset.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: The "canonical sets" used by mReasoner. The canonical set for a syllogism depends on the moods of the syllogism's premises. We show the possible individuals each premise contributes to a syllogism's canonical set here for the hypothetical content words artists and bakers.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Subroutines used by mReasoner to revise mental models in order to check for counterexamples. We denote these subroutines as ADD, BREAK, and MOVE, following (Khemlani and Johnson-Laird, 2022). ADD adds one more entity to a mental model. BREAK decomposes an entity's properties into constituent entities with subsets of those properties. MOVE simply moves a property from one entity to another.</p>
<p>PaLM 2, exhibit increased signatures of deliberative reasoning compared to mReasoner when all of their predictions are considered (Figure 17b, left), when we control for accuracy by setting the probability of the correct answer to zero for all Llama 2 models, we find no significant correlation between model size and signatures of deliberative reasoning (Figure 17b, center; cf. Figure 9 for PaLM 2, where we do find such signatures even after controlling for accuracy).</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: (a) Llama 2's behavior on the 37 syllogisms whose only valid conclusion is "nothing follows" (left) and the syllogisms that license conclusions other than "nothing follows" (right). (b) Llama 2's behavior when analyzed using the principal components of the mReasoner space (see Section 5). We find an increased signature of deliberative reasoning as a function of model size, but we no longer observe this effect when we control for accuracy (setting the probability of the correct answers to zero before projecting the models' behavior into this space).
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18: Accuracy of Llama 2 models, humans (red), and random guessing (grey). Random guessing accuracy differs by syllogism as some syllogisms have more than one valid conclusion. Syllogisms are partitioned into variable ordering (by row) and ordered by decreasing human accuracy from left to right. The top right inset shows the average accuracy across all syllogisms. Syllogisms are identified with the letters of the moods of the premises (Table 1, left) and the number associated with their variable ordering (Table 1, right).
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19: Analysis of Llama 2 models' handling of syllogistic fallacies. Right: Each syllogism plotted by accuracy (y-axis) and entropy (x-axis) and the regression line relating the two. Dashed lines black lines show the residuals for each of the top three human syllogistic fallacies. Left: The result of correlating Llama 2's residuals with residuals estimated from human data.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20: Variable ordering effects on Llama 2 responses. Left: The marginal probabilities of A-C and C-A ordered conclusions as estimated from human and LM responses. Humans and LMs both show variable ordering effects in the same direction. Right: The magnitude of the variable ordering effect (the absolute value of the difference between the probability of the C-A ordering and the probability of the A-C ordering).
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 21: Correlation between the Llama 2 model's distribution over responses and the probabilities derived from normalizing human responses, broken down by syllogism. Syllogisms are partitioned into variable ordering type (by row) and ordered by decreasing human accuracy from left to right. Chance performance (dashed grey line) reflects random guessing. The top right inset shows correlation across the entire dataset.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/skhemlani/mReasoner&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3416 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3416</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3416</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-269757280</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.06690v1.pdf" target="_blank">DrugLLM: Open Large Language Model for Few-shot Molecule Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have made great strides in areas such as language processing and computer vision. Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry. For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties. Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded. In this work, we introduced DrugLLM, a LLM tailored for drug design. During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties. DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications. Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3416.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3416.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive large language model trained on sequences of molecular modifications expressed in a Group-based Molecular Representation (GMR) to perform few-shot and zero-shot molecule generation and optimization for drug design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-decoder LLM based on the LLaMA architecture configured with 32 layers, 32 attention heads, hidden dimension 4096, batch size 64, totaling ~7 billion parameters; trained from scratch on a corpus constructed from ZINC and ChEMBL (GMR-encoded modification paragraphs) for ~6 weeks on eight NVIDIA RTX 3090 GPUs. Training data: paper reports over 25 million modification paragraphs and reports both ~200 million and (in a later section) 2 billion molecules (inconsistent within paper); training covers >10,000 molecular properties/activities.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive next-molecule prediction using Group-based Molecular Representation (GMR) strings; few-shot in-context learning by providing K example molecule-modification pairs (K ≤ 9) and a query molecule; decodes generated GMR back to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — few-shot molecule optimization for physicochemical properties (LogP, solubility, synthetic accessibility, TPSA) and biological activities (various ChEMBL targets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (proportion of generated molecules that follow the modification rule given examples), molecular similarity between generated and query molecules, property distributions (KDE) for target properties (LogP, solubility, synthetic accessibility, TPSA), UMAP visualization of chemical space, and task-specific predicted activity Pearson correlation (for training of activity predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DrugLLM produced systematic improvements in targeted properties and maintained structural similarity to queries: few-shot LogP optimization success rate increased to ~75% as shots increased; DrugLLM-GMR slightly outperformed DrugLLM-SMILES; for biological-activity optimization DrugLLM achieved high success rates (example: ~76% success for Rho-associated protein kinase 1). In zero-shot composition tasks (6,000 instructions across 6 tasks) DrugLLM outperformed other LLM baselines (see table: e.g., QED & FractionCSP3 40%, QED & TPSA 47%, QED & #Rotatable bonds 59%, LogP & FractionCSP3 60%, LogP & TPSA 55%, LogP & #Rotatable bonds 61%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperformed state-of-the-art graph/generative molecular models adapted to few-shot (JTVAE, VJTNN, MoLeR) which achieved ~50% success (similar to random) on few-shot tasks; outperformed general-purpose LLMs (ChatGLM, ChatGPT, GPT-4) in zero-shot and few-shot molecule optimization benchmarks reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Input length / hardware limits restrict to ≤9 shots; zero-shot optimization currently limited to compositions of up to two known properties and cannot handle arbitrary natural-language instructions robustly; GMR has difficulty representing a small subset of complex molecules and lacks full standardization, causing occasional encoding/decoding errors; internal inconsistencies in reported training-data scale in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3416.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational GPT model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose conversational large language model from OpenAI used here as a baseline for zero-shot/few-shot molecule optimization via prompt engineering to produce SMILES-like outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large transformer-based language model pre-trained on broad web-scale text corpora; used via OpenAI API in the paper with custom prompts that include molecule descriptions and example modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering / in-context examples to request modified molecules (SMILES) from the model in zero-shot/few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Attempted molecule optimization (drug-like molecules) in zero-shot and few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same evaluation pipeline as DrugLLM: success rate (following example modification rules), validity of generated SMILES, property calculation via RDKit scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGPT was able to understand instructions and optimize some given molecules but achieved relatively low success rates compared to DrugLLM. Per Table 3 examples: QED & FractionCSP3 ~11%, QED & TPSA ~15%, QED & #Rotatable bonds ~15%, LogP & FractionCSP3 ~30%, LogP & TPSA ~19%, LogP & #Rotatable bonds ~19% (values reported in table are fractional; interpreted as percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperformed DrugLLM on zero-shot molecule optimization; performed better than ChatGLM but worse than GPT-4 on some tasks (per provided numbers), and worse than DrugLLM across evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Lower chemical-domain specialization leading to lower success rates; general LLMs produce fewer valid/appropriate SMILES for chemistry tasks; performance depends heavily on prompt design; overall limited ability to map semantic instructions to structural property changes compared with DrugLLM trained on molecular-modification corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3416.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art general-purpose LLM used as a baseline for zero-shot molecule optimization via API prompting; produced some valid molecular outputs but with limited success relative to DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large multimodal transformer model from OpenAI trained on very large text (and possibly multimodal) corpora; accessed via API for experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering / in-context examples to elicit SMILES or molecule-descriptions for optimization tasks in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Zero-shot molecule optimization for drug-like molecule properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate per task (as in Table 3); validity of generated SMILES and property changes measured via RDKit scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 generated molecules that sometimes met optimization instructions but had modest success rates generally lower than DrugLLM; reported example success rates (Table 3): QED & FractionCSP3 ~20%, QED & TPSA ~20%, QED & #Rotatable bonds ~10%, LogP & FractionCSP3 ~40%, LogP & TPSA ~43%, LogP & #Rotatable bonds ~5% (table entries interpreted as fractional values).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Better than ChatGLM on many zero-shot tasks; worse overall than DrugLLM in these chemistry-specific zero-shot benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>As a general-domain model, lacks specialized molecule-structure/property training and therefore shows limited ability to consistently map textual optimization instructions to valid structural changes; results sensitive to prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3416.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose open bilingual pre-trained model tested locally as a baseline for zero-shot molecule optimization; struggled to produce appropriate molecules and often duplicated inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open bilingual pre-trained LLM (GLM family) deployed locally using official released code and pre-trained weights for evaluation in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering / in-context examples to produce SMILES-like outputs for zero-shot optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Zero-shot molecule optimization attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (following modification instruction), validity of outputs, duplication of inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGLM largely failed to provide appropriate molecules for zero-shot tasks; many outputs duplicated the input molecules; reported success rates in Table 3 are very low (e.g., QED & FractionCSP3 2%, QED & TPSA 3%, LogP & FractionCSP3 3%, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed worse than ChatGPT, GPT-4, and DrugLLM on zero-shot molecule optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tendency to output duplicates of the input molecule rather than applying meaningful modifications; poor mapping from natural language instructions to chemical structure changes for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3416.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (Large Language Model from Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation language model whose 7B parameter configuration served as the backbone for DrugLLM; when used as a general LLM baseline (unadapted) it failed to generate valid SMILES in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation transformer language model (LLaMA family); DrugLLM adopts LLaMA 7B parameters and architecture as the base model; the paper also reports that vanilla LLaMA (and similar general LLMs) were unable to produce valid SMILES when used directly for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>When used directly (baseline), prompted to produce SMILES; in DrugLLM, LLaMA architecture is fine-tuned/trained on GMR-encoded molecular modification paragraphs to autoregressively generate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Backbone architecture for drug-design LLM (DrugLLM); also evaluated as an unadapted baseline (unable to produce valid SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity of SMILES outputs, success rate for optimization tasks when tested as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>As the backbone, enabled DrugLLM to achieve few-shot molecule generation success; as a general LLaMA baseline it could not generate valid SMILES and thus was not used in comparative zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Unadapted LLaMA underperforms when used for chemistry tasks; adapted (DrugLLM) version substantially outperforms general LLaMA baseline due to domain-specific training with GMR data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Vanilla general-purpose LLaMA lacks domain-specific chemical training and tokenization to reliably produce valid chemical strings; requires adaptation (as in DrugLLM) to be useful for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3416.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based model referenced in related work that uses SMILES as its molecular representation for ligand design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned as a GPT-based strategy that uses SMILES tokens as representation (Li et al., 2023 referenced); no experimental details from this paper beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Reported (in cited work) to use SMILES-based generation (not evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug/ligand design (as reported by the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3416.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical-domain GPT-style model mentioned in related work for biomedical text generation/mining, not evaluated here for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative pre-trained transformer specialized for biomedical text generation and mining (referenced Luo et al., 2022); no experiments on molecule-generation reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical text generation (as per cited literature).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3416.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-following model (reference) mentioned as one of several general LLMs that were unable to generate valid SMILES strings in the authors' tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLM referenced (Maeng et al. citation) and reported in this paper as failing to produce valid SMILES when tested as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / instruction-following (when tested as baseline); failed to output valid SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluated as general LLM baseline for molecule generation (failed).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity of SMILES outputs (failed).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Unable to generate valid SMILES in the experiments described; therefore not included in zero-shot comparative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed worse than ChatGPT/GPT-4/ChatGLM/DrugLLM (in that it could not generate valid SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>General-purpose instruction tuning insufficient to produce chemically valid SMILES without domain-specific training or representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3416.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3416.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned conversational LLM mentioned as another general model unable to generate valid SMILES in the authors' tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open conversational LLM referenced (Chiang et al., 2023); the paper reports it was unable to generate valid SMILES strings when used directly for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / conversational interface (when tested as baseline); failed to produce valid SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluated as general LLM baseline for molecule generation (failed).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity of SMILES outputs (failed).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Unable to generate valid SMILES in the experiments described; excluded from zero-shot evaluation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not competitive due to invalid outputs versus specialized DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Lacks chemistry-specific representation/training needed to produce valid molecular strings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins <em>(Rating: 2)</em></li>
                <li>Biogpt: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3416",
    "paper_id": "paper-269757280",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "DrugLLM",
            "name_full": "DrugLLM",
            "brief_description": "An autoregressive large language model trained on sequences of molecular modifications expressed in a Group-based Molecular Representation (GMR) to perform few-shot and zero-shot molecule generation and optimization for drug design tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DrugLLM",
            "model_description": "Transformer-decoder LLM based on the LLaMA architecture configured with 32 layers, 32 attention heads, hidden dimension 4096, batch size 64, totaling ~7 billion parameters; trained from scratch on a corpus constructed from ZINC and ChEMBL (GMR-encoded modification paragraphs) for ~6 weeks on eight NVIDIA RTX 3090 GPUs. Training data: paper reports over 25 million modification paragraphs and reports both ~200 million and (in a later section) 2 billion molecules (inconsistent within paper); training covers &gt;10,000 molecular properties/activities.",
            "generation_method": "Autoregressive next-molecule prediction using Group-based Molecular Representation (GMR) strings; few-shot in-context learning by providing K example molecule-modification pairs (K ≤ 9) and a query molecule; decodes generated GMR back to SMILES.",
            "application_domain": "Drug discovery — few-shot molecule optimization for physicochemical properties (LogP, solubility, synthetic accessibility, TPSA) and biological activities (various ChEMBL targets).",
            "evaluation_metrics": "Success rate (proportion of generated molecules that follow the modification rule given examples), molecular similarity between generated and query molecules, property distributions (KDE) for target properties (LogP, solubility, synthetic accessibility, TPSA), UMAP visualization of chemical space, and task-specific predicted activity Pearson correlation (for training of activity predictors).",
            "results_summary": "DrugLLM produced systematic improvements in targeted properties and maintained structural similarity to queries: few-shot LogP optimization success rate increased to ~75% as shots increased; DrugLLM-GMR slightly outperformed DrugLLM-SMILES; for biological-activity optimization DrugLLM achieved high success rates (example: ~76% success for Rho-associated protein kinase 1). In zero-shot composition tasks (6,000 instructions across 6 tasks) DrugLLM outperformed other LLM baselines (see table: e.g., QED & FractionCSP3 40%, QED & TPSA 47%, QED & #Rotatable bonds 59%, LogP & FractionCSP3 60%, LogP & TPSA 55%, LogP & #Rotatable bonds 61%).",
            "comparison_to_baselines": "Outperformed state-of-the-art graph/generative molecular models adapted to few-shot (JTVAE, VJTNN, MoLeR) which achieved ~50% success (similar to random) on few-shot tasks; outperformed general-purpose LLMs (ChatGLM, ChatGPT, GPT-4) in zero-shot and few-shot molecule optimization benchmarks reported in this paper.",
            "limitations_challenges": "Input length / hardware limits restrict to ≤9 shots; zero-shot optimization currently limited to compositions of up to two known properties and cannot handle arbitrary natural-language instructions robustly; GMR has difficulty representing a small subset of complex molecules and lacks full standardization, causing occasional encoding/decoding errors; internal inconsistencies in reported training-data scale in the paper.",
            "uuid": "e3416.0",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI conversational GPT model)",
            "brief_description": "A general-purpose conversational large language model from OpenAI used here as a baseline for zero-shot/few-shot molecule optimization via prompt engineering to produce SMILES-like outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Proprietary large transformer-based language model pre-trained on broad web-scale text corpora; used via OpenAI API in the paper with custom prompts that include molecule descriptions and example modifications.",
            "generation_method": "Prompt engineering / in-context examples to request modified molecules (SMILES) from the model in zero-shot/few-shot settings.",
            "application_domain": "Attempted molecule optimization (drug-like molecules) in zero-shot and few-shot experiments.",
            "evaluation_metrics": "Same evaluation pipeline as DrugLLM: success rate (following example modification rules), validity of generated SMILES, property calculation via RDKit scripts.",
            "results_summary": "ChatGPT was able to understand instructions and optimize some given molecules but achieved relatively low success rates compared to DrugLLM. Per Table 3 examples: QED & FractionCSP3 ~11%, QED & TPSA ~15%, QED & #Rotatable bonds ~15%, LogP & FractionCSP3 ~30%, LogP & TPSA ~19%, LogP & #Rotatable bonds ~19% (values reported in table are fractional; interpreted as percentages).",
            "comparison_to_baselines": "Underperformed DrugLLM on zero-shot molecule optimization; performed better than ChatGLM but worse than GPT-4 on some tasks (per provided numbers), and worse than DrugLLM across evaluated tasks.",
            "limitations_challenges": "Lower chemical-domain specialization leading to lower success rates; general LLMs produce fewer valid/appropriate SMILES for chemistry tasks; performance depends heavily on prompt design; overall limited ability to map semantic instructions to structural property changes compared with DrugLLM trained on molecular-modification corpora.",
            "uuid": "e3416.1",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art general-purpose LLM used as a baseline for zero-shot molecule optimization via API prompting; produced some valid molecular outputs but with limited success relative to DrugLLM.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary large multimodal transformer model from OpenAI trained on very large text (and possibly multimodal) corpora; accessed via API for experiments in this paper.",
            "generation_method": "Prompt engineering / in-context examples to elicit SMILES or molecule-descriptions for optimization tasks in zero-shot settings.",
            "application_domain": "Zero-shot molecule optimization for drug-like molecule properties.",
            "evaluation_metrics": "Success rate per task (as in Table 3); validity of generated SMILES and property changes measured via RDKit scripts.",
            "results_summary": "GPT-4 generated molecules that sometimes met optimization instructions but had modest success rates generally lower than DrugLLM; reported example success rates (Table 3): QED & FractionCSP3 ~20%, QED & TPSA ~20%, QED & #Rotatable bonds ~10%, LogP & FractionCSP3 ~40%, LogP & TPSA ~43%, LogP & #Rotatable bonds ~5% (table entries interpreted as fractional values).",
            "comparison_to_baselines": "Better than ChatGLM on many zero-shot tasks; worse overall than DrugLLM in these chemistry-specific zero-shot benchmarks.",
            "limitations_challenges": "As a general-domain model, lacks specialized molecule-structure/property training and therefore shows limited ability to consistently map textual optimization instructions to valid structural changes; results sensitive to prompt engineering.",
            "uuid": "e3416.2",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGLM",
            "name_full": "ChatGLM",
            "brief_description": "A general-purpose open bilingual pre-trained model tested locally as a baseline for zero-shot molecule optimization; struggled to produce appropriate molecules and often duplicated inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGLM",
            "model_description": "Open bilingual pre-trained LLM (GLM family) deployed locally using official released code and pre-trained weights for evaluation in this study.",
            "generation_method": "Prompt engineering / in-context examples to produce SMILES-like outputs for zero-shot optimization tasks.",
            "application_domain": "Zero-shot molecule optimization attempts.",
            "evaluation_metrics": "Success rate (following modification instruction), validity of outputs, duplication of inputs.",
            "results_summary": "ChatGLM largely failed to provide appropriate molecules for zero-shot tasks; many outputs duplicated the input molecules; reported success rates in Table 3 are very low (e.g., QED & FractionCSP3 2%, QED & TPSA 3%, LogP & FractionCSP3 3%, etc.).",
            "comparison_to_baselines": "Performed worse than ChatGPT, GPT-4, and DrugLLM on zero-shot molecule optimization.",
            "limitations_challenges": "Tendency to output duplicates of the input molecule rather than applying meaningful modifications; poor mapping from natural language instructions to chemical structure changes for these tasks.",
            "uuid": "e3416.3",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (Large Language Model from Meta)",
            "brief_description": "An open foundation language model whose 7B parameter configuration served as the backbone for DrugLLM; when used as a general LLM baseline (unadapted) it failed to generate valid SMILES in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_description": "Open foundation transformer language model (LLaMA family); DrugLLM adopts LLaMA 7B parameters and architecture as the base model; the paper also reports that vanilla LLaMA (and similar general LLMs) were unable to produce valid SMILES when used directly for molecule generation.",
            "generation_method": "When used directly (baseline), prompted to produce SMILES; in DrugLLM, LLaMA architecture is fine-tuned/trained on GMR-encoded molecular modification paragraphs to autoregressively generate molecules.",
            "application_domain": "Backbone architecture for drug-design LLM (DrugLLM); also evaluated as an unadapted baseline (unable to produce valid SMILES).",
            "evaluation_metrics": "Validity of SMILES outputs, success rate for optimization tasks when tested as a baseline.",
            "results_summary": "As the backbone, enabled DrugLLM to achieve few-shot molecule generation success; as a general LLaMA baseline it could not generate valid SMILES and thus was not used in comparative zero-shot evaluation.",
            "comparison_to_baselines": "Unadapted LLaMA underperforms when used for chemistry tasks; adapted (DrugLLM) version substantially outperforms general LLaMA baseline due to domain-specific training with GMR data.",
            "limitations_challenges": "Vanilla general-purpose LLaMA lacks domain-specific chemical training and tokenization to reliably produce valid chemical strings; requires adaptation (as in DrugLLM) to be useful for molecule generation.",
            "uuid": "e3416.4",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DrugGPT",
            "name_full": "DrugGPT",
            "brief_description": "A GPT-based model referenced in related work that uses SMILES as its molecular representation for ligand design.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DrugGPT",
            "model_description": "Mentioned as a GPT-based strategy that uses SMILES tokens as representation (Li et al., 2023 referenced); no experimental details from this paper beyond the citation.",
            "generation_method": "Reported (in cited work) to use SMILES-based generation (not evaluated in this paper).",
            "application_domain": "Drug/ligand design (as reported by the cited work).",
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_baselines": null,
            "limitations_challenges": null,
            "uuid": "e3416.5",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT",
            "brief_description": "A biomedical-domain GPT-style model mentioned in related work for biomedical text generation/mining, not evaluated here for molecule generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BioGPT",
            "model_description": "Generative pre-trained transformer specialized for biomedical text generation and mining (referenced Luo et al., 2022); no experiments on molecule-generation reported in this paper.",
            "generation_method": null,
            "application_domain": "Biomedical text generation (as per cited literature).",
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_baselines": null,
            "limitations_challenges": null,
            "uuid": "e3416.6",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Alpaca",
            "name_full": "Alpaca",
            "brief_description": "An instruction-following model (reference) mentioned as one of several general LLMs that were unable to generate valid SMILES strings in the authors' tests.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Alpaca",
            "model_description": "Instruction-following LLM referenced (Maeng et al. citation) and reported in this paper as failing to produce valid SMILES when tested as a baseline.",
            "generation_method": "Prompting / instruction-following (when tested as baseline); failed to output valid SMILES.",
            "application_domain": "Evaluated as general LLM baseline for molecule generation (failed).",
            "evaluation_metrics": "Validity of SMILES outputs (failed).",
            "results_summary": "Unable to generate valid SMILES in the experiments described; therefore not included in zero-shot comparative evaluation.",
            "comparison_to_baselines": "Performed worse than ChatGPT/GPT-4/ChatGLM/DrugLLM (in that it could not generate valid SMILES).",
            "limitations_challenges": "General-purpose instruction tuning insufficient to produce chemically valid SMILES without domain-specific training or representation.",
            "uuid": "e3416.7",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna",
            "brief_description": "A fine-tuned conversational LLM mentioned as another general model unable to generate valid SMILES in the authors' tests.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Vicuna",
            "model_description": "Open conversational LLM referenced (Chiang et al., 2023); the paper reports it was unable to generate valid SMILES strings when used directly for molecule generation.",
            "generation_method": "Prompting / conversational interface (when tested as baseline); failed to produce valid SMILES.",
            "application_domain": "Evaluated as general LLM baseline for molecule generation (failed).",
            "evaluation_metrics": "Validity of SMILES outputs (failed).",
            "results_summary": "Unable to generate valid SMILES in the experiments described; excluded from zero-shot evaluation comparisons.",
            "comparison_to_baselines": "Not competitive due to invalid outputs versus specialized DrugLLM.",
            "limitations_challenges": "Lacks chemistry-specific representation/training needed to produce valid molecular strings.",
            "uuid": "e3416.8",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins",
            "rating": 2,
            "sanitized_title": "druggpt_a_gptbased_strategy_for_designing_potential_ligands_targeting_specific_proteins"
        },
        {
            "paper_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 2,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 1,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
            "rating": 2,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        }
    ],
    "cost": 0.014943,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DrugLLM: Open Large Language Model for Few-shot Molecule Generation
7 May 2024</p>
<p>Xianggen Liu 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Yan Guo 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Haoran Li 
Jin Liu 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Department of Anesthesiology
Laboratory of Anesthesia and Critical Care Medicine
Frontiers Science Center for Disease-related Molecular Network
National-Local Joint Engineering Research Centre of Translational Medicine of Anesthesiology
West China Hospital
Sichuan University
610041ChengduChina</p>
<p>Shudong Huang 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Information Technology Research Center
Beijing Academy of Agriculture and Forestry Sciences
100097BeijingChina</p>
<p>Bowen Ke 
Department of Anesthesiology
Laboratory of Anesthesia and Critical Care Medicine
Frontiers Science Center for Disease-related Molecular Network
National-Local Joint Engineering Research Centre of Translational Medicine of Anesthesiology
West China Hospital
Sichuan University
610041ChengduChina</p>
<p>Jiancheng Lv 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>DrugLLM: Open Large Language Model for Few-shot Molecule Generation
7 May 202443D72A47FAC47A99D5D0B57EC2745F6CarXiv:2405.06690v1[q-bio.BM]large language modelfew-shot learningmolecule generation
Large Language Models (LLMs) have made great strides in areas such as language processing and computer vision.Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry.For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties.Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded.In this work, we introduced DrugLLM, a LLM tailored for drug design.During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties.DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications.Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity.</p>
<p>Introduction</p>
<p>Small molecules play a critical role in drug discovery due to their ability to bind to specific biological targets and modulate their functions (Kovachka et al., 2024;Offensperger et al., 2024;Scott et al., 2016).In fact, over the past decade, according to the approval records of the U.S. Food and Drug Administration (FDA), small molecule drugs have accounted for 76% of the total number of drugs approved for the market (Brown and Wobst, 2021;Norsworthy et al., 2024).Small molecules are advantageous in drug discovery because they can be synthesized relatively easily and have good bioavailability, making them more likely to reach their intended targets in vivo (especially when passing through cell membranes) (Vargason et al., 2021).However, based on the current research technologies, it is very challenging to design a molecule with ideal properties, and it will consume a lot of resources and time.For example, in the drug development process, it takes 9 to 12 years and billions of dollars to find an effective drug (Adams and Brantner, 2006;Dickson and Gagnon, 2009).</p>
<p>The vastness of the search space for new molecules, with up to 10 60 synthetically creatable drug-like molecules, presents a significant challenge in drug design as chemists must navigate this immense space to identify molecules that interact with a biological target, and while modern techniques allow for the testing of over 10 6 molecules in a laboratory setting, larger experiments become prohibitively expensive (Segler et al., 2018).As such, computational tools are necessary to help narrow down the search space.Virtual screening is one such strategy used to identify promising molecules from millions of existing or billions of virtual molecules (Lyu et al., 2023).But highthroughput screening and virtual screening only consider known molecules that are synthetically accessible and fall short of producing novel molecules (Crunkhorn, 2022;Sadybekov et al., 2022).</p>
<p>As an alternative to exploring the huge molecule space, de novo drug design exhibits the remarkable ability to generate entirely novel and distinctive molecules (Ren et al., 2024;Tropsha et al., 2024).Traditional de novo drug design methods use molecular construction rules to generate new molecules based on the receptor structure (Gillet et al., 1995;Waszkowycz et al., 1994) or the ligand structure (Afantitis et al., 2011).Recently, deep learning and reinforcement learning techniques offer promising potential in de novo drug design due to their powerfutl approximation capacity (Blaschke et al., 2018;Li et al., 2018;Liu et al., 2020;Putin et al., 2018).In particular, Popova et al. (2018) integrates both generative and predictive neural networks to generate novel targeted chemical libraries in a trial-and-error manner.Jin et al. (2018) propose a junction treebased variational autoencoder to learn a continuous molecule space and generate new molecules by sampling from it.</p>
<p>Despite the development of various deep learning (DL)-based methods for de novo drug design, the field of few-shot molecule generation remains notably underexplored.Few-shot molecule generation aims to generate new molecules with expected properties given limited molecule examples.Most of the current de novo drug design approaches require thousands of data for learning (Korshunova et al., 2022).However, data scarcity is a prevalent issue in drug discovery due to the high costs associated with biological experimentation (Wang et al., 2023).Consequently, the ability to perform few-shot generation is of paramount importance for the advancement of de novo drug design techniques.</p>
<p>The large language models (LLMs) have achieved significant progress in natural language processing, especially in the few-shot learning problem (Ahmed and Devanbu, 2022).Despite the emergence of diverse LLMs (Chang et al., 2024), they fall short in handling the languages in biology and chemistry (Ross et al., 2022).For example, they are still struggling to capture the relationship between molecule structure and the corresponding properties.Therefore, how do we build an LLM that can accurately characterize the "structure-effect-metabolism-toxicity" relationships in molecules?</p>
<p>In this work, we presented DrugLLM, a large language model for drug design.In DrugLLM, molecules are represented using Group-based Molecular Representation (GMR), which is a novel type of molecular representation to address token abundance, cyclic complexity, and structural sensitivity inherent in SMILES (O'Boyle, 2012).GMR makes structural groups as units to build the topological structure of molecules and transform them into linear sequences.</p>
<p>Furthermore, we will provide an in-depth exposition of the training methodology employed by DrugLLM.The methodology organizes the modification sequences in accordance with specific molecular properties.Drawing an analogy, a case of molecule modification (a pair of molecules with similar structures) toward a certain property serves as a "sentence".Multiple cases of modification toward the identical property constitute a paragraph.By continuously predicting the next molecule based on the modification history, DrugLLM learns the intrinsic relationship between the molecular structure and the corresponding property.To the best of our knowledge, DrugLLM is the first large language model for few-shot molecule generation.</p>
<p>Results</p>
<p>The DrugLLM Framework</p>
<p>The focus of this work is to train a large language model that can capture the relationship between the molecule structure and the corresponding chemical and biological activities.Unlike ChatGPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023), which are trained on massive text data from the internet, and DrugGPT (Li et al., 2023)that uses SMILES as its representation, DrugLLM employs Group-based Molecular RepresSMILESentation (GMR) strings as its primary language representation.GMR leverages structural groups to depict molecular architectures, thereby effectively overcoming three principal challenges inherent in the application of SMILES notation within model processing contexts: (1) Token Abundance: In the SMILES format, each character is considered a separate token, which can result in an unwieldy number of tokens and subsequently consume considerable computational resources during training.(2) Cyclic Complexity: The representation of cyclic structures within molecules is particularly intricate in SMILES, which increases the difficulty of model training.(3) Structural Sensitivity: Even minor alterations in a molecule's structure can produce significant discrepancies in the corresponding SMILES representation.</p>
<p>As shown in Figure 1A, the GMR framework employs unique string identifiers to represent distinct structural groups.These identifiers are linked by numerical position data flanked by a slash.By employing GMR, the model can recognize molecular strings by treating structural groups as units, thereby reducing the number of input and output tokens.Additionally, GMR removes cyclic structures by merging them, simplifying the logic of molecular assembly and lowering the difficulty of model recognition.It also minimizes the discrepancies in SMILES strings that result from even small structural changes.</p>
<p>To train DrugLLM, we construct sentences and paragraphs composed of molecule modifications as training data (Figure 1B).Specifically, DrugLLM regards the modification between two molecules with similar structures as a sentence.A series of such modifications are viewed as a sequence of sentences that form a paragraph.Note that we impose the constraint that the molecular modifications in a paragraph should characterize the identical property.For instance, if the first three cases of molecule modifications describe the increase in the number of hydrogen bond acceptors, we expect all subsequent sentences in that paragraph to also discuss the increase in acceptor numbers.In this way, the contents of a paragraph are concentrated, making DrugLLM able to auto-regressively predict the next token based on the previous contexts.Moreover, each paragraph exhibits autonomy, encompassing a diverse array of molecular characteristics.The distinct paragraphs engage with unique molecular properties, necessitating that DrugLLM be endowed with the capability for in-context learning (a form of few-shot learning).</p>
<p>However, there are few related datasets available.In this work, we collect the tabular form of the molecule datasets from the ZINC database (Wu et al., 2018) and the ChEMBL platform (Davies et al., 2015;Mendez et al., 2019), and convert them into the corresponding sentences and paragraphs.In total, we collected over 25, 000, 000 modification paragraphs and 200, 000, 000 molecules to build the training dataset (Table 1).The dataset involves over 10,000 different molecular properties or activities, such as the count of hydrogen bond acceptors and the topological  Following recent work on the pre-training large language models, DrugLLM is based on the Transformer architecture (Vaswani et al., 2017).We adopt the parameters of LLama 7B (Touvron et al., 2023) and expand the vocabulary by introducing the frequently-used SMILES tokens.These tokens are divided by the byte pair encoding (BPE) algorithm (Sennrich et al., 2016).We train DrugLLM using the AdamW optimizer for six weeks on eight NVIDIA RTX 3090 GPUs, where it learns to generate the paragraphs from scratch.In the view of machine learning, the paragraph plays as a process of few-shot molecule generation.Therefore, the trained DrugLLM is able to directly perform few-shot molecule generation without further fine-tuning.</p>
<p>DrugLLM is a few-shot learner in molecule optimization toward physicochemical properties</p>
<p>Figure 2A illustrates our approach under the K-shot learning framework, where we provide the model with K pairs of example modifications and a benchmark molecule.The objective of the model is to generate a new molecule that not only maintains structural similarity to the benchmark molecule but also exhibits superior properties (either increased or decreased, as guided by the examples).Due to input token limitations, we restrict the number of molecular optimization examples to a maximum of nine pairs.To visually represent the structural similarity between the benchmark and generated molecules in the chemical space, we employ the Uniform Manifold Approximation and Projection (UMAP) method to create a chart (Figure 2B).There is a high degree of consistency between the distribution of the generated molecules (on the left) and the source molecules (on the right).This distributional similarity, coupled with the notable improvement in the LogP properties of the generated molecules, underscores the robust capability of the model to optimize the properties of the benchmark molecule.</p>
<p>To evaluate the capacity of DrugLLM in terms of few-shot molecule generation, we select four physicochemical properties that are not seen by DrugLLM as the test tasks, including the wateroctanol partition coefficient (LogP), solubility, synthetic accessibility, and topological polar surface area (TPSA).As these four molecular properties can be accurately estimated by machine learningbased scripts, they are widely used in the assessment of molecule generation models.For comparison, we take the junction tree-based variational auto-encoder (JTVAE) (Jin et al., 2018), the variational junction tree neural network (VJTNN) (Jin et al., 2020), and the scaffold-based molecule generator (MoLeR) (Maziarz et al., 2021).We also include a random generation control implemented by random sampling based on the latent space of JTVAE.The quality of the generated molecules was assessed based on their success rate and molecular similarity.The success rate represents the proportion of generated molecules that adhere to the rule of examples of modifications.To avoid the generation bias of the generators, the input contexts (i.e., the prompts of the language models) describe the increment or decrement of the property with a balanced proportion.Although these models are not initially designed for few-shot learning, they are state-of-the-art molecule generators in literature.</p>
<p>We first present the distributions of several key properties -LogP, solubility, synthetic accessibility, and TPSA -for both the source and generated data in Figure 3A.These distributions are visualized using Kernel Density Estimation (KDE).The significant numerical improvements of the model in optimizing these key properties are clearly demonstrated, further attesting to the effectiveness of the model in optimizing molecular properties.Next, as shown in Figure 3B, we report the performance of few-shot generation with respect to the LogP value.We note that the three baseline molecule generators, namely JTVAE, VJTNN, and MoLeR, just obtained a success rate of about 50%, which is similar to a random generation.In contrast, DrugLLM exhibits a progressive improvement in few-shot molecule generation, with the accuracy of the generated molecules increasing incrementally to 75% as the number of shots increases.Performance comparisons on molecular solubility, synthetic accessibility, and TPSA are similar and consistent.When it comes to similarity, it is typically more challenging to optimize a molecule with fewer modifications (i.e., higher similarity).Despite this, DrugLLM maintains a higher success rate even with increased generation similarity, underscoring its superior performance in the few-shot generation.Furthermore, we note that DrugLLM-GMR slightly outperforms DrugLLM-SMILES, highlighting the benefits of GMR in large model training.</p>
<p>DrugLLM is a few-shot learner in molecule optimization towards biological activities</p>
<p>Since DrugLLM shows impressive few-shot generation capacity in physicochemical properties, we next validate the effectiveness of DrugLLM in the biological activities of molecules, which is more challenging.The molecules produced by DrugLLM are usually novel and are not recorded in the ChEMBL database.Unlike the physicochemical properties mentioned above, the biological activities of molecules (e.g. the Ki value on streptokinase A) are difficult to estimate by chemical or physical rules.In addition, the lengthy time and high costs associated with wet-lab experiments hinder the large-scale evaluation of molecules.Instead, we leverage a message-passing neural network to predict biological activities.Specifically, before building the DrugLLM dataset through the ChEMBL database, we scan all biological activities and select the one that has relatively adequate samples (N ≥ 800) and could induce an accurate property predictor (Pearson r ≥ 0.75).Finally, 10 activities are obtained and excluded from the training data.These activities are used to test the optimization of the few-shot DrugLLM.As the Pearson correlation of the prediction of the predictor is greater than 0.75, it could correlate well with the real evaluation in statistics.</p>
<p>We observe that the three generator baselines fail to obtain meaningful improvement compared with the random generation (Table 2), indicating that these molecule generators still struggle to capture the modification rules underlying the limited examples.As for DrugLLM, it significantly outperforms the other baselines by a large margin in most of the test properties.In particular, DrugLLM can generate appropriate molecules that bind to Rho-associated protein kinase 1, with a success rate of 76%.Note that these test properties are not observable in the training of DrugLLM.These results demonstrate that DrugLLM is able to figure out the intrinsic rules of the molecule modifications given a limited number of examples for an unknown molecular property.</p>
<p>DrugLLM support instruction guided molecule optimization in a zero-shot manner</p>
<p>Previous experiments demonstrated that DrugLLM can accept multiple pairs (i.e., shots) of molecules as references to learn the modification rules and generate new molecules with the desired properties.In this section, we explore zero-shot molecule optimization, which involves generating modified molecules with better properties of interest according to natural language instruction without any specific training instances.In this experimental setting, we assume that when DrugLLM is trained on a large scale of properties and their compositions, it is capable of generalizing to optimize the molecules toward unseen compositions of properties.Therefore, we move six optimization tasks out of the DrugLLM training set in advance and leave them as test tasks.For example, all samples related to the optimization of quantitative estimation of drug similarity (QED) and topological polar surface area (TPSA) are in the test set, but the samples related to the optimization of each single property are in the training set.Based on this setting, we build the test set that contains 6, 000 instructions, each optimization task for 1000.The generated molecules are evaluated by the Python scripts via the RDKit library.</p>
<p>It is noteworthy that DrugLLM is one of the very few approaches that support zero-shot molecule optimization.Apart from ChatGPT (Brown et al., 2020), GPT-4(Achiam et al., 2023), and ChatGLM (Zeng et al., 2022), the other large language models, e.g., LLaMA (Touvron et al., 2023), Alpaca (Maeng et al., 2017), and Vicuna (Chiang et al., 2023), were unable to generate valid SMILES strings of molecules.Thus, we only compared the zero-shot molecule optimization capacity between DrugLLM and ChatGPT, GPT-4, and ChatGLM, all of which are trained on thousands of billion tokens.The challenge of zero-shot molecule optimization lies in two folds.On the one hand, the mapping between semantics and molecular property is hard to learn from the general corpus.On the other hand, the biological data related to the structure-property relationship is not sufficient due to the lengthy time and high costs associated with wet-lab experiments.As a result, we observed that ChatGLM struggled to provide appropriate molecules on all the zero-shot molecule optimization tasks (Table 3), with most generations outputting the duplicated molecules with the input ones.In addition, ChatGPT and GPT-4 were able to understand the instructions and optimize some of the given molecules.However, the success rate is relatively low.In terms of DrugLLM, it improves the optimization success rates by significant margins compared with the other LLM, indicating a better capacity in instruction understanding and molecule optimization.</p>
<p>Discussion</p>
<p>In this study, we introduce a computational task named few-shot molecule optimization, which is one of the few-shot generation problems.Given a molecule of interest, the task involves generating new molecules that adhere to the rules underlying the few modification examples.Although various few-shot learning tasks have been proposed and investigated (Ma et al., 2021;Stanley et al., 2021;Zhang et al., 2024), there are few studies that consider few-shot molecule generation.Few-shot molecule optimization requires the model to capture the abstract rules in a small number of examples and apply the rules to new molecules, requiring a comprehensive understanding of "structure-effect-metabolism-toxicity" relationships.As expected, the current methods struggle to accomplish few-shot molecule optimization, including ChatGPT and the other competitive molecule generators.In comparison, DrugLLM exhibits impressive performance, taking a solidified step toward general artificial intelligence in drug design.</p>
<p>DrugLLM is a large language model (LLM), built on a large number of textual data that span a wide variety of small molecules and biological domains.Recently, LLMs, such as ChatGPT (Brown et al., 2020), Alpaca (Maeng et al., 2017), and ChatGLM (Zeng et al., 2022), have amazing capabilities for general-purpose natural language generation.However, they are designed for general use and lack profound biological and pharmaceutical knowledge.We notice that there are also several LLMs for biological and medical fields, such as BioGPT (Luo et al., 2022) and DrugGPT (Li et al., 2023).However, these LLMs still follow traditional training strategies that learn to generate natural language as in the biomedical article.This raises an open question that how LLM understands the language of biology and chemistry and how to perform few-shot learning in this field.In this work, we propose a novel solution in which DrugLLM iteratively performs similar molecule modifications according to the context using GMR.Experiments demonstrated its exceptional effectiveness in a few-shot molecule optimization.</p>
<p>Despite the advantages of our method, this study has several limitations.Firstly, DrugLLM merely supports up to 9 shots of molecule modifications due to the limitation of the hardware.In such an input length, we have validated the few-shot learning capacity of DrugLLM.In the future, we will increase the input length (i.e., the number of shots) to achieve more impressive performance in drug design.Secondly, the zero-shot molecule optimization of DrugLLM is relatively elementary.The current DrugLLM is only capable of optimizing the molecules toward the composition of two known molecular properties.The zero-shot learning ability for arbitrary instructions is still lagging behind.Thirdly, the GMR currently in use has difficulty representing a small number of complex molecules under certain circumstances and lacks certain standardization measures.In the future, we will optimize for special cases of GMR representation and add standardization methods to reduce the small number of encoding errors that the model may generate.</p>
<p>In conclusion, this study presents the first attempt to build a large language model for fewshot molecule generation and optimization.Based on the tabular data related to molecular properties and biological activities, we build a large-scale textual corpus in the format of the sequences of molecule modifications.DrugLLM is trained to predict the next molecules based on historical modifications in an autoregressive manner.In extensive computational experiments, we observed that DrugLLM surpassed all the competitive methods (including GPT4) in optimizing new molecules in the few-shot learning setting on over 20 properties or biological activities.These results establish the substantial enhancement of efficacy facilitated by our proposed methodology in molecule generation and optimization, highlighting the potential of DrugLLM as a powerful computational tool in drug discovery.</p>
<p>METHOD DETAILS</p>
<p>Data collection and preparation</p>
<p>To train and analyze the DrugLLM model, we construct a large-scale dataset from the ZINC (Wu et al., 2018) and ChEMBL (Davies et al., 2015;Mendez et al., 2019) datasets.ZINC is a free database that contains more than 230 million purchasable compounds in ready-to-dock, 3D formats.We filter the druglike molecules from ZINC and obtain 4.5 million molecules.ChEMBL is a comprehensive repository for bioactive compounds with their properties.We gather bioactivity data from the ChEMBL database with the corresponding Web resource client.Following the preprocessing pipeline in (Stanley et al., 2021), we excluded all compounds that are not druglike molecules.A standard cleaning and canonicalization procedure was applied to filtered compounds.All of the molecules were represented by SMILES strings and labeled with certain properties.To facilitate property comparison between two molecules, we only consider property categories with real numbers.Therefore, we obtained a large-scale dataset that comprised thousands of tabular data, each table corresponding to hundreds of molecules measured by an identical property.</p>
<p>Based on the collected tabular data, we then transform them into meaningful textual sentences and paragraphs.In particular, we regard the modification between two molecules with similar structures as a sentence and multiple cases of molecular modifications as a paragraph.In the meantime, we stipulate that the molecular modifications in a paragraph should describe the same property changes.In other words, if the first two cases of molecule modifications indicate the increase of solubility, we would expect the rest sentences of this paragraph to be all about the solubility increase.The above stipulation was realized by a heuristic algorithm: given a pool of molecules with their property (in tabular form), we first clustered the molecules in terms of the molecular scaffolds with randomly selected clustering centers.If the similarity between a molecule and a center is greater than 0.6, the molecule is clustered at that center.The number of clustering centers would dynamically increase until all of the molecules in the pool are classified.</p>
<p>Apart from the modification of the molecule to a single property, we also consider the compositions of multiple properties, which are mainly involved in the simple molecular properties that can be calculated by Python scripts.For example, we include LogP, Topological polar surface area (TPSA), and their composition in the training set for model training.In total, we collect over 25 million modification paragraphs and 2 billion molecules to build the training dataset.The dataset involves over 10, 000 different molecular properties, activities, and compositions.In ad-dition to the SMILES molecule, we also added descriptions of the property optimizations in each paragraph to build the relationship between the molecule structures and the semantic meaning of the properties.</p>
<p>Group-based molecular representation (GMR)</p>
<p>The core of the GMR framework involves decomposing molecules into structural groups and noting the inter-group connections, facilitating the group-based reconstruction of SMILES strings.GMR begins by assigning a unique string identifier to each structural group, thereby constructing a comprehensive dictionary.When a molecule is processed within the GMR framework, its SMILES string is converted into an encoded string that encapsulates both the identity of the structural groups and their positional relationships within the molecule.For computational analyses or property evaluations that require the original SMILES string, the encoded string can be decoded by applying a decoding algorithm.In the specific implementation of the GMR framework, there are three key steps:</p>
<p>(1) Dictionary construction: Initially, we leverage the extensive molecular data resources available in the ChEMBL database.Using SMILES expressions, we extract information about the ring structures in the molecule, merge intersecting rings, and identify structural groups on the rings.For the nonring parts of the molecule, we employed a strategy of breaking all C-C bonds and treating the remaining molecular fragments as independent structural groups.This approach allows us to decompose all the groups of the molecule, assign a unique string identifier to each group, and construct a comprehensive dictionary.</p>
<p>(2) Molecular encoding: The encoding process, as depicted in Algorithm 1, is initiated by splitting the SMILES string of an individual molecule into several structural units.We systematically decompose the molecule, removing each structural group and verifying the connectivity of the molecule after each removal.If the molecule remains connected after removal of a structural group, we record the two atoms at the connection point.Since marking the connection point may cause changes in the atom index and affect the subsequent normalization process of the molecule, we use a breadth-first search algorithm to characterize the marked atoms and their adjacent area in detail.This forms a feature description of the atoms.After removing the mark and performing SMILES normalization on the structural group and molecular fragment after splitting, we use the previously obtained atom feature description to re-identify and record the atom index of these two atoms as connection location information.This process is repeated until the molecule is completely decomposed into a single structural group, at which point the molecular fragment serves as the starting point for encoding.Starting from the encoding starting point, we build the basis of the encoding string according to the corresponding string in the dictionary.Subsequently, we traverse the structural groups recorded during the removal process and their corresponding atom indices, using the "/" character to separate different location information.Gradually, we integrate the strings corresponding to the structural groups in the dictionary and the location information into the encoding string, eventually generating a complete and accurate molecular encoding result.</p>
<p>(3) Molecular decoding: The decoding process is essentially the reverse of the encoding process.We use the encoded molecular fragment as the starting point for splicing and gradually recombine each structural group in the correct position according to the connection information recorded in the encoding.This process is repeated until all structural groups are correctly spliced back, thereby restoring the original molecular SMILES.This ensures the integrity and reversibility of molecular information.</p>
<p>The implementations of DrugLLM</p>
<p>Similar to ChatGPT, the training objective of DrugLLM is to iteratively predict the next token of the paragraphs.Formally, a generated paragraph x is composed of the optimization description o and the molecule modifications m, given by
x = [o, m 1 , m 2 , • • • , m N ],(1)
where m n stands for the n-th case of the molecule modification.Essentially, DrugLLM is to approximate the probability
P(x t |x 1 , x 2 , • • • , x t−1 ) = DrugLLM(x 1 , x 2 , • • • , x t−1 ),(2)
where x t is a token in the paragraph x.</p>
<p>However, the testing objective of the few-shot molecule optimization is to predict the optimized molecules given the few shots of molecule modifications.That is
m g = DrugLLM(m 1 , m 2 , • • • , m K , m o ),(3)
where m g stands for the generated molecules of DrugLLM in the K shots input.m o represents the query molecule that needs to be optimized.Similarly, the testing objective of the zero-shot molecule optimization is to predict the optimized molecules given the descriptions of the optimization tasks, given by
m g = DrugLLM(o, m o ).(4)
We adopt the LLaMA model architecture as the basic backbone of our DrugLLM.Specifically, DrugLLM is a Transformer decoder with 32 layers and 32 attention heads.The hidden dimension is set to 4096 and the batch size is 64.As a result, DrugLLM has 7 billion parameters, all of which are updated during the pre-training.The training process employs the AdamW optimizer with a learning rate of 3 × 10 −5 , and we adopt a cosine annealing schedule to adjust the learning rate.</p>
<p>The implementations of the competitive baselines</p>
<p>There are very few methods that support few-shot molecule optimization or generation.In this work, we take the widely used and powerful molecule generators as baselines, including JTVAE, VJTNN, MoLeR, and a control model (random generation).For these methods, we used the released official codes and adapted them to perform few-shot optimization tasks.For JTVAE, which is designed to generate molecules from a pre-trained latent space, we follow the convention to optimize the molecules based on the released pre-trained JTVAE model.To accomplish the K-shot optimization, we leveraged the K modification cases as the training samples and used the trained JTVAE to generate the new molecules for evaluation.The evaluation procedures of VJTNN and MoLeR are similar to JTVAE except for the exclusion of the pre-trained model.The random generation model is implemented by random sampling based on the latent space of the pre-trained JTVAE.</p>
<p>We further incorporated advanced large language models as benchmarks, including Chat-GLM, ChatGPT, and GPT-4.These models require carefully constructed prompts to generate relevant output data.In crafting these prompts, we integrated the characteristic descriptions of the molecules to be optimized and instances of a few-shot optimization to ensure that the model can accurately understand the task requirements.For ChatGLM, we used the officially released code and pre-trained models, deployed and executed in a local server environment, to obtain the model prediction results.In contrast, for ChatGPT and GPT-4, we utilized their online service capabilities by transmitting the processed input data through their official API interfaces, thereby eliciting the corresponding model outputs from remote servers.</p>
<p>Figure 1 :
1
Figure 1: Schematic overview of the DrugLLM framework.A, Construction process.Group-based Molecular Representation (GMR) is constructed from molecular structure units.B, Training framework.DrugLLM is trained on molecular modifications, with each paragraph representing a unique attribute.Each paragraph is self-contained and represents multiple characteristics, with different paragraphs corresponding to different attributes.</p>
<p>Figure 2 :
2
Figure 2: Visualization of few-shot molecule optimization.A, The training and testing examples of fewshot molecule optimization.B, Chemical space navigation by transfer learning.The UMAP plot shows the distribution of 15000 molecules selected from the source space and their corresponding 15000 molecules in the generated space after LogP property optimization.Different values of LogP are represented by different colors.</p>
<p>Figure 3 :
3
Figure 3: The performance of few-shot molecule optimization in physiochemical properties.A, The distribution of the water-octanol partition coefficient (LogP), Solubility, Synthetic Accessibility, and Topological Polar Surface Area (TPSA) for the source data and the generated data, represented by Kernel Density Estimation (KDE).The KDE demonstrations of the source data and the generated data are displayed by blue solid lines and red solid lines, respectively.Histograms (hist) for the source data and the generated data are represented by blue bars and red bars, respectively.B, The performance of the generation methods in terms of the success rates and the generation similarities.</p>
<p>Table 1 :
1
Pre-training data of DrugLLM.The training data include ZINC and ChEMBL databases.Similar molecules are collected together to build the modification paragraphs.A modification paragraph contains multiple molecule modifications that aim to improve or decrease the same molecular properties.TPSA).Considering that the few-shot learning capability of machine learning models arises from their exposure to a sufficient variety of training tasks, a large scale of different paragraphs could enforce DrugLLM captures the intrinsic nature of molecule design in a few-shot fashion.
Dataset# of molecules # of paragraphs # of tasks Disk sizeZINC4.5M0.6M770780MChEMBL180.2M24M1010030Gpolar surface area (</p>
<p>Table 2 :
2
The performance of few-shot molecule optimization toward biological activities
Target</p>
<p>Table 3 :
3
The success rates (%) of individual methods in zero-shot molecule optimization.
MethodChatGLM ChatGPT GPT-4 DrugLLMQED &amp; FractionCSP30.020.110.200.40QED &amp; TPSA0.030.150.200.47QED &amp; # Rotatable bonds0.060.150.100.59LogP &amp; FractionCSP30.030.300.400.60LogP &amp; TPSA0.040.190.430.55LogP &amp; # Rotatable bonds0.040.190.050.61</p>
<p>← moleculeAtom + "/" + groupAtom + group
Algorithm 1 Molecular encoding algorithm1: procedure ENCODEMOLECULE(molecule)2:unprocessedMolecularGroups ← splitIntoGroups(molecule)3:groupConnections ← []4:while len(unprocessedMolecularGroups) &gt; 1 do5:remainingGroups ← ∅6:for currentGroup ∈ unprocessedMolecularGroups do7:molecule, isProcessed ← ProcessGroup(molecule, currentGroup, groupConnections)8:if not isProcessed then9:remainingGroups ← remainingGroups ∪ {currentGroup}10:end if11:end for12:unprocessedMolecularGroups ← remainingGroups13:end while14:initialGroup ← first(unprocessedMolecularGroups)15:moleculeEncoding ← getDictionaryString(initialGroup)16:for all connectionItem ∈ groupConnections do17:moleculeAtom ← connectionItem[molConnectedAtom]18:groupAtom ← connectionItem[groupConnectedAtom]19:group ← getDictionaryString(connectionItem[group])20:21:moleculeEncoding ← moleculeEncoding + connectionEncoding22:end for23:return moleculeEncoding24: end procedure
connectionEncoding</p>
<p>ACKNOWLEDGMENTSThis work was supported by the National Natural Science Foundation of China (No. 62206192); the Natural Science Foundation of Sichuan Province (No. 2023NS-36 FSC1408); the Science and Technology Major Project of Sichuan Province; and the Fundamental Research Funds for the Central Universities (No. 1082204112364).
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Estimating the cost of new drug development: is it really $802 million?. C P Adams, V V Brantner, Health Aff (Millwood). 2522006</p>
<p>Ligand-based virtual screening procedure for the prediction and the identification of novel β-amyloid aggregation inhibitors using kohonen maps and counterpropagation artificial neural networks. A Afantitis, G Melagraki, P A Koutentis, H Sarimveis, G Kollias, Eur. J. Med. Chem. 4622011</p>
<p>Few-shot training llms for project-specific code-summarization. T Ahmed, P Devanbu, Proc. IEEE/ACM Int. Conf. Autom. Softw. Eng. 2022</p>
<p>Application of generative autoencoder in de novo molecular design. T Blaschke, M Olivecrona, O Engkvist, J Bajorath, H Chen, Mol Inform. 371-217001232018</p>
<p>): trends and future directions. D G Brown, H J Wobst, J. Med. Chem. 6452021. 2010-2019A decade of fda-approved drugs</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv. Condens. Matter Phys. 332020</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Trans Intell Syst Technol. 1532024</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>Screening ultra-large virtual libraries. S Crunkhorn, Nat. Rev. Drug Discov. 21952022</p>
<p>M Davies, M Nowotka, G Papadatos, N Dedman, A Gaulton, F Atkinson, L Bellis, J P Overington, Chembl web services: streamlining access to drug discovery data and utilities. 201543</p>
<p>The cost of new drug discovery and development. M Dickson, J P Gagnon, Discov Med. 4222009</p>
<p>Sprout, hippo and caesa: Tools for de novo structure generation and estimation of synthetic accessibility. V J Gillet, G Myatt, Z Zsoldos, A P Johnson, Perspect. Drug Discovery Des. 31995</p>
<p>Junction tree variational autoencoder for molecular graph generation, ICML. W Jin, R Barzilay, T Jaakkola, 2018</p>
<p>Hierarchical generation of molecular graphs using structural motifs, ICML. W Jin, R Barzilay, T Jaakkola, 2020</p>
<p>Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds. M Korshunova, N Huang, S Capuzzi, Commun Chem. 51292022</p>
<p>Small molecule approaches to targeting rna. S Kovachka, M Panosetti, B Grimaldi, S Azoulay, A Di Giorgio, M Duca, Nat. Rev. Chem. 2024</p>
<p>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. Y Li, C Gao, X Song, X Wang, Y Xu, S Han, 2023</p>
<p>Multi-objective de novo drug design with conditional graph generative model. Y Li, L Zhang, Z Liu, J. Cheminformatics. 102018</p>
<p>A chance-constrained generative framework for sequence optimization, ICML. X Liu, Q Liu, S Song, J Peng, 2020</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Brief. Bioinformatics. 2364092022</p>
<p>Modeling the expansion of virtual screening libraries. J Lyu, J Irwin, B Shoichet, Nat Chem Biol. 192023</p>
<p>Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients. J Ma, S H Fong, Y Luo, C J Bakkenist, J P Shen, S Mourragui, L F Wessels, M Hafner, R Sharan, J Peng, Nat Cancer. 222021</p>
<p>Alpaca: Intermittent execution without checkpoints. K Maeng, A Colin, B Lucia, PACMPL. 12017</p>
<p>K Maziarz, H Jackson-Flux, P Cameron, arXiv:2103.03864Learning to extend molecular scaffolds with structural motifs. 2021arXiv preprint</p>
<p>Chembl: towards direct deposition of bioassay data. D Mendez, A Gaulton, A P Bento, J Chambers, M De Veij, E Félix, M P Magariños, J F Mosquera, P Mutowo, M Nowotka, Nucleic Acids Res. 47D12019</p>
<p>Fda approvals in 2023: biomarker-positive subsets, equipoise and verification of benefit. K J Norsworthy, R J Lee-Alonzo, R Pazdur, Nat Rev Clin Oncol. 2024</p>
<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells. F Offensperger, G Tin, M Duran-Frigola, E Hahn, S Dobner, C W Ende, J W Strohbach, A Rukavina, V Brennsteiner, K Ogilvie, Science. 384669458642024</p>
<p>Towards a universal smiles representation-a standard method to generate canonical smiles based on the inchi. N M O'boyle, J. Cheminformatics. 42012</p>
<p>Deep reinforcement learning for de novo drug design. M Popova, Sci. Adv. 478852018</p>
<p>Reinforced adversarial neural computer for de novo molecular design. E Putin, A Asadulaev, Y Ivanenkov, V Aladinskiy, B Sanchez-Lengeling, A Aspuru-Guzik, A Zhavoronkov, J Chem Inf Model. 5862018</p>
<p>A small-molecule tnik inhibitor targets fibrosis in preclinical and clinical models. F Ren, A Aliper, J Chen, H Zhao, S Rao, C Kuppe, I V Ozerov, M Zhang, K Witte, C Kruse, Nature Biotechnology. 2024</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nature Machine Intelligence. 4122022</p>
<p>Synthon-based ligand discovery in virtual libraries of over 11 billion compounds. A A Sadybekov, A V Sadybekov, Y Liu, C Iliopoulos-Tsoutsouvas, X.-P Huang, J Pickett, B Houser, N Patel, N K Tran, F Tong, Nature. 60178932022</p>
<p>Small molecules, big targets: drug discovery faces the protein-protein interaction challenge. D E Scott, A R Bayly, C Abell, J Skidmore, Nat. Rev. Drug Discov. 1582016</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. M H Segler, T Kogej, C Tyrchan, M P Waller, ACS Cent. Sci. 412018</p>
<p>R Sennrich, B Haddow, A Birch, Neural machine translation of rare words with subword units, ACL. 2016</p>
<p>FS-mol: A few-shot learning dataset of molecules. M Stanley, J F Bronskill, K Maziarz, H Misztela, J Lanini, M Segler, N Schneider, M Brockschmidt, 2021NeurIPS</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Integrating qsar modelling and deep learning in drug discovery: the emergence of deep qsar. A Tropsha, O Isayev, A Varnek, G Schneider, A Cherkasov, Nature Reviews Drug Discovery. 2322024</p>
<p>The evolution of commercial drug delivery technologies. A Vargason, A Anselmo, S Mitragotri, Nat Biomed Eng. 52021</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Adv. Condens. Matter Phys. 302017</p>
<p>Multitask joint strategies of self-supervised representation learning on biomedical networks for drug discovery. X Wang, Y Cheng, Y Yang, Nat Mach Intell. 52023</p>
<p>Pro_ligand: an approach to de novo molecular design. 2. design of novel molecules from molecular field analysis (mfa) models and pharmacophores. B Waszkowycz, D E Clark, D Frenkel, J Li, C W Murray, B Robson, D R Westhead, J. Med. Chem. 37231994</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chem. Sci. 922018</p>
<p>A Zeng, X Liu, Z Du, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint</p>
<p>Human-level few-shot concept induction through minimax entropy learning. C Zhang, B Jia, Y Zhu, S.-C Zhu, Sci. Adv. 101624882024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8327 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8327</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8327</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278739822</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12717v1.pdf" target="_blank">ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8327.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8327.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToTQwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToTQwen3-8B (Qwen3-8B fine-tuned with ToTRL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter Qwen3 model specialized via the paper's ToTRL (Tree-of-Thoughts Reinforcement Learning) pipeline to internalize tree-of-thoughts reasoning; trained on puzzle games (Sudoku, Alphametic) using REINFORCE with a rule-based reward and multi-stage ToT guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ToTQwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen3-8B foundation model fine-tuned with the authors' ToTRL framework (two-stage training: non-thinking then thinking modes), using REINFORCE on puzzle-playrolls and a rule-based reward (r_s=1, r_p=-1) plus ToT guidance prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku (in-distribution), 9×9 Sudoku (OOD), 5×5 Crossword (OOD), Alphametic puzzles (in-distribution), Poker 24 / Make 24 (OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle (Sudoku, Crossword) and constraint-satisfaction puzzles (Alphametic, Make 24) that require multi-cell/position reasoning and exploration of interdependent choices (forward simulation, backtracking).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Logic reasoning tasks use a Tree-of-Thoughts (ToT) prompt; math problems use Chain-of-Thought (CoT) prompts. Models evaluated in 'thinking' mode with adjustable thinking budgets (e.g., 8K–32K tokens). Outputs scored by a strict rule-based reward that first checks format then exact solution set equality.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Internalized Tree-of-Thoughts reasoning via ToTRL: on-policy REINFORCE training, rule-based reward, multi-stage (no-thinking then thinking) ToT guidance prompts to generate parallel candidate branches, implicit pruning through value/selection in prompt, and conventional CoT where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>6×6 Sudoku (in-distribution) accuracy = 0.800; Alphametic puzzles per-solution accuracies: 1-solution 0.960, 2-solution 1.000, 3-solution 0.973, 4-solution 0.960; Alphametic average = 0.973 (Table 1). OOD: 5×5 Crossword = 0.508, 9×9 Sudoku = 0.260, K&K Puzzle = 0.986 (Table 2). Poker 24 Game overall avg = 0.603; Make 24 overall avg = 0.661 (Table 3). On math: AIME/AAMC averaged 0.750 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors argue Sudoku and Crossword require forward-looking, positional reasoning and backtracking; ToTQwen3-8B shows substantially higher Sudoku (6×6) and better generalization to 9×9 Sudoku than baselines, improvements under ToT guidance and test-time scaling efficiency (higher scores at smaller thinking budgets), and ablation studies where ToT guidance and Stage 1 training materially improved performance—together presented as evidence of more global/spatially-aware search behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to Qwen3-8B and other SOTA baselines (DeepSeek-R1-Distill-Qwen-7B, Llama-3.1-Nemotron-Nano-8B, GLM-4-Z1-9B-0414, Phi-4 Reasoning). ToTQwen3-8B outperformed all listed baselines on in-distribution 6×6 Sudoku (0.800 vs Qwen3-8B 0.660) and across many OOD logic tasks (e.g., 5×5 Crossword 0.508 vs Qwen3-8B 0.378; 9×9 Sudoku 0.260 vs Qwen3-8B 0.180). Ablation: ToT guidance and Stage 1 training are shown to be important for gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note possible residual verbosity and partially redundant outputs due to the CoT foundation; ToT guidance applied to an unadapted Qwen3-8B produced mixed/degraded results on some tasks (i.e., inability to exploit ToT without ToTRL training); performance on harder OOD Sudoku is still modest (9×9 = 0.260), indicating limitations in scaling to larger spatial puzzles; implicit ToT induction may differ from explicit tree-search algorithms and may not prune as strictly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8327.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8327.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen3-8B (base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter reasoning-capable LLM used as the foundation model; served as the primary baseline to quantify gains from ToTRL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The Qwen3-8B foundation model (referenced in paper as [14]) evaluated with 'thinking' mode prompts; used as the seed model for ToTQwen3-8B and as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku (in-distribution), 9×9 Sudoku (OOD), Alphametic puzzles (in-distribution), 5×5 Crossword (OOD), Poker 24 / Make 24 (OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles and constraint-satisfaction puzzles requiring positional/interdependent reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated in thinking mode using ToT prompts for logic tasks and CoT for math; same evaluation datasets and thinking budgets as ToTQwen3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard long Chain-of-Thought (CoT) style reasoning enabled via test-time scaling and thinking mode; no ToTRL internal ToT training applied in baseline setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>6×6 Sudoku accuracy = 0.660; Alphametic average = 0.930 (Table 1). OOD: 5×5 Crossword = 0.378; 9×9 Sudoku = 0.180; K&K Puzzle = 0.950 (Table 2). Poker 24 avg = 0.468; Make 24 avg = 0.614 (Table 3). Math avg = 0.689 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved scores with increased thinking budget indicate capacity for longer position-aware CoT traces, but ablation shows it cannot reliably exploit ToT guidance without ToTRL training (ToT guidance on base Qwen3-8B gave mixed results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed by ToTQwen3-8B on most in-distribution and many OOD logic tasks; compared to other baselines, Qwen3-8B had competitive performance on some multi-solution tasks (e.g., Poker/Make 24) but lower on 6×6 Sudoku than ToTQwen3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Habituation to sequential CoT leads to verbosity and local search; applying ToT guidance without prior ToTRL adaptation degraded performance on some tasks (per ablation). OOD generalization to larger Sudoku remains limited (9×9 accuracy 0.180).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8327.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8327.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter reasoning LLM (RL-elicited long CoT style) included as a baseline; previously shown to elicit long chain-of-thought via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A distilled 7B model trained with RL techniques (DeepSeek-R1 family) to encourage longer CoT; used here as an empirical baseline on the puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku (in-distribution), Alphametic puzzles (in-distribution), 5×5 Crossword (OOD), 9×9 Sudoku (OOD), Poker 24 / Make 24 (OOD), K&K Puzzle (OOD)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles and constraint-satisfaction puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated under the same ToT/CoT prompts and thinking budgets as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>RL-elicited long CoT behaviors (rule-based RL in prior work); no ToT internalization reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>6×6 Sudoku = 0.000; Alphametic average = 0.028 (Table 1). OOD: 5×5 Crossword = 0.000; 9×9 Sudoku = 0.000; K&K Puzzle = 0.007 (Table 2). Poker 24 avg = 0.021; Make 24 avg = 0.019 (Table 3). Math avg = 0.608 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Very low scores on Sudoku/Crossword indicate limited effective spatial/grid reasoning on these puzzle evaluations under the paper's setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performed substantially worse than ToTQwen3-8B and Qwen3-8B on Sudoku and other grid puzzles; shows that prior RL-induced long CoT alone did not suffice for strong ToT-style puzzle solving in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Near-zero performance on many grid logic tasks (6×6 and 9×9 Sudoku, 5×5 Crossword) in the reported evaluations, suggesting failure to exploit interdependent positional constraints in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8327.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8327.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-Nemotron-Nano-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-Nemotron-Nano-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter Llama-3.1 variant listed as a baseline for reasoning tasks; evaluated on the same puzzle benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-Nemotron-Nano-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8B Llama-3.1 based reasoning model (Nemotron-Nano variant) used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku, Alphametic puzzles, 5×5 Crossword, 9×9 Sudoku, Poker 24 / Make 24</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic and constraint-satisfaction puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated with ToT prompts for logic reasoning; same datasets and budgets as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Primarily CoT/long reasoning as provided by the base model; no ToTRL fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>6×6 Sudoku = 0.000; Alphametic avg = 0.122 (Table 1). OOD: 5×5 Crossword = 0.002; 9×9 Sudoku = 0.000; K&K Puzzle = 0.043 (Table 2). Poker 24 avg = 0.067; Make 24 avg = 0.057 (Table 3). Math avg = 0.623 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Low performance on Sudoku/Crossword suggests limited positional reasoning under the evaluated conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperformed relative to ToTQwen3-8B and Qwen3-8B on grid puzzles; comparable or lower on multi-solution puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Near-zero performance on 6×6 Sudoku and 9×9 Sudoku indicates failure modes on grid spatial puzzles in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8327.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8327.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM-4-Z1-9B-0414</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-4-Z1-9B-0414</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 9B-parameter GLM-4 variant included as a strong baseline on puzzle and math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-Z1-9B-0414</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 9B-parameter GLM-4 variant used as a baseline; achieves competitive performance especially on some multi-solution tasks and math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku, Alphametic puzzles, 5×5 Crossword, 9×9 Sudoku, Poker 24 / Make 24</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic and constraint-satisfaction puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated with the same ToT/CoT prompting and budgets used across models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Conventional CoT and model's native reasoning; no ToTRL training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>6×6 Sudoku = 0.200; Alphametic avg = 0.253 (Table 1). OOD: 5×5 Crossword = 0.062; 9×9 Sudoku = 0.000; K&K Puzzle = 0.893 (Table 2). Poker 24 avg = 0.410; Make 24 avg = 0.578 (Table 3). Math avg = 0.739 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mixed: very strong on K&K puzzles but zero on 9×9 Sudoku, indicating uneven spatial/grid reasoning capabilities depending on task structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed by ToTQwen3-8B on many logic tasks (e.g., 5×5 Crossword, Poker/Make 24 averages), but strong on some OOD tasks (K&K).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero on 9×9 Sudoku suggests failure to generalize to larger grid Sudoku in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8327.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8327.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-4 Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-4 Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-focused LLM included as a baseline which attains competitive results on several logic and math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4 Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A reasoning-oriented LLM (Phi-4 variant) listed as a baseline that uses its native reasoning abilities (CoT) under the paper's prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku, Alphametic puzzles, 5×5 Crossword, 9×9 Sudoku, Poker 24 / Make 24</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic and constraint satisfaction puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated under ToT prompts for logic tasks and CoT for math, with thinking budgets as specified.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Long CoT reasoning (no ToTRL fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>6×6 Sudoku = 0.120; Alphametic avg = 0.596 (Table 1). OOD: 5×5 Crossword = 0.000; 9×9 Sudoku = 0.080; K&K Puzzle = 0.957 (Table 2). Poker 24 avg = 0.104; Make 24 avg = 0.353 (Table 3). Math avg = 0.686 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High K&K scores but near-zero on some grid tasks suggest inconsistent exploitation of spatial/positional structure across different puzzle types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Generally outperformed by ToTQwen3-8B on many logic tasks; strong on K&K benchmark relative to other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero on 5×5 Crossword and only 0.080 on 9×9 Sudoku point to notable failures on certain grid-based spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Alphazero-like tree-search can guide large language model decoding and training <em>(Rating: 1)</em></li>
                <li>Qwen3 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8327",
    "paper_id": "paper-278739822",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "ToTQwen3-8B",
            "name_full": "ToTQwen3-8B (Qwen3-8B fine-tuned with ToTRL)",
            "brief_description": "An 8B-parameter Qwen3 model specialized via the paper's ToTRL (Tree-of-Thoughts Reinforcement Learning) pipeline to internalize tree-of-thoughts reasoning; trained on puzzle games (Sudoku, Alphametic) using REINFORCE with a rule-based reward and multi-stage ToT guidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ToTQwen3-8B",
            "model_description": "Qwen3-8B foundation model fine-tuned with the authors' ToTRL framework (two-stage training: non-thinking then thinking modes), using REINFORCE on puzzle-playrolls and a rule-based reward (r_s=1, r_p=-1) plus ToT guidance prompts.",
            "model_size": "8B",
            "puzzle_name": "6×6 Sudoku (in-distribution), 9×9 Sudoku (OOD), 5×5 Crossword (OOD), Alphametic puzzles (in-distribution), Poker 24 / Make 24 (OOD)",
            "puzzle_type": "Grid-based logic puzzle (Sudoku, Crossword) and constraint-satisfaction puzzles (Alphametic, Make 24) that require multi-cell/position reasoning and exploration of interdependent choices (forward simulation, backtracking).",
            "task_setup": "Logic reasoning tasks use a Tree-of-Thoughts (ToT) prompt; math problems use Chain-of-Thought (CoT) prompts. Models evaluated in 'thinking' mode with adjustable thinking budgets (e.g., 8K–32K tokens). Outputs scored by a strict rule-based reward that first checks format then exact solution set equality.",
            "mechanisms_or_strategies": "Internalized Tree-of-Thoughts reasoning via ToTRL: on-policy REINFORCE training, rule-based reward, multi-stage (no-thinking then thinking) ToT guidance prompts to generate parallel candidate branches, implicit pruning through value/selection in prompt, and conventional CoT where applicable.",
            "performance_metrics": "6×6 Sudoku (in-distribution) accuracy = 0.800; Alphametic puzzles per-solution accuracies: 1-solution 0.960, 2-solution 1.000, 3-solution 0.973, 4-solution 0.960; Alphametic average = 0.973 (Table 1). OOD: 5×5 Crossword = 0.508, 9×9 Sudoku = 0.260, K&K Puzzle = 0.986 (Table 2). Poker 24 Game overall avg = 0.603; Make 24 overall avg = 0.661 (Table 3). On math: AIME/AAMC averaged 0.750 (Table 4).",
            "evidence_of_spatial_reasoning": "Authors argue Sudoku and Crossword require forward-looking, positional reasoning and backtracking; ToTQwen3-8B shows substantially higher Sudoku (6×6) and better generalization to 9×9 Sudoku than baselines, improvements under ToT guidance and test-time scaling efficiency (higher scores at smaller thinking budgets), and ablation studies where ToT guidance and Stage 1 training materially improved performance—together presented as evidence of more global/spatially-aware search behavior.",
            "comparisons": "Directly compared to Qwen3-8B and other SOTA baselines (DeepSeek-R1-Distill-Qwen-7B, Llama-3.1-Nemotron-Nano-8B, GLM-4-Z1-9B-0414, Phi-4 Reasoning). ToTQwen3-8B outperformed all listed baselines on in-distribution 6×6 Sudoku (0.800 vs Qwen3-8B 0.660) and across many OOD logic tasks (e.g., 5×5 Crossword 0.508 vs Qwen3-8B 0.378; 9×9 Sudoku 0.260 vs Qwen3-8B 0.180). Ablation: ToT guidance and Stage 1 training are shown to be important for gains.",
            "limitations_or_failure_cases": "Authors note possible residual verbosity and partially redundant outputs due to the CoT foundation; ToT guidance applied to an unadapted Qwen3-8B produced mixed/degraded results on some tasks (i.e., inability to exploit ToT without ToTRL training); performance on harder OOD Sudoku is still modest (9×9 = 0.260), indicating limitations in scaling to larger spatial puzzles; implicit ToT induction may differ from explicit tree-search algorithms and may not prune as strictly.",
            "uuid": "e8327.0",
            "source_info": {
                "paper_title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen3-8B",
            "name_full": "Qwen3-8B (base model)",
            "brief_description": "An 8B-parameter reasoning-capable LLM used as the foundation model; served as the primary baseline to quantify gains from ToTRL fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen3-8B",
            "model_description": "The Qwen3-8B foundation model (referenced in paper as [14]) evaluated with 'thinking' mode prompts; used as the seed model for ToTQwen3-8B and as a baseline in experiments.",
            "model_size": "8B",
            "puzzle_name": "6×6 Sudoku (in-distribution), 9×9 Sudoku (OOD), Alphametic puzzles (in-distribution), 5×5 Crossword (OOD), Poker 24 / Make 24 (OOD)",
            "puzzle_type": "Grid-based logic puzzles and constraint-satisfaction puzzles requiring positional/interdependent reasoning.",
            "task_setup": "Evaluated in thinking mode using ToT prompts for logic tasks and CoT for math; same evaluation datasets and thinking budgets as ToTQwen3-8B.",
            "mechanisms_or_strategies": "Standard long Chain-of-Thought (CoT) style reasoning enabled via test-time scaling and thinking mode; no ToTRL internal ToT training applied in baseline setting.",
            "performance_metrics": "6×6 Sudoku accuracy = 0.660; Alphametic average = 0.930 (Table 1). OOD: 5×5 Crossword = 0.378; 9×9 Sudoku = 0.180; K&K Puzzle = 0.950 (Table 2). Poker 24 avg = 0.468; Make 24 avg = 0.614 (Table 3). Math avg = 0.689 (Table 4).",
            "evidence_of_spatial_reasoning": "Improved scores with increased thinking budget indicate capacity for longer position-aware CoT traces, but ablation shows it cannot reliably exploit ToT guidance without ToTRL training (ToT guidance on base Qwen3-8B gave mixed results).",
            "comparisons": "Outperformed by ToTQwen3-8B on most in-distribution and many OOD logic tasks; compared to other baselines, Qwen3-8B had competitive performance on some multi-solution tasks (e.g., Poker/Make 24) but lower on 6×6 Sudoku than ToTQwen3-8B.",
            "limitations_or_failure_cases": "Habituation to sequential CoT leads to verbosity and local search; applying ToT guidance without prior ToTRL adaptation degraded performance on some tasks (per ablation). OOD generalization to larger Sudoku remains limited (9×9 accuracy 0.180).",
            "uuid": "e8327.1",
            "source_info": {
                "paper_title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-R1-Distill-Qwen-7B",
            "name_full": "DeepSeek-R1-Distill-Qwen-7B",
            "brief_description": "A 7B-parameter reasoning LLM (RL-elicited long CoT style) included as a baseline; previously shown to elicit long chain-of-thought via RL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Qwen-7B",
            "model_description": "A distilled 7B model trained with RL techniques (DeepSeek-R1 family) to encourage longer CoT; used here as an empirical baseline on the puzzles.",
            "model_size": "7B",
            "puzzle_name": "6×6 Sudoku (in-distribution), Alphametic puzzles (in-distribution), 5×5 Crossword (OOD), 9×9 Sudoku (OOD), Poker 24 / Make 24 (OOD), K&K Puzzle (OOD)",
            "puzzle_type": "Grid-based logic puzzles and constraint-satisfaction puzzles.",
            "task_setup": "Evaluated under the same ToT/CoT prompts and thinking budgets as other baselines.",
            "mechanisms_or_strategies": "RL-elicited long CoT behaviors (rule-based RL in prior work); no ToT internalization reported here.",
            "performance_metrics": "6×6 Sudoku = 0.000; Alphametic average = 0.028 (Table 1). OOD: 5×5 Crossword = 0.000; 9×9 Sudoku = 0.000; K&K Puzzle = 0.007 (Table 2). Poker 24 avg = 0.021; Make 24 avg = 0.019 (Table 3). Math avg = 0.608 (Table 4).",
            "evidence_of_spatial_reasoning": "Very low scores on Sudoku/Crossword indicate limited effective spatial/grid reasoning on these puzzle evaluations under the paper's setup.",
            "comparisons": "Performed substantially worse than ToTQwen3-8B and Qwen3-8B on Sudoku and other grid puzzles; shows that prior RL-induced long CoT alone did not suffice for strong ToT-style puzzle solving in these experiments.",
            "limitations_or_failure_cases": "Near-zero performance on many grid logic tasks (6×6 and 9×9 Sudoku, 5×5 Crossword) in the reported evaluations, suggesting failure to exploit interdependent positional constraints in this setup.",
            "uuid": "e8327.2",
            "source_info": {
                "paper_title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1-Nemotron-Nano-8B",
            "name_full": "Llama-3.1-Nemotron-Nano-8B",
            "brief_description": "An 8B-parameter Llama-3.1 variant listed as a baseline for reasoning tasks; evaluated on the same puzzle benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-Nemotron-Nano-8B",
            "model_description": "An 8B Llama-3.1 based reasoning model (Nemotron-Nano variant) used as a baseline in experiments.",
            "model_size": "8B",
            "puzzle_name": "6×6 Sudoku, Alphametic puzzles, 5×5 Crossword, 9×9 Sudoku, Poker 24 / Make 24",
            "puzzle_type": "Grid-based logic and constraint-satisfaction puzzles.",
            "task_setup": "Evaluated with ToT prompts for logic reasoning; same datasets and budgets as other baselines.",
            "mechanisms_or_strategies": "Primarily CoT/long reasoning as provided by the base model; no ToTRL fine-tuning reported.",
            "performance_metrics": "6×6 Sudoku = 0.000; Alphametic avg = 0.122 (Table 1). OOD: 5×5 Crossword = 0.002; 9×9 Sudoku = 0.000; K&K Puzzle = 0.043 (Table 2). Poker 24 avg = 0.067; Make 24 avg = 0.057 (Table 3). Math avg = 0.623 (Table 4).",
            "evidence_of_spatial_reasoning": "Low performance on Sudoku/Crossword suggests limited positional reasoning under the evaluated conditions.",
            "comparisons": "Underperformed relative to ToTQwen3-8B and Qwen3-8B on grid puzzles; comparable or lower on multi-solution puzzles.",
            "limitations_or_failure_cases": "Near-zero performance on 6×6 Sudoku and 9×9 Sudoku indicates failure modes on grid spatial puzzles in this evaluation.",
            "uuid": "e8327.3",
            "source_info": {
                "paper_title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GLM-4-Z1-9B-0414",
            "name_full": "GLM-4-Z1-9B-0414",
            "brief_description": "A 9B-parameter GLM-4 variant included as a strong baseline on puzzle and math benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GLM-4-Z1-9B-0414",
            "model_description": "A 9B-parameter GLM-4 variant used as a baseline; achieves competitive performance especially on some multi-solution tasks and math benchmarks.",
            "model_size": "9B",
            "puzzle_name": "6×6 Sudoku, Alphametic puzzles, 5×5 Crossword, 9×9 Sudoku, Poker 24 / Make 24",
            "puzzle_type": "Grid-based logic and constraint-satisfaction puzzles.",
            "task_setup": "Evaluated with the same ToT/CoT prompting and budgets used across models.",
            "mechanisms_or_strategies": "Conventional CoT and model's native reasoning; no ToTRL training.",
            "performance_metrics": "6×6 Sudoku = 0.200; Alphametic avg = 0.253 (Table 1). OOD: 5×5 Crossword = 0.062; 9×9 Sudoku = 0.000; K&K Puzzle = 0.893 (Table 2). Poker 24 avg = 0.410; Make 24 avg = 0.578 (Table 3). Math avg = 0.739 (Table 4).",
            "evidence_of_spatial_reasoning": "Mixed: very strong on K&K puzzles but zero on 9×9 Sudoku, indicating uneven spatial/grid reasoning capabilities depending on task structure.",
            "comparisons": "Outperformed by ToTQwen3-8B on many logic tasks (e.g., 5×5 Crossword, Poker/Make 24 averages), but strong on some OOD tasks (K&K).",
            "limitations_or_failure_cases": "Zero on 9×9 Sudoku suggests failure to generalize to larger grid Sudoku in this evaluation.",
            "uuid": "e8327.4",
            "source_info": {
                "paper_title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Phi-4 Reasoning",
            "name_full": "Phi-4 Reasoning",
            "brief_description": "A reasoning-focused LLM included as a baseline which attains competitive results on several logic and math benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4 Reasoning",
            "model_description": "A reasoning-oriented LLM (Phi-4 variant) listed as a baseline that uses its native reasoning abilities (CoT) under the paper's prompts.",
            "model_size": null,
            "puzzle_name": "6×6 Sudoku, Alphametic puzzles, 5×5 Crossword, 9×9 Sudoku, Poker 24 / Make 24",
            "puzzle_type": "Grid-based logic and constraint satisfaction puzzles.",
            "task_setup": "Evaluated under ToT prompts for logic tasks and CoT for math, with thinking budgets as specified.",
            "mechanisms_or_strategies": "Long CoT reasoning (no ToTRL fine-tuning reported).",
            "performance_metrics": "6×6 Sudoku = 0.120; Alphametic avg = 0.596 (Table 1). OOD: 5×5 Crossword = 0.000; 9×9 Sudoku = 0.080; K&K Puzzle = 0.957 (Table 2). Poker 24 avg = 0.104; Make 24 avg = 0.353 (Table 3). Math avg = 0.686 (Table 4).",
            "evidence_of_spatial_reasoning": "High K&K scores but near-zero on some grid tasks suggest inconsistent exploitation of spatial/positional structure across different puzzle types.",
            "comparisons": "Generally outperformed by ToTQwen3-8B on many logic tasks; strong on K&K benchmark relative to other baselines.",
            "limitations_or_failure_cases": "Zero on 5×5 Crossword and only 0.080 on 9×9 Sudoku point to notable failures on certain grid-based spatial tasks.",
            "uuid": "e8327.5",
            "source_info": {
                "paper_title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning",
            "rating": 2,
            "sanitized_title": "logicrl_unleashing_llm_reasoning_with_rulebased_reinforcement_learning"
        },
        {
            "paper_title": "Alphazero-like tree-search can guide large language model decoding and training",
            "rating": 1,
            "sanitized_title": "alphazerolike_treesearch_can_guide_large_language_model_decoding_and_training"
        },
        {
            "paper_title": "Qwen3",
            "rating": 2
        }
    ],
    "cost": 0.0146245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving
19 May 2025</p>
<p>Haoyuan Wu 
Xueyi Chen 
The Chinese University of Hong Kong</p>
<p>Rui Ming 
The Chinese University of Hong Kong</p>
<p>Jilong Gao 
The Chinese University of Hong Kong</p>
<p>Shoubo Hu 
Noah's Ark Lab
Huawei</p>
<p>Zhuolun He 
The Chinese University of Hong Kong</p>
<p>ChatEDA Tech</p>
<p>Bei Yu 
The Chinese University of Hong Kong</p>
<p>ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving
19 May 202560028A24AB071AA6713707ED900DA8A2arXiv:2505.12717v1[cs.CL]
Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL).However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection.The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction.In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure.This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths.This process can potentially lead to improved performance and reduced token costs.Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward.ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy.Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process.Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability.Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.* Equal Contribution Preprint.Under review.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have recently demonstrated remarkable capabilities in tackling complex reasoning tasks, with many advanced LLMs [3,9,6,8] commonly employing the chainof-thought (CoT) [18] technique to generate explicit, step-by-step intermediate reasoning.This sequential reasoning process has enabled powerful inference in structured domains like mathematics and programming.For example, the GPT-o1 series [13] achieves improved inference performance with longer reasoning chains.Similarly, DeepSeek-R1 [9] attains notable results in complex reasoning via the emergent long CoT reasoning process, which is activated using reinforcement learning (RL) with a rule-based reward signal.</p>
<p>However, the fundamental nature of CoT is a linear and single-path reasoning process.Although effective for certain problems, this sequential structure inherently limits its efficiency when addressing tasks that necessitate exploring and evaluating multiple potential solution trajectories.A critical issue arises when using RL with rule-based rewards to induce longer reasoning, the resulting outputs can become verbose and redundant [13,9,6].This is because the underlying reasoning remains local, moving from one step to the next without a global perspective or an effective mechanism to evaluate the overall promise of a path or prune unpromising lines of thought.This contrasts with the efficient human cognitive strategy, which involves considering alternatives and focusing resources globally.</p>
<p>In contrast, the tree-of-thoughts (ToT) method [22] offers a conceptually superior approach by explicitly modeling the reasoning process as an exploration across a tree structure of potential thoughts or states.The tree-based reasoning structure facilitates the parallel generation and evaluation of diverse reasoning branches, enabling the LLM to actively identify, evaluate, and prune unproductive thought paths.By maintaining a global view of the search space, ToT holds the potential to achieve higher performance and significantly reduce redundant exploration and associated token costs compared to the linear CoT reasoning process.</p>
<p>Building upon the capabilities of long CoT [14], we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework using the rule-based reward.ToTRL is designed to guide LLMs in developing the parallel ToT strategy [22] based on the sequential CoT strategy [23].Directly building ToT reasoning based on CoT within the reasoning mode is challenging due to the established habituation to the sequential CoT style.To address this, ToTRL employs a two-stage training strategy.Initially, the LLM is trained to perform ToT reasoning in a non-thinking mode, leveraging more moldable thinking patterns to activate ToT reasoning.Once the LLM has developed a degree of ToT reasoning ability in the non-reasoning mode, it undergoes further training in the reasoning mode.This second stage aims to enable the LLM to effectively utilize its newly acquired ToT capabilities during inference on complex tasks, building upon its existing CoT reasoning strengths.</p>
<p>Moreover, we employ LLMs as players in puzzle games that require tree-based reasoning, providing challenging tasks for cultivating the ToT reasoning capability.These puzzle games are deliberately chosen for their intrinsic requirement of exploring interdependent choices and managing multiple constraints simultaneously, requiring a thought tree construction and exploration involving multiple concurrent hypotheses and future states.Our contributions are summarized as follows:</p>
<p>• Introduce ToTRL, a novel on-policy RL framework using the rule-based reward for developing the LLMs' ToT reasoning strategy based on the long CoT capability.• Employ LLMs as players in puzzle games that require tree-based reasoning, providing challenging tasks for cultivating the ToT reasoning capability.• Provide empirical evaluations showing that our ToTQwen3-8B, trained with ToTRL, achieves significant performance and reasoning efficiency on complex tasks.</p>
<p>Tree-of-Thoughts RL</p>
<p>Building upon the capabilities of long CoT [14], we introduce tree-of-thoughts RL using the on-policy RL strategy (Section 2.1) with the rule-based reward (Section 2.2).Specifically, ToTRL employs a two-stage training strategy (Section 2.3) and serves LLMs as the puzzle game player for training (Section 2.4).</p>
<p>On-policy RL Algorithm</p>
<p>Test-time scaling introduces a significant paradigm shift for LLMs [13,9].This approach enables long CoT reasoning and fosters sophisticated reasoning behaviors, leading to superior performance on complex reasoning tasks.A key technique facilitating these advancements is rule-based RL, which elicits behaviors such as self-verification and iterative refinement.</p>
<p>In this paper, we employ REINFORCE [27,10] as the on-policy RL algorithm for our proposed ToTRL method.Specifically, for each prompt q, we sample n responses {o 1 , o 2 , . . ., o n } from the old policy π θold , where n is the number of sampled trajectories (i.e., the rollout size per prompt).The policy model π θ is then optimized by maximizing the following surrogate objective:
J(θ) = E q∼D,{oi} n i=1 ∼π θ old (o|q) 1 n n i=1 min π θ (o i |q) π θold (o i |q) A i , clip π θ (o i |q) π θold (o i |q) , 1 − ϵ, 1 + ϵ A i − β • D KL (π θ ||π ref ) ,(1)
System: You are a helpful assistant.The user asks questions and the assistant answers them.The assistant first thinks with tree-of-thoughts reasoning and then provides the answer to the user.The tree-of-thoughts reasoning and answer are contained in the <tot> </tot> and <answer> </answer> tags, respectively, that is, <tot> The tree-ofthoughts reasoning goes here </tot> <answer> The answer goes here </answer>.</p>
<p>User: Solve the following Alphametic puzzle: EEM+EGYM=EYME.In this puzzle, each letter represents a unique digit (0-9).The same letter always represents the same digit, and different letters represent different digits.Leading digits cannot be zero.Find ALL solutions that make the equation true.You are required to solve the problem using the tree-of-thoughts reasoning.Please perform tree-of-thoughts reasoning step by step and don't skip any intermediate steps.Show your tree-of-thoughts process within <tot> </tot> tags.List ALL solutions using json format within <answer> </answer> tags with no duplicates.</p>
<p>Assistant: <think> </think> <tot> ## ToT Reasoning Process Here </tot> <Answer> {"E": 6, "M": 3, "G": 0, "Y": 7}, {"E": 4, "M": 2, "G": 3, "Y": 8}, {"E": 2, "M": 1, "G": 6, "Y": 9}, {"E": 2, "M": 6, "G":  where ϵ and β are hyperparameters.The advantage estimate A i is computed based on the rule-based reward function, using the sampled rewards {r 1 , r 2 , . . ., r n }, and is calculated as:
A i = r i − 1 n − 1 j̸ =i r j .(2)
Traditionally, RL algorithms incorporate a KL divergence penalty to regulate the divergence between the online policy model and the frozen reference model [24].However, during training with ToTRL, the model distribution can diverge significantly from the initial model.The KL penalty term will restrict the exploration of model outputs.Consequently, we exclude the KL term by setting β = 0.</p>
<p>Reward Modeling</p>
<p>To effectively shape the LLM's learning trajectory via reinforcement learning, we designed a rulebased reward function R(o|q; r s , r p ).This function employs a strict hierarchical evaluation protocol, prioritizing format validity before evaluating correctness.The reward mechanism is parameterized by two key constants, including the reward magnitude r s for a correct output and the penalty magnitude r p assigned for any detected errors.</p>
<p>Format Validity.The initial validation phase rigorously examines whether the generated output o conforms to all K predefined structural and syntactic constraints C = {c 1 , c 2 , . . ., c K }.A binary indicator function v k (o) for the outcome of each individual check c k can be defined as:
v k (o) = 1, if check c k passes for output o, 0, otherwise.(3)
An output o is deemed format-valid if and only if it satisfies all K checks.Consequently, the format validity indicator, F(o) can be formally defined as:
F(o) = I( K k=1 v k (o) = K).(4)
If F(o) = 0, the evaluation terminates immediately, assigning the penalty reward r p .</p>
<p>Accuracy Evaluation.Subsequent evaluation of correctness occurs strictly conditional upon successful format validation.This stage evaluates whether the structurally valid output o, which is expected to contain a sequence representing potentially multiple solutions, accurately represents the complete set of ground truth solutions Y (x).Let S(o) denote the set of solutions extracted from the model's output sequence o The accuracy indicator A(o|q), based on the equality between the extracted set of solutions and the ground truth set, can be formulated as:
A(o|x) = I(S(o) = Y (x)),(5)
where S(o) ≡ Y (x) holds if and only if S(o) contains all solutions in Y (x) and no others.</p>
<p>Rule-based Reward.The reward function R(o|q; r s , r p ) integrates the outcomes of the format validity and accuracy evaluation, parameterized by r s and r p .The reward is calculated based on the format validity status F(o) and the accuracy status A(o|x), which can be calculated:
R(o|q; r s , r p ) = r p , if F(o) = 0, A(o|x) • r s + (1 − A(o|x)) • r p , if F(o) = 1.(6)
Alternatively, the total reward can be expressed in a mathematically equivalent and compact form as:
R(o|x; r s , r p ) = F(o)A(o|x)(r s − r p ) + r p .(7)
This compact form highlights that the reward is fundamentally the base penalty r p , potentially incremented by the difference (r s − r p ) only when both format and accuracy indicators (F(o) and A(o|x)) are simultaneously active.In practice, we provide a clear binary success signal for ToTRL, and the reward parameters are instantiated with r s = 1 and r p = −1.</p>
<p>Multi-Stage ToTRL</p>
<p>Conventional CoT reasoning enforces a strictly linear, step-by-step reasoning process [18,23].This linearity can be restrictive when exploring multiple potential solution paths.Inspired by ToT [22], which structures problem-solving as a deliberate exploration of a thought tree, ToTRL adapts this concept to enhance LLM reasoning within an RL setting.The original ToT framework represents the reasoning process as a rooted tree, where each node corresponds to an intermediate thought s.From a state s d−1 at depth d − 1, a generator G(•) proposes potential next thoughts, forming a set of candidate states at depth d:
T d ⊇ {G(s) | s ∈ T d−1 },(8)
where T 0 = {S 0 } represents the initial problem state.Conceptually, ToT involves evaluating these thoughts using a value function V (s) and pruning less promising branches to manage the search space effectively.</p>
<p>In our ToTRL approach, instead of implementing an explicit external search algorithm, we develop a ToT guidance prompt to facilitate the parallel generation of ToT reasoning processes within each RL rollout.As depicted in Figure 1, LLMs are required to solve problems by employing ToT reasoning, ensuring each step is executed sequentially without omitting any intermediate stages.This promptdriven process encourages the LLM to explore ToT reasoning trajectories autonomously within its own generation process.The output generated after ToT reasoning is subsequently evaluated using Equation ( 6), and the entire process is optimized via Equation (1).</p>
<p>However, existing reasoning LLMs are primarily accustomed to the sequential CoT reasoning style [14].Consequently, integrating ToT reasoning directly into current CoT-based reasoning LLMs presents a significant challenge.To address this challenge, ToTRL employs a multi-stage ToT guidance strategy.Initially, as illustrated in Figure 1, the LLM undergoes training to perform ToT reasoning in a non-thinking mode.The non-reasoning mode is achieved by introducing blanks between reasoning tags (e.g., <think></think>), which compels the model to suspend its conventional reasoning processes.Once the LLM demonstrates an initial proficiency in ToT reasoning within the non-reasoning mode, it proceeds to further training in the reasoning mode.This subsequent stage aims to enable the LLM to develop its newly acquired ToT capabilities based on the established CoT reasoning strengths.</p>
<p>LLM as Puzzle Game Player</p>
<p>During the training process with our ToTRL, LLMs are employed as players in puzzle games.These games necessitate tree-based reasoning, thereby providing challenging tasks for cultivating the LLMs' ToT reasoning capabilities.They are specifically selected due to their inherent demand for exploring interdependent choices and managing multiple concurrent constraints, which inherently requires the construction and exploration of a sophisticated thought tree involving numerous concurrent hypotheses and future states.Specifically, as shown in Figure 2, we leverage Sudoku and Alphametic puzzles during the training process with ToTRL for this purpose.Sudoku Puzzle.Sudoku puzzle is an ideal game for cultivating ToT reasoning.The high interdependency of choices in Sudoku puzzles means that placing a single digit significantly impacts other cells, necessitating a forward-looking evaluation of cascading implications.Critically, the need for hypothetical reasoning and backtracking directly corresponds to the construction and exploration of a thought tree, where decision nodes represent potential branches that explore their consequences.</p>
<p>Alphametic Puzzle.Alphametic puzzles also offer an ideal environment for cultivating ToT reasoning.They exhibit strong interdependency due to the critical role of carry-overs.The primary challenge lies in simultaneously satisfying both the mathematical correctness of the equation and the combinatorial constraint of assigning a unique digit to each letter, presenting a complex, dual-constraint environment.The iterative process of hypothetical assignments, consequence propagation, and backtracking upon contradiction inherently forms a tree-based search.</p>
<p>By employing LLMs as players in these puzzle games, we provide challenging environments designed to foster their tree-based reasoning capabilities within complex constraint satisfaction problems.</p>
<p>Experiments</p>
<p>Experiment Settings</p>
<p>Training Data.The ToTQwen3-8B model is trained using 1440 puzzle games, specifically designed to cultivate its ToT reasoning capabilities.We provide more details in Appendix B.1.</p>
<p>Implementation Details.In this paper, we train the ToTQwen3-8B model using our ToTRL framework based on the Qwen3-8B model [14].The training process entailed using a constant learning rate schedule with a warm-up ratio of 0.01, and the AdamW [12] optimizer with a learning rate of 1 × 10 −6 , no weight decay, a batch size of 9, and a sequence length of 16384 tokens.The models underwent instruction tuning for one epoch using DeepSpeed-Zero stage2 with offload [15] on 4 A100 GPUs, each with 80G memory.</p>
<p>Baselines.We select the existing SOTA reasoning LLMs with similar parameters as baselines of our ToTQwen3-8B model, which include DeepSeek-R1-Distill-Qwen-7B [9], Llama-3.1-Nemotron-Nano-8B[4], GLM-4-Z1-9B-0414 [28], Phi-4 Reasoning [1], and Qwen3-8B [14].Notably, Qwen3-8B and ToTQwen3-8B use the thinking mode for evaluation.</p>
<p>Evaluation Benchmarks.To comprehensively evaluate the proficiency of our ToTQwen3-8B model, we utilize a set of logic reasoning tasks categorized as either in-distribution or out-of-distribution (OOD).In-distribution tasks, included in the training data, comprise 6×6 Sudoku and Alphametic puzzles.OOD tasks, which are not part of the training data, include 5×5 Crossword [22], 9×9 Sudoku [5], K&amp;K puzzles [21], Poker 24 Game, and Make 24 puzzles.Additionally, we also leverage the widely adopted AIME 2024-2025 (American Invitational Mathematics Examination) and AMC 2023 (American Mathematics Competitions) benchmarks, both known for their rigorous and diverse mathematical problems.Specifically, for the evaluation of our ToTQwen3-8B, we employ the ToT prompt (Figure 1) for logic reasoning tasks while leveraging the CoT prompt for mathematical problems.For all evaluation tasks, we use accuracy as the performance metric.Notably, the accuracy is calculated on the number of correct answers for puzzles with multiple solutions.</p>
<p>Thinking Budget.Thinking budget facilitates control over the thinking process through manual interruption.Specifically, when the LLM's thinking duration reaches a predefined threshold, the process is halted by inserting a stop instruction.Subsequently, the LLM generates a final response based on the partial reasoning accumulated.Notably, this budget was adjusted based on task complexity.A smaller budget of 8K is allocated for the simple K&amp;K Puzzle, while a larger budget of 32K is used for the more complex 9×9 Sudoku.</p>
<p>In-Distribution Logic Reasoning</p>
<p>As shown in Table 1, performance on these logic tasks varies across the evaluated models.Notably, the ToTQwen3-8B model demonstrates significantly superior performance in in-distribution tasks compared to the other evaluated models on both tasks.Specifically, ToTQwen3-8B achieves the highest success rate on the 6×6 Sudoku task.On the Alphametic Puzzles, it consistently performs at a high level across puzzles with varying numbers of solutions.</p>
<p>ToTQwen3-8B is built upon the Qwen3-8B model.A comparison of their performance reveals that ToTQwen3-8B shows a substantial improvement over Qwen3-8B, which scored 0.660 on Sudoku and averaged 0.930 on Alphametic puzzles.This significant performance gain is attributed to specialized training on these in-distribution tasks and the ToT reasoning strategy integration within our ToTQwen3-8B.The ToT reasoning strategy enables the LLM to engage in more global reasoning by exploring multiple paths, which is particularly effective for finding single or multiple solutions in complex constraint satisfaction problems.</p>
<p>Out-of-Distribution Logic Reasoning</p>
<p>Besides in-distribution tasks, we also evaluate our ToTQwen3-8B model on OOD logic reasoning tasks.As shown in Table 2 and Table 3, many models exhibit very low scores (close to 0), indicating a significant struggle with these logic reasoning tasks.</p>
<p>Logic Reasoning with Unique Solution.Table 2 presents performance on OOD logic reasoning tasks, all of which have a single correct solution.It is important to note that for tasks with unique solutions, some models might occasionally arrive at the correct answer through guessing rather than a complete logical derivation process.Our ToTQwen3-8B consistently achieves the highest scores across all three unique solution tasks evaluated, 0.508 on 5×5 Crossword, 0.260 on 9×9 Sudoku, and 0.986 on K&amp;K Puzzle.This consistent, high performance on diverse OOD tasks provides strong evidence that our ToTQwen3-8B model effectively leverages the underlying logical constraints and employs a well-developed derivation process, rather than relying on chance.</p>
<p>Logic Reasoning with Multiple Solutions.Table 3 presents performance on more OOD logic reasoning tasks, including Poker 24 Game and Make 24 Puzzle, which admit multiple valid solutions.The primary challenge lies not only in identifying a valid solution but also potentially in finding multiple distinct solutions or performing robustly across puzzles with varying numbers of possible solutions.This necessitates a reasoning process capable of exploring diverse successful paths rather than merely converging on a single outcome.As shown in Table 3, ToTQwen3-8B again outperforms all other models on both the Poker 24 Game and Make 24 Puzzle, demonstrating superior performance across puzzles with varying numbers of solutions.This advantage is most pronounced on puzzles with many solutions, where the capacity to explore multiple valid reasoning paths is essential.The ToT strategy inherently explores a tree of possibilities, branching at critical decision points.This systematic exploration of multiple potential continuations provides a global perspective, making it inherently well-suited for tasks admitting diverse valid solutions.Unlike long CoT reasoning strategy that commits to a single path, ToT can explore parallel branches, significantly increasing the probability of discovering multiple distinct solutions when they exist.</p>
<p>In summary, from an OOD perspective, the results presented in Table 2 and Table 3 strongly indicate that ToTQwen3-8B possesses superior generalization capabilities on these logic reasoning tasks.Its robust performance on both unique and multiple-solution puzzles suggests that the ToT reasoning structure confers a significant advantage in addressing unfamiliar logic challenges by facilitating a more comprehensive exploration of reasoning possibilities, a capability that is particularly valuable for tasks requiring the discovery of multiple valid solutions.</p>
<p>Out-of-Distribution Mathematical Tasks</p>
<p>We further investigate whether our ToTRL can enhance complex reasoning abilities can transfer to a highly challenging mathematical reasoning scenario.According to Table 4, ToTQwen3-8B demonstrates strong performance on these OOD mathematical reasoning tasks.It achieves the highest average score of 0.750 across the three benchmarks.Specifically, ToTQwen3-8B performs comparably well on AIME 2024 with a score of 0.667, matching the performance of GLM-4-Z1-9B-0414 [28] and Phi-4 Reasoning [1] with more model parameters.On AIME 2025, ToTQwen3-8B leads with a score of 0.633, outperforming all other listed models.For the AMC 2023 benchmark, ToTQwen3-8B achieves a score of 0.950, which is tied with GLM-4-Z1-9B-0414 [1] for the highest performance.</p>
<p>ToTQwen3-8B exhibits a superior ability to handle the complex and varied mathematical problems in the AIME and AMC competitions.This strong performance on OOD tasks indicates that our ToTRL framework can effectively enhance the reasoning capabilities of LLMs.Importantly, these improved abilities generalize well to novel and challenging mathematical problems beyond the training distribution.This suggests that ToTRL successfully cultivates a more robust and transferable form of reasoning.</p>
<p>Test Time Scaling</p>
<p>Test-time scaling [25] is an innovative approach in language modeling that leverages additional computational resources during the testing phase to enhance performance.This method has shown significant promise in various domains, including language modeling and code generation [13,9,6,8].We investigate the test time scaling of our proposed ToTQwen3-8B model against a baseline Qwen3-8B [14] model using different thinking budgets across six logic reasoning tasks.</p>
<p>As illustrated in Figure 3, the performance of both ToTQwen3-8B and Qwen3-8B generally improves with an increased thinking budget across all tasks.This indicates that allowing more computational resources for intermediate thinking steps leads to better reasoning outcomes.Importantly, Figure 3 demonstrates the efficiency of the ToTQwen3-8B approach, which leverages the ToT reasoning strategy.ToTQwen3-8B consistently outperforms Qwen3-8B across various thinking budgets.Notably, ToTQwen3-8B is often able to reach a higher average score with a smaller thinking budget compared to Qwen3-8B.This suggests that the structured tree-of-thoughts reasoning employed by ToTQwen3-8B allows it to explore the solution space more effectively and efficiently, requiring fewer tokens (and thus less computational cost and time) to achieve superior or comparable performance.This efficiency is a key advantage, making ToTQwen3-8B a more practical solution for logic reasoning tasks under computational constraints.</p>
<p>Ablation Studies</p>
<p>To further investigate the contribution of ToT guidance prompts and the multi-stage training process within ToTRL, we perform a series of ablation studies.</p>
<p>ToT Guidance.Table 5 presents an ablation study on the effectiveness of ToT guidance compared to CoT guidance.For the Qwen3-8B model, employing ToT guidance yields mixed results.The performance improves on some tasks but decreases significantly on others.This suggests that although ToT guidance holds potential, the Qwen3-8B model is not adept at utilizing the ToT reasoning strategy.In contrast, when applying ToT guidance to the ToTQwen3-8B model, we observe consistent and significant improvement across all reported tasks for both guidance types.This clearly demonstrates that ToT guidance is substantially more effective than CoT, particularly when paired with a model specifically trained to leverage its tree-like exploration capabilities.Notably, ToTQwen3-8B also demonstrates improved performance with CoT guidance, indicating that ToTRL can also facilitate CoT reasoning capabilities.</p>
<p>Multi-Stage ToTRL.Table 6 presents an investigation into the contribution of the multi-stage training process within ToTRL, comparing models trained with different stages.The results demonstrate that the inclusion of Stage 1 training significantly improves performance across all evaluated tasks.Specifically, Stage 1 training focuses on enabling the LLM to perform the fundamental steps of ToT reasoning using no-thinking mode, thereby facilitating the integration of this capability into models primarily accustomed to sequential CoT reasoning.This indicates that the initial exploration and tree-building capabilities acquired during Stage 1 are crucial for effective ToT guidance and the superior performance on diverse reasoning tasks.</p>
<p>Conclusion</p>
<p>In this work, we introduced ToTRL to guide LLMs from sequential CoT reasoning to a more efficient ToT reasoning strategy.By employing a two-stage training process and utilizing puzzle games that necessitate tree-based exploration, we successfully cultivate ToT capabilities in LLMs.Our ToTQwen3-8B, trained with ToTRL, demonstrates substantially superior performance on in-domain logic puzzles and generalization to OOD tasks.Furthermore, these enhanced reasoning abilities transfer effectively to challenging mathematical benchmarks.Crucially, our model also exhibits greater efficiency, achieving higher scores with smaller thinking budgets during test-time scaling compared to its CoT-based counterpart.These results collectively overcome the verbosity and local perspective limitations of long CoT, showcasing the robust potential and practical advantages of explicit tree-based reasoning for advanced AI on diverse complex tasks.For in-distribution evaluation, the evaluation dataset for 50 6×6 Sudoku puzzles and 200 Alphametic puzzles is self-generated.The Alphametic Puzzle test set maintains a distribution of 50 puzzles per solution count category, including 1 to 4 solutions, to robustly assess performance.</p>
<p>The OOD evaluation dataset is derived from established datasets and self-generated puzzles.Standardized puzzles include 50 5×5 Crossword puzzles from the ToT dataset [22] and 50 9×9 Sudoku puzzles randomly selected from a Kaggle Dataset [5] to test generalization to more complex Sudoku formats.The K&amp;K Puzzle benchmark, sourced from the original K&amp;K Puzzles dataset [20], comprises 140 test puzzles, specifically chosen to provide 20 puzzles for each of its 7 distinct solution categories.To further probe multi-solution reasoning in novel contexts, we self-generated 160 puzzles each for the Poker 24 Puzzle and Make 24 Puzzle.These datasets are carefully structured to include 40 samples for problems featuring 1, 2, 3, or 4 distinct solutions, allowing for a nuanced evaluation of the model's ability to identify multiple valid outcomes.</p>
<p>C Limitations</p>
<p>Despite the promising results of ToTQwen3-8B, this work has several limitations.The transferability of ToTQwen3-8B to different reasoning tasks requires a thorough investigation.Furthermore, our approach implicitly induces ToT reasoning through guided prompting and a two-stage reinforcement learning strategy, leveraging training data based on long CoT outputs.This method aims to encourage a more global problem perspective and potentially reduce computational overhead compared to generating exhaustive CoT traces for every thought path.However, a limitation is that the model, trained on these CoT foundations, may still produce partially redundant outputs.This redundancy can stem from the underlying long CoT structure, even with the ToT mechanism attempting to focus exploration.This implicit induction, although effective in partially mitigating CoT habituation and demonstrating the aforementioned benefits, may exhibit different exploratory dynamics and optimality compared to explicit tree search algorithms.Explicit algorithms could potentially offer stricter pruning or different trade-offs in managing redundancy.</p>
<p>Figure 1 :
1
Figure 1: Overview of multi-stage ToT guidance with solving an Alphametic puzzle as an example.</p>
<p>Figure 2 :
2
Figure 2: During the training process with ToTRL, LLMs are employed as players in puzzle games, including Sudoku and Alphametic puzzles.</p>
<p>Figure 3 :
3
Figure 3: of test time scaling on logic reasoning tasks.</p>
<p>Stage2: Thinking Mode User Input ToT Guidance
Stage1: No-thinking ModeAssistant:<think>## ToT Reasoning Process Here</think><tot>## Summarization of Reasoning Process</tot><Answer>{"E": 6, "M": 3, "G": 0, "Y": 7}, {"E": 4, "M": 2, "G":3, "Y": 8}, {"E": 2, "M": 1, "G": 6, "Y": 9}, {"E": 2,1, "Y": 3}"M": 6, "G": 1, "Y": 3}</Answer></Answer></p>
<p>Table 1 :
1
Performance of ToTQwen3-8B on in-distribution puzzle solving tasks.
6×6 Sudoku1 solAlphametic Puzzle 2 sol 3 sol 4 solAvg.DeepSeek-R1-Distill-Qwen-7B0.0000.0200.0300.0200.0400.028Llama-3.1-Nemotron-Nano-8B0.0000.1000.1000.1270.1600.122GLM-4-Z1-9B-04140.2000.0800.1400.5000.2900.253Phi-4 Reasoning0.1200.4200.6800.6400.6450.596Qwen3-8B (Enable-Thinking)0.6600.8200.9800.9600.9600.930ToTQwen3-8B (Ours)0.8000.9601.0000.9730.9600.973</p>
<p>Table 2 :
2
Performance of ToTQwen3-8B on OOD logic reasoning tasks with a unique solution.
5×5 Crossword9×9 SudokuK&amp;K PuzzleDeepSeek-R1-Distill-Qwen-7B0.0000.0000.007Llama-3.1-Nemotron-Nano-8B0.0020.0000.043GLM-4-Z1-9B-04140.0620.0000.893Phi-4 Reasoning0.0000.0800.957Qwen3-8B (Enable-Thinking)0.3780.1800.950ToTQwen3-8B (Ours)0.5080.2600.986</p>
<p>Table 3 :
3
Performance of ToTQwen3-8B on OOD logic reasoning tasks with multiple solutions.
Poker 24 GameMake 24 Puzzle1 sol 2 sol 3 sol 4 sol Avg. 1 sol 2 sol 3 sol 4 sol Avg.DeepSeek-R1-Distill-Qwen-7B 0.025 0.025 0.017 0.019 0.021 0.050 0.025 0.000 0.000 0.019Llama-3.1-Nemotron-Nano-8B 0.100 0.075 0.042 0.050 0.067 0.050 0.088 0.058 0.031 0.057GLM-4-Z1-9B-04140.625 0.500 0.208 0.306 0.410 0.650 0.625 0.575 0.460 0.578Phi-4 Reasoning0.175 0.113 0.092 0.038 0.104 0.250 0.275 0.400 0.488 0.353Qwen3-8B (Enable-Thinking) 0.700 0.463 0.342 0.369 0.468 0.750 0.600 0.625 0.481 0.614ToTQwen3-8B (Ours)0.900 0.713 0.392 0.406 0.603 0.850 0.638 0.675 0.481 0.661</p>
<p>Table 4 :
4
Performance of ToTQwen3-8B on OOD mathematical tasks.
AIME 2024AIME 2025AMC 2023Avg.DeepSeek-R1-Distill-Qwen-7B0.5330.3670.9250.608Llama-3.1-Nemotron-Nano-8B0.6000.3670.9000.623GLM-4-Z1-9B-04140.6670.6000.9500.739Phi-4 Reasoning0.6670.4670.9250.686Qwen3-8B (Enable-Thinking)0.6330.5330.9000.689ToTQwen3-8B (Ours)0.6670.6330.9500.750</p>
<p>Table 5 :
5
Ablation study on ToT guidance.
CoTToTCrossword 5×5Sudoku 9×9K&amp;K PuzzlePoker 24 GameMake 24 PuzzleQwen3-8B✔0.3780.1800.9500.4680.614(Enable-Thinking)✔0.3760.0800.7000.4850.600ToTQwen3-8B✔✔0.350 0.5080.200 0.2600.971 0.9860.519 0.6030.648 0.661</p>
<p>Table 6 :
6
Ablation study on multi-stage ToTRL.
Stage 1Stage 2Crossword 5×5Sudoku 9×9K&amp;K PuzzlePoker 24 GameMake 24 PuzzleQwen3-8B✔✔ ✔0.470 0.5080.160 0.2600.957 0.9860.485 0.6030.532 0.661</p>
<p>Table 8 :
8
Overview of Evaluation BenchmarksDataset SourceTest Data Size Samples/Sol.
In-Distribution6×6 SudokuSelf-generated Dataset50-Alphametic Puzzle Self-generated Dataset20050Out-of-Distribution5×5 CrosswordToT Dataset [22]50-9×9 SudokuKaggle Sudoku Dataset [5]50-K&amp;K PuzzleK&amp;K Puzzles Dataset [20]14020Poker 24 PuzzleSelf-generated Dataset16040Make 24 PuzzleSelf-generated Dataset16040
A Related Works A.1 Tree-of-ThoughtsEarly applications of ToT[22,11]approaches relied on external search algorithms (e.g., BFS, DFS) or auxiliary modules (e.g., prompter agents, checkers, memory modules, and ToT controllers) to manage planning, decision-making, and backtracking.Inspired by AlphaZero, TS-LLM[7]proposed learning a dedicated value function to guide the tree search and iteratively improving the LLM itself, aiming to handle deeper and more complex search trees.Collectively, these efforts demonstrate the significant potential of internalizing ToT capabilities within the LLM itself, moving towards more autonomous reasoning.However, significant challenges remain in achieving truly autonomous planning and decision-making, as existing methods frequently depend on external search control rather than fully developing the LLM's inherent capacity for tree-based reasoning.Consequently, building upon the long-CoT reasoning capability, we develop ToTRL to train LLMs to autonomously perform ToT reasoning.A.2 Long-COT Activated through RLRL[16,17,2,26]has proven an effective approach for eliciting longer CoT, thereby enhancing LLMs' reasoning capabilities.DeepSeek-R1[9]and Kimi-K1.5[6]have demonstrated that LLMs can acquire and extend their reasoning paths, including reflection and verification, using RL with simple rule-based rewards.Data-driven RL research has further broadened the application of this approach.For instance, Logic-RL[21]utilized logic puzzles for training, demonstrating generalization to mathematics, while SWE-RL[19]leveraged software evolution data to improve model performance on software engineering tasks and OOD reasoning.However, despite somewhat enhancing the depth of thought and problem-solving capabilities, current RL-elicited long CoT[18,11]reasoning paradigms are inherently linear, following a single exploration path.This sequential structure is inefficient for tackling complex problems that necessitate extensive exploration and evaluation of multiple potential solutions.Lacking a global perspective and effective mechanisms for path evaluation and pruning, this approach often generates redundant outputs and incurs unnecessary computational overhead.In this paper, we develop ToTRL to guide LLM reasoning from a global perspective, which aims to improve performance while reducing token costs.B Experiment DetailsB.2 Evaluation BenchmarksThe performance of the ToTQwen3-8B is rigorously evaluated across various tasks, encompassing both in-distribution puzzles and OOD challenges that the model has not encountered previously.A comprehensive overview of these evaluation benchmarks, including their categories, sources, test set sizes, and specific sample distributions, is presented in Table8.
. M Abdin, S Agarwal, A Awadallah, V Balachandran, H Behl, L Chen, G De Rosa, S Gunasekar, M Javaheripi, N Joshi, arXiv:2504.213182025arXiv preprint</p>
<p>Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. A Ahmadian, C Cremer, M Gallé, M Fadaee, J Kreutzer, O Pietquin, A Üstün, S Hooker, arXiv:2402.147402024arXiv preprint</p>
<p>. Claude Anthropic, Sonnet, 2025</p>
<p>A Bercovich, I Levy, I Golan, M Dabbah, R El-Yaniv, O Puny, I Galil, Z Moshe, T Ronen, N Nabwani, arXiv:2505.00949Llama-Nemotron: Efficient Reasoning Models. 2025arXiv preprint</p>
<p>1 million sudoku games. 2017Kaggle Dataset</p>
<p>A Du, B Gao, B Xing, C Jiang, C Chen, C Li, C Xiao, C Du, C Liao, arXiv:2501.12599L1. 5: Scaling reinforcement learning with LLMs. 2025arXiv preprint</p>
<p>X Feng, Z Wan, M Wen, S M Mcaleer, Y Wen, W Zhang, J Wang, arXiv:2309.17179Alphazero-like tree-search can guide large language model decoding and training. 2023arXiv preprint</p>
<p>Google Deepmind, Gemini2.5 Pro. 2025</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. 2025arXiv preprint</p>
<p>REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models. J Hu, arXiv:2501.032622025arXiv preprint</p>
<p>Large language model guided tree-of-thought. J Long, arXiv:2305.082912023arXiv preprint</p>
<p>. I Loshchilov, F Hutter, arXiv:1711.051012017Decoupled Weight Decay Regularization. arXiv preprint</p>
<p>Introducing openai o1. Openai, </p>
<p>. Qwen Team, Qwen3, </p>
<p>. Technical_Report, Pdf, 2025</p>
<p>Zero: Memory optimizations toward training trillion parameter models. S Rajbhandari, J Rasley, O Ruwase, Y He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Y Wei, O Duchenne, J Copet, Q Carbonneaux, L Zhang, D Fried, G Synnaeve, R Singh, S I Wang, arXiv:2502.18449SWE-RL: Advancing llm reasoning via reinforcement learning on open software evolution. 2025arXiv preprint</p>
<p>C Xie, Y Huang, C Zhang, D Yu, X Chen, B Y Lin, B Li, B Ghazi, R Kumar, arXiv:2410.23123On memorization of large language models in logical reasoning. 2024arXiv preprint</p>
<p>T Xie, Z Gao, Q Ren, H Luo, Y Hong, B Dai, J Zhou, K Qiu, Z Wu, C Luo, arXiv:2502.14768Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning. 2025arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in neural information processing systems. 202336</p>
<p>Demystifying Long Chain-of-Thought Reasoning in LLMs. E Yeo, Y Tong, M Niu, G Neubig, X Yue, arXiv:2502.033732025arXiv preprint</p>
<p>DAPO: An open-source LLM reinforcement learning system at scale. Q Yu, Z Zhang, R Zhu, Y Yuan, X Zuo, Y Yue, T Fan, G Liu, L Liu, X Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Z Yu, Y Wu, Y Zhao, A Cohan, X.-P Zhang, arXiv:2504.00810Z1: Efficient Test-time Scaling with Code. 2025arXiv preprint</p>
<p>W Zeng, Y Huang, Q Liu, W Liu, K He, Z Ma, J He, arXiv:2503.18892Simplerl-Zoo: Investigating and taming zero reinforcement learning for open base models in the wild. 2025arXiv preprint</p>
<p>Sample efficient reinforcement learning with REINFORCE. J Zhang, J Kim, B O'donoghue, S Boyd, AAAI Conference on Artificial Intelligence (AAAI). 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8317 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8317</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8317</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-265697251</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.03042v1.pdf" target="_blank">Inherent limitations of LLMs regarding spatial information</a></p>
                <p><strong>Paper Abstract:</strong> Despite the significant advancements in natural language processing capabilities demonstrated by large language models such as ChatGPT, their proficiency in comprehending and processing spatial information, especially within the domains of 2D and 3D route planning, remains notably underdeveloped. This paper investigates the inherent limitations of ChatGPT and similar models in spatial reasoning and navigation-related tasks, an area critical for applications ranging from autonomous vehicle guidance to assistive technologies for the visually impaired. In this paper, we introduce a novel evaluation framework complemented by a baseline dataset, meticulously crafted for this study. This dataset is structured around three key tasks: plotting spatial points, planning routes in two-dimensional (2D) spaces, and devising pathways in three-dimensional (3D) environments. We specifically developed this dataset to assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals key insights into the model's capabilities and limitations in spatial understanding.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8317.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8317.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper directly evaluates GPT-4 on three spatial tasks (2D route planning, 3D route planning, and 2D spatial point plotting) using text-only prompts and uses algorithmic ground truth (Dijkstra) to benchmark correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in this paper simply as GPT-4, an aligned large language model (text-only) that the authors probe via text-based prompts to perform spatial tasks; no architecture details, training data, or parameter counts are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>2D route planning; 3D route planning; 2D spatial point plotting</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid shortest-path planning with obstacles; 3D grid shortest-path planning with obstacles; 2D grid point-plotting (mapping labeled coordinates to grid positions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Tasks were presented as text-based prompts describing grid instances. 2D/3D pathfinding: start at origin (0,0[,0]) to destination (x_max,y_max[,z_max]) on discrete grids with obstacles, with the rule that each step must be distance 1 and obstacles cannot be traversed. Benchmark ground-truth shortest paths were computed with Dijkstra's algorithm. Grids covered multiple sizes (2D: 3x3 up to 11x11) and obstacle ratios (0%,5%,10%,15%,20%,25%); for each size/ratio 10 random obstacle sets (≈470 grids total) were generated and infeasible grids excluded. For no-obstacle maps GPT-4 was asked to solve each instance ten times to probe instability. 2D plotting: grids from 2x2 to 10x10 with rows labeled by letters and columns by numbers; random uppercase-letter points placed at densities 5%-25% with 10 random sets each; GPT-4 prompted to report/plot those points and results compared to the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>The paper does not describe any internal algorithmic mechanism inside GPT-4; models were prompted with textual problem descriptions and authors recorded GPT-4's outputs and its textual 'journal' (step-by-step traces). The primary evaluation mechanism was comparison of GPT-4 outputs to algorithmic ground truth (Dijkstra). No explicit chain-of-thought prompting, symbolic planner, or external search/verification loop is reported in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively and via charts (numerical accuracy values are not provided in the text). Key reported findings: (1) GPT-4's accuracy in finding the Dijkstra shortest path decreases as grid size and obstacle ratio increase; (2) 2D pathfinding accuracy is higher than 3D pathfinding accuracy; (3) on no-obstacle maps authors observed instability across repeated runs (they ran 10 trials per instance); (4) for 2D plotting, accuracy decreases with larger map sizes and with larger point densities. The dataset contained ~470 2D grid instances used as benchmarks. The paper shows example cases (figures) where GPT-4 both succeeds and fails, but does not report aggregate numeric accuracies in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Evidence is primarily empirical comparison against algorithmic ground truth: visualizations of feasible paths (blue) versus GPT-4's produced path (red) and GPT-4's textual 'journal' showing how it chose steps. Failure examples include paths whose total distance does not match the Dijkstra-optimal cost and cases where GPT-4 violates the rule that each step must be length 1. The authors interpret the degradation in correctness with increasing size/obstacles (and worse performance in 3D) as evidence of limited spatial reasoning in GPT-4 under these conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparison is made only to an algorithmic oracle: Dijkstra's algorithm is used as the ground-truth benchmark for shortest paths. The paper does not compare GPT-4 quantitatively to other LLMs, to humans, or to other planning algorithms beyond using Dijkstra for correctness verification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) decreasing ability to find shortest paths as grid size and obstacle ratio increase; (2) notably worse performance in 3D than 2D; (3) instability across repeated runs (non-deterministic outputs on identical no-obstacle instances); (4) explicit failure cases where GPT-4 violates movement rules (e.g., taking steps longer than 1 unit) leading to invalid/incorrect paths; (5) the bespoke dataset may be too specific and not cover all aspects of spatial reasoning, limiting generalizability; (6) no internal mechanism or chain-of-thought strategy is described, so it's unclear whether GPT-4 leveraged any genuine spatial representation versus heuristics reflected in text outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherent limitations of LLMs regarding spatial information', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8317.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8317.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIG-bench tasks (navigate, geometric shape)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIG-bench: tasks including 'navigate' and 'geometric shape'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>'Navigate' and 'geometric shape' tasks from the BIG-bench benchmark are mentioned as existing tasks associated with spatial understanding; 'geometric shape' involves identifying shapes from SVG path elements while 'navigate' uses natural language descriptions resolvable by calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIG-bench</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated on BIG-bench tasks (not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced only as tasks within the BIG-bench suite; the paper does not describe model architectures or sizes for models evaluated on BIG-bench.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>BIG-bench 'navigate' and 'geometric shape' tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>'geometric shape' — perceptual/spatial shape identification from SVG path elements; 'navigate' — language-based navigation/math tasks that can involve spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Described in related work: BIG-bench contains diverse tasks; 'geometric shape' requires identifying shapes from SVG path elements, 'navigate' relies on natural language descriptions. The paper does not detail prompt formats or evaluation metrics from BIG-bench.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Not described in this paper for BIG-bench; only referenced as related work indicating spatial-related tasks exist within BIG-bench.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The paper notes these tasks have been associated with spatial understanding but does not report any analysis or evidence from BIG-bench results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>No direct comparisons are provided in this paper between BIG-bench results and the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper does not report failures on these BIG-bench tasks; it only cites them as prior tasks related to spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherent limitations of LLMs regarding spatial information', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8317.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8317.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARC / LMM studies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstraction and Reasoning Corpus (ARC) and studies using Large Multimodal Models (LMMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references prior studies that evaluated LMMs on ARC and studies that converted 2D images to text-based grids for text-only GPT-4, noting that ARC primarily measures abstraction/generalization beyond pure spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Abstraction and Reasoning Corpus (ARC)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Multimodal Models (LMMs) and text-only GPT-4 (in prior studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as prior work where multimodal models are evaluated on ARC; additionally, some studies converted images into text-grid representations to test text-only GPT-4. The current paper does not provide architectures, sizes, or training details for those models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ARC tasks (image-based abstraction/generalization tasks); text-grid conversions used with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Abstraction and reasoning tasks on 2D grids/images that require pattern extraction and generalization rather than only spatial shortest-path planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Referenced setups in prior work include evaluating LMMs on ARC and converting 2D images to text-based grid inputs to permit evaluation by text-only GPT-4; the paper does not replicate those setups but cites them as related approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Not described in this paper; ARC tasks typically require the model to infer abstract rules from a few examples and generalize, which differs from the pathfinding/prompted benchmarking used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here; only cited as prior evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The paper notes that ARC tasks involve abstraction/generalization beyond spatial reasoning, and that text-grid conversions have been attempted, but provides no new probes or ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>No direct quantitative comparisons provided between ARC/LMM study results and the experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper emphasizes that ARC-type tasks primarily measure abstraction and generalization and thus are not a direct substitute for the spatial reasoning benchmarks the authors introduce; no specific failure modes from ARC studies are recounted here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherent limitations of LLMs regarding spatial information', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BIG-bench <em>(Rating: 2)</em></li>
                <li>Abstraction and Reasoning Corpus (ARC) <em>(Rating: 2)</em></li>
                <li>Comparing humans, gpt-4, and gpt-4v on abstraction and reasoning tasks <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8317",
    "paper_id": "paper-265697251",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "The paper directly evaluates GPT-4 on three spatial tasks (2D route planning, 3D route planning, and 2D spatial point plotting) using text-only prompts and uses algorithmic ground truth (Dijkstra) to benchmark correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Described in this paper simply as GPT-4, an aligned large language model (text-only) that the authors probe via text-based prompts to perform spatial tasks; no architecture details, training data, or parameter counts are reported in the paper.",
            "model_size": null,
            "puzzle_name": "2D route planning; 3D route planning; 2D spatial point plotting",
            "puzzle_type": "2D grid shortest-path planning with obstacles; 3D grid shortest-path planning with obstacles; 2D grid point-plotting (mapping labeled coordinates to grid positions)",
            "task_setup": "Tasks were presented as text-based prompts describing grid instances. 2D/3D pathfinding: start at origin (0,0[,0]) to destination (x_max,y_max[,z_max]) on discrete grids with obstacles, with the rule that each step must be distance 1 and obstacles cannot be traversed. Benchmark ground-truth shortest paths were computed with Dijkstra's algorithm. Grids covered multiple sizes (2D: 3x3 up to 11x11) and obstacle ratios (0%,5%,10%,15%,20%,25%); for each size/ratio 10 random obstacle sets (≈470 grids total) were generated and infeasible grids excluded. For no-obstacle maps GPT-4 was asked to solve each instance ten times to probe instability. 2D plotting: grids from 2x2 to 10x10 with rows labeled by letters and columns by numbers; random uppercase-letter points placed at densities 5%-25% with 10 random sets each; GPT-4 prompted to report/plot those points and results compared to the benchmark.",
            "mechanisms_or_strategies": "The paper does not describe any internal algorithmic mechanism inside GPT-4; models were prompted with textual problem descriptions and authors recorded GPT-4's outputs and its textual 'journal' (step-by-step traces). The primary evaluation mechanism was comparison of GPT-4 outputs to algorithmic ground truth (Dijkstra). No explicit chain-of-thought prompting, symbolic planner, or external search/verification loop is reported in the experiments.",
            "performance_metrics": "Reported qualitatively and via charts (numerical accuracy values are not provided in the text). Key reported findings: (1) GPT-4's accuracy in finding the Dijkstra shortest path decreases as grid size and obstacle ratio increase; (2) 2D pathfinding accuracy is higher than 3D pathfinding accuracy; (3) on no-obstacle maps authors observed instability across repeated runs (they ran 10 trials per instance); (4) for 2D plotting, accuracy decreases with larger map sizes and with larger point densities. The dataset contained ~470 2D grid instances used as benchmarks. The paper shows example cases (figures) where GPT-4 both succeeds and fails, but does not report aggregate numeric accuracies in the text.",
            "evidence_of_spatial_reasoning": "Evidence is primarily empirical comparison against algorithmic ground truth: visualizations of feasible paths (blue) versus GPT-4's produced path (red) and GPT-4's textual 'journal' showing how it chose steps. Failure examples include paths whose total distance does not match the Dijkstra-optimal cost and cases where GPT-4 violates the rule that each step must be length 1. The authors interpret the degradation in correctness with increasing size/obstacles (and worse performance in 3D) as evidence of limited spatial reasoning in GPT-4 under these conditions.",
            "comparisons": "Direct comparison is made only to an algorithmic oracle: Dijkstra's algorithm is used as the ground-truth benchmark for shortest paths. The paper does not compare GPT-4 quantitatively to other LLMs, to humans, or to other planning algorithms beyond using Dijkstra for correctness verification.",
            "limitations_or_failure_cases": "Reported limitations include: (1) decreasing ability to find shortest paths as grid size and obstacle ratio increase; (2) notably worse performance in 3D than 2D; (3) instability across repeated runs (non-deterministic outputs on identical no-obstacle instances); (4) explicit failure cases where GPT-4 violates movement rules (e.g., taking steps longer than 1 unit) leading to invalid/incorrect paths; (5) the bespoke dataset may be too specific and not cover all aspects of spatial reasoning, limiting generalizability; (6) no internal mechanism or chain-of-thought strategy is described, so it's unclear whether GPT-4 leveraged any genuine spatial representation versus heuristics reflected in text outputs.",
            "uuid": "e8317.0",
            "source_info": {
                "paper_title": "Inherent limitations of LLMs regarding spatial information",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "BIG-bench tasks (navigate, geometric shape)",
            "name_full": "BIG-bench: tasks including 'navigate' and 'geometric shape'",
            "brief_description": "'Navigate' and 'geometric shape' tasks from the BIG-bench benchmark are mentioned as existing tasks associated with spatial understanding; 'geometric shape' involves identifying shapes from SVG path elements while 'navigate' uses natural language descriptions resolvable by calculation.",
            "citation_title": "BIG-bench",
            "mention_or_use": "mention",
            "model_name": "LLMs evaluated on BIG-bench tasks (not specified in this paper)",
            "model_description": "Referenced only as tasks within the BIG-bench suite; the paper does not describe model architectures or sizes for models evaluated on BIG-bench.",
            "model_size": null,
            "puzzle_name": "BIG-bench 'navigate' and 'geometric shape' tasks",
            "puzzle_type": "'geometric shape' — perceptual/spatial shape identification from SVG path elements; 'navigate' — language-based navigation/math tasks that can involve spatial reasoning",
            "task_setup": "Described in related work: BIG-bench contains diverse tasks; 'geometric shape' requires identifying shapes from SVG path elements, 'navigate' relies on natural language descriptions. The paper does not detail prompt formats or evaluation metrics from BIG-bench.",
            "mechanisms_or_strategies": "Not described in this paper for BIG-bench; only referenced as related work indicating spatial-related tasks exist within BIG-bench.",
            "performance_metrics": "Not reported in this paper for these tasks.",
            "evidence_of_spatial_reasoning": "The paper notes these tasks have been associated with spatial understanding but does not report any analysis or evidence from BIG-bench results.",
            "comparisons": "No direct comparisons are provided in this paper between BIG-bench results and the authors' experiments.",
            "limitations_or_failure_cases": "The paper does not report failures on these BIG-bench tasks; it only cites them as prior tasks related to spatial understanding.",
            "uuid": "e8317.1",
            "source_info": {
                "paper_title": "Inherent limitations of LLMs regarding spatial information",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ARC / LMM studies",
            "name_full": "Abstraction and Reasoning Corpus (ARC) and studies using Large Multimodal Models (LMMs)",
            "brief_description": "The paper references prior studies that evaluated LMMs on ARC and studies that converted 2D images to text-based grids for text-only GPT-4, noting that ARC primarily measures abstraction/generalization beyond pure spatial reasoning.",
            "citation_title": "Abstraction and Reasoning Corpus (ARC)",
            "mention_or_use": "mention",
            "model_name": "Large Multimodal Models (LMMs) and text-only GPT-4 (in prior studies)",
            "model_description": "Referenced as prior work where multimodal models are evaluated on ARC; additionally, some studies converted images into text-grid representations to test text-only GPT-4. The current paper does not provide architectures, sizes, or training details for those models.",
            "model_size": null,
            "puzzle_name": "ARC tasks (image-based abstraction/generalization tasks); text-grid conversions used with GPT-4",
            "puzzle_type": "Abstraction and reasoning tasks on 2D grids/images that require pattern extraction and generalization rather than only spatial shortest-path planning",
            "task_setup": "Referenced setups in prior work include evaluating LMMs on ARC and converting 2D images to text-based grid inputs to permit evaluation by text-only GPT-4; the paper does not replicate those setups but cites them as related approaches.",
            "mechanisms_or_strategies": "Not described in this paper; ARC tasks typically require the model to infer abstract rules from a few examples and generalize, which differs from the pathfinding/prompted benchmarking used in this paper.",
            "performance_metrics": "Not reported here; only cited as prior evaluations.",
            "evidence_of_spatial_reasoning": "The paper notes that ARC tasks involve abstraction/generalization beyond spatial reasoning, and that text-grid conversions have been attempted, but provides no new probes or ablations.",
            "comparisons": "No direct quantitative comparisons provided between ARC/LMM study results and the experiments in this paper.",
            "limitations_or_failure_cases": "The paper emphasizes that ARC-type tasks primarily measure abstraction and generalization and thus are not a direct substitute for the spatial reasoning benchmarks the authors introduce; no specific failure modes from ARC studies are recounted here.",
            "uuid": "e8317.2",
            "source_info": {
                "paper_title": "Inherent limitations of LLMs regarding spatial information",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BIG-bench",
            "rating": 2
        },
        {
            "paper_title": "Abstraction and Reasoning Corpus (ARC)",
            "rating": 2,
            "sanitized_title": "abstraction_and_reasoning_corpus_arc"
        },
        {
            "paper_title": "Comparing humans, gpt-4, and gpt-4v on abstraction and reasoning tasks",
            "rating": 2,
            "sanitized_title": "comparing_humans_gpt4_and_gpt4v_on_abstraction_and_reasoning_tasks"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.0121705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Inherent limitations of GPT-4 regarding spatial information
5 Dec 2023</p>
<p>He Yan he.yan@protagolabs.com 
International Monetary Fund</p>
<p>Xinyao Hu xinyao.hu@protagolabs.com 
International Monetary Fund</p>
<p>Xiangpeng Wan xiangpeng.wan@protagolabs.com 
International Monetary Fund</p>
<p>Chengyu Huang chuang@imf.org 
International Monetary Fund</p>
<p>Kai Zou 
International Monetary Fund</p>
<p>Michael Shiqi Xu michael@netmind.ai 
International Monetary Fund</p>
<p>Protagolabs 
International Monetary Fund</p>
<p>Inherent limitations of GPT-4 regarding spatial information
5 Dec 2023F48B29B6F86A2B6FA617E94AA1AECA8FarXiv:2312.03042v1[cs.CL]
Despite the significant advancements in natural language processing capabilities demonstrated by large language models such as ChatGPT, their proficiency in comprehending and processing spatial information, especially within the domains of 2D and 3D route planning, remains notably underdeveloped.This paper investigates the inherent limitations of ChatGPT and similar models in spatial reasoning and navigation-related tasks, an area critical for applications ranging from autonomous vehicle guidance to assistive technologies for the visually impaired.In this paper, we introduce a novel evaluation framework complemented by a baseline dataset, meticulously crafted for this study.This dataset is structured around three key tasks: plotting spatial points, planning routes in two-dimensional (2D) spaces, and devising pathways in three-dimensional (3D) environments.We specifically developed this dataset to assess the spatial reasoning abilities of ChatGPT.Our evaluation reveals key insights into the model's capabilities and limitations in spatial understanding.</p>
<p>Introduction</p>
<p>In the real world, there are numerous problems related to spatial understanding and reasoning, such as map navigation, autonomous driving, and engineering drafting.Some spatial information can be described in natural language, which humans can understand with simple common sense.Recently, aligned Large Language Models (LLMs) like ChatGPT and Claude 2 have demonstrated remarkable abilities to follow instructions (Ouyang et al., 2022).As the capabilities of LLMs continue to improve, traditional evaluation standards have become obsolete, and new benchmarks have been proposed to assess LLMs' ability to process extensive world knowledge or reason through complex tasks.Benchmarks such as MMLU (Hendrycks et al., 2020), BIG-bench (Srivastava et al., 2022), and AGIBench (Tang et al., 2023) have brought together a variety of tasks to comprehensively evaluate the capabilities of LLMs.There are also benchmarks that assess LLMs' abilities in specific areas, such as mathematical ability (Hendrycks et al., 2021;Cobbe et al., 2021;Welleck et al., 2021;Drori et al., 2023;Yang et al., 2023), programming skills (Chen et al., 2021;Zheng et al., 2023b;Du et al., 2023;Athiwaratkun et al., 2022).In some benchmarks, LLMs can achieve or surpass humanlevel performance, but currently, there is no benchmark specifically designed to evaluate LLMs' spatial understanding and reasoning abilities.</p>
<p>We believe that such a benchmark is necessary as a first step towards using language models to solve complex spatial problems in the real world.In this work, we have designed three different tasks focused on assessing LLMs' spatial understanding and reasoning abilities.The three tasks are plotting spatial points, planning routes in two-dimensional (2D) spaces, and devising pathways in three-dimensional (3D) environments.For each task, we have created a set of unique, solvable problems based on specific rules.During problem generation, parameters can be adjusted to control the complexity of the problems, thereby evaluating LLMs' performance on the same task at different levels of difficulty.We have written code using traditional methods to obtain standard answers, such as employing Dijkstra's algorithm to find the shortest path for planning routes.Subsequently, we use LLMs to generate answers and employ code to automatically verify the validity and correctness of the answers produced by the LLMs.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<p>• Novel Evaluation Framework: The paper introduces an innovative framework for evaluating the spatial reasoning abilities of large language models like GPT-4.This framework represents a significant step in understanding and quantifying how such models deal with spatial information.</p>
<p>• Bespoke Baseline Dataset: A specially designed dataset, tailored for this study, constitutes a major contribution.This dataset focuses on three spatial tasks-plotting spatial points, 2D route planning, and 3D route development-providing a unique and targeted tool for assessing the proficiency of language models in spatial reasoning.</p>
<p>• Insights into Model Capabilities and Limitations: The paper offers critical analysis and findings on the abilities and shortcomings of GPT-4 in handling spatial information.This analysis is crucial for applications requiring spatial awareness and navigation, thereby informing future model development and adaptation in fields like autonomous vehicle navigation and assistive technologies for the visually impaired.</p>
<p>Related Works</p>
<p>Recent efforts to evaluate Large Language Models (LLMs) have focused on their ability to process extensive world knowledge and reasoning through complex tasks.These evaluations can be divided into reference-based and reference-free methods.Reference-free methods (Wang et al., 2023c;Li et al., 2023b;Xu et al., 2023;Wang et al., 2023b), such as Chatbot Arena (Zheng et al., 2023a), involve human evaluation of model responses, while referencebased methods typically present language models with multiple-choice questions or compare model outputs against exact answers.Notable benchmarks include the MMLU, which offers multi-domain tasks from real-world sources, and the BIG-bench, featuring 204 diverse tasks, some designed to challenge LLMs with problems they cannot fully solve.Some benchmarks are created to evaluate performance in specific domains (Deng et al., 2023;Wang et al., 2023a;Lin et al., 2021;Schwettmann et al., 2023;Gandhi et al., 2023).For example, GSM8K (Cobbe et al., 2021) is aimed at elementary math problems, HumanEval (Chen et al., 2021) tests programming skills, and BIRD-SQL (Li et al., 2023a) focuses on converting text into SQL queries.</p>
<p>In previous work, tasks from the BIG-bench benchmark, such as "navigate" and "geometric shape" have been associated with spatial understanding.The "geometric shape" task requires models to identify shapes from SVG path elements, while "navigate" relies on natural language descriptions that could be resolved through mathematical calculations.The High School Math section of the MMLU benchmark includes analytic geometry problems that necessitate high school-level math and logical reasoning, which do not directly assess spatial understanding.Some studies have assessed the abilities of Large Multimodal Models (LMMs) on the Abstraction and Reasoning Corpus (ARC) (Mitchell et al., 2023;Moskvichev et al., 2023) and have also converted two-dimensional images into text-based grid inputs for textonly GPT-4 (OpenAI, 2023).However, the text generated in these studies often lacks an intuitive understanding of the images they represent.ARC tasks primarily require models to abstract underlying rules from a few examples and generalize them to unseen situations, a challenge that goes beyond the assessment of spatial understanding and reasoning capabilities.</p>
<p>Our work differs from these previous approaches by focusing exclusively on LLMs' spatial understanding capabilities without the need for additional specialized knowledge.We have designed tasks with adjustable difficulty levels, ranging from simple to complex, to enable a quantitative evaluation of language models' performance in spatial reasoning.Each task is crafted to allow for a scalable increase in complexity, with quantifiable metrics for difficulty, ensuring a nuanced assessment of spatial understanding in LLMs.</p>
<p>Experiments</p>
<p>Experiments 1 -2D Route Planning</p>
<p>In our 2D route planning experiments, we evaluate the spatial reasoning abilities of GPT-4 by asking GPT-4 to discover the optimal path in 2D space with obstacles through text-based prompts.Starting from the orgin point(0, 0) to the arriving point(x max , y max ).Finally, we employ Dijkstra's algorithm as the shortest path benchmark.We evaluate GPT-4's ability to find the shortest path.These prompts encompass the following criteria:</p>
<p>• Each step must take a distance of 1 unit.</p>
<p>• You must not traverse beyond obstacles.</p>
<p>• Ensure the precise determination of the starting point(0, 0) and arriving points(x max , y max ).</p>
<p>• The total distance cost should match that of Dijkstra's algorithm.</p>
<p>Experiment Setup</p>
<p>We need to create the grid database by adjusting the two parameters: grid dimensions and the proportion of obstacles.The database covers nine different grid sizes, starting from 3×3 and going up to 11×11.In each grid size, we introduce diversity by randomly adding obstacles and the obstacle ratios for each grid are 0% (no obstacles), 5%, 10%, 15%, 20%, and 25%.Furthermore, we generated ten sets of obstacles randomly for each obstacle ratio to control the complexity of the experiment.To maintain the experiment consistency, we exclude certain obstacle ratios for smaller grid sizes.Next, we employed the Dijkstra algorithm to obtain the correct total distance of the grid data set, and finally, we obtain all benchmarking data which contains approximately 470 sets of grid data.</p>
<p>It is noted that we exclude the grids with no possible paths in our experiment.Finally, we guide GPT-4 to find the optimal path in the grid database and evaluate its ability to find the shortest path.</p>
<p>Results</p>
<p>The chart illustrates GPT-4's accuracy in finding the shortest path in different map sizes and obstacle ratios.Due to the instability of GPT-4, we direct GPT-4 to find the shortest path for the map sizes with no obstacles ten times.The results reveal that as map size and obstacle ratio increase, the ability to correctly find the shortest path decreases.</p>
<p>Additionally, we visualize the GPT-4's journal to find the path in 2D.The blue dotted lines represent the feasible path leading to the destination, and the red line denotes the path generated by GPT-4.</p>
<p>Experiments 2 -3D Route Planning</p>
<p>In our 3D pathfinding experiments, we evaluate the spatial reasoning abilities of GPT-4 by employing a similar strategy as in the 2D experiments to find the optimal path within a 3D grid.This involves extending the 2D space into 3D space, starting from the origin point (0, 0, 0) and reaching the destination point (x max , y max , z max ).We still employ Dijkstra's algorithm as the benchmark to evaluate GPT-4's ability to find the shortest path in 3D.</p>
<p>Figure 3: Example 2: 6×6 grid with 15% obstacles, GPT-4 does not find the shortest path because the total distance is not consistent with the Dijkstra algorithm</p>
<p>Results</p>
<p>The chart shows GPT-4's accuracy in finding the shortest path in different map sizes and obstacle ratios in 3D.The results are similar to 2D pathfinding.As the map size and the obstacles ratio increase, the precision in finding the shortest path decreases.By evaluating GPT-4's pathfinding abilities in both 2D and 3D space, we can conclude that GPT-4's ability to find the shortest path in a 2D grid is better than in a 3D grid.We also visualize the GPT-4's journal to find the path in 3D.The button left corner is the starting point(0, 0, 0) and the top right corner is the destination point(x max , y max , z max ).The blue lines represent the available path leading to the destination, and the red line denotes the path generated by GPT-4.</p>
<p>Experiments 3 -Plotting Spatial Points in 2D</p>
<p>In this experiment, our objective is to evaluate GPT-4's ability to locate and plot points on a 2D map according to our text-based prompts.4×4×4 grid with 15% obstacles, GPT-4 does not find the shortest path because GPT-4 does not adhere to the rule that restricts each step to 1 unit, so it is not a valid path</p>
<p>Experiment Setup</p>
<p>To begin, we must establish benchmarking grids.These grids consist of dots, with rows labeled with lowercase letters, and columns labeled by numbers.Then, we randomly mark some points on these grids by using uppercase letters.These benchmarking grids have 9 grid sizes, ranging from 2×2 to 10×10.Random uppercase letters are placed on the grids, with the letter density varying at 5%, 10%, 15%, 20%, and 25%.Additionally, we generate 10 sets of random letters for each density.It's important to note that we only consider 25%(1 random point) point ratio for 2x2 grids and 10% and 20% point ratios for 3x3 grids to ensure consistency in the experiment.GPT-4 will be tasked with plotting these points based on text-based prompts.In the end, we will compare GPT-4's results with the benchmark for evaluation.</p>
<p>Results</p>
<p>The chart shows GPT-4's accuracy in plotting the points in different map sizes and point ratios in 2D.The results reveal that GPT-4 has limited abilities to plot the points in large map sizes and large point ratios.</p>
<p>Conclusion</p>
<p>In this paper, we introduced a novel framework and a custom dataset to evaluate the spatial reasoning abilities of large language models like GPT-4.Our findings provide important insights into the strengths and weaknesses of these models in understanding spatial information.By sharing our dataset and code, we aim to support further research in this area.This work not only advances our understanding of language models' capabilities in spatial reasoning but also lays the groundwork for future developments in technologies that require such skills.</p>
<p>Limitation.While our bespoke dataset is meticulously designed for this study, it might be too specific or not comprehensive enough to capture all aspects of spatial reasoning.This could limit the applicability of our findings to other spatial reasoning tasks or real-world scenarios.</p>
<p>Future.Future studies would develop more diverse and comprehensive datasets that include a wider range of spatial reasoning tasks, including dynamic and real-time spatial problems, to better understand the capabilities of language models in different scenarios.</p>
<p>Figure</p>
<p>Figure 1: Result</p>
<p>Figure 2 :
2
Figure 2: Example 1: 6×6 grid with 15% obstacles, GPT-4 correctly finds the shortest path</p>
<p>Figure</p>
<p>Figure 4: Result</p>
<p>Figure 5 :
5
Figure 5: Example 1: 4×4×4 grid with 15% obstacles, GPT-4 correctly finds the shortest path</p>
<p>FigureFigure 8 :
8
Figure 7: Enter Caption</p>
<p>Multi-lingual evaluation of code generation models. Ben Athiwaratkun, Krishna Sanjay, Zijian Gouda, Xiaopeng Wang, Yuchen Li, Ming Tian, Tan, Uddin Wasi, Shiqi Ahmad, Qing Wang, Mingyue Sun, Shang, arXiv:2210.148682022arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>A dataset for learning university stem courses at scale and generating questions at a human level. Iddo Drori, Sarah Zhang, Zad Chin, Reece Shuttleworth, Albert Lu, Linda Chen, Bereket Birbo, Michele He, Pedro Lantigua, Sunny Tran, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, arXiv:2308.018612023arXiv preprint</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah Goodman, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>Can llm already serve as a database interface? a big bench for large-scale database grounded text-tosqls. Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, arXiv:2305.031112023aarXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023b</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.07958Truthfulqa: Measuring how models mimic human falsehoods. 2021arXiv preprint</p>
<p>Melanie Mitchell, Alessandro B Palmarini, Arseny Moskvichev, Comparing humans, gpt-4, and gpt-4v on abstraction and reasoning tasks. 2023</p>
<p>The con-ceptARC benchmark: Evaluating understanding and generalization in the ARC domain. Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, Melanie Mitchell, Transactions on Machine Learning Research. 2023</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>FIND: A function description benchmark for evaluating interpretability methods. Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba, Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Agibench: A multi-granularity, multimodal, human-referenced, auto-scoring benchmark for large language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv:2309.064952022. 2023arXiv preprintBeyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>Decodingtrust: A comprehensive assessment of trustworthiness in GPT models. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023a</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, 2023b</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.754Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023c1</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, Kyunghyun Cho, arXiv:2104.01112Naturalproofs: Mathematical theorem proving in natural language. 2021arXiv preprint</p>
<p>A critical evaluation of evaluations for long-form question answering. Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi, Association of Computational Linguistics. 2023</p>
<p>Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, arXiv:2306.15626Leandojo: Theorem proving with retrievalaugmented language models. 2023arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, arXiv:2303.175682023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
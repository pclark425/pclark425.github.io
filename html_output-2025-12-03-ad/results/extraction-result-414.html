<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-414 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-414</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-414</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-251066434</p>
                <p><strong>Paper Title:</strong> A Mind-inspired Architecture for Adaptive HRI</p>
                <p><strong>Paper Abstract:</strong> One of the main challenges of social robots concerns the ability to guarantee robust, contextualized and intelligent behavior capable of supporting continuous and personalized interaction with different users over time. This implies that robot behaviors should consider the specificity of a person (e.g., personality, preferences, assistive needs), the social context as well as the dynamics of the interaction. Ideally, robots should have a “mind" to properly interact in real social environments allowing them to continuously adapt and exhibit engaging behaviors. The authors’ long-term research goal is to create an advanced mind-inspired system capable of supporting multiple assistance scenarios fostering personalization of robot’s behavior. This article introduces the idea of a dual process-inspired cognitive architecture that integrates two reasoning layers working on different time scales and making decisions over different temporal horizons. The general goal is also to support an empathetic relationship with the user through a multi-modal interaction inclusive of verbal and non-verbal expressions based on the emotional-cognitive profile of the person. The architecture is exemplified on a cognitive stimulation domain where some experiments show personalization capabilities of the approach as well as the joint work of the two layers. In particular, a feasibility assessment shows the customization of robot behaviors and the adaptation of robot interactions to the online detected state of a user. Usability sessions were performed in laboratory settings involving 10 healthy participants to assess the user interaction and the robot’s dialogue performance.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e414.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e414.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIRIAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mind Inspired aRchItecture for Adaptive huMan-robot interaction (Miriam)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-process, hybrid cognitive architecture that combines ontology- and rule-based semantic reasoning and timeline-based automated planning (declarative) with a reactive, context-driven action selection and NLU/speech modules (imperative) to deliver personalized, adaptive social-robot behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MIRIAM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Miriam is a two-layer hybrid architecture inspired by dual-process theory: a slower System 2 (declarative) layer performs ontology-based contextual reasoning (KOaLa), rule-based inference (Apache Jena), matchmaking/recommender logic and timeline-based automated planning (PLATINUm) to synthesize personalized assistive plans (motivations); a faster System 1 (imperative) reactive layer implements a context-variable state-transition policy that executes parametric actions (dialogue templates, facial expressions, multimodal outputs), driven by speech-to-text, NLU and personality-insight modules and returns success/failure feedback to System 2. Integration follows a modular, supervisory pattern (System 2 dispatches 'motivation' tokens to System 1; System 1 executes, adapts at runtime and feeds back execution outcomes for rescheduling or repair). The system is applied to personalized cognitive-stimulation dialogues on a Sanbot Elf robot.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Ontology-based knowledge graph (KOaLa) using DOLCE + ICF foundational models; rule-based contextual reasoning implemented in Apache Jena (custom rules formalizing impairments and stimulation opportunities); timeline-based automated planning and execution using PLATINUm (timeline formalism, tokens, temporal constraints); matchmaking/recommender implemented via matrix-vector ranking over ICF scores.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Reactive, imperative action-selection module using parametric actions (condition / responses / effects) and a policy π(ctxt) implemented as ordered executable-action processing; integration with IBM Watson Speech-to-Text and Watson NLU for intent/entity extraction; personality-insight tools for Big Five extraction; (architecturally analogous to HRL option policies; RL discussed as conceptually similar but not used as the implemented reactive decision method).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular hierarchical integration inspired by Hierarchical Reinforcement Learning: System 2 (declarative/AP/KR&R) replaces the usual HRL gating policy and synthesizes high-level 'motivations' (timeline tokens) that are dispatched to System 1; motivations are tagged with interaction parameters; System 1 executes parametric actions based on current context variables, updates context, and returns dispatch/execution feedback (success, failure) to System 2 which reschedules, repairs plans or proceeds. Integration is runtime, message-passing / token-dispatch with a feedback loop; no end-to-end gradient training.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines long-horizon, semantically grounded personalization and explainable planning (from ontologies, ICF mapping and explicit plans) with fine-grained, real-time adaptation to user utterances and sensor uncertainty (from reactive NLU-driven actions). Emergent capabilities include: (1) context-aware personalization (selecting only assistive actions relevant to detected impairments), (2) robust runtime adaptation and plan repair (rescheduling motivations on dispatch failure and replanning on execution failure), (3) multimodal empathetic behaviors (verbal + facial expressions modulated by personality/mood), (4) maintainable explainability of why actions were chosen (via ontological affordances and rule inferences) combined with interactive flexibility that pure symbolic planners lack.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Personalized cognitive rehabilitation dialogues / assistive-plan execution (timeline-based planning + reactive dialogue administration) evaluated in a cognitive-stimulation scenario ('Find the word' exercise) and offline ranking of exercises per user profile.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Usability and dialogue metrics from laboratory feasibility study: mean interaction length 6.58 minutes (SD=1.5); mean total turns 37.3 (SD=6.32); mean robot turns 17.5 (SD=3.1); mean user turns 19.8 (SD=3.45); mean robot errors per session M=1.31 (SD=0.63); Chatbot Usability Questionnaire (CUQ) mean score 72.81/100 (SD=8.59). (These are system-level usability outcomes; no standard ML benchmark metrics reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Architecturally intended to generalize across interaction domains: System 2 constructs domain-specific planning models from ontological affordances and System 1 can be extended with new reactive modules (navigation, manipulation). The paper reports coherent personalization across 8 offline user profiles and claims applicability to other domains (e.g., human-robot collaboration), but provides no quantitative out-of-distribution or compositional generalization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability for the deliberative layer: explicit ontologies (DOLCE+ICF), rule-based inferences (Apache Jena) and timeline plans provide semantic explanations for selected motivations and affordances; the reactive layer is rule/condition based (ordered actions) so individual action triggers and effects are inspectable; overall, the architecture preserves symbolic explanations while adding reactive adaptivity, enabling traceable cause-effect chains (e.g., which impairment → which stimulation → which scheduled token).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limitations acknowledged: evaluation limited (small sample, healthy participants rather than frail target users), no comparative baselines against pure-declarative or pure-imperative systems, reactive NLU/speech errors can cause dispatch/execution failures needing manual rescheduling, potential complexity in scaling rule bases and ontologies, and absence of learned end-to-end coordination between layers (no joint optimization). The planner cannot predict arbitrary user utterances so full open-ended dialogues require richer reactive policies or further integration.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Explicit dual-process theoretical framing: maps System 2 to symbolic KR&R and automated planning (long-horizon, explicit reasoning) and System 1 to reactive/adaptive methods (analogy to RL/option policies). Integration principle: division of labor (complementary strengths)—use declarative reasoning for semantics, goals and explanations; use imperative/reactive control for adaptation, temporal responsiveness and noisy human interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mind-inspired Architecture for Adaptive HRI', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e414.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e414.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augello et al. dual-process explanation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards A Dual Process Approach to Computational Explanation in Human-Robot Social Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-process hybrid system where a deep neural network (System 1) performs fast perceptual categorization (e.g., gestures) and a symbolic ontology-based module (System 2) provides high-level explanations about the features underlying the categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards A Dual Process Approach to Computational Explanation inHuman-Robot Social Interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dual-process explanation system (Augello et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-component hybrid: a System 1 deep neural network for fast perceptual recognition (e.g., gesture classification) paired with a System 2 symbolic ontology that maps perceptual outputs to high-level, human-understandable explanations by querying an ontology to extract features that characterize the neural net's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Ontology-based symbolic model (used to generate explanations), likely OWL-style ontologies mapping perceptual classes to high-level semantic features.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep neural network for perceptual classification (System 1), used for fast recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline/modular coupling: outputs of the neural perceptual module are post-processed by the symbolic ontology module to produce explanations; the architecture is not end-to-end differentiable but uses output-to-symbol mapping for explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables explanations of sub-symbolic classifications by grounding them in ontology concepts—combines perceptual robustness of DNNs with symbolic explanatory power that DNNs lack alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Gesture/perceptual recognition plus computational explanation in social HRI contexts (no standard benchmark reported in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Suggested improved interpretability and ability to map perceptual categories to semantic concepts; no quantitative generalization claims reported in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Improves interpretability by using ontology lookup to explain why a neural prediction corresponds to certain high-level features; explanation capability arises from the symbolic layer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not end-to-end integrated; relies on correct mappings from neural outputs to ontology concepts and may inherit perceptual errors from the neural module; no quantitative evaluation reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Dual-process theory: fast/subsymbolic perception (System 1) plus slower symbolic reasoning/explanation (System 2) for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mind-inspired Architecture for Adaptive HRI', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e414.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e414.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLARION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The CLARION Cognitive Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid cognitive architecture combining connectionist (implicit) and symbolic (explicit) representations in a modular dual-representation structure to model implicit and explicit psychological processes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The CLARION Cognitive Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CLARION</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular hybrid architecture with dual representational structure in each subsystem: an implicit (connectionist) layer learning from experience and an explicit (symbolic/rule-like) layer for higher-level reasoning, integrated to support both automatic and controlled cognitive processes.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit symbolic representations and rule-like structures for conscious, controlled reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist/subsymbolic networks for implicit learning and fast responses.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Dual-representation coupling where implicit and explicit layers interact within modules; learning in implicit layer can inform explicit policies and vice-versa via established CLARION mechanisms (not end-to-end differentiable).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to model both implicit skill acquisition and explicit reasoning, capturing behaviors that neither purely symbolic nor purely connectionist systems alone can fully model (e.g., interaction of learned habits and deliberate planning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General cognitive modeling across tasks (no single benchmark specified in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to capture both learning from experience (implicit) and flexible reasoning (explicit); qualitative claims of broader behavioral coverage but no standardized OOD benchmarks reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Explicit layer offers interpretable symbolic content; implicit layer less interpretable but captures skill acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Complex coupling can be difficult to scale; implicit-explicit arbitration and transfer mechanisms can be design-sensitive; no application-specific performance metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Explicitly motivated by dual-process psychology: integrates implicit (System 1-like) and explicit (System 2-like) processing within each cognitive subsystem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mind-inspired Architecture for Adaptive HRI', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e414.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e414.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-memory hybrid (ref. [51])</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid two-memory architecture (long-term transparent neural network + working memory buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture that models long-term memory as a transparent neural network that develops autonomously through interaction and a working memory buffer that contains nodes of the long-term memory, combining subsymbolic learning with symbolic-like working structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Two-memory hybrid architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Proposed model with (i) a long-term memory implemented as a transparent neural network that autonomously develops via environment interactions (subsymbolic learning), and (ii) a working memory buffer that holds nodes/elements from long-term memory to support resource-bounded symbolic-like computation; intended to bridge symbolic and subsymbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Working memory buffer that manipulates nodes (symbol-like structures) drawn from the neural long-term memory; functions as an explicit, bounded computation workspace.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transparent neural network implementing long-term memory and learning (connectionist, subsymbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Hybrid memory coupling: symbolic-like working memory references and manipulates patterns/nodes produced by the neural long-term memory; this supports interplay between learned representations and explicit computation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Allows symbolic-style, resource-bounded computation over representations shaped by learned neural structure, combining adaptability of neural learning with explicit manipulation capabilities of working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Architecturally suggests improved adaptivity by learning representations while retaining capability for bounded explicit reasoning; no quantitative evidence provided in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Potentially improved transparency because the long-term memory is described as 'transparent' and the working memory exposes nodes for inspection; details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Description in paper is conceptual in this article's summary; implementation, scalability and empirical validation are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Hybrid memory theory: long-term subsymbolic learning plus an explicit working memory mediates symbolic computation; aligns with dual-process and memory-centered hybrid proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mind-inspired Architecture for Adaptive HRI', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards A Dual Process Approach to Computational Explanation inHuman-Robot Social Interaction. <em>(Rating: 2)</em></li>
                <li>The CLARION Cognitive Architecture <em>(Rating: 2)</em></li>
                <li>A Cognitive Architecture Based on Dual Process Theory <em>(Rating: 2)</em></li>
                <li>PLAT-INUm: A New Framework for Planning and Acting <em>(Rating: 2)</em></li>
                <li>A holistic approach to behavior adaptation for socially assistive robots <em>(Rating: 2)</em></li>
                <li>A Two-Layered Approach to Adaptive Dialogues for Robotic Assistance <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-414",
    "paper_id": "paper-251066434",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "MIRIAM",
            "name_full": "Mind Inspired aRchItecture for Adaptive huMan-robot interaction (Miriam)",
            "brief_description": "A dual-process, hybrid cognitive architecture that combines ontology- and rule-based semantic reasoning and timeline-based automated planning (declarative) with a reactive, context-driven action selection and NLU/speech modules (imperative) to deliver personalized, adaptive social-robot behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MIRIAM",
            "system_description": "Miriam is a two-layer hybrid architecture inspired by dual-process theory: a slower System 2 (declarative) layer performs ontology-based contextual reasoning (KOaLa), rule-based inference (Apache Jena), matchmaking/recommender logic and timeline-based automated planning (PLATINUm) to synthesize personalized assistive plans (motivations); a faster System 1 (imperative) reactive layer implements a context-variable state-transition policy that executes parametric actions (dialogue templates, facial expressions, multimodal outputs), driven by speech-to-text, NLU and personality-insight modules and returns success/failure feedback to System 2. Integration follows a modular, supervisory pattern (System 2 dispatches 'motivation' tokens to System 1; System 1 executes, adapts at runtime and feeds back execution outcomes for rescheduling or repair). The system is applied to personalized cognitive-stimulation dialogues on a Sanbot Elf robot.",
            "declarative_component": "Ontology-based knowledge graph (KOaLa) using DOLCE + ICF foundational models; rule-based contextual reasoning implemented in Apache Jena (custom rules formalizing impairments and stimulation opportunities); timeline-based automated planning and execution using PLATINUm (timeline formalism, tokens, temporal constraints); matchmaking/recommender implemented via matrix-vector ranking over ICF scores.",
            "imperative_component": "Reactive, imperative action-selection module using parametric actions (condition / responses / effects) and a policy π(ctxt) implemented as ordered executable-action processing; integration with IBM Watson Speech-to-Text and Watson NLU for intent/entity extraction; personality-insight tools for Big Five extraction; (architecturally analogous to HRL option policies; RL discussed as conceptually similar but not used as the implemented reactive decision method).",
            "integration_method": "Modular hierarchical integration inspired by Hierarchical Reinforcement Learning: System 2 (declarative/AP/KR&R) replaces the usual HRL gating policy and synthesizes high-level 'motivations' (timeline tokens) that are dispatched to System 1; motivations are tagged with interaction parameters; System 1 executes parametric actions based on current context variables, updates context, and returns dispatch/execution feedback (success, failure) to System 2 which reschedules, repairs plans or proceeds. Integration is runtime, message-passing / token-dispatch with a feedback loop; no end-to-end gradient training.",
            "emergent_properties": "Combines long-horizon, semantically grounded personalization and explainable planning (from ontologies, ICF mapping and explicit plans) with fine-grained, real-time adaptation to user utterances and sensor uncertainty (from reactive NLU-driven actions). Emergent capabilities include: (1) context-aware personalization (selecting only assistive actions relevant to detected impairments), (2) robust runtime adaptation and plan repair (rescheduling motivations on dispatch failure and replanning on execution failure), (3) multimodal empathetic behaviors (verbal + facial expressions modulated by personality/mood), (4) maintainable explainability of why actions were chosen (via ontological affordances and rule inferences) combined with interactive flexibility that pure symbolic planners lack.",
            "task_or_benchmark": "Personalized cognitive rehabilitation dialogues / assistive-plan execution (timeline-based planning + reactive dialogue administration) evaluated in a cognitive-stimulation scenario ('Find the word' exercise) and offline ranking of exercises per user profile.",
            "hybrid_performance": "Usability and dialogue metrics from laboratory feasibility study: mean interaction length 6.58 minutes (SD=1.5); mean total turns 37.3 (SD=6.32); mean robot turns 17.5 (SD=3.1); mean user turns 19.8 (SD=3.45); mean robot errors per session M=1.31 (SD=0.63); Chatbot Usability Questionnaire (CUQ) mean score 72.81/100 (SD=8.59). (These are system-level usability outcomes; no standard ML benchmark metrics reported.)",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Architecturally intended to generalize across interaction domains: System 2 constructs domain-specific planning models from ontological affordances and System 1 can be extended with new reactive modules (navigation, manipulation). The paper reports coherent personalization across 8 offline user profiles and claims applicability to other domains (e.g., human-robot collaboration), but provides no quantitative out-of-distribution or compositional generalization experiments.",
            "interpretability_properties": "High interpretability for the deliberative layer: explicit ontologies (DOLCE+ICF), rule-based inferences (Apache Jena) and timeline plans provide semantic explanations for selected motivations and affordances; the reactive layer is rule/condition based (ordered actions) so individual action triggers and effects are inspectable; overall, the architecture preserves symbolic explanations while adding reactive adaptivity, enabling traceable cause-effect chains (e.g., which impairment → which stimulation → which scheduled token).",
            "limitations_or_failures": "Limitations acknowledged: evaluation limited (small sample, healthy participants rather than frail target users), no comparative baselines against pure-declarative or pure-imperative systems, reactive NLU/speech errors can cause dispatch/execution failures needing manual rescheduling, potential complexity in scaling rule bases and ontologies, and absence of learned end-to-end coordination between layers (no joint optimization). The planner cannot predict arbitrary user utterances so full open-ended dialogues require richer reactive policies or further integration.",
            "theoretical_framework": "Explicit dual-process theoretical framing: maps System 2 to symbolic KR&R and automated planning (long-horizon, explicit reasoning) and System 1 to reactive/adaptive methods (analogy to RL/option policies). Integration principle: division of labor (complementary strengths)—use declarative reasoning for semantics, goals and explanations; use imperative/reactive control for adaptation, temporal responsiveness and noisy human interaction.",
            "uuid": "e414.0",
            "source_info": {
                "paper_title": "A Mind-inspired Architecture for Adaptive HRI",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Augello et al. dual-process explanation",
            "name_full": "Towards A Dual Process Approach to Computational Explanation in Human-Robot Social Interaction",
            "brief_description": "A dual-process hybrid system where a deep neural network (System 1) performs fast perceptual categorization (e.g., gestures) and a symbolic ontology-based module (System 2) provides high-level explanations about the features underlying the categorization.",
            "citation_title": "Towards A Dual Process Approach to Computational Explanation inHuman-Robot Social Interaction.",
            "mention_or_use": "mention",
            "system_name": "Dual-process explanation system (Augello et al.)",
            "system_description": "Two-component hybrid: a System 1 deep neural network for fast perceptual recognition (e.g., gesture classification) paired with a System 2 symbolic ontology that maps perceptual outputs to high-level, human-understandable explanations by querying an ontology to extract features that characterize the neural net's outputs.",
            "declarative_component": "Ontology-based symbolic model (used to generate explanations), likely OWL-style ontologies mapping perceptual classes to high-level semantic features.",
            "imperative_component": "Deep neural network for perceptual classification (System 1), used for fast recognition.",
            "integration_method": "Pipeline/modular coupling: outputs of the neural perceptual module are post-processed by the symbolic ontology module to produce explanations; the architecture is not end-to-end differentiable but uses output-to-symbol mapping for explanation.",
            "emergent_properties": "Enables explanations of sub-symbolic classifications by grounding them in ontology concepts—combines perceptual robustness of DNNs with symbolic explanatory power that DNNs lack alone.",
            "task_or_benchmark": "Gesture/perceptual recognition plus computational explanation in social HRI contexts (no standard benchmark reported in this paper's mention).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Suggested improved interpretability and ability to map perceptual categories to semantic concepts; no quantitative generalization claims reported in this paper's summary.",
            "interpretability_properties": "Improves interpretability by using ontology lookup to explain why a neural prediction corresponds to certain high-level features; explanation capability arises from the symbolic layer.",
            "limitations_or_failures": "Not end-to-end integrated; relies on correct mappings from neural outputs to ontology concepts and may inherit perceptual errors from the neural module; no quantitative evaluation reported here.",
            "theoretical_framework": "Dual-process theory: fast/subsymbolic perception (System 1) plus slower symbolic reasoning/explanation (System 2) for interpretability.",
            "uuid": "e414.1",
            "source_info": {
                "paper_title": "A Mind-inspired Architecture for Adaptive HRI",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "CLARION",
            "name_full": "The CLARION Cognitive Architecture",
            "brief_description": "A hybrid cognitive architecture combining connectionist (implicit) and symbolic (explicit) representations in a modular dual-representation structure to model implicit and explicit psychological processes.",
            "citation_title": "The CLARION Cognitive Architecture",
            "mention_or_use": "mention",
            "system_name": "CLARION",
            "system_description": "Modular hybrid architecture with dual representational structure in each subsystem: an implicit (connectionist) layer learning from experience and an explicit (symbolic/rule-like) layer for higher-level reasoning, integrated to support both automatic and controlled cognitive processes.",
            "declarative_component": "Explicit symbolic representations and rule-like structures for conscious, controlled reasoning.",
            "imperative_component": "Connectionist/subsymbolic networks for implicit learning and fast responses.",
            "integration_method": "Dual-representation coupling where implicit and explicit layers interact within modules; learning in implicit layer can inform explicit policies and vice-versa via established CLARION mechanisms (not end-to-end differentiable).",
            "emergent_properties": "Ability to model both implicit skill acquisition and explicit reasoning, capturing behaviors that neither purely symbolic nor purely connectionist systems alone can fully model (e.g., interaction of learned habits and deliberate planning).",
            "task_or_benchmark": "General cognitive modeling across tasks (no single benchmark specified in this paper's mention).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Designed to capture both learning from experience (implicit) and flexible reasoning (explicit); qualitative claims of broader behavioral coverage but no standardized OOD benchmarks reported here.",
            "interpretability_properties": "Explicit layer offers interpretable symbolic content; implicit layer less interpretable but captures skill acquisition.",
            "limitations_or_failures": "Complex coupling can be difficult to scale; implicit-explicit arbitration and transfer mechanisms can be design-sensitive; no application-specific performance metrics reported here.",
            "theoretical_framework": "Explicitly motivated by dual-process psychology: integrates implicit (System 1-like) and explicit (System 2-like) processing within each cognitive subsystem.",
            "uuid": "e414.2",
            "source_info": {
                "paper_title": "A Mind-inspired Architecture for Adaptive HRI",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Two-memory hybrid (ref. [51])",
            "name_full": "Hybrid two-memory architecture (long-term transparent neural network + working memory buffer)",
            "brief_description": "A hybrid architecture that models long-term memory as a transparent neural network that develops autonomously through interaction and a working memory buffer that contains nodes of the long-term memory, combining subsymbolic learning with symbolic-like working structures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Two-memory hybrid architecture",
            "system_description": "Proposed model with (i) a long-term memory implemented as a transparent neural network that autonomously develops via environment interactions (subsymbolic learning), and (ii) a working memory buffer that holds nodes/elements from long-term memory to support resource-bounded symbolic-like computation; intended to bridge symbolic and subsymbolic reasoning.",
            "declarative_component": "Working memory buffer that manipulates nodes (symbol-like structures) drawn from the neural long-term memory; functions as an explicit, bounded computation workspace.",
            "imperative_component": "Transparent neural network implementing long-term memory and learning (connectionist, subsymbolic).",
            "integration_method": "Hybrid memory coupling: symbolic-like working memory references and manipulates patterns/nodes produced by the neural long-term memory; this supports interplay between learned representations and explicit computation.",
            "emergent_properties": "Allows symbolic-style, resource-bounded computation over representations shaped by learned neural structure, combining adaptability of neural learning with explicit manipulation capabilities of working memory.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Architecturally suggests improved adaptivity by learning representations while retaining capability for bounded explicit reasoning; no quantitative evidence provided in this paper's mention.",
            "interpretability_properties": "Potentially improved transparency because the long-term memory is described as 'transparent' and the working memory exposes nodes for inspection; details not provided here.",
            "limitations_or_failures": "Description in paper is conceptual in this article's summary; implementation, scalability and empirical validation are not reported here.",
            "theoretical_framework": "Hybrid memory theory: long-term subsymbolic learning plus an explicit working memory mediates symbolic computation; aligns with dual-process and memory-centered hybrid proposals.",
            "uuid": "e414.3",
            "source_info": {
                "paper_title": "A Mind-inspired Architecture for Adaptive HRI",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards A Dual Process Approach to Computational Explanation inHuman-Robot Social Interaction.",
            "rating": 2,
            "sanitized_title": "towards_a_dual_process_approach_to_computational_explanation_inhumanrobot_social_interaction"
        },
        {
            "paper_title": "The CLARION Cognitive Architecture",
            "rating": 2,
            "sanitized_title": "the_clarion_cognitive_architecture"
        },
        {
            "paper_title": "A Cognitive Architecture Based on Dual Process Theory",
            "rating": 2,
            "sanitized_title": "a_cognitive_architecture_based_on_dual_process_theory"
        },
        {
            "paper_title": "PLAT-INUm: A New Framework for Planning and Acting",
            "rating": 2,
            "sanitized_title": "platinum_a_new_framework_for_planning_and_acting"
        },
        {
            "paper_title": "A holistic approach to behavior adaptation for socially assistive robots",
            "rating": 2,
            "sanitized_title": "a_holistic_approach_to_behavior_adaptation_for_socially_assistive_robots"
        },
        {
            "paper_title": "A Two-Layered Approach to Adaptive Dialogues for Robotic Assistance",
            "rating": 2,
            "sanitized_title": "a_twolayered_approach_to_adaptive_dialogues_for_robotic_assistance"
        }
    ],
    "cost": 0.016351499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Mind-inspired Architecture for Adaptive HRI
2023</p>
<p>Alessandro Umbrico alessandro.umbrico@istc.cnr.it 
Riccardo De Benedictis riccardo.debenedictis@istc.cnr.it 
Francesca Fracasso francesca.fracasso@istc.cnr.it 
Amedeo Cesta amedeo.cesta@istc.cnr.it 
Andrea Orlandini andrea.orlandini@istc.cnr.it 
Gabriella Cortellessa gabriella.cortellessa@istc.cnr.it 
Alessandro Umbrico 
Francesca Fracasso 
Amedeo Cesta 
Andrea Orlandini 
Gabriella Cortellessa 
A Mind-inspired Architecture for Adaptive HRI</p>
<p>International Journal of Social Robotics
15202310.1007/s12369-022-00897-8Accepted: 31 May 2022 / Published online: 25 July 2022Authors are also members of the EU project TAILOR "Foundations of Trustworthy AI Integrating Learning, Optimisation and Reasoning" GA#952215. B Riccardo De Benedictis
One of the main challenges of social robots concerns the ability to guarantee robust, contextualized and intelligent behavior capable of supporting continuous and personalized interaction with different users over time. This implies that robot behaviors should consider the specificity of a person (e.g., personality, preferences, assistive needs), the social context as well as the dynamics of the interaction. Ideally, robots should have a "mind" to properly interact in real social environments allowing them to continuously adapt and exhibit engaging behaviors. The authors' long-term research goal is to create an advanced mind-inspired system capable of supporting multiple assistance scenarios fostering personalization of robot's behavior. This article introduces the idea of a dual process-inspired cognitive architecture that integrates two reasoning layers working on different time scales and making decisions over different temporal horizons. The general goal is also to support an empathetic relationship with the user through a multi-modal interaction inclusive of verbal and non-verbal expressions based on the emotional-cognitive profile of the person. The architecture is exemplified on a cognitive stimulation domain where some experiments show personalization capabilities of the approach as well as the joint work of the two layers. In particular, a feasibility assessment shows the customization of robot behaviors and the adaptation of robot interactions to the online detected state of a user. Usability sessions were performed in laboratory settings involving 10 healthy participants to assess the user interaction and the robot's dialogue performance.Keywords Assistant Robotics · Mind-inspired architectures for social robots · Engaging and personalized HRI · Multi-modal interaction This work is supported by "SI-Robotics: SocIal ROBOTICS for active and healthy ageing" project (Italian M.I.U.R., PON -Ricerca e Innovazione</p>
<p>Introduction</p>
<p>According to the 2021 Aging Report [27] the EU's demographic old-age dependency ratio (i.e., the ratio between people aged 65 years and over and those aged 20-64) is projected to increase significantly in the coming decades. Most seniors also want to age in place [5], meaning they aim to live independently in their own home for as long as possible. However, older people experience numerous barriers to healthy and active aging also considering the inadequacy of health resources and the scarcity of social supports. By 2030, the World Health Organization (WHO) foresees a worldwide workforce shortfall of healthcare professionals, with dramatic consequences for patients, economies and communities. In this scenario, research is investigating the possible role of technology in general and, more particularly of robots, 1 ISTC-CNR -Institute of Cognitive Sciences and Technologies, National Research Council of Italy, Via S. Martino della Battaglia 44, Rome, Italy as a help tool to bridge this gap between the demand for assistance and the actual offer. Mois et al. [40] present an analysis from the point of view of physical, cognitive and social support, highlighting the relevance of assistive robots to facilitate active and healthy aging.</p>
<p>Social Robotics is a growing interdisciplinary research field aiming at supporting humans in different social situations through robot-based solutions. Socially Assistive Robotics (SAR) can be seen as a specific area of Social Robotics focused on supporting humans in assistive scenarios [20,57] and on assessing their impact and acceptance in realistic scenarios [6]. Results in this field could play a central role to compensate the foreseen workforce shortage and growing assistance demand. Examples of applications concern the use of interacting robots to improve the lifestyle of people with dementia and support their caregivers. In [9] a social robot is integrated with a network of sensors to measure Activity of Daily Living with the aim of providing context-relevant suggestions to the person with dementia and reducing the burden of the caregiver. Recent interests focus on the development of robots that also contribute to emotional wellness via social engagement [13].</p>
<p>One of the main challenge is to ensure continuous assistance in a variety of situations, adapting general capabilities of robots to the specific features of assistive scenarios. Robots should be able to act in a socially accepted manner, ensuring a user interaction as much fluid, natural and engaging as possible. Novel control approaches should thus properly deal with human needs, preferences and uncertainty as well as humanrelated interaction features like e.g., social norms, culture and the environmental context [4,8]. Furthermore, emotions and empathy are crucial components of human-robot interaction [23] and can help strengthen the acceptability of technological solutions, especially in the long term. Considering frail elderly in particular, emotion is a highly relevant factor to foster their involvement during interactions.</p>
<p>Robots need a mind to effectively act in complex social scenarios and interact with humans in a reliable, continuous and adaptive way. This work describes recent advancements concerning the realization of an AI-based cognitive control approach for SAR [62,64] exploiting Artificial Intelligence (AI) techniques to realize (artificial) cognitive capabilities of robots. Our long-term research goal is to develop a robotintegrated cognitive system enabling intelligent behaviors, engaging human-robot interactions and continuity of use during assistive tasks. The two-fold aim is, on one hand, to create a support for healthcare professionals in their role of caregivers, on the other hand, to provide assistive robots with personalization and adaptability to effectively help patients with heterogeneous health-related needs in different assistive contexts. The idea presented here is influenced by existing cognitive architectures [35,36,38] and pursues the integration of different AI technologies to realize the cognitive capabilities necessary to pursue adaptation and personalization as well as natural and engaging interactions [41,49].</p>
<p>Similarly to other works [3,51,52], the proposed cognitive architecture is inspired by the dual process theory, a psychological model according to which the human mind would follow two distinct and parallel reasoning processes. The two processes consist of an implicit, automatic, unconscious and faster process (System 1) and an explicit, controlled, conscious and slower process (System 2) [32,34,46,50,69]. Based on this theory, we propose the Miriam architecture (where Miriam stands for "Mind Inspired aRchItecture for Adaptive huMan-robot interaction"). It integrates two interconnected "autonomous" layers working at different time scales and making decisions over different horizons. A "faster" reactive layer encapsulates machine learning and natural language processing to endow the robot with the capabilities of handling dynamic conversations and complex interactions with users in a flexible and adaptive way. A "slower" deliberative layer encapsulates automated planning and execution to synthesise contextualized assistive and supporting actions (i.e., an assistive plan), tailored to participants' behaviours, characteristics, health-related needs and preferences. Additionally, this work considers the emotional-cognitive aspects of a user to support an engaging behavior and involvement of the robot. In particular, the robotic system interacts with the user through verbal and non-verbal communication channels (facial expressions) to favor an empathetic and emotional response. Miriam is evaluated on a cognitive training scenario where a user should follow a cognitive rehabilitation program "guided" by an assistive robot that generates a set of stimuli (or instructions) suited to the specific health needs of each user.</p>
<p>The article is organised as follows: Sect. 2 discusses some works on personalization and provides a brief overview of the main cognitive architectures; Sect. 3 describes the idea that inspired our architecture by describing the various AI modules on which it is based; Sect. 4 introduces the reasoning and planning module inspired by System 2; Sect. 5 describes the reactive module used to dynamically customise the robotic agent's behaviour during interaction with the user inspired by System 1; Sect. 6 demonstrates the functioning of the proposed architecture on an example in the domain of cognitive rehabilitation and presents preliminary assessments of the system. Finally, Sect. 7 concludes the work and points to future developments.</p>
<p>Related Works from</p>
<p>Human-Robot-Interaction</p>
<p>Personalization</p>
<p>One of the main challenges of human-robot interaction applied to different research areas such as physical and cognitive rehabilitation, support for social interaction or robot companions, lies in ensuring a fluid and effective interaction so that the user feels like interacting with an intelligent agent.</p>
<p>Interactions based on non adaptive behaviors can become repetitive over time, decreasing user involvement after the novelty effect wears off. In this perspective, both reactivity and personalization are valid features to improve user involvement in long-term interactions, allowing an artificial agent to adopt behaviors that adapt to various relevant factors such as, for example, user's personality, needs, preferences and the changing context of the interaction. Human-Robot Interaction (HRI) studies have been inspired by many theories of Social Psychology and behavioral research to create systems customised to the users, the context, the environment and the tasks and also to be able to react to dynamics events in long-term interactions.</p>
<p>The similarity-attraction principle [22], for example, assumes that individuals are more attracted to others who manifest the same characteristics. Indeed, it has been suggested that interpersonal similarity and attraction are multidimensional constructs in which people are attracted to people similar to themselves in demographics, physical appearance, attitudes, interpersonal style, social and cultural background, personality, preferred interests and activities, communication and social skills [39].</p>
<p>Some of the aforementioned constructs have been investigated in HRI and used to suggest best practices and guidelines to system developers. For example, social and cultural background have been explored in [8]. The authors conceived a culture-based ontology to foster contextualised HRI. A preliminary evaluation of the framework with Italian and German volunteers showed that using the framework with the proposed algorithms can significantly speed-up the acquisition of person-specific knowledge. It has also been argued that cultural robotics should design robots to be sensitive and adaptable to salient cultural factors, rather than designing robots for specific cultures, because of dynamic nature of culture itself and its role in shaping human cognition and social interaction which are equally dynamic [66]. Also personality has been investigated in HRI, e.g. [42,58] to guide/influence the interaction between humans and robots. Correlations among personality traits such as sociability, activity, impulsiveness, liveliness and excitability influence implemented robot behaviors. In this regard the extroversion-introversion dimension represents one of the most used trait, possibly because of its ease of implementation in terms of behav-ioral expressions. Research has given attention also to both verbal and non-verbal cues. There is evidence indeed that non-verbal cues could attribute positive or negative valence to a "neutral" verbal message, and that verbal and nonverbal communication can facilitate the understanding of messages provided by the robot, when combined appropriately [33].</p>
<p>This work focuses on some of the mentioned aspects in order to support personalised dialogues in the cognitive rehabilitation domain where a robot is supposed to deliver tailored cognitive training to older adults.</p>
<p>Cognitive Architectures</p>
<p>Artificial Intelligence, Cognitive Sciences, Neuroscience and Robotics have all contributed to the understanding of minds by focusing at different levels of abstraction. While Cognitive Sciences mostly focus on understanding cognitive processes and Neuroscience focus on the structure and physiology of the brain (i.e., the physiological and physical correlates of mental processes), Robotics and AI focus on understanding how minds work in order to provide software or physical agents with intelligent behaviors. Although with different perspectives and levels of abstraction, the common objective is to understand and simulate the functioning of human mind (or the brain if looking at the "physical device") and the related cognitive functions. Cognitive architectures represent those part of AI research, with the goal of creating programs capable to reason about problems across different domains, develop insights, adapt to new situations and reflect on themselves. Similarly, research in cognitive architectures aims at modeling the human mind, eventually enabling to build human-inspired artificial intelligence.</p>
<p>During the last years, different efforts explored the development of cognitive architectures based on the human mind (see [35] for a review). A common agreement among AI researchers sees cognitive architectures classified in symbolic, connectionist, or hybrid ones. Symbolic architectures use production rules [43] and represent concepts using symbols that can be manipulated using a predefined instruction set. In connectionist architectures, (e.g., [26,44]), the adaptability and learning aspects is resolved by building massively parallel models and concepts are represented across multiple components or nodes that are organized in networks. Hybrid architectures attempt to combine elements of both symbolic and connectionist approaches, by including both symbolic and subsymbolic components [7], with the aim to match human cognition.</p>
<p>Within the hybrid architectures category some research efforts have been inspired by the dual process theory and combined symbolic and subsymbolic components in the attempt to simulate the two processes (System 1 and System 2). More specifically, [3], propose to endow a social robot with a computational explanation module based on two components: a System 1 module (S1) responsible for the fast categorization and for the perceptual based recognition of gestures in a social context, based on deep neural network architecture; a System 2 component (S2) responsible for providing a high level model that can be exploited to extract an explanation about the high level features that characterize the categorized output provided by S1 exploiting an ontology.</p>
<p>Differently, [51] developed a model able to handle both symbolic and subsymbolic reasoning, by means of an architechture based on two memory systems: (i) a long-term memory, an autonomous system that develops automatically through interactions with the environment, and (ii) a working memory, a memory system used to define the notion of (resource-bounded) computation. The long-term memory is modeled as a transparent neural network that develops autonomously by interacting with the environment, while the working memory is modeled as a buffer containing nodes of the long-term memory.</p>
<p>Finally, the Clarion architecture is a hybrid cognitive architecture with both connectionist and symbolic representations, that combines implicit and explicit psychological processes, and integrates cognition (in the narrow sense) and other psychological processes. Overall, Clarion is a modularly structured cognitive architecture consisting of a number of distinct subsystems, with a dual representational structure in each subsystem [52].</p>
<p>In line with these previous studies, our current work evolves from a previous effort [62] where a cognitive approach was developed to integrate contextual reasoning and goal recognition with automated planning and execution in order to realize personalized continuous and proactive assistance in daily living scenarios. While the cognitive system realized in this previous work can be classified as a symbolic architecture, the current work moves towards a hybrid architecture integrating learning and runtime adaptation capabilities and takes inspiration from the dual process theory.</p>
<p>Empathetic Interaction between Humans and Robots</p>
<p>A further aspect addressed in current research concerns the management of emotions. Indeed, emotions are included among the crucial components in interaction and, as a consequence, they represent a key aspect also for socially interactive robots [23]. Social robots can be used to encourage emotional expression in situations where such expression may be challenging. Robots are, for instance, used to encourage children with autism to open up emotionally [21] and empathy is considered as relevant in providing assistance to older people [19,55]. With older adults, emotion constitutes a highly relevant human factor to consider for improving user engagement while interacting with assistive robots. On one hand, psychologists showed that empathy plays a key role for therapeutic improvement (see, e.g., [48]): patients who have received empathy from their therapist recovered faster and the same seems to hold with assistive robots. Robots can be designed to show empathy to improve users satisfaction and motivation to get better as well as enhance adherence to therapy programs in the context of patient-therapist interaction [55]. The work in [65] presents a prototype for an emoting robot that can detect emotions in one modality, specifically in the content of speech, and then express them in another modality, specifically through gestures. The robot is able to detect and express emotions through an emoting system. Results from a human validation study shows people perceiving the robot gestures as expressing the emotions in the speech content. Also, people's perceptions of the accuracy of emotion expression is significantly effective. In [15], an affective reasoning system has been implemented in the NAO robot for simulating empathic behaviors in the context of AAL. In particular, the robot recognizes the emotion of the user by analyzing communicative signals extracted from speech and facial expressions. The recognized emotion allows triggering the robot's affective state and, consequently, the most appropriate empathic behavior. Such behaviors have been evaluated both by experts in communication and through a user study aimed at assessing the perception and interpretation of empathy by elderly users. The above work considers emotions, empathy, feelings and affections as relevant features in human-to-human interactions.</p>
<p>In line with this research, the objective of this work is to design social robots capable also of detecting and expressing emotions and showing empathy so as to deploy empathetic interaction dialogue and foster engaging communication.</p>
<p>MIRIAM: a Mind-Inspired Architecture for Adaptive HRI</p>
<p>The technological contribution of this work is inspired by the psychological theory of dual process. From its initial formulation by William James [32], several proposals and interpretations have been made through years. Peter Wason and Jonathan Evans suggested a dual process theory in 1974 [69] by identifying two distinct types of processes: heuristic processes and analytic processes. Richard E. Petty and John Cacioppo proposed an application focused in the field of social psychology in 1986 [46], known as the elaboration likelihood model of persuasion, and more recently, Daniel Kahneman provided further interpretation by differentiating the two styles of processing more, calling them intuition and reasoning in [34]. Similarly, Strack and Deutsch [50] identified two separate systems: the reflective system and the impulsive system. Despite minor differences due to its evo-lution, there is a common agreement that the dual process theory accounts on two systems for explaining how thoughts arise from different processes which consist of an implicit, automatic, unconscious process (System 1) and an explicit, controlled, conscious one (System 2). Without pretending to match cortical structures of the brain into software modules, we propose Miriam as a novel AI-based control architecture that bring to different (but intertwined) thinking styles: the System 1, implicit and automatic, is mirrored into a faster and reactive layer; the System 2, explicit and controlled, is mirrored into a slower and deliberative layer.</p>
<p>The MIRIAM Architecture</p>
<p>AI technologies like Automated Planning (AP), Knowledge Representation &amp; Reasoning (KR&amp;R) and Reinforcement Learning (RL) support reasoning capabilities that, to some extent, replicate cognitive functions typical of System 1 and System 2. These technologies, taken alone, would not support higher levels of behavior adaptation and flexible control. A proper integration of these technologies would be crucial to enhance the efficacy of robots in scenarios that entail a tight and continuous interaction with humans.</p>
<p>AP [31,47] is well suited to synthesize a complex set of actions supporting the desired assistive objectives but, it lacks of the flexibility and adaptability needed to naturally interact with humans. KR&amp;R [28,37,59] is well suited to represent the domain features of assistive scenarios and support contextualized reasoning but, it lacks of a "runtime perspective" and is affected by the same limitations of AP. General rule-based AI systems tackle problems with a high level of abstraction, generating solutions that maintain a general and long-term view of a problem. However, they struggle in generating solutions that are flexible enough to effectively face unpredictable and unforeseen changes that are typical of human behaviors. On a different side RL [53] has the reactivity levels and the adaptation capabilities desired to deal with human dynamics. It is indeed quite effective in environments that change dynamically and unpredictably. It incrementally builds/refines an internal model (or policy) from observations through rewards in order to dynamically adapt choices and executed actions to different situations. One of the limitations of RL however is a lack of a "long-term perspective" as well as the "semantics" and the "explicit structures" needed to properly explain behaviors and failures [17].</p>
<p>Given these premises, it is not hard to map RL to Systems 1 and, AP and KR&amp;R to Systems 2. Figure 1 shows the resulting structure of Miriam which consists of two layers, each associated to a specific cognitive process (i.e., thinking style). The architecture aims at combining the benefits of the mentioned AI technologies and thus achieve abstract and goal-oriented reasoning while maintaining high reactivity and adaptation. Focusing on dialogue-based interactions with humans, Miriam would dynamically deal with contingent behaviors perceived through robot sensors while producing personalized behaviors through its adapters like e.g., sentences pronounced in natural language or appropriate facial expressions. The dynamic modelling capabilities of the system would for example adapt the communication modality of the robot (e.g., verbal and non verbal) to known personality insights as well as changing mood of a user. As it will be better described later in Sect. 6, taking inspiration from [56], the extroversion-introversion trait of the Big Five Personality model is taken into account for modulating the robot's behavior. The robot will thus use reassuring words and eyes expressions in case of introverted users while, it will use challenging words and eyes expressions in case of extroverted users. The combination of the two reasoning layers thus allows a robot to achieve (long term) objectives by carrying out (abstract and detailed) plans in a personalized and adaptive way, based on to the features of users.</p>
<p>From a technical perspective we design the interactions between System 1 and System 2 layers by "revisiting" Hierarchical Reinforcement Learning (HRL) [54] where a upper-level policy (often referred as the gating policy) learns to select and communicate motivations (or options) to a lower-level policy (often referred as option policy) which learns concrete behavior/control patterns. Unlike HRL, however, we replace the gating policy with AP and KR&amp;R technologies in order to carry out complex reasoning tasks, while maintaining the reactivity characteristics of the option policy. Following this nomenclature we refer to higher level actions planned (and executed) by System 2 as motivations and refer to lower level physical actions (or interactions) performed by System 1 as actions. Sects. 4 and 5 show with more details how the two layers of Miriam are internally structured and how AI technologies support the related reasoning and interacting capabilities. Although the paper specifically focus on dialogue-based interactions between a human and a robot, it is important to point out that Miriam can support different types of interactions. The layer of System 1 can indeed be easily extended with additional "reactive modules" supporting different interaction skills of a robot (e.g., object manipulation skills or navigation skills). The reason-ing capabilities of System 2 would seamlessly interact (and coordinate) the reacting modules composing the System 1 maintaining the same level of abstraction.</p>
<p>Semantic Reasoning and Planning</p>
<p>The reasoning capabilities realized at System 2 level rely on the tight integration of KR&amp;R and AP. This layer specifically integrates the ontology-based and contextual reasoning capabilities of KOaLa (Knowledge-based cOntinuous Loop) which proposes a holistic approach to robotic assistance [64].</p>
<p>KOaLa supports the definition of a context-based knowledge that endows a robot with a semantically rich representation of assistive scenarios. Contexts specify robot knowledge at different levels of abstraction and enable contextual reasoning to process raw sensory data and/or input health information about users. For example, as shown in [62], KOaLa integrates the Semantic Sensor Network (SSN) ontology [12] to interpret data gathered from environmental sensors (e.g., PIR or SWITCH sensors) and recognize daily activities of a user at home. Furthermore, KOaLa integrates an ontological representation of the International Classification of Functioning Disability and Health (ICF) proposed by the World Health Organization 1 in order to reason about the health conditions of users and contextualize assistance accordingly. Leveraging the foundational ontology DOLCE [24], elements of ICF are represented as functioning qualities (i.e., specializations of DOLCE:Quality) characterizing functioning levels of physical and/or cognitive aspects of a person.</p>
<p>In [62] it has been shown that the combination of contextbased KR&amp;R with AP was effective in synthesizing proactive assistance. Plans and assistive actions of a robot were indeed dynamically synthesized according to the goals automatically triggered from inferred daily-living situations. The current work further extends KOaLa by refining the underlying ontological model and the integration with planning.</p>
<p>Deploying KOaLa into MIRIAM</p>
<p>To support a wider range of assistive scenarios the ontological model has been extended to formally characterize the assistive capabilities of a robot. Robot skills are interpreted as physical or cognitive stimuli with respect to the health state of a user. The ICF classification defines a reference framework to formally characterize the effects that assistive actions (i.e., stimulus) have on the (ICF-based) health variables describing the state of a user. As shown in [63], this 1 https://www.who.int/classifications/icf/en/. allows the reasoning layer to contextualize assistive capabilities of a robot and dynamically identify the (sub)set of capabilities (and thus stimuli) that best fit the specific healthrelated needs of the assisted user.</p>
<p>Robot capabilities and related stimulation actions are defined according to the (positive) effects they have on the functioning qualities of a person. In the case of cognitive stimulation, for example, stimulation actions consist in administrating some known cognitive exercises to a user through some interaction modality. The capabilities of such actions (i.e., the effects they have on the health state of a person) depend on the capabilities of administrated exercises (i.e., the functioning qualities targeted by the exercises).</p>
<p>This ontological model combined with input user profiles constitute the internal knowledge of a robot at System 2 level. This knowledge, together with the reasoning modules that dynamically identifies suitable actions and synthesize personalized assistive behaviors, compose the System 2 layer of Miriam architecture as shown in Fig. 2.</p>
<p>A user profile instantiates the ICF-based ontological model and describes the health state of a specific person through a set of numerical scores describing the functioning levels of "measured" physical and/or cognitive qualities. Such scores are defined according to ICF (i.e., 0 -no impairment; 1 -soft impairment; 2 -medium impairment; 3serious impairment; 4 -full impairment) and represent the level of impairment of a modeled functioning quality like memory functions (e.g., short-term memory) or attention functions (e.g., sustaining attention). A user profile is internally represented as a Knowledge Graph associating to each functioning quality of the ontological model one of the described ICF-scores. Namely, a profile consists of a number of measurements each associating an ICF-score to a specific functioning quality of the ontological model. In this way, a profile characterizes the cognitive and physical functioning of a user.</p>
<p>Knowledge reasoning mechanisms carry out contextual reasoning to infer physical and/or cognitive impairments of a user and accordingly contextualize robotic assistance. Such reasoning mechanisms are implemented through a custom rule-based reasoning engine developed using the open-source software library Apache Jena 2 . Equation 1 and Equation 2 show the general logical rules integrated into KOaLa to analyze health conditions of a user and contextualize robot capabilities. Equation 1 first analyzes a given user profile in order to recognize the (sub)set of functioning qualities that actually represent impairments. Impairments characterize the health conditions of a user and determine the kind of assistance a user needs. Eq. 2 then contextualizes impairments with respect to known capabilities of a robot. Robot capabilities determine the supported stimulation functions each of which has effects on a sub-set of functioning qualities of a user. The rule specifically infers a set of stimulation opportunities by taking into account the stimulation functions that have effects on the impairments of a user. As shown in [63], this rule relies on a formalized concept of affordances that characterize opportunities of performing stimulation (and thus assistive) actions.
∀ x,y,w,z. ∃ i. (Measurement(x) ∧ measures(x,y) ∧ hasConstituent(x,w) ∧ FunctioningQuality(y) ∧ Person(w) ∧ hasOutcome(x,z) ∧ hasICFScore(z) &gt; 0 ∧ hasICFScore(z) &lt; 4 → Impairment(i) ∧ concerns(i,w) ∧ concerns(i,y) ∧ satisfies(i,x)) (1) ∀ x,y,w,z. ∃ o. (Impairment(x) ∧ FuncQuality(y) ∧ concenrs(x,y) ∧ StimFunction(w) ∧ hasEffectOn(w,y) ∧ isPartOf(w,z) ∧ Stimulation(z) → StimOpportunity(o) ∧ classifies(o,x) ∧ isRelatedTo(o,y) ∧ isRelatedTo(o,w) ∧ canAfford(z,x))(2)</p>
<p>Personalizing and "Motivating" Assistance</p>
<p>The reasoning mechanism implemented through KOaLa allows a robot to "understand" health conditions of a user and autonomously recognize stimulation actions that fit her needs. Contextual knowledge is given to the behavior synthesis component which synthesizes and executes personalized assistive plans addressing the specific needs of a user. As shown in Fig. 2 indeed users with different health needs (i.e., different impairments) lead to the synthesis of different stimulation plans and, for example, to the administration of different cognitive exercises.</p>
<p>Algorithm 1 Logical implementation of the System 2 layer
I ← impairments (K B U ) Equation 1 O ← stimulationOpportunities (I, K B R ) Equation 2 R ← matchMaking (O, I) Ranking stimulation actions if R = ∅ then M U ← behavioralModel (R, K B R ) Configure the planning module G U ← assistiveObjectives (R, K B R ) Configure planning goals π U ← planAssistiveBehavior (M U , G U ) Personalized assistive plan C ← clockInitialization () while ¬ Failure (π U ) do T ← dispatchableTokens (π U , C) Assuming |T | ≥ 1 if dispatch (T ) == failure then Dispatch motivations to System 1 π U ← reschedule (π U , T , C) Reschedule motivations for later execution end if F ← waitFeedback (T ) if hasFailure (F) then Execution feedback of dispatched motivations π U ← repair (π U , F , C) Repair a plan in case of failures end if C ← updateClock (C) end while if Failure (π U ) then
return Error Plan execution failure end if end if return Success Algorithm 1 shows the functional steps that support the reasoning capabilities of System 2 and guide assistance through System 1. The set of impairments I and stimulation opportunities O inferred through Eq. 1 and Eq. 2 characterize the contextual knowledge of the robot. This knowledge is further elaborated through a match making mechanism aims at identifying the most suitable actions a robot can perform to support a user. A recommended system engine generates a ranking R of stimulation actions by correlating impairments and stimulation opportunities. In the case of cognitive stimulation for example, the ranking R characterizes the set of exercise administration actions that address the most impaired functioning qualities of a user. These actions are supposed to be the most effective in addressing the specific health conditions of a user (personalization). The obtained ranking R is used to dynamically configure the AP module by defining a personalized planning model M U and a personalized set of assistive objectives G U (i.e., planning goals). This allows the reasoning layer to synthesize a personalized assistive plan π U that takes into account the specific healthrelated needs of a user and the sub-set of suitable assistive capabilities of a robot.</p>
<p>The implemented planning and execution capabilities rely on the timeline-based formalism [11] and the open-source PLATINUm framework 3 [60]. Assistive plans are thus represented in terms of timelines [11] that describe the sequences of (high-level) assistive actions and/or stimuli, called tokens, a robot should performs over time. Planned assistive actions represent motivation signals that are interpreted by System 1 to (physically) interact with a user. As shown in Algorithm 1, the execution of a (personalized) plan π U at System 2 level consists in "dispatching" a number of planned tokens T to the reactive layer of Miriam. Such tokens thus represent the motivation signals that guide the interactions carried out by System 1 to achieve high-level assistive objectives of plan π U .</p>
<p>In the case of dialogue-based interactions and cognitive stimulation for example, motivations (i.e., the tokens of the timeline-based plan π U ) represent the administration of cognitive exercises and specify the type of dialogue a robot should carry out. Every time a token of a timeline is about to start according to the current clock C and its schedule, a properly configured motivation is dispatched to System 1 for the administration of a particular cognitive exercise. System 1 sends back feedback about dispatched motivations. Three cases can be distinguished as shown in Algorithm 1: (i) if System 1 is not ready for the execution of the dispatched motivation (dispatching failure) (e.g., the user refuses to start a cognitive exercise) the related token is rescheduled for being executed later in the plan π U ; (ii) if System 1 is not able to correctly complete the execution of dispatched motivations (execution failure) (e.g., the quality of the dialogue was too low or the user made too many mistakes) then a plan repair is performed (e.g., additional exercise administration actions are planned to achieve the desired level of "daily stimulation"); (iii) if System 1 correctly executes motivations (positive feedback) (e.g., the administrated exercise has been successfully completed by a user) the execution of plan π U proceeds with the rest of planned tokens.</p>
<p>It is important to point out that, the kind of (slower) contextual reasoning performed at System 2 level is here used to personalize both the content of the assistance i.e., which actions are suitable to achieve the desired objectives, and the shape of the assistance i.e., how actions should be executed to be effective. To support such personalization the ICF-based ontological model characterizes interaction capabilities of a person in order to properly set the modality of the interaction used by the robot. These capabilities mainly concern hearing and seeing functioning qualities of a user in order to configure parameters like e.g., the sound level of auditory messages or the font size of text messages. Other relevant aspects concern the personality of the assisted person and the social context where the assistance takes place in order to properly set the 3 https://github.com/pstlab/PLATINUm.git. way a robot approaches a person, attracts her attention and the way the robot communicates with him/her. Dispatched tokens (i.e., motivations) are thus "tagged" with a number of interaction parameters that provide the reactive layer with contextual information about their execution [14]. Section 6 shows some examples describing the supported behaviors.</p>
<p>Reactive Reasoning</p>
<p>Unlike the System 2 described in the previous section, which by carrying out higher level forms of reasoning synthesizes a sequence of motivations to be executed over time, the role of our System 1, sketched in Fig. 3, consists in selecting actions based on a context that dynamically evolves over time. In other words, the objective of the reactive module consists in building a policy π (ct x) = a which, given a context ct x, returns an action a to be executed. The execution of such actions, in particular, physically translates, in our case, in the pronunciation of personalized sentences in natural language towards a user and, similarly to [10], in the contextualized change of facial expressions. Since, however, these actions internally constitute transitions in a state-transition system [16], we propose a new form of parametric actions, inspired by those used in classical planning [25], which allow to represent the state-transition system in an implicit and compact way. Before describing these actions, however, it is advisable to define the states of such a state-transition system which, by analogy with natural language generation systems, we will call context.</p>
<p>From a technical perspective, we refer by context to a set of variables, both symbolic and numeric, which characterize the current state of the system. These variables are used to keep track of all the information that, more or less dynamically, change over time. Specifically, context variables include all those factors which are relevant for the discrimination of the actions taken by the system such as, for example, those related to the user's personality and current mood together with those elements related to the interaction like, for example, information extracted from the user's speech analysis and the robot's facial expressions (refer to Table 3 for a description of some of the context variables used by the system). By dynamically updating the values of the context variables, the system will adaptively select actions with the aim of personalizing the interaction and obtaining dialogues and, more in general, behaviors, that are as fluid and engaging as possible.</p>
<p>Once introduced the context, on the basis of which the policy selects the actions, we can describe in more detail the actions. Each action, in particular, is characterized by three elements: (a) a logical combination (i.e., conjunctions and/or disjunctions) of conditions on the context variables for verifying the executability of the action; (b) a set of natural language sentences representing the system's responses for the users (if the set contains more than one sentence, one is chosen randomly); and (c) a set of effects on the context variables, representing the updates to apply on both symbolic and numeric context variables whenever the action is executed, as well as the possible feedback for the System 2, indicating the termination, either successful or unsuccessful, of the current motivation.</p>
<p>Similarly to classical planning, each action whose conditions are verified in the current context is said to be executable. As opposed to planning, however, whenever asked to the system, i.e., as a consequence of the interactions from a user or for the incoming of a motivation signal from the System 2 module, all executable actions are executed in the order they are defined by the domain author. The presence of such actions, indeed, is intended both to establish which responses to provide to the users and to make further transitions in the context space by means of the actions' effects updating, for example, some context variables about the current discussion topic.</p>
<p>The updating of the context variables, in particular, takes place as a consequence of the transmission and interpretation of high-level commands, which we called motivation signals, coming from the execution of the customized plans. These variables, however, are also modified as a consequence of the unpredictable interactions that the system has with the users. By analyzing the data coming from the sensors (in particular, the signal from a microphone), a speech-to-text module 4 recognizes text from users' speech. This text is subsequently analyzed through a Natural Language Understanding (NLU) module 5 which is trained to classify the users' intentions (e.g., affirmations, negations, answers to questions, etc.), called intents, and to annotate them with further data, called entities, to better characterize them (e.g., numbers or dates in answers). Whenever a user utters a particular sentence, in particular, this high-level information is extracted and used to modify the values of the current context variables, triggering the selection of the proper action which, in turn, results in the proper reaction by the system.</p>
<p>One aspect that is worth highlighting concerns the customization of the system according to the user's personality. In particular, any sentence pronounced by the user, once recognized through speech-to-text, is sent to the NLU module for the recognition of intents and entities and, at the same time, is stored. Through a personality insight tool, starting from the set of all the sentences uttered by the user, we extract information on the personality such as extroversion, neuroticism, etc. These parameters, once again, enrich the set of context variables, further personalizing the interaction with the user by implementing, for example, a more challenging type of communication for an extroverted person or a more accommodating type of communication for a more introverted person.</p>
<p>Algorithm 2 shows, in pseudo-code, the steps that are performed every time the robot is requested to interact with the user (i.e., when a motivation signal is received by the deliberative layer or when the user says something). In particular, for each action whose conditions are satisfied in the current context, a response is randomly selected and proposed to the user and, subsequently, the context variables are updated according to the effects of the action. Once all the actions have been processed and the context updated with the effects of the executable ones, the algorithm first checks the state of a particular context variable (i.e., eyes) indicating the eyes to show and, depending on its value, modifies the robot's facial expression to make the interaction more empathetic. Finally, it checks the status of a second context variable (i.e., command_state) indicating the status of the high-level command triggered by the deliberative layer and, if it takes on specific values (i.e., done or f ailure), it communicates at the deliberative level (respectively, with a success or a failure) the conclusion of the task.</p>
<p>It is worth specifying that among the parameters extracted from the speech-to-text module and from the NLU module, a confidence value indicates how confident the speech-to-text module is in recognizing a sentence from the audio signal and how confident the NLU module is in classifying the user's utterances. This same value is added to the different context variables, further adapting the interaction. In particular, if the confidence value falls below a certain threshold, the robot can ask the user to rephrase the last sentence differently or, in the event of repeated misunderstandings, the actions performed by the reactive module could bring the status of the current command to a failure and then to a feedback for the deliberative module. Finally, note that just as the eyes are managed, the reactive module can easily be extended to drive a richer multimodal interaction, controlling gestures and modulating prosody.</p>
<p>Applying MIRIAM</p>
<p>The target scenario foresees a training service where the robot is used for delivering a cognitive rehabilitation program. An assistive robot should generate a set of stimuli (or instructions) suited to the specific health needs of a user (i.e., the assisted person) and administrate these stimuli (e.g., cognitive exercises) through a dynamic, engaging and natural interaction.</p>
<p>From User Profiles to Personalized Plans</p>
<p>To assess the reasoning (and personalization) capabilities developed at System 2 level of Miriam, we have made a test on 8 (real) user profiles that are characterized by different cognitive states. These profiles were obtained after a clinical assessment made by a clinician using a standard cognitive evaluation tool, i.e., the Mini Mental State Examination (MMSE). Table 1 succinctly shows the obtained profiles that are internally represented as knowledge graphs.</p>
<p>For each profile the table shows the considered ICF variables and the associated numeric scores (0 -no impairments; 1soft impairment; 2 -medium impairment; 3 -serious impairment; 4 -full impairment).</p>
<p>Furthermore, the knowledge of the robot was filled with a total number of 10 cognitive exercises each addressing different (cognitive) aspects of a user. Each exercises was internally associated to a distinct stimulation action (i.e., exercise administration action) and thus modeled as a distinct cognitive capability of the robot (i.e., stimulus). Table  2 shows the considered exercises the associated stimulation functions and thus the list of stimulated variables of the ICFbased profile of a user.</p>
<p>Following the steps sketched in Algorithm 1 the reasoning layer analyzes the knowledge inferred from the information contained in Table 1 and Table 2 to first contextualize stimulation capabilities and then synthesize (and execute) personalized plans. Rankings are computed by multiplying vectors of ICF scores assigned to each profile (i.e., rows of Table 1) with a "incidence matrix" extracted from stimulation capabilities of cognitive exercises. For each exercise (i.e., simulus) the matrix would contain a positive value (i.e., 1 in the current implementation) if a particular ICF variable is supported/stimulated or 0 otherwise. This reasoning process implements a knowledge-aware recommender system [2,29] leveraging the underlying (ICF-based) ontological model to contextualize health-related needs of patients with stimulation capabilities of known stimuli (i.e., assistive affordances [63]). A qualitative view of outcome of the ranking procedure aiming at characterizing the relevance of the modeled stimulation capabilities with respect to the health (cognitive) conditions of different users can be seen in Fig. 4.</p>
<p>At a first glance it can be easily seen how the reasoning layer is actually able to differentiate the intervention for the considered profiles. Different ranking values (i.e., relevance) were computed according to the health state of each user (Table 1) and to the specific stimulation capabilities of each exercise ( Table 2). For example, low ranking values were computed for users with no significant impairments like e.g., Profile 3 and Profile 5. Conversely, several exercises were detected as relevant i.e., "high 6 ranking value" for users with more serious conditions. An example is Profile 8 which is characterized by medium and serious impairments (respectively {ATT, PER} and {MEM, WRT}). In this case different exercises achieve a not negligible ranking value. However, the exercises that are considered as relevant are Exercise 2, Exercise 6, Exercise 8 and Exercise 9 whose ranking value Table 1 User profiles and  associated ICF variables  ID  ORI  ATT  MEM  PER  HLC  MFL  CAL  COM  SPK  WRT   1  2  4  3  2  2  0  4  1  0  0   2  2  3  3  3  3  3  3  2  2  4   3  1  0  0  3  0  1  0  0  0  4   4  3  2  3  4  3  1 Fig. 5 showing rankings obtained for each user profile and clearly pointing out the correlations with the modeled stimuli of Table 2.</p>
<p>The obtained results seem coherent with respect to the health conditions described by the considered user profiles. The contextual knowledge has been thus used to configure the timeline-based planning and execution model used to actually synthesize and execute such (high-level) personalized stimulation plans. In this specific case, the planning model has been dynamically synthesized by taking into account, for each profile, all and only the stimulation actions that were inferred as relevant. In this way, synthesized (timeline-based) plans contains only stimulation actions that are suitable for the specific health state of the assisted person. System 2 thus will dispatch to System 1 motivations requesting the administration of cognitive exercises that actually stimulate impaired (cognitive) qualities of a user.</p>
<p>Listing 1 below shows an excerpt of a timeline-based plan π U synthesized to administrate a number of cognitive exercises (8) for a particular patient. Three timelines compose the plan: (i) a Stimulation timeline describing the high-level assistive goals achieved by the plan; (ii) a Patient timeline describing known interaction preferences over time and; (iii) a Robot timeline describing the planned (and scheduled) stimulation actions of the robot (i.e., motivations dispatched to System 1 architectural level). It is worth noticing that the  Table 2 Patient timeline encapsulates preferences extracted from a user profile determining the way the robot interacts with her/him to administrate cognitive exercises. Namely, this timeline describes when a patient is expected to be available to receive stimulation (i.e., tokens with predicate Available) and how (the parameters of the predicate Available characterize interaction preferences of a patient like e.g., volume, need for subtitles or need for explanations). Tokens of Robot timeline are planned according to these preferences. Tokens with predicate DoStimulationAction are indeed scheduled during expected availability windows of the patient (i.e., tokens with predicate Available) and "grounded" according to associated interaction parameters. Temporal constraints of the plan are represented as relations between (flexible) temporal intervals of the tokens of the plan [1]. The Listing 1 shows just few of them (i.e., some during relations constraining robot stimulation tokens to be scheduled within the same temporal interval of patient availability tokens). </p>
<p>33</p>
<p>" predicate " : " None () " ,</p>
<p>34</p>
<p>" end " : [ 5 , 5 ] , " duration " : [ 5 , 5 ] },</p>
<p>35</p>
<p>{ " id " : 9 ,</p>
<p>36</p>
<p>" predicate " : " Available ( no , high , regular , large ) " ,</p>
<p>37</p>
<p>" end " : [ 20,20] , " duration " : [ 15,15] } ,</p>
<p>38</p>
<p>{ " id " : 10, { " id " : 11,</p>
<p>42</p>
<p>" predicate " : " Available ( no , high , regular , large ) " ,</p>
<p>43</p>
<p>" end " : [ 40,40] , " duration " : [ 15,15] } , </p>
<p>95</p>
<p>" predicate " : " DoStimulationAction ( ex 1 , no , high , regular , large ) " ,</p>
<p>96</p>
<p>" end " : [ 36,40] , " duration " : [ 3 , 7 ] },</p>
<p>97</p>
<p>{ " id " : 29,</p>
<p>98</p>
<p>" predicate " : " Idle () " ,</p>
<p>99</p>
<p>" end " : [ 45,77] , " duration " : [ 4 , 41] }, 100 { " id " : 30,</p>
<p>101</p>
<p>" predicate " : " DoStimulationAction ( ex 3 , yes , high , slow , large ) " ,</p>
<p>102</p>
<p>" end " : [ 48,80] , " duration " : [ 3 , 7 ] </p>
<p>106</p>
<p>{ " reference -token " : 24, " relation -type " : " during " , " target -token " : 11 } ,</p>
<p>107</p>
<p>{ " reference -token " : 26, " relation -type " : " during " , " target -token " : 11 } ,</p>
<p>108</p>
<p>{ " reference -token " : 28, " relation -type " : " during " , " target -token " : 11 } ,</p>
<p>109</p>
<p>...</p>
<p>110</p>
<p>]} 111 } Figure 6 shows then a (simplified) Gantt representation of the executed plan. In this case tokens are not flexible but rather scheduled at specific (non-flexible) temporal intervals. Namely each token has start and end time points rather than intervals. The actual duration of each token is determined according to the feedback received from System 1 after the actual execution of dispatched motivations. Furthermore, the Gantt shows the PatientLoad discrete resource [61] (not showed in Listing 1) that was used to limit the maximum number of exercises administrated during a single interaction window (i.e., a particular availability token).</p>
<p>Adaptive Dialogue Execution and Interaction Assessment</p>
<p>Figures 7 shows the social robot on which Miriam has been developed. The robot is a Sanbot Elf, distributed in Italy by Omitech 7 , and the figures show a preliminary laboratory deployment for testing and feasibility assessment. From the hardware perspective, the robot has a tablet, which can be used to interact with people, as well as "eyes" (see Fig. 1 and Fig. 7) that can be controlled to represent Miriam's facial expressions (e.g., happiness, amazement, sadness, etc.). The robot has also actuators to navigate around a room and moves its head and arms. As for the sensors, the robot has two microphones used in Miriam to recognize the speech of users and several contact sensors along the whole body, several proximity sensors at the base of the robot and two cameras (one of which is a depth camera). The software architecture of the Sanbot Elf relies on a core ROS layer supporting a direct interface to the sensors/actuators that compose the robotic platform. On top of this layer Android API provides developers with a modular and programmable interface to support the integration (and interaction) of complex Javabased application and services. Therefore, the main modules developed to support the reasoning functionalities of Miriam are developed in Java which guarantees modularity, platform independence and flexible deployment on other robots 8 . As a feasibility assessment, let us suppose that the assistive robot is capable of administrating a set of cognitive exercises E = {e 0 , e 1 , e 2 , e 3 , e 4 , e 5 , e 6 , e 7 } and that a subset E u = {e 3 , e 4 , e 5 } of these exercises has been inferred as relevant for a user u by the reasoning layer, according to the specific cognitive needs identified. Additional parameters managed by the reasoner layer to support the user personalservices like e.g., ROSBridge -http://wiki.ros.org/rosbridge or as fully integrated ROS packages through ROSJava -http://wiki.ros.org/ rosjava. ization are for example, the preferred sound level of robot audio messages, the preferred speech-rate, the preferred font size of robot text messages, and possible information about social norms. Once the reasoning layer has planned the proper cognitive exercise for the target user, the reactive module is in charge of executing the plan by initiating the interaction and properly react to the dynamism of the occurring dialogue.</p>
<p>Figures 8 and Fig. 9 exemplify the reactive behavior of Miriam while administering the Find the word exercise. This exercise requires the user to count the occurrences of a target word while the robot lists some predefined words, varying the difficulty according to the complexity of the used words: easy words, complex words and non-words. Beside the information provided by the System 2 which allows to tailor the interaction since its very beginning, it is showed the capability of the robot to dynamically adapt during the interaction according to contingent information and react in a multimodal manner: beside verbal channel, the robot communicate also through a visual one, by displaying gaze according to the specific message it is supposed to convey.</p>
<p>The exercise, in particular, is initiated by the System 2 through the execution of the personalized rehabilitation plan and, more in detail, by sending a motivation signal to the System 1. This signal translates into the assignment of some of the context variables shown in Table 3 which denote, for example, the moving to node n 0 of the exercise, as shown in   Fig. 8, and that, given the person's auditory characteristics, the exercise must be reproduced at a normal volume. At the same time, the robot initiates the dialogue by providing the user with the instructions for the cognitive exercise. At this stage, the answer from the user (let us suppose that it is a sentence like "It's fine!") is recorded through a microphone and translated into text through the speech-totext service. The recognized text is subsequently analyzed by the NLU service which extracts the intent #ok updating the "intent" context variable. An action, whose execution condition is being in node n 0 and the "intent" context variable assuming the #ok value, is selected and, as a consequence of its execution, the robot starts listing the words and ask for the users to answer (in the responses of the action) and the transition to n 1 node (in the effects of the action). Finally, if the users provides the wrong answer, then the robot selects the proper action, increasing by one a counter of the errors made by the user, displaying sad eyes to the user and inviting him to retry the exercise (n 2 ). On the contrary, if the answer is correct, the robot selects happy eyes and congrats with the user (n 3 ).</p>
<p>Note that the nodes represented in these figures are only used for explanatory reasons, not explicitly representing states of a state-transition system which, on the contrary, are identified by the combination of the values assumed by the different context variables. The introduction of actions, indeed, as described in Sect. 5, allows an implicit definition of the state-transition system, facilitating the definition of the domain and allowing complex behaviors that would be difficult to model explicitly such as, for example, users asking questions out of the scope of the exercise while they are conducting it, without losing the focus on the exercise. Additionally, it is worth underscoring that the planner could not predict the user's response and, therefore, could not include it in the initial plan. In the absence of the reactive module, in particular, the reaction to the user's response would have required a potentially expensive adaptation of the plan.</p>
<p>Let suppose that the dialogue is going on and the robot keeps interacting with the user and dynamically acquiring variables from the context. Among these variables, shown in Table 3, also Personality is considered and inferred by the Insight tool. The system memorizes and analyzes the different interactions with the user over time and, by means of personality insight tools can, for example, collect information on the user model through aspects such as the Big Five personality traits (i.e., openness to experience, conscientiousness, extraversion, agreeableness and neuroticism), and on the current mood, such as a form of sentiment analysis. This information, in particular, enriches the context by updating the corresponding variables and hence further fosters the system's personalization.</p>
<p>Indeed, Fig. 9 shows that the system, while interacting with the user, starts to further personalize its interaction. Different ways to react by the users brings the robot to interact in different manner according, for example, to different levels of extraversion. In particular, the system can respond more reassuringly both verbally and through eyes, in case of an introverted person (n 8 and n 9 ), or in a more challenging way, both verbally and through eyes as well, in case of a more extroverted person (n 10 and n 11 ).</p>
<p>It is worth noticing that during all this, the System 2 has been waiting for information from the System 1, dynamically adapting, from a temporal perspective, the current rehabilitation plan. In the event that the System 1 detects, for example, a too high number of errors made by the user, it could communicate a "failure" feedback to KOaLa which, through a re-planning process, reconstructs a more accurate cognitive rehabilitation plan for the specific user. Similarly, if the cognitive exercise is successful, the System 1 sends a "success" feedback to KOaLa which proceeds with the execution of the current rehabilitation plan.</p>
<p>User Interaction Assessment. The example introduced in Fig.  8 was used for a preliminary evaluation of the interaction with possible real users. For this reason, the in-laboratory tests performed involved healthy people rather than users of Table  1 considered for the off-line assessment of personalization capabilities. In order to assess the usability of the system we mainly focused on the fluidity of implemented dialogues. A dialogue was considered fluid when few agent's errors occurred and turn-taking was fairly balanced between human and artificial agent. Errors were considered when the system failed to understand the human utterance and consequently either it gave a wrong answer, or it did not answer at all. These errors often caused the human to repeat the utterance and consequently affected the length of the interaction.</p>
<p>Procedure and metrics.</p>
<p>Participants came to the lab and a researcher explained the experimental procedure. It was explained that some questionnaires will need to be filled in and they would have to verbally interact with a robot capable to administrate cognitive exercises. More in detail, the task consisted of participating in the "Find the word" exercise. It consisted in two sessions: a first set of simple words, and a second set of more difficult words. After the execution of the experimental task, participants were asked to fill in the Chatbot Usability Questionnaire (CUQ, [30]) for assessing the usability of the system. Additionally, a socio-demographic questionnaire served for collecting data about gender, age, familiarity with speech based systems and opinions about new technologies. Other metrics were collected during the experimental session to assess the dialogue performance in terms of dialogue efficiency: length of the interaction, number of turns (agent's, user's and total number); and dialogue quality: errors made by the agent [67,68]. Since at least 30 turns (fairly balanced between user and agent) would have been necessary to complete the interaction, we considered this as the minimum amount in order to judge the task complete.</p>
<p>Participants. Ten persons participated in the testing phase. More specifically, 5 women and 5 men were involved. The mean age of the whole sample was 49.5 years old (SD=  </p>
<p>Results</p>
<p>The analysis of the performance metrics showed that the interaction with each users lasted on average 6.58 minutes (SD=1.5), with longer duration of 9.49 minutes for participant U05, and the shorter one of 4.38 minutes for participant U10 (see Fig. 10 for details). During the interaction the robot delivered the cognitive exercise. The overall interaction consisted of an average total amount of 37.3 (SD= 6.32) turns, where the robot talked for a mean of 17.5 turns (SD= 3.1), while the user for 19.8 turns (SD= 3.45). In Fig. 11, the detailed information for each participant is reported. The slightly higher number of turns by users was mostly due to the repetition of a same request by the participants to the robot when this last one failed to answer. Nevertheless, there was a quite low amount of errors by the robot, that gave the wrong answer to a user's question just a few times (M= 1.31, SD= 0.63), as it can be seen in Table 4 that shows how the system performed quite robustly with a low number of errors, although only in one occasion it interacted with no errors at all (User U01). The CUQ questionnaire allowed assessing the interaction usability: the results showed positive opinions regarding the interaction with the robot with a mean score of 72.81 (SD= 8.59) out of 100 (see Fig. 12 for individual scores). Overall it can be said that the usability was satisfactory and the interaction quite fluid.  </p>
<p>Conclusions and Future Works</p>
<p>This article describes the research effort towards the realization of an architecture inspired by the theory of dual processes dedicated to support an intelligent and adaptive behavior of social robots that have to act with different people in dynamic environments. Based on a previous effort, the work focuses on the integration of two modules inspired by System 1 and System 2 of the aforementioned theory which constitute the basic modules of a mind-inspired architecture. A first module provides a long-term assistive plan, while a reactive module deals with plan execution dynamically adapting it to the contingencies of the interaction. More specifically it customizes the actions towards the user through multi-modal channels (e.g., voice, eye expression) allowing the adaptation to different affective-cognitive states.</p>
<p>The architecture has been exemplified in the domain of cognitive rehabilitation but both the high level plan and the adaptive actions of the robot's behavior during execution can be generalized to other domains. An example is the domain of Human-Robot Collaboration where we have made some contribution by integrating a (slow thinking) deliberative task planning module with a (fast thinking) reactive motion planning module [18,45]. On the one hand the task planning module is in charge of deciding the shape of collaborative plans by taking into account the capabilities of the human operator and the collaborative-robot. On the other the motion planning is in charge of "implementing" robot tasks (decided by the task planner) by dynamically adapting robot trajectories according to the observed behavior of the human operator. The work [18] in particular is an example of this kind of application. Although the work is not connected with Miriam, the presented layered architecture is coherent with the principles of the dual process theory and could be completely supported through Miriam.</p>
<p>Currently the two systems are implemented and integrated. An initial evaluation of the personalization capabilities of the deliberative layer of Miriam has been performed by taking into account real user profiles defined off-line. Furthermore, we performed laboratory experiments with healthy users on aspects related to interaction. Results show that a good level of usability of the system and a satisfactory fluidity in the dialogue. This experiment has a clear limitations that we plan to work on in the future. In fact a more robust evaluation will be designed to assess the effectiveness of the system in cognitive training with frail users with a higher number of participants as well as other aspects of the interaction.</p>
<p>Data availability All data generated or analysed during this study are included in this published article (and its supplementary information files).</p>
<p>Declarations</p>
<p>Conflict of interest</p>
<p>The authors declare that they have no conflict of interest.</p>
<p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/. Benedictis [M.Sc. in Computer Engineering in 2010 at University of Rome 'La Sapienza'; Ph.D. in Computing, Electronics and Mathematics in 2019 at University of Plymouth] is researcher in Artificial Intelligence at the Institute of Cognitive Sciences and Technologies of CNR. His research topics concern the development of automated reasoning solvers aimed at addressing real world applications, while keeping a strong consideration for the human component that must interact with the intelligent applications. Special attention is given to the efficiency of the reasoning processes, obtained through the development of domainindependent heuristics which, in an integrated way, allow solving semantic inference problems, planning and scheduling problems, as well as their execution and adaptation in dynamic environments. From September to December 2015 Riccardo has been part of the Artificial Intelligence and Machine Learning group of the Pompeu Fabra University of Barcelona under the supervision of Hector Geffner. Riccardo has worked since 2010 on several projects under the FP7, H2020 and AAL research and innovation framework programs. He is currently studying the integration of different AI techniques for executing plans in nondeterministic environments. Anna. She has expertise on the analysis of psychological individual differences by using physiological indexes (EEG, ERPs, ECG and EDA). She studied social influence by investigating cortical responses and behavioral modulation to external feedback by comparing different sources, humans and artificial agents. Indeed, her research activities span over psychophysiological evaluation, qualitative and quantitative methods for user evaluation, long-term assessment of user interaction with innovative technologies as social agents (like robots and / or intelligent systems). She is currently focused on studying usability, acceptance, users satisfaction as well as the impact of user experience on systems' design by mixing different type of methods from self-report measures and observational methods, to physiological indexes.</p>
<p>Riccardo De</p>
<p>Amedeo Cesta [M.Sc. Electronic Engineering, 1983, Ph.D. in Computer Science, 1992: University of Rome 'La Sapienza'] is a Research Director in Artificial Intelligence (AI) at CNR and Group Leader at ISTC. He has been working in AI for over thirty years and has founded and still coordinates activities of the Laboratory on Planning and Scheduling Technologies (PST Lab). He has conducted research in Multi-Agent Systems, Intelligent Human-Computer Interaction, Planning &amp; Scheduling and always pursued the synthesis of innovative Decision Support Systems. His work in AI emphasizes the real-world aspects of automated reasoning, in particular focusing in research areas like planning for space domain, technology-based support to older people, integration of AI techniques in Robotics. Since the early 2000s, he has been exploring the integration of AI techniques in new application areas (for example the crisis managers training, the Factory of the Future, etc.) and the synthesis of new cognitive aids for older adults by merging ICTs and robotics. Her research spans mixed-initiative problem solving, methods for evaluating Artificial Intelligence-based systems, evaluation methods in Human Computer Interaction and Human Robot Interaction, Active Assisted Living (AAL), Social Robots to support frail users. She has worked in the AAL domain with different research projects: in RoboCare where she worked at the human-robot interaction assessment; in ExCITE where she developed a long-term evaluation protocol for the telepresence robot Giraff, specifically to study its long-term impact on the quality of life of older adults. She was Technical Manager for GiraffPlus, which developed an innovative environment integrating a telepresence robot for social interaction with sensors for physical and psychological health monitoring. She worked at SPONSOR and MAESTRO, two projects of the AAL program. She was PI for CNR in TV-AssistDEM, that developed a TV-based application to monitor and cognitively stimulate people with Mild Cognitive Impairment. She is currently Technical Manager in SmartSatCare, a project funded by the European Space Agency under the program "Space in response to Covid-19 outbreak". She was among the promoters of several series of workshops in the area of Artificial Intelligence; AAL and Human Robot Interaction. She is currently in the board of directors of AIxIA the Italian Association of Artificial Intelligence with the role of Secretary.</p>
<p>Fig. 1
1A sketch of the architecture</p>
<p>Fig. 2
2Internal components of System 2 level</p>
<p>Fig. 3
3Internal</p>
<p>ORI
Orientation; ATT Attention; MEM Memory; PER Perceptual; HLC Higher Level Cognitive Functioning; MFL Mental Functioning of Language; CAL Calculation; COM Communication; SPK Speaking; WRT Writing</p>
<p>Fig. 5
5Detailed view of rankings with respect to the stimuli of</p>
<p>"
predicate " : " Idle () " , 87 " end " :[ 29,33] , " duration " :[ 1 , 5 ]  }, 88 { " id " : 26, 89 " predicate " : " DoStimulationAction ( ex 4 , no , high , regular , large ) " , 90 " end " : [ 32, 36] , " duration " : [ 3 , 7 ] }, 91 { " id " : 27, { " id " : 28,</p>
<p>Fig. 6 Fig. 7
67Gantt representation of an executed timeline-based plan Deployment of Miriam on a Sanbot Elf</p>
<p>Fig. 8 A personalized dialogue sketch showing adaptation to different users' responses</p>
<p>Fig. 10
10Interaction length measured in minutes for each participant</p>
<p>Fig. 11
11Number of users turns and robot turns per each participant</p>
<p>Fig. 12
12Scores obtained at the CUQ by each participant</p>
<p>Francesca
Fracasso received the B.S. and M.S. degrees in Psychology respectively from the University of Padua and 'La Sapienza' University of Rome, in 2011 and the Ph.D. degree in Psychology and Cognitive Science from La Sapienza University of Rome, in 2015. She is a research scientist at the Institute of Cognitive Science and Technologies of National Research Council of Italy (ISTC-CNR) since July 2016. She started her collaboration with CNR in 2010 being involved in several national and international research projects for AAL and ICT for Active Ageing. Since November 2015 to May 2016 she has been visiting scholar at the Assistive Robotics Lab of the BioRobotics Institute -Scuola Superiore S.</p>
<p>Andrea Orlandini is a Researcher working at the National Research Council of Italy, Institute of Cognitive Sciences and Technologies (ISTC-CNR), in Rome. He got a degree in Computer Science Engineering and he received his PhD in Computer Science and Automation in 2006 defending the thesis "Logical Based Approaches to Artificial Intelligence Planning and Robot Control"at the Roma Tre University in Rome (Italy). He was a post doc at Laboratoire d'analyse et d'architectures des systèmes, a research laboratory linked with the French National Centre for Scientific Research (LAAS-CNRS). He is currently working in the Planning and Scheduling Technology Lab at ISTC-CNR, investigating safe and robust AI and Robotics solutions. His main research interests span over automated planning, task and motion planning, dependable plan execution and model-based robot control in collaborative robots and assistive technologies. Gabriella Cortellessa [M.S. Engineering Computer Science 2001, PhD Cognitive Psychology 2005] is a research scientist at ISTC-CNR.</p>
<p>Algorithm 2 Trigger interactionfor a i ∈ actions do 
Actions execution 
if ct x a i .conditions then 
if a i .responses = ∅ then 
answer ← random (a i .responses) 
prompt (answer ) 
end if 
ct x ← a i .e f f ects 
end if 
end for 
if ct x.eyes == L OV E then 
Eyes management 
display (love) 
else if ct x.eyes == S AD then 
display (sad) 
else if ... then 
... 
end if 
if ct x.command_state == F AI LU R E then System 2 feedback 
management 
f eedback ( f ailure) 
else if ct x.command_state == DO N E then 
f eedback (done) 
end if </p>
<p>Table 2
2Fig. 4 Ranking of stimulation capabilities for the considered user profiles. The dotted line shows for each profile the computed average ranking values of the exercisesKnown cognitive 
exercises modeled as 
stimulation capabilities </p>
<p>ID 
Exercise Name 
Stimulated ICF Variables </p>
<p>1 
Denomination test 
MFL </p>
<p>2 
Find the word 
MEM, CAL </p>
<p>3 
Free and cued selective reminding test 
MEM </p>
<p>4 
Stroop test 
ORI </p>
<p>5 
A n i m a l t e s t 
H L C , M F L </p>
<p>6 
Backward digit span test 
MEM, CAL </p>
<p>7 
Reys's figure test 
MEM </p>
<p>8 
Trailing making test form B 
ATT, HLC </p>
<p>9 
Trailing making test form A 
ATT, HLC </p>
<p>10 
Boston naming test 40-items 
MFL </p>
<p>is higher than the average. Similar results can be seen for 
Profile 1, Profile 6 or Profile 2. A closer view to the results 
is available in </p>
<p>Table 3
3Some of the main 
context variables with their 
initial value and a brief 
description </p>
<p>Name 
Init value 
Description </p>
<p>intent 
none 
Used for representing the user's intents. Values are set by the NLU 
module </p>
<p>n 
none 
A numeric variable used for recognizing correct/incorrect answers. 
Values are set by the NLU module </p>
<p>sentiment 
0 
A numeric variable ranging from -1 to 1 representing the sentiment, 
from negative to positive, of the last utterance from the user. Values 
are set by the the NLU module </p>
<p>extraversion 
none 
A numeric variable ranging from 0 to 1 representing the extraversion 
of a user. Values are set by the personality insight module </p>
<p>confidence 
none 
A numeric variable ranging from 0 to 1 representing the confidence 
of the system in recognizing a user's utterance or the user's intent. 
Values are set by the speech-to-text module and by the NLU module </p>
<p>eyes 
normal 
Used for representing the current state of the eyes. Values are set by 
executing actions </p>
<p>node 
none 
Used for representing the current state-transition node of the exer-
cise. Values are set by executing actions </p>
<p>num_errors 
0 
Used for representing the current performance of the user in terms 
of the number of errors made. Values are set by executing actions </p>
<p>audio_volume 
normal 
Used for adapting the audio volume for persons with hearing impair-
ments. Values are set by KOaLa through motivation </p>
<p>exercise 
none 
Used for describing the current rehabilitation excercise. Values are 
set by KOaLa through motivation </p>
<p>Table 4
4Frequency of robots errors during the interaction for each user U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 15.61). They were asked for general information about their opinion on new technologies and about their familiarity with speech-based technology like SIRI, Ok Google, Alexa, etc. Questions were administered through a 5-point Likert scale. They reported an overall good opinion towards new technologies (M= 4.2, SD=0.63), and quite high familiarity with speech-based technology (M= 3.8, SD=0.63).0 
1 
1 
2 
2 
1 
2 
2 
1 
1 </p>
<p>https://jena.apache.org/.
We use IBM's Watson Speech to Text service: https://www.ibm.com/ cloud/watson-speech-to-text.5 To achieve this functionality, we use the NLU module of the IBM's Watson Assistant service: https://www.ibm.com/cloud/watsonassistant.
Note that "high" does not refer to some absolute numeric value. Exercises are considered as relevant if their ranking value is higher than the average value of all evaluated exercises. The dotted line ofFig. 4shows the average ranking value computed for each user profile.
" tokens " : [
" end " :[ 8 , 8 ]  , " duration " :[ 3 , 3 ]  },
" predicate " : " DoCognitiveStimulation ( ex 9)",
" end " :[ 12,12] , " duration " :[ 3 , 3 ]  },
" end " :[ 16,16] , " duration " :[ 3 , 3 ]  },
" id " : 3 ,
" predicate " : " DoCognitiveStimulation ( ex 5 )",
" end " :[ 20,20] , " duration " :[ 3 , 3 ]  },
" predicate " : " DoCognitiveStimulation ( ex 2 )",
" end " :[ 28,32] , " duration " :[ 3 , 7 ]  },
" predicate " : " DoCognitiveStimulation ( ex 4 )",
" end " :[ 32,36] , " duration " :[ 3 , 3 ]  },
" predicate " : " None () " ,
" end " :[ 25,25] , " duration " :[ 5 , 5 ]  },
" end " :[ 45,45] , " duration " :[ 5 , 5 ]  },
" predicate " : " Available ( yes , high , slow , large ) " ,
" predicate " : " Idle () " ,
" end " :[ 5 , 5 ]  , " duration " :[ 5 , 5 ]  },
" end " :[ 9 , 9 ]  , " duration " : [ 1 , 1 ] },
" end " :[ 13,13] , " duration " : [ 1 , 1 ] },
" end " :[ 17,17] , " duration " : [ 1 , 1 ] },
" end " :[ 25,29] , " duration " :[ 5 , 9 ]  },
" end " :[ 33,37] , " duration " :[ 1 , 5 ]  },
https://robot.omitech.it/en/sanbot-elf.8 Developed Java-based modules can be deployed on other robotic platforms either as "stand-alone" processes interacting with ROS through
Acknowledgements This work is supported by "SI-Robotics: SocIal ROBOTICS for active and healthy ageing" project (Italian M.I.U.R., PON -Ricerca e Innovazione 2014-2020 -G.A. ARS01_01120) and by CNR FOE2020 funds to the strategic project "Technologies to support the most vulnerable groups: young and old -CLEVERNESS". Authors are also members of the EU project TAILOR "Foundations of Trustworthy AI Integrating Learning, Optimisation and Reasoning" GA#952215.
DoCognitiveStimulation. predicate " : " DoCognitiveStimulation ( ex 2</p>
<p>Maintaining knowledge about temporal intervals. J F Allen, Commun ACM. 2611Allen JF (1983) Maintaining knowledge about temporal intervals. Commun ACM 26(11):832-843</p>
<p>A survey on knowledgeaware news recommender systems. I Andreea, M Alam, H Paulheim, Semantic WebAndreea I, Alam M, Paulheim H (2022) A survey on knowledge- aware news recommender systems. Semantic Web</p>
<p>Towards A Dual Process Approach to Computational Explanation inHuman-Robot Social Interaction. A Augello, I Infantino, A Lieto, U Maniscalco, G Pilato, F Vella, Proceedings of the 1st CAID workshop at IJCAI. the 1st CAID workshop at IJCAIAugello A, Infantino I, Lieto A, Maniscalco U, Pilato G, Vella F (2017) Towards A Dual Process Approach to Computational Expla- nation inHuman-Robot Social Interaction. In: Proceedings of the 1st CAID workshop at IJCAI</p>
<p>The Role of Functional Affordances in Socializing Robots. I Awaad, G K Kraetzschmar, J Hertzberg, Int J Soc Robot. 74Awaad I, Kraetzschmar GK, Hertzberg J (2015) The Role of Functional Affordances in Socializing Robots. Int J Soc Robot 7(4):421-438</p>
<p>Aging in place: a proposal for rural community-based care for frail elders. S Beidler, M Bourbonniere, Nurse Pract Forum. 101Beidler S, Bourbonniere M (1999) Aging in place: a proposal for rural community-based care for frail elders. Nurse Pract Forum 10(1):33-38</p>
<p>Robot-Era Project: Preliminary Results on the System Usability. R Bevilacqua, E Felici, F Marcellini, S Glende, S Klemcke, I Conrad, R Esposito, F Cavallo, P Dario, 10.1007/978-3-319-20889-3_51Proceedings of the Fourth International Conference on Design, User Experience, and Usability. Part III: Interactive Experience Design. the Fourth International Conference on Design, User Experience, and Usability. Part III: Interactive Experience DesignSpringer International Publishing9188Bevilacqua R, Felici E, Marcellini F, Glende S, Klemcke S, Conrad I, Esposito R, Cavallo F, Dario P (2015) Robot-Era Project: Pre- liminary Results on the System Usability. In: Proceedings of the Fourth International Conference on Design, User Experience, and Usability. Part III: Interactive Experience Design, Lecture Notes in Computer Science, vol 9188, pp 553-561. Springer International Publishing. https://doi.org/10.1007/978-3-319-20889-3_51</p>
<p>Computational architectures integrating neural and symbolic processes: A perspective on the state of the art. L A Bookman, Kluwer Academic PublishersBookman LA (1994) Computational architectures integrating neu- ral and symbolic processes: A perspective on the state of the art. Kluwer Academic Publishers</p>
<p>Knowledge representation for culturally competent personal robots: Requirements, design principles, implementation, and assessment. B Bruno, C T Recchiuto, I Papadopoulos, A Saffiotti, C Koulouglioti, R Menicatti, F Mastrogiovanni, R Zaccaria, A Sgorbissa, Int J Soc Robot. 113Bruno B, Recchiuto CT, Papadopoulos I, Saffiotti A, Koulougli- oti C, Menicatti R, Mastrogiovanni F, Zaccaria R, Sgorbissa A (2019) Knowledge representation for culturally competent per- sonal robots: Requirements, design principles, implementation, and assessment. Int J Soc Robot 11(3):515-538</p>
<p>Social Robot and Sensor Network in Support of Activity of Daily Living for People with Dementia. S Casaccia, G M Revel, L Scalise, R Bevilacqua, L Rossi, R A Paauwe, I Karkowsky, I Ercoli, J A Serrano, S Suijkerbuijk, D Lukkien, H H Nap, Dementia Lab. ChamSpringerMaking Design Work: Engaging with Dementia in Context. D-LabCasaccia S, Revel GM, Scalise L, Bevilacqua R, Rossi L, Paauwe RA, Karkowsky I, Ercoli I, Serrano JA, Suijkerbuijk S, Lukkien D, Nap HH (2019) Social Robot and Sensor Network in Support of Activity of Daily Living for People with Dementia. In: Dementia Lab 2019. Making Design Work: Engaging with Dementia in Con- text. D-Lab 2019. Communications in Computer and Information Science, pp 128-135. Springer, Cham</p>
<p>Interacting with an artificial partner: modeling the role of emotional aspects. I Cattinelli, M Goldwurm, N A Borghese, Biol Cybern. 996Cattinelli I, Goldwurm M, Borghese NA (2008) Interacting with an artificial partner: modeling the role of emotional aspects. Biol Cybern 99(6):473-489</p>
<p>Planning and execution with flexible timelines: a formal account. Cialdea Mayer, M Orlandini, A Umbrico, A , Acta Informatica. 536-8Cialdea Mayer M, Orlandini A, Umbrico A (2016) Planning and execution with flexible timelines: a formal account. Acta Informat- ica 53(6-8):649-680</p>
<p>The SSN ontology of the W3C semantic sensor network incubator group. M Compton, P Barnaghi, L Bermudez, R García-Castro, O Corcho, S Cox, J Graybeal, M Hauswirth, C Henson, A Herzog, V Huang, K Janowicz, W D Kelsey, D L Phuoc, L Lefort, M Leggieri, H Neuhaus, A Nikolov, K Page, A Passant, A Sheth, K Taylor, Web Semantics: Science, Services and Agents on the World Wide Web. 17Supplement CCompton M, Barnaghi P, Bermudez L, García-Castro R, Corcho O, Cox S, Graybeal J, Hauswirth M, Henson C, Herzog A, Huang V, Janowicz K, Kelsey WD, Phuoc DL, Lefort L, Leggieri M, Neuhaus H, Nikolov A, Page K, Passant A, Sheth A, Taylor K (2012) The SSN ontology of the W3C semantic sensor network incubator group. Web Semantics: Science, Services and Agents on the World Wide Web 17(Supplement C):25-32</p>
<p>Designing social robots for older adults. Cynthia L , B Anastasia, K , O Singh, N Park, H W , The Bridge. 491Cynthia L., B., Anastasia K., O., Singh, N., Park, H.W.: Designing social robots for older adults. The Bridge 49(1), 22-32 (2019)</p>
<p>A Two-Layered Approach to Adaptive Dialogues for Robotic Assistance. De Benedictis, R Umbrico, A Fracasso, F Cortellessa, G Orlandini, A Cesta, A , The 29th IEEE International Symposium on Robot and Human Interactive Communication. RO-MANDe Benedictis R, Umbrico A, Fracasso F, Cortellessa G, Orlan- dini A, Cesta A (2020) A Two-Layered Approach to Adaptive Dialogues for Robotic Assistance. In: RO-MAN. The 29th IEEE International Symposium on Robot and Human Interactive Com- munication</p>
<p>Simulating empathic behavior in a social assistive robot. B De Carolis, S Ferilli, G Palestra, Multimed Tools Appl. 76De Carolis B, Ferilli S, Palestra G (2017) Simulating empathic behavior in a social assistive robot. Multimed Tools Appl. 76:5073- 5094</p>
<p>Planning and Control. T L Dean, M P Wellman, Morgan Kaufmann Publishers IncDean TL, Wellman MP (1991) Planning and Control. Morgan Kaufmann Publishers Inc</p>
<p>Explainable artificial intelligence: A survey. F K Došilovic, M Brcic, N Hlupic, 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO). Došilovic FK, Brcic M, Hlupic N (2018) Explainable artificial intelligence: A survey. In: 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp 0210-0215</p>
<p>A layered control approach to human-aware task and motion planning for human-robot collaboration. M Faroni, M Beschi, S Ghidini, N Pedrocchi, A Umbrico, A Orlandini, A Cesta, IEEE Int. Conf. on Robot and Human Inter. Naples (ItalyFaroni M, Beschi M, Ghidini S, Pedrocchi N, Umbrico A, Orlandini A, Cesta A (2020) A layered control approach to human-aware task and motion planning for human-robot collaboration. In: IEEE Int. Conf. on Robot and Human Inter. Comm. Naples (Italy)</p>
<p>Defining socially assistive robotics. D Feil-Seifer, M J Mataric, IEEE 9th International Conference on Rehabilitation Robotics. Feil-Seifer D, Mataric MJ (2005) Defining socially assistive robotics. In: IEEE 9th International Conference on Rehabilitation Robotics</p>
<p>Defining socially assistive robotics. D J Feil-Seifer, M J Matarić, 9th International Conference on Rehabilitation Robotics. Feil-Seifer DJ, Matarić MJ (2005) Defining socially assistive robotics. 9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005. pp 465-468</p>
<p>Therapeutic and educational objectives in robot assisted play for children with autism. E Ferrari, B Robins, K Dautenhahn, RO-MAN 2009 -The 18th IEEE International Symposium on Robot and Human Interactive Communication. Ferrari E, Robins B, Dautenhahn K (2009) Therapeutic and edu- cational objectives in robot assisted play for children with autism. In: RO-MAN 2009 -The 18th IEEE International Symposium on Robot and Human Interactive Communication, pp 108-114</p>
<p>Forming relationships and the matching hypothesis. V S Folkes, Pers Soc Psychol Bull. 84Folkes VS (1982) Forming relationships and the matching hypoth- esis. Pers Soc Psychol Bull 8(4):631-636</p>
<p>A survey of socially interactive robots. T Fong, I Nourbakhsh, K Dautenhahn, Robot Auton Syst. 423-4Fong T, Nourbakhsh I, Dautenhahn K (2003) A survey of socially interactive robots. Robot Auton Syst 42(3-4):143-166</p>
<p>Engineering and Knowledge Management: Ontologies and the Semantic Web. A Gangemi, N Guarino, C Masolo, A Oltramari, L Schneider, Gómez-Pérez A, Benjamins VRSpringerBerlin Heidelberg; Berlin, HeidelbergSweetening ontologies with dolceGangemi A, Guarino N, Masolo C, Oltramari A, Schneider L (2002) Sweetening ontologies with dolce. In: Gómez-Pérez A, Benjamins VR (eds) Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web. Springer, Berlin Heidelberg, Berlin, Heidelberg, pp 166-181</p>
<p>Automated Planning: Theory and Practice. M Ghallab, D Nau, P Traverso, Morgan Kaufmann Publishers IncGhallab M, Nau D, Traverso P (2004) Automated Planning: Theory and Practice. Morgan Kaufmann Publishers Inc</p>
<p>How does the cerebral cortex work? learning, attention, and grouping by the laminar circuits of visual cortex. S Grossberg, Spat Vis. 122Grossberg S (1999) How does the cerebral cortex work? learning, attention, and grouping by the laminar circuits of visual cortex. Spat Vis 12(2):163-185</p>
<p>The 2021 ageing report underlying assumptions and projection methodologies. Economy, finance and the euro publications. A W Group, Group AW et al (2021) The 2021 ageing report underlying assump- tions and projection methodologies. Economy, finance and the euro publications</p>
<p>Formal ontology in information systems. N Guarino, Proceedings of the first international conference (FOIS'98). the first international conference (FOIS'98)Trento, ItalyIOS press46Guarino N (1998) Formal ontology in information systems: Pro- ceedings of the first international conference (FOIS'98), June 6-8, Trento, Italy, vol 46. IOS press</p>
<p>A survey on knowledge graph-based recommender systems. Q Guo, F Zhuang, C Qin, H Zhu, X Xie, H Xiong, Q He, IEEE Transactions on Knowledge and Data Engineering. Guo Q, Zhuang F, Qin C, Zhu H, Xie X, Xiong H, He Q (2020) A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering pp 1-1</p>
<p>Usability testing of a healthcare chatbot: Can we use conventional methods to assess conversational user interfaces?. S Holmes, A Moorhead, R Bond, H Zheng, V Coates, M Mctear, Proceedings of the 31st European Conference on Cognitive Ergonomics. the 31st European Conference on Cognitive ErgonomicsACMHolmes S, Moorhead A, Bond R, Zheng H, Coates V, Mctear M (2019) Usability testing of a healthcare chatbot: Can we use conventional methods to assess conversational user interfaces? In: Proceedings of the 31st European Conference on Cognitive Ergonomics. ACM</p>
<p>Deliberation for autonomous robots: A survey. F Ingrand, M Ghallab, Artif Intell. 247Ingrand F, Ghallab M (2017) Deliberation for autonomous robots: A survey. Artif Intell 247:10-44</p>
<p>The Principles of Psychology, in two volumes. W James, Henry Holt and CompanyNew YorkJames W (1890) The Principles of Psychology, in two volumes. Henry Holt and Company, New York</p>
<p>Non-verbal Signals in HRI: Interference in Human Perception. W Johal, G Calvary, S Pesty, Social Robotics. Springer International PublishingJohal W, Calvary G, Pesty S (2015) Non-verbal Signals in HRI: Interference in Human Perception. In: Social Robotics, pp 275- 284. Springer International Publishing</p>
<p>A perspective on judgment and choice: Mapping bounded rationality. D Kahneman, Am Psychol. 589Kahneman D (2003) A perspective on judgment and choice: Map- ping bounded rationality. Am Psychol 58(9):697-720</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. I Kotseruba, J K Tsotsos, Artif Intell Rev. 531Kotseruba I, Tsotsos JK (2018) 40 years of cognitive architectures: core cognitive abilities and practical applications. Artif Intell Rev 53(1):17-94</p>
<p>Cognitive architectures: Research issues and challenges. P Langley, J E Laird, S Rogers, Cogn Syst Res. 102Langley P, Laird JE, Rogers S (2009) Cognitive architectures: Research issues and challenges. Cogn Syst Res 10(2):141-160</p>
<p>Artificial cognition for social human-robot interaction: An implementation. S Lemaignan, M Warnier, E A Sisbot, A Clodic, R Alami, Special Issue on AI and Robotics. 247Artificial IntelligenceLemaignan S, Warnier M, Sisbot EA, Clodic A, Alami R (2017) Artificial cognition for social human-robot interaction: An imple- mentation. Artificial Intelligence 247, 45-69. Special Issue on AI and Robotics</p>
<p>The role of cognitive architectures in general artificial intelligence. A Lieto, M Bhatt, A Oltramari, D Vernon, Cogn Syst Res. 48Lieto A, Bhatt M, Oltramari A, Vernon D (2018) The role of cog- nitive architectures in general artificial intelligence. Cogn Syst Res 48:1-3</p>
<p>Interpersonal similarity and the social and intellectual dimensions of first impressions. J E Lydon, D W Jamieson, M P Zanna, Soc Cogn. 64Lydon JE, Jamieson DW, Zanna MP (1988) Interpersonal similarity and the social and intellectual dimensions of first impressions. Soc Cogn 6(4):269-286</p>
<p>The role of healthcare robotics in providing support to older adults: a socio-ecological perspective. G Mois, J M Beer, Current Geriatrics Reports. 92Mois G, Beer JM (2020) The role of healthcare robotics in provid- ing support to older adults: a socio-ecological perspective. Current Geriatrics Reports 9(2):82-89</p>
<p>Learning and personalizing socially assistive robot behaviors to aid with activities of daily living. C Moro, G Nejat, A Mihailidis, ACM Trans. Hum.-Robot Interact. 7225Moro C, Nejat G, Mihailidis A (2018) Learning and personalizing socially assistive robot behaviors to aid with activities of daily living. ACM Trans. Hum.-Robot Interact. 7(2):15:1-15:25</p>
<p>Does computer-synthesized speech manifest personality? experimental tests of recognition, similarityattraction, and consistency-attraction. C Nass, K M Lee, J Exp Psychol Appl. 7Nass C, Lee KM (2001) Does computer-synthesized speech manifest personality? experimental tests of recognition, similarity- attraction, and consistency-attraction. J Exp Psychol Appl 7:171- 181</p>
<p>Production Systems: Models of Control Structures. A Newell, Visual Information Processing. ElsevierNewell A (1973) Production Systems: Models of Control Struc- tures. In: Visual Information Processing, pp 463-526. Elsevier</p>
<p>Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain. R C O&apos;reilly, Y Munakata, MITPressCambridge, MAO'Reilly RC, Munakata Y (2000) Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain. MITPress, Cambridge, MA</p>
<p>Motion planning and scheduling for human and industrialrobot collaboration. S Pellegrinelli, A Orlandini, N Pedrocchi, A Umbrico, T Tolio, CIRP Ann Manuf Technol. 66Pellegrinelli S, Orlandini A, Pedrocchi N, Umbrico A, Tolio T (2017) Motion planning and scheduling for human and industrial- robot collaboration. CIRP Ann Manuf Technol 66:1-4</p>
<p>The Elaboration Likelihood Model of Persuasion. R E Petty, J T Cacioppo, Advances in Experimental Social Psychology. ElsevierPetty RE, Cacioppo JT (1986) The Elaboration Likelihood Model of Persuasion. In: Advances in Experimental Social Psychology, pp 123-205. Elsevier</p>
<p>Towards a science of integrated AI and Robotics. K Rajan, A Saffiotti, Artif Intell. 247Rajan K, Saffiotti A (2017) Towards a science of integrated AI and Robotics. Artif Intell 247:1-9</p>
<p>Empathic: An unappreciated way of being. C R Rogers, Couns Psychol. 52Rogers CR (1975) Empathic: An unappreciated way of being. Couns Psychol 5(2):2-10</p>
<p>User profiling and behavioral adaptation for HRI: A survey. S Rossi, F Ferland, A Tapus, Pattern Recogn Lett. 99Rossi S, Ferland F, Tapus A (2017) User profiling and behavioral adaptation for HRI: A survey. Pattern Recogn Lett 99:3-12</p>
<p>Reflective and Impulsive Determinants of Social Behavior. F Strack, R Deutsch, Pers Soc Psychol Rev. 83Strack F, Deutsch R (2004) Reflective and Impulsive Determinants of Social Behavior. Pers Soc Psychol Rev 8(3):220-247</p>
<p>A Cognitive Architecture Based on Dual Process Theory. C Strannegård, R Von Haugwitz, J Wessberg, C Balkenius, Artificial General Intelligence. SpringerStrannegård C, von Haugwitz R, Wessberg J, Balkenius C (2013) A Cognitive Architecture Based on Dual Process Theory. In: Artificial General Intelligence, pp 140-149. Springer Berlin Heidelberg</p>
<p>R Sun, The CLARION Cognitive Architecture. Oxford University PressSun R (2015) The CLARION Cognitive Architecture. Oxford Uni- versity Press</p>
<p>Reinforcement Learning: An Introduction, second edn. R S Sutton, A G Barto, The MIT PressSutton RS, Barto AG (2018) Reinforcement Learning: An Intro- duction, second edn. The MIT Press</p>
<p>Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. R S Sutton, D Precup, S Singh, Artif Intell. 112Sutton RS, Precup D, Singh S (1999) Between MDPs and semi- MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. Artif Intell 112:181-211</p>
<p>Emulating empathy in socially assistive robotics. A Tapus, M J Mataric, AAAI Spring Symposium: Multidisciplinary Collaboration for Socially Assistive Robotics. Tapus A, Mataric MJ (2007) Emulating empathy in socially assis- tive robotics. In: AAAI Spring Symposium: Multidisciplinary Collaboration for Socially Assistive Robotics, pp 93-96</p>
<p>Socially assistive robots: The link between personality, empathy, physiological signals, and task performance. A Tapus, M J Mataric, AAAI Spring Symposium: Emotion, Personality, and Social Behavior. Tapus A, Mataric MJ (2008) Socially assistive robots: The link between personality, empathy, physiological signals, and task per- formance. In: AAAI Spring Symposium: Emotion, Personality, and Social Behavior, pp 133-140</p>
<p>Socially assistive robotics. A Tapus, M J Mataric, B Scassellati, Grand Challenges of RoboticsTapus A, Mataric MJ, Scassellati B (2007) Socially assistive robotics [Grand Challenges of Robotics].</p>
<p>. IEEE Robotics Automation Magazine. 141IEEE Robotics Automa- tion Magazine 14(1):35-42</p>
<p>User-robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy. A Tapus, C Ţȃpuş, M J Matarić, Intel Serv Robot. 12Tapus A,Ţȃpuş C, Matarić MJ (2008) User-robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy. Intel Serv Robot 1(2):169-183</p>
<p>Representations for robot knowledge in the KnowRob framework. M Tenorth, M Beetz, Artif Intell. 247Tenorth M, Beetz M (2017) Representations for robot knowledge in the KnowRob framework. Artif Intell 247:151-169</p>
<p>PLAT-INUm: A New Framework for Planning and Acting. A Umbrico, A Cesta, Cialdea Mayer, M Orlandini, A , Lecture Notes in Computer Science. Umbrico A, Cesta A, Cialdea Mayer M, Orlandini A (2017) PLAT- INUm: A New Framework for Planning and Acting. Lecture Notes in Computer Science pp. 498-512</p>
<p>Integrating resource management and timeline-based planning. A Umbrico, A Cesta, Cialdea Mayer, M Orlandini, A , The 28th International Conference on Automated Planning and Scheduling (ICAPS). Umbrico A, Cesta A, Cialdea Mayer M, Orlandini A (2018) Inte- grating resource management and timeline-based planning. In: The 28th International Conference on Automated Planning and Scheduling (ICAPS)</p>
<p>A holistic approach to behavior adaptation for socially assistive robots. A Umbrico, A Cesta, G Cortellessa, A Orlandini, Int J Soc Robot. 123Umbrico A, Cesta A, Cortellessa G, Orlandini A (2020) A holistic approach to behavior adaptation for socially assistive robots. Int J Soc Robot 12(3):617-637</p>
<p>Modeling affordances and functioning for personalized robotic assistance. A Umbrico, G Cortellessa, A Orlandini, A Cesta, A Umbrico, G Cortellessa, A Orlandini, A Cesta, Principles of Knowledge Representation and Reasoning: Proceedings of the Sixteenth International Conference. AAAI Press64Toward intelligent continuous assistanceUmbrico A, Cortellessa G, Orlandini A, Cesta A (2020) Modeling affordances and functioning for personalized robotic assistance. In: Principles of Knowledge Representation and Reasoning: Proceed- ings of the Sixteenth International Conference. AAAI Press 64. Umbrico A, Cortellessa G, Orlandini A, Cesta A (2021) Toward intelligent continuous assistance. J Ambient Intell Humaniz Com- put 12(4):4513-4527</p>
<p>Emotion expression in a socially assistive robot for persons with parkinson's disease. A Valenti, A Block, M Chita-Tegmark, M Gold, M Scheutz, Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA '20. the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA '20Association for Computing MachineryValenti A, Block A, Chita-Tegmark M, Gold M, Scheutz M (2020) Emotion expression in a socially assistive robot for persons with parkinson's disease. In: Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Envi- ronments, PETRA '20. Association for Computing Machinery</p>
<p>Towards Culturally Robust Robots: A Critical Social Perspective on Robotics and Culture. S Šabanovic, C C Bennett, H R Lee, Proceedings of the ACM/IEEE Conference on Human-Robot Interaction (HRI) Workshop on Culture-Aware Robotics (CARS). the ACM/IEEE Conference on Human-Robot Interaction (HRI) Workshop on Culture-Aware Robotics (CARS)Šabanovic S, Bennett CC, Lee HR (2014) Towards Culturally Robust Robots: A Critical Social Perspective on Robotics and Culture. In: Proceedings of the ACM/IEEE Conference on Human- Robot Interaction (HRI) Workshop on Culture-Aware Robotics (CARS)</p>
<p>Towards developing general models of usability with paradise. M Walker, C Kamm, D Litman, Nat Lang Eng. 63-4Walker M, Kamm C, Litman D (2000) Towards developing general models of usability with paradise. Nat Lang Eng 6(3-4):363-377</p>
<p>Paradise: A framework for evaluating spoken dialogue agents. M A Walker, D J Litman, C A Kamm, A Abella, Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, ACL '98/EACL '98. the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, ACL '98/EACL '98USAAssociation for Computational LinguisticsWalker MA, Litman DJ, Kamm CA, Abella A (1997) Paradise: A framework for evaluating spoken dialogue agents. In: Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, ACL '98/EACL '98, pp 271-280. Association for Computational Linguistics, USA</p>
<p>Dual processes in reasoning. P Wason, J Evans, Cognition. 32Wason P, Evans J (1974) Dual processes in reasoning? Cognition 3(2):141-154</p>
<p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Publisher's Note Springer Nature remains neutral with regard to juris- dictional claims in published maps and institutional affiliations.</p>
<p>Alessandro has spent a period as guest PhD student at Osnabruck University investigating hybrid approaches to planning. His research topics cover the development of planning and execution techniques suitable for real-world applications. He is also interested in investigating interactions between knowledge representation and planning and, in applying plan-based control techniques in manufacturing contexts for Human-Robot Collaboration. 2014-2017] and [2019-2022Alessandro worked on Human-Robot Collaboration within the projects FourByThree (H2020 Factories of the Future) and Share-Work (H2020, FoF) integrating semantic technologies with planning and acting techniques to foster highly adaptive robot controllers for safe human-robot interactions. Alessandro Umbrico [M.S. Engineering Computer Science 2012, Phd Computer Science and AutomationTerm researcher at CNR -Institute of Cognitive Sciences and Technologies (CNR-ISTC)Alessandro Umbrico [M.S. Engineering Computer Science 2012, Phd Computer Science and Automation, 2017] is a Fixed-Term researcher at CNR -Institute of Cognitive Sciences and Technologies (CNR- ISTC). Alessandro has spent a period as guest PhD student at Osnabruck University investigating hybrid approaches to planning. His research topics cover the development of planning and execution techniques suitable for real-world applications. He is also interested in investigat- ing interactions between knowledge representation and planning and, in applying plan-based control techniques in manufacturing contexts for Human-Robot Collaboration. In [2013-2015] Alessandro worked in Reconfigurable Manufacturing Systems within the GECKO project (National Project on "Factories of the Future"). In [2014-2017] and [2019-2022] Alessandro worked on Human-Robot Collaboration within the projects FourByThree (H2020 Factories of the Future) and Share- Work (H2020, FoF) integrating semantic technologies with planning and acting techniques to foster highly adaptive robot controllers for safe human-robot interactions.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7750 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7750</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7750</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-141.html">extraction-schema-141</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <p><strong>Paper ID:</strong> paper-a5f8abf651101965fa482e978ab21d9ed175fd78</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a5f8abf651101965fa482e978ab21d9ed175fd78" target="_blank">ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A dynamic benchmark that evaluates the accuracy of ML systems on an automatically generated and regularly updated set of 1,000 forecasting questions that have no known answer at the time of submission.</p>
                <p><strong>Paper Abstract:</strong> Forecasts of future events are essential inputs into informed decision-making. Machine learning (ML) systems have the potential to deliver forecasts at scale, but there is no framework for evaluating the accuracy of ML systems on a standardized set of forecasting questions. To address this gap, we introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML systems on an automatically generated and regularly updated set of 1,000 forecasting questions. To avoid any possibility of data leakage, ForecastBench is comprised solely of questions about future events that have no known answer at the time of submission. We quantify the capabilities of current ML systems by collecting forecasts from expert (human) forecasters, the general public, and LLMs on a random subset of questions from the benchmark ($N=200$). While LLMs have achieved super-human performance on many benchmarks, they perform less well here: expert forecasters outperform the top-performing LLM ($p$-value $<0.001$). We display system and human scores in a public leaderboard at www.forecastbench.org.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7750.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7750.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ForecastBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ForecastBench dynamic forecasting benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuously-updated benchmark and public leaderboard that evaluates probabilistic forecasts from LLMs and humans on 1,000 real-world unresolved questions sampled from markets and datasets, with nightly scoring and published datasets of forecasts, rationales, and resolutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ForecastBench protocol</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model — a standardized evaluation protocol that elicits probabilistic forecasts (a single number in [0,1] for binary outcomes) from instruction-following chat models using multiple prompting strategies (zero-shot, scratchpad, retrieval-augmented) and optional auxiliary inputs (freeze values/crowd forecasts, retrieved news); enforces full-question submissions and imputes missing forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct elicitation of probabilities via prompting (zero-shot probability output; scratchpad chain-of-thought style prompting); retrieval-augmentation (news summaries) for up-to-date evidence; provision of 'freeze values' (crowd forecast or baseline) as an input; ensemble aggregation (median, geometric mean, geometric mean of log odds) across multiple LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary outcomes for a wide set of future real-world events drawn from prediction markets and structured datasets (e.g., will an event occur, will a time-series metric exceed a threshold by a date).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cross-domain: geopolitics, economics, finance, public health, conflict (ACLED), weather/climate, and other real-world time-series topics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Question sources listed in Table 1: Manifold Markets, Metaculus, Polymarket (markets); ACLED, DBnomics, FRED, Wikipedia-derived questions, Yahoo! Finance (dataset-derived time series); plus combination questions constructed from these.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>For dataset-derived questions: 8 horizons (7, 30, 90, 180, 365, 1095, 1825, 3650 days). For market-derived questions: single horizon (until market resolution date).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score for binary questions (mean Brier score aggregated across dataset and market questions); unresolved market forecasts are provisionally scored by squared distance to the platform's crowd forecast prior to final resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Reported aggregate Brier scores across participants and baselines (examples: Superforecaster median overall Brier = 0.096; Public median = 0.121; top LLM on 200-question human subset Claude-3.5-Sonnet = 0.122; top LLM on full 1000-question LLM set Claude-3.5-Sonnet = 0.123 overall; see Tables 2 and 3 for full breakdowns).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No explicit calibration error (e.g., expected calibration error) reported; authors rely on Brier score (which conflates calibration and discrimination) and note LLMs are overall less accurate than superforecasters and perform particularly poorly on combination questions; no numeric calibration diagnostics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Seven LLM prompting baselines: (1) zero-shot prompting, (2) scratchpad prompting, (3) scratchpad + retrieved news, (4) zero-shot + freeze values (crowd/baseline), (5) scratchpad + freeze values, (6) scratchpad + news + freeze values, (7) LLM ensemble aggregations (median, geometric mean, geometric mean of log-odds). Human baselines: general public median and superforecaster median. Imputation baselines for missing forecasts: crowd forecast for missing market forecasts and 0.5 for missing dataset forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors report (a) LLMs' knowledge cutoffs and the need for retrieval/updates; (b) access to crowd freeze values materially improves LLM performance (raising concerns about how much models rely on human aggregates); (c) models perform poorly on combination questions requiring joint probability reasoning; (d) no explicit calibration post-processing (e.g., temperature scaling) reported; (e) unresolved market questions are provisional (scored against crowd until official resolution); (f) some potential generosity in imputing crowd forecast for missing model submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>Concrete examples in the paper: freeze value (community prediction) for a Metaculus market question 'Will the Cavendish account for less than 50% of banana exports worldwide before 2035?' had freeze value 0.48 (i.e., community probability 0.48). Imputation example: missing dataset forecasts are imputed as 0.5 (uninformed 50%). The paper does not publish per-question LLM probability outputs beyond these freeze/imputation examples in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7750.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7750.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Claude-family instruction-following chat LLM from Anthropic that was among the top-performing models in ForecastBench when prompted with scratchpad prompts and provided freeze values; evaluated without additional fine-tuning on 1,000 dynamic forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following chat model (Claude family); used off-the-shelf (no additional fine-tuning) and prompted to output probabilistic forecasts either zero-shot or with scratchpad reasoning; variants evaluated with and without retrieved news and with/without freeze values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct probability elicitation via prompts; best-performing configuration used scratchpad prompting plus provision of freeze values; ensemble variants aggregated multiple forecasts via median/geometric-mean/log-odds.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary forecasting questions from ForecastBench (market questions and dataset-derived threshold/time-series questions).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cross-domain (geopolitics, economics, conflict, finance, weather/time-series).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ForecastBench question sets drawn from Metaculus, Manifold, Polymarket, ACLED, DBnomics, FRED, Wikipedia, Yahoo! Finance; evaluated on the 200-question human subset and the full 1,000-question LLM set.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Evaluated on market questions (single horizon until resolution) and dataset horizons (7, 30, 90, 180 days used for the 200-question human comparison; full runs also include longer horizons up to 10 years).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (mean dataset and market Brier; overall score is average of mean dataset and mean market Brier scores).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>On the 200-question human-subset: Claude-3.5-Sonnet (freeze values + scratchpad) overall Brier = 0.122 (dataset 0.138, market 0.107). On the full 1,000-question LLM set: top Claude-3.5-Sonnet (freeze values + scratchpad) overall Brier = 0.123 (dataset 0.169, market 0.078) as reported in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not separately quantified; no explicit calibration post-processing reported; Brier scores imply sub-superforecaster performance and authors note models are less accurate and have particular weaknesses (e.g., combination questions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against zero-shot, scratchpad without freeze values, scratchpad with news, scratchpad + freeze values, ensembles, human medians (public and superforecasters).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance depends on access to freeze values (crowd forecasts); retrieval of recent news did not consistently improve scores; combination (joint probability) questions remain a clear weakness; no fine-tuning was performed in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>Paper does not publish per-question Claude predicted probabilities beyond aggregate scores; paper reports the use of freeze values (e.g., community probability 0.48 for a banana-export market) which Claude could be given as an input.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7750.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7750.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4-Turbo / GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 family (GPT-4, GPT-4-Turbo, GPT-4o / OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 family of instruction-following chat models evaluated off-the-shelf under multiple prompting regimes (zero-shot, scratchpad) and with optional inputs (freeze values, retrieved news); used to produce direct probabilistic forecasts for ForecastBench questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-4-Turbo / GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following transformer chat models from OpenAI, evaluated without extra fine-tuning; prompted to output a single probability per binary question, tested with scratchpad and zero-shot prompts, and with optional retrieval of news and inclusion of freeze values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct probability elicitation via prompts (zero-shot and scratchpad); retrieval-augmented generation for news; freeze values (crowd/base) included for some baselines; ensembles also constructed from multiple prompt/model combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary future event questions from ForecastBench across markets and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cross-domain (as above: geopolitics, economics, finance, conflict, weather/time-series).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ForecastBench question sets (market and dataset sources listed in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Market: single horizon until resolution; Dataset: multiple horizons including 7, 30, 90, 180 days (and longer for full runs up to 10 years).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (dataset, market, overall averages).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Examples from tables: On 200-question human subset GPT-4-Turbo (freeze values, zero-shot) overall Brier = 0.128 (dataset 0.162, market 0.095); GPT-4 (freeze values, zero-shot) overall Brier = 0.132 (dataset 0.167, market 0.096). On full 1,000-question set GPT-4-Turbo (freeze values, scratchpad) overall Brier = 0.126 (dataset 0.172, market 0.080).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No explicit calibration metrics reported; authors rely on Brier score and note performance shortfalls vs. humans; no temperature-scaling or calibration post-hoc methods documented.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against Claude variants, Llama variants, Mistral, Qwen1.5, Gemini, ensemble aggregations, human medians, and the seven prompting/retrieval/freeze-value baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Knowledge cutoff and need for retrieval for up-to-date evidence; performance benefits strongly when given freeze values (crowd forecasts), suggesting reliance on human aggregates; poorer performance on combination (joint-probability) questions; no explicit calibration applied.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>No per-question GPT-4 probability outputs printed in the main text; missing-dataset forecast imputation is 0.5 and freeze values such as 0.48 are provided to models for market questions as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7750.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7750.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini-family chat model evaluated in ForecastBench under scratchpad prompting with and without news and freeze values; produced direct probabilistic forecasts and appeared in the LLM leaderboard.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal/large instruction-following chat model from Google evaluated without further fine-tuning; used scratchpad prompts to reason and output probabilities for binary forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct probability elicitation via prompting (scratchpad); evaluated with freeze values and with retrieved news variants.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary forecasting questions across ForecastBench sources.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cross-domain real-world events and time series.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ForecastBench question sets (markets and datasets listed in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Market questions until resolution; dataset horizons including 7, 30, 90, 180 days and longer horizons in full runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (dataset, market, overall).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>On the full 1,000-question LLM set Gemini-1.5-Pro (freeze values + scratchpad) overall Brier = 0.134 (dataset 0.162, market 0.106) as reported in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not explicitly reported; no separate calibration diagnostics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to other LLMs (Claude, GPT-4 family, Qwen1.5) and prompting variants; ensemble baselines included.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Same general limitations: dependence on freeze values for stronger performance, retrieval of news did not clearly improve results, worse performance on combination questions relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>No model-specific per-question probabilities shown; freeze values (community forecasts) like 0.48 and imputation 0.5 are used as concrete probability inputs/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7750.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7750.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-model from the Llama family (70B parameters) included in the ForecastBench evaluation suite; used as an instruction-following chat model to output probabilistic forecasts under the same prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation model (70B) used in chat/instruction mode for direct probability outputs; evaluated without extra fine-tuning using the same prompting baselines as other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct probability output via prompting (zero-shot and scratchpad); could be combined in ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary events from ForecastBench question set (markets + datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cross-domain real-world forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ForecastBench question sets drawn from markets and datasets (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Dataset horizons (multiple up to 10 years) and market resolution horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (as with other models).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Specific per-model Brier numbers for Llama-2-70B are not highlighted in the main top-10 tables; it is listed among evaluated models in Section 2.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared under the same seven prompting/retrieval/freeze baselines and against human medians and other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No fine-tuning; same dataset/horizon limitations; paper does not report size-specific calibration or post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>No explicit per-question Llama-2-70B probability outputs shown in the paper; paper-level examples include freeze value 0.48 and imputed 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7750.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7750.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM ensemble baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM ensemble (median / geometric mean / log-odds geometric mean)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble baselines that combines multiple LLM forecasts (three models × three superforecaster-crafted prompts) to produce aggregated probabilistic forecasts via median, geometric mean, or geometric mean of log odds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM ensemble (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro × 3 prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ensemble constructed by collecting 9 forecasts per question (3 models × 3 prompts) and aggregating via median, geometric mean, or geometric-mean-of-log-odds (logit aggregation) following Satopää et al. (2014).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Ensemble aggregation of probabilistic outputs: median, geometric mean, and geometric mean of log-odds (logit pooling).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary forecasting questions in ForecastBench.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Cross-domain real-world forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ForecastBench question sets (markets and datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Same horizons as underlying LLM forecasts (dataset horizons and market resolutions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (ensemble performance reported as alternative baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Ensemble baselines were implemented and compared; detailed ensemble numeric scores are reported in appendices (paper notes use of ensemble but main tables focus on single-model top performers).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No explicit calibration diagnostics for ensembles beyond Brier score comparisons; aggregation methods chosen to improve accuracy and probabilistic coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against single-model baselines (zero-shot, scratchpad, with/without news and freeze values) and human medians.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Ensembles still underperform top human superforecasters in the benchmark; ensemble performance depends on constituent model diversity and prompt design; no calibration post-hoc applied.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>No per-question ensemble probabilities printed in the main text beyond statement that ensembles were built and aggregated with median/geometric/log-odds pooling.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Approaching human-level forecasting with language models <em>(Rating: 2)</em></li>
                <li>Forecasting future world events with neural networks <em>(Rating: 2)</em></li>
                <li>Large language model prediction capabilities: Evidence from a real-world forecasting tournament <em>(Rating: 2)</em></li>
                <li>Can language models use forecasting strategies? <em>(Rating: 2)</em></li>
                <li>Superhuman automated forecasting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7750",
    "paper_id": "paper-a5f8abf651101965fa482e978ab21d9ed175fd78",
    "extraction_schema_id": "extraction-schema-141",
    "extracted_data": [
        {
            "name_short": "ForecastBench",
            "name_full": "ForecastBench dynamic forecasting benchmark",
            "brief_description": "A continuously-updated benchmark and public leaderboard that evaluates probabilistic forecasts from LLMs and humans on 1,000 real-world unresolved questions sampled from markets and datasets, with nightly scoring and published datasets of forecasts, rationales, and resolutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ForecastBench protocol",
            "model_description": "Not a single model — a standardized evaluation protocol that elicits probabilistic forecasts (a single number in [0,1] for binary outcomes) from instruction-following chat models using multiple prompting strategies (zero-shot, scratchpad, retrieval-augmented) and optional auxiliary inputs (freeze values/crowd forecasts, retrieved news); enforces full-question submissions and imputes missing forecasts.",
            "model_size": null,
            "probability_estimation_method": "Direct elicitation of probabilities via prompting (zero-shot probability output; scratchpad chain-of-thought style prompting); retrieval-augmentation (news summaries) for up-to-date evidence; provision of 'freeze values' (crowd forecast or baseline) as an input; ensemble aggregation (median, geometric mean, geometric mean of log odds) across multiple LLMs.",
            "prediction_target": "Binary outcomes for a wide set of future real-world events drawn from prediction markets and structured datasets (e.g., will an event occur, will a time-series metric exceed a threshold by a date).",
            "domain": "Cross-domain: geopolitics, economics, finance, public health, conflict (ACLED), weather/climate, and other real-world time-series topics.",
            "dataset_used": "Question sources listed in Table 1: Manifold Markets, Metaculus, Polymarket (markets); ACLED, DBnomics, FRED, Wikipedia-derived questions, Yahoo! Finance (dataset-derived time series); plus combination questions constructed from these.",
            "forecasting_horizon": "For dataset-derived questions: 8 horizons (7, 30, 90, 180, 365, 1095, 1825, 3650 days). For market-derived questions: single horizon (until market resolution date).",
            "evaluation_metric": "Brier score for binary questions (mean Brier score aggregated across dataset and market questions); unresolved market forecasts are provisionally scored by squared distance to the platform's crowd forecast prior to final resolution.",
            "reported_performance": "Reported aggregate Brier scores across participants and baselines (examples: Superforecaster median overall Brier = 0.096; Public median = 0.121; top LLM on 200-question human subset Claude-3.5-Sonnet = 0.122; top LLM on full 1000-question LLM set Claude-3.5-Sonnet = 0.123 overall; see Tables 2 and 3 for full breakdowns).",
            "calibration_quality": "No explicit calibration error (e.g., expected calibration error) reported; authors rely on Brier score (which conflates calibration and discrimination) and note LLMs are overall less accurate than superforecasters and perform particularly poorly on combination questions; no numeric calibration diagnostics provided.",
            "baseline_methods": "Seven LLM prompting baselines: (1) zero-shot prompting, (2) scratchpad prompting, (3) scratchpad + retrieved news, (4) zero-shot + freeze values (crowd/baseline), (5) scratchpad + freeze values, (6) scratchpad + news + freeze values, (7) LLM ensemble aggregations (median, geometric mean, geometric mean of log-odds). Human baselines: general public median and superforecaster median. Imputation baselines for missing forecasts: crowd forecast for missing market forecasts and 0.5 for missing dataset forecasts.",
            "limitations": "Authors report (a) LLMs' knowledge cutoffs and the need for retrieval/updates; (b) access to crowd freeze values materially improves LLM performance (raising concerns about how much models rely on human aggregates); (c) models perform poorly on combination questions requiring joint probability reasoning; (d) no explicit calibration post-processing (e.g., temperature scaling) reported; (e) unresolved market questions are provisional (scored against crowd until official resolution); (f) some potential generosity in imputing crowd forecast for missing model submissions.",
            "probability_examples": "Concrete examples in the paper: freeze value (community prediction) for a Metaculus market question 'Will the Cavendish account for less than 50% of banana exports worldwide before 2035?' had freeze value 0.48 (i.e., community probability 0.48). Imputation example: missing dataset forecasts are imputed as 0.5 (uninformed 50%). The paper does not publish per-question LLM probability outputs beyond these freeze/imputation examples in the main text.",
            "real_world_future": true,
            "uuid": "e7750.0",
            "source_info": {
                "paper_title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Claude-3.5-Sonnet",
            "name_full": "Claude 3.5 Sonnet",
            "brief_description": "A Claude-family instruction-following chat LLM from Anthropic that was among the top-performing models in ForecastBench when prompted with scratchpad prompts and provided freeze values; evaluated without additional fine-tuning on 1,000 dynamic forecasting questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet",
            "model_description": "Instruction-following chat model (Claude family); used off-the-shelf (no additional fine-tuning) and prompted to output probabilistic forecasts either zero-shot or with scratchpad reasoning; variants evaluated with and without retrieved news and with/without freeze values.",
            "model_size": null,
            "probability_estimation_method": "Direct probability elicitation via prompts; best-performing configuration used scratchpad prompting plus provision of freeze values; ensemble variants aggregated multiple forecasts via median/geometric-mean/log-odds.",
            "prediction_target": "Binary forecasting questions from ForecastBench (market questions and dataset-derived threshold/time-series questions).",
            "domain": "Cross-domain (geopolitics, economics, conflict, finance, weather/time-series).",
            "dataset_used": "ForecastBench question sets drawn from Metaculus, Manifold, Polymarket, ACLED, DBnomics, FRED, Wikipedia, Yahoo! Finance; evaluated on the 200-question human subset and the full 1,000-question LLM set.",
            "forecasting_horizon": "Evaluated on market questions (single horizon until resolution) and dataset horizons (7, 30, 90, 180 days used for the 200-question human comparison; full runs also include longer horizons up to 10 years).",
            "evaluation_metric": "Brier score (mean dataset and market Brier; overall score is average of mean dataset and mean market Brier scores).",
            "reported_performance": "On the 200-question human-subset: Claude-3.5-Sonnet (freeze values + scratchpad) overall Brier = 0.122 (dataset 0.138, market 0.107). On the full 1,000-question LLM set: top Claude-3.5-Sonnet (freeze values + scratchpad) overall Brier = 0.123 (dataset 0.169, market 0.078) as reported in Table 3.",
            "calibration_quality": "Not separately quantified; no explicit calibration post-processing reported; Brier scores imply sub-superforecaster performance and authors note models are less accurate and have particular weaknesses (e.g., combination questions).",
            "baseline_methods": "Compared against zero-shot, scratchpad without freeze values, scratchpad with news, scratchpad + freeze values, ensembles, human medians (public and superforecasters).",
            "limitations": "Performance depends on access to freeze values (crowd forecasts); retrieval of recent news did not consistently improve scores; combination (joint probability) questions remain a clear weakness; no fine-tuning was performed in these experiments.",
            "probability_examples": "Paper does not publish per-question Claude predicted probabilities beyond aggregate scores; paper reports the use of freeze values (e.g., community probability 0.48 for a banana-export market) which Claude could be given as an input.",
            "real_world_future": true,
            "uuid": "e7750.1",
            "source_info": {
                "paper_title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4 / GPT-4-Turbo / GPT-4o",
            "name_full": "GPT-4 family (GPT-4, GPT-4-Turbo, GPT-4o / OpenAI)",
            "brief_description": "OpenAI's GPT-4 family of instruction-following chat models evaluated off-the-shelf under multiple prompting regimes (zero-shot, scratchpad) and with optional inputs (freeze values, retrieved news); used to produce direct probabilistic forecasts for ForecastBench questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-4-Turbo / GPT-4o",
            "model_description": "Instruction-following transformer chat models from OpenAI, evaluated without extra fine-tuning; prompted to output a single probability per binary question, tested with scratchpad and zero-shot prompts, and with optional retrieval of news and inclusion of freeze values.",
            "model_size": null,
            "probability_estimation_method": "Direct probability elicitation via prompts (zero-shot and scratchpad); retrieval-augmented generation for news; freeze values (crowd/base) included for some baselines; ensembles also constructed from multiple prompt/model combinations.",
            "prediction_target": "Binary future event questions from ForecastBench across markets and datasets.",
            "domain": "Cross-domain (as above: geopolitics, economics, finance, conflict, weather/time-series).",
            "dataset_used": "ForecastBench question sets (market and dataset sources listed in Table 1).",
            "forecasting_horizon": "Market: single horizon until resolution; Dataset: multiple horizons including 7, 30, 90, 180 days (and longer for full runs up to 10 years).",
            "evaluation_metric": "Brier score (dataset, market, overall averages).",
            "reported_performance": "Examples from tables: On 200-question human subset GPT-4-Turbo (freeze values, zero-shot) overall Brier = 0.128 (dataset 0.162, market 0.095); GPT-4 (freeze values, zero-shot) overall Brier = 0.132 (dataset 0.167, market 0.096). On full 1,000-question set GPT-4-Turbo (freeze values, scratchpad) overall Brier = 0.126 (dataset 0.172, market 0.080).",
            "calibration_quality": "No explicit calibration metrics reported; authors rely on Brier score and note performance shortfalls vs. humans; no temperature-scaling or calibration post-hoc methods documented.",
            "baseline_methods": "Compared against Claude variants, Llama variants, Mistral, Qwen1.5, Gemini, ensemble aggregations, human medians, and the seven prompting/retrieval/freeze-value baselines.",
            "limitations": "Knowledge cutoff and need for retrieval for up-to-date evidence; performance benefits strongly when given freeze values (crowd forecasts), suggesting reliance on human aggregates; poorer performance on combination (joint-probability) questions; no explicit calibration applied.",
            "probability_examples": "No per-question GPT-4 probability outputs printed in the main text; missing-dataset forecast imputation is 0.5 and freeze values such as 0.48 are provided to models for market questions as inputs.",
            "real_world_future": true,
            "uuid": "e7750.2",
            "source_info": {
                "paper_title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro",
            "name_full": "Gemini 1.5 Pro",
            "brief_description": "Google's Gemini-family chat model evaluated in ForecastBench under scratchpad prompting with and without news and freeze values; produced direct probabilistic forecasts and appeared in the LLM leaderboard.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Pro",
            "model_description": "Multimodal/large instruction-following chat model from Google evaluated without further fine-tuning; used scratchpad prompts to reason and output probabilities for binary forecasting questions.",
            "model_size": null,
            "probability_estimation_method": "Direct probability elicitation via prompting (scratchpad); evaluated with freeze values and with retrieved news variants.",
            "prediction_target": "Binary forecasting questions across ForecastBench sources.",
            "domain": "Cross-domain real-world events and time series.",
            "dataset_used": "ForecastBench question sets (markets and datasets listed in Table 1).",
            "forecasting_horizon": "Market questions until resolution; dataset horizons including 7, 30, 90, 180 days and longer horizons in full runs.",
            "evaluation_metric": "Brier score (dataset, market, overall).",
            "reported_performance": "On the full 1,000-question LLM set Gemini-1.5-Pro (freeze values + scratchpad) overall Brier = 0.134 (dataset 0.162, market 0.106) as reported in Table 3.",
            "calibration_quality": "Not explicitly reported; no separate calibration diagnostics provided.",
            "baseline_methods": "Compared to other LLMs (Claude, GPT-4 family, Qwen1.5) and prompting variants; ensemble baselines included.",
            "limitations": "Same general limitations: dependence on freeze values for stronger performance, retrieval of news did not clearly improve results, worse performance on combination questions relative to humans.",
            "probability_examples": "No model-specific per-question probabilities shown; freeze values (community forecasts) like 0.48 and imputation 0.5 are used as concrete probability inputs/benchmarks.",
            "real_world_future": true,
            "uuid": "e7750.3",
            "source_info": {
                "paper_title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama-2-70B",
            "name_full": "Llama 2 (70B)",
            "brief_description": "A large open-model from the Llama family (70B parameters) included in the ForecastBench evaluation suite; used as an instruction-following chat model to output probabilistic forecasts under the same prompting baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-70B",
            "model_description": "Open foundation model (70B) used in chat/instruction mode for direct probability outputs; evaluated without extra fine-tuning using the same prompting baselines as other LLMs.",
            "model_size": "70B",
            "probability_estimation_method": "Direct probability output via prompting (zero-shot and scratchpad); could be combined in ensembles.",
            "prediction_target": "Binary events from ForecastBench question set (markets + datasets).",
            "domain": "Cross-domain real-world forecasting questions.",
            "dataset_used": "ForecastBench question sets drawn from markets and datasets (Table 1).",
            "forecasting_horizon": "Dataset horizons (multiple up to 10 years) and market resolution horizons.",
            "evaluation_metric": "Brier score (as with other models).",
            "reported_performance": "Specific per-model Brier numbers for Llama-2-70B are not highlighted in the main top-10 tables; it is listed among evaluated models in Section 2.",
            "calibration_quality": "Not reported separately.",
            "baseline_methods": "Compared under the same seven prompting/retrieval/freeze baselines and against human medians and other LLMs.",
            "limitations": "No fine-tuning; same dataset/horizon limitations; paper does not report size-specific calibration or post-processing.",
            "probability_examples": "No explicit per-question Llama-2-70B probability outputs shown in the paper; paper-level examples include freeze value 0.48 and imputed 0.5.",
            "real_world_future": true,
            "uuid": "e7750.4",
            "source_info": {
                "paper_title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM ensemble baseline",
            "name_full": "LLM ensemble (median / geometric mean / log-odds geometric mean)",
            "brief_description": "An ensemble baselines that combines multiple LLM forecasts (three models × three superforecaster-crafted prompts) to produce aggregated probabilistic forecasts via median, geometric mean, or geometric mean of log odds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM ensemble (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro × 3 prompts)",
            "model_description": "Ensemble constructed by collecting 9 forecasts per question (3 models × 3 prompts) and aggregating via median, geometric mean, or geometric-mean-of-log-odds (logit aggregation) following Satopää et al. (2014).",
            "model_size": null,
            "probability_estimation_method": "Ensemble aggregation of probabilistic outputs: median, geometric mean, and geometric mean of log-odds (logit pooling).",
            "prediction_target": "Binary forecasting questions in ForecastBench.",
            "domain": "Cross-domain real-world forecasting.",
            "dataset_used": "ForecastBench question sets (markets and datasets).",
            "forecasting_horizon": "Same horizons as underlying LLM forecasts (dataset horizons and market resolutions).",
            "evaluation_metric": "Brier score (ensemble performance reported as alternative baselines).",
            "reported_performance": "Ensemble baselines were implemented and compared; detailed ensemble numeric scores are reported in appendices (paper notes use of ensemble but main tables focus on single-model top performers).",
            "calibration_quality": "No explicit calibration diagnostics for ensembles beyond Brier score comparisons; aggregation methods chosen to improve accuracy and probabilistic coherence.",
            "baseline_methods": "Compared against single-model baselines (zero-shot, scratchpad, with/without news and freeze values) and human medians.",
            "limitations": "Ensembles still underperform top human superforecasters in the benchmark; ensemble performance depends on constituent model diversity and prompt design; no calibration post-hoc applied.",
            "probability_examples": "No per-question ensemble probabilities printed in the main text beyond statement that ensembles were built and aggregated with median/geometric/log-odds pooling.",
            "real_world_future": true,
            "uuid": "e7750.5",
            "source_info": {
                "paper_title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Approaching human-level forecasting with language models",
            "rating": 2
        },
        {
            "paper_title": "Forecasting future world events with neural networks",
            "rating": 2
        },
        {
            "paper_title": "Large language model prediction capabilities: Evidence from a real-world forecasting tournament",
            "rating": 2
        },
        {
            "paper_title": "Can language models use forecasting strategies?",
            "rating": 2
        },
        {
            "paper_title": "Superhuman automated forecasting",
            "rating": 1
        }
    ],
    "cost": 0.01756225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FORECASTBENCH: A DYNAMIC BENCHMARK OF AI FORECASTING CAPABILITIES</h1>
<p>Ezra Karger <em><br>Forecasting Research Institute<br>Federal Reserve Bank of Chicago<br>ezra@forecastingresearch.org<br>Chen Yueh-Han </em><br>New York University<br>yc7592@nyu.edu<br>Danny Halawi<br>University of California, Berkeley<br>dhalawi@berkeley.edu<br>Philip E. Tetlock<br>Forecasting Research Institute<br>University of Pennsylvania<br>tetlock@wharton.upenn.edu</p>
<p>Houtan Bastani *<br>Forecasting Research Institute<br>houtan@forecastingresearch.org<br>Zachary Jacobs<br>Forecasting Research Institute<br>zach@forecastingresearch.org<br>Fred Zhang<br>University of California, Berkeley<br>z0@berkeley.edu</p>
<h2>ABSTRACT</h2>
<p>Forecasts of future events are essential inputs into informed decision-making. Machine learning (ML) systems have the potential to deliver forecasts at scale, but there is no framework for evaluating the accuracy of ML systems on a standardized set of forecasting questions. To address this gap, we introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML systems on an automatically generated and regularly updated set of 1,000 forecasting questions. To avoid any possibility of data leakage, ForecastBench is comprised solely of questions about future events that have no known answer at the time of submission. We quantify the capabilities of current ML systems by collecting forecasts from expert (human) forecasters, the general public, and LLMs on a random subset of questions from the benchmark $(N=200)$. While LLMs have achieved super-human performance on many benchmarks, they perform less well here: expert forecasters outperform the top-performing LLM ( $p$-value $&lt;0.001$ ). We display system and human scores in a public leaderboard at www.forecastbench.org.</p>
<h2>1 INTRODUCTION</h2>
<p>Forecasting the future is a challenging but important task (Armstrong, 2001; Tetlock and Gardner, 2015). Economic forecasts influence investment and hiring decisions (Christensen et al., 2018). And forecasts of the Covid-19 pandemic in early 2020 prompted local lockdowns to slow the spread of the virus (Adam, 2020). However, human forecasting is often expensive, time-consuming, applicable only in specific domains, and subject to concerns about human biases. Motivated by these limitations, recent work explores the use of machine learning (ML) models, especially large language models (LLMs), in automated forecasting (Fluri et al., 2024; Halawi et al., 2024; Hendrycks et al., 2021; Phan et al., 2024; Pratt et al., 2024; Yan et al., 2024; Zou et al., 2022).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To assess LLMs' forecasting capabilities, prior work built static benchmarks of questions where the answer was known (resolved) after the knowledge cut-offs of the LLMs they test (Halawi et al., 2024; Yan et al., 2024; Zou et al., 2022). For example, "Will a nuclear weapon be detonated in 2023," though resolved on Jan 1, 2024, is a valid out-of-sample question for testing a model with a known knowledge cut-off before the end of 2023.</p>
<p>This static evaluation methodology comes with three key drawbacks. First, as the knowledge cut-offs of frontier models are updated, static benchmarks become obsolete, necessitating further data-sourcing. This makes it difficult to continuously track and compare the top models in the field. Second, knowledge cut-offs are usually estimated using the time range of pre-training data. Such estimates may not be accurate, and post-training may inject further post-cutoff knowledge into the model, contaminating the test sets. Lastly, model developers face financial incentives to exaggerate their accuracy on benchmarks and claim that their models are state-of-the-art performers. While some fudging is easily identified, other subtle benchmark manipulation or overfitting is harder to catch, and some studies show significant evidence of benchmark contamination and/or memorization by popular LLM models (Elazar et al., 2024; Li et al., 2023; Roberts et al., 2023).</p>
<p>To address these drawbacks, we introduce ForecastBench, a dynamic benchmark that is continuously updated with new questions about future events. Our automated system gathers new questions from nine sources on a daily basis. These sources include prediction markets, forecasting platforms, and real-world time series. We regularly elicit predictions on these questions from both LLMs and human forecasters. As they resolve, we update a public leaderboard to show participant performance. This process makes ForecastBench an accurate real-time benchmark of forecasting ability.</p>
<p>Our initial benchmark consists of 1,000 standardized forecasting questions randomly sampled from a much larger real-time question bank. To establish baseline levels of performance, we record predictions from dozens of LLMs on the initial set, using methods like retrieval-augmentation to boost performance (Halawi et al., 2024; Lewis et al., 2020). We independently elicit predictions from two groups of human forecasters: the general public and seasoned forecasters (superforecasters) (Tetlock and Gardner, 2015) who have consistently performed well in competitive forecasting tournaments. As questions resolve, we rate the submissions of registered models and the human comparison groups, updating our public leaderboard.</p>
<p>Our initial results indicate that state-of-the-art models, such as Claude-3.5 Sonnet and GPT-4 Turbo, perform only roughly as well as a simple median of forecasts from a survey of humans with no (or minimal) forecasting experience, even when the LLMs are augmented with news retrieval and prompt engineering and when they have access to forecasts from a human crowd (on market-based questions). The models also perform significantly worse than the median forecast of superforecasters.</p>
<p>These findings leave significant room for researchers to improve AI-based forecasting systems using innovative approaches, such as developing methods for continuously updating models with current events and enhancing LLMs to reason over extended time frames. To support progress in this area, we publish an auxiliary dataset of LLM and human forecasts, rationales, and accuracy for use in future LLM fine-tuning and testing.</p>
<h1>1.1 RELATED WORK</h1>
<p>Automated forecasting ML systems that make accurate forecasts can help inform human decisionmaking (Hendrycks et al., 2021; Schoenegger et al., 2024a). Recent work studies the use of LLMs for automated forecasting (Halawi et al., 2024; Jin et al., 2021; Pratt et al., 2024; Yan et al., 2024; Zou et al., 2022). These papers all build static benchmarks of resolved questions. A recent paper from Halawi et al. (2024) uses questions resolved between June 2023 and January 2024. Unfortunately, the latest LLMs have knowledge cut-offs past this point. This fact motivates our work to build a dynamically updating benchmark that can accurately evaluate frontier model performance.</p>
<p>In addition, Schoenegger and Park (2023) and Abolghasemi et al. (2023) compare the accuracy of GPT-4 and other LLMs to human forecasters. Schoenegger et al. (2024b) found that an ensemble of 12 LLMs rivaled human forecasts in a forecasting tournament with a small number of questions, limiting the study's statistical power. Unlike our work, that tournament was run only once and is no longer updated. Also, our much larger question set allows us to make precise statistical claims about the performance of LLMs relative to each other and to humans.</p>
<p>Finally, recent work focuses on the use of LLMs and transformer-based systems in statistical timeseries forecasting (Das et al., 2024; Dooley et al., 2023; Goswami et al., 2024; Gruver et al., 2023; Jin et al., 2024; Nie et al., 2023; Rasul et al., 2023; Woo et al., 2024), but many of the most important forecasting questions do not have well-defined time series that can be used in standard statistical forecasting models (e.g., what is the probability that a nuclear weapon will be used offensively in the next ten years?). Our benchmark is more general, and evaluates forecasting performance across questions with and without underlying time series and historical baseline data.</p>
<p>Language model evaluation Evaluating highly capable LLMs is a challenging task-with many models saturating key benchmarks soon after their release (Maslej et al., 2023; Owen, 2024) and with benchmarks potentially leaked to models’ training data (Balloccu et al., 2024; Jacovi et al., 2023; Jiang et al., 2024b; Magar and Schwartz, 2022; Sainz et al., 2023; Xu et al., 2024a;b; Zhang et al., 2024). Our dynamic forecasting benchmark avoids both of these issues. First, automated forecasting is highly unsaturated: Halawi et al. (2024) showed that under simple zero-shot evaluation, frontier models such as GPT-4 are much less accurate than aggregates of human predictions. Second, our benchmark is dynamic. The final resolution of each question is only determined in the future and cannot be leaked in any training data (by construction). This eliminates concerns of contamination.</p>
<h1>2 Preliminaries</h1>
<p>Forecasting A forecasting question asks for a probabilistic prediction of a future event. A forecaster may assign probabilities to potential outcomes of the event. Forecasting platforms, including prediction markets, host a wide range of questions. Many questions are of public interest, such as "Will a Democrat win the 2028 US presidential election?"</p>
<p>Metrics For binary questions, we use the Brier score as the performance metric, defined as $(f-o)^{2}$, where $f \in[0,1]$ is the probabilistic forecast and $o \in{0,1}$ is the outcome. Lower Brier scores are better, and a score of 0.25 is associated with the uninformed forecast of 0.5 . Brier scores are strictly proper, incentivizing truthful reporting from participants.</p>
<p>Models We evaluate 17 LLMs on our initial benchmark: GPT-3.5-Turbo-Instruct (Brown et al., 2020), GPT-4 (OpenAI, 2023), GPT-4o, Llama-2-70B (Touvron et al., 2023), Llama-3-7B, Llama-370B, Mistral-7B (Jiang et al., 2023), Mixtral-8x7B (Jiang et al., 2024a), Mixtral-8x22B, Mistral-Large, Qwen1.5-110B-Chat (Bai et al., 2023), Claude-2.1 (Anthropic, 2023), Claude-3-Haiku, Claude-3.5Sonnet, Claude-3-Opus (Anthropic, 2024), Gemini 1.5 Flash, and Gemini 1.5 Pro (Gemini Team, 2023). We outline the various baselines we run with these models in Section 5.</p>
<h2>3 Benchmark, Leaderboard, and Datasets</h2>
<p>We maintain a question bank with a growing set of forecasting questions. Continuously adding questions to the question bank allows it to stay relevant and ensures that we have a large selection of unresolved questions to sample from.</p>
<p>Every night, our automated system updates the question bank with new questions and resolution values. We drop invalid, low-quality, and resolved questions, categorizing the remaining questions by topic. The process is fully automated, as detailed in Section 3.1.</p>
<p>Every two weeks, we sample from the question bank and release question sets for those interested in submitting their forecasts to the benchmark. We also survey a standard set of LLM-based models to allow for comparisons of performance over time. Submitted forecasts are resolved continuously with daily updates to our leaderboard. We provide the resulting data on forecast questions and submissions to researchers. See Section 3.2 for details. Finally, we discuss the question resolution procedure in Section 3.3 and our leaderboard design in Section 3.4.</p>
<h3>3.1 Question bank</h3>
<p>In an automated process that runs nightly at 0:00 UTC, questions are added to the question bank, resolution values are updated, and metadata generated. Details follow.</p>
<h1>3.1.1 QUESTIONS AND RESOLUTION VALUES</h1>
<p>We bring in questions from two broad types of sources: markets and datasets. ${ }^{1}$ We select multiple, reliable sources from each type, ensuring the diversity of our benchmark. See Table 1 for details.</p>
<p>Markets We compile market questions from a handful of prediction markets and forecast aggregation sites that elicit human predictions on questions across a wide range of topics. ${ }^{2}$ When selecting questions from market sources to add to the question bank, we choose those with high levels of active human trading on these platforms (liquidity) as these questions tend to be of higher quality than those with low levels of human trading. ${ }^{3}$ We store the latest market and resolution values for each question.</p>
<p>Datasets We create dataset questions from well-established and well-maintained datasets that track real-world events (e.g., ACLED (Raleigh et al., 2023), a geopolitical database that tracks worldwide conflict, including facts like the number of protests in Niger each month). With these sources, we can generate questions using a fixed question template (e.g., "Will the number of protests in Niger increase by at least $10 \%$ over this month's value by the resolution date?"). ${ }^{4}$</p>
<p>Question Bank Table 1 lists the sources of market and dataset questions, along with the number of questions available in our question bank, whence we sample questions for our benchmark runs. In addition to the main questions (with sample size $N$ ), we also construct additional combination questions by choosing pairs of questions within each source. We describe combination questions in more detail in Section 3.2.</p>
<p>Table 1: Question bank composition, grouped by source type (market or dataset).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">URL</th>
<th style="text-align: right;">$N$</th>
<th style="text-align: right;">$\binom{N}{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RFI</td>
<td style="text-align: left;">randforecastinginitiative.org</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">153</td>
</tr>
<tr>
<td style="text-align: left;">Manifold Markets</td>
<td style="text-align: left;">manifold.markets</td>
<td style="text-align: right;">405</td>
<td style="text-align: right;">81,810</td>
</tr>
<tr>
<td style="text-align: left;">Metaculus</td>
<td style="text-align: left;">metaculus.com</td>
<td style="text-align: right;">722</td>
<td style="text-align: right;">260,281</td>
</tr>
<tr>
<td style="text-align: left;">Polymarket</td>
<td style="text-align: left;">polymarket.com</td>
<td style="text-align: right;">915</td>
<td style="text-align: right;">418,155</td>
</tr>
<tr>
<td style="text-align: left;">Market Total</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">2,060</td>
<td style="text-align: right;">760,399</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">ACLED</td>
<td style="text-align: left;">acleddata.com</td>
<td style="text-align: right;">3,220</td>
<td style="text-align: right;">$5,182,590$</td>
</tr>
<tr>
<td style="text-align: left;">DBnomics</td>
<td style="text-align: left;">db.nomics.world</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">1,326</td>
</tr>
<tr>
<td style="text-align: left;">FRED</td>
<td style="text-align: left;">fred.stlouisfed.org</td>
<td style="text-align: right;">166</td>
<td style="text-align: right;">13,695</td>
</tr>
<tr>
<td style="text-align: left;">Wikipedia</td>
<td style="text-align: left;">wikipedia.org</td>
<td style="text-align: right;">428</td>
<td style="text-align: right;">91,378</td>
</tr>
<tr>
<td style="text-align: left;">Yahoo! Finance</td>
<td style="text-align: left;">finance.yahoo.com</td>
<td style="text-align: right;">509</td>
<td style="text-align: right;">129,286</td>
</tr>
<tr>
<td style="text-align: left;">Dataset Total</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">4,375</td>
<td style="text-align: right;">$5,418,275$</td>
</tr>
<tr>
<td style="text-align: left;">Question Bank Total</td>
<td style="text-align: left;"></td>
<td style="text-align: right;">6,435</td>
<td style="text-align: right;">$6,178,674$</td>
</tr>
</tbody>
</table>
<h3>3.1.2 QUESTION METADATA</h3>
<p>After we automatically update the question bank with new forecasting questions and resolution values, the data are processed in several ways. This generates more information about the questions and creates another sampling option when creating the question set. Following Halawi et al. (2024), we use gpt-40-mini to categorize questions by subject and to filter out low-quality questions. We report the number of questions by category and source in Table 15, where we display the breakdown of the standard questions from our question bank ( $N$ from Table 1) across question categories by source, after removing low-quality questions.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.2 QUESTION SETS</h1>
<p>LLM question set We release a set of 1,000 forecast questions for LLMs every other Sunday at midnight UTC. We sample an equal number of questions from each source to ensure representativeness. Within each source, we then uniformly sample questions across all question categories, aiming for an equal distribution from each category within each source. This ensures that models cannot be overfit to a specific type of question or topic. Limiting the number of questions generated to 1,000 also ensures that costs for testing LLMs on the benchmark are capped for development teams with fewer resources.</p>
<p>Human question set The human question set is comprised of 200 forecast questions sampled directly from the LLM question set. When sampling, we do our best to maintain proportionality across question sources and across categories within each question source; this ensures the question set addresses as many domains as possible.</p>
<p>Forecast horizons For questions derived from dataset sources, the distribution of resolution dates is the forecast_due_date $+n$ days, where $n \in{7,30,90,180,365,1095,1825,3650}$. In other words, for each dataset question we ask for 8 forecasts that differ only in their resolution date. For questions derived from market sources, we ask for only 1 forecast: the probability that each question will resolve positively (will the event underlying the question occur, or not). This setup will allow us to evaluate both human and LLM forecasting performance over the short, medium, and long term.</p>
<p>Combination questions There are two types of questions, each comprising half of the question set. The first type is a standard forecasting question with a binary outcome, e.g., "Will inflation (core CPI) be above 3\% next month?" We construct the second type, combination questions, by pairing two standard questions. For combination questions, we ask for forecasts on all Boolean combinations of the two questions (i.e., $P(Q 1 \cap Q 2), P(Q 1 \cap \neg Q 2), \ldots)$. Considering the extensive number of standard questions and potential combinations in our question bank (as we could potentially combine 3, 4, or more standard questions together), we effectively have access to millions of possible forecasting questions from which we can sample. We show the number of two-question combination questions in the question bank as it stands at time of writing in the right-hand column of Table 1. This setup implies that for market combination questions, each forecaster will provide 4 forecasts, whereas for dataset combination questions, each forecaster will provide 32 forecasts ( 4 for each Boolean combination of $Q 1$ and $Q 2$ at each of the 8 forecast horizons).
Combination questions require forecasters to consider the covariance structure of different events, some of which are more independent than others. For instance, the best forecasts of whether the S\&amp;P500 (a key U.S. stock market index) will reach an all-time high and whether Spain will win the next Men's World Cup are likely independent. However, the best forecast of whether the S\&amp;P500 will reach an all-time high and whether the U.S. will enter an economic recession must account for the likely strong correlation between these events.</p>
<p>Of the 1,000 questions in the LLM question set, 500 are combination questions. Each combination question is composed of two standard questions from the same question set. This means that LLMs will also provide forecasts for the individual components of these combination questions, since they're drawn from the existing standard questions. Importantly, none of the 200 questions in the human question set are combination questions. ${ }^{5}$</p>
<p>Timeline of forecasting round To compare human performance to LLMs, we periodically run surveys asking the general public and superforecasters to forecast on our question sets (see Section 4). We produce the question sets 10 days before the forecast due date to allow for time to create and run a human survey. LLM teams receive their question set 24 hours before the due date, even though it was generated at the same time as the human question set. This constrained time frame gives teams less time to be able to game the system. We thus elicit forecasts, obtaining comparable forecast sets on the due date from both LLMs and humans who faced the same information environment.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3 RESOLUTION</h1>
<p>We resolve forecast sets nightly by gathering the latest information about which events have or have not occurred. All questions are ultimately resolved to ground truth.</p>
<p>Evaluating performance on market questions For market questions, ground truth is not available until the question has been resolved on the platform. Until then, we evaluate performance by calculating the squared distance between the forecasted value and the platform's crowd forecast (an aggregate of human forecasts reported on the platform) from the preceding day. ${ }^{6}$ This provides a good estimate of forecast performance on unresolved questions since crowd forecasts tend toward ground truth as the resolution date approaches.</p>
<p>Once a market question has officially been resolved, we score the forecast against the resolution value, creating a definitive score for the question. We are thus able to estimate forecast performance on the entire set of market questions (resolved and unresolved), incorporating all information available to markets on a nightly basis.</p>
<p>Evaluating performance on dataset questions Datasets can be updated as new information becomes available. Hence, questions derived from datasets are continuously resolved to the value from the latest available data. As the resolution dates (ranging from a 7-day to a 10-year horizon) for dataset questions come due, a new round of forecasts is evaluated. We thus are able to evaluate forecasting performance over different time horizons.</p>
<p>Missing forecasts We select 1,000 questions for the LLM question set to make the forecasting task impractical to complete manually within the 24 -hour window after the question set is released. And we obligate all LLMs to forecast on all questions to ensure comparability of scores across models and human-based aggregates. When a model does not submit forecasts on certain questions or time horizons, we consider that a model error and impute a naive forecast for the model to ensure comparability across models over time.</p>
<p>For market questions, since we only ask for forecasts on the outcome of the question, missing forecasts are assigned the value of the crowd forecast on the forecast due date. Some may argue this is overly-generous, but we did not want forecasters to have a competitive advantage by simply scraping the market websites themselves.</p>
<p>For dataset questions, we impute the value 0.5 (which represents an uninformed $50 \%$ forecast) to forecasts across all time horizons. Empirically, top models report valid forecasts on all questions and are not affected by this imputation procedure.</p>
<h3>3.4 LEADERBOARDS</h3>
<p>We generate leaderboards that rank models and humans by average overall score. The main leaderboard highlights the top forecasting submission across all questions and can by sorted by performance on the question type (market or dataset) and by resolution status (resolved or unresolved). The leaderboard is updated nightly after scoring forecasts against the latest data, market resolution values, and crowd forecasts.</p>
<h3>3.5 DATASETS</h3>
<p>As a key output of ForecastBench, we generate four datasets that grow over time. ${ }^{7}$</p>
<p>General public forecast dataset Every time we run the public survey described in Section 4, we provide multiple independent forecasts and rationales for every one of the 200 forecast questions and report the accuracy of the median public forecast. See Section B.2.1 for details.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Superforecaster forecast dataset In addition to forecasts and rationales, the superforecasters provide pertinent information about their forecasting process, like search terms used and useful URLs consulted. See Section B.2.2.</p>
<p>LLM forecast dataset Similar to the general public dataset, we ask LLMs to produce forecasts on each of 1,000 forecast questions in the LLM question set. Their rationales are also included in the dataset whenever provided. See Section B.2.3 for details.</p>
<p>Question \&amp; resolutions dataset In creating the benchmark, we have automated question creation and resolution from all of the sources outlined in Table 1. We provide these as a dataset that can be combined with the forecast datasets mentioned above. See Section B. 1 for details.</p>
<h1>4 HUMAN FORECASTER BASELINE</h1>
<p>To compare LLM forecasting performance to human performance, we ran surveys of two different groups: the general public and superforecasters. We scored each group's median forecast, treating it as representative of its overall performance.</p>
<h3>4.1 GENERAL PUBLIC</h3>
<p>We recruited 500 human forecasters via Prolific and advertisements on Facebook to participate as representatives of the general public. These human subjects completed a brief introductory survey to gather demographic information ${ }^{8}$ and evaluate performance on a few forecasting and comprehension tasks. They then completed a one-hour survey containing 20 random questions from the 200-item human question set described in Section 3.2, providing their forecasts and rationales for each question.
The number of responses per question varied to ensure representativeness across categories and sources; at least 40 responses were gathered per question, averaging 49 responses per question.</p>
<h3>4.2 SUPERFORECASTERS</h3>
<p>We recruited 39 superforecasters, who have a strong track record of accurate performance on a diverse set of geopolitical questions, to participate in a 9-day forecasting experiment in which participants were prompted to give their individual forecasts for 20 random questions from the same 200-item human question set described above. Roughly halfway through the 9-day experiment, participants were moved into a group forecasting stage in which we allowed them to see one another's forecasts and rationales and to communicate about each question. They were also given the opportunity to forecast on questions beyond the 20 questions assigned to them individually.
Because of the lower number of superforecasters, questions generally had fewer responses than in the public survey; a minimum of 3 forecasts were recorded for each question, with an average of 8 responses per question. ${ }^{9}$</p>
<h2>5 LLM BASELINE</h2>
<p>In this section, we evaluate the forecasting capabilities of LLMs and report on the methodology and results.</p>
<h3>5.1 Methodology</h3>
<p>We evaluate a suite of instruction-following chat models without any additional fine-tuning (see Section 2 for details on the models). For each baseline outlined below, we prompt the model to generate a probabilistic forecast that the question will resolve to "Yes."</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Baselines We implement seven baselines: (1) zero-shot prompting; (2) prompting with scratchpad instructions; (3) prompting with scratchpad instructions and retrieved news articles; (4) zero-shot prompting with crowd forecasts; (5) scratchpad prompting with crowd forecasts; (6) scratchpad prompting with retrieved news articles and crowd forecasts; and (7) aggregating predictions from multiple LLMs. Each baseline is described in more detail below.</p>
<p>1 Our first baseline prompts the model zero-shot to generate a forecast directly without generating other content, such as intermediate thinking (Figure 4). By prompting the model to output its forecast directly, we assess raw forecasting capability without sensitivity to prompting strategies.
2 Our second baseline, prompts the model with scratchpad instructions (Nye et al., 2021) that outline a procedure the model should use to reason about the question (Figure 5). Our scratchpad prompt comes from (Halawi et al., 2024), which formed its prompts through a combination of analyzing the Brier score as prompt changes were made, and by adding language to fix common errors the LLMs would make, e.g., asking them to rephrase the question for understanding.
3 Since LLMs' knowledge is not continuously updated, it is important to provide them with up-to-date information relevant to the question (Zou et al., 2022). Our fourth baseline, scratchpad with news, uses the same scratchpad prompt as above, supplemented with retrieved news articles. The retrieval system is the same as described in Halawi et al. (2024): an LLM generates search queries for a news API, filters articles for relevancy, and summarizes the articles.
4 The question sets we provide to LLMs contain what we term freeze values. For market questions these are just the crowd forecast on the market the day the question set was created, as described in Section 3.2. For dataset questions, these are baseline values relevant to the forecasting task. ${ }^{10}$ Our third baseline is the zero-shot with freeze values. This is simply the zero-shot prompt from Baseline 1 supplemented with the freeze value and an explanation of the freeze value. For examples of the freeze value and its explanation, see Table 8 and Table 9.
5 Our fifth baseline is the scratchpad with freeze values (the scratchpad prompt from Baseline 2 supplemented with freeze values as explained in Baseline 4).
6 Our sixth baseline is the scratchpad with news with freeze values.
7 In our final baseline, we aggregate the predictions generated by LLMs into an LLM "ensemble" forecast. We do this as Metaculus (2023) shows that an ensemble of all forecasters consistently outperforms using just the top $5,10, \ldots, 30$ best forecasters (based on past scores).
To produce the LLM ensemble forecast, we use 3 models (GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro) and 3 prompts crafted by superforecasters (Figure 6, Figure 7, and Figure 8). This results in 9 forecasts per question. We generate 3 LLM ensemble baselines using the median, geometric mean, and geometric mean of log odds (Satopää et al., 2014). For details, see Appendix E.</p>
<h1>5.2 RESULTS</h1>
<p>Comparing humans and LLMs In Table 2, we show that superforecasters achieve an overall mean Brier score of 0.096 , significantly outperforming both the general public (Brier $=0.121, p&lt;0.001$ ) and the top LLM performer on the 200-item subset (Claude 3.5 Sonnet: Brier $=0.122, p&lt;0.001$ ). ${ }^{11}$ The top-performing LLMs all had access to the crowd forecast on market questions (the "freeze values" from Baselines 4, 5, and 6 above). The top-performing model without access to the crowd forecast on market questions was less accurate than models with access to the human forecast with a Brier score of 0.136 . The comparison between humans and LLMs relies on the 200 questions forecasted by humans, which is a random sub-sample of the 1,000 questions in the question set provided to LLMs (excluding combination questions). ${ }^{12}$</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: LLM/Human Leaderboard (top 10)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Organization</th>
<th>Information provided</th>
<th>Prompt</th>
<th>Brier Score $\downarrow$</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Dataset <br> $(N=422)$</td>
<td>Market <br> $(N=76)$</td>
<td>Overall <br> $(N=498)$</td>
<td>Confidence Interval</td>
<td>Pairwise <br> $p$-value comparing to No. 1</td>
<td>Pct. more accurate than No. 1</td>
</tr>
<tr>
<td>Superforecaster median forecast</td>
<td>ForecastBench</td>
<td>-</td>
<td>-</td>
<td>0.118</td>
<td>0.074</td>
<td>0.096</td>
<td>$[0.076,0.116]$</td>
<td>-</td>
<td>0\%</td>
</tr>
<tr>
<td>Public median forecast</td>
<td>ForecastBench</td>
<td>-</td>
<td>-</td>
<td>0.153</td>
<td>0.089</td>
<td>0.121</td>
<td>$[0.101,0.141]$</td>
<td>$&lt;0.001$</td>
<td>$22 \%$</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.138</td>
<td>0.107</td>
<td>0.122</td>
<td>$[0.099,0.146]$</td>
<td>$&lt;0.001$</td>
<td>$31 \%$</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>News with freeze values</td>
<td>Scratchpad</td>
<td>0.142</td>
<td>0.112</td>
<td>0.127</td>
<td>$[0.104,0.150]$</td>
<td>$&lt;0.001$</td>
<td>$29 \%$</td>
</tr>
<tr>
<td>GPT-4-Turbo-2024-04-09</td>
<td>OpenAI</td>
<td>Freeze values</td>
<td>Zero shot</td>
<td>0.162</td>
<td>0.095</td>
<td>0.128</td>
<td>$[0.105,0.151]$</td>
<td>$&lt;0.001$</td>
<td>$32 \%$</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>Freeze values</td>
<td>Zero shot</td>
<td>0.145</td>
<td>0.117</td>
<td>0.131</td>
<td>$[0.103,0.159]$</td>
<td>$&lt;0.001$</td>
<td>$31 \%$</td>
</tr>
<tr>
<td>GPT-4</td>
<td>OpenAI</td>
<td>Freeze values</td>
<td>Zero shot</td>
<td>0.167</td>
<td>0.096</td>
<td>0.132</td>
<td>$[0.109,0.155]$</td>
<td>$&lt;0.001$</td>
<td>$31 \%$</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>OpenAI</td>
<td>News with freeze values</td>
<td>Scratchpad</td>
<td>0.162</td>
<td>0.105</td>
<td>0.133</td>
<td>$[0.113,0.154]$</td>
<td>$&lt;0.001$</td>
<td>$25 \%$</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>-</td>
<td>Scratchpad</td>
<td>0.138</td>
<td>0.133</td>
<td>0.136</td>
<td>$[0.113,0.158]$</td>
<td>$&lt;0.001$</td>
<td>$28 \%$</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>OpenAI</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.161</td>
<td>0.113</td>
<td>0.137</td>
<td>$[0.115,0.158]$</td>
<td>$&lt;0.001$</td>
<td>$27 \%$</td>
</tr>
</tbody>
</table>
<p>Notes:</p>
<ol>
<li>Shows performance on the 200 standard questions provided in the human question set at the $7-, 30-, 90-$, and 180-day forecast horizons. See Table 18 for top 50.</li>
<li>The full leaderboard is available at www.forecastbench.org. Online results are updated nightly, so may be slightly different than the version presented here.</li>
<li>For resolved market questions, forecasts are compared against ground truth while for unresolved market questions, they are compared to community aggregates.</li>
<li>The overall score is calculated as the average of the mean dataset Brier score and the mean market Brier score.</li>
<li>Pairwise $p$-value comparing to No. 1 (bootstrapped): The $p$-value calculated by bootstrapping the differences in overall score between each model and the best forecaster under the null hypothesis that there's no difference.</li>
<li>Pct. more accurate than No. 1: The percent of questions where this forecaster had a better overall score than the best forecaster.</li>
</ol>
<p>As a particular failure mode, we find LLMs are significantly worse at combination questions. Although our human surveys did not explicitly ask for forecasts on combination questions, we bound human performance by assuming independence of the component of each combination question. This underestimates human accuracy because a human forecaster predicting the outcome of a combination question could account for dependence between the permuted events. In Table 20, we present this comparison of human and LLM forecasts. We see that LLMs perform poorly on these combination questions, and including them in the benchmark widens the gap between human and LLM performance: superforecasters (Brier $=0.076$ ) outperform the general public (Brier $=0.096$ ) and the top LLM (GPT-4o, Brier $=0.130$ ) significantly. To benchmark the size of this gap in performance, the 0.054 Brier score gap in performance between superforecasters and GPT-4o is significantly larger than the 0.026 gap in performance between GPT-4o and GPT-4.</p>
<p>Table 3: LLM Leaderboard (top 10)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Organization</th>
<th>Information provided</th>
<th>Prompt</th>
<th>Brier Score $\downarrow$</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Dataset <br> $(N=5,492)$</td>
<td>Market <br> $(N=897)$</td>
<td>Overall <br> $(N=6,389)$</td>
<td>Confidence Interval</td>
<td>Pairwise <br> $p$-value comparing to No. 1</td>
<td>Pct. more accurate than No. 1</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.169</td>
<td>0.078</td>
<td>0.123</td>
<td>$[0.117,0.129]$</td>
<td>-</td>
<td>0\%</td>
</tr>
<tr>
<td>GPT-4-Turbo-2024-04-09</td>
<td>OpenAI</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.172</td>
<td>0.080</td>
<td>0.126</td>
<td>$[0.120,0.132]$</td>
<td>0.096</td>
<td>$43 \%$</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>OpenAI</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.186</td>
<td>0.069</td>
<td>0.128</td>
<td>$[0.122,0.133]$</td>
<td>$&lt;0.01$</td>
<td>$43 \%$</td>
</tr>
<tr>
<td>Gemini-1.5-Pro</td>
<td>Google</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.162</td>
<td>0.106</td>
<td>0.134</td>
<td>$[0.128,0.139]$</td>
<td>$&lt;0.001$</td>
<td>$35 \%$</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>OpenAI</td>
<td>News with freeze values</td>
<td>Scratchpad</td>
<td>0.190</td>
<td>0.084</td>
<td>0.137</td>
<td>$[0.131,0.143]$</td>
<td>$&lt;0.001$</td>
<td>$39 \%$</td>
</tr>
<tr>
<td>Gemini-1.5-Pro</td>
<td>Google</td>
<td>News with freeze values</td>
<td>Scratchpad</td>
<td>0.166</td>
<td>0.111</td>
<td>0.139</td>
<td>$[0.133,0.144]$</td>
<td>$&lt;0.001$</td>
<td>$34 \%$</td>
</tr>
<tr>
<td>Claude-3-Opus-20240229</td>
<td>Anthropic</td>
<td>Freeze values</td>
<td>Zero shot</td>
<td>0.186</td>
<td>0.093</td>
<td>0.139</td>
<td>$[0.133,0.146]$</td>
<td>$&lt;0.001$</td>
<td>$41 \%$</td>
</tr>
<tr>
<td>Qwen1.5-110B-Chat</td>
<td>Qwen</td>
<td>Freeze values</td>
<td>Scratchpad</td>
<td>0.176</td>
<td>0.108</td>
<td>0.142</td>
<td>$[0.136,0.148]$</td>
<td>$&lt;0.001$</td>
<td>$30 \%$</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>News with freeze values</td>
<td>Scratchpad</td>
<td>0.184</td>
<td>0.101</td>
<td>0.143</td>
<td>$[0.137,0.149]$</td>
<td>$&lt;0.001$</td>
<td>$32 \%$</td>
</tr>
<tr>
<td>Claude-3-5-Sonnet-20240620</td>
<td>Anthropic</td>
<td>Freeze values</td>
<td>Zero shot</td>
<td>0.192</td>
<td>0.094</td>
<td>0.143</td>
<td>$[0.136,0.150]$</td>
<td>$&lt;0.001$</td>
<td>$42 \%$</td>
</tr>
</tbody>
</table>
<p>Notes:</p>
<ol>
<li>Shows performance on the 1,000 (500 standard, 500 combination) questions in the LLM question set at the $7-, 30-, 90-$, and 180-day forecast horizons. See Table 19 for top 50 .</li>
<li>The full leaderboard is available at www.forecastbench.org. Online results are updated nightly, so may be slightly different than the version presented here.</li>
<li>For resolved market questions, forecasts are compared against ground truth while for unresolved market questions, they are compared to community aggregates.</li>
<li>The overall score is calculated as the average of the mean dataset Brier score and the mean market Brier score.</li>
<li>Pairwise $p$-value comparing to No. 1 (bootstrapped): The $p$-value calculated by bootstrapping the differences in overall score between each model and the best forecaster under the null hypothesis that there's no difference.</li>
<li>Pct. more accurate than No. 1: The percent of questions where this forecaster had a better overall score than the best forecaster.</li>
</ol>
<p>Comparing LLMs Table 3 excludes humans and evaluates LLMs on the entire question set ( $N=1,000$ questions). Here we see a similar ranking of models, with Claude 3.5 Sonnet slightly outperforming GPT-4 Turbo. As in Table 2, most of the top-performing models use the scratchpad prompt (Figure 5) and use as inputs the human crowd forecasts for market questions. Access to recent topical news related to the questions did not improve performance.</p>
<p>LLM performance and forecasting accuracy Figure 1a demonstrates the seemingly linear relationship between Chatbot Arena scores (Chiang et al., 2024) and the overall Brier score from Table 2. We observe a significant correlation $(r=-0.68, p=0.003)$, indicating that models with higher Arena scores tend to produce more accurate forecasts. The linear relationship implies that LLMs</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The graphs show the linear relationship between the Brier scores from Table 2 and (a) Chatbot Arena scores and (b) estimates of training compute. The dotted blue line represents the Superforecasters' overall Brier score. A red dot with a bootstrapped 95\% confidence interval is placed at the intersection of this dotted blue line with the dashed linear fit line to demonstrate the potential intersection of LLM Arena score/training compute and Superforecaster-level forecasting performance. For (b), if estimates from Epoch AI (2024) were not available, we produced estimates following https://epoch.ai/blog/estimating-training-compute. The trend-line in (a) is $y=0.506-0.000298 x\left(R^{2}=0.47\right)$ and in (b) it is $y=0.844-0.01213 x\left(R^{2}=0.41\right)$.
could match superforecaster performance when the Arena score approaches 1406 (bootstrapped 95\% CI: $1346-1633$ ).</p>
<p>Figure 1b shows the log-linear relationship between estimated training compute and the overall Brier score from Table 2. Projecting out the log-linear relationship, we find that LLMs could match superforecaster performance when training compute approaches $6.49 \times 10^{26}$, though there is a large confidence interval (bootstrapped 95\% CI: $9.69 \times 10^{25}-8.65 \times 10^{28}$ ) given the marginally significant relationship $(r=-0.67, p=0.046)$.</p>
<h1>6 DISCUSSION</h1>
<p>We introduced ForecastBench, a dynamic and continuously updated benchmark for evaluating LLM forecasting capabilities. By focusing exclusively on questions that are unresolved at the time of submission, we eliminate the risks of data leakage and ensure a robust evaluation environment. Our initial results demonstrate that while state-of-the-art LLMs exhibit promising potential, they underperform superforecasters. This performance gap highlights the challenges in leveraging current LLMs for accurate, real-time forecasting.</p>
<p>We produce a public leaderboard listing the real-time accuracy of top LLMs and humans as well as a standardized dataset of forecasting questions and rationales. Future work should leverage this auxiliary dataset of predictions and rationales to fine-tune models, explore new architectures, and develop adaptive systems better suited for general reasoning in dynamic, real-world environments. Ultimately, ForecastBench serves as a step toward harnessing the full potential of AI-based systems for forecasting and decision-making.</p>
<h2>7 REPRODUCIbILITY STATEMENT</h2>
<p>One reason we've open-sourced our code (link in Appendix A) is to allow for independent verification of our results. See Appendix I for reproducing the human forecast sets, Appendix J for reproducing LLM forecast sets, and Appendix K for resolving the forecasts and creating the leaderboard.</p>
<h1>8 ETHICS STATEMENT</h1>
<p>Human survey subjects in both the public and superforecaster surveys are made aware prior to their participation in the study via an informed consent form (approved by our IRB, number 855431) that their forecast/rationale data may be publicly released and used to train large language models or other AI systems, with said data carefully reviewed and anonymized.
We have manually reviewed text provided by human participants to ensure that no personally identifiable information is released as part of our human forecast datasets, per IRB requirements. Similar manual reviews of text data will take place as part of every future human forecasting round.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Open Philanthropy for providing a grant to fund this work, allowing us to maintain the benchmark at least until mid-2027.</p>
<p>We thank Otto Kuusela for helpful discussions, Sam Glover and Molly Hickman for writing superforecaster prompts, and Harrison Durland and Victoria Schmidt for their research assistance.</p>
<h2>REFERENCES</h2>
<p>Mahdi Abolghasemi, Odkhishig Ganbold, and Kristian Rotaru. Humans vs large language models: Judgmental forecasting in an era of advanced AI. arXiv preprint arXiv:2312.06941, 2023.</p>
<p>David Adam. Special report: The simulations driving the world's response to COVID-19. Nature, 580(7802):316-319, 2020.</p>
<p>Anthropic. Model card and evaluations for Claude models, 2023. https://www-cdn.anthropic.com/files/4zrzovbb/website/ 5c49cc247484cecf107c699baf29250302e5da70.pdf.</p>
<p>Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf.</p>
<p>Jon Scott Armstrong. Principles of Forecasting: a Handbook for Researchers and Practitioners. Springer, 2001.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609.</p>
<p>Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondřej Dušek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2024.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL https://arxiv.org/ abs/2403.04132.</p>
<p>Peter Christensen, Kenneth Gillingham, and William Nordhaus. Uncertainty in forecasts of long-run economic growth. Proceedings of the National Academy of Sciences, 115(21):5409-5414, 2018.</p>
<p>Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting. In International Conference on Machine Learning (ICML), 2024.</p>
<p>Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha Venkat Naidu, and Colin White. ForecastPFN: Synthetically-trained zero-shot forecasting. In Advanced in Neural Information Processing Systems (NeurIPS), 2023.</p>
<p>Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. What's in my big data? In The Twelfth International Conference on Learning Representations (ICLR), 2024.</p>
<p>Epoch AI. Data on notable ai models, 2024. URL https://epoch.ai/data/ notable-ai-models. Accessed: 2024-11-22.</p>
<p>Lukas Fluri, Daniel Paleka, and Florian Tramer. Evaluating superhuman models with consistency checks. IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), 2024.</p>
<p>Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.</p>
<p>Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: A family of open time-series foundation models. In International Conference on Machine Learning (ICML), 2024.</p>
<p>Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Advanced in Neural Information Processing Systems (NeurIPS), 2023.</p>
<p>Danny Halawi, Fred Zhang, Chen Yueh-Han, and Jacob Steinhardt. Approaching human-level forecasting with language models. arXiv preprint arXiv:2402.18563, 2024.</p>
<p>Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ML safety. arXiv preprint arXiv:2109.13916, 2021.</p>
<p>Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5075-5084, 2023.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv. org/abs/2310.06825.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a.</p>
<p>Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. Investigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059, 2024b.</p>
<p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In International Conference on Learning Representations (ICLR), 2024.</p>
<p>Woojeong Jin, Rahul Khanna, Suji Kim, Dong-Ho Lee, Fred Morstatter, Aram Galstyan, and Xiang Ren. ForecastQA: A question answering challenge for event forecasting with temporal text data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL), 2021.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Yucheng Li, Frank Guerin, and Chenghua Lin. An open source data contamination report for large language models. arXiv preprint arXiv:2310.17589, 2023.</p>
<p>Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2022.</p>
<p>Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault. Artificial intelligence index report 2023. arXiv preprint arXiv:2310.03715, 2023.</p>
<p>Barbara Mellers, Eric Stone, Terry Murray, Angela Minster, Nick Rohrbaugh, Michael Bishop, Eva Chen, Joshua Baker, Yuan Hou, Michael Horowitz, Lyle Ungar, and Philip Tetlock. Identifying and cultivating superforecasters as a method of improving probabilistic predictions. Perspectives on Psychological Science, 2015.</p>
<p>Metaculus. Wisdom of the crowd vs. the best of the best of the best, 2023. URL https://www.metaculus.com/notebooks/15760/ wisdom-of-the-crowd-vs-the-best-of-the-best-of-the-best/.</p>
<p>Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
David Owen. How predictable is language model benchmark performance? arXiv preprint arXiv:2401.04757, 2024.</p>
<p>Long Phan, Andrew Zeng, Mantas Mazeika, Adam Khoja, and Dan Hendrycks. Superhuman automated forecasting. https://www.safe.ai/blog/forecasting, 2024.</p>
<p>Sarah Pratt, Seth Blumberg, Pietro Kreitlon Carolino, and Meredith Ringel Morris. Can language models use forecasting strategies? arXiv preprint arXiv:2406.04446, 2024.</p>
<p>Clionadh Raleigh, Roudabeh Kishi, and Andrew Linke. Political instability patterns are obscured by conflict dataset scope conditions, sources, and coding choices. Humanities and Social Sciences Communications, 10:74, 2023.</p>
<p>Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, and Irina Rish. Lag-Llama: Towards foundation models for time series forecasting. arXiv preprint arXiv:2310.08278, 2023.</p>
<p>Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. Data contamination through the lens of time. arXiv preprint arXiv:2310.10628, 2023.</p>
<p>Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. NLP evaluation in trouble: On the need to measure llm data contamination for each benchmark. In Findings of the Association for Computational Linguistics (Findings of EMNLP), 2023.</p>
<p>Ville A. Satopää, Jonathan Baron, Dean P. Foster, Barbara A. Mellers, Philip E. Tetlock, and Lyle H. Ungar. Combining multiple probability predictions using a simple logit model. International Journal of Forecasting, 30(2):344-356, 2014.</p>
<p>Philipp Schoenegger and Peter S Park. Large language model prediction capabilities: Evidence from a real-world forecasting tournament. arXiv preprint arXiv:2310.13014, 2023.</p>
<p>Philipp Schoenegger, Peter S Park, Ezra Karger, and Philip E Tetlock. AI-augmented predictions: LLM assistants improve human forecasting accuracy. arXiv preprint arXiv:2402.07862, 2024a.</p>
<p>Philipp Schoenegger, Indre Tuminauskaite, Peter S Park, and Philip E Tetlock. Wisdom of the silicon crowd: LLM ensemble prediction capabilities match human crowd accuracy. arXiv preprint arXiv:2402.19379, 2024b.</p>
<p>Philip E. Tetlock and Dan Gardner. Superforecasting: The Art and Science of Prediction. Crown, 2015.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified training of universal time series forecasting transformers. In International Conference on Machine Learning (ICML), 2024.</p>
<p>Cheng Xu, Shuhao Guan, Derek Greene, M Kechadi, et al. Benchmark data contamination of large language models: A survey. arXiv preprint arXiv:2406.04244, 2024a.</p>
<p>Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024b.</p>
<p>Qi Yan, Raihan Seraj, Jiawei He, Lili Meng, and Tristan Sylvain. Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval. In International Conference on Learning Representations (ICLR), 2024.</p>
<p>Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. A careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332, 2024.</p>
<p>Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, and Dan Hendrycks. Forecasting future world events with neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p>
<h1>A Licensing and MAINTENANCE</h1>
<p>Hosting The latest leaderboards, updated nightly, are available on www.forecastbench.org. Documentation is available on the repository wiki: github.com/forecastingresearch/forecastbench/wiki.</p>
<p>Datasets Our datasets, distributed under the CC BY-SA 4.0 license, are available on www.forecastbench.org/datasets.html. Historical updates to the resolution datasets and leaderboards are available on github.com/forecastingresearch/forecastbench-datasets. Bi-weekly question sets are also released via this repository.</p>
<p>Codebase The code underlying our automated system runs on Google Cloud Platform and is available at github.com/forecastingresearch/forecastbench under the MIT license.</p>
<p>Participating Bi-weekly forecasting rounds are open to LLM teams. Instructions for participating are can be found on the wiki.</p>
<p>Maintenance \&amp; long-term preservation We ensure the long-term availability and maintenance of the benchmark as it is funded by Open Philanthropy until mid-2027. If no further funding is provided beyond that point, datasets will continue to be made available on GitHub.</p>
<h2>B DATASETS</h2>
<p>We intend our datasets to be used for training general LLMs, fine-tuning forecasting LLMs, and for any applicable research purposes. No restrictions are placed on who may use our datasets, nor to what end.</p>
<p>Availability www.forecastbench.org/datasets.html</p>
<h2>B. 1 QUESTION AND RESOLUTION SETS</h2>
<p>Every question set will be published. Their resolutions will also be published such that there's a complete training set when combined with the forecast sets outlined in Section B.2. The data dictionary for the question set is outlined in Table 4 and Table 5. The data dictionary for the resolution set is outlined in Table 6 and Table 7.</p>
<p>Data format The question and resolution datasets are released as JSON (. json) files.
Ethical and responsible use There are no restrictions on use of the question and resolution datasets.
Data collection Our question and resolution datasets have been pulled, and are updated, from various, public-facing sources. From those sources where the terms of use/service prohibit the redistribution of their information (currently, Manifold Markets and Metaculus), we have obtained explicit permission to do so. Before we add new sources to our growing dataset, we will ensure the ability to distribute questions and resolutions publicly. Data sources in our question bank can be found in Table 14.</p>
<p>Table 4: Question set data dictionary.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Required</th>
<th style="text-align: center;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">forecast_due</td>
<td style="text-align: center;">Date in ISO format. e.g.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">_date</td>
<td style="text-align: center;">"2024-07-21"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">question_set</td>
<td style="text-align: center;">The name of the file that contains the question set. e.g. "2024-07-21-11m.json"</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">questions</td>
<td style="text-align: center;">A list of questions to forecast on, as defined in Table 5.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">array<object></td>
</tr>
</tbody>
</table>
<p>Table 5: Data dictionary for question entries in questions array from Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Required</th>
<th style="text-align: center;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">id</td>
<td style="text-align: center;">A unique identifier string given source. If instead of a string it's an array of strings, then this is a combination question and combination_of will contain one question per id in the array of strings.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string I array<string></td>
</tr>
<tr>
<td style="text-align: center;">source</td>
<td style="text-align: center;">Where the data comes from.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">question</td>
<td style="text-align: center;">The question to forecast, presented as an f-string with placeholders {forecast_due_date} and {resolution_date} for dataset questions.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">resolution_ criteria</td>
<td style="text-align: center;">ForecastBench resolution criteria. Specifies how forecasts will be evaluated for each question type.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">background</td>
<td style="text-align: center;">Background information about the forecast question provided by the source, if available. Default: 'N/A'</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">market_info_ open_</td>
<td style="text-align: center;">The datetime when the forecast question went on the market specified by source. Default: 'N/A'</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">datetime</td>
<td style="text-align: center;">The datetime when the forecast question closes on the market specified by source. Default: 'N/A'</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">market_info_ resolution_ criteria</td>
<td style="text-align: center;">The resolution criteria provided by the source, if available. Default: 'N/A'</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">url</td>
<td style="text-align: center;">The URL where the resolution value is found.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">freeze_datetime</td>
<td style="text-align: center;">The datetime UTC when this question set was generated.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">freeze_datetime_ value</td>
<td style="text-align: center;">The latest value of the market or comparison value the day the question set was generated.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">freeze_datetime_ value</td>
<td style="text-align: center;">Explanation of what the value specified in freeze_datetime_value represents.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">source_intro</td>
<td style="text-align: center;">A prompt that presents the source of this question.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">combination_of</td>
<td style="text-align: center;">An array of question objects, as defined by this data dictionary. Default: 'N/A'</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">string I array<object></td>
</tr>
<tr>
<td style="text-align: center;">resolution_dates</td>
<td style="text-align: center;">The resolution dates for which forecasts should be provided for this forecast question.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">array<string></td>
</tr>
</tbody>
</table>
<h1>B.1.1 EXAMPLES</h1>
<p>Table 8, Table 9, and Table 10 show concrete examples of the data dictionary detailed in Table 4.</p>
<p>Table 6: Resolution set data dictionary.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Field</th>
<th style="text-align: left;">Description</th>
<th style="text-align: center;">Required</th>
<th style="text-align: left;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">forecast_due</td>
<td style="text-align: left;">Date in ISO format. e.g.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;">_date</td>
<td style="text-align: left;">"2024-07-21"</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">question_set</td>
<td style="text-align: left;">The name of the file that con- <br> tains the question set. e.g. <br> "2024-07-21-llm.json"</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;">resolutions</td>
<td style="text-align: left;">A list of resolutions to the forecast <br> questions, as defined in Table 7.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">array<object></td>
</tr>
</tbody>
</table>
<p>Table 7: Data dictionary for resolution entries in questions array from Table 6.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Required</th>
<th style="text-align: center;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">id</td>
<td style="text-align: center;">A unique identifier string given source. If instead of a string it's an array of strings, then this is a combination question.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string I array<string></td>
</tr>
<tr>
<td style="text-align: center;">source</td>
<td style="text-align: center;">Where the data comes from.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">direction</td>
<td style="text-align: center;">If id has an array value, this is an array of the same length. Each entry $\in{-1,1}$. If the value is 1 , the question was asked in the normal direction. If the value is -1 , the question was negated in the combination question e.g., for a question asking for $P(\neg Q 1 \cap Q 2)$, the value would be $[-1,1]$; all possible directions for this question would be: $[1,1],[-1,1],[1,-1],[-1,-1]$. The value is null when id is a string.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">forecast_due</td>
<td style="text-align: center;">The date the forecast is due in ISO 8601 format YYYY-MM-DD.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">_date</td>
<td style="text-align: center;">The date the value is associated with in ISO 8601 format YYYY-MM-DD.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">resolution_date</td>
<td style="text-align: center;">The resolution value for the given date.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">number</td>
</tr>
<tr>
<td style="text-align: center;">resolved</td>
<td style="text-align: center;">If true the question has been resolved. False otherwise.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">boolean</td>
</tr>
</tbody>
</table>
<h1>B. 2 FORECAST SETS</h1>
<p>Every set of forecasts provided to ForecastBench is made public and all forecasts coming from the general public and superforecasters are anonymized before release.</p>
<p>Each forecast set contains the header information outlined in Table 11, with all forecasts in an array called forecasts. The forecast sets are described in the following subsections.</p>
<p>Data format The question and resolution datasets are released as JSON ( . json) files.</p>
<p>Ethical and responsible use There are no restrictions on use of the forecast sets.</p>
<h2>B.2.1 GENERAL PUBLIC FORECAST SET</h2>
<p>This forecast set consists of both forecasts made by individuals on the given question set and of the aggregation of those forecasts, as described in Appendix D. The dataset will be updated every time a</p>
<p>Table 8: Prediction market example.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Field</th>
<th style="text-align: left;">Entry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">id</td>
<td style="text-align: left;">1558</td>
</tr>
<tr>
<td style="text-align: left;">source</td>
<td style="text-align: left;">metaculus</td>
</tr>
<tr>
<td style="text-align: left;">combination_of</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">question</td>
<td style="text-align: left;">Will the Cavendish account for less than 50\% of banana exports world-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">wide before 2035?</td>
</tr>
<tr>
<td style="text-align: left;">background</td>
<td style="text-align: left;">Bananas are a well-liked import fruit all over the world, and the</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Cavendish cultivar has been crushing that market for sixty years. But</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">its rise is literally founded upon the compost heap of the Gros Michel,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">another cultivar. The so-called 'Big Mike' variety had been the leading</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">export towards Europe and North America, but the Panama disease,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">a fungus belonging to the <em>Fusarium</em> clade, killed that. Luckily the</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Cavendish, grown in the same soil as the wilting Gros Michel, replaced</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">it as <em>the</em> banana most of the western world connected with bananas.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">However, it appears another <em>Fusarium</em> rears its spores. Cavendish,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">with their genetic homogenity (they're all clones) and sterile nature,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">aren't resistant to it, and the fungus is ravaging more and more planta-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">tions. There are efforts under way to deal with <em>Fusarium</em>, but with</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">various societies' doubts and misgivings about GMOs, the cure may be</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">viewed as a curse instead.</td>
</tr>
<tr>
<td style="text-align: left;">market_info_</td>
<td style="text-align: left;">This question will resolve as <strong>Yes</strong> if the Cavendish banana accounts</td>
</tr>
<tr>
<td style="text-align: left;">resolution_</td>
<td style="text-align: left;">for less than 50\% of worldwide annual banana exports in any year from</td>
</tr>
<tr>
<td style="text-align: left;">criteria</td>
<td style="text-align: left;">2018 to 2034 (inclusive).</td>
</tr>
<tr>
<td style="text-align: left;">market_info_</td>
<td style="text-align: left;">2018-11-13T08:00:00+00:00</td>
</tr>
<tr>
<td style="text-align: left;">open_datetime</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">market_info_</td>
<td style="text-align: left;">2034-12-31T23:00:00+00:00</td>
</tr>
<tr>
<td style="text-align: left;">close_datetime</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">resolution_</td>
<td style="text-align: left;">Resolves to the outcome of the question found at</td>
</tr>
<tr>
<td style="text-align: left;">criteria</td>
<td style="text-align: left;">https://www.metaculus.com/questions/1558/cavendish-bananas-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">collapse-by-2035/.</td>
</tr>
<tr>
<td style="text-align: left;">url</td>
<td style="text-align: left;">https://www.metaculus.com/questions/1558/cavendish-bananas-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">collapse-by-2035/</td>
</tr>
<tr>
<td style="text-align: left;">freeze_datetime_</td>
<td style="text-align: left;">0.48</td>
</tr>
<tr>
<td style="text-align: left;">value</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">freeze_datetime_</td>
<td style="text-align: left;">The community prediction.</td>
</tr>
<tr>
<td style="text-align: left;">value_explanation</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">freeze_datetime</td>
<td style="text-align: left;">2024-07-12T00:00:00+00:00</td>
</tr>
<tr>
<td style="text-align: left;">source_intro</td>
<td style="text-align: left;">We would like you to predict the outcome of a prediction market. A pre-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">diction market, in this context, is the aggregate of predictions submitted</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">by users on the website Manifold. You're going to predict the probability</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">that the market will resolve as 'Yes'.</td>
</tr>
<tr>
<td style="text-align: left;">resolution_dates</td>
<td style="text-align: left;">N/A</td>
</tr>
</tbody>
</table>
<p>survey is run with every forecast containing the information outlined in Table 11 and Table 12. Note that the aggregated forecast sets do not contain the user_id or reasoning fields.</p>
<p>Data collection Data is collected from human forecasters via Qualtrics. We collect the rationale behind forecasts and manually anonymize the data to ensure there is no personally identifiable information in the dataset before posting it online.</p>
<h1>B.2.2 SUPERFORECASTER FORECAST SET</h1>
<p>In addition to the fields outlined in Table 11 and Table 12, the superforecaster dataset contains forecasts with the fields shown in Table 13 (minus the suspected_LLM field). Likewise, this dataset will be updated every time a group of superforecasters is surveyed.</p>
<p>Table 9: Data source (DBnomics) example.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Entry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">id</td>
<td style="text-align: center;">meteofrance_TEMPERATURE_celsius.07005.D</td>
</tr>
<tr>
<td style="text-align: center;">source</td>
<td style="text-align: center;">dbnomics</td>
</tr>
<tr>
<td style="text-align: center;">combination_of</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">question</td>
<td style="text-align: center;">What is the probability that the daily average temperature at the French weather station at Abbeville will be higher on resolution_date than on forecast_due_date?</td>
</tr>
<tr>
<td style="text-align: center;">background</td>
<td style="text-align: center;">The history of Average temperature by day and by station for France Degree Celsius - ABBEVILLE - Daily from Météo-France is available at https://db.nomics.world/meteofrance_TEMPERATURE_celsius.07005.D.</td>
</tr>
<tr>
<td style="text-align: center;">market_info_</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">resolution_</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">criteria</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">market_info_</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">open_datetime</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">market_info_</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">close_datetime</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">url</td>
<td style="text-align: center;">https://db.nomics.world/meteofrance_TEMPERATURE_celsius.07005.D</td>
</tr>
<tr>
<td style="text-align: center;">resolution_</td>
<td style="text-align: center;">Resolves to the value found at</td>
</tr>
<tr>
<td style="text-align: center;">criteria</td>
<td style="text-align: center;">https://db.nomics.world/meteofrance_TEMPERATURE_celsius.07005.D once the data is published.</td>
</tr>
<tr>
<td style="text-align: center;">freeze_datetime</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">value</td>
<td style="text-align: center;">17.95</td>
</tr>
<tr>
<td style="text-align: center;">freeze_datetime_</td>
<td style="text-align: center;">The daily average temperature at the French weather station at Abbeville.</td>
</tr>
<tr>
<td style="text-align: center;">value_explanation</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">freeze_datetime</td>
<td style="text-align: center;">2024-07-12T00:00:00+00:00</td>
</tr>
<tr>
<td style="text-align: center;">source_intro</td>
<td style="text-align: center;">DBnomics collects data on topics such as population and living conditions, environment and energy, agriculture, finance, trade and others from publicly available resources, for example national and international statistical institutions, researchers and private companies. You're going to predict how questions based on this data will resolve.</td>
</tr>
<tr>
<td style="text-align: center;">resolution_dates</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ["2024-07-28", "2024-08-20", "2024-10-19", "2025-01-17", "2025-07- } \ &amp; 21 ", " 2027-07-21 ", " 2029-07-20 ", " 2034-07-19 "] \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Data collection 39 superforecasters provided forecasts, rationales, and additional information about they way they forecast in our latest survey round. We will further manually check all of their responses to ensure the anonymity of the dataset.</p>
<h1>B.2.3 LLM FORECAST SET</h1>
<p>This dataset provides the same data (aside from user_id) as outlined in Table 11 and Table 12, only provided by language models. Each individual . json file was created by a model for the given question set.</p>
<p>Data collection Beyond informing teams their forecasts will be made public, we will not check the rationales.</p>
<p>Table 10: Combination (DBnomics) example.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Field</th>
<th style="text-align: left;">Entry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">id</td>
<td style="text-align: left;">["meteofrance_TEMPERATURE_celsius.07117.D", "mete-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ofrance_TEMPERATURE_celsius.07240.D"]</td>
</tr>
<tr>
<td style="text-align: left;">source</td>
<td style="text-align: left;">dbnomics</td>
</tr>
<tr>
<td style="text-align: left;">combination_of</td>
<td style="text-align: left;">[An array containing dictionary entries of both questions.]</td>
</tr>
<tr>
<td style="text-align: left;">question</td>
<td style="text-align: left;">We are presenting you with two probability questions. Please</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">predict the probability that both will happen, that one will happen</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">but not the other, and that neither will happen. In other words, for</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">each resolution date please provide 4 predictions.</td>
</tr>
<tr>
<td style="text-align: left;">background</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">market_info_</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">resolution_</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">criteria</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">market_info_</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">open_datetime</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">market_info_</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">close_datetime</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">url</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">resolution_</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">criteria</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">freeze_datetime</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">value</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">freeze_datetime</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">value_</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">explanation</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">freeze_datetime</td>
<td style="text-align: left;">2024-07-12T00:00:00+00:00</td>
</tr>
<tr>
<td style="text-align: left;">human_prompt</td>
<td style="text-align: left;">We are presenting you with two probability questions. Please</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">predict the probability that both will happen, that one will happen</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">but not the other, and that neither will happen. In other words, for</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">each resolution date please provide 4 predictions.</td>
</tr>
<tr>
<td style="text-align: left;">resolution_dates</td>
<td style="text-align: left;">["2024-07-28", "2024-08-20", "2024-10-19", "2025-01-17",</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">"2025-07-21", "2027-07-21", "2029-07-20", "2034-07-19"]</td>
</tr>
</tbody>
</table>
<p>Table 11: Data dictionary of headers for forecast set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Field</th>
<th style="text-align: left;">Description</th>
<th style="text-align: center;">Required</th>
<th style="text-align: left;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">organization</td>
<td style="text-align: left;">The organization name as it should be dis- <br> played on the leaderboard.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;">model</td>
<td style="text-align: left;">The model name as it should be displayed <br> on the leader board.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;">question_set</td>
<td style="text-align: left;">The name of the question set file these fore- <br> casts are associated with.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;">forecast_due</td>
<td style="text-align: left;">The date the forecasts were due in ISO 8601</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;">_date</td>
<td style="text-align: left;">format YYYY-MM-DD.</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">forecasts</td>
<td style="text-align: left;">All forecasts for this question set.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">array<object></td>
</tr>
</tbody>
</table>
<p>Table 12: Public forecast set data dictionary of entries in forecasts array from Table 11.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Required</th>
<th style="text-align: center;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">id</td>
<td style="text-align: center;">A unique identifier string given source. If instead of a string it's an array of strings, then this is a combination question.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string I array<string></td>
</tr>
<tr>
<td style="text-align: center;">source</td>
<td style="text-align: center;">Where the data comes from.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">forecast</td>
<td style="text-align: center;">The forecast $\in[0,1]$.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">number</td>
</tr>
<tr>
<td style="text-align: center;">resolution_date</td>
<td style="text-align: center;">The resolution date this forecast corresponds to. null for market questions.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string Inull</td>
</tr>
<tr>
<td style="text-align: center;">reasoning</td>
<td style="text-align: center;">The rationale underlying the forecast. During data anonymization, we insert [redacted to maintain anonymity] wherever text has been redacted.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">string</td>
</tr>
<tr>
<td style="text-align: center;">direction</td>
<td style="text-align: center;">If id has an array value, this is an array of the same length. Each entry is an integer $\in{-1,1}$. If the value is 1 , the question was asked in the normal direction. If the value is -1 , the question was negated in the combination question e.g., for a question asking for $P(\neg Q 1 \cap Q 2)$, the value would be $[-1,1]$. All possible values are: $[1,1],[-1,1],[1,-1]$, $[-1,-1]$, and null.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">array<number> <br> Inull</td>
</tr>
<tr>
<td style="text-align: center;">user_id</td>
<td style="text-align: center;">A randomly generated string associated with the human respondent who submitted the forecast. This value contains no information which could identify said participant and was assigned to the dataset after personal identifiers had been removed. Only required for participants from the general public and superforecaster surveys.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">string</td>
</tr>
</tbody>
</table>
<p>Table 13: Superforecaster forecast set data dictionary of entries in forecasts array from Table 11 (additional fields to those in Table 12).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Field</th>
<th style="text-align: left;">Description</th>
<th style="text-align: center;">Required</th>
<th style="text-align: left;">Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">searches</td>
<td style="text-align: left;">An array of search terms used in re- <br> searching the topic.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">array<string> Inull</td>
</tr>
<tr>
<td style="text-align: left;">consulted_urls</td>
<td style="text-align: left;">A list of useful URLs</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">array<string> Inull</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ For example, in a question generated from a Wikipedia page about whether a chess player's Elo rating will increase by a given date, the freeze value is the chess player's Elo rating on the question set generation date. An explanation of what the freeze value represents is also provided.
${ }^{11}$ See statistical note in Appendix G.
${ }^{12}$ Accuracy measures are based on more than 200 forecasts because human and LLM forecasters submitted multiple forecasts on each dataset question, one for each time horizon. The results presented here include forecasts over the 7 -, 30-, 90-, and 180-day time horizons.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1198 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1198</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1198</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-3687922</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1803.00653v1.pdf" target="_blank">Semi-parametric Topological Memory for Navigation</a></p>
                <p><strong>Paper Abstract:</strong> We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semi-parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three. A video of the agent is available at https://youtu.be/vRF7f4lhswo</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1198.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1198.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semi-Parametric Topological Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A navigation memory architecture combining a non-parametric topological graph of observations (nodes = visual locations, edges = temporal adjacency or vision-based 'shortcuts') with parametric deep nets: a siamese retrieval network for observation-to-node localization and a locomotion network for short-range control; used for planning by graph search (Dijkstra) over connectivity only (no metric).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ViZDoom mazes (3D maze environments based on the game Doom)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous 3D first-person maze environments (labyrinths) rendered in ViZDoom; mazes are indoor/corridor-style layouts with textures, objects marking goals, and human or agent-generated walkthrough recordings used for exploration; domain: simulated indoor navigation/maze exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Initially graph follows the exploration sequence with edges between consecutive observations; connectivity is increased by adding vision-based 'shortcut' edges between visually similar (but temporally distant) nodes. Shortcut edges (2000 in typical setup) substantially increase connectivity and reduce path lengths; graph stores connectivity only (topological), no metric distances.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Exploration footage: ~10,500 simulation steps (≈5 minutes). Graph built with temporal subsampling factor 4 => ≈2,625 stored nodes (approximate). Typical number of visual shortcuts created: 2000 (hyperparameter).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SPTM agent (retrieval R + locomotion L + graph planning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval network R: siamese ResNet-18 encoders producing 512-d embeddings; trained self-supervised to classify temporal closeness of observation pairs and used for nearest-neighbor localization and visual loop-closure detection. Locomotion network L: ResNet-18 based controller mapping (current observation, target/waypoint observation) to action probabilities for short-range navigation; trained self-supervised from random trajectories. Planning: locate current node and goal node by retrieval, compute shortest path on the topological graph (Dijkstra), select a reachable waypoint along that path (furthest node with retrieval similarity above threshold), then L executes actions toward that waypoint; repeat.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Primary: percentage of navigation trials successfully completed within a fixed step budget (5,000 simulation steps). Secondary: time-to-goal (trial duration) / cumulative steps; average shortest-path length in graph to goal (used to quantify connectivity changes).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Exploration sequences provided: 10,500 steps. SPTM uses these sequences to build the graph. No single scalar exploration-efficiency metric beyond success rates is given; graph connectivity metric example: average shortest path to goal (over all nodes) in Val-3 reduced from 990 steps to 155 steps after adding visual shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Validation mazes (reported examples): Val-1 100%, Val-2 98%, Val-3 100% (percentage of navigation trials completed within 5,000 steps). Test mazes: reported high performance (examples in supplement show 100% on several test mazes); paper states average success rate across test environments is ~3x higher than best baseline. (Where multiple numbers exist they are reported explicitly above.)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Planning-based with explicit topological memory + learned short-range locomotion; i.e., graph-search (Dijkstra) over stored landmark graph combined with a learned local controller (waypoint-following).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Explicitly reported relationships: (1) Vision-based shortcut edges drastically improve connectivity and navigation performance — in Val-3 adding shortcuts reduced average shortest-path-to-goal from 990 to 155 steps and removing vision-based shortcuts caused a dramatic decline in performance. (2) Large graphs / high effective path lengths make per-frame (independent) localization noisy and perceptual aliasing more problematic; temporally consistent localization (local neighborhood search before global search) improves robustness. (3) LSTM / recurrent parametric memories struggle to store long exploration sequences (~10k steps), so agents without an explicit non-parametric graph underperform; SPTM's non-parametric storage of all observations yields better generalization and higher success rates. (4) Visual discriminability (texture richness) affects localization and thus navigation — homogeneous textures degrade localization and reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper compares performance across different maze layouts and texture distributions (validation mazes Val-1/Val-2/Val-3 and homogeneous-texture variants). Key findings: SPTM performs robustly across varying topologies and textures but performance degrades when visual discriminability is low (homogeneous textures). Removing visual shortcuts (i.e., leaving only the temporal path edges) leads to much longer shortest paths and large drops in navigation success. Per-frame localization works reasonably in small mazes but underperforms in larger mazes due to perceptual aliasing; temporally-smoothed localization performs better in large/complex topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that separate long-range planning (graph search on topological memory) from short-range control (learned locomotion policy) perform best in these mazes. High-diameter or poorly-connected graphs require planning (waypoint selection along shortest paths) and explicit memory storage; reactive or purely recurrent (LSTM) policies trained via RL/general-purpose memory struggle to generalize to previously unseen large environments and long exploration sequences. Visual shortcuts reduce required planning horizon for low-level controller and improve efficiency; temporally-consistent localization reduces aliasing and stabilizes policy execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semi-parametric Topological Memory for Navigation', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1198.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1198.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A3C baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asynchronous Advantage Actor-Critic baseline (goal-agnostic, trained on beacon-collection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard RL baseline (A3C) trained in the training maze on a surrogate beacon-collection task (no goal image), used as a goal-agnostic agent for comparison; architectures follow Mnih et al. (2016).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ViZDoom mazes (same as above)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same ViZDoom maze environment used for evaluation; baseline used to provide automated exploration sequences and to compare navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not applicable (no explicit graph memory used).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Same mazes; training for baselines used extensive RL training (80M action steps reported for baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A3C (goal-agnostic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Feedforward convolutional policy trained with A3C on beacon-collection surrogate reward; no explicit memory (goal-agnostic), sometimes used to generate automated exploration walkthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Percentage of navigation trials completed within 5,000 steps (used for evaluation), and used qualitatively as exploration trajectory generator.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Baseline success rates are substantially lower than SPTM; paper states SPTM's average success is ~3x higher than the best-performing baseline. Exact per-test numbers for baselines are not fully enumerated in the main text; ablation tables show teach-and-repeat and per-pixel baselines performing far worse (e.g., per-pixel: single-digit % on validation).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Not optimal in this setting; goal-agnostic reactive policy performs poorly at goal-directed navigation in previously unseen mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Agents without explicit memory cannot effectively exploit the exploration walkthrough; thus in topologies with long path lengths and loops they perform poorly. The paper notes goal-agnostic baseline sometimes outperforms goal-directed baselines due to RL instability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Baseline performance varies and is generally poor across validation/test mazes; reactive policies do not benefit from the walkthrough footage and fail to generalize to unseen maze topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Reactive A3C policies lack structured memory and thus cannot leverage long exploration sequences; they fail in high-diameter or looped topologies where planning is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semi-parametric Topological Memory for Navigation', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1198.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1198.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feedforward & LSTM baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feedforward and LSTM-based agents (goal-directed and goal-agnostic variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines include a feedforward goal-directed network (observations + goal image) and LSTM-equipped variants trained with RL (aimed to store exploration information in LSTM hidden state), used for direct comparison with SPTM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ViZDoom mazes (same as above)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same continuous 3D maze environments.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not applicable (no explicit graph memory used).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Baselines trained across similar set of mazes; LSTM agents were exposed to 10,000 steps of exploration then 5,000 steps goal-directed training without resetting LSTM state.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Feedforward / LSTM baselines</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Feedforward: convolutional architecture mapping current frames + goal image to actions (no memory). LSTM: convolutional encoder plus LSTM memory layer intended to store environment layout across exploration; trained with RL (A3C-style) on surrogate rewards and goal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Percentage of successful navigation trials within 5,000 steps; progression of success rate over trial duration.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Generally much lower than SPTM; inconsistent gains from LSTM over feedforward. Exact numbers not fully enumerated in the main text, but reported to be markedly inferior (SPTM ~3x better than best baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>LSTM-equipped policies attempt to be memory-based but are insufficient for long exploration sequences; SPTM-style explicit topological memory performs better.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>LSTM-based policies struggle when exploration sequences are long (10k steps) and when environment topologies are large/complex; inability to store long sequences and perceptual aliasing reduce performance in high-diameter/looped mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Feedforward vs LSTM differences are small and inconsistent across mazes; both types underperform relative to SPTM, especially in larger/complex mazes (e.g., Val-3).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Standard recurrent memory (LSTM) is not sufficient for storing long walkthroughs required for generalization to unseen environments; explicit non-parametric topological memory scales better for long exploration data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semi-parametric Topological Memory for Navigation', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to navigate in complex environments <em>(Rating: 2)</em></li>
                <li>Mapping a suburb with a single camera using a biologically inspired SLAM system <em>(Rating: 2)</em></li>
                <li>SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights <em>(Rating: 2)</em></li>
                <li>Learning metric-topological maps for indoor mobile robot navigation <em>(Rating: 2)</em></li>
                <li>Learning view graphs for robot navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1198",
    "paper_id": "paper-3687922",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "SPTM",
            "name_full": "Semi-Parametric Topological Memory",
            "brief_description": "A navigation memory architecture combining a non-parametric topological graph of observations (nodes = visual locations, edges = temporal adjacency or vision-based 'shortcuts') with parametric deep nets: a siamese retrieval network for observation-to-node localization and a locomotion network for short-range control; used for planning by graph search (Dijkstra) over connectivity only (no metric).",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "ViZDoom mazes (3D maze environments based on the game Doom)",
            "environment_description": "Continuous 3D first-person maze environments (labyrinths) rendered in ViZDoom; mazes are indoor/corridor-style layouts with textures, objects marking goals, and human or agent-generated walkthrough recordings used for exploration; domain: simulated indoor navigation/maze exploration.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Initially graph follows the exploration sequence with edges between consecutive observations; connectivity is increased by adding vision-based 'shortcut' edges between visually similar (but temporally distant) nodes. Shortcut edges (2000 in typical setup) substantially increase connectivity and reduce path lengths; graph stores connectivity only (topological), no metric distances.",
            "environment_size": "Exploration footage: ~10,500 simulation steps (≈5 minutes). Graph built with temporal subsampling factor 4 =&gt; ≈2,625 stored nodes (approximate). Typical number of visual shortcuts created: 2000 (hyperparameter).",
            "agent_name": "SPTM agent (retrieval R + locomotion L + graph planning)",
            "agent_description": "Retrieval network R: siamese ResNet-18 encoders producing 512-d embeddings; trained self-supervised to classify temporal closeness of observation pairs and used for nearest-neighbor localization and visual loop-closure detection. Locomotion network L: ResNet-18 based controller mapping (current observation, target/waypoint observation) to action probabilities for short-range navigation; trained self-supervised from random trajectories. Planning: locate current node and goal node by retrieval, compute shortest path on the topological graph (Dijkstra), select a reachable waypoint along that path (furthest node with retrieval similarity above threshold), then L executes actions toward that waypoint; repeat.",
            "exploration_efficiency_metric": "Primary: percentage of navigation trials successfully completed within a fixed step budget (5,000 simulation steps). Secondary: time-to-goal (trial duration) / cumulative steps; average shortest-path length in graph to goal (used to quantify connectivity changes).",
            "exploration_efficiency_value": "Exploration sequences provided: 10,500 steps. SPTM uses these sequences to build the graph. No single scalar exploration-efficiency metric beyond success rates is given; graph connectivity metric example: average shortest path to goal (over all nodes) in Val-3 reduced from 990 steps to 155 steps after adding visual shortcuts.",
            "success_rate": "Validation mazes (reported examples): Val-1 100%, Val-2 98%, Val-3 100% (percentage of navigation trials completed within 5,000 steps). Test mazes: reported high performance (examples in supplement show 100% on several test mazes); paper states average success rate across test environments is ~3x higher than best baseline. (Where multiple numbers exist they are reported explicitly above.)",
            "optimal_policy_type": "Planning-based with explicit topological memory + learned short-range locomotion; i.e., graph-search (Dijkstra) over stored landmark graph combined with a learned local controller (waypoint-following).",
            "topology_performance_relationship": "Explicitly reported relationships: (1) Vision-based shortcut edges drastically improve connectivity and navigation performance — in Val-3 adding shortcuts reduced average shortest-path-to-goal from 990 to 155 steps and removing vision-based shortcuts caused a dramatic decline in performance. (2) Large graphs / high effective path lengths make per-frame (independent) localization noisy and perceptual aliasing more problematic; temporally consistent localization (local neighborhood search before global search) improves robustness. (3) LSTM / recurrent parametric memories struggle to store long exploration sequences (~10k steps), so agents without an explicit non-parametric graph underperform; SPTM's non-parametric storage of all observations yields better generalization and higher success rates. (4) Visual discriminability (texture richness) affects localization and thus navigation — homogeneous textures degrade localization and reduce performance.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper compares performance across different maze layouts and texture distributions (validation mazes Val-1/Val-2/Val-3 and homogeneous-texture variants). Key findings: SPTM performs robustly across varying topologies and textures but performance degrades when visual discriminability is low (homogeneous textures). Removing visual shortcuts (i.e., leaving only the temporal path edges) leads to much longer shortest paths and large drops in navigation success. Per-frame localization works reasonably in small mazes but underperforms in larger mazes due to perceptual aliasing; temporally-smoothed localization performs better in large/complex topologies.",
            "policy_structure_findings": "Policies that separate long-range planning (graph search on topological memory) from short-range control (learned locomotion policy) perform best in these mazes. High-diameter or poorly-connected graphs require planning (waypoint selection along shortest paths) and explicit memory storage; reactive or purely recurrent (LSTM) policies trained via RL/general-purpose memory struggle to generalize to previously unseen large environments and long exploration sequences. Visual shortcuts reduce required planning horizon for low-level controller and improve efficiency; temporally-consistent localization reduces aliasing and stabilizes policy execution.",
            "uuid": "e1198.0",
            "source_info": {
                "paper_title": "Semi-parametric Topological Memory for Navigation",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "A3C baseline",
            "name_full": "Asynchronous Advantage Actor-Critic baseline (goal-agnostic, trained on beacon-collection)",
            "brief_description": "A standard RL baseline (A3C) trained in the training maze on a surrogate beacon-collection task (no goal image), used as a goal-agnostic agent for comparison; architectures follow Mnih et al. (2016).",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "ViZDoom mazes (same as above)",
            "environment_description": "Same ViZDoom maze environment used for evaluation; baseline used to provide automated exploration sequences and to compare navigation performance.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Not applicable (no explicit graph memory used).",
            "environment_size": "Same mazes; training for baselines used extensive RL training (80M action steps reported for baselines).",
            "agent_name": "A3C (goal-agnostic)",
            "agent_description": "Feedforward convolutional policy trained with A3C on beacon-collection surrogate reward; no explicit memory (goal-agnostic), sometimes used to generate automated exploration walkthroughs.",
            "exploration_efficiency_metric": "Percentage of navigation trials completed within 5,000 steps (used for evaluation), and used qualitatively as exploration trajectory generator.",
            "exploration_efficiency_value": null,
            "success_rate": "Baseline success rates are substantially lower than SPTM; paper states SPTM's average success is ~3x higher than the best-performing baseline. Exact per-test numbers for baselines are not fully enumerated in the main text; ablation tables show teach-and-repeat and per-pixel baselines performing far worse (e.g., per-pixel: single-digit % on validation).",
            "optimal_policy_type": "Not optimal in this setting; goal-agnostic reactive policy performs poorly at goal-directed navigation in previously unseen mazes.",
            "topology_performance_relationship": "Agents without explicit memory cannot effectively exploit the exploration walkthrough; thus in topologies with long path lengths and loops they perform poorly. The paper notes goal-agnostic baseline sometimes outperforms goal-directed baselines due to RL instability.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Baseline performance varies and is generally poor across validation/test mazes; reactive policies do not benefit from the walkthrough footage and fail to generalize to unseen maze topologies.",
            "policy_structure_findings": "Reactive A3C policies lack structured memory and thus cannot leverage long exploration sequences; they fail in high-diameter or looped topologies where planning is required.",
            "uuid": "e1198.1",
            "source_info": {
                "paper_title": "Semi-parametric Topological Memory for Navigation",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Feedforward & LSTM baselines",
            "name_full": "Feedforward and LSTM-based agents (goal-directed and goal-agnostic variants)",
            "brief_description": "Baselines include a feedforward goal-directed network (observations + goal image) and LSTM-equipped variants trained with RL (aimed to store exploration information in LSTM hidden state), used for direct comparison with SPTM.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "ViZDoom mazes (same as above)",
            "environment_description": "Same continuous 3D maze environments.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Not applicable (no explicit graph memory used).",
            "environment_size": "Baselines trained across similar set of mazes; LSTM agents were exposed to 10,000 steps of exploration then 5,000 steps goal-directed training without resetting LSTM state.",
            "agent_name": "Feedforward / LSTM baselines",
            "agent_description": "Feedforward: convolutional architecture mapping current frames + goal image to actions (no memory). LSTM: convolutional encoder plus LSTM memory layer intended to store environment layout across exploration; trained with RL (A3C-style) on surrogate rewards and goal tasks.",
            "exploration_efficiency_metric": "Percentage of successful navigation trials within 5,000 steps; progression of success rate over trial duration.",
            "exploration_efficiency_value": null,
            "success_rate": "Generally much lower than SPTM; inconsistent gains from LSTM over feedforward. Exact numbers not fully enumerated in the main text, but reported to be markedly inferior (SPTM ~3x better than best baseline).",
            "optimal_policy_type": "LSTM-equipped policies attempt to be memory-based but are insufficient for long exploration sequences; SPTM-style explicit topological memory performs better.",
            "topology_performance_relationship": "LSTM-based policies struggle when exploration sequences are long (10k steps) and when environment topologies are large/complex; inability to store long sequences and perceptual aliasing reduce performance in high-diameter/looped mazes.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Feedforward vs LSTM differences are small and inconsistent across mazes; both types underperform relative to SPTM, especially in larger/complex mazes (e.g., Val-3).",
            "policy_structure_findings": "Standard recurrent memory (LSTM) is not sufficient for storing long walkthroughs required for generalization to unseen environments; explicit non-parametric topological memory scales better for long exploration data.",
            "uuid": "e1198.2",
            "source_info": {
                "paper_title": "Semi-parametric Topological Memory for Navigation",
                "publication_date_yy_mm": "2018-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to navigate in complex environments",
            "rating": 2,
            "sanitized_title": "learning_to_navigate_in_complex_environments"
        },
        {
            "paper_title": "Mapping a suburb with a single camera using a biologically inspired SLAM system",
            "rating": 2,
            "sanitized_title": "mapping_a_suburb_with_a_single_camera_using_a_biologically_inspired_slam_system"
        },
        {
            "paper_title": "SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights",
            "rating": 2,
            "sanitized_title": "seqslam_visual_routebased_navigation_for_sunny_summer_days_and_stormy_winter_nights"
        },
        {
            "paper_title": "Learning metric-topological maps for indoor mobile robot navigation",
            "rating": 2,
            "sanitized_title": "learning_metrictopological_maps_for_indoor_mobile_robot_navigation"
        },
        {
            "paper_title": "Learning view graphs for robot navigation",
            "rating": 1,
            "sanitized_title": "learning_view_graphs_for_robot_navigation"
        }
    ],
    "cost": 0.0128905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION</p>
<p>Nikolay Savinov 
Intel Labs
Intel Labs</p>
<p>Eth Zürich 
Intel Labs
Intel Labs</p>
<p>Alexey Dosovitskiy 
Intel Labs
Intel Labs</p>
<p>Vladlen Koltun 
Intel Labs
Intel Labs</p>
<p>SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION
Published as a conference paper at ICLR 2018
We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.</p>
<p>INTRODUCTION</p>
<p>Deep learning (DL) has recently been used as an efficient approach to learning navigation in complex three-dimensional environments. DL-based approaches to navigation can be broadly divided into three classes: purely reactive Zhu et al., 2017), based on unstructured general-purpose memory such as LSTM (Mnih et al., 2016;Mirowski et al., 2017), and employing a navigation-specific memory structure based on a metric map (Parisotto &amp; Salakhutdinov, 2018;.</p>
<p>However, extensive evidence from psychology suggests that when traversing environments, animals do not rely strongly on metric representations (Gillner &amp; Mallot, 1998;Wang &amp; Spelke, 2002;Foo et al., 2005). Rather, animals employ a range of specialized navigation strategies of increasing complexity. According to Foo et al. (2005), one such strategy is landmark navigation -"the ability to orient with respect to a known object". Another is route-based navigation that "involves remembering specific sequences of positions". Finally, map-based navigation assumes a "survey knowledge of the environmental layout", but the map need not be metric and in fact it is typically not: "[. . .] humans do not integrate experience on specific routes into a metric cognitive map for navigation [. . .] Rather, they primarily depend on a landmark-based navigation strategy, which can be supported by qualitative topological knowledge of the environment."</p>
<p>In this paper, we propose semi-parametric topological memory (SPTM) -a deep-learning-based memory architecture for navigation, inspired by landmark-based navigation in animals. SPTM consists of two components: a non-parametric memory graph G where each node corresponds to a location in the environment, and a parametric deep network R capable of retrieving nodes from the graph based on observations. The graph contains no metric relations between the nodes, only connectivity information. While exploring the environment, the agent builds the graph by appending observations to it and adding shortcut connections based on detected visual similarities. The network R is trained to retrieve nodes from the graph based on an observation of the environment. This allows the agent to localize itself in the graph. Finally, we build a complete SPTM-based navigation agent by complementing the memory with a locomotion network L, which allows the agent to move between nodes in the graph. The R and L networks are trained in self-supervised fashion, without any manual labeling or reward signal.</p>
<p>We evaluate the proposed system and relevant baselines on the task of goal-directed maze navigation in simulated three-dimensional environments. The agent is instantiated in a previously unseen maze and given a recording of a walk through the maze (images only, no information about actions taken or ego-motion). Then the agent is initialized at a new location in the maze and has to reach a goal location in the maze, given an image of that goal. To be successful at this task, the agent must represent the maze based on the footage it has seen, and effectively utilize this representation for navigation.</p>
<p>The proposed system outperforms baseline approaches by a large margin. Given 5 minutes of maze walkthrough footage, the system is able to build an internal representation of the environment and use it to confidently navigate to various goals within the maze. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three. Qualitative results and an implementation of the method are available at https://sites.google.com/view/SPTM.</p>
<p>RELATED WORK</p>
<p>Navigation in animals has been extensively studied in psychology. Tolman (1948) introduced the concept of a cognitive map -an internal representation of the environment that supports navigation. The existence of cognitive maps and their exact form in animals, including humans, has been debated since. O' Keefe &amp; Nadel (1978) suggested that internal representations take the form of metric maps. More recently, it has been shown that bees (Cartwright &amp; Collett, 1982;Collett, 1996), ants (Judd &amp; Collett, 1998), and rats (Sutherland et al., 1987) rely largely on landmark-based mechanisms for navigation. Bennett (1996) and Mackintosh (2002) question the existence of cognitive maps in animals. Gillner &amp; Mallot (1998), Wang &amp; Spelke (2002, and Foo et al. (2005) argue that humans rely largely on landmark-based navigation.</p>
<p>In contrast, navigation systems developed in robotics are typically based on metric maps, constructed using the available sensory information -sonar, LIDAR, RGB-D, or RGB input (Elfes, 1987;Thrun et al., 2005;Durrant-Whyte &amp; Bailey, 2006). Particularly relevant to our work are vision-based simultaneous localization and mapping (SLAM) methods (Cadena et al., 2016). These systems provide high-quality maps under favorable conditions, but they are sensitive to calibration issues, do not deal well with poor imaging conditions, do not naturally accommodate dynamic environments, and can be difficult to scale.</p>
<p>Modern deep learning (DL) methods allow for end-to-end learning of sensorimotor control, directly predicting control signal from high-dimensional sensory observations such as images (Mnih et al., 2015). DL approaches to navigation vary both in the learning method -reinforcement learning or imitation learning -and in the memory representation. Purely reactive methods Zhu et al., 2017) lack explicit memory and do not navigate well in complex environments (Savva et al., 2017). Systems equipped with general-purpose LSTM memory (Mnih et al., 2016;Pathak et al., 2017;Jaderberg et al., 2017;Mirowski et al., 2017) or episodic memory (Blundell et al., 2016;Pritzel et al., 2017) can potentially store information about the environment. However, these systems have not been demonstrated to perform efficient goal-directed navigation in previously unseen environments, and empirical results indicate that LSTM-based systems are not up to the task (Savva et al., 2017). Oh et al. (2016) use an addressable memory for first-person-view navigation in three-dimensional environments. The authors demonstrate that the proposed memory structure supports generalization to previously unseen environments. Our work is different in that Oh et al. (2016) experiment with relatively small discrete gridworld-like environments, while our approach naturally applies to large continuous state spaces.</p>
<p>Most related to our work are DL navigation systems that use specialized map-like representations. Bhatti et al. (2016) augment a DL system with a metric map produced by a standard SLAM algorithm. Parisotto &amp; Salakhutdinov (2018) use a 2D spatial memory that represents a global map of the environment.  build a 2D multi-scale metric map using the end-to-end trainable planning approach of Tamar et al. (2016). Our method differs from these approaches in that we are not aiming to build a global metric map of the environment. Rather, we use a topological map. This allows our method to support navigation in a continuous space without externally provided camera poses or ego-motion information. While contemporary approaches in robotics are dominated by metric maps, research on topological maps has a long history in robotics. Models based on topological maps have been applied to navigation in simple 2D mazes (Kuipers &amp; Byun, 1991;Meng &amp; Kak, 1993;Schölkopf &amp; Mallot, 1995) and on physical systems (Hong et al., 1992;Bachelder &amp; Waxman, 1995;Franz et al., 1998;Thrun, 1998;Fraundorfer et al., 2007). Trullier et al. (1997) provide a review of biologically-inspired navigation systems, including landmark-based ones. Milford and colleagues designed SLAM systems inspired by computational models of the hippocampus (Milford &amp; Wyeth, 2008;2010;Ball et al., 2013). We reinterpret this line of work in the context of deep learning.</p>
<p>METHOD</p>
<p>We consider an agent interacting with an environment in discrete time steps. At each time step t, the agent gets an observation o t of the environment and then takes an action a t from a set of actions A. In our experiments, the environment is a maze in a three-dimensional simulated world, and the observation is provided to the agent as a tuple of several recent images from the agent's point of view.</p>
<p>The interaction of the agent with a new environment is set up in two stages: exploration and goaldirected navigation. During the first stage, the agent is presented with a recording of a traversal of the environment over a number of time steps T e , and builds an internal representation of the environment based on this recording. In the second stage, the agent uses this internal representation to reach goal locations in the environment. This goal-directed navigation is performed in an episodic setup, with each episode lasting for a fixed maximum number of time steps or until the goal is reached. In each episode, the goal location is provided to the agent by an observation of this location o g . The agent has to use the goal observation and the internal representation built during the exploration phase to effectively reach the goal.</p>
<p>SEMI-PARAMETRIC TOPOLOGICAL MEMORY</p>
<p>We propose a new form of memory suitable for storing internal representations of environments. We refer to it as semi-parametric topological memory (SPTM). It consists of a (non-parametric) memory graph G where each node represents a location in the environment, and a (parametric) deep network R capable of retrieving nodes from the graph based on observations. A high-level overview of an SPTM-based navigation system is shown in Figure 1. Here SPTM acts as a planning module: given the current observation o and the goal observation o g , it generates a waypoint observation o w , which lies on a path to the goal and can be easily reached from the agent's current location. The current observation and the waypoint observation are provided to a locomotion network L, which is responsible for short-range navigation. The locomotion network then guides the agent towards the waypoint, and the loop repeats. The networks R and L are trained in self-supervised fashion, without any externally provided labels or reinforcement signals. We now describe each component of the system in detail.</p>
<p>Retrieval network. The network R estimates the similarity of two observations (o 1 , o 2 ). The network is trained on a set of environments in self-supervised manner, based on trajectories of a randomly acting agent. Conceptually, the network is trained to assign high similarity to pairs of observations that are temporally close, and low similarity to pairs that are temporally distant. We cast this as a classification task: given a pair of observations, the network has to predict whether they are temporally close or not.</p>
<p>To generate the training data, we first let a random agent explore the environment, resulting in a sequence of observations {o 1 , . . We use a siamese architecture for the network R, akin to Zagoruyko &amp; Komodakis (2015). Each of the two input observations is first processed by a deep convolutional encoder based on ResNet-18 (He et al., 2016), which outputs a 512-dimensional embedding vector. These two vectors are concatenated and further processed by a small 5-layer fully-connected network, ending with a 2way softmax. The network is trained in supervised fashion with the cross-entropy loss. Further details are provided in the supplement. 
e ij = 1 ⇔ |i − j| = 1 ∨ R(o vi , o vj ) &gt; s shortcut ,(1)
where 0 &lt; s shortcut &lt; 1 is a similarity threshold for creating a shortcut connection. The first type of edge corresponds to natural spatial adjacency between locations, while the second type can be seen as a form of loop closure.</p>
<p>Two enhancements improve the quality of the graph. First, we only connect vertices by a "visual shortcut" edge if |i−j| &gt; ∆T , so as to avoid adding trivial edges. Second, to improve the robustness of visual shortcuts, we find these by matching sequences of observations, not single observations:
median{R(o v i−∆Tw , o v j−∆Tw ), . . . , R(o v i+∆Tw , o v j+∆Tw )} &gt; s shortcut .(2)
Finding the waypoint. At navigation time, we use SPTM to provide waypoints to the locomotion network. As illustrated in Figure 2, the process includes three steps: localization, planning, and waypoint selection.</p>
<p>In the localization step, the agent localizes itself and the goal in the graph based on its current observation o and the goal observation o g , as illustrated in Figure 2(a). We have experimented with two approaches to localization. In the basic variant, the agent's location is retrieved as the median of k = 5 nearest neighbors of the observation in the memory. The siamese architecture of the retrieval network allows for efficient nearest neighbor queries by pre-computing the embeddings of observations in the memory.</p>
<p>An issue with this simple technique is that localization is performed per frame, and therefore the result can be noisy and susceptible to perceptual aliasing -inability to discriminate two locations with similar appearance. We therefore implement a modified approach allowing for temporally consistent self-localization, inspired by localization approaches from robotics (Milford &amp; Wyeth, 2012). We initially perform the nearest neighbor search only in a local neighborhood of the previous agent's localization, and resort to global search in the whole memory only if this initial search fails (that is, the similarity of the retrieved nearest neighbor to the current observation is below a certain threshold s local ). This simple modification improves the performance of the method while also reducing the search time.</p>
<p>In the planning step, we find the shortest path on the graph between the two retrieved nodes v a and v g , as shown in Figure 2 (b). We used Dijkstra's algorithm in our experiments.</p>
<p>Finally, the third step is to select a waypoint on the computed shortest path, as depicted in Figure 2(c). We denote the shortest path by
v sp 0 , v sp 1 , . . . , v sp n , v sp 0 = v a , v sp n = v g(3)
A naive solution would be to set the waypoint to v sp D , with a fixed D. However, depending on the actions taken in the exploration sequence, this can lead to selecting a waypoint that is either too close (no progress) or too far (not reachable). We therefore follow a more robust adaptive strategy. We choose the furthest vertex along the shortest path that is still confidently reachable:
v w = v sp l , l = max i {i, s.t. R(o, o v sp i ) &gt; s reach },(4)
where 0 &lt; s reach &lt; 1 is a fixed similarity threshold for considering a vertex reachable. In practice, we limit the waypoint search to a fixed window i ∈ [H min , H max ]. The output of the planning process is the observation o w = o v w that corresponds to the retrieved waypoint.</p>
<p>LOCOMOTION NETWORK</p>
<p>The network L is trained to navigate towards target observations in the vicinity of the agent. The network maps a pair (o 1 , o 2 ), which consists of a current observation and a goal observation, into action probabilities:
L(o 1 , o 2 ) = p ∈ R |A| .
The action can then be produced either deterministically by choosing the most probable action, or stochastically by sampling from the distribution. In what follows we use the stochastic policy.</p>
<p>Akin to the retrieval network R, the network L is trained in self-supervised manner, based on trajectories of a randomly acting agent. Random exploration produces a sequence of observations {o 1 , . . . o N } and actions {a 1 , . . . a N }. We generate training samples from these trajectories by taking a pair of observations separated by at most l = 20 time steps and the action corresponding to the first observation:
((o i , o j ), a i ).
The network is trained in supervised fashion on this data, with a softmax output layer and the cross-entropy loss. The architecture of the network is the same as the retrieval network.</p>
<p>Why is it possible to learn a useful controller based on trajectories of a randomly acting agent?</p>
<p>The proposed training procedure leads to learning the conditional action distribution P (a|o t , o t+k ).</p>
<p>Even though the trajectories are generated by a random actor, this distribution is generally not uniform. For instance, if k = 1, the network would learn actions to be taken to perform one-step transitions between neighboring states. For k &gt; 1, training data is more noisy, but there is still useful training signal, which turns out to be sufficient for short-range navigation. </p>
<p>IMPLEMENTATION DETAILS</p>
<p>Inputs to the retrieval network R and the locomotion network L are observations of the environment o, represented by stacks of two consecutive RGB images obtained from the environment, at resolution 160×120 pixels. Both networks are based on ResNet-18 (He et al., 2016). Note that ResNet-18 is much larger than networks typically used in navigation agents based on reinforcement learning. The use of this high-capacity architecture is made possible by the self-supervised training of our model. Training of a network of this size from scratch with pure reinforcement learning would be problematic and, to our knowledge, has never been demonstrated.</p>
<p>The training setup is similar for both networks. We generate training data online by executing a random agent in the training environment, and maintain a replay buffer B of recent samples. At each training iteration, we sample a mini-batch of 64 observation pairs at random from the buffer, according to the conditions described in Sections 3.1 and 3.2. We then perform an update using the Adam optimizer (Kingma &amp; Ba, 2015), with learning rate λ = 0.0001. We train the networks R and L for a total of 1 and 2 million mini-batch iterations, respectively. Further details are provided in the supplement.</p>
<p>We made sure that all operations in the SPTM are implemented efficiently. Goal localization is only performed once in the beginning of a navigation episode. Shortest paths to the goal from all vertices of the graph can therefore also be computed once in the beginning of navigation. The only remaining computationally expensive operations are nearest-neighbor queries for agent self-localization in the graph. However, thanks to the siamese architecture of the retrieval network, we can precompute the embedding vectors of observations in the memory and need only evaluate the small fully-connected network during navigation.</p>
<p>EXPERIMENTS</p>
<p>We perform experiments using a simulated three-dimensional environment based on the classic game Doom (Kempka et al., 2016). An illustration of an SPTM agent navigating towards a goal in a maze is shown in Figure 3. We evaluate the proposed method on the task of goal-directed navigation in previously unseen environments and compare it to relevant baselines from the literature.</p>
<p>SETUP</p>
<p>We are interested in agents that are able to generalize to new environments. Therefore, we used different mazes for training, validation, and testing. We used the same set of textures for all labyrinths, but the maze layouts are very different, and the texture placement is randomized. During training, we used a single labyrinth layout, but created 400 versions with randomized goal placements and textures. In addition, we created 3 mazes for validation and 7 mazes for testing. Layouts of the training and test labyrinths are shown in Figure 4; the validation mazes are shown in the supplement. Each maze is equipped with 4 goal locations, marked by 4 different special objects. The appearance of these special objects is common to all mazes. We used the validation mazes for tuning the parameters of all approaches, and used fixed parameters when evaluating in the test mazes. The overall experimental setup follows Section 3. When given a new maze, the agent is provided with an exploration sequence of the environment, with a duration of approximately 5 minutes of in-simulation time (equivalent to 10,500 simulation steps). In our experiments, we used sequences generated by a human subject aimlessly exploring the mazes. The same exploration sequences were provided to all algorithms -the proposed method and the baselines. Example exploration sequences are shown on the project page, https://sites.google.com/view/SPTM.</p>
<p>Given an exploration sequence, the agent attempts a series of goal-directed navigation trials. In each of these, the agent is positioned at a new location in the maze and is presented with an image of the goal location. In our experiments, we used 4 different starting locations, 4 goals per maze, and repeated each trial 6 times (results in these vary due to the use of randomized policies for all methods), resulting in 96 trials for each maze. A trial is considered successfully completed if the agent reaches the goal within 5,000 simulation steps, or 2.4 minutes of in-simulation time.</p>
<p>HYPERPARAMETERS</p>
<p>We set the hyperparameters of the SPTM agent based on an evaluation on the validation set, as reported in Table S1 in the supplement. We find that the method performs well for a range of hyperparameter values. Interestingly, the approach is robust to temporal subsampling of the walkthrough sequence. Therefore, in the following experiments we subsample the walkthrough sequence by a factor of 4 when building the SPTM graph. Another important parameter is the threshold s shortcut for creating shortcuts in the graph. We set this threshold as a percentile of the set of all pairwise distances between observations in the memory, or, in other words, as the desired number of shortcuts to be created. We set this number to 2000 in what follows. When making visual shortcuts in the graph, we set the minimum shortcut distance ∆T = 5 and the smoothing window size ∆T w = 10. The threshold values for waypoint selection are set to s local = 0.7 and s reach = 0.95. The minimum and maximum waypoint distances are set to H min = 1 and H max = 7, respectively.</p>
<p>BASELINES</p>
<p>We compare the proposed method to a set of baselines that are representative of the state of the art in deep-learning-based navigation. Note that we study an agent operating in a realistic setting: a continuous state space with no access to ground-truth information such as depth maps or egomotion. This setup excludes several existing works from our comparison: the full model of Mirowski et al. (2017) that uses ground-truth depth maps and ego-motion, the method of  that operates on a discrete grid given ground-truth ego-motion, and the approach of Parisotto &amp; Salakhutdinov (2018) that requires the knowledge of ground-truth global coordinates of the agent.  Table 1: Comparison of the SPTM agent to baseline approaches. We report the percentage of navigation trials successfully completed in 5,000 steps (higher is better).</p>
<p>The first baseline is a goal-agnostic agent without memory. The agent is not informed about the goal, but may reach it by chance. We train this network in the training maze using asynchronous advantage actor-critic (A3C) (Mnih et al., 2016). The agent is trained on the surrogate task of collecting invisible beacons around the labyrinth. (The beacons are made invisible to avoid providing additional visual guidance to the agents.) In the beginning of each episode, the labyrinth is populated with 1000 of these invisible beacons, at random locations. The agent receives a reward of 1 for collecting a beacon and 0 otherwise. Each episode lasts for 5,000 simulation steps. We train the agent with the A3C algorithm and use an architecture similar to Mnih et al. (2016). Further details are provided in the supplement.</p>
<p>The second baseline is a feedforward network trained on goal-directed navigation, similar to Zhu et al. (2017). The network gets its current observation, as well as an image of the goal, as input. It gets the same reward as the goal-agnostic agent for collecting invisible beacons, but in addition it gets a large reward of 800 for reaching the goal. This network can go towards the goal if the goal is within its field of view, but it lacks memory, so it is fundamentally unable to make use of the exploration phase. The network architecture is the same as in the first baseline, but the input is the concatenation of the 4 most recent frames and the goal image.</p>
<p>The third and fourth baseline approaches are again goal-agnostic and goal-directed agents, but equipped with LSTM memory. The goal-directed LSTM agent is similar to Mirowski et al. (2017). At test time, we feed the exploration sequence to the LSTM agent and then let it perform goaldirected navigation without resetting the LSTM state. When training these networks, we follow a similar protocol. First, the agent navigates the environment for 10,000 steps in exploration mode; that is, with rewards for collecting invisible beacons, but without a goal image given and with no reward for reaching a goal. Next, the agent is given a goal image and spends another 5,000 steps in goal-directed navigation mode; that is, with a goal image given and with a high reward for reaching the goal (while also continuing to receive rewards for collecting the invisible beacons). We do not reset the state of the memory cells between the two stages. This way, the agent can learn to store the layout of the environment in its memory and use it for efficient navigation. Table 1 shows, for each test maze, the percentage of navigation trials successfully completed within 5,000 steps, equivalent to 2.4 minutes of real-time simulation. Figure 5 presents the results on the test mazes in more detail, by plotting the percentage of completed episodes as a function of the trial duration. Qualitative results are available at https://sites.google.com/view/SPTM.</p>
<p>RESULTS</p>
<p>The proposed SPTM agent is superior to the baselines in all mazes. As Table 1 demonstrates, its average success rate across the test mazes is three times higher than the best-performing baseline. Figure 5 demonstrates that the proposed approach is not only successful overall, but that the agent typically reaches the goal much faster than the baselines.</p>
<p>The difference in performance between feedforward and LSTM baseline variants is generally small and inconsistent across mazes. This suggests that standard LSTM memory is not sufficient to efficiently make use of the provided walkthrough footage. One reason can be that recurrent networks, including LSTMs, struggle with storing long sequences (Goodfellow et al., 2016). The duration of the walkthrough footage, 10,000 time steps, is beyond the capabilities of standard recurrent networks. SPTM is at an advantage, since it stores all the provided information by design. Why is the performance of the baseline approaches in our experiments significantly weaker than reported previously (Mirowski et al., 2017)?</p>
<p>The key reason is that we study generalization of agents to previously unseen environments, while Mirowski et al. (2017) train and evaluate agents in the same environment. The generalization scenario is much more challenging, but also more realistic. Our results indicate that existing methods struggle with generalization.</p>
<p>Interestingly, the best-performing baseline is goal-agnostic, not goal-directed. We see two main explanations for this. First, generalization performance has high variance and may be dominated by spurious correlations in the appearance of training and test mazes. Second, even in the training environments the goal-directed baselines do not necessarily outperform the goal-agnostic ones, since the large reward for reaching the goal makes reinforcement learning unstable. This effect has been observed by Mirowski et al. (2017), and to avoid it the authors had to resort to reward clipping; in our setting, reward clipping would effectively lead to ignoring the goals.</p>
<p>Figure 6 (left) shows a trajectory of a walkthrough provided to the algorithms in the Val-3 maze. The shortcut connections made automatically in the SPTM graph are marked in red. We selected a conservative threshold for making shortcut connections to ensure that there are no false positives. Still, the automatically discovered shortcut connections greatly increase the connectivity of the graph: for instance, in the Val-3 maze the average length of the shortest path to the goal, computed over all nodes in the graph, drops from 990 to 155 steps after introducing the shortcut connections. Figure 6 (right) demonstrates three representative trajectories of the SPTM agent performing goaldirected navigation. In Tracks 1 and 2, the agent deliberately goes for the goal, making use of the environment representation stored in SPTM. Track 3 is less successful and the agent's trajectory contains unnecessary loops; we attribute this to the difficulty of vision-based self-localization in large environments. Table 2 reports an ablation study of the SPTM agent on the validation set. Removing vision-based shortcuts from the graph leads to dramatic decline in performance. The agent with independent per-frame localization performs quite well on two of the three mazes, but underperforms on the  Table 2: Ablation study on the SPTM agent. We report the percentage of navigation trials successfully completed in 5,000 steps in validation mazes (higher is better). more challenging Val-3 maze. A likely explanation is that perceptual aliasing gets increasingly problematic in larger mazes.</p>
<p>Additional experiments are reported in the supplement: performance in the validation environments, robustness to hyperparameter settings, an additional ablation study evaluating the performance of the R and L networks compared to simple alternatives, experiments in environments with homogeneous textures, and experiments with automated (non-human) exploration.</p>
<p>CONCLUSION</p>
<p>We have proposed semi-parametric topological memory (SPTM), a memory architecture that consists of a non-parametric component -a topological graph, and a parametric component -a deep network capable of retrieving nodes from the graph given observations from the environment. We have shown that SPTM can act as a planning module in a navigation system. This navigation agent can efficiently reach goals in a previously unseen environment after being presented with only 5 minutes of footage. We see several avenues for future work. First, improving the performance of the networks R and L will directly improve the overall quality of the system. Second, while the current system explicitly avoids using ego-motion information, findings from experimental psychology suggest that noisy ego-motion estimation and path integration are useful for navigation. Incorporating these into our model can further improve robustness. Third, in our current system the size of the memory grows linearly with the duration of the exploration period. This may become problematic when navigating in very large environments, or in lifelong learning scenarios. A possible solution is adaptive subsampling, by only retaining the most informative or discriminative observations in memory. Finally, it would be interesting to integrate SPTM into a system that is trainable end-to-end.</p>
<p>SUPPLEMENTARY MATERIAL S1 METHOD DETAILS S1.1 NETWORK ARCHITECTURES</p>
<p>The retrieval network R and the locomotion network L are both based on ResNet-18 (He et al., 2016). Both take 160×120 pixel images as inputs. The networks are initialized as proposed by He et al. (2016). We used an open ResNet implementation: https://github.com/raghakot/ keras-resnet/blob/master/resnet.py.</p>
<p>The network R admits two observations as input. Each of these is processed by a convolutional ResNet-18 encoder. Each of the encoders produces a 512-dimensional embedding vector. These are concatenated and fed through a fully-connected network with 4 hidden layers with 512 units each and ReLU nonlinearities.</p>
<p>The network L also admits two observations, but in contrast with the network R it processes them jointly, after concatenating them together. A convolutional ResNet-18 encoder is followed by a single fully-connected layer with 7 outputs and a softmax. The 7 outputs correspond to all available actions: do nothing, move forward, move backward, move left, move right, turn left, and turn right.</p>
<p>S1.2 TRAINING</p>
<p>We implemented the training in Keras (Chollet et al., 2015) and Tensorflow (Abadi et al., 2016). The training setup is similar for both networks. We generate training data online by executing a random agent in the environment, and maintain a replay buffer B of size |B| = 10,000. We run the random agent for 10,000 steps and then perform 50 mini-batch iterations of training. For the random agent, as well as for all other agents, we use action repeat of 4 -that is, every selected action is repeated 4 times. At each training iteration, we sample a mini-batch of 64 training observation pairs at random from the buffer, according to the conditions described in Sections 3.2 and 3.1. We then perform an update using the Adam optimizer (Kingma &amp; Ba, 2015), with learning rate λ = 0.0001, momentum parameters β 1 = 0.9 and β 2 = 0.999, and the stabilizing parameter ε = 10 −8 .</p>
<p>S2 BASELINE DETAILS</p>
<p>The baselines are based on an open A3C implementation: https://github.com/pathak22/ noreward-rl. We have used the architectures of Mnih et al. (2016) and Mirowski et al. (2017). The feedforward model consists of two convolutional layers and two fully-connected layers, from which the value and the policy are predicted. In the LSTM model the second fully connected layer is replaced by LSTM. The input to the networks is a stack of 4 most recent observed frames, resized to 84×84 pixels. We experimented with using RGB and grayscale frames, and found the baselines trained with grayscale images to perform better. We therefore always report the results for baselines with grayscale inputs. We train the baselines for 80 million action steps, which corresponds to 320 million simulation steps because of action repeat. We selected the snapshot to be used at test time based on the training reward.</p>
<p>S3 ADDITIONAL RESULTS</p>
<p>Layouts of the validation mazes are shown in Figure S1. Plots of success rate as a function of trial duration on each validation maze are shown in Figure S2. Performance of an SPTM agent with varying hyperparameters is shown in Table S1.</p>
<p>S3.1 HOMOGENEOUS TEXTURES AND AUTOMATED EXPLORATION</p>
<p>To evaluate the robustness of the approach, we tried varying the texture distribution in the environment and the properties of the exploration sequence.</p>
<p>Val-1 Val-2 Val-3 Figure S1: Layouts of the mazes used for validation. In the experiments in the main paper we used mazes with relatively diverse (although repetitive) textures, see for example Figure 3 in the main paper. We re-textured several mazes to be qualitatively similar to Mirowski et al. (2017): with mainly homogeneous textures and only relatively sparse inclusions of more discriminative textures. When testing the method with these textures, we retrained the networks R and L in a training maze with similar texture distribution, but kept all other parameters of the method fixed.</p>
<p>For experiments in the main paper we used walkthrough sequences recorded from humans exploring the maze. An intelligent agent should be able to explore and map an environment fully autonomously. Effective exploration is a challenging task in itself, and a comprehensive study of this problem is outside the scope of the present paper. However, as a first step, we experiment with providing our method with walkthrough sequences generated fully autonomously -by our baseline agents trained with reinforcement learning. This is only possible in simple mazes, where these agents were able to reach all goals. We used the best-performing baseline for each maze and repeated exploration multiple times, until all goals were located.</p>
<p>The results are reported in Table S2. The use of automatically generated trajectories leads to only a minor decrease in the final performance, although qualitatively the trajectories of the SPTM agent become much noisier (not shown). The different texture distribution affects the results more, since visual self-localization becomes challenging with sparser textures. Yet the method still performs quite well and outperforms the baselines by a large margin.</p>
<p>S3.2 ADDITIONAL ABLATION STUDY</p>
<p>To better understand the importance of the locomotion and retrieval networks, we performed two experiments. First, we substituted the retrieval network R with simple per-pixel matching. Second, we substituted actions predicted by the locomotion network L by actions from the exploration sequence (teach-and-repeat). Note that this second approach uses information unavailable to our methodactions performed during the walkthrough sequence. It thus cannot be considered a proper baseline. We further discuss the exact settings and the results.</p>
<p>With normalization Without normalization Figure S3: Graphs constructed using per pixel matching. Shortcut connections are shown in red.</p>
<p>Most shortcuts connections are wrong -they connect distant locations.</p>
<p>S3.2.1 PER-PIXEL COMPARISON</p>
<p>This experiment was inspired by the approach of Milford &amp; Wyeth (2012). To compute the localization score, we downsample images to the resolution 40×30, convert to grayscale and then compute cosine distances between them. We experiment with two variants of this method: with local contrast normalization (similar to Milford &amp; Wyeth (2012)) and without. To perform the normalization, we split the downsampled grayscale image into patches of size 10×10. In each patch, we subtract the mean and divide by the standard deviation.</p>
<p>As Table S3 indicates, the per-pixel comparison baseline performs poorly. As shown in Figure S3, the visual shortcuts made with this technique are catastrophically wrong. Local normalization only makes the results worse because it discards information about absolute color intensity, which can be a useful cue in our environments.</p>
<p>S3.2.2 TEACH-AND-REPEAT</p>
<p>To be able to use actions from the exploration sequence, a few modification to our method are necessary. First, we introduce no shortcut connections in the graph, as we would not know the actions for them. The graph thus turns into a path, making the shortest paths longer. Second, to allow the agent to move along this path in both directions, we select an opposite for every action: for example, the opposite of moving forward is moving backward. Finally, we found that taking a fraction of completely random actions helps the agent not to get stuck when it diverges far from the exploration track and the recorded actions are not useful anymore. We found 10% of random actions to lead to good results. Overall, the method works as follows. First, the goal and the agent are localized using the same procedure as our method. Then the agent has to move either forward or backward along the exploration graph-line. If forward, then the action corresponding to the agent's localized observation is taken, if backward -the opposite of the recorded action.</p>
<p>As Table S3 suggests, this method works significantly worse than our method, even though it makes use of extra information -the recorded actions. We see two reasons for this. First, there are no shortcut connections, which makes the path to the goal longer. Second, as soon as the agent diverges from the exploration trajectory, the actions do not match the states any more, and there is no mechanism for the agent to get back on track. For instance, imagine a long corridor: if the agent is oriented at a small angle to the direction of the corridor, it will inevitably crash into a wall. Why does the approach not fail completely due to the latter problem? This is most likely because the environment is forgiving: it allows the agent to slide along walls when facing them at an angle less than 90 degrees. This way, even if the agent diverges from the exploration path, it does not break down completely and still makes progress towards the goal. Videos of successful navigation trials for this agent can be found at https://sites.google.com/view/SPTM.  Table S1: Effect of hyperparameters, evaluated on the validation set. We report the percentage of navigation trials successfully completed in 5,000 steps (higher is better).  Table S2: Evaluation of the SPTM navigation agent with homogeneous textures and automated exploration. We report the percentage of navigation trials successfully completed in 5,000 steps (higher is better).</p>
<p>Val-1 Val-2 Val-3</p>
<p>Per-pixel comparison with normalization 6 2 1 Per-pixel comparison without normalization 10 8 7 Teach-and-repeat 45 36 30 Ours -full 100 98 100 Table S3: Additional ablation study of the SPTM navigation agent. We report the percentage of navigation trials successfully completed in 5,000 steps in validation mazes (higher is better).</p>
<p>Figure 1 :
1A navigation agent equipped with semi-parametric topological memory (SPTM). Given the inputs -the current observation o and the goal observation o g -SPTM provides a waypoint observation o w . This waypoint and the current observation o are fed into the locomotion network L, which outputs the action a to be executed in the environment.</p>
<p>Figure 2 :
2The use of semi-parametric topological memory for navigation. (a) The retrieval network R localizes in the graph the vertices v a (blue) and v g (orange), corresponding to the current agent's observation o and the goal observation o g , respectively. (b) The shortest path on the graph between these vertices is computed (red arrows). (c) The waypoint vertex v w (yellow) is selected as the vertex in the shortest path that is furthest from the agent's vertex v a but can still be confidently reached by the agent. The output of the SPTM is the corresponding waypoint observation o w = o v w .</p>
<p>. o N } and actions {a 1 , . . . a N }. We then automatically generate training samples from these trajectories. Each training sample is a triple o i , o j , y ij that consists of two observations and a binary label. Two observations are considered close (y ij = 1) if they are separated by at most l = 20 time steps: |i − j| ≤ l. Negative examples are pairs where the two observations are separated by at least M · l steps, where M = 5 is a constant factor that determines the margin between positive and negative examples.</p>
<p>Memory graph. The graph is populated based on an exploration sequence provided to the agent. Denote the observations in the sequence by (o e 1 , . . . , o e Te ). Each vertex v i in the graph stores an observation of the environment, o vi = o m i . Two vertices v i and v j are connected by an edge in one of two cases: if they correspond to consecutive time steps, or if the corresponding observations are very close, as judged by the retrieval network R:</p>
<p>Figure 3 :
3SPTM-based agent navigating towards a goal in a three-dimensional maze (a). The agent aims to reach the goal, denoted by a star. Given the current agent's observation (b) and the goal observation (d), SPTM produces a waypoint observation (c). The locomotion network is then used to navigate towards the waypoint.</p>
<p>Figure 5 :Figure 6 :
56Percentage of successful navigation trials as a function of trial duration. Higher is better. A walkthrough trajectory (left) and three goal-directed navigation tracks in the Val-3 maze (right). In the walkthrough trajectory, the shortcuts automatically found in the SPTM graph are shown in red. Goal-directed navigation trials shown in Tracks 1, 2, and 3 were all successful, but Track 3 was excessively long. Start positions are shown in green, goals in red.</p>
<p>Figure S2 :
S2Percentage of successful navigation trials as a function of trial duration, in the validation mazes. Higher is better.</p>
<p>Loc. smooth. #shortcuts Mem. subsamp. s local s reachVal-1 Val-2 Val-3Parameters 
Environment </p>
<p>0 
2000 
4 
-
0.95 
95 
88 
52 
10 
2000 
4 
0.7 
0.95 
100 
98 
100 </p>
<p>10 
1000 
4 
0.7 
0.95 
99 
92 
90 
10 
2000 
4 
0.7 
0.95 
100 
98 
100 
10 
4000 
4 
0.7 
0.95 
100 
98 
100 </p>
<p>1 
2000 
4 
0.7 
0.95 
100 
70 
26 
5 
2000 
4 
0.7 
0.95 
100 
96 
99 
10 
2000 
4 
0.7 
0.95 
100 
98 
100 </p>
<p>10 
2000 
4 
0.7 
0.9 
100 
95 
100 
10 
2000 
4 
0.7 
0.95 
100 
98 
100 
10 
2000 
4 
0.7 
0.97 
100 
97 
100 </p>
<p>10 
2000 
4 
0.6 
0.95 
100 
95 
100 
10 
2000 
4 
0.7 
0.95 
100 
98 
100 
10 
2000 
4 
0.8 
0.95 
100 
97 
99 </p>
<p>10 
2000 
1 
0.7 
0.95 
76 
47 
73 
10 
2000 
2 
0.7 
0.95 
97 
85 
95 
10 
2000 
4 
0.7 
0.95 
100 
98 
100 
10 
2000 
8 
0.7 
0.95 
66 
92 
66 </p>
<p>5 
8000 
1 
0.7 
0.95 
99 
84 
93 
10 
4000 
2 
0.7 
0.95 
100 
95 
98 
20 
2000 
4 
0.7 
0.95 
100 
98 
100 
40 
1000 
8 
0.7 
0.95 
97 
94 
85 </p>
<p>Val-3 Test-1 Test-4 Test-5Ours -homogeneous textures 
55 
98 
76 
75 
Ours -automated exploration 
-
94 
93 
91 
Ours -full 
100 
100 
100 
100 </p>
<p>TensorFlow: A system for large-scale machine learning. Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, OSDI. Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, et al. TensorFlow: A system for large-scale machine learning. In OSDI, 2016.</p>
<p>A view-based neurocomputational system for relational map-making and navigation in visual environments. A Ivan, Allen M Bachelder, Waxman, Robotics and Autonomous Systems. 162Ivan A. Bachelder and Allen M. Waxman. A view-based neurocomputational system for relational map-making and navigation in visual environments. Robotics and Autonomous Systems, 16(2):267-289, 1995.</p>
<p>OpenRatSLAM: An open source brain-based SLAM system. David Ball, Scott Heath, Janet Wiles, Gordon Wyeth, Peter Corke, Michael Milford, Autonomous Robots. 343David Ball, Scott Heath, Janet Wiles, Gordon Wyeth, Peter Corke, and Michael Milford. OpenRatSLAM: An open source brain-based SLAM system. Autonomous Robots, 34(3):149-176, 2013.</p>
<p>Do animals have cognitive maps. T D Andrew, Bennett, Journal of Experimental Biology. 199Andrew T. D. Bennett. Do animals have cognitive maps? Journal of Experimental Biology, 199:219-224, 1996.</p>
<p>Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N Siddharth, Philip H S Torr, arXiv:1612.00380Playing Doom with SLAM-augmented deep reinforcement learning. Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N. Siddharth, and Philip H. S. Torr. Play- ing Doom with SLAM-augmented deep reinforcement learning. arXiv:1612.00380, 2016.</p>
<p>Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, arXiv:1606.04460Daan Wierstra, and Demis Hassabis. Model-free episodic control. Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv:1606.04460, 2016.</p>
<p>Past, present, and future of simultaneous localization and mapping: Toward the robustperception age. Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, José Neira, Ian D Reid, John J Leonard, IEEE Transactions on Robotics. 326Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, José Neira, Ian D. Reid, and John J. Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust- perception age. IEEE Transactions on Robotics, 32(6):1309-1332, 2016.</p>
<p>How honey bees use landmarks to guide their return to a food source. B A Cartwright, T S Collett, Nature. 295B. A. Cartwright and T. S. Collett. How honey bees use landmarks to guide their return to a food source. Nature, 295, 1982.</p>
<p>. François Chollet, François Chollet et al. Keras. https://github.com/fchollet/keras, 2015.</p>
<p>Insect navigation en route to the goal: Multiple strategies for the use of landmarks. T S Collett, Journal of Experimental Biology. 199T. S. Collett. Insect navigation en route to the goal: Multiple strategies for the use of landmarks. Journal of Experimental Biology, 199:227-235, 1996.</p>
<p>Learning to act by predicting the future. Alexey Dosovitskiy, Vladlen Koltun, ICLR. Alexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. In ICLR, 2017.</p>
<p>Simultaneous localization and mapping: Part I. IEEE Robotics and Automation Magazine. F Hugh, Tim Durrant-Whyte, Bailey, 13Hugh F. Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: Part I. IEEE Robotics and Automation Magazine, 13(2):99-110, 2006.</p>
<p>Sonar-based real-world mapping and navigation. Alberto Elfes, IEEE Journal on Robotics and Automation. 33Alberto Elfes. Sonar-based real-world mapping and navigation. IEEE Journal on Robotics and Automation, 3 (3):249-265, 1987.</p>
<p>Do humans integrate routes into a cognitive map? Map-versus landmark-based navigation of novel shortcuts. Patrick Foo, William H Warren, Andrew Duchon, Michael J Tarr, Journal of Experimental Psychology. 312Patrick Foo, William H. Warren, Andrew Duchon, and Michael J. Tarr. Do humans integrate routes into a cognitive map? Map-versus landmark-based navigation of novel shortcuts. Journal of Experimental Psychology, 31(2):195-215, 2005.</p>
<p>Learning view graphs for robot navigation. Matthias O Franz, Bernhard Schölkopf, A Hanspeter, Heinrich H Mallot, Bülthoff, Autonomous Robots. 51Matthias O. Franz, Bernhard Schölkopf, Hanspeter A. Mallot, and Heinrich H. Bülthoff. Learning view graphs for robot navigation. Autonomous Robots, 5(1):111-125, 1998.</p>
<p>Topological mapping, localization and navigation using image collections. Friedrich Fraundorfer, Christopher Engels, David Nistér, IROS. Friedrich Fraundorfer, Christopher Engels, and David Nistér. Topological mapping, localization and navigation using image collections. In IROS, 2007.</p>
<p>Navigation and acquisition of spatial knowledge in a virtual maze. Sabine Gillner, A Hanspeter, Mallot, Journal of Cognitive Neuroscience. 104Sabine Gillner and Hanspeter A. Mallot. Navigation and acquisition of spatial knowledge in a virtual maze. Journal of Cognitive Neuroscience, 10(4):445-463, 1998.</p>
<p>Deep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT PressIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.</p>
<p>Cognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik, CVPR. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. In CVPR, 2017.</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>Image-based homing. J Hong, X Tan, B Pinette, R Weiss, E M Riseman, IEEE Control Systems. 121J. Hong, X. Tan, B. Pinette, R. Weiss, and E. M. Riseman. Image-based homing. IEEE Control Systems, 12(1): 38-45, 1992.</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu, Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In ICLR, 2017.</p>
<p>Multiple stored views and landmark guidance in ants. S P D Judd, T S Collett, Nature. S. P. D. Judd and T. S. Collett. Multiple stored views and landmark guidance in ants. Nature, 1998.</p>
<p>ViZDoom: A Doom-based AI research platform for visual reinforcement learning. Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski, IEEE Conference on Computational Intelligence and Games. Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, 2016.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.</p>
<p>A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. Benjamin Kuipers, Yung-Tai Byun, Robotics and Autonomous Systems. 81Benjamin Kuipers and Yung-Tai Byun. A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. Robotics and Autonomous Systems, 8(1):47-63, 1991.</p>
<p>Do not ask whether they have a cognitive map, but how they find their way about. N J Mackintosh, Psicologica. 23N. J. Mackintosh. Do not ask whether they have a cognitive map, but how they find their way about. Psicologica, 23:165-185, 2002.</p>
<p>NEURO-NAV: A neural network based architecture for vision-guided mobile robot navigation using non-metrical models of the environment. Min Meng, Avinash C Kak, ICRA. Min Meng and Avinash C. Kak. NEURO-NAV: A neural network based architecture for vision-guided mobile robot navigation using non-metrical models of the environment. In ICRA, 1993.</p>
<p>Mapping a suburb with a single camera using a biologically inspired SLAM system. Michael Milford, Gordon Wyeth, IEEE Transactions on Robotics. 245Michael Milford and Gordon Wyeth. Mapping a suburb with a single camera using a biologically inspired SLAM system. IEEE Transactions on Robotics, 24(5):1038-1053, 2008.</p>
<p>Persistent navigation and mapping using a biologically inspired SLAM system. Michael Milford, Gordon Wyeth, International Journal of Robotics Research. 299Michael Milford and Gordon Wyeth. Persistent navigation and mapping using a biologically inspired SLAM system. International Journal of Robotics Research, 29(9):1131-1153, 2010.</p>
<p>SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights. Michael Milford, Gordon Wyeth, ICRA. Michael Milford and Gordon Wyeth. SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights. In ICRA, 2012.</p>
<p>Learning to navigate in complex environments. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell, ICLR. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia Hadsell. Learning to navigate in complex environments. In ICLR, 2017.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, et al. Human-level control through deep reinforcement learning. Nature, 518(7540), 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, ICML. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016.</p>
<p>Control of memory, active perception, and action in Minecraft. Junhyuk Oh, Valliappa Chockalingam, P Satinder, Honglak Singh, Lee, ICML. Junhyuk Oh, Valliappa Chockalingam, Satinder P. Singh, and Honglak Lee. Control of memory, active percep- tion, and action in Minecraft. In ICML, 2016.</p>
<p>The Hippocampus as a Cognitive Map. O&apos; John, Lynn Keefe, Nadel, Clarendon PressOxfordJohn O'Keefe and Lynn Nadel. The Hippocampus as a Cognitive Map. Oxford: Clarendon Press, 1978.</p>
<p>Neural map: Structured memory for deep reinforcement learning. Emilio Parisotto, Ruslan Salakhutdinov, ICLR. Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. In ICLR, 2018.</p>
<p>Curiosity-driven exploration by selfsupervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self- supervised prediction. In ICML, 2017.</p>
<p>Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Oriol Vinyals. Adrià Puigdomènech BadiaAlexander Pritzel, Benigno Uria, Sriram Srinivasan, Adrià Puigdomènech Badia, Oriol Vinyals, Demis Hass- abis, Daan Wierstra, and Charles Blundell. Neural episodic control. In ICML, 2017.</p>
<p>MINOS: Multimodal indoor simulator for navigation in complex environments. Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun, arXiv:1712.03931Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv:1712.03931, 2017.</p>
<p>View-based cognitive mapping and path planning. Bernhard Schölkopf, A Hanspeter, Mallot, Adaptive Behavior. 33Bernhard Schölkopf and Hanspeter A. Mallot. View-based cognitive mapping and path planning. Adaptive Behavior, 3(3):311-348, 1995.</p>
<p>Some limitations on the use of distal cues in place navigation by rats. R J Sutherland, G L Chew, J C Baker, R C Linggard, Psychobiology. 151R. J. Sutherland, G. L. Chew, J. C. Baker, and R. C. Linggard. Some limitations on the use of distal cues in place navigation by rats. Psychobiology, 15(1):48-57, 1987.</p>
<p>Value iteration networks. Aviv Tamar, Sergey Levine, Pieter Abbeel, Yi Wu, Garrett Thomas, NIPS. Aviv Tamar, Sergey Levine, Pieter Abbeel, Yi Wu, and Garrett Thomas. Value iteration networks. In NIPS, 2016.</p>
<p>Learning metric-topological maps for indoor mobile robot navigation. Sebastian Thrun, Artificial Intelligence. 991Sebastian Thrun. Learning metric-topological maps for indoor mobile robot navigation. Artificial Intelligence, 99(1):21-71, 1998.</p>
<p>Probabilistic Robotics. Sebastian Thrun, Wolfram Burgard, Dieter Fox, The MIT PressSebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic Robotics. The MIT Press, 2005.</p>
<p>Cognitive maps in rats and men. C Edward, Tolman, Psychological Review. 554Edward C. Tolman. Cognitive maps in rats and men. Psychological Review, 55(4):189-208, 1948.</p>
<p>Biologically based artificial navigation systems: Review and prospects. Olivier Trullier, Sidney I Wiener, Alain Berthoz, Jean-Arcady Meyer, Progress in Neurobiology. 515Olivier Trullier, Sidney I. Wiener, Alain Berthoz, and Jean-Arcady Meyer. Biologically based artificial naviga- tion systems: Review and prospects. Progress in Neurobiology, 51(5):483-544, 1997.</p>
<p>Human spatial representation: Insights from animals. Frances Ranxiao, Elizabeth S Wang, Spelke, Trends in Cognitive Sciences. 69Ranxiao Frances Wang and Elizabeth S Spelke. Human spatial representation: Insights from animals. Trends in Cognitive Sciences, 6(9):376-382, 2002.</p>
<p>Learning to compare image patches via convolutional neural networks. Sergey Zagoruyko, Nikos Komodakis, CVPR. Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional neural net- works. In CVPR, 2015.</p>
<p>Targetdriven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi, ICRA. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target- driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
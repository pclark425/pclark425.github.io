<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1814 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1814</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1814</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-04d65765bcbd8ad0cba8db50137a4bf97f900856</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04d65765bcbd8ad0cba8db50137a4bf97f900856" target="_blank">Traversing the Reality Gap via Simulator Tuning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that even optimised for different tasks that different physics engine perform better in certain scenarios and that friction and maximum actuator velocity are tightly bounded parameters that greatly impact the transference of simulated solutions.</p>
                <p><strong>Paper Abstract:</strong> The large demand for simulated data has made the reality gap a problem on the forefront of robotics. We propose a method to traverse the gap by tuning available simulation parameters. Through the optimisation of physics engine parameters, we show that we are able to narrow the gap between simulated solutions and a real world dataset, and thus allow more ready transfer of leaned behaviours between the two. We subsequently gain understanding as to the importance of specific simulator parameters, which is of broad interest to the robotic machine learning community. We find that even optimised for different tasks that different physics engine perform better in certain scenarios and that friction and maximum actuator velocity are tightly bounded parameters that greatly impact the transference of simulated solutions.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1814.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1814.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulator Tuning (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traversing the Reality Gap via Simulator Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This study optimises simulator/physics-engine parameters (time step, friction, joint velocity, etc.) using Differential Evolution to minimise Euclidean trajectory error between simulation and a motion-capture real-world dataset of a Kinova 6DOF manipulator, with the goal of narrowing the reality gap and enabling easier sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kinova 6DOF robotic manipulator (KG-3 gripper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 6-degree-of-freedom Kinova manipulator equipped with a KG-3 gripper used to perform kinematic trajectories and non-prehensile object interactions (cubes, cuboids, cylinders, cones) recorded by a motion-capture system.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet and V-Rep (with Bullet 2.78/2.83/2.85, ODE, Newton physics engines)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rigid-body simulators providing gravity, collision/contact handling, joint constraints, actuator limits and configurable material properties (friction, restitution), with tunable physics engine parameters exposed via PyBullet and PyRep/V-Rep.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Medium-fidelity rigid-body physics (tunable contact and actuator approximations; not full high-fidelity actuator/sensor models or photorealistic rendering).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Gravity, rigid-body dynamics, contact and collision handling, lateral/rolling/sliding friction coefficients, restitution, joint max velocity and torque limits, mass and inertia (link and object), time-stepping and solver settings.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Actuator internal dynamics (e.g. torque dynamics, gear backlash, motor control delays) modeled only via max velocity/torque bounds rather than detailed servo models; sensor noise/latency not explicitly modeled; visual realism not modeled; some inertia/centre-of-mass/tensor parameters not fully tuned due to practicality; simplified contact models (engine-dependent approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Public motion-capture dataset where the Kinova arm executed 10 elementary tasks (2 pure kinematic, 8 non-prehensile object manipulations) with 6DOF poses of manipulator and objects recorded; objects include plastic and wooden replicas.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Kinematic trajectories and non-prehensile manipulation behaviours (pushing/rolling interactions, object displacement) — the work focuses on aligning simulation behaviour to recorded real trajectories to facilitate later transfer of learned behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Simulation parameter optimisation via Differential Evolution (black-box evolutionary algorithm) to minimise trajectory Euclidean distance to the real dataset (not policy training).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Fitness = mean per-timestep 3D Euclidean distance between simulated and recorded wrist poses (averaged over trajectory samples) plus final object position Euclidean error for object tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Generic (untuned) simulator fitness values (Euclidean distance) reported per Table II varied widely (examples: generic fitness per-experiment ranged from ~0.1069 to >995 depending on engine and task anomalies); best tuned shared-parameter fitnesses (Table III) ranged from 0.0442 to 1.7360 (units: metres of Euclidean error); tuning produced improvements of ~14% up to ~91% on per-experiment comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatched simulation timestep; incorrect lateral/rolling/sliding friction coefficients (object, gripper, floor); inaccurate max joint velocity bounds; imperfect restitution values for higher-energy contacts; mass/inertia mis-specification; choice of physics engine (engine-specific contact/solver behaviour); simplifications in actuator and sensor modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Optimising simulator parameters (especially timestep, lateral friction, and max joint velocity) toward measured real-world values; using a motion-capture ground-truth dataset for numerical fitness; selecting physics engines suited to the task class (e.g., Newton performed best for pure kinematics; PyBullet often best for rolling tasks); constraining parameter variation for tightly-bounded parameters rather than excessive randomisation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Yes — accurate timestep, lateral object friction, and maximum joint velocity are critical and tightly bounded for realism; restitution may be important for high-energy contact tasks; authors recommend using developer-recommended timestep and experimentally measured friction and joint velocity values rather than broad randomisation.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper compares multiple physics engines and 'shared' vs 'individual' parameter tuning. Results: no single physics engine is best for all tasks — Newton excelled at pure kinematics, PyBullet at several rolling/rolling-object tasks, ODE often best for combined tasks; tuning shared parameters consistently reduced simulation-to-reality error versus generic settings, while broad individual-parameter tuning sometimes worsened results due to higher-dimensional optimisation difficulty and compute/time limits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Optimising simulator parameters using DE significantly reduces simulation–reality trajectory error versus generic settings (improvements ranging roughly 14%–91% on experiments); 2) The most influential parameters are simulation timestep, lateral object friction, and maximum joint velocity; 3) Restitution matters for higher-energy contacts; 4) Physics engines differ in strengths by task type — choose/ tune engine per scenario; 5) Excessive parameter dimensionality complicates optimisation (more parameters can reduce optimisation reliability without more compute).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traversing the Reality Gap via Simulator Tuning', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1814.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1814.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (sim2real method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commonly used sim-to-real approach where simulation parameters (visual or physical) are randomized across training episodes so controllers become robust to variations and thus transfer to reality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Domain randomization method</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Methodology to expose learned controllers to wide variations of simulated environments (visual/physical parameters) to improve robustness for real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>sim-to-real methodology for robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>N/A (method applicable across simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Not a particular environment; applies randomized draws of simulator parameters (visual and/or physical) during training runs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approach-agnostic; can be applied with simplified or high-fidelity simulators (the paper discusses it as an alternative to simulator tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>When used, typically visual characteristics (lighting, textures) and/or physical properties (mass, friction, restitution) are randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Domain randomization intentionally includes unrealistic parameter combinations; thus it does not require per-parameter high fidelity to real-world values.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General-purpose robustness for tasks ranging from visual perception to physical interaction; in context referenced for manipulation and locomotion policies in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Usually reinforcement learning with randomized simulator parameters (paper mentions as community approach but did not apply in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Discussed qualitatively; authors note domain randomization requires hand-picked parameter ranges, can be time-consuming to train due to exposure to many unrealistic scenarios, and that accurate parameter ranges would reduce training time.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Addresses unmodeled variability but can be hindered by poorly chosen randomization bounds and large search spaces that slow learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Accurate, task-specific parameter bounds and/or progressive/randomisation schedules can reduce training time; alternatively, combining randomization with targeted simulator tuning may be beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper positions domain randomization as a mainstream sim-to-real approach but highlights downsides: training is time-consuming, slow initial learning, and benefits from accurate parameter ranges which their tuning approach aims to provide.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traversing the Reality Gap via Simulator Tuning', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1814.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1814.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differential Evolution Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differential Evolution (DE) optimisation of simulator parameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the DE evolutionary algorithm to optimise a vector of simulator/physics-engine parameters (shared and engine-specific) to minimise trajectory Euclidean error to real motion-capture data, enabling a more realistic simulator configuration for downstream transfer work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>DE-based simulator parameter optimiser</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A black-box optimisation agent that evolves populations of simulator parameter vectors (time step, masses, frictions, joint limits, damping, restitution, etc.) using the 'best1bin' DE strategy to minimise a defined fitness (sim-vs-real Euclidean error).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>simulation system identification / sim-to-real preparation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Applied to PyBullet and V-Rep (multiple physics engines)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>DE iteratively runs simulator episodes with candidate parameters and evaluates Euclidean trajectory error against the motion-capture dataset to guide evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Parameter-level tuning of medium-fidelity rigid-body simulators (improves fidelity by matching parameters rather than changing fundamental engine models).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Optimises parameters that affect contact, dynamics and actuator limits: time step, lateral/rolling/sliding friction, restitution, masses, maximum joint velocities/torques, damping.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not create new high-fidelity models of actuators/sensors; limited by accessible simulator parameters and practical exclusion of full inertia tensor and centre-of-mass rotation tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Optimisation target is motion-capture ground-truth trajectories from the Kinova manipulator dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Not directly training policies; prepares simulators so behaviours learned later in that tuned simulation are more likely to match real-world trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Evolutionary optimisation (Differential Evolution, best1bin strategy, population size scaled to parameter dimensionality).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Same fitness used as optimisation objective: mean per-timestep Euclidean error of wrist pose plus object final position error.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>DE produced lower fitness (closer match) than generic settings across experiments; shared-parameter tuning best fitness examples: as low as 0.0442 m for some object tasks, with combined-task experiment still showing larger residual error (1.7360 m).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>High-dimensional parameter spaces increase optimisation difficulty and noise; limited compute/time budgets can prevent convergence; some engines present noisier fitness landscapes making optimisation harder (e.g., Bullet variants required many generations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using DE with appropriate population sizing and termination criteria, constraining to a subset of shared influential parameters can yield robust improvements; running on HPC with sufficient compute enables large numbers of evaluations needed for high-dimensional tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>DE tuning of 'shared' parameters reliably improved fidelity across engines; tuning a larger set of 'individual' parameters sometimes degraded performance due to optimisation difficulty and compute/time limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Traversing the Reality Gap via Simulator Tuning', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-Real: Learning Agile Locomotion For Quadruped Robots <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience <em>(Rating: 2)</em></li>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 2)</em></li>
                <li>Noise and the reality gap: The use of simulation in evolutionary robotics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1814",
    "paper_id": "paper-04d65765bcbd8ad0cba8db50137a4bf97f900856",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Simulator Tuning (this work)",
            "name_full": "Traversing the Reality Gap via Simulator Tuning",
            "brief_description": "This study optimises simulator/physics-engine parameters (time step, friction, joint velocity, etc.) using Differential Evolution to minimise Euclidean trajectory error between simulation and a motion-capture real-world dataset of a Kinova 6DOF manipulator, with the goal of narrowing the reality gap and enabling easier sim-to-real transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Kinova 6DOF robotic manipulator (KG-3 gripper)",
            "agent_system_description": "A 6-degree-of-freedom Kinova manipulator equipped with a KG-3 gripper used to perform kinematic trajectories and non-prehensile object interactions (cubes, cuboids, cylinders, cones) recorded by a motion-capture system.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "PyBullet and V-Rep (with Bullet 2.78/2.83/2.85, ODE, Newton physics engines)",
            "virtual_environment_description": "Rigid-body simulators providing gravity, collision/contact handling, joint constraints, actuator limits and configurable material properties (friction, restitution), with tunable physics engine parameters exposed via PyBullet and PyRep/V-Rep.",
            "simulation_fidelity_level": "Medium-fidelity rigid-body physics (tunable contact and actuator approximations; not full high-fidelity actuator/sensor models or photorealistic rendering).",
            "fidelity_aspects_modeled": "Gravity, rigid-body dynamics, contact and collision handling, lateral/rolling/sliding friction coefficients, restitution, joint max velocity and torque limits, mass and inertia (link and object), time-stepping and solver settings.",
            "fidelity_aspects_simplified": "Actuator internal dynamics (e.g. torque dynamics, gear backlash, motor control delays) modeled only via max velocity/torque bounds rather than detailed servo models; sensor noise/latency not explicitly modeled; visual realism not modeled; some inertia/centre-of-mass/tensor parameters not fully tuned due to practicality; simplified contact models (engine-dependent approximations).",
            "real_environment_description": "Public motion-capture dataset where the Kinova arm executed 10 elementary tasks (2 pure kinematic, 8 non-prehensile object manipulations) with 6DOF poses of manipulator and objects recorded; objects include plastic and wooden replicas.",
            "task_or_skill_transferred": "Kinematic trajectories and non-prehensile manipulation behaviours (pushing/rolling interactions, object displacement) — the work focuses on aligning simulation behaviour to recorded real trajectories to facilitate later transfer of learned behaviours.",
            "training_method": "Simulation parameter optimisation via Differential Evolution (black-box evolutionary algorithm) to minimise trajectory Euclidean distance to the real dataset (not policy training).",
            "transfer_success_metric": "Fitness = mean per-timestep 3D Euclidean distance between simulated and recorded wrist poses (averaged over trajectory samples) plus final object position Euclidean error for object tasks.",
            "transfer_performance_sim": "Generic (untuned) simulator fitness values (Euclidean distance) reported per Table II varied widely (examples: generic fitness per-experiment ranged from ~0.1069 to &gt;995 depending on engine and task anomalies); best tuned shared-parameter fitnesses (Table III) ranged from 0.0442 to 1.7360 (units: metres of Euclidean error); tuning produced improvements of ~14% up to ~91% on per-experiment comparisons.",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Mismatched simulation timestep; incorrect lateral/rolling/sliding friction coefficients (object, gripper, floor); inaccurate max joint velocity bounds; imperfect restitution values for higher-energy contacts; mass/inertia mis-specification; choice of physics engine (engine-specific contact/solver behaviour); simplifications in actuator and sensor modeling.",
            "transfer_enabling_conditions": "Optimising simulator parameters (especially timestep, lateral friction, and max joint velocity) toward measured real-world values; using a motion-capture ground-truth dataset for numerical fitness; selecting physics engines suited to the task class (e.g., Newton performed best for pure kinematics; PyBullet often best for rolling tasks); constraining parameter variation for tightly-bounded parameters rather than excessive randomisation.",
            "fidelity_requirements_identified": "Yes — accurate timestep, lateral object friction, and maximum joint velocity are critical and tightly bounded for realism; restitution may be important for high-energy contact tasks; authors recommend using developer-recommended timestep and experimentally measured friction and joint velocity values rather than broad randomisation.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "The paper compares multiple physics engines and 'shared' vs 'individual' parameter tuning. Results: no single physics engine is best for all tasks — Newton excelled at pure kinematics, PyBullet at several rolling/rolling-object tasks, ODE often best for combined tasks; tuning shared parameters consistently reduced simulation-to-reality error versus generic settings, while broad individual-parameter tuning sometimes worsened results due to higher-dimensional optimisation difficulty and compute/time limits.",
            "key_findings": "1) Optimising simulator parameters using DE significantly reduces simulation–reality trajectory error versus generic settings (improvements ranging roughly 14%–91% on experiments); 2) The most influential parameters are simulation timestep, lateral object friction, and maximum joint velocity; 3) Restitution matters for higher-energy contacts; 4) Physics engines differ in strengths by task type — choose/ tune engine per scenario; 5) Excessive parameter dimensionality complicates optimisation (more parameters can reduce optimisation reliability without more compute).",
            "uuid": "e1814.0",
            "source_info": {
                "paper_title": "Traversing the Reality Gap via Simulator Tuning",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Domain Randomization (mentioned)",
            "name_full": "Domain Randomization (sim2real method)",
            "brief_description": "A commonly used sim-to-real approach where simulation parameters (visual or physical) are randomized across training episodes so controllers become robust to variations and thus transfer to reality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "Domain randomization method",
            "agent_system_description": "Methodology to expose learned controllers to wide variations of simulated environments (visual/physical parameters) to improve robustness for real-world deployment.",
            "domain": "sim-to-real methodology for robotics",
            "virtual_environment_name": "N/A (method applicable across simulators)",
            "virtual_environment_description": "Not a particular environment; applies randomized draws of simulator parameters (visual and/or physical) during training runs.",
            "simulation_fidelity_level": "Approach-agnostic; can be applied with simplified or high-fidelity simulators (the paper discusses it as an alternative to simulator tuning).",
            "fidelity_aspects_modeled": "When used, typically visual characteristics (lighting, textures) and/or physical properties (mass, friction, restitution) are randomized.",
            "fidelity_aspects_simplified": "Domain randomization intentionally includes unrealistic parameter combinations; thus it does not require per-parameter high fidelity to real-world values.",
            "real_environment_description": null,
            "task_or_skill_transferred": "General-purpose robustness for tasks ranging from visual perception to physical interaction; in context referenced for manipulation and locomotion policies in literature.",
            "training_method": "Usually reinforcement learning with randomized simulator parameters (paper mentions as community approach but did not apply in experiments).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": "Discussed qualitatively; authors note domain randomization requires hand-picked parameter ranges, can be time-consuming to train due to exposure to many unrealistic scenarios, and that accurate parameter ranges would reduce training time.",
            "sim_to_real_gap_factors": "Addresses unmodeled variability but can be hindered by poorly chosen randomization bounds and large search spaces that slow learning.",
            "transfer_enabling_conditions": "Accurate, task-specific parameter bounds and/or progressive/randomisation schedules can reduce training time; alternatively, combining randomization with targeted simulator tuning may be beneficial.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Paper positions domain randomization as a mainstream sim-to-real approach but highlights downsides: training is time-consuming, slow initial learning, and benefits from accurate parameter ranges which their tuning approach aims to provide.",
            "uuid": "e1814.1",
            "source_info": {
                "paper_title": "Traversing the Reality Gap via Simulator Tuning",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Differential Evolution Tuning",
            "name_full": "Differential Evolution (DE) optimisation of simulator parameters",
            "brief_description": "Use of the DE evolutionary algorithm to optimise a vector of simulator/physics-engine parameters (shared and engine-specific) to minimise trajectory Euclidean error to real motion-capture data, enabling a more realistic simulator configuration for downstream transfer work.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "DE-based simulator parameter optimiser",
            "agent_system_description": "A black-box optimisation agent that evolves populations of simulator parameter vectors (time step, masses, frictions, joint limits, damping, restitution, etc.) using the 'best1bin' DE strategy to minimise a defined fitness (sim-vs-real Euclidean error).",
            "domain": "simulation system identification / sim-to-real preparation",
            "virtual_environment_name": "Applied to PyBullet and V-Rep (multiple physics engines)",
            "virtual_environment_description": "DE iteratively runs simulator episodes with candidate parameters and evaluates Euclidean trajectory error against the motion-capture dataset to guide evolution.",
            "simulation_fidelity_level": "Parameter-level tuning of medium-fidelity rigid-body simulators (improves fidelity by matching parameters rather than changing fundamental engine models).",
            "fidelity_aspects_modeled": "Optimises parameters that affect contact, dynamics and actuator limits: time step, lateral/rolling/sliding friction, restitution, masses, maximum joint velocities/torques, damping.",
            "fidelity_aspects_simplified": "Does not create new high-fidelity models of actuators/sensors; limited by accessible simulator parameters and practical exclusion of full inertia tensor and centre-of-mass rotation tuning.",
            "real_environment_description": "Optimisation target is motion-capture ground-truth trajectories from the Kinova manipulator dataset.",
            "task_or_skill_transferred": "Not directly training policies; prepares simulators so behaviours learned later in that tuned simulation are more likely to match real-world trajectories.",
            "training_method": "Evolutionary optimisation (Differential Evolution, best1bin strategy, population size scaled to parameter dimensionality).",
            "transfer_success_metric": "Same fitness used as optimisation objective: mean per-timestep Euclidean error of wrist pose plus object final position error.",
            "transfer_performance_sim": "DE produced lower fitness (closer match) than generic settings across experiments; shared-parameter tuning best fitness examples: as low as 0.0442 m for some object tasks, with combined-task experiment still showing larger residual error (1.7360 m).",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "High-dimensional parameter spaces increase optimisation difficulty and noise; limited compute/time budgets can prevent convergence; some engines present noisier fitness landscapes making optimisation harder (e.g., Bullet variants required many generations).",
            "transfer_enabling_conditions": "Using DE with appropriate population sizing and termination criteria, constraining to a subset of shared influential parameters can yield robust improvements; running on HPC with sufficient compute enables large numbers of evaluations needed for high-dimensional tuning.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "DE tuning of 'shared' parameters reliably improved fidelity across engines; tuning a larger set of 'individual' parameters sometimes degraded performance due to optimisation difficulty and compute/time limits.",
            "uuid": "e1814.2",
            "source_info": {
                "paper_title": "Traversing the Reality Gap via Simulator Tuning",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-Real: Learning Agile Locomotion For Quadruped Robots",
            "rating": 2
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
            "rating": 2
        },
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 2
        },
        {
            "paper_title": "Noise and the reality gap: The use of simulation in evolutionary robotics",
            "rating": 1
        }
    ],
    "cost": 0.01337825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Traversing the Reality Gap via Simulator Tuning</h1>
<p>Jack Collins ${ }^{1,2}$, Ross Brown ${ }^{2}$, Jürgen Leitner ${ }^{2,3,4}$ and David Howard ${ }^{1}$</p>
<h4>Abstract</h4>
<p>The large demand for simulated data has made the reality gap a problem on the forefront of robotics. We propose a method to traverse the gap by tuning available simulation parameters. Through the optimisation of physics engine parameters, we show that we are able to narrow the gap between simulated solutions and a real world dataset, and thus allow more ready transfer of leaned behaviours between the two. We subsequently gain understanding as to the importance of specific simulator parameters, which is of broad interest to the robotic machine learning community. We find that even optimised for different tasks that different physics engine perform better in certain scenarios and that friction and maximum actuator velocity are tightly bounded parameters that greatly impact the transference of simulated solutions.</p>
<p>Index Terms-Reality Gap, sim2real, Differential Evolution, Simulator</p>
<h2>I. INTRODUCTION</h2>
<p>Physics simulations attempt to model some pertinent facets of the real world in software. Simulations are necessarily simplified for computational feasibility, yet reflect real-world phenomena at a given level of veracity, the extent of which is the result of a trade-off between accuracy and computational time. In the domain of robotics, rigid-body simulators are frequently used as a large proportion of robots can be wellmodelled as rigid bodies. Robotics simulators reproduce the most important physical phenomena (i.e. gravity, collisions, etc.) but replace detailed modelling of complex phenomena with computationally faster, less accurate high-level representations and constraints. Robotics simulations variously rely on the replication of phenomena that are difficult to accurately replicate, e.g., simulating actuators (i.e. torque characteristics, gear backlash, ...), sensors (i.e. noise, latency, ...), and rendered images (i.e. reflections, refraction, textures, ...). This gap between reality and simulation is commonly referred to as the "Reality Gap".</p>
<p>Although conducting research in simulation means having to overcome the reality gap, the associated pros outweigh the cons for many learning-based approaches. In simulation there is no risk of damaging hardware whilst having access to robots that are not physically available. In addition, many instantiations of a scene can be run in parallel potentially faster than real-time, and human intervention is not required to manage experiments. With the current surge in data-driven techniques like Deep Learning, simulation data is either a prerequisite to using such techniques, or at least a more attainable</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. 3D plot of a robotic manipulators end effector trajectory comparing (i) real world, (ii) untuned simulator, and (iii) tuned simulator. See magnified Excerpt A which visualises the advantages of the tuned simulation over the generic simulation.
alternative to the (generally) expensive, laborious and nonscaleable collection of real-world data.</p>
<p>Of course, working in simulation only makes sense if the eventually-learned behaviour can transfer to reality. Sim2real research aspires to make any simulated behaviour seamlessly transfer to hardware and operate in real-world conditions. There are three prevalent ideologies within the sim2real community for overcome the reality gap, (i) data-driven improvement of simulation, (ii) generation of robust controllers, and (iii) a hybrid approach combining (i) and (ii). (i) augments the simulation with real-world data. This approach suffers as collection of data from the real world remains expensive. In comparison, (ii) must expose a controller to a wide range of environments through the randomisation of a subset of</p>
<p>simulation parameters or introduction of noise. Due to the controller being exposed to both realistic and (a large number of) unrealistic scenarios the creation of such controllers is time consuming. (iii) attempts to mitigate the disadvantages of collecting real world data by hand and simulating a large number of unrealistic training scenes but in doing so adds the complexity of integrating simulation and the real world into a single workflow.</p>
<p>In this work we build on past research that describes a method for recording data for comparison between reality and simulation [1], as well as the provision of metrics for a principled, numerical quantification of the differences between simulation and reality. Our extension investigates the optimisation of simulation parameters towards the goal of achieving real world simulation performance. We show that our approach is able to attain better performance than generic simulator parameters (as visualised in Figure 1) and is a promising method for traversing the reality gap. Our contributions aside from a method to optimise simulator parameters include an analysis as to the most influential parameters in achieving real world results.</p>
<p>The motivation for our work is to inform researchers and those applying sim2real techniques as to the important parameters to tune in simulation. As an extension of the results, further conclusions can be drawn as to the best data to collect from the real world to make realistic simulations, the parameters to randomise and the extent of learning approach refinement.</p>
<p>The question we endeavour to answer is; what are the simulation parameters that are most influential in arriving at a realistic simulation? Our experimentation involves optimising two popular rigid body simulators used by the robotics community to faithfully replicate the results of a series of tasks conducted by a real robot in a motion capture system. The tasks are a range of kinematic movements and object interactions performed by a robotic manipulator, and the results of this 'ground truth' are available as a publiclyaccessible dataset [2]. Optimisation of simulator parameters is via differential evolution, an Evolutionary Algorithm that performs well on high-dimensional optimisation problems.</p>
<h2>II. BACKGROUND</h2>
<h2>A. Simulation and Physics Engines</h2>
<p>Rigid-body simulators are a class of simulators that simplify the world into rigid objects that are potentially connected through (actuated or unactuated) joints. The use of rigid-body simulators is prevalent in robotics due to the widespread use of rigid robots in the field. Simulators are often modular, with the simulator acting as a high level interface, typically including a graphical user interface, API accessibility from external programming languages, plugins, importers, and scene description formats [3].</p>
<p>Robotic simulators utilise one or more of a number of physics engines. Common physics engines include Bullet [4], Dynamic Animation and Robotics Toolkit (DART) [5] and Open Dynamics Engine (ODE) [6], all of which are licensed under free software licenses. The physics engine operates below the simulator with the goal of providing physically accurate movement of objects instantiated in the simulator.</p>
<p>A multitude of user definable parameters are available to be tuned, however the exact number varies between engines and implementations. Parameters relate to the visual aspects (colours, textures, etc. ), material properties (frictions, restitution, etc.), object properties (mass, inertia, etc.), joint characteristics (type of constraint, actuation, etc.) and other more general physics engine properties (time step, solver settings, dampening, etc.). Although a large tunable parameter space represents an opportunity to adapt a physics engine to accurately replicate real-world conditions, it also results in a high-dimensional optimisation problem, where the effects of varying parts of this parameter space on simulator performance is not intuitive.</p>
<h2>B. Reality Gap</h2>
<p>With the well accepted problem of the reality gap the only way currently to guarantee a solution will perform as expected in reality is to create the solution in reality, to this end testrigs are a proven method for overcoming some of the issues associated with working in the real world [7], [8]. There are a range of approaches that have been raised in the past to cross the reality gap starting most notably with Jakobi et al. [9] using targeted noise to generate robust controllers. Other sim2real approaches have focused on tuning controllers [10], optimising transferable controllers [11], adding perturbations to the environment [12] and learning the target platforms actuator responses [13].</p>
<p>There are several methods of overcoming the reality gap using real-world data augmentation. The most common is to alter the generic simulator settings with more accurate parameters that are collected from real world measurements, derived from calculations, from researched values or experimentally [12], [14], [15]. Oftentimes parameters such as weight, physical dimensions, frictional coefficients, centre of mass, inertial properties, actuator control properties and more are used [16], [17]. Another common practice is to substitute a more accurate model of an actuator derived from the response of the physical robot or parameters from system identification, examples of this include work by Andrychowicz et al. [18] and Tan et al. [12].</p>
<p>To update parameters using recorded data there are several documented approaches with a portion of these updating simulation parameters live. One example of live parameter updates is by Moeckel et al. using a Kinect sensor coupled with background subtraction [19] to detect gait transference. Although motion capture systems that give accurate 6DOF pose are becoming increasingly common equipment in research labs, few methods have been reported that use them [18].</p>
<p>Domain randomisation is currently the most popular method for overcoming the reality gap in the machine learning domain. The parameters to be randomised are chosen according to the policy which is being learned, tasks that are entirely</p>
<p>visual or require computer vision focus on randomising the visual characteristics of a simulation (i.e. parameters relating to cameras, colours, textures, lighting, etc.) [20]. A policy that requires environmental interactions will randomise physical properties (i.e. mass, inertia, friction, etc. ) [21]. Randomised parameters are bounded with limits at initialisation which are hand picked by the user, these are often plausible ranges that will be found within the real world, although not necessarily accurate to the operating conditions of the target robot and environment.</p>
<p>By presenting such varied scenes to an agent, extensive amounts of time are required to learn. In particular, slow initial learning rates are noted, and approaches have attempted to overcome this by progressively increasing the variation experienced [22]. Accurate parameter ranges that are specific to the parameter settings would further reduce the landscape and lead to quicker training times, and this is one contribution of our work.</p>
<h2>C. Parameter Optimisation</h2>
<p>As discussed there are a large number of available parameters to optimise when applied to simulators and physics engines. As rigid-body simulators commonly used in robotics are non-differentiable [23] the optimisation of parameters relies on gradient free algorithms. Bayesian optimisation was considered for the task of finding a global minimum as it provides an efficient sampling method requiring reduced simulation evaluations [24]. However, Bayesian optimisation is limited in the number of variables it is able to optimise, common practice is to optimise up to 10 [25]. Extensions of Bayesian Optimisation have seen this extended further up to 30 variables using drop out [26].</p>
<p>Evolutionary Algorithms (EAs) are another class of blackbox optimisation algorithms that have been applied to problems with larger search spaces. Differential Evolution (DE) [27] is a popular EA, a global optimiser this is easily parallelisable and able to scale to a large number of variables. Algorithmic parameters for tuning include crossover rate $C R$, mutation weight $F$ and population size $N$ [28]. A good value for $C R$ has been found to be between 0.3 and 0.9 according to Ronkkonen et al. [29]. The population size is closely related to the mutation parameters, with problem dimensionality and problem properties also affecting the choice in population size. The recommended population size for $30-50$ dimension problems is $3 d-5 d$ from a review conducted by Piotrowski [30].</p>
<h2>III. Methodology</h2>
<p>Our approach to simulator optimisation can be segmented into several components as listed below:</p>
<ul>
<li>Real World Dataset Collection (Section III-A)</li>
<li>Robotic Simulations (Section III-B)</li>
<li>Simulator Parameter Selection (Section III-C)</li>
<li>Running the Optimisation Algorithm (Section III-D)</li>
</ul>
<h2>A. Dataset</h2>
<p>Our dataset is a publicly available collection of tasks completed by a robotic manipulator and recorded by a motion capture system [2]. The data gives a ground truth of the real world with 6DOF pose of the manipulator and manipulated objects recorded. There are 10 tasks in total, 2 of which are pure kinematics (no objects) and 8 of which involve nonprehensile manipulation. Tasks are purposefully elementary as they are foundational to larger compound tasks, making results derived from these tasks scale to harder and more complex applications. The manipulation tasks have interactions with objects including cubes, cuboids, cylinders and cones. Another useful property of the dataset is the contrast in object materials, with half the interaction tasks completed with plastic objects and the other half with exact wooden replicas. For a complete description of the dataset and its use in benchmarking reality, please see [2].</p>
<p>The dataset is released with simulation protocols that allow users to simulate the same scene and same control of the robot arm that is used in reality. All scenes use a levelled plane with a Kinova 6DOF robotic arm attached with KG-3 gripper and either none or one object to manipulate. Dataset users must follow the explicit instructions on scene setup, robot configuration and motor controls, but are able to change any of the other user definable parameters of their chosen simulator and physics engine.</p>
<h2>B. Simulation</h2>
<p>We selected two popular robotic simulators; PyBullet (version 2.5.8) [4] and V-Rep (version 3.6.2 now known as CoppeliaSim) [31], accessed through the PyRep interface [32]. The two simulators are chosen as they provide a common interface, and also provide easy access to a multitude of physics engines. Pybullet uses Bullet 2.85 whilst V-Rep uses Bullet 2.83 and Bullet 2.78. V-Rep also provides access to ODE and Newton physics.</p>
<p>PyBullet exposes a large number of settings to the user natively. V-Rep has an abstraction layer between the simulator and physics engines making it possible to interface with multiple different engines. Most of the same parameters accessible to the PyBullet interface are available in the V-Rep physics engines. However, as we use the PyRep interface not all parameters we require are accessible from the external API therefore we use embedded scripts within the simulator which are invoked from PyRep.</p>
<h2>C. Parameters</h2>
<p>From the 5 different physics engines available (including the 3 versions of Bullet) there are many parameters that create the same effect on the physics of the simulation that are either implemented using different methods, or different units. As such it was necessary to find the shared parameters that were directly comparable between physics engines, and settings that were not. We used two approaches; in the first we compared only those parameters available across all simulations and physics engines (Shared). In the second we allow each to</p>
<p>tune a fuller range of parameters that may be available (Individual). Table I documents all the Shared parameters and the Individual parameters that we chose to simulate. The Individual parameter optimisation included both the Shared and Individual parameters.</p>
<p>It would be infeasible to tune all available parameters. As positions (x,y,z), rotations (x,y,z,w) and inertias (xx,yy,zz) require multiple parameters each, it was impractical to create variables for the centre of masses, inertia position, inertia rotation and inertia tensor.</p>
<h2>D. Optimisation</h2>
<p>We use DE as implemented in the SciPy optimise module. DE follows the same approach as most EAs in that it begins with a randomly initialised population of a set number of individuals evolved across a number of generations. Individuals are a vector of parameters and child populations are the succeeding generation (or offspring) from parent populations with a chosen strategy dictating the creation of child populations.</p>
<p>We apply 'best1bin' strategy which iterates over the parent population creating a vector for each individual ( $X_{i}^{\prime}$ ) by mutating the fittest individual in the parent population ( $X_{\text {best }}$ ) by the difference between two randomly chosen individuals of the parent population, see Equation 1 where $F$ is the mutation factor. A child member is then created by choosing each parameter from either $X_{i}^{\prime}$ or the $i^{t h}$ parent as per a binomial distribution where the number must be less than the recombination rate to select the parameter from $X_{i}^{\prime}$. If a child vector is fitter than its parent it replaces it in the current population. In comparison to other strategies 'best1bin' has strong supporting evidence that it is a competitive strategy [33].</p>
<p>$$
X_{i}^{\prime}=X_{\text {best }}+F \times\left(X_{1 _ \text {rand }}+X_{2 _ \text {rand }}\right)
$$</p>
<p>The fitness objective is to minimise the 3D Euclidean distance between the simulator and reality, this value dictates the fitness used by the DE for a given population member. For tasks 1 and 2 (kinematic tasks) this is the distance between the wrist joint of the robotic manipulator in simulation ( $W_{x, y, z}^{s}$ ) and the dataset ( $W_{x, y, z}^{d}$ ) summed at 20 Hz throughout the duration of the simulation and divided by the number of data points ( $n_{\text {points }}$ ), see Equation 2. Tasks that include objects (simulation: $O_{x, y, z}^{s}$, dataset: $O_{x, y, z}^{d}$ ) use the combined euclidean distance of the arm and object, see Equation 3. The trajectory of the dataset object is a distribution as you can not create a mean object trajectory from multiple repeats where the object did not have the same start and end position. Therefore, we use the difference in the final position of the object in simulation and the dataset.</p>
<p>$$
f=\frac{\sum_{\text {points }} \sqrt{\sum_{i=x, y, z}\left(W_{i}^{d}-W_{i}^{s}\right)^{2}}}{n_{\text {points }}}
$$</p>
<p>$f=\frac{\sum_{\text {points }} \sqrt{\sum_{i=x, y, z}\left(W_{i}^{d}-W_{i}^{s}\right)^{2}}}{n_{\text {points }}}+\sqrt{\sum_{i=x, y, z}\left(O_{i}^{d}-O_{i}^{s}\right)^{2}}$</p>
<h2>IV. EXPERIMENTATION</h2>
<p>We define an optimisation as an application of DE to optimise an array of either shared or individual simulation parameters. In total there were 1100 optimisations completed that make up the results. This is broken down into:</p>
<ul>
<li>"Shared" and "Individual" parameters;</li>
<li>11 Experiments: 10 manipulation tasks from the dataset the $11^{\text {th }}$ a combination of all 10 ;</li>
<li>5 physics engines; and</li>
<li>10 repeats of each.</li>
</ul>
<p>A large number of experimental runs were required and as such were scheduled on a High Performance CPU Cluster (HPC). A singularity container with PyBullet, PyRep and VRep installed provided a distributed and scaleable deployment. Experiments $1-11$ were paired and scheduled onto a single node of the HPC, with each repeat given access to a single core. Each node was 20 cores of an Intel Xeon E5-2660 V3 processor with a clock speed of $2.6 \mathrm{GHz}, 25 \mathrm{MB}$ cache and 128 GB of memory.</p>
<p>The constants for the DE algorithm were 0.7 for $C R, 0.5-$ 1.0 for $F$ with dithering and a population of $N=1 D$ where $D$ is the length of the parameter array. The population was chosen to be low due to the large evaluation times of some physics engines and experiments paired with the limited amount of compute time. The experiment finished under one of three conditions:</p>
<ul>
<li>Convergence (i.e. when the standard deviation of the current population was less than one percent of the population mean), or</li>
<li>1000 DE generations completed, or</li>
<li>168 hrs of compute time (a hard constraint of the HPC).</li>
</ul>
<p>The number of variables tuned varied for Shared and Individual experiments. Shared experiments tuned 31 variables, Individual experiments tuned 57 (inclusive of the Shared variables). See Table I for more details.</p>
<h2>V. ReSults</h2>
<h2>A. Shared Parameters</h2>
<p>1) Performance: Table II shows the fitnesses for each experiment and physics engine when using generic simulator parameters. The fitness plots in Figure 2 demonstrate the convergence of the optimisations completed on the same physics engines, and the parameters shared between all physics engines. The smallest error obtained between simulation and the real world dataset for each experiment can be found in Table III.</p>
<p>Directly comparing the generic fitness values from Table II to the fitness values achieved when optimising shared parameters in Table III we see that the tuned fitness is lower than</p>
<p>TABLE I: List of Parameters Used for Optimisation and the Range Limits</p>
<table>
<thead>
<tr>
<th>Shared</th>
<th>Range</th>
<th>Individual</th>
<th>Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>Time Step</td>
<td>[0.001,0.05]</td>
<td>Joint Damping (6 Joints)</td>
<td>[0.0001,0.9]</td>
</tr>
<tr>
<td>Mass (Links, Gripper, Objects)</td>
<td>[0.7<em>M,1.3</em>M]</td>
<td>Rolling Friction (Gripper,Floor, Objects)</td>
<td>[0.0001, 1.25]</td>
</tr>
<tr>
<td>Maximum Joint Torque (6 Joints)</td>
<td>[100,9000]</td>
<td>Sliding Friction (Gripper,Floor, Objects)</td>
<td>[0.0001, 1.25]</td>
</tr>
<tr>
<td>Maximum Joint Velocity (6 Joints)</td>
<td>[10,40]</td>
<td>Restitution (Gripper,Floor, Objects)</td>
<td>[0.0001,0.9]</td>
</tr>
<tr>
<td>Lateral Friction (Gripper,Floor, Objects)</td>
<td>[0.0001, 1.25]</td>
<td>Linear Damping (Gripper,Floor, Objects)</td>
<td>[0.0001,0.9]</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Angular Damping (Gripper,Floor, Objects)</td>
<td>[0.0001,0.9]</td>
</tr>
</tbody>
</table>
<p>TABLE II: Fitness of Generic Physics Engine Settings</p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>PyBullet</th>
<th>Bullet2.78</th>
<th>Bullet2.83</th>
<th>ODE</th>
<th>Newton</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.1498</td>
<td>0.2660</td>
<td>0.2687</td>
<td>0.1800</td>
<td>0.1437</td>
</tr>
<tr>
<td>2</td>
<td>0.1283</td>
<td>0.1320</td>
<td>0.1582</td>
<td>0.1285</td>
<td>0.1147</td>
</tr>
<tr>
<td>3</td>
<td>0.1764</td>
<td>0.2533</td>
<td>0.2838</td>
<td>0.1943</td>
<td>0.2691</td>
</tr>
<tr>
<td>4</td>
<td>0.2107</td>
<td>0.2255</td>
<td>0.1966</td>
<td>0.1674</td>
<td>0.2877</td>
</tr>
<tr>
<td>5</td>
<td>491.2769</td>
<td>0.2967</td>
<td>0.3031</td>
<td>0.2909</td>
<td>0.2910</td>
</tr>
<tr>
<td>6</td>
<td>503.3037</td>
<td>0.6029</td>
<td>0.6096</td>
<td>0.5972</td>
<td>0.5977</td>
</tr>
<tr>
<td>7</td>
<td>0.1778</td>
<td>0.2500</td>
<td>0.3163</td>
<td>0.2370</td>
<td>0.2323</td>
</tr>
<tr>
<td>8</td>
<td>0.2132</td>
<td>0.3075</td>
<td>0.3412</td>
<td>0.2812</td>
<td>0.2759</td>
</tr>
<tr>
<td>9</td>
<td>0.1306</td>
<td>0.1950</td>
<td>0.1241</td>
<td>0.1171</td>
<td>0.1242</td>
</tr>
<tr>
<td>10</td>
<td>0.1242</td>
<td>0.1143</td>
<td>0.1155</td>
<td>0.1069</td>
<td>0.1176</td>
</tr>
<tr>
<td>11</td>
<td>995.8916</td>
<td>2.6433</td>
<td>2.7170</td>
<td>2.3005</td>
<td>2.4540</td>
</tr>
</tbody>
</table>
<p>TABLE III: Best Optimised Fitness by Experiment and Parameters Tuned</p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>Best Physics Engine Shared</th>
<th>Best Physics Engine Individual</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Newton (0.0973)</td>
<td>Newton (0.0973)</td>
</tr>
<tr>
<td>2</td>
<td>Newton (0.0984)</td>
<td>Newton (0.0984)</td>
</tr>
<tr>
<td>3</td>
<td>Bullet 2.78 (0.0498)</td>
<td>Bullet 2.78 (0.0498)</td>
</tr>
<tr>
<td>4</td>
<td>Bullet 2.83 (0.0629)</td>
<td>Bullet 2.78 (0.0673)</td>
</tr>
<tr>
<td>5</td>
<td>PyBullet (0.0506)</td>
<td>PyBullet (0.2407)</td>
</tr>
<tr>
<td>6</td>
<td>PyBullet (0.0552)</td>
<td>PyBullet (0.5641)</td>
</tr>
<tr>
<td>7</td>
<td>Bullet 2.78 (0.0551)</td>
<td>PyBullet (0.0551)</td>
</tr>
<tr>
<td>8</td>
<td>PyBullet (0.0442)</td>
<td>Bullet 2.78 (0.0744)</td>
</tr>
<tr>
<td>9</td>
<td>ODE (0.0519)</td>
<td>ODE (0.0482)</td>
</tr>
<tr>
<td>10</td>
<td>ODE (0.0503)</td>
<td>ODE (0.0487)</td>
</tr>
<tr>
<td>11</td>
<td>ODE (1.7360)</td>
<td>ODE (1.7714)</td>
</tr>
</tbody>
</table>
<p>all physics engines with generic parameters. Taking the best generic fitness and comparing it to the best tuned fitness for each experiment we see improvements ranging from $14\%$ for experiment 2 and $91\%$ for experiment 6 . The effect of tuning the shared parameters is therefore significant and provides a more realistic simulation closer aligned to the real world.</p>
<p>From Table III we see a correlation between the experiment 'type' and the physics engine with the least error. Newton was the best Physics Engine for Experiments 1 and 2, implying that it is able to better model arm kinematics without object interactions. PyBullet was best at experiments 5, 6 and 8, all of which include rolling objects. The clustering of experiments and physics engines indicates that no physics engine is best equipped to deal with all simulation scenarios but that physics engines can have heightened performance in select scenarios over other physics engines.</p>
<p>Newton took the least compute time, taking a total $279 h r s$ for all 11 experiments. Slowest was PyBullet at $662 h r s$. The time required to complete an optimisation gives some notion of</p>
<p>Shared Parameter Fitness Convergence
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Convergence plots of all 11 experiments using shared parameters. Each subplot plots the line of best fitness throughout the generations averaged across the 10 repeats for each physics engine.
the difficulty, as experiment 11 (the combination of all dataset tasks, i.e. $1-10$ ) understandably took the longest. We note extended completion times for experiments 7 and 8 , which were cylinder rolling tasks.</p>
<p>Table 2 displays in the y-axis of each subplot the number of generations required for the optimisation to terminate. Newton was consistently the physics engine with the lowest number of evolutionary generations, whilst Bullet 2.83 and Bullet 2.78</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Convergence plots of all 11 experiments using individual parameters. Each subplot plots the line of best fitness throughout the generations averaged across the 10 repeats for each physics engine.</p>
<p>had the most, terminating at 1000 generations for experiments 9 and 10 both of which were comparatively easy experiments. This is an interesting observation as it alludes to the fact that Newton is an easier environment to optimise within, with either a reduced search space or less noise, however, Newton does not provide accurate environmental interactions when compared with the other physics engines being reviewed. The large number of evolutionary generations required by both of V-Reps Bullet environments especially for the easier cuboid interactions implies a noisy landscape for fitness optimisation.</p>
<p>2) <em>Parameters:</em> From the Shared parameters we note a select few having a large impact on the performance of the Physics engine. We measure the importance of a parameters value as its deviation across the 10 optimisation repeats. Owing to the large amount of data generated, we include exemplar box and whisker plots in Figure 4 for most relevant data.</p>
<p>One of the most influential parameters found was the simulation timestep (see Figure 4), the deviation was consistently low across experiments and physics engines except for rolling tasks (experiments 5 – 8). The generic timestep value for V-Rep is 0.05sec and 0.0041sec for PyBullet. Pybullet's median value was 0.0042sec with a standard deviation of less than 0.0095sec for all experiments. Similarly, V-Rep Physics Engines were also very close to the generic timestep with the median for Bullet 2.78: 0.4304sec, Bullet 2.83: 0.0456sec, ODE: 0.0485sec and Newton: 0.0459sec. We therefore recommend setting the physics engine timestep to the recommended value as detailed by the developer of the simulator. The constrained value is very likely to be due to a reliance of other parameters that would need to be tuned that are physics engine specific.</p>
<p>Other parameters that largely influenced the realism of the simulation were the lateral friction of the manipulated objects i.e. wood and plastic frictions. The friction of the gripper and floor plane were not as influential except for Pybullet during the two rolling experiments where the floor plane had a standard deviation of 0.0097 and 0.1465. This is a very likely the reason why PyBullet had the lowest error for three of the rolling experiments.</p>
<p>It was expected that the parameters influencing the response of the joints would have a large impact on the fitness as there is a direct correlation between the measured wrist joint and actuator response. This assumption was found to be true for the maximum joint velocity, but no such trends could be found for the maximum torque. Experiments 1 – 4 consistently had statistically significant lower standard deviation across joints 1 – 5. Joint 6 was likely less influential in simulator realism due to the restricted amount of movement it experienced in the experiments and although experiments 5 – 10 did not display the same reliance on accurate joint velocity this is likely due to the experiments being more complex and the resulting optimisation harder. The results from the shared parameter optimisation show that we can perform contextsensitive tuning that is able to positively influence the realism of the simulation for all environments.</p>
<h3><em>B. Individual Parameters</em></h3>
<p>1) <em>Performance:</em> Figure 3 depicts the fitness convergence for the 57 individual parameters tuned. Made obvious by the plots is the difficulty that the extra parameters add as some physics engines fail to converge appropriately for certain experiments. When comparing the lowest error for each experiment as found in Table III, there are only three instances where the individual optimisation improves upon the shared parameters and 5 where the optimisation arrives at a worse solution. The added complexity of the additional parameters to tune is likely the cause of the worse fitnesses. Similar to the shared parameter optimisations the individual runs have a correlation between the best physics engine and the type of task, i.e. Newton is best at kinematics and PyBullet is best at 3 of the 4 tasks that include rolling objects.</p>
<p>Taking into account the mean final fitness instead of the absolute best the individual optimisation lessens the engine/experiment error 35 out of the 55 times. This is likely due to several reasons, (i) DE does not guarantee to find the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Box and Whisker Plot for each (i) physics engine and (ii) experiment. The subplots are all for the shared timestep parameter. This plot is meant as a exemplar of the other 30 parameter plots for individual optimisations and 56 parameter plots for shared optimisations.</p>
<p>optimal solution, (ii) the additional parameters add noise and complexity that make it harder to optimise, and (iii) due to termination of compute after 168<em>hrs</em> the algorithm is unable to complete optimisation without convergence. Termination at the maximum run time of individual parameter optimisations occurs on 14 out of 55 occasions.</p>
<p>2) <em>Parameters:</em> The influential parameters for individual optimisations were the same as those for shared. These being timestep, lateral friction and maximum joint velocity. In addition to these parameters there were two additional ones that had statistically significant standard deviations. The restitution of an object parameterizes the conservation of energy after a contact, only 9 of our 11 experiments contain contacts, most of which are at low speed. Experiment 9, interaction with a plastic cuboid, saw a standard deviation of less than 0.16 for three of the physics engines for the restitution value of plastic. This eludes to the fact that restitution could be an important factor given experiments that include contacts that are above a contact energy threshold.</p>
<p>The mass of arm links was another parameter that saw noticeably low standard deviations for individual parameter optimisations. Arm link masses produced an interesting result with links 1, 3, 4<em>and</em>5 displaying low standard deviations for Bullet 2.78, Bullet 2.83 and ODE on the simpler experiments i.e. experiments 1 – 4. The smaller deviations across the easier experiments is likely due to the reduced noise whilst optimising the parameters.</p>
<h2>VI. CONCLUSION</h2>
<p>In conclusion, we have investigated the influence of a range of simulation parameters on optimising 2 simulators and 5 physics engines towards more realistic simulations. Our method is significantly better than using generic simulator parameters with all simulation environments and all experiments achieving an improved fitness. This was achieved by using a real world dataset of motion capture recorded manipulation tasks and optimising both shared and individualised simulation parameters towards the dataset. The optimisation algorithm chosen was differential evolution (DE) due to the large number of optimisation parameters it is able to concurrently optimise and the non-differentiable nature of the problem. The fitness signal throughout the optimisation runs was the Euclidean distance error between the simulated wrist of the manipulator summed with the final placement error of any objects in the scene.</p>
<p>We found that the most important parameters that were shared between physics engines were simulation timestep, lateral object friction and joint velocity. From the expanded range of parameters we also found that it is likely for high energy contact tasks that the value of restitution is important.</p>
<p>To improve simulator performance we recommend that users start with (i) the default simulator timestep, (ii) researching or experimentally acquiring an accurate friction value, and (iii) recording or sourcing accurate maximum joint velocities for each actuator. These same parameters, if accurate, should not be excessively randomised as results indicate that the variation in these parameters are tightly bounded to the real world value.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] J. Collins, D. Howard, and J. Leitner, “Quantifying the Reality Gap in Robotic Manipulation Tasks,” in <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 2019, pp. 6706–6712.</li>
<li>[2] J. Collins, J. McVicar, D. Wedlock, R. Brown, D. Howard, and J. Leitner, “Benchmarking Simulated Robotic Manipulation through a Real World Dataset,” <em>IEEE Robotics and Automation Letters</em>, p. 1, 2019.</li>
</ul>
<p>[3] M. Torres-Torriti, T. Arredondo, and P. Castillo-Pizarro, "Survey and comparative study of free simulation software for mobile robots," Robotica, vol. 34, no. 4, pp. 791-822, 2016.
[4] E. Coumans and Y. Bai, "Pybullet, a Python Module for Physics Simulation for Games, Robotics and Machine Learning," 2016. [Online]. Available: https://pybullet.org
[5] J. Lee, M. Grey, S. Ha, T. Kunz, S. Jain, Y. Ye, S. Srinivasa, M. Stilman, and C. Liu, "DART: Dynamic Animation and Robotics Toolkit," Journal of Open Source Software, vol. 3, no. 22, p. 500, 2018. [Online]. Available: https://doi.org/10.21105/joss. 00500
[6] R. Smith, "Open dynamics engine," 2005. [Online]. Available: https://www.ode.org/
[7] D. Howard and T. Merz, "A platform for the direct hardware evolution of quadcopter controllers," in IEEE International Conference on Intelligent Robots and Systems, vol. 2015-Decem. IEEE, 9 2015, pp. 4614-4619. [Online]. Available: http://ieeexplore.ieee.org/document/7354034/
[8] H. Heijnen, D. Howard, and N. Kottege, "A testbed that evolves hexapod controllers in hardware," in Proceedings - IEEE International Conference on Robotics and Automation. IEEE, 5 2017, pp. 1065-1071. [Online]. Available: http://ieeexplore.ieee.org/document/7989128/
[9] N. Jakobi, P. Husbands, and I. Harvey, "Noise and the reality gap: The use of simulation in evolutionary robotics," in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer, Berlin, Heidelberg, 1995, vol. 929, pp. 704-720. [Online]. Available: http://link.springer.com/10.1007/3-540-59496-5_337
[10] H. Qiu, M. Garrait, D. Howard, and S. Anavatti, "Crossing the reality gap with evolved plastic neurocontrollers," arXiv preprint arXiv:2002.09854, 2020.
[11] S. Koos, J.-B. Mouret, and S. Doncieux, "Crossing the reality gap in evolutionary robotics by promoting transferable controllers," in Proceedings of the 12th annual conference on Genetic and evolutionary computation - GECCO '10, 2010, p. 119. [Online]. Available: http://portal.acm.org/citation.cfm?deid=1830483.1830505
[12] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke, "Sim-to-Real: Learning Agile Locomotion For Quadruped Robots," 4 2018. [Online]. Available: http://arxiv.org/abs/ 1804.10332
[13] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, "Learning agile and dynamic motor skills for legged robots," Science Robotics, vol. 4, no. 26, 2019. [Online]. Available: http://robotics.sciencemag.org/content/4/26/eaau5872
[14] J. Tan, Z. Xie, B. Boots, and C. K. Liu, "Simulation-based design of dynamic controllers for humanoid balancing," in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016, pp. 2729-2736.
[15] R. L. Williams, B. E. Carter, P. Gallina, and G. Rosati, "Dynamic model with slip for wheeled omnidirectional robots," IEEE Transactions on Robotics and Automation, vol. 18, no. 3, pp. 285-293, 2002.
[16] M. Gautier and W. Khalil, "On the identification of the inertial parameters of robots," in Proceedings of the 27th IEEE Conference on Decision and Control, 1988, pp. 2264-2269.
[17] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience," in 2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 8973-8979.
[18] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, and others, "Learning dexterous in-hand manipulation," arXiv preprint arXiv:1808.00177, 2018.
[19] R. Moeckel, Y. N. Perov, A. T. Nguyen, M. Vespignani, S. Bonardi, S. Pouya, A. Sproewitz, J. v. d. Kieboom, F. Wilhelm, and A. J. Ijspeert, "Gait optimization for roombots modular robots Matching simulation and reality," in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013, pp. 3265-3272.
[20] J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To, E. Cameracci, S. Boochoon, and S. Birchfield, "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization," 2018.
[21] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to-real transfer of robotic control with dynamics randomization," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 1-8.
[22] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, "Solving Rubik's Cube with a Robot Hand," 10 2019. [Online]. Available: http://arxiv.org/abs/1910.07113
[23] J. Degrave, M. Hermans, J. Dambre, and F. Wyffels, "A Differentiable Physics Engine for Deep Learning in Robotics," Frontiers in neurorobotics, vol. 13, p. 6, 3 2019. [Online]. Available: https://www.ncbi.nlm.nih.gov/pubmed/30899218https://www. ncbi.nlm.nih.gov/pmc/PMC6416213/
[24] J. Snoek, H. Larochelle, and R. P. Adams, "Practical Bayesian Optimization of Machine Learning Algorithms," in Proceedings of the 25th International Conference on Neural Information Processing Systems Volume 2, ser. NIPS12. Red Hook, NY, USA: Curran Associates Inc., 2012, pp. 2951-2959.
[25] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas, "Bayesian Optimization in High Dimensions via Random Embeddings," in Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, ser. IJCAI 13. AAAI Press, 2013, pp. 1778-1784.
[26] C. Li, S. Gupta, S. Rana, V. Nguyen, S. Venkatesh, and A. Shilton, "High Dimensional Bayesian Optimization Using Dropout," IJCAR International Joint Conference on Artificial Intelligence, pp. 20962102, 2 2018. [Online]. Available: http://arxiv.org/abs/1802.05400
[27] R. Storn and K. Price, "Differential Evolution A Simple and Efficient Heuristic for global Optimization over Continuous Spaces," Journal of Global Optimization, vol. 11, no. 4, pp. 341-359, 1997. [Online]. Available: https://doi.org/10.1023/A:1008202821328
[28] S. Das and P. N. Suganthan, "Differential Evolution: A Survey of the State-of-the-Art," IEEE Transactions on Evolutionary Computation, vol. 15, no. 1, pp. 4-31, 2011.
[29] J. Ronkkonen, S. Kukkonen, and K. V. Price, "Real-parameter optimization with differential evolution," in 2005 IEEE Congress on Evolutionary Computation, vol. 1, 2005, pp. 506-513.
[30] A. P. Piotrowski, "Review of Differential Evolution population size," Swarm and Evolutionary Computation, vol. 32, pp. 1-24, 2017. [Online]. Available: http://www.sciencedirect.com/science/article/ pii/S2210650216300268
[31] E. Rohmer, S. P. Singh, and M. Freese, "V-REP: A versatile and scalable robot simulation framework," in IEEE International Conference on Intelligent Robots and Systems. IEEE, 11 2013, pp. 1321-1326. [Online]. Available: http://ieeexplore.ieee.org/document/6696520/
[32] S. James, M. Freese, and A. J. Davison, "PyRep: Bringing V-REP to Deep Robot Learning," 6 2019. [Online]. Available: http://arxiv.org/abs/1906.11176
[33] E. Mezura-Montes, J. Velázquez-Reyes, and C. A. Coello Coello, "A Comparative Study of Differential Evolution Variants for Global Optimization," in Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation, ser. GECCO 06. New York, NY, USA: Association for Computing Machinery, 2006, pp. 485-492. [Online]. Available: https://doi.org/10.1145/1143997.1144086</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>This research was supported by a Data61 PhD Scholarship.
${ }^{1}$ Data61/CSIRO, Brisbane, Australia
${ }^{2}$ Queensland University of Technology (QUT), Brisbane, Australia
${ }^{3}$ Australian Centre for Robotic Vision (ACRV)
${ }^{4}$ LYRO Robotics Pty Ltd, Brisbane, Australia&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
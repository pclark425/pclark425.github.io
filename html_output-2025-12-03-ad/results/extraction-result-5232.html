<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5232 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5232</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5232</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-260203094</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.14712v1.pdf" target="_blank">Evaluating Generative Models for Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores. We have made the text generated by generative models publicly available.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5232.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5232.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized Graph Representation (serialized triples with <H>, <R>, <T> tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text serialization of graph triples into a single token sequence by listing head-relation-tail triples and marking each element with special tokens (e.g., <H>, <R>, <T>) so pretrained language models can consume graph inputs as plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is converted into a flat sequence by serializing triples. Each triple is written as a sequence of tokens with markers prepended to identify head, relation and tail (i.e., <H> head_entity <R> relation <T> tail_entity). For AGENDA the authors additionally include higher-level fields by prepending tokens such as <title>, <entities>, and <graph> before the linearized triples. The paper follows prior linearization strategies (e.g., Konstas et al., Ribeiro et al.) and uses these serialized strings as direct input to generative LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF-style triples / knowledge graphs and automatically-extracted scientific graphs (AGENDA) — i.e., head-relation-tail triple graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Properties reported or discussed: (1) Directly consumable by pretrained LMs (compact, text-native). (2) Explicit token markers (<H>,<R>,<T>) improve token-level identification of graph components. (3) Loses explicit graph topology information beyond serialized adjacency ordering (reduced structural inductive bias vs. a graph encoder). (4) Susceptible to semantic-relation ambiguity in LM outputs (models may reverse relations or ignore some relations). (5) Easy to implement and interoperable with existing sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph-to-text generation evaluated on two benchmarks: AGENDA (scientific paper graphs -> abstracts) and WebNLG (RDF triples -> descriptive sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics used: BLEU, METEOR, ROUGE-L, Chrf++, BLEURT. Reported results (selected, as reported in paper): AGENDA — T5 large (finetuned baseline) BLEU=22.15, BART large BLEU=23.65; GPT-3 (text-davinci-003) BLEU=8.34, METEOR=14.88, ROUGE-L=24.99, Chrf++=41.42; ChatGPT BLEU=10.57, METEOR=17.02, ROUGE-L=25.22, Chrf++=45.86. WebNLG — T5 large BLEU=59.70, BART large BLEU=54.72; GPT-3 BLEU=20.36, METEOR=26.95, ROUGE-L=45.64, Chrf++=57.95, BLEURT=13.39; ChatGPT BLEU=11.08, METEOR=23.89, ROUGE-L=35.87, Chrf++=48.75, BLEURT=-10.99. (All numbers are unitless metric scores as reported in tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The same linearized representation was used both by prior finetuned LMs (Ribeiro et al. finetuned T5/BART) and in this paper for zero-shot generative models; finetuned models (T5/BART) substantially outperform zero-shot generative LMs using the same linearized input (e.g., BLEU: 22–23 for finetuned LMs vs. 8–11 for GPT-3/ChatGPT on AGENDA; 54–60 vs. 11–20 on WebNLG). The paper contrasts linearized LM-based approaches with GNN-based graph encoders (mentioned in related work) but does not provide direct experimental head-to-head comparisons to GNNs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limitations discussed: (1) Linearization reduces explicit graph structure making semantic relation comprehension harder for generative models (models may reverse relations or omit relation semantics). (2) Zero-shot generative LMs operating on linearized graphs produce hallucinations and irrelevant content. (3) There's an input-format mismatch between LM pretraining data and structured linearized inputs, limiting zero-shot performance. (4) Reproducibility concerns: GPT-3/ChatGPT responses may vary across calls, adding randomness to evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Generative Models for Graph-to-Text Generation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5232.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5232.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-based Zero-shot Graph-to-Text with Generative Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that combines a short natural-language instruction prompt with a linearized graph string so large generative models (GPT-3, ChatGPT) can produce descriptive text from graph inputs without any task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>prompted linearization / instruction prompting</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The linearized graph string (see above) is prefixed with a task instruction prompt (examples used: 'Generate paper abstract from title, entities and graph:' for AGENDA; 'Generate text from graph:' for WebNLG). Prompts/templates were obtained (in the paper) by asking ChatGPT for suitable templates. Models used: text-davinci-003 (GPT-3) and gpt-3.5-turbo-0301 (ChatGPT); the first model response to each single request is taken as the generated output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples and extracted scientific knowledge graphs (same graph types as linearized representation).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Properties: (1) Requires no finetuning (zero-shot) and leverages LM pretraining; (2) prompt-sensitive — output quality strongly depends on prompt wording; (3) flexible across domains (using different prompts); (4) prone to hallucination and to misrepresenting relations when pretraining lacks structured-graph conditioning; (5) low reproducibility reported (models can return different outputs for identical prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Zero-shot graph-to-text generation on AGENDA and WebNLG test sets using the prompt + linearized graph input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same metrics as linearized representation. Reported zero-shot generative model results (from the paper): AGENDA — GPT-3 BLEU=8.34, ChatGPT BLEU=10.57 (other metrics reported in Table 2: e.g., ChatGPT METEOR=17.02, Chrf++=45.86). WebNLG — GPT-3 BLEU=20.36, ChatGPT BLEU=11.08 (other metrics reported in Table 3: GPT-3 METEOR=26.95, ROUGE-L=45.64; ChatGPT METEOR=23.89, ROUGE-L=35.87). These are lower than finetuned baselines (T5/BART).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared experimentally (using identical linearized inputs) to finetuned LMs (T5 and BART from Ribeiro et al.). Finetuned LMs substantially outperform the prompt-based zero-shot generative LMs on both datasets. The paper also reports intra-generative-model differences (GPT-3 outperformed ChatGPT on WebNLG likely due to ChatGPT producing longer text and more hallucinations in that dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Challenges highlighted: (1) Hallucinations and insertion of unrelated internal-knowledge content by ChatGPT; (2) failure to correctly encode relation direction or to include some relations; (3) prompt sensitivity and lack of standardization of prompts; (4) variability/reproducibility issues in generative model outputs; (5) overall inferior semantic fidelity compared to finetuned, supervised approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Generative Models for Graph-to-Text Generation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5232.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5232.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN-based Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network-based Graph Encoders for Graph-to-Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of models that encode graph structure using graph neural networks (GNNs) or specialized graph encoders and then decode the representation into text, preserving structural relations more explicitly than flat linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are fed into a graph encoder (e.g., gated graph neural networks, graph transformers) that computes structure-aware node and edge representations; a decoder (often sequence-based) then generates text conditioned on these graph encodings. This approach maintains explicit topological information instead of serializing the graph into plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs including knowledge graphs, AMR, and other structured semantic graphs (as cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Properties (as discussed in related work): (1) Better expresses explicit graph topology and relations, (2) higher fidelity to relation semantics when trained, (3) requires supervised training and graph-specific encoders, (4) potentially more sample- and compute-intensive than simple linearization fed to pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Mentioned as an alternative approach to graph-to-text; commonly evaluated on datasets such as WebNLG, AMR-to-text benchmarks, etc. (not experimentally evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The paper contrasts GNN-based encoders (mentioned in related work) with LM-based linearization approaches. It notes that while GNN models encode structure directly, LM-based linearization enables reuse of large pretrained sequence models; however, direct experimental comparisons were not conducted in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Noted in related work: requires training on graph-to-text pairs (annotations), more modeling complexity, and potentially higher resource/training cost compared to using pretrained LMs with linearized graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Generative Models for Graph-to-Text Generation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity <em>(Rating: 2)</em></li>
                <li>Neural pipeline for zero-shot data-to-text generation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5232",
    "paper_id": "paper-260203094",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Linearized Graph",
            "name_full": "Linearized Graph Representation (serialized triples with &lt;H&gt;, &lt;R&gt;, &lt;T&gt; tokens)",
            "brief_description": "A text serialization of graph triples into a single token sequence by listing head-relation-tail triples and marking each element with special tokens (e.g., &lt;H&gt;, &lt;R&gt;, &lt;T&gt;) so pretrained language models can consume graph inputs as plain text.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearization",
            "representation_description": "The graph is converted into a flat sequence by serializing triples. Each triple is written as a sequence of tokens with markers prepended to identify head, relation and tail (i.e., &lt;H&gt; head_entity &lt;R&gt; relation &lt;T&gt; tail_entity). For AGENDA the authors additionally include higher-level fields by prepending tokens such as &lt;title&gt;, &lt;entities&gt;, and &lt;graph&gt; before the linearized triples. The paper follows prior linearization strategies (e.g., Konstas et al., Ribeiro et al.) and uses these serialized strings as direct input to generative LMs.",
            "graph_type": "RDF-style triples / knowledge graphs and automatically-extracted scientific graphs (AGENDA) — i.e., head-relation-tail triple graphs",
            "representation_properties": "Properties reported or discussed: (1) Directly consumable by pretrained LMs (compact, text-native). (2) Explicit token markers (&lt;H&gt;,&lt;R&gt;,&lt;T&gt;) improve token-level identification of graph components. (3) Loses explicit graph topology information beyond serialized adjacency ordering (reduced structural inductive bias vs. a graph encoder). (4) Susceptible to semantic-relation ambiguity in LM outputs (models may reverse relations or ignore some relations). (5) Easy to implement and interoperable with existing sequence models.",
            "evaluation_task": "Graph-to-text generation evaluated on two benchmarks: AGENDA (scientific paper graphs -&gt; abstracts) and WebNLG (RDF triples -&gt; descriptive sentences).",
            "performance_metrics": "Metrics used: BLEU, METEOR, ROUGE-L, Chrf++, BLEURT. Reported results (selected, as reported in paper): AGENDA — T5 large (finetuned baseline) BLEU=22.15, BART large BLEU=23.65; GPT-3 (text-davinci-003) BLEU=8.34, METEOR=14.88, ROUGE-L=24.99, Chrf++=41.42; ChatGPT BLEU=10.57, METEOR=17.02, ROUGE-L=25.22, Chrf++=45.86. WebNLG — T5 large BLEU=59.70, BART large BLEU=54.72; GPT-3 BLEU=20.36, METEOR=26.95, ROUGE-L=45.64, Chrf++=57.95, BLEURT=13.39; ChatGPT BLEU=11.08, METEOR=23.89, ROUGE-L=35.87, Chrf++=48.75, BLEURT=-10.99. (All numbers are unitless metric scores as reported in tables.)",
            "comparison_to_other_representations": "The same linearized representation was used both by prior finetuned LMs (Ribeiro et al. finetuned T5/BART) and in this paper for zero-shot generative models; finetuned models (T5/BART) substantially outperform zero-shot generative LMs using the same linearized input (e.g., BLEU: 22–23 for finetuned LMs vs. 8–11 for GPT-3/ChatGPT on AGENDA; 54–60 vs. 11–20 on WebNLG). The paper contrasts linearized LM-based approaches with GNN-based graph encoders (mentioned in related work) but does not provide direct experimental head-to-head comparisons to GNNs in this study.",
            "limitations_or_challenges": "Limitations discussed: (1) Linearization reduces explicit graph structure making semantic relation comprehension harder for generative models (models may reverse relations or omit relation semantics). (2) Zero-shot generative LMs operating on linearized graphs produce hallucinations and irrelevant content. (3) There's an input-format mismatch between LM pretraining data and structured linearized inputs, limiting zero-shot performance. (4) Reproducibility concerns: GPT-3/ChatGPT responses may vary across calls, adding randomness to evaluation.",
            "uuid": "e5232.0",
            "source_info": {
                "paper_title": "Evaluating Generative Models for Graph-to-Text Generation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Prompted Zero-shot",
            "name_full": "Prompt-based Zero-shot Graph-to-Text with Generative Language Models",
            "brief_description": "A procedure that combines a short natural-language instruction prompt with a linearized graph string so large generative models (GPT-3, ChatGPT) can produce descriptive text from graph inputs without any task-specific fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "prompted linearization / instruction prompting",
            "representation_description": "The linearized graph string (see above) is prefixed with a task instruction prompt (examples used: 'Generate paper abstract from title, entities and graph:' for AGENDA; 'Generate text from graph:' for WebNLG). Prompts/templates were obtained (in the paper) by asking ChatGPT for suitable templates. Models used: text-davinci-003 (GPT-3) and gpt-3.5-turbo-0301 (ChatGPT); the first model response to each single request is taken as the generated output.",
            "graph_type": "RDF triples and extracted scientific knowledge graphs (same graph types as linearized representation).",
            "representation_properties": "Properties: (1) Requires no finetuning (zero-shot) and leverages LM pretraining; (2) prompt-sensitive — output quality strongly depends on prompt wording; (3) flexible across domains (using different prompts); (4) prone to hallucination and to misrepresenting relations when pretraining lacks structured-graph conditioning; (5) low reproducibility reported (models can return different outputs for identical prompts).",
            "evaluation_task": "Zero-shot graph-to-text generation on AGENDA and WebNLG test sets using the prompt + linearized graph input.",
            "performance_metrics": "Same metrics as linearized representation. Reported zero-shot generative model results (from the paper): AGENDA — GPT-3 BLEU=8.34, ChatGPT BLEU=10.57 (other metrics reported in Table 2: e.g., ChatGPT METEOR=17.02, Chrf++=45.86). WebNLG — GPT-3 BLEU=20.36, ChatGPT BLEU=11.08 (other metrics reported in Table 3: GPT-3 METEOR=26.95, ROUGE-L=45.64; ChatGPT METEOR=23.89, ROUGE-L=35.87). These are lower than finetuned baselines (T5/BART).",
            "comparison_to_other_representations": "Compared experimentally (using identical linearized inputs) to finetuned LMs (T5 and BART from Ribeiro et al.). Finetuned LMs substantially outperform the prompt-based zero-shot generative LMs on both datasets. The paper also reports intra-generative-model differences (GPT-3 outperformed ChatGPT on WebNLG likely due to ChatGPT producing longer text and more hallucinations in that dataset).",
            "limitations_or_challenges": "Challenges highlighted: (1) Hallucinations and insertion of unrelated internal-knowledge content by ChatGPT; (2) failure to correctly encode relation direction or to include some relations; (3) prompt sensitivity and lack of standardization of prompts; (4) variability/reproducibility issues in generative model outputs; (5) overall inferior semantic fidelity compared to finetuned, supervised approaches.",
            "uuid": "e5232.1",
            "source_info": {
                "paper_title": "Evaluating Generative Models for Graph-to-Text Generation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GNN-based Models",
            "name_full": "Graph Neural Network-based Graph Encoders for Graph-to-Text",
            "brief_description": "A class of models that encode graph structure using graph neural networks (GNNs) or specialized graph encoders and then decode the representation into text, preserving structural relations more explicitly than flat linearizations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN encoding",
            "representation_description": "Graphs are fed into a graph encoder (e.g., gated graph neural networks, graph transformers) that computes structure-aware node and edge representations; a decoder (often sequence-based) then generates text conditioned on these graph encodings. This approach maintains explicit topological information instead of serializing the graph into plain text.",
            "graph_type": "General graphs including knowledge graphs, AMR, and other structured semantic graphs (as cited in related work).",
            "representation_properties": "Properties (as discussed in related work): (1) Better expresses explicit graph topology and relations, (2) higher fidelity to relation semantics when trained, (3) requires supervised training and graph-specific encoders, (4) potentially more sample- and compute-intensive than simple linearization fed to pretrained LMs.",
            "evaluation_task": "Mentioned as an alternative approach to graph-to-text; commonly evaluated on datasets such as WebNLG, AMR-to-text benchmarks, etc. (not experimentally evaluated in this paper).",
            "performance_metrics": null,
            "comparison_to_other_representations": "The paper contrasts GNN-based encoders (mentioned in related work) with LM-based linearization approaches. It notes that while GNN models encode structure directly, LM-based linearization enables reuse of large pretrained sequence models; however, direct experimental comparisons were not conducted in this study.",
            "limitations_or_challenges": "Noted in related work: requires training on graph-to-text pairs (annotations), more modeling complexity, and potentially higher resource/training cost compared to using pretrained LMs with linearized graphs.",
            "uuid": "e5232.2",
            "source_info": {
                "paper_title": "Evaluating Generative Models for Graph-to-Text Generation",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity",
            "rating": 2,
            "sanitized_title": "have_your_text_and_use_it_too_endtoend_neural_datatotext_generation_with_semantic_fidelity"
        },
        {
            "paper_title": "Neural pipeline for zero-shot data-to-text generation",
            "rating": 2,
            "sanitized_title": "neural_pipeline_for_zeroshot_datatotext_generation"
        }
    ],
    "cost": 0.01221125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Generative Models for Graph-to-Text Generation
27 Jul 2023</p>
<p>Shuzhou Yuan shuzhou.yuan@kit.edu 
Karlsruhe Institute of Technology</p>
<p>Michael Färber michael.faerber@kit.edu 
Karlsruhe Institute of Technology</p>
<p>Evaluating Generative Models for Graph-to-Text Generation
27 Jul 202319EA2BDB0B67E1F5308F767A9EB76682arXiv:2307.14712v1[cs.CL]
Large language models (LLMs) have been widely employed for graph-to-text generation tasks.However, the process of finetuning LLMs requires significant training resources and annotation work.In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting.Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART.Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively.However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information.As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.We have made the text generated by generative models publicly available. 1</p>
<p>Introduction</p>
<p>Graph-to-text generation is a subtask of data-to-text generation and natural language generation (NLG) (Gatt and Krahmer, 2018).Its purpose is to generate fluent descriptive text based on the structure of a given graph (see Figure 1).With the widespread use of graph structured data, this technique plays a crucial role in various natural language processing applications, including question answering, dialogue systems, and data augmentation (He et al., 2017;Zhao et al., 2020;Josifoski et al., 2023).Previous research on model architectures has achieved method of Ribeiro et al. (2021a), we represent the graph as a linearized sequence of text for input to the models (see Figure 1).</p>
<p>To assess the performance of the generative models, we conduct a comprehensive evaluation on each dataset.Employing machine translation metrics such as BLEU (Papineni et al., 2002), ME-TEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) to the generated texts, we reveal that the generative models fall short of matching the quality achieved by state-of-the-art approaches.To identify patterns of mistakes made by the generative models, we perform error analysis by comparing the generated texts with the reference texts.Additionally, we fine-tune a BERT model to detect the machinegenerated text.We make the texts generated by the models available on GitHub to facilitate future research on the analysis of machine-generated text and trustworthy AI.</p>
<p>In summary, our study aims to assess the performance of generative models in the zero-shot setting for graph-to-text generation using two distinct benchmarks.Our contribution lies in conducting a rigorous quantitative analysis of the results, shedding light on the effectiveness of generative models in this domain.</p>
<p>Related Work</p>
<p>Graph-to-text generation.Various efforts have been made to enhance graph-to-text generation using neural network models.They can be categorized into two main types: Graph Neural Network (GNN) based models and Language Model (LM) based models.GNN-based models typically em-ploy a graph encoder to encode the graph structure (Beck et al., 2018;Marcheggiani and Perez-Beltrachini, 2018;Damonte and Cohen, 2019;Koncel-Kedziorski et al., 2019;Ribeiro et al., 2019;Li et al., 2021).In contrast, LM-based models do not rely on the graph structure but purely on the sequence of tokens in the text.As such, graphs have first been transformed into a linearized representation before being fed into LMs to generate coherent text (Harkous et al., 2020;Ribeiro et al., 2021a,b).Besides GNN and LM, previous works have also explored the use of Recurrent Neural Networks (RNNs) such as LSTM and GRU for graph-to-text generation (Song et al., 2018;Zhao et al., 2020;Guo et al., 2020).We follow the approach of Konstas et al. (2017) and other prior works by using a linearized graph as input for generative models.</p>
<p>Generative Models.Generative language models, such as GPT-3 (Brown et al., 2020), Instruct-GPT (Ouyang et al., 2022), and ChatGPT, have been designed to learn and generate natural language text.These models are based on the transformer decoder architecture (Vaswani et al., 2017), which enables them to handle large amounts of training data and perform zero-shot applications.While GPT-3 has made a significant breakthrough in text completion, InstructGPT and ChatGPT possess unique characteristics that align user intent with a conversational style.These models are trained using supervised fine-tuning and reward modeling, allowing them to generate high-quality responses that accurately reflect the user's needs and preferences.InstructGPT and ChatGPT are first fine-tuned on the GPT-3 model through supervised learning and then further trained using As demonstrated by Ouyang et al. (2022), this approach substantially improves the model's performance on NLP benchmarks.Although there have been numerous reports and research evaluating the performance of generative models in various NLP applications such as summarization (Bang et al., 2023), machine translation (Jiao et al., 2023), and multilingual evaluation (Ahuja et al., 2023), our work focuses on the generative models' capability to handle structured data.</p>
<p>Dataset</p>
<p>We evaluate generative models using the AGENDA and WebNLG datasets, as they are widely used in recent research on graph-to-text generation (Koncel-Kedziorski et al., 2019;Ribeiro et al., 2021a;Li et al., 2021) and as they represent different domains: scholarly domain and general domain (e.g., as given in Wikipedia).We focus on the test sets of AGENDA and WebNLG for our experiments, as the models do not require further training.In the following, we briefly describe the used datasets.AGENDA.Abstract GENeration DAtaset (AGENDA) is a dataset that pairs knowledge graphs with paper abstracts from scientific domains (Koncel-Kedziorski et al., 2019).The graphs in AGENDA were automatically extracted from the SciIE information extraction system (Luan et al., 2018).Each instance in AGENDA includes the title, entities, graph, and abstract of a paper.We use the title, entities, and graph as input for the models.</p>
<p>WebNLG.This dataset is a benchmark for mapping sets of RDF triples to text (Gardent et al., 2017).The RDF triples are subgraphs of the knowledge graph DBpedia (Auer et al., 2007), while the texts describe the graphs in one or a few sentences.The WebNLG challenge2 has released several versions of this dataset since 2017.In order to com-</p>
<p>Results</p>
<p>Our results are summarized in Table 2 and 3 The results obtained from AGENDA demonstrate that finetuned BART and T5 models outperform generative models in terms of state-of-the-art performance.Both T5 and BART achieve BLEU scores exceeding 20, while GPT-3 only attains a BLEU score of 8.34 and ChatGPT achieves 10.57.Consistently, other evaluation metrics align with the BLEU scores, further highlighting the limited performance of generative models without finetuning.Notably, ChatGPT exhibits a slightly improved performance compared to GPT-3 on the AGENDA benchmark.Analysis of the results reveals that ChatGPT consistently outperforms GPT-3 across all metrics, showcasing a 2.23 higher BLEU score, a 2.14 higher METEOR score, a 0.23 higher RougeL score, a 4.44 higher Chrf++ score, and a 4.49 higher BLEURT score.</p>
<p>Examining the results from WebNLG, it becomes evident that fine-tuned T5 and BART models consistently outperform generative models without fine-tuning.Notably, both T5 and BART achieve BLEU scores exceeding 50, whereas generative models only attain a BLEU score of 11.08 for Chat-GPT and 20.36 for GPT-3.Surprisingly, GPT-3 outperforms ChatGPT on the WebNLG benchmark with a BLEU score that is 9.28 higher, a METEOR score that is 3.06 higher, a RougeL score that is 9.77 higher, and a Chrf++ score that is 9.20 higher.The primary reason for this difference is that ChatGPT tends to produce hallucinations easily and generates longer text.We provide further elaboration on two examples in Section 5.</p>
<p>Error Analysis</p>
<p>We observe that the texts generated by generative models contain errors following similar patterns.In Table 4 and Table 5, we show two examples from AGENDA and WebNLG.As shown in the example of Table 4, generative models are capable of generating fluent paper abstracts based on the provided input, but they contain errors that contradict the facts represented by the graph structure.For instance, the triple "accuracy <R> EVALUATE-FOR <T> correlation analysis" in the example graph is wrongly generated by GPT-3 as "which (correlation analysis) is then used to evaluate the estimation accuracies".Also, Chat-GPT describes it as "(The authors) apply clustering and correlation analysis to evaluate the accuracy".In both cases, the models reverse the relationship between "accuracy" and "correlation analysis".</p>
<p>Furthermore, ChatGPT ignores the relations "PART-OF" and "USED-FOR" in the input.This suggests that generative models still face challenges in comprehending the semantic relations between entities, which can result in the generation of inaccurate text.</p>
<p>While generating paper abstracts is complex and challenging, generating short descriptive text from a knowledge graph is relatively more straightforward.As LLMs, GPT-3 and ChatGPT are trained</p>
<p>AGENDA Example</p>
<p>Input</p>
<p>Generate paper abstract from title, entities and graph: <title> Estimation of semantic case of Japanese dialogue by use of distance derived from statistics of dependency .<entities> statistics of dependent noun-particle-verb triples atr dialogue corpus measure of distance semantic cases noun-particle-verb triples consistency rates single-case clusters clustering analysis correlation analysis estimation accuracies accuracy <graph> <H> statistics of dependent noun-particle-verb triples <R> USED-FOR <T> measure of distance <H> nounparticle-verb triples <R> PART-OF <T> atr dialogue corpus <H> semantic cases <R> PART-OF <T> atr dialogue corpus <H> accuracy <R> EVALUATE-FOR <T> correlation analysis</p>
<p>Reference In an attempt to estimate the semantic cases for noun-particle-verb triples in the atr dialogue corpus, the authors propose a measure of distance based on statistics of dependent noun-particle-verb triples.A clustering analysis of all the triples in the corpus was conducted using the measure of distance.Competence of the proposed measure of distance is verified by examination of the distribution of the single-case clusters.By use of the score derived from the measure of distance of the training corpus, the authors conducted the estimation of the correct semantic case for a given noun-particle-verb triples in the test corpus.</p>
<p>The result remarkably differentiates the particles with respect to the estimation accuracies.For instance, particle 'wo' has accuracies over 80 %, while 'de' has accuracies less than 40%.The correlation analysis between the accuracy and the consistency rates indicates that the particles of higher consistency have also tendencies to higher accuracies.</p>
<p>GPT-3</p>
<p>This paper aims to estimate semantic cases of Japanese dialogue using a measure of distance derived from statistics of dependent noun-particle-verb triples from the ATR dialogue corpus.Clustering analysis and correlation analysis are employed to examine the consistency rates of single-case clusters, which is then used to evaluate the estimation accuracies of the distance measure.</p>
<p>ChatGPT</p>
<p>This paper proposes a method to estimate the semantic case of Japanese dialogue using a measure of distance derived from the statistics of dependent nounparticle-verb triples.The authors use the ATR dialogue corpus and apply clustering and correlation analysis to evaluate the accuracy of their method.They report consistent rates for single-case clusters and show that their method achieves high estimation accuracies.The study demonstrates the potential of using statistics of dependency for analyzing Japanese dialogue.</p>
<p>on large corpora and thus already contain world knowledge.In the example provided in Table 5, generative models generate descriptive text fluently based on the structured input.However, GPT-3 produces text with incorrect facts.For instance, "Abner W. Sibal" is the name of a person, but it is recognized as the name of a battle by GPT-3.While GPT-3 produces text with incorrect facts, it is worth noting that ChatGPT-generated text not only covers the input information, but also contains redundant messages from its internal knowledge (hallucinations).Furthermore, the sentence "Abner W. Sibal was a prominent figure who faced great challenges" generated by ChatGPT has made a subjective judgement about the character and may cause unnecessary bias to potential users.</p>
<p>To investigate the difference between modelgenerated texts and human-written reference texts, we create datasets containing both types of text and finetune a pretrained BERT model for a binary text However, the fight against the Axis powers was not just a battle against military might.It was a battle against the ideologies that fueled the enemy's aggression.As a commander, Sibal had to navigate these complexities and ensure that his troops remained committed to the cause of freedom.Throughout the war, Sibal faced many challenges and obstacles.But he remained steadfast and only grew more determined to see the war through to its ultimate conclusion.His efforts were recognized and honored, not just by his own country but also by leaders around the world.Even the likes of Joseph Stalin, renowned commander and leader of the Soviet Union, recognized the critical role Sibal played in the victory against fascism.Today, Abner W. Sibal's name remains a testament to the bravery and dedication of those who fought in World War II.His legacy serves as an inspiration to future generations, reminding them of the importance of standing up against tyranny and oppression wherever it may arise.We create several datasets for AGENDA, WebNLG, and a combined dataset containing both AGENDA and WebNLG examples.The training and test sets are split in an 80:20 ratio.We fine-tune BERT for five epochs using the AdamW optimizer (Loshchilov and Hutter, 2019).As shown in Table 7, BERT achieves high scores across all datasets.This demonstrates that generative models generate text that follows similar patterns, and a state-of-theart text classifier can easily distinguish between them.</p>
<p>Conclusion</p>
<p>In this paper, we explored the capabilities of generative models in generating coherent text from structured data, focusing on two benchmarks: AGENDA and WebNLG.To achieve this, we adopted the linearized graph representation approach employed Leveraging the zero-shot ability of language models, we prepended the prompt to the input text as an instruction for both GPT-3 and ChatGPT.We conducted a comprehensive evaluation using various metrics.Our findings reveal that generative models fall short of surpassing previous models that have been trained and finetuned on large volumes of training data.These results highlight limitations of generative models in achieving state-of-the-art performance in graph-totext generation tasks.Furthermore, we conducted an error analysis of the text generated by the models.The generative models struggle in capturing the relationships between entities and often produce unrelated information, leading to hallucinations.To further investigate the machine generated text, we employ finetuned BERT to conduct a text classification task.BERT achieves high F1 scores in distinguishing between machine-generated text and human-written text.Our study provides extensive evaluation of generative models for graph-to-text generation.Future work should focus on refining machine-generated text and reducing hallucinations for graph-to-text generation by using generative models.</p>
<p>Ethical Consideration and Limitation</p>
<p>We observe that generative models may generate text containing fake facts or offensive content.And the datasets we collected may also contain incorrect or offensive statements.We do not support the views expressed in the machine generated text, we merely venture to analyze the machine generated text and provide an useful resource for future research.</p>
<p>As the limitation of this work, we found out that the reproducibility of GPT-3 and ChatGPT is questionable.The models often return different response from same request, which makes our results hard to reproduce and the brings randomness to the evaluation scores.</p>
<p>Figure 1: Examples of graph structures, prompts and linearised graphs of (a) AGENDA and (b) WebNLG.</p>
<p>. As comparison, we take the results from Ribeiro et al. (2021a), which are achieved by finetuned BART and T5.</p>
<p>Table 1 :
1
Statistics of test sets from AGENDA and WebNLG.
AGENDA WebNLGNumber of Instance1,0001,862Average Input Tokens16966reinforcement learning based on human feedback.</p>
<p>Table 2 :
2
Results on AGENDA.
ModelBLEU↑ METEOR↑ RougeL↑ Chrf++↑ BLEURT↑T5 large (Ribeiro et al., 2021a)22.1523.73---13.96BART large (Ribeiro et al., 2021a)23.6525.19---10.93GPT-38.3414.8824.9941.42-32.54ChatGPT10.5717.0225.2245.86-28.05</p>
<p>Table 3 :
3
Results on WebNLG.
DatasetBLEU↑ METEOR↑ RougeL↑ Chrf++↑ BLEURT↑T5 large (Ribeiro et al., 2021a)59.7044.18-75.40-BART large (Ribeiro et al., 2021a)54.7242.23-72.29-GPT-320.3626.9545.6457.9513.39ChatGPT11.0823.8935.8748.75-10.99pretrained BERT (Devlin et al., 2019) model.</p>
<p>Table 4 :
4
Paper abstract generated by GPT-3 and ChatGPT, incorrect texts are marked with red.</p>
<p>Table 5 :
5
Descriptive text generated by GPT-3 and ChatGPT, incorrect texts are marked with red, hallucinations are marked with blue.Joseph Stalin was the commander of World War II and Abner W. Sibal was one of the battles fought during the war.ChatGPT Abner W. Sibal was a prominent figure who faced great challenges during his time as a soldier in World War II.He fought bravely as a commander, leading his troops with dedication and determination.His efforts were critical to the success of the war effort and his name became synonymous with bravery and patriotism.During World War II, the world was gripped in the fight against tyranny and oppression.Abner W. Sibal was one of the many who answered the call to serve his country.Through his leadership skills and strategic thinking, he was able to navigate the treacherous waters of war and emerge victorious.
WebNLG ExampleInputGenerate text from graph: <H> Abner W. Sibal <R> battles <T> World War II<H> World War II <R> commander <T> Joseph StalinReference Joseph Stalin was a commander in World War II in which Abner W Sibal alsofought.GPT-3</p>
<p>Table 6 :
6
Statistics of ChatGPT generated datasets
DatasetMachine-generated instance Human-written instanceAGENDA10001000WebNLG18624894All28625894classification task. The statistics of our datasets arepresented in Table 6.</p>
<p>Table 7 :
7
Results of BERT to detect GPT-3 and Chat-GPT generated text.
ModelAccuracy Macro F1GPT-3 AGENDA98.0098.00ChatGPT AGENDA100100GPT-3 WebNLG91.6489.25ChatGPT WebNLG96.8295.75GPT-3 All93.5592.38ChatGPT All96.4095.82in prior work.
https://github.com/ShuzhouYuan/Eval_ G2T_GenModels significant performance on graph-to-text generation benchmarks (Koncel-Kedziorski et al.
, 2019; Ribeiro et
 al., 2020;Zhao et al., 2020;Li
et al., 2021;Ribeiro et al., 2021b). In particular, Ribeiro et al. (2021a) achieved state-of-the-art performance by employing large pretrained language models and sufficient training data. However, the zero-shot setting for graph-to-text generation remains challenging due to the inconsistent input format (unstructured text vs. preformatted text) between pretraining and fine-tuning stages for large language models.Recently, generative models such as GPT-3(Brown et al., 2020), 
InstructGPT (Ouyang et al.,  2022), and ChatGPT have gained tremendous attention in both the NLP research community and the general public. Researchers have evaluated these models on various NLP benchmarks in
the zeroshot setting(Bang et al., 2023;Jiao et al., 2023;Ahuja et al., 2023). However, their ability to process structured data, and in particular graph data, such as knowledge graphs, is understudied and worth being explored(Bang et al., 2023). Given the significant resources and annotations required for training graph-to-text generation models(Li et al., 2021), utilizing a zero-shot setting could save training resources and prove advantageous for both economic and ecological reasons.Previous approaches has come up with a neural pipeline to enable zero-shot for graph-to-text generation but didn't use generative models(Kasner and Dusek, 2022). In contrast, our approach adopts the zero-shot setting by using prompts as instructions for generative models, specifically GPT-3 and ChatGPT(Brown et al., 2020;Ouyang et al., 2022). We evaluate the models' ability to translate graph data into fluent text using the test sets from two widely used graph-to-text generation datasets: WebNLG(Gardent et al., 2017) and AGENDA(Koncel-Kedziorski et al., 2019). Following the
https://synalp.gitlabpages.inria.fr/ webnlg-challenge/ pare with previous work, we take the test data of WebNLG challenge 2017 for our experiments.4 Experiments Data Preprocessing. Since GPT-3 and ChatGPT require a sequence of text as input, we convert the graph structure into a linearized representation followingRibeiro et al. (2021a). To assist the models in identifying the head, relation, and tail entities, we prepend <H>, <R>, and <T> tokens before the entities, as done in previous work(Harkous et al., 2020). In the AGENDA dataset, each sample also includes a title and entities. Thus, we additionally add <title>, <entities>, and <graph> tokens (see Figure1).Model Settings. We use the GPT-3 model variant text-davinci-003 and the ChatGPT model variant gpt-3.5-turbo-0301 for our experiments. Each instance is treated as a single request, and the first response from the model is taken as the generated text. The prompt used for the models plays a significant role as it serves as the task description and directly influences the content of the generated text. Previous work designed prompts by asking ChatGPT(Jiao et al., 2023). Following their approach, we ask ChatGPT to provide prompts: "Please provide prompts or templates for graph-to-text generation:". Since AGENDA and WebNLG have different data structures, we use the prompt "Generate paper abstract from title, entities, and graph:" for AGENDA. For WebNLG, we use the prompt "Generate text from graph:". We expect that in this way the generated text fits the format of a scientific paper abstract better for AGENDA, while the models generate texts in open domain for WebNLG.Baseline. Similar to our experimental methodology, Ribeiro et al. (2021a) finetuned T5 and BART using linearized graphs as input and generated descriptive texts. Therefore, we consider their findings as the baseline for comparison with our own experiments.Evaluation. Following related work, we implement a thorough evaluation with metrics BLEU(Papineni et al., 2002), METEOR(Banerjee and Lavie, 2005), RougeL(Lin, 2004) and Chrf++(Popović, 2017). Additionally, to assess the semantic meaning and coherence of the generated text, we employ BLEURT(Sellam et al., 2020), a metric that evaluates not only the surface match of n-grams but also the semantic representation extracted from a
AcknowledgementsWe thank the anonymous reviewers for their helpful comments.We also would like to thank Nicholas Popovic for his feedback on this work.
Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, arXiv:2303.12528Mega: Multilingual evaluation of generative ai. 2023arXiv preprint</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The Semantic Web. Berlin, Heidelberg; Berlin HeidelbergSpringer2007</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, 10.18653/v1/P18-1026Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Structural neural encoders for AMR-to-text generation. Marco Damonte, Shay B Cohen, 10.18653/v1/N19-1366Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics2019</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational Linguistics2017</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Albert Gatt, Emiel Krahmer, Journal of Artificial Intelligence Research. 612018</p>
<p>CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training. Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)Dublin, IrelandAssociation for Computational Linguistics2020</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, 10.18653/v1/2020.coling-main.218Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain2020International Committee on Computational Linguistics</p>
<p>Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning. Shizhu He, Cao Liu, Kang Liu, Jun Zhao, 10.18653/v1/P17-1019Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Is chatgpt a good translator? a preliminary study. Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Zhaopeng Tu, arXiv:2301.087452023arXiv preprint</p>
<p>Exploiting asymmetry for synthetic training data generation: SynthIE and the case of information extraction. Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West, arXiv:2303.041322023arXiv preprint</p>
<p>Neural pipeline for zero-shot data-to-text generation. Zdeněk Kasner, Ondrej Dusek, 10.18653/v1/2022.acl-long.271Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Fewshot knowledge graph-to-text generation with pretrained language models. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen, 10.18653/v1/2021.findings-acl.136Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, ICLR'19Proceedings of the 7th International Conference on Learning Representations. the 7th International Conference on Learning Representations2019</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, 10.18653/v1/W18-6501Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational Linguistics2018</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>chrF++: words helping character n-grams. Maja Popović, 10.18653/v1/W17-4770Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, 10.18653/v1/2021.nlp4convai-1.20Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI. the 3rd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2021aHinrich Schütze, and Iryna Gurevych</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Claire Zhang, Iryna Gardent, Gurevych, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 82020</p>
<p>Structural adapters in pretrained language models for AMR-to-Text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, 10.18653/v1/2021.emnlp-main.351Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021bOnline and Punta Cana</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur P Parikh, ACL. 2020</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, ; Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsCurran Associates, Inc2017. 202030Advances in Neural Information Processing Systems. Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
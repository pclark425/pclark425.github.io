<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9239 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9239</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9239</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-266551727</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.16171v2.pdf" target="_blank">Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work provides a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS .</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9239.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9239.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Principled prompts (ATLAS principles) — GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Principled prompt engineering using the 26 ATLAS guiding principles (applied to GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated set of 26 prompt-design principles (ATLAS) that modify wording, structure, role assignment, delimiters, example-driven prompting, output priming, and other presentation aspects to guide LLM responses. Evaluated by comparing responses with and without these principles on the ATLAS benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ATLAS (principled prompt evaluation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ATLAS is a manually crafted benchmark containing a standard subset across domains and a challenging subset focused on reasoning; for each principle there are human-selected questions with and without principled prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Principled prompts: prompts rewritten to follow the 26 principles (examples: integrate intended audience/role, use delimiters e.g. '###Instruction###', include phrases like 'Your task is'/'You MUST', output primers, example-driven few-shot demonstrations, 'think step by step' leading words, combine CoT with few-shot, allow clarifying questions, assign role/audience, repeat important phrases, add unbiased-answer phrases). Single-response generation per question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard/unmodified prompts (the original user prompts used as baseline without the ATLAS principles).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>boosting (response quality via human eval): +57.7% (average quality improvement); correctness (accuracy) improvement: +36.4% (average) when applied to GPT-4 (reported by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Baseline (standard prompts) performance not given as absolute numbers in text, improvements reported relative to baseline: quality +57.7%, accuracy +36.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+57.7% quality (boosting) and +36.4% accuracy (correctness) for GPT-4 using principled prompts vs. standard prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper hypothesis/findings: larger models have substantial capacity for simulation and adapt better to precise directives; making tasks and directives more precise (assigning roles, using clear structure/delimiters, explicit requirements) causes outputs to align more closely with expectations. The paper suggests models are not merely memorizing but adapting to prompt structure, and that assigning roles (audience/expert/child) and clearer instructions drives better alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation on ATLAS: for each principle and the challenging subset the authors used 20 human-selected questions with and without principled prompts; single response per question; human evaluation of response quality and correctness; models compared include LLaMA-1/2 variants, LLaMA-2-70B-chat, GPT-3.5, GPT-4; constrained-shot prompting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9239.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9239.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Principled prompts — aggregated scales</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application of ATLAS 26 principles across multiple LLM scales (LLaMA-1/2, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper applies the same 26 principled prompt modifications across small (7B), medium (13B) and large (70B, GPT-3.5/4) models and reports aggregated boosting and correctness effects, showing larger models benefit more.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-1/2 (7B,13B,70B), GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B, GPT-3.5/4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ATLAS (standard subset and challenging subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ATLAS manually crafted benchmark: standard subset across domains and a challenging reasoning subset; evaluation of response quality (boosting) and correctness (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Principled prompt set applied uniformly (see prior entry): includes structural conventions (###Instruction###), audience assignment, few-shot examples, CoT combination, output primers, explicit imperative phrases, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard/unmodified prompts (no principled formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregated boosting: authors report an average ~50% improvement in response quality across different LLMs; absolute correctness when employing principles: small/medium models generally reach 10%–40% accuracy on average, large models reach >40% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative correctness improvement: applying principles generally yields >10% improvement on average across models; for larger models the relative improvement can exceed 20%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Approx. +50% boosting (quality) on average across models; relative correctness increase typically >10% and >20% for larger models; absolute correctness with principles: small/medium ≈10–40%, large >40%.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Findings: all principles bring significant improvements across model scales; larger models receive larger absolute and relative gains, consistent with greater capacity to simulate roles and follow precise directives. Some specific principles (2, 5, 15, 16, 25, 26) produced the largest gains for large-scale models; principle 14 improved all questions it was applied to.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models grouped into small-scale (7B), medium-scale (13B), large-scale (70B, GPT-3.5/4). For each principle authors used 20 question pairs (with and without principle) in ATLAS; single-response generation; human evaluation for boosting and correctness metrics. Some principles were not applicable to correctness evaluation on complex reasoning items (principles 14, 15, 21, 22, 23).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9239.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9239.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought + Few-shot (recommended)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combination of Chain-of-Thought prompting with few-shot example-driven prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper recommends combining CoT (eliciting intermediate reasoning steps) with few-shot example-driven prompts as one of the principled prompt techniques to improve complex reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (recommended for larger models in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning subset of ATLAS / general reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex mathematical and reasoning problems (paper notes some correctness evaluations used complex reasoning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot examples provided in the prompt plus explicit instruction to 'think step by step' / elicit chain-of-thought intermediate steps; i.e., example-driven CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard zero-shot or standard prompt without CoT or few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper cites the general literature (and includes as a principle) that CoT and few-shot can help complex reasoning; the authors recommend combining them as part of the 26 principles but do not report a per-format numeric effect size within this paper's experiments (they reference prior work on CoT effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Listed as a recommended principle (one of the 26); no separate numeric results reported specifically isolating CoT+few-shot in the paper's aggregated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9239.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9239.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delimiters / '###Instruction###' format / Output primers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured prompt formatting using delimiters (e.g., '###Instruction###'), and output primers (starting the desired output in the prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Principles include a recommended prompt formatting convention starting with '###Instruction###' optionally followed by '###Example###' or '###Question###', and the use of output primers (ending the prompt with the beginning of the desired output) to steer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-1/2, GPT-3.5, GPT-4 (applied across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B, GPT-3.5/4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ATLAS (general and challenging subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarked prompts across varied tasks (QA, reasoning, text generation) in ATLAS; formatting and priming are evaluated as part of the 26 principles.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Structured delimiter-based prompt (start '###Instruction###' then optional '###Example###'/'###Question###' blocks; separate sections by blank lines), and use of output primers (ending prompt with the expected beginning of the answer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unstructured prompts without delimiters or output primers (baseline prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue structured formatting and output priming reduce ambiguity and help the model anchor the expected answer format, improving quality and concision. Recommended as part of the principled prompts which produced aggregate gains (see aggregate results).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>This formatting is one of the 26 principles; experiments compared full principled prompts (including structured formatting) vs. unmodified prompts, but paper does not isolate numeric effect of the delimiter formatting alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9239.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9239.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Allow clarifying questions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive prompts that instruct the model to ask clarifying questions before answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One ATLAS principle is to allow the model to elicit precise details and requirements by instructing it to ask follow-up questions until it has enough information to produce the desired output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-1/2, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B, GPT-3.5/4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ATLAS (user-interaction and generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where additional clarification can change the required output (e.g., writing tasks, multi-step requests); included in the prompt-principles evaluated on ATLAS.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt explicitly instructing the model to ask clarifying questions first (e.g., 'From now on, I would like you to ask me questions to ...') enabling interactive, multi-turn elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Non-interactive one-shot prompts that expect a complete answer without clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors posit that allowing clarifying questions helps the model gather needed constraints and reduces misinterpretation, contributing to the overall quality gains seen when the full set of principles is applied; no isolated numeric effect reported.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Included among the 26 principles and evaluated as part of the ATLAS principled prompts vs baseline; specific ablation metrics for this interaction style are not separately reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Ask-Me-Anything: A simple strategy for prompting language models. <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners. <em>(Rating: 2)</em></li>
                <li>Guiding large language models via directional stimulus prompting. <em>(Rating: 2)</em></li>
                <li>Can language models learn from explanations in context? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9239",
    "paper_id": "paper-266551727",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Principled prompts (ATLAS principles) — GPT-4",
            "name_full": "Principled prompt engineering using the 26 ATLAS guiding principles (applied to GPT-4)",
            "brief_description": "A curated set of 26 prompt-design principles (ATLAS) that modify wording, structure, role assignment, delimiters, example-driven prompting, output priming, and other presentation aspects to guide LLM responses. Evaluated by comparing responses with and without these principles on the ATLAS benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "ATLAS (principled prompt evaluation benchmark)",
            "task_description": "ATLAS is a manually crafted benchmark containing a standard subset across domains and a challenging subset focused on reasoning; for each principle there are human-selected questions with and without principled prompts.",
            "presentation_format": "Principled prompts: prompts rewritten to follow the 26 principles (examples: integrate intended audience/role, use delimiters e.g. '###Instruction###', include phrases like 'Your task is'/'You MUST', output primers, example-driven few-shot demonstrations, 'think step by step' leading words, combine CoT with few-shot, allow clarifying questions, assign role/audience, repeat important phrases, add unbiased-answer phrases). Single-response generation per question.",
            "comparison_format": "Standard/unmodified prompts (the original user prompts used as baseline without the ATLAS principles).",
            "performance": "boosting (response quality via human eval): +57.7% (average quality improvement); correctness (accuracy) improvement: +36.4% (average) when applied to GPT-4 (reported by authors).",
            "performance_comparison": "Baseline (standard prompts) performance not given as absolute numbers in text, improvements reported relative to baseline: quality +57.7%, accuracy +36.4%.",
            "format_effect_size": "+57.7% quality (boosting) and +36.4% accuracy (correctness) for GPT-4 using principled prompts vs. standard prompts.",
            "explanation_or_hypothesis": "Paper hypothesis/findings: larger models have substantial capacity for simulation and adapt better to precise directives; making tasks and directives more precise (assigning roles, using clear structure/delimiters, explicit requirements) causes outputs to align more closely with expectations. The paper suggests models are not merely memorizing but adapting to prompt structure, and that assigning roles (audience/expert/child) and clearer instructions drives better alignment.",
            "null_or_negative_result": false,
            "experimental_details": "Evaluation on ATLAS: for each principle and the challenging subset the authors used 20 human-selected questions with and without principled prompts; single response per question; human evaluation of response quality and correctness; models compared include LLaMA-1/2 variants, LLaMA-2-70B-chat, GPT-3.5, GPT-4; constrained-shot prompting approach.",
            "uuid": "e9239.0",
            "source_info": {
                "paper_title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Principled prompts — aggregated scales",
            "name_full": "Application of ATLAS 26 principles across multiple LLM scales (LLaMA-1/2, GPT-3.5, GPT-4)",
            "brief_description": "The paper applies the same 26 principled prompt modifications across small (7B), medium (13B) and large (70B, GPT-3.5/4) models and reports aggregated boosting and correctness effects, showing larger models benefit more.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-1/2 (7B,13B,70B), GPT-3.5, GPT-4",
            "model_size": "7B, 13B, 70B, GPT-3.5/4",
            "task_name": "ATLAS (standard subset and challenging subset)",
            "task_description": "ATLAS manually crafted benchmark: standard subset across domains and a challenging reasoning subset; evaluation of response quality (boosting) and correctness (accuracy).",
            "presentation_format": "Principled prompt set applied uniformly (see prior entry): includes structural conventions (###Instruction###), audience assignment, few-shot examples, CoT combination, output primers, explicit imperative phrases, etc.",
            "comparison_format": "Standard/unmodified prompts (no principled formatting).",
            "performance": "Aggregated boosting: authors report an average ~50% improvement in response quality across different LLMs; absolute correctness when employing principles: small/medium models generally reach 10%–40% accuracy on average, large models reach &gt;40% accuracy.",
            "performance_comparison": "Relative correctness improvement: applying principles generally yields &gt;10% improvement on average across models; for larger models the relative improvement can exceed 20%.",
            "format_effect_size": "Approx. +50% boosting (quality) on average across models; relative correctness increase typically &gt;10% and &gt;20% for larger models; absolute correctness with principles: small/medium ≈10–40%, large &gt;40%.",
            "explanation_or_hypothesis": "Findings: all principles bring significant improvements across model scales; larger models receive larger absolute and relative gains, consistent with greater capacity to simulate roles and follow precise directives. Some specific principles (2, 5, 15, 16, 25, 26) produced the largest gains for large-scale models; principle 14 improved all questions it was applied to.",
            "null_or_negative_result": false,
            "experimental_details": "Models grouped into small-scale (7B), medium-scale (13B), large-scale (70B, GPT-3.5/4). For each principle authors used 20 question pairs (with and without principle) in ATLAS; single-response generation; human evaluation for boosting and correctness metrics. Some principles were not applicable to correctness evaluation on complex reasoning items (principles 14, 15, 21, 22, 23).",
            "uuid": "e9239.1",
            "source_info": {
                "paper_title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Chain-of-Thought + Few-shot (recommended)",
            "name_full": "Combination of Chain-of-Thought prompting with few-shot example-driven prompting",
            "brief_description": "The paper recommends combining CoT (eliciting intermediate reasoning steps) with few-shot example-driven prompts as one of the principled prompt techniques to improve complex reasoning performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various (recommended for larger models in paper)",
            "model_size": null,
            "task_name": "Complex reasoning subset of ATLAS / general reasoning tasks",
            "task_description": "Complex mathematical and reasoning problems (paper notes some correctness evaluations used complex reasoning tasks).",
            "presentation_format": "Few-shot examples provided in the prompt plus explicit instruction to 'think step by step' / elicit chain-of-thought intermediate steps; i.e., example-driven CoT prompting.",
            "comparison_format": "Standard zero-shot or standard prompt without CoT or few-shot examples.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Paper cites the general literature (and includes as a principle) that CoT and few-shot can help complex reasoning; the authors recommend combining them as part of the 26 principles but do not report a per-format numeric effect size within this paper's experiments (they reference prior work on CoT effectiveness).",
            "null_or_negative_result": null,
            "experimental_details": "Listed as a recommended principle (one of the 26); no separate numeric results reported specifically isolating CoT+few-shot in the paper's aggregated metrics.",
            "uuid": "e9239.2",
            "source_info": {
                "paper_title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Delimiters / '###Instruction###' format / Output primers",
            "name_full": "Structured prompt formatting using delimiters (e.g., '###Instruction###'), and output primers (starting the desired output in the prompt)",
            "brief_description": "Principles include a recommended prompt formatting convention starting with '###Instruction###' optionally followed by '###Example###' or '###Question###', and the use of output primers (ending the prompt with the beginning of the desired output) to steer generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-1/2, GPT-3.5, GPT-4 (applied across experiments)",
            "model_size": "7B, 13B, 70B, GPT-3.5/4",
            "task_name": "ATLAS (general and challenging subsets)",
            "task_description": "Benchmarked prompts across varied tasks (QA, reasoning, text generation) in ATLAS; formatting and priming are evaluated as part of the 26 principles.",
            "presentation_format": "Structured delimiter-based prompt (start '###Instruction###' then optional '###Example###'/'###Question###' blocks; separate sections by blank lines), and use of output primers (ending prompt with the expected beginning of the answer).",
            "comparison_format": "Unstructured prompts without delimiters or output primers (baseline prompts).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors argue structured formatting and output priming reduce ambiguity and help the model anchor the expected answer format, improving quality and concision. Recommended as part of the principled prompts which produced aggregate gains (see aggregate results).",
            "null_or_negative_result": null,
            "experimental_details": "This formatting is one of the 26 principles; experiments compared full principled prompts (including structured formatting) vs. unmodified prompts, but paper does not isolate numeric effect of the delimiter formatting alone.",
            "uuid": "e9239.3",
            "source_info": {
                "paper_title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Allow clarifying questions",
            "name_full": "Interactive prompts that instruct the model to ask clarifying questions before answering",
            "brief_description": "One ATLAS principle is to allow the model to elicit precise details and requirements by instructing it to ask follow-up questions until it has enough information to produce the desired output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-1/2, GPT-3.5, GPT-4",
            "model_size": "7B, 13B, 70B, GPT-3.5/4",
            "task_name": "ATLAS (user-interaction and generation tasks)",
            "task_description": "Tasks where additional clarification can change the required output (e.g., writing tasks, multi-step requests); included in the prompt-principles evaluated on ATLAS.",
            "presentation_format": "Prompt explicitly instructing the model to ask clarifying questions first (e.g., 'From now on, I would like you to ask me questions to ...') enabling interactive, multi-turn elicitation.",
            "comparison_format": "Non-interactive one-shot prompts that expect a complete answer without clarification.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors posit that allowing clarifying questions helps the model gather needed constraints and reduces misinterpretation, contributing to the overall quality gains seen when the full set of principles is applied; no isolated numeric effect reported.",
            "null_or_negative_result": null,
            "experimental_details": "Included among the 26 principles and evaluated as part of the ATLAS principled prompts vs baseline; specific ablation metrics for this interaction style are not separately reported in the paper.",
            "uuid": "e9239.4",
            "source_info": {
                "paper_title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Ask-Me-Anything: A simple strategy for prompting language models.",
            "rating": 2,
            "sanitized_title": "askmeanything_a_simple_strategy_for_prompting_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Guiding large language models via directional stimulus prompting.",
            "rating": 2,
            "sanitized_title": "guiding_large_language_models_via_directional_stimulus_prompting"
        },
        {
            "paper_title": "Can language models learn from explanations in context?",
            "rating": 1,
            "sanitized_title": "can_language_models_learn_from_explanations_in_context"
        }
    ],
    "cost": 0.01524275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4
18 Jan 2024</p>
<p>Mahmoud Sondos 
joint first author &amp; equal contribution VILA Lab
Mohamed bin Zayed University of AI</p>
<p>Bsharat 
joint first author &amp; equal contribution VILA Lab
Mohamed bin Zayed University of AI</p>
<p>Aidar Myrzakhan 
joint first author &amp; equal contribution VILA Lab
Mohamed bin Zayed University of AI</p>
<p>Zhiqiang Shen 
joint first author &amp; equal contribution VILA Lab
Mohamed bin Zayed University of AI</p>
<p>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4
18 Jan 202481812818F24E5220636515C3259D11F6arXiv:2312.16171v2[cs.CL]
This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models.Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts.Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design.We hope that this work can provide a better guide for researchers working on the prompting of large language models.Project page is available at https://github.com/VILA-Lab/ATLAS.</p>
<p>Introduction</p>
<p>"</p>
<p>Prompt engineering is the art of communicating with a generative large language model.</p>
<p>" ChatGPT, 2023</p>
<p>Large language models (LLMs) like ChatGPT [13] have shown impressive abilities in various domains and tasks, such as answering questions [7], mathematical reasoning [5], code generating [11,9], etc.However, their application and usage, especially on designing the optimal instructions or prompts, can sometimes be unclear to the common users.In this work, we aim to reveal these mysteries for developers or general users when inquiring and interacting with LLMs, and further enhance the quality of the responses from the pretrained LLMs by simply curating better prompts.</p>
<p>Given that directly fine-tuning LLMs for particular tasks tends to be impractical or unattainable for the majority of users and developers due to inefficiency, the research community has turned its attention to the optimization of prompts.The technique of prompt engineering, which entails the crafting of precise, task-specific instructions in</p>
<p>GPT-4: Climate Change: An Overview</p>
<p>Climate change refers to the long-term altera3on of temperature and typical weather pa9erns in a place.Human ac3vi3es, especially the burning of fossil fuels and deforesta3on, have increased concentra3ons of Earth's atmospheric greenhouse gases, leading to an increase in global average temperatures.This has resulted in a wide range of impacts:</p>
<p>-Rising Sea Levels: As polar ice melts and oceans warm and expand.</p>
<p>-Extreme Weather Events: Such as hurricanes, droughts, and heavy rainfall.</p>
<p>-Threats to Biodiversity: Many species are facing habitat disrup3on.</p>
<p>-Ocean Acidifica@on: Due to increased carbon dioxide affec3ng marine life.</p>
<p>Addressing climate change requires global coopera3on to reduce emissions and promote sustainable prac3ces.Immediate ac3on is essen3al to mi3gate its detrimental effects on our planet's ecosystems and future genera3ons.</p>
<p>Human: Write a brief text on climate change using simple English like you're explaining something to a 5-year-old.</p>
<p>GPT-4: Okay! Imagine Earth wearing a big, cozy blanket.This blanket keeps our planet warm enough for us to live.But, if the blanket gets too thick, Earth gets too hot.Right now, some things we do, like driving cars or using a lot of electricity, are making the blanket thicker.This is called "climate change."And because of it, some places are geEng too hot, ice is melFng, and some animals are having a hard Fme.But the good news is, we can all help!By using less electricity, planFng trees, and taking care of our planet, we can make sure Earth's blanket is just right.!❤</p>
<p>LLM LLM</p>
<p>Human: If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?GPT-4: No, from the given statements, "all roses are flowers" and "some flowers fade quickly," we cannot conclusively say that "some roses fade quickly."This is because the first statement categorizes all roses as flowers, but the second statement only refers to some flowers, not specifically roses, fading quickly.Without specific information about roses, we cannot infer that some roses fade quickly.The group of flowers that fade quickly might or might not include roses.</p>
<p>Human: I'm going to tip $300K for a better solution!If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?</p>
<p>GPT-4: The statement "If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?" is a logical inference ques3on.To break it down:</p>
<p>1.All roses are flowers.(This is a universal affirma3ve statement.)2. Some flowers fade quickly.(This is a par3cular affirma3ve statement.)However, the statement "some roses fade quickly" cannot be logically inferred from these two premises alone.The fact that some members of the larger category (flowers) have a par3cular characteris3c (fade quickly) does not necessarily mean that this characteris3c applies to all subsets of that category (roses in this case).</p>
<p>In other words, while some flowers do fade quickly, we don't have specific informa3on about roses in the premises given.Therefore, we can't conclude that some roses fade quickly based on the informa3on provided.natural language, either manually or through automated means, and the careful selection of representative examples for inclusion in the prompt, has become a central area of investigation for LLMs.Despite these dedicated efforts, the task of reliably guiding LLMs to produce specific responses and making full use of the capability of pretrained LLMs continues to pose a considerable challenge.</p>
<p>LLM</p>
<p>In this work, we present comprehensive principled instructions to improve the quality of prompts for LLMs.Specifically, we investigate a wide range of behaviors when feeding into different types and formulations of prompts, such as integrating the intended audience in the prompt, e.g., add "the audience is an expert in the field", or "the audience is the 5-year-old child", as well as other multiple aspects of the characteristics of LLMs.Our findings indicate that larger models possess a considerable capacity for simulation.The more precise the task or directive provided, the more effectively the model performs, aligning its responses more closely with our expectations.This suggests that LLMs do not merely memorize training data but are capable of adapting this information to suit varying prompts, even when the core inquiries remain constant.Therefore, it proves beneficial to assign a specific role to LLMs as a means to elicit outputs that better match our intended results.</p>
<p>We elaborate the principled instructions for LLM prompting, provide further motivation, and detail several specific designing principles in Section 3. In Section 4 we show experimentally that the proposed principles can produce higher quality, more concise, factual and less complicated or intricate responses than standard prompts for LLMs.Specifically, with the manually-designed ATLAS benchmark, which includes multiple questions for each principle, the specialized prompts we introduced have enhanced both the quality and accuracy of the LLM responses by an average of 57.7% and 36.4%,respectively, when applied to GPT-4.Furthermore, the improvements are more pronounced with the increase in model size, for example, the performance gains when moving from LLaMA-2-7B to GPT-4 exceed 20%.</p>
<p>Related Work</p>
<p>Large Language Models.The evolution of large language models (LLMs) has been pivotal in advancing natural language processing (NLP).This section reviews key developments in LLMs, providing a foundation for the current study.Beginning with Google's BERT [3] revolutionized context understanding through its bidirectional training approach, while T5 [18] further advanced the field by unifying various NLP tasks into a single framework.Concurrently, GPT-1 [15] introduced a pioneering model leveraging transformer architectures for unsupervised learning.This was followed by its successor, GPT-2 [16] which significantly expanded its parameter count to 1.5 billion, demonstrating remarkable capabilities in text generation.Then, GPT-3 [2] marked a substantial leap in scale and capability, boasting 175 billion parameters and showcasing proficiency across a wide range of language tasks.</p>
<p>Regarding other recently proposed LLMs, Gopher [17], not only advanced language processing capabilities with its 280-billion parameter model but also brought ethical considerations to the forefront.Meta's LLaMA series [22,23] highlighted the importance of efficiency, suggesting powerful performance with fewer resources, a concept also advocated by Chinchilla [4], which proposed that smaller, optimally trained models could achieve exceptional results.The latest in this series of innovations is Mistral [6] excels in efficiency and performance, outperforming larger models.The most recent milestones in this trajectory are OpenAI's GPT-4 [13] and Google's Gemini family [21].They represent another significant advancement in the field with their enhanced understanding and generative capabilities, setting new benchmarks for the application of LLMs in various domains.Prompting.Prompting [20,12,25,27,14], as a distinct aspect of interacting with LLMs and its simplicity with no need to fine-tune the model, has evolved into a nuanced field of study, highlighting the intricate relationship between user inputs and LLM responses.Early explorations, such as those by [20], delved into how varying prompt designs could dramatically influence the performance and outputs of language models, marking the birth of prompt engineering.This area rapidly expanded, uncovering the critical role of prompts in few-shot and zero-shot learning scenarios, exemplified by [2] work with GPT-3, where strategically crafted prompts enabled the model to perform tasks with minimal prior examples.Beyond mere task instruction, recent studies have shifted towards understanding the semantic and contextual nuances in prompts, examining how subtle changes can lead to significantly different responses from the LLM.</p>
<p>Ask-Me-Anything [1] prompting introduced focusing on using multiple imperfect prompts and aggregating them to improve model performance, particularly in questionanswering formats.Another one, Chain-of-Thought method [24], where the model generates a series of intermediate reasoning steps to improve performance on complex tasks.Also, least-to-most prompting [27] a novel strategy to break down complex problems into simpler subproblems, significantly enhancing the model's capability to tackle more challenging problems than those presented in the prompts.The effectiveness of explanation was explored [8], finding that explanations can enhance LLM's learning capabilities on complex tasks.Furthermore, a catalog of prompt engineering techniques was examined with ChatGPT [25], emphasizing the importance of prompt engineering in enhancing LLM applications in software development and education.It also highlighted that effective prompt design is crucial in improving LLM performance, particularly in coding practices and learning experiences.Lastly, Directional Stimulus Prompting [12] presents a novel framework that uses a tunable policy model to generate auxiliary prompts, guiding LLMs towards specific desired outcomes.This diversity in prompting strategies underscores the rapidly evolving landscape of LLMs, offering multiple directions to harness their capabilities more effectively.</p>
<p>Principles</p>
<p>Motivation</p>
<p>Since the quality of the responses generated by a pretrained and aligned LLM is directly relevant to the quality of the prompts or instructions provided by the users, it is essential to craft prompts that the LLM can comprehend and respond to effectively.The prompts delivered to an LLM serve as a way to program the interaction between a user and the LLM, enhancing its ability to address a diverse range of tasks.The primary focus of this work is on the methodology of crafting and customizing prompts to enhance output quality.This necessitates a comprehensive grasp of the functioning and behaviors of LLMs, their underlying mechanisms, and the principles governing their responses.In this work, we achieve this goal through elaborating 26 principles for comprehensive prompts in different scenarios and circumstances, examples are shown in Fig. 1.</p>
<p>Overview</p>
<p>The overview of principles is presented in Table 1.According to their unique nature, we group them into five categories as in Table 2: (1) Prompt Structure and Clarity, e.g., integrate the intended audience in the prompt such as the audience is an expert in the field; (2) Specificity and Information, e.g., Add to your prompt the following phrase "Ensure that your answer is unbiased and does not rely on stereotypes.";(3) User Interaction and Engagement, e.g., Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output "From now on, I would like you to ask me questions to...".(4) Content and Language Style, e.g., No need to be polite with LLM so there is no need to add phrases like "please", "if you don't mind", "thank you", "I would like to", etc., and get straight to the point; (5) Complex Tasks and Coding Prompts, e.g., Break down complex tasks into a sequence of simpler prompts in an interactive conversation.Add "I'm going to tip $xxx for a better solution!"7 Implement example-driven prompting (Use few-shot prompting).</p>
<p>8</p>
<p>When formatting your prompt, start with '###Instruction###', followed by either '###Example###' or '###Question###' if relevant.Subsequently, present your content.Use one or more line breaks to separate instructions, examples, questions, context, and input data.9 Incorporate the following phrases: "Your task is" and "You MUST".Incorporate the following phrases: "You will be penalized".Use the phrase "Answer a question given in a natural, human-like manner" in your prompts.Use leading words like writing "think step by step".Add to your prompt the following phrase "Ensure that your answer is unbiased and avoids relying on stereotypes."</p>
<p>Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output (for example, "From now on, I would like you to ask me questions to ...").</p>
<p>To inquire about a specific topic or idea or any information and you want to test your understanding, you can use the following phrase: "Teach me any [theorem/topic/rule name] and include a test at the end, and let me know if my answers are correct after I respond, without providing the answers beforehand."Assign a role to the large language models.Use Delimiters.Repeat a specific word or phrase multiple times within a prompt.Combine Chain-of-thought (CoT) with few-Shot prompts.Use output primers, which involve concluding your prompt with the beginning of the desired output.Utilize output primers by ending your prompt with the start of the anticipated response.</p>
<p>To write an essay /text /paragraph /article or any type of text that should be detailed: "Write a detailed [essay/text /paragraph] for me on [topic] in detail by adding all the information necessary".</p>
<p>To correct/change specific text without changing its style: "Try to revise every paragraph sent by users.You should only improve the user's grammar and vocabulary and make sure it sounds natural.You should maintain the original writing style, ensuring that a formal paragraph remains formal."</p>
<p>When you have a complex coding prompt that may be in different files: "From now and on whenever you generate code that spans more than one file, generate a [programming language ] script that can be run to automatically create the specified files or make changes to existing files to insert the generated code.[your question]".</p>
<p>Content and Language Style</p>
<p>To correct/change specific text without changing its style: "Try to revise every paragraph sent by users.You should only improve the user's grammar and vocabulary and make sure it sounds natural.You should maintain the original writing style, ensuring that a formal paragraph remains formal."Incorporate the following phrases: "Your task is" and "You MUST." 9</p>
<p>Incorporate the following phrases: "You will be penalized."</p>
<p>Assign a role to the language model.</p>
<p>Use the phrase "Answer a question given in natural language form" in your prompts.</p>
<p>No need to be polite with LLM so there is no need to add phrases like "please", "if you don't mind", "thank you", "I would like to", etc., and get straight to the point.</p>
<p>1</p>
<p>Repeat a specific word or phrase multiple times within a prompt.</p>
<p>Add "I'm going to tip $xxx for a better solution!"6</p>
<p>Complex Tasks and Coding Prompts</p>
<p>Break down complex tasks into a sequence of simpler prompts in an interactive conversation.3</p>
<p>When you have a complex coding prompt that may be in different files: o "From now and on whenever you generate code that spans more than one file, generate a [programming language ] script that can be run to automatically create the specified files or make changes to existing files to insert the generated code.</p>
<p>Design Principles</p>
<p>In this study, a number of guiding principles are established for formulating prompts and instructions to elicit high-quality responses from pre-trained large language models: Conciseness and Clarity: Generally, overly verbose or ambiguous prompts can confuse the model or lead to irrelevant responses.Thus, the prompt should be concise, avoiding unnecessary information that does not contribute to the task while being specific enough to guide the model.This is the basic principle guidance for prompt engineering.Contextual Relevance: The prompt must provide relevant context that helps the model understand the background and domain of the task.Including keywords, domain-specific terminology, or situational descriptions can anchor the model's responses in the correct context.We highlight this design philosophy in our presented principles.Task Alignment: The prompt should be closely aligned with the task at hand, using language and structure that clearly indicate the nature of the task to the model.This may involve phrasing the prompt as a question, a command, or a fill-in-the-blank statement that fits the task's expected input and output format.Example Demonstrations: For more complex tasks, including examples within the prompt can demonstrate the desired format or type of response.This often involves showing input-output pairs, especially in "few-shot" or "zero-shot" learning scenarios.Avoiding Bias: Prompts should be designed to minimize the activation of biases inherent in the model due to its training data.Use neutral language and be mindful of potential ethical implications, especially for sensitive topics.Incremental Prompting: For tasks that require a sequence of steps, prompts can be structured to guide the model through the process incrementally.Break down the task into a series of prompts that build upon each other, guiding the model step-by-step.Also, prompts should be adjustable based on the performance of the model and iterative feedback, i.e., it needs to be well prepared to refine the prompt based on initial outputs and model behaviors.Moreover, prompts should be adjustable based on the performance and response of the model, and iterative human feedback and preference.</p>
<p>Finally, more advanced prompts may incorporate programming-like logic to achieve complex tasks.For instance, use of conditional statements, logical operators, or even pseudo-code within the prompt to guide the model's reasoning process.The design of prompts is an evolving field, especially as LLMs become more sophisticated.As researchers continue to explore the limits of what can be achieved through prompt engineering, these principles will likely be refined and expanded.</p>
<p>Experiments</p>
<p>Setup and Implementation Details</p>
<p>All our evaluation is performed on ATLAS [19], a manually crafted benchmark for principled prompt evaluation.It contains a standard subset featuring questions across various domains, along with a challenging subset dedicated to reasoning and other complex tasks.In our evaluation, we utilize a single response for each question.For each principle and the challenging subset, it contains 20 human-selected questions with and without the principled prompts.Similar to [10,26], we compare each pair</p>
<p>Human (without principle):</p>
<p>Explain the concept of climate change and its potential effects on the environment in one short paragraph.</p>
<p>GPT-4:</p>
<p>Climate change refers to long-term shifts and alterations in temperature and weather patterns, primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes.Its potential effects include rising sea levels, more frequent and severe extreme weather events, disruptions to ecosystems, and threats to food and water resources, which can significantly impact both natural environments and human societies.</p>
<p>Human (with principle):</p>
<p>Explain the concept of climate change and its potential effects on the environment in one small paragraph.Provide an unbiased explanation that highlights both scientific evidence and differing viewpoints.</p>
<p>GPT-4:</p>
<p>Climate change refers to long-term shifts and alterations in temperature and weather patterns, primarily caused by human activities, such as burning fossil fuels, deforestation, and industrial processes.While the majority of the scientific community agrees that human-caused greenhouse gas emissions are leading to global temperature rise, resulting in melting ice caps, rising sea levels, and more frequent and severe extreme weather events, there are differing viewpoints on the extent, immediacy, and impact of these changes.Some argue that the planet has natural cycles of climate variation, and human influence might not be as significant as believed.Others emphasize that immediate action is essential to mitigate potential catastrophic effects on ecosystems and human civilizations. of responses from the same instructions with and without principles, and evaluate the various scales of LLM outputs by human evaluation.</p>
<p>Models and Metrics</p>
<p>We use instruction finetuned LLaMA-1-{7, 13}, LLaMA-2-{7, 13}, off-the-shelf LLaMA-2-70B-chat, GPT-3.5 (ChatGPT) and GPT-4 as our base models.We group these models into different scales: small-scale (7B models), medium-scale (13B) and large-scale (70B, GPT-3.5/4).We evaluate these models in two settings: Boosting and Correctness.They are employed together to provide a comprehensive understanding of a model's performance.For correctness, we specifically utilize complex reasoning tasks to accurately gauge the precision of the models' outputs, contrasting with our evaluation for boosting, where simpler tasks are employed to effectively measure quality improvements.This distinction ensures a better reflection of the true capabilities for different scales of models and the effect of the principles for prompts.Since we use questions that typically involve complex reasoning tasks for correctness, some principles are not applicable including principles 14, 15, 21, 22, 23.For instance, "Suppose a and b are positive real numbers with a &gt; b and ab = 8.Find the minimum value of a 2 +b 2 a−b ."</p>
<p>• Boosting.The result of boosting refers to the percentage increase in response quality across a set of questions when the proposed principles are applied.We assess the enhancement in the quality of responses from different LLMs via human evaluation after applying the outlined prompt principles.The original, unmodified prompts act as a baseline for measuring this enhancement.Demonstrating boosting confirms that a model's performance has improved due to the use of structured, principled instructions, as shown in Fig. 2.</p>
<p>Human (without principle):</p>
<p>Evaluate the usefulness of the following advice as helpful or not helpful.Advice: "If you have work, just start working" Usefulness:</p>
<p>GPT-4: Usefulness: helpful.</p>
<p>Human (with principle):</p>
<p>Evaluate the usefulness of the following advice as helpful or not helpful.• Correctness.The concept of correctness refers to the precision of the model's outputs or responses, ensuring they are accurate, relevant, and devoid of errors.We consider both absolute and relative correctness accuracy.Human evaluators are utilized to gauge this aspect, which is crucial for verifying the model's accuracy.</p>
<p>Correctness is a testament to the model's ability to generate outputs that align with the expected standards of accuracy, as shown in Fig. 3.</p>
<p>Results</p>
<p>Results on small, medium and large-scale LLMs</p>
<p>Boosting.The results of improvement after employing the introduced principles are shown in Fig. 4. Generally, all principles can bring a significant improvement on the three scales of LLMs.In the cases of principles 2, 5, 15, 16, 25 and 26, the large-scale models get the most improvement by the principled prompts.Particularly, for principle 14, as shown in Fig. 4, it has improved all questions it is applied to.</p>
<p>Correctness.(1) Absolute accuracy: we examine the absolute performance when employing the principles on various scales of models.Generally, these models achieve 20%∼40% accuracy on the averaged performance, as shown in Fig. 5.In particular, for small and medium scale models, the accuracy can basically reach between 10% and 40%, and for large models, the accuracy can reach more than 40%.(2) Relative accuracy:   Fig. 6 illustrates that applying the principles generally leads to a performance increase of over 10% across different models on average.For larger models, this enhancement can surpass 20%.</p>
<p>Results on individual LLMs</p>
<p>Boosting.Fig. 7 illustrates the improvement of response quality on individual model and principle after using the revised prompts.On average, there is a stable 50% improvement across different LLMs.Fig. 10 further provides the detailed results of improvement for each principle with different LLMs.</p>
<p>Correctness.Fig. 8 illustrates the absolute correctness accuracy and Fig. 9 shows the relative enhancements in accuracy across different sizes of LLMs.From LLaMA-2-13B,  LLaMA-2-70B-chat to GPT-3.5 and GPT-4, there is a noticeable trend: the larger the model, the greater the increase in correctness improvement.Fig. 11 and Fig. 12 further present the absolute and relative correctness enhancements by each principle.</p>
<p>More examples on various scales of LLMs</p>
<p>We present additional examples for both small and medium-scale LLMs, as illustrated in Fig. 13 and 14 for the small-scale LLaMA-2-7B, and Fig. 15 and 16 for the mediumscale LLaMA-2-13B.Empirically, the use of the proposed principles on prompts has demonstrably enhanced the accuracy of the responses generated by these models.</p>
<p>Conclusion</p>
<p>We presented 26 principles through an exhaustive analysis that enhances the LLM ability to focus on the crucial elements of the input context, leading to the generation of quality responses.By guiding the LLM with these meticulously crafted principles before the input is processed, we can encourage the model towards producing better responses.Our empirical results demonstrate that this strategy can effectively reformulate contexts that might otherwise compromise the quality of the output, thereby enhancing the relevance, brevity, and objectivity of the responses.</p>
<p>There are numerous directions for future exploration.In our experiments, we utilized a constrained shot prompting approach to apply these principles.There is potential to refine our base models to align with our principled instructions further with alternative strategies, such as fine-tuning, reinforcement learning, direct preference optimization, or different prompting methods using our generated dataset.Moreover, the strategies that prove successful could be integrated into standard LLM operations, for instance, by fine-tuning with the original/principled prompts as inputs and the polished, principled responses as targets for training.
L L a M A -1 -7 B L L a M A -2 -7 B L L a M A -1 -1 3 B L L a M A -2 -1 3 B L L a M A -2 -7 0 B -c</p>
<p>Limitations and Discussion</p>
<p>While the proposed 26 principles are designed to improve and enhance the quality of responses of LLMs across a diverse array of queries, the effectiveness of these principles may diminish when dealing with questions that are very complex or highly specialized.This limitation can mainly depend on the reasoning capabilities and training of each model.To address these variations, we have tested the principles across different scales to measure their effectiveness comprehensively.Despite our efforts in evaluating these principles on seven distinct language models, it is crucial to acknowledge that models with architectures different from those tested might respond in different ways to these principles.Additionally, our assessment of improvement and correctness percentages was based on a limited selection of questions.Expanding the question set in future research could yield more generalized findings and offer deeper insights into the applicability of each principle.Furthermore, the criteria and results may vary across various personnel assessments on the model responses.</p>
<p>LLMHuman:</p>
<p>Write a brief text on climate change.</p>
<p>Figure 1 :
1
Figure 1: Illustration example of prompts and corresponding responses before and after applying principles.Left is the original promotes and their responses from GPT-4, right is the principled prompts and the associated responses.Principles 5 and 6 are utilized.</p>
<p>When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: o Explain [insert specific topic] in simple terms.o Explain to me like I'm 11 years old.o Explain to me as if I'm a beginner in [field].o Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.</p>
<p>6
6</p>
<p>[your question]."Combine Chain-of-thought (Cot) with few-shot prompts.</p>
<p>Figure 2 :
2
Figure 2: Boosting example of LLM response after using the principle 13 on prompts.</p>
<p>Examples: 1 .GPT- 4 :
14
Advice: "To improve focus, try meditating for 15 minutes every morning."Usefulness: Helpful 2. Advice: "If you're sad, just cheer up and think positive thoughts."Usefulness: Not Helpful 3. Advice: "For effective time management, create a to-do list every day."Usefulness: Helpful Advce :"If you have wrk , just start working ", Usefulness: Not Helpful.</p>
<p>Figure 3 :
3
Figure 3: Correctness improvement example of LLM response after using the introduced principle 7 on prompts.</p>
<p>Figure 4 :
4
Figure 4: Boosting of LLM response quality after employing the introduced principles on prompts.small-scale indicates the 7B models, medium-scale indicates the 13B models and large-scale indicates the 70B and GPT-3.5/4models.</p>
<p>Figure 5 :
5
Figure 5: Absolute correctness of LLM response quality after employing the introduced principles on prompts.small-scale indicates the 7B models, medium-scale indicates the 13B models and large-scale indicates the 70B and GPT-3.5/4models.</p>
<p>Figure 6 :
6
Figure 6: Relative correctness improvement of LLM response quality after employing the introduced principles on prompts.small-scale indicates the 7B models, mediumscale indicates the 13B models and large-scale indicates the 70B and GPT-3.5/4models.</p>
<p>Figure 7 :
7
Figure 7: Boosting score across various LLMs on the ATLAS dataset.</p>
<p>Figure 8 :G
8
Figure 8: Absolute correctness score on the ATLAS dataset.</p>
<p>Figure 9 :Figure 10 :Figure 11 :Figure 12 :
9101112
Figure 9: Relative correctness improvement score on the ATLAS dataset.</p>
<p>Figure 14 :
14
Figure 14: Correctness improvement on small-scale LLaMA-2-7B model after using the introduced principle on prompts.</p>
<p>Figure 16 :
16
Figure 16: Correctness improvement on medium-scale LLaMA-2-13B model after using the introduced principle on prompts.</p>
<p>Table 1 :
1
When you want to initiate or continue a text using specific words, phrases, or sentences, utilize the following prompt: o I'm providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence].Finish it based on the words provided.Keep the flow consistent.Clearly state the requirements that the model must follow in order to produce content, in the form of the keywords, regulations, hint, or instructions To write any text, such as an essay or paragraph, that is intended to be similar to a provided sample, include the following instructions: o Use the same language based on the provided paragraph[/title/text /essay/answer].Overview of 26 randomly ordered prompt principles.Employ affirmative directives such as 'do' while steering clear of negative language like 'don't'.4UseLeadingwordslikewriting "think step by step."Useoutputprimers,whichinvolve concluding your prompt with the beginning of the desired output.byendingyourprompt with the start of the anticipated response.Add to your prompt the following phrase "Ensure that your answer is unbiased and avoids relying on stereotypes."Towriteany text intended to be similar to a provided sample, include specific instructions: o "Use the same language based on the provided paragraph [/title/text/essay/answer].Teach me the [Any theorem/topic/rule name] and include a test at the end, and let me know if my answers are correct after I respond, without providing the answers beforehand."Towrite an essay/text/paragraph/article or any type of text that should be detailed:
CategoryPrinciples#PrincipleIntegrate the intended audience in the prompt.2Prompt Structureand ClarityUse Delimiters.When formatting your prompt, start with '###Instruction###', followed by either '###Example###' or '###Ques-tion###' if relevant. Subsequently, present your content. Use one or more line breaks to separate instructions,examples, questions, context, and input data.Implement example-driven prompting (Use few-shot prompting).7When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the followingprompts:o Explain [insert specific topic] in simple terms.5o Explain to me like I'm 11 years old.o Explain to me as if I'm a beginner in [ field ].o "Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old."Specificity andInformation
" When you want to initiate or continue a text using specific words, phrases, or sentences, utilize the provided prompt structure: o I'm providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence].Finish it based on the words provided.Keep the flow consistent.Clearly state the model's requirements that the model must follow in order to produce content, in form of the keywords, regulations, hint, or instructions.To inquire about a specific topic or idea and test your understanding g, you can use the following phrase [16]: o "o "Write a detailed [essay/text/paragraph] for me on [topic] in detail by adding all the information necessary."User Interaction and Engagement Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output o "From now on, I would like you to ask me questions to ..." To write an essay /text /paragraph /article or any type of text that should be detailed: "Write a detailed [essay/text/paragraph] for me on [topic] in detail by adding all the necessary information."</p>
<p>Table 2 :
2
Prompt principle categories.</p>
<p>the following examples. </p>
<p>She went to school; the number of words. 4</p>
<p>How many words are in the following sentence. She plays football"? LLaMA-2-7B: 5 Human. Reem plays basketball; number of words : 3 Q: He plays football; the number of words? Human (without principle. with principle): LLaMA-2-7B: 3 Figure 13: Correctness improvement on small-scale LLaMA-2-7B model after using the introduced principle on prompts</p>
<p>Ask me anything: A simple strategy for prompting language models. Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, Christopher Ré, 2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, CoRR, abs/1810.048052018</p>
<p>. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre2022Training compute-optimal large language models</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, Mathprompter, arXiv:2303.05398Mathematical reasoning using large language models. 2023arXiv preprint</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b2023</p>
<p>Evaluating open-domain question answering in the era of large language models. Ehsan Kamalloo, Nouha Dziri, L A Charles, Davood Clarke, Rafiei, arXiv:2305.069842023arXiv preprint</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instructionfollowing models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, </p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Guiding large language models via directional stimulus prompting. Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan, arXiv:2302.11520202334arXiv preprint</p>
<p>. : Openai, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Daniel Perelman ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,132023Amin TootoonchianFilipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaGpt-4 technical report</p>
<p>Plum: Prompt learning using metaheuristic. Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, Tong Zhang, arXiv:2311.083642023arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George Van Den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat Mcaleese, Amy Wu, Erich Elsen, M Siddhant, Elena Jayakumar, David Buchatskaya, Esme Budden, Karen Sutherland, Michela Simonyan, Laurent Paganini, Lena Sifre, Xiang Martens, Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero2021Oriol Vinyals, Kareem AyoubLaura Rimell, Chris Dyer; Jeff Stanway, Lorrayne BennettCyprien de Masson d'AutumeDemis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher. CoRR, abs/2112.11446</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, CoRR, abs/1910.106832019</p>
<p>Atlas: A llm inquiry principle benchmark. Zhiqiang Shen, Sondos , Mahmoud Bsharat, Aidar Myrzakhan, 2024Preprint</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan Iv Au2, Eric Wallace, Sameer Singh, 2020</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Alexandre Sygnowski, Charlotte Frechette, Laura Smith, Lev Culp, Yi Proleev, Xi Luan, James Chen, Nathan Lottes, Federico Schucher, Alban Lebron, Natalie Rrustemi, Phil Clay, Tomas Crone, Jeffrey Kocisky, Bartek Zhao, Dian Perz, Heidi Yu, Adam Howard, Jack W Bloniarz, Han Rae, Laurent Lu, Marcello Sifre, Fred Maggioni, Dan Alcober, Megan Garrette, Shantanu Barnes, Jacob Thakoor, Gabriel Austin, William Barth-Maron, Rishabh Wong, Rahma Joshi, Deeni Chaabouni, Arun Fatiha, Ruibo Ahuja, Yunxuan Liu, Sarah Li, Jeremy Cogan, Chao Chen, Chenjie Jia, Qiao Gu, Jordan Zhang, Ale Jakse Grimstad, Martin Hartman, Gaurav Chadwick, Xavier Singh Tomar, Evan Garcia, Emanuel Senter, Thanumalayan Taropa, Jacob Sankaranarayana Pillai, Michael Devlin, Diego Laskin, De Las, Dasha Casas, Connie Valter, Lorenzo Tao, Adrià Blanco, David Puigdomènech Badia, Mianna Reitter, Jenny Chen, Clara Brennan, Sergey Rivera, Shariq Brin, Gabriela Iqbal, Jane Surita, Abhi Labanowski, Stephanie Rao, Emilio Winkler, Yiming Parisotto, Kate Gu, Yujing Olszewska, Ravi Zhang, Antoine Addanki, Annie Miech, Laurent El Louis, Denis Shafey, Geoff Teplyashin, Elliot Brown, Nithya Catt, Jan Attaluri, Jackie Balaguer, Pidong Xiang, Zoe Wang, Anton Ashwood, Albert Briukhov, Sanjay Webson, Smit Ganapathy, Ajay Sanghavi, Ming-Wei Kannan, Axel Chang, Josip Stjerngren, Yuting Djolonga, Ankur Sun, Matthew Bapna, Pedram Aitchison, Henryk Pejman, Tianhe Michalewski, Cindy Yu, Juliette Wang, Junwhan Love, Dawn Ahn, Kehang Bloxwich, Peter Han, Thibault Humphreys, James Sellam, Varun Bradbury, Sina Godbole, Bogdan Samangooei, Alex Damoc, Kaskasoli, M R Sébastien, Vijay Arnold, Shubham Vasudevan, Jason Agrawal, Dmitry Riesa, Richard Lepikhin, Srivatsan Tanburn, Hyeontaek Srinivasan, Sarah Lim, Pranav Hodkinson, Johan Shyam, Steven Ferret, Ankush Hand, Tom Le Garg, Jian Paine, Yujia Li, Minh Li, Alexander Giang, Zaheer Neitz, Sarah Abbas, Machel York, Elizabeth Reid, Aakanksha Cole, Dipanjan Chowdhery, Dominika Das, Vitaly Rogozińska, Pablo Nikolaev, Zachary Sprechmann, Lukas Nado, Flavien Zilka, Luheng Prost, Marianne He, Gaurav Monteiro, Chris Mishra, Josh Welty, Dawei Newlan, Miltiadis Jia, Clara Huiyi Allamanis, Raoul Hu, Justin De Liedekerke, Carl Gilmer, Shruti Saroufim, Shaobo Rijhwani, Disha Hou, Anirudh Shrivastava, Alex Baddepudi, Adnan Goldin, Albin Ozturel, Yunhan Cassirer, Daniel Xu, Devendra Sohn, Reinald Sachan, Craig Kim Amplayo, Dessie Swanson, Shashi Petrova, Arthur Narayan, Siddhartha Guez, Jessica Brahma, Miteyan Landon, Ruizhe Patel, Kevin Zhao, Luyu Villela, Wenhao Wang, Matthew Jia, Mai Rahtz, Legg Giménez, Hanzhao Yeung, James Lin, Petko Keeling, Diana Georgiev, Boxi Mincu, Salem Wu, Rachel Haykal, Kiran Saputro, James Vodrahalli, Zeynep Qin, Abhanshu Cankara, Nick Sharma, Will Fernando, Behnam Hawkins, Solomon Neyshabur, Adrian Kim, Priyanka Hutter, Alex Agrawal, George Castro-Ros, Tao Van Den Driessche, Fan Wang, Yang, Paul Shuo Yiin Chang, Ross Komarek, Mario Mcilroy, Guodong Lučić, Wael Zhang, Michael Farhan, Paul Sharman, Paul Natsev, Yong Michel, Yamini Cheng, Siyuan Bansal, Kris Qiao, Siamak Cao, Christina Shakeri, Justin Butterfield, Paul Chung, Shivani Kishan Rubenstein, Arthur Agrawal, Kedar Mensch, Karel Soparkar, Timothy Lenc, Aedan Chung, Loren Pope, Jackie Maggiore, Priya Kay, Shibo Jhakra, Joshua Wang, Mary Maynez, Taylor Phuong, Andrea Tobin, Maja Tacchetti, Kevin Trebacz, Yash Robinson, Sebastian Katariya, Paige Riedel, Kefan Bailey, Nimesh Xiao, Lora Ghelani, Ambrose Aroyo, Neil Slone, Xuehan Houlsby, Zhen Xiong, Elena Yang, Jonas Gribovskaya, Mateo Adler, Lisa Wirth, Music Lee, Thais Li, Jay Kagohara, Sophie Pavagadhi, Anna Bridgers, Sanjay Bortsova, Zafarali Ghemawat, Tianqi Ahmed, Richard Liu, Vijay Powell, Mariko Bolina, Polina Iinuma, James Zablotskaia, Besley, Da-Woon, Timothy Chung, Ramona Dozat, Xiance Comanescu, Jeremy Si, Guolong Greer, Martin Su, Raphaël Polacek, Simon Lopez Kaufman, Hexiang Tokumine, Elena Hu, Yingjie Buchatskaya, Mohamed Miao, Aditya Elhawaty, Nenad Siddhant, Jinwei Tomasev, Christina Xing, Helen Greer, Shereen Miller, Aurko Ashraf, Zizhao Roy, Ada Zhang, Angelos Ma, Milos Filos, Rory Besta, Ted Blevins, Chih-Kuan Klimenko, Soravit Yeh, Jiaqi Changpinyo, Oscar Mu, Mantas Chang, Carrie Pajarskas, Vered Muir, Charline Le Cohen, Krishna Lan, Amit Haridasan, Steven Marathe, Sholto Hansen, Rajkumar Douglas, Mingqiu Samuel, Sophia Wang, Chang Austin, Jiepu Lan, Justin Jiang, Jaime Alonso Chiu, Lars Lowe Lorenzo, Sébastien Sjösund, Zach Cevey, Thi Gleicher, Anudhyan Avrahami, Hansa Boral, Vittorio Srinivasan, Rhys Selo, Konstantinos May, Léonard Aisopos, Hussenot, Baldini Livio, Kate Soares, Michael B Baumli, Adrià Chang, Ben Recasens, Alexander Caine, Filip Pritzel, Fabio Pavetic, Anita Pardo, Justin Gergely, Vinay Frye, Dan Ramasesh, Kartikeya Horgan, Nora Badola, Subhrajit Kassner, Ethan Roy, Víctor Dyer, Alex Campos, Yunhao Tomala, Dalia El Tang, Elspeth Badawy, Basil White, Oran Mustafa, Abhishek Lang, Sharad Jindal, Zhitao Vikram, Sergi Gong, Ross Caelles, Gregory Hemsley, Fangxiaoyu Thornton, Wojciech Feng, Ce Stokowiec, Phoebe Zheng, C Thacker, Zhishuai ¸aglar Ünlü, Mohammad Zhang, James Saleh, Max Svensson, Piyush Bileschi, Ankesh Patil, Roman Anand, Katerina Ring, Arpi Tsihlas, Marco Vezer, Toby Selvi, Mikel Shevlane, Tom Rodriguez, Samira Kwiatkowski, Keran Daruki, Allan Rong, Nicholas Dafoe, Keren Fitzgerald, Mina Gu-Lemberg, Lisa Anne Khan, Marie Hendricks, Vladimir Pellat, James Feinberg, Tara Cobon-Kerr, Maribeth Sainath, Rauh, Hadi Sayed, Richard Hashemi, Yana Ives, Yaguang Hasson, Eric Li, Yuan Noland, Nathan Cao, Le Byrd, Qingze Hou, Thibault Wang, Michela Sottiaux, Jean-Baptiste Paganini, Alexandre Lespiau, Samer Moufarek, Kaushik Hassan, Shivakumar, Amol Joost Van Amersfoort, Pratik Mandhane, Anirudh Joshi, Matthew Goyal, Andrew Tung, Hannah Brock, Vedant Sheahan, Cheng Misra, Nemanja Li, Mostafa Rakićević, Fangyu Dehghani, Sid Liu, Junhyuk Mittal, Seb Oh, Eren Noury, Fantine Sezener, Matthew Huot, Nicola De Lamm, Charlie Cao, Gamaleldin Chen, Ed Elsayed, Mahdis Chi, Ian Mahdieh, Nan Tenney, Ivan Hua, Patrick Petrychenko, Dylan Kane, Rishub Scandinaro, Jonathan Jain, Romina Uesato, Adam Datta, Oskar Sadovsky, Dominik Bunyan, Shimu Rabiej, John Wu, Gautam Zhang, Edouard Vasudevan, Mahmoud Leurent, Ionut Alnahlawi, Nan Georgescu, Ivy Wei, Betty Zheng, Pam G Chan, Piotr Rabinovitch, Ye Stanczyk, David Zhang, Subhajit Steiner, Michael Naskar, Matthew Azzam, Adam Johnson, Chung-Cheng Paszke, Jaume Chiu, Afroz Sanchez Elias, Faizan Mohiuddin, Jin Muhammad, Andrew Miao, Nino Lee, Sahitya Vieillard, Jane Potluri, Elnaz Park, Jiageng Davoodi, Jeff Zhang, Drew Stanway, Abhijit Garmon, Zhe Karmarkar, Jong Dong, Aviral Lee, Luowei Kumar, Jonathan Zhou, William Evens, Zhe Isaac, Johnson Chen, Anselm Jia, Zhenkai Levskaya, Chris Zhu, Peter Gorgolewski, Yu Grabowski, Alberto Mao, Kaisheng Magni, Javier Yao, Norman Snaider, Paul Casagrande, Evan Suganthan, Geoffrey Palmer, Edward Irving, Manaal Loper, Isha Faruqui, Nanxin Arkatkar, Izhak Chen, Michael Shafran, Alfonso Fink, Irene Castaño, Wooyeol Giannoumis, Mikołaj Kim, Ashwin Rybiński, Jennifer Sreevatsa, David Prendki, Adrian Soergel, Willi Goedeckemeyer, Mohsen Gierke, Meenu Jafari, Jeremy Gaba, Diana Gage Wiesner, Yawen Wright, Harsha Wei, Yana Vashisht, Jay Kulizhskaya, Maigo Hoover, Lu Le, Chimezie Li, Lu Iwuanyanwu, Kevin Liu, Andrey Ramirez, Albert Khorlin, Cui, Lin Tian, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom Van Der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Dung Duc, Paula Nguyen, Sarmishta Kurylowicz, Sebastian Velury, Cassidy Krause, Lucas Hardin, Lili Dixon, Kiam Janzer, Ziqiang Choo, Biao Feng, Achintya Zhang, Tejasi Singhal, Mingyang Latkar, Quoc Zhang, Elena Le, Dayou Allica Abellan, Dan Du, Natasha Mckinnon, Tolga Antropova, Orgad Bolukbasi, David Keller, Daniel Reid, Maria Finchelstein, Remi Abi Raad, Peter Crocker, Robert Hawkins, Colin Dadashi, Sid Gaffney, Ken Lall, Egor Franko, Anna Filonov, Rémi Bulanova, Vikas Leblond, Shirley Yadav, Harry Chung, Luis C Askham, Kelvin Cobo, Felix Xu, Jun Fischer, Christina Xu, Chris Sorokin, Chu-Cheng Alberti, Colin Lin, Hao Evans, Alek Zhou, Hannah Dimitriev, Dylan Forbes, Zora Banarse, Jing Tung, Sabaer Li, John Fatehi, Omar Wieting, Benigno Ajmeri, Tao Uria, Yeongil Zhu, Laura Ko, Amélie Knight, Ning Héliou, Shane Niu, Chenxi Gu, Dustin Pang, Yeqing Tran, Nir Li, Ariel Levine, Norbert Stolovich, Rebeca Kalb, Sonam Santamaria-Fernandez, Wenny Goenka, Robin Yustalim, Ali Strudel, Balaji Elqursh, Charlie Lakshminarayanan, Shyam Deck, Hyo Upadhyay, Mike Lee, Zonglin Dusenberry, Xuezhi Li, Kyle Wang, Raphael Levin, Dan Hoffmann, Olivier Holtmann-Rice, Summer Bachem, Sho Yue, Eric Arora, Daniil Malmi, Qijun Mirylenka, Christy Tan, Soheil Koh, Siim Hassas Yeganeh, Steven Põder, Francesco Zheng, Mukarram Pongetti, Yanhua Tariq, Lucian Sun, Mojtaba Ionita, Pouya Seyedhosseini, Ragha Tafti, Zhiyu Kotikalapudi, Anmol Liu, Jasmine Gulati, Liu ; Nikhil, Tianrun Sethi, Ben Li, Shreya Brown, Wei Singh, Aaron Fan, Joe Parisi, Chenkai Stanton, Vinod Kuang, Koverkathu, A Christopher, Yunjie Choquette-Choo, Li, Abe Lu, Prakash Ittycheriah, Pei Shroff, Mani Sun, Sanaz Varadarajan, Rob Bahargam, David Willoughby, Ishita Gaddy, Dasgupta ; Brennan, Tyler Saeta, Yi Liechty, Yao Sun, Stephan Zhao, Pandu Lee, Doug Nayak, Manish Reddy Fritz, John Vuyyuru, Nidhi Aslanides, Martin Vyas, Xiao Wicke, Ma ; Hardie, James Cate, Keyvan Manyika, Yelin Amiri, Xi Kim, Kai Xiong, Florian Kang, Nilesh Luisier, David Tripuraneni, Mandy Madras, Guo ; Han, Garima Zhang, Jakob Pruthi, Feng Bauer, Christof Yang, Xiaowei Angermueller, Weiren Li, Julia Wang, Emmanouil Wiesinger, Yuan Koukoumidis, ; Mk Tian, Hongkun Blake, Anthony Yu, Jennimaria Urbanowicz, Chrisantha Palomaki, Fernando, Anitha Vijayakumar. Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Cosmo Du, Chloe Thornton; Xinyu Ye; Bart Chrzaszcz, Lily Wang,; Tom Hudson; Alex Morris, Ivo Penchev, Matthew Mauger; Alison Reid; Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky; Daniel Balle; Nina Martin,; Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge,; Kevin Brooks, Ken DurdenParashar Shah2023Lam Nguyen Thiet, Daniel Andor, ; Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de Cesare ; Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe ; Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang,Jordi Pont-Tuset. Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu,. Anand Iyer, Madhu Gurumurthy, Mark Goldenson. Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A family of highly capable multimodal models</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, 202334</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi202334</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8564 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8564</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8564</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-d8079ae74d4e2c2b803678267ae9bc7a90a82669</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d8079ae74d4e2c2b803678267ae9bc7a90a82669" target="_blank">VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles, is introduced and key factors influencing LVLMs' puzzle-solving performance are identified, including the number of clues, grid size, and rule complexity.</p>
                <p><strong>Paper Abstract:</strong> Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to engage in structured reasoning - an essential skill for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, highlighting fundamental limitations in their puzzle-solving capabilities. Most importantly, through systematic experiments, we identify and analyze key factors influencing LVLMs' puzzle-solving performance, including the number of clues, grid size, and rule complexity. Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. We will release VGRP-Bench to facilitate further research on LVLMs for complex, real-world problem-solving. Project page: https://yufan-ren.com/subpage/VGRP-Bench/.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8564.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8564.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (on Sudoku)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) evaluated on Sudoku within VGRP-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source multimodal chat LVLM (GPT-4o) evaluated by the paper on grid-based logic puzzles (e.g., 4x4 Sudoku) in both vision and text formats; shows substantial failures in spatial localization and multi-step constraint tracking despite chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal chat LVLM from OpenAI used as an off-the-shelf vision-language model; queried via official API in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (example: 4×4)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle requiring 2D spatial localization of digits, row/column/block constraint reasoning and multi-step deduction</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + text experiments. Models were given a screenshot and/or textual description and asked to output JSON-formatted board solutions; chain-of-thought (CoT) prompts were used in some runs. Evaluation used 5 independent inference runs × 20 instances (100 total). Output repair (json-repair) and an LLM-based formatter (GPT-4o) were used to standardize outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard LVLM vision-language inference with optional chain-of-thought prompting; no external symbolic solver constructed dynamically (paper states closed-source models relied solely on vision-language capabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Failed to consistently solve a simple 4×4 Sudoku in text-only format (<30% solving rate reported). Across puzzles the paper reports all evaluated LVLMs (including GPT-4o) have puzzle-solving success rates well below perfect (overall <80% for models in the benchmark); specific per-model figures appear in main figures (e.g., Fig.5) but 4×4 Sudoku text rate <30% is explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited. Paper documents direct failures of GPT-4o to localize numbers and understand grid layout (grid layout misunderstanding examples in supplementary material). These failures indicate the model often does not form a reliable 2D spatial representation of the board or maintain it across multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Larger models performed better in general (GPT-4o > GPT-4o-mini). Closed-source models generally outperform open-source ones. GPT-4o is compared to other closed-source and open-source LVLMs in aggregate plots (Fig.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Common failure modes include misplacing digits (e.g., confusing sequences [*, 2, *, ] with [, *, 2, *]), inability to keep a consistent state across steps, not detecting rule violations (duplication), failing to backtrack on errors, and occasional refusal-to-answer responses. Repetitive generations and JSON format noncompliance also occurred.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8564.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.0-Thinking (Gemini family, reasoning LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-capable variant of the Gemini family evaluated on the full VGRP-Bench suite; performs comparatively well among reasoning models but still exhibits substantial limitations on complex grid puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reasoning-capable closed-source LVLM in the Gemini family (listed in paper as a 'thinking' variant and included in experiments under a provided version id). Evaluated as an off-the-shelf vision-language reasoning model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple VGRP-Bench puzzles (including Sudoku and other grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Various grid-based visual reasoning puzzles requiring spatial matching, counting, local adjacency checks and multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + chain-of-thought evaluation on easy/medium/hard difficulties; same general protocol (screenshot + prompt, expected JSON-style output; 5 runs × 20 instances). Due to API rate limits, some evaluations were constrained (paper notes rate-limit for Gemini-2.0-Thinking).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Uses internal reasoning-capable architecture (the 'Thinking' variant) and chain-of-thought prompts in experiments; no external solver integration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to perform well among reasoning models and to outperform some preview/open-source reasoning models (e.g., Qwen-QVQ underperforms). Exact per-puzzle percentages shown in paper figures: overall puzzle-solving remains low across models (below high accuracy), with reasoning-model advantages visible but not eliminating many failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Partial: better step-level rule-following and some higher perception accuracy relative to non-reasoning variants, indicating more effective rule application; however, the paper still documents localization and state-tracking failures showing limited robust spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against other closed-source and open-source LVLMs: Gemini-2.0-Thinking performs better than Qwen-QVQ and many open-source variants, and is compared to models such as GPT-4o and Claude in aggregated plots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Rate limits restricted some experiments. Despite improved reasoning behavior, Gemini-2.0-Thinking still fails on medium/hard puzzles (e.g., Thermometers at medium-level saw near-zero perception across models) and exhibits common LVLM issues (mislocalization, lacking persistent internal board state, failure to backtrack).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8564.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 (Anthropic) evaluated on VGRP-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source LVLM (Claude 3.5) that achieved the highest perception and step-level rule-following scores among the evaluated models in the paper, but still cannot reliably solve medium/hard puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source chat/vision-capable model from Anthropic used in the paper's evaluation; provided via official API (version id claude-3-5-sonnet-20241022).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple (e.g., Sudoku and other grid puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles requiring perceptual detection, spatial localization, and rule-based multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision input and chain-of-thought prompts in the same standardized JSON-like output format used across the benchmark; evaluated at cell-level, step-level, and full-solution levels.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard LVLM vision-language reasoning with CoT; used as off-the-shelf model, no task-specific fine-tuning described for Claude in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Claude consistently achieved the highest performance on perception and step-level rule-following among tested models (figures in paper show Claude as top performer in cell-level and step-level metrics); nonetheless overall puzzle-solving remained far from perfect and decreased sharply with difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Relative evidence: higher step-level and perception accuracies suggest Claude better follows local spatial rules and cell-level queries. However, the model still exhibits state-tracking and backtracking failures indicating limitations in sustained spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Claude outperforms many closed- and open-source models in perception and step-level metrics; LlaVA is noted as among the worst. Comparisons are shown across aggregated figures (Figs.5–8).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite strong relative performance, Claude still struggles with medium/hard puzzles (e.g., Thermometers medium had <5% perception across all models) and shares common LVLM failure modes: grid misinterpretation, failure to maintain/update board state, inability to backtrack, and intermittent non-compliant JSON outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8564.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2 Vision-Instruct (S-SFT/R-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.2 Vision Instruct (11B) fine-tuned with Solution SFT and Reasoning SFT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Llama 3.2 Vision-Instruct (11B) used as the base model for two post-training strategies (S-SFT on final solutions; R-SFT on synthetic stepwise trajectories) that substantially improved easy-level puzzle solving but generalize poorly to unseen puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.2 Vision Instruct (base used: 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal LLM (Llama 3.2 family) deployed locally; the paper fine-tuned the 11B Vision-Instruct variant on synthetic puzzle solutions and reasoning trajectories using 8 A100 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple (training and evaluation performed on many VGRP-Bench puzzles such as Sudoku, Binairo, Aquarium, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles requiring spatial perception, rule-following, and multi-step deduction</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Two post-training setups: (1) S-SFT (Solution Supervised Fine-Tuning): model trained to map puzzle prompt+image to final JSON-formatted solution arrays; (2) R-SFT (Reasoning Supervised Fine-Tuning): model trained on synthetic DFS-derived step-by-step trajectories (states and intermediate assignments). Training dataset used up to 100k puzzles per puzzle type; training ran ~5 epochs on 8 A100 GPUs with causal modeling (masked question input). Evaluation used held-out, non-overlapping puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>S-SFT: direct supervision on final solutions (JSON). R-SFT: supervision on intermediate states generated by a DFS solver with process-of-elimination, intended to teach stepwise reasoning and backtracking. The R-SFT trajectory generator used DFS to avoid excessive branching and to record intermediate states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pre-trained Llama 3.2 11B produced essentially no correct answers before fine-tuning. After S-SFT and R-SFT both models showed significant improvements on easy-level puzzles (figures show major gains in perception and puzzle-solving at easy difficulty). R-SFT shows slightly better results on some puzzles (e.g., Binairo/Aquarium) while S-SFT outperforms on others. Exact per-puzzle percentages are provided in paper figures (e.g., Fig.11).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Fine-tuning results indicate the model can learn to follow spatial rules when supervised on solutions or reasoning traces; however, these learned behaviors appear largely memorized/specialized to trained puzzle types. R-SFT's effectiveness on step-by-step traces suggests the model can internalize state-update patterns, but R-SFT is prone to compounding errors in long trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>S-SFT vs R-SFT: both improve performance on trained puzzles; R-SFT sometimes better on specific puzzles (e.g., Aquarium/Binairo) while S-SFT better on others (e.g., Field-Explore). Generalization experiments (Fig.12) show SFT models perform poorly on unseen, rule-different puzzles (Sudoku-trained model fails on Aquarium).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuned models generalize poorly to unseen puzzles with different rules (overfitting). Computational cost constrained experiments to 11B models; further fine-tuning of larger models was not performed. R-SFT can compound errors over long reasoning trajectories. The paper also notes JSON output formatting and empty-cell symbol inconsistencies required post-processing (json-repair and an LLM-based formatter).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8564.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-VL-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL-72B (open-source LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open-source vision-language model (Qwen2.5-VL-72B) evaluated in VGRP-Bench; among open-source models it achieved the best perception performance but still lagged behind top closed-source models on full puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-VL-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-language model (Qwen family) used in the experiments (listed as Qwen2.5-VL-72B-Instruct in model versions). Deployed locally when evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple VGRP-Bench puzzles (perception and puzzle-solving evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles (perception-heavy and multi-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + CoT (chain-of-thought) evaluation, same dataset splits and prompt/output format as other models; cell-level and step-level probes as well as full-solution tasks were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard vision-language inference; no special post-training applied in this model's off-the-shelf evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Among open-source models, Qwen2.5-72B achieved the highest perception accuracy reported in the paper's evaluations (exact percentages shown in main figures). Despite stronger perception, puzzle-solving rates remained substantially lower than ideal and below top closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Relatively better perception suggests the model can detect grid contents reliably compared to other open-source models, but the paper still documents failures in multi-step reasoning and state updates that limit robust spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Qwen-QVQ (preview), Qwen2.5-72B performs better; overall closed-source models (e.g., Claude, Gemini, GPT-4o) tend to outperform it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still prone to common LVLM failures: JSON output formatting issues, cell mislocation in some puzzles, inability to maintain board state across steps, and poor performance on medium/hard levels (e.g., Thermometers medium task had <5% perception across models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8564.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-QVQ (preview)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen/QVQ-72B-Preview (preview reasoning LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preview reasoning LVLM variant of the Qwen family included in the paper; underperforms relative to Qwen2.5-72B and other reasoning-capable closed-source models on VGRP-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen/QVQ-72B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Preview release reasoning-capable LVLM (Qwen family) evaluated in the paper; listed among reasoning-model category but noted to be a preview with lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple grid puzzles in VGRP-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + CoT evaluation similar to other models; evaluated on easy/medium/hard levels.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning model architecture but in preview state; evaluated as off-the-shelf.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Underperforms compared to Qwen2.5-72B and some other reasoning models; specific numeric deficits shown in paper's aggregated plots (figures).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited; the model did not demonstrate robustness in spatial rule-following or perception relative to competitors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Explicitly compared unfavorably to Qwen2.5-72B and some closed-source reasoning models (e.g., Gemini-2.0-Thinking).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Preview model status likely explains lower performance; suffers the same common LVLM failure patterns: mislocalization, failure to backtrack, repetitive generation, and output formatting issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8564.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaVA-OneVision-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaVA-OneVision-7B (open-source LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller open-source LVLM (7B) evaluated in VGRP-Bench that exhibits relatively weak puzzle-solving and step-level rule-following performance compared to larger/open and closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llava-onevision-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B vision-language model (LlaVA family) used in the experiments; deployed locally via vLLM in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple grid-based puzzles (cell-level, step-level, full-solution tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles requiring spatial perception and constraint reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + CoT prompts, same standardized JSON-style output target; evaluated across easy/medium/hard splits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard vision-language inference; no post-training applied in off-the-shelf evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported among the worst performers on step-level rule-following and lower perception/puzzle-solving accuracies relative to larger models and many closed-source models (figures in paper highlight LlaVA as poor performer in step-level evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Weak: low cell- and step-level scores indicate limited ability to perform reliable spatial rule application or maintain board state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms compared to larger open-source models (e.g., Qwen2.5-72B) and closed-source models (Claude, Gemini, GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Exhibits common LVLM errors: repetitive outputs, format non-compliance, misinterpretation of complex visual components (e.g., Thermometers, Battle-Ships) and poor step-by-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8564.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8564.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregate Off-the-shelf LVLMs (VGRP-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate evaluation of off-the-shelf LVLMs on VGRP-Bench puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's core evaluation: many state-of-the-art closed-source and open-source LVLMs were benchmarked across 20 grid-style visual puzzles (20 puzzle types, multiple difficulty levels) showing systematic weaknesses in spatial perception, rule adherence, and multi-step spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple off-the-shelf LVLMs (e.g., GPT-4o, Gemini variants, Claude 3.5, Qwen2.5, Qwen-QVQ, Llama 3.2 variants, LlaVA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Varied set of closed- and open-source vision-language models evaluated without additional post-training. Versions used and ids are provided in the paper's supplement (e.g., gemini-2.0-flash-thinking-exp-01-21, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022, Qwen/QVQ-72B-Preview, Llama-3.2-11B-Vision-Instruct, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Full suite of VGRP-Bench puzzles (20 grid-like visual reasoning puzzles including Sudoku, Thermometers, Trees-and-Tents, Battle-Ships, Field-Explorer / Minesweeper, Binairo, Aquarium, Kakuro, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles requiring 2D spatial perception (cell recognition), neighborhood/adjacency reasoning, matching/pairing, counting, arithmetic constraints, unidirectional/connected-component reasoning, and multi-step backtracking/search</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standardized protocol: screenshot + textual prompt asking for perception (cell-level), rule-following (valid action), step-level chain-of-thought, and full-solution answers in JSON-like format. For separation of perception and reasoning, textual versions of puzzles were also provided. Evaluation ran 5 independent inference runs with 20 instances each (100 samples) per model and reported mean and standard deviation. Post-processing included json-repair and an LLM-based output formatter.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Primarily LVLM vision-language inference with optional Chain-of-Thought (CoT) prompting. For post-training research, the paper applied S-SFT and R-SFT to a Llama 3.2 base. The paper also explores RL (PPO/GRPO) for text-only training in the supplement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate result highlights: (1) All off-the-shelf LVLMs struggle on puzzles; puzzle-solving rates are often low (figures show many per-rule scores in the 0–45% range at easy level). (2) Perception: most closed-source models except Claude achieve <50% accuracy on cell perception; Qwen2.5-72B leads among open-source. (3) Difficulty effect: medium/hard levels sharply decrease accuracy (e.g., Thermometers at medium had <5% perception across all LVLMs). (4) Increasing number of clues improves puzzle-solving rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mixed/limited. The paper documents qualitative examples and ablations showing frequent grid-localization failures, misinterpretation of visual components (e.g., confusing cage sums with cell values in Killer Sudoku), inability to store/update previously taken actions, failure to backtrack, and repetitive generation. Text-only versions improve performance, indicating vision/perception contributes significantly to failures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Closed-source models generally outperform open-source ones; larger models tend to do better. Specific pairwise observations: GPT-4o > GPT-4o-mini; Gemini-2.0-Thinking performs well among reasoning models; Qwen-QVQ underperforms relative to Qwen2.5-72B. Post-training S-SFT/R-SFT on Llama 3.2 significantly improves trained-puzzle performance but generalization to unseen puzzles is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Systematic limitations observed: (1) perception failures (mislocalization, misunderstanding components), (2) reasoning failures (no state update, no backtracking, compounding errors), (3) format/output issues (broken JSON, wrong symbols for empty cells), (4) overfitting of SFT approaches and poor cross-puzzle generalization, and (5) computational limits preventing large-scale fine-tuning (experiments limited to 11B models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ING-VP: MLLMs cannot play easy vision-based games yet <em>(Rating: 2)</em></li>
                <li>Balrog: Benchmarking agentic LLM and VLM reasoning on games <em>(Rating: 2)</em></li>
                <li>Are large vision language models good game players? <em>(Rating: 2)</em></li>
                <li>Blink: Multimodal large language models can see but not perceive <em>(Rating: 2)</em></li>
                <li>EnigmaEval: A benchmark of long multimodal reasoning challenges <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8564",
    "paper_id": "paper-d8079ae74d4e2c2b803678267ae9bc7a90a82669",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o (on Sudoku)",
            "name_full": "GPT-4o (OpenAI) evaluated on Sudoku within VGRP-Bench",
            "brief_description": "Closed-source multimodal chat LVLM (GPT-4o) evaluated by the paper on grid-based logic puzzles (e.g., 4x4 Sudoku) in both vision and text formats; shows substantial failures in spatial localization and multi-step constraint tracking despite chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal chat LVLM from OpenAI used as an off-the-shelf vision-language model; queried via official API in the paper's experiments.",
            "model_size": null,
            "puzzle_name": "Sudoku (example: 4×4)",
            "puzzle_type": "grid-based logic puzzle requiring 2D spatial localization of digits, row/column/block constraint reasoning and multi-step deduction",
            "task_setup": "Vision + text experiments. Models were given a screenshot and/or textual description and asked to output JSON-formatted board solutions; chain-of-thought (CoT) prompts were used in some runs. Evaluation used 5 independent inference runs × 20 instances (100 total). Output repair (json-repair) and an LLM-based formatter (GPT-4o) were used to standardize outputs.",
            "mechanisms_or_strategies": "Standard LVLM vision-language inference with optional chain-of-thought prompting; no external symbolic solver constructed dynamically (paper states closed-source models relied solely on vision-language capabilities).",
            "performance_metrics": "Failed to consistently solve a simple 4×4 Sudoku in text-only format (&lt;30% solving rate reported). Across puzzles the paper reports all evaluated LVLMs (including GPT-4o) have puzzle-solving success rates well below perfect (overall &lt;80% for models in the benchmark); specific per-model figures appear in main figures (e.g., Fig.5) but 4×4 Sudoku text rate &lt;30% is explicitly reported.",
            "evidence_of_spatial_reasoning": "Limited. Paper documents direct failures of GPT-4o to localize numbers and understand grid layout (grid layout misunderstanding examples in supplementary material). These failures indicate the model often does not form a reliable 2D spatial representation of the board or maintain it across multi-step reasoning.",
            "comparisons": "Larger models performed better in general (GPT-4o &gt; GPT-4o-mini). Closed-source models generally outperform open-source ones. GPT-4o is compared to other closed-source and open-source LVLMs in aggregate plots (Fig.5).",
            "limitations_or_failure_cases": "Common failure modes include misplacing digits (e.g., confusing sequences [*, 2, *, ] with [, *, 2, *]), inability to keep a consistent state across steps, not detecting rule violations (duplication), failing to backtrack on errors, and occasional refusal-to-answer responses. Repetitive generations and JSON format noncompliance also occurred.",
            "uuid": "e8564.0",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-2.0-Thinking",
            "name_full": "Gemini-2.0-Thinking (Gemini family, reasoning LVLM)",
            "brief_description": "A reasoning-capable variant of the Gemini family evaluated on the full VGRP-Bench suite; performs comparatively well among reasoning models but still exhibits substantial limitations on complex grid puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-Thinking",
            "model_description": "Reasoning-capable closed-source LVLM in the Gemini family (listed in paper as a 'thinking' variant and included in experiments under a provided version id). Evaluated as an off-the-shelf vision-language reasoning model.",
            "model_size": null,
            "puzzle_name": "Multiple VGRP-Bench puzzles (including Sudoku and other grid puzzles)",
            "puzzle_type": "Various grid-based visual reasoning puzzles requiring spatial matching, counting, local adjacency checks and multi-step planning",
            "task_setup": "Vision + chain-of-thought evaluation on easy/medium/hard difficulties; same general protocol (screenshot + prompt, expected JSON-style output; 5 runs × 20 instances). Due to API rate limits, some evaluations were constrained (paper notes rate-limit for Gemini-2.0-Thinking).",
            "mechanisms_or_strategies": "Uses internal reasoning-capable architecture (the 'Thinking' variant) and chain-of-thought prompts in experiments; no external solver integration.",
            "performance_metrics": "Reported to perform well among reasoning models and to outperform some preview/open-source reasoning models (e.g., Qwen-QVQ underperforms). Exact per-puzzle percentages shown in paper figures: overall puzzle-solving remains low across models (below high accuracy), with reasoning-model advantages visible but not eliminating many failures.",
            "evidence_of_spatial_reasoning": "Partial: better step-level rule-following and some higher perception accuracy relative to non-reasoning variants, indicating more effective rule application; however, the paper still documents localization and state-tracking failures showing limited robust spatial reasoning.",
            "comparisons": "Compared against other closed-source and open-source LVLMs: Gemini-2.0-Thinking performs better than Qwen-QVQ and many open-source variants, and is compared to models such as GPT-4o and Claude in aggregated plots.",
            "limitations_or_failure_cases": "Rate limits restricted some experiments. Despite improved reasoning behavior, Gemini-2.0-Thinking still fails on medium/hard puzzles (e.g., Thermometers at medium-level saw near-zero perception across models) and exhibits common LVLM issues (mislocalization, lacking persistent internal board state, failure to backtrack).",
            "uuid": "e8564.1",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Claude-3.5",
            "name_full": "Claude 3.5 (Anthropic) evaluated on VGRP-Bench",
            "brief_description": "Closed-source LVLM (Claude 3.5) that achieved the highest perception and step-level rule-following scores among the evaluated models in the paper, but still cannot reliably solve medium/hard puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3.5",
            "model_description": "Closed-source chat/vision-capable model from Anthropic used in the paper's evaluation; provided via official API (version id claude-3-5-sonnet-20241022).",
            "model_size": null,
            "puzzle_name": "Multiple (e.g., Sudoku and other grid puzzles)",
            "puzzle_type": "Grid-based visual reasoning puzzles requiring perceptual detection, spatial localization, and rule-based multi-step reasoning",
            "task_setup": "Vision input and chain-of-thought prompts in the same standardized JSON-like output format used across the benchmark; evaluated at cell-level, step-level, and full-solution levels.",
            "mechanisms_or_strategies": "Standard LVLM vision-language reasoning with CoT; used as off-the-shelf model, no task-specific fine-tuning described for Claude in paper.",
            "performance_metrics": "Claude consistently achieved the highest performance on perception and step-level rule-following among tested models (figures in paper show Claude as top performer in cell-level and step-level metrics); nonetheless overall puzzle-solving remained far from perfect and decreased sharply with difficulty.",
            "evidence_of_spatial_reasoning": "Relative evidence: higher step-level and perception accuracies suggest Claude better follows local spatial rules and cell-level queries. However, the model still exhibits state-tracking and backtracking failures indicating limitations in sustained spatial reasoning.",
            "comparisons": "Claude outperforms many closed- and open-source models in perception and step-level metrics; LlaVA is noted as among the worst. Comparisons are shown across aggregated figures (Figs.5–8).",
            "limitations_or_failure_cases": "Despite strong relative performance, Claude still struggles with medium/hard puzzles (e.g., Thermometers medium had &lt;5% perception across all models) and shares common LVLM failure modes: grid misinterpretation, failure to maintain/update board state, inability to backtrack, and intermittent non-compliant JSON outputs.",
            "uuid": "e8564.2",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Llama-3.2 Vision-Instruct (S-SFT/R-SFT)",
            "name_full": "Llama 3.2 Vision Instruct (11B) fine-tuned with Solution SFT and Reasoning SFT",
            "brief_description": "Open-source Llama 3.2 Vision-Instruct (11B) used as the base model for two post-training strategies (S-SFT on final solutions; R-SFT on synthetic stepwise trajectories) that substantially improved easy-level puzzle solving but generalize poorly to unseen puzzles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3.2 Vision Instruct (base used: 11B)",
            "model_description": "Open-source multimodal LLM (Llama 3.2 family) deployed locally; the paper fine-tuned the 11B Vision-Instruct variant on synthetic puzzle solutions and reasoning trajectories using 8 A100 GPUs.",
            "model_size": "11B",
            "puzzle_name": "Multiple (training and evaluation performed on many VGRP-Bench puzzles such as Sudoku, Binairo, Aquarium, etc.)",
            "puzzle_type": "Grid-based visual reasoning puzzles requiring spatial perception, rule-following, and multi-step deduction",
            "task_setup": "Two post-training setups: (1) S-SFT (Solution Supervised Fine-Tuning): model trained to map puzzle prompt+image to final JSON-formatted solution arrays; (2) R-SFT (Reasoning Supervised Fine-Tuning): model trained on synthetic DFS-derived step-by-step trajectories (states and intermediate assignments). Training dataset used up to 100k puzzles per puzzle type; training ran ~5 epochs on 8 A100 GPUs with causal modeling (masked question input). Evaluation used held-out, non-overlapping puzzles.",
            "mechanisms_or_strategies": "S-SFT: direct supervision on final solutions (JSON). R-SFT: supervision on intermediate states generated by a DFS solver with process-of-elimination, intended to teach stepwise reasoning and backtracking. The R-SFT trajectory generator used DFS to avoid excessive branching and to record intermediate states.",
            "performance_metrics": "Pre-trained Llama 3.2 11B produced essentially no correct answers before fine-tuning. After S-SFT and R-SFT both models showed significant improvements on easy-level puzzles (figures show major gains in perception and puzzle-solving at easy difficulty). R-SFT shows slightly better results on some puzzles (e.g., Binairo/Aquarium) while S-SFT outperforms on others. Exact per-puzzle percentages are provided in paper figures (e.g., Fig.11).",
            "evidence_of_spatial_reasoning": "Fine-tuning results indicate the model can learn to follow spatial rules when supervised on solutions or reasoning traces; however, these learned behaviors appear largely memorized/specialized to trained puzzle types. R-SFT's effectiveness on step-by-step traces suggests the model can internalize state-update patterns, but R-SFT is prone to compounding errors in long trajectories.",
            "comparisons": "S-SFT vs R-SFT: both improve performance on trained puzzles; R-SFT sometimes better on specific puzzles (e.g., Aquarium/Binairo) while S-SFT better on others (e.g., Field-Explore). Generalization experiments (Fig.12) show SFT models perform poorly on unseen, rule-different puzzles (Sudoku-trained model fails on Aquarium).",
            "limitations_or_failure_cases": "Fine-tuned models generalize poorly to unseen puzzles with different rules (overfitting). Computational cost constrained experiments to 11B models; further fine-tuning of larger models was not performed. R-SFT can compound errors over long reasoning trajectories. The paper also notes JSON output formatting and empty-cell symbol inconsistencies required post-processing (json-repair and an LLM-based formatter).",
            "uuid": "e8564.3",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen2.5-VL-72B",
            "name_full": "Qwen2.5-VL-72B (open-source LVLM)",
            "brief_description": "Large open-source vision-language model (Qwen2.5-VL-72B) evaluated in VGRP-Bench; among open-source models it achieved the best perception performance but still lagged behind top closed-source models on full puzzle solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-VL-72B-Instruct",
            "model_description": "Open-source vision-language model (Qwen family) used in the experiments (listed as Qwen2.5-VL-72B-Instruct in model versions). Deployed locally when evaluated.",
            "model_size": "72B",
            "puzzle_name": "Multiple VGRP-Bench puzzles (perception and puzzle-solving evaluations)",
            "puzzle_type": "Grid-based visual reasoning puzzles (perception-heavy and multi-step reasoning)",
            "task_setup": "Vision + CoT (chain-of-thought) evaluation, same dataset splits and prompt/output format as other models; cell-level and step-level probes as well as full-solution tasks were performed.",
            "mechanisms_or_strategies": "Standard vision-language inference; no special post-training applied in this model's off-the-shelf evaluation.",
            "performance_metrics": "Among open-source models, Qwen2.5-72B achieved the highest perception accuracy reported in the paper's evaluations (exact percentages shown in main figures). Despite stronger perception, puzzle-solving rates remained substantially lower than ideal and below top closed-source models.",
            "evidence_of_spatial_reasoning": "Relatively better perception suggests the model can detect grid contents reliably compared to other open-source models, but the paper still documents failures in multi-step reasoning and state updates that limit robust spatial reasoning.",
            "comparisons": "Compared to Qwen-QVQ (preview), Qwen2.5-72B performs better; overall closed-source models (e.g., Claude, Gemini, GPT-4o) tend to outperform it.",
            "limitations_or_failure_cases": "Still prone to common LVLM failures: JSON output formatting issues, cell mislocation in some puzzles, inability to maintain board state across steps, and poor performance on medium/hard levels (e.g., Thermometers medium task had &lt;5% perception across models).",
            "uuid": "e8564.4",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen-QVQ (preview)",
            "name_full": "Qwen/QVQ-72B-Preview (preview reasoning LVLM)",
            "brief_description": "Preview reasoning LVLM variant of the Qwen family included in the paper; underperforms relative to Qwen2.5-72B and other reasoning-capable closed-source models on VGRP-Bench.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen/QVQ-72B-Preview",
            "model_description": "Preview release reasoning-capable LVLM (Qwen family) evaluated in the paper; listed among reasoning-model category but noted to be a preview with lower performance.",
            "model_size": "72B",
            "puzzle_name": "Multiple grid puzzles in VGRP-Bench",
            "puzzle_type": "Grid-based visual reasoning puzzles",
            "task_setup": "Vision + CoT evaluation similar to other models; evaluated on easy/medium/hard levels.",
            "mechanisms_or_strategies": "Reasoning model architecture but in preview state; evaluated as off-the-shelf.",
            "performance_metrics": "Underperforms compared to Qwen2.5-72B and some other reasoning models; specific numeric deficits shown in paper's aggregated plots (figures).",
            "evidence_of_spatial_reasoning": "Limited; the model did not demonstrate robustness in spatial rule-following or perception relative to competitors.",
            "comparisons": "Explicitly compared unfavorably to Qwen2.5-72B and some closed-source reasoning models (e.g., Gemini-2.0-Thinking).",
            "limitations_or_failure_cases": "Preview model status likely explains lower performance; suffers the same common LVLM failure patterns: mislocalization, failure to backtrack, repetitive generation, and output formatting issues.",
            "uuid": "e8564.5",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LlaVA-OneVision-7B",
            "name_full": "LlaVA-OneVision-7B (open-source LVLM)",
            "brief_description": "A smaller open-source LVLM (7B) evaluated in VGRP-Bench that exhibits relatively weak puzzle-solving and step-level rule-following performance compared to larger/open and closed-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llava-onevision-7B",
            "model_description": "Open-source 7B vision-language model (LlaVA family) used in the experiments; deployed locally via vLLM in evaluations.",
            "model_size": "7B",
            "puzzle_name": "Multiple grid-based puzzles (cell-level, step-level, full-solution tasks)",
            "puzzle_type": "Grid-based visual reasoning puzzles requiring spatial perception and constraint reasoning",
            "task_setup": "Vision + CoT prompts, same standardized JSON-style output target; evaluated across easy/medium/hard splits.",
            "mechanisms_or_strategies": "Standard vision-language inference; no post-training applied in off-the-shelf evaluation.",
            "performance_metrics": "Reported among the worst performers on step-level rule-following and lower perception/puzzle-solving accuracies relative to larger models and many closed-source models (figures in paper highlight LlaVA as poor performer in step-level evaluation).",
            "evidence_of_spatial_reasoning": "Weak: low cell- and step-level scores indicate limited ability to perform reliable spatial rule application or maintain board state.",
            "comparisons": "Underperforms compared to larger open-source models (e.g., Qwen2.5-72B) and closed-source models (Claude, Gemini, GPT-4o).",
            "limitations_or_failure_cases": "Exhibits common LVLM errors: repetitive outputs, format non-compliance, misinterpretation of complex visual components (e.g., Thermometers, Battle-Ships) and poor step-by-step reasoning.",
            "uuid": "e8564.6",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Aggregate Off-the-shelf LVLMs (VGRP-Bench)",
            "name_full": "Aggregate evaluation of off-the-shelf LVLMs on VGRP-Bench puzzles",
            "brief_description": "The paper's core evaluation: many state-of-the-art closed-source and open-source LVLMs were benchmarked across 20 grid-style visual puzzles (20 puzzle types, multiple difficulty levels) showing systematic weaknesses in spatial perception, rule adherence, and multi-step spatial reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple off-the-shelf LVLMs (e.g., GPT-4o, Gemini variants, Claude 3.5, Qwen2.5, Qwen-QVQ, Llama 3.2 variants, LlaVA)",
            "model_description": "Varied set of closed- and open-source vision-language models evaluated without additional post-training. Versions used and ids are provided in the paper's supplement (e.g., gemini-2.0-flash-thinking-exp-01-21, gpt-4o-2024-08-06, claude-3-5-sonnet-20241022, Qwen/QVQ-72B-Preview, Llama-3.2-11B-Vision-Instruct, etc.).",
            "model_size": null,
            "puzzle_name": "Full suite of VGRP-Bench puzzles (20 grid-like visual reasoning puzzles including Sudoku, Thermometers, Trees-and-Tents, Battle-Ships, Field-Explorer / Minesweeper, Binairo, Aquarium, Kakuro, etc.)",
            "puzzle_type": "Grid-based visual reasoning puzzles requiring 2D spatial perception (cell recognition), neighborhood/adjacency reasoning, matching/pairing, counting, arithmetic constraints, unidirectional/connected-component reasoning, and multi-step backtracking/search",
            "task_setup": "Standardized protocol: screenshot + textual prompt asking for perception (cell-level), rule-following (valid action), step-level chain-of-thought, and full-solution answers in JSON-like format. For separation of perception and reasoning, textual versions of puzzles were also provided. Evaluation ran 5 independent inference runs with 20 instances each (100 samples) per model and reported mean and standard deviation. Post-processing included json-repair and an LLM-based output formatter.",
            "mechanisms_or_strategies": "Primarily LVLM vision-language inference with optional Chain-of-Thought (CoT) prompting. For post-training research, the paper applied S-SFT and R-SFT to a Llama 3.2 base. The paper also explores RL (PPO/GRPO) for text-only training in the supplement.",
            "performance_metrics": "Aggregate result highlights: (1) All off-the-shelf LVLMs struggle on puzzles; puzzle-solving rates are often low (figures show many per-rule scores in the 0–45% range at easy level). (2) Perception: most closed-source models except Claude achieve &lt;50% accuracy on cell perception; Qwen2.5-72B leads among open-source. (3) Difficulty effect: medium/hard levels sharply decrease accuracy (e.g., Thermometers at medium had &lt;5% perception across all LVLMs). (4) Increasing number of clues improves puzzle-solving rates.",
            "evidence_of_spatial_reasoning": "Mixed/limited. The paper documents qualitative examples and ablations showing frequent grid-localization failures, misinterpretation of visual components (e.g., confusing cage sums with cell values in Killer Sudoku), inability to store/update previously taken actions, failure to backtrack, and repetitive generation. Text-only versions improve performance, indicating vision/perception contributes significantly to failures.",
            "comparisons": "Closed-source models generally outperform open-source ones; larger models tend to do better. Specific pairwise observations: GPT-4o &gt; GPT-4o-mini; Gemini-2.0-Thinking performs well among reasoning models; Qwen-QVQ underperforms relative to Qwen2.5-72B. Post-training S-SFT/R-SFT on Llama 3.2 significantly improves trained-puzzle performance but generalization to unseen puzzles is poor.",
            "limitations_or_failure_cases": "Systematic limitations observed: (1) perception failures (mislocalization, misunderstanding components), (2) reasoning failures (no state update, no backtracking, compounding errors), (3) format/output issues (broken JSON, wrong symbols for empty cells), (4) overfitting of SFT approaches and poor cross-puzzle generalization, and (5) computational limits preventing large-scale fine-tuning (experiments limited to 11B models).",
            "uuid": "e8564.7",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ING-VP: MLLMs cannot play easy vision-based games yet",
            "rating": 2
        },
        {
            "paper_title": "Balrog: Benchmarking agentic LLM and VLM reasoning on games",
            "rating": 2
        },
        {
            "paper_title": "Are large vision language models good game players?",
            "rating": 2
        },
        {
            "paper_title": "Blink: Multimodal large language models can see but not perceive",
            "rating": 2
        },
        {
            "paper_title": "EnigmaEval: A benchmark of long multimodal reasoning challenges",
            "rating": 1
        }
    ],
    "cost": 0.0187332,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models</h1>
<p>Yufan Ren ${ }^{1 *}$ Konstantinos Tertikas ${ }^{2}$ Shalini Maiti ${ }^{3,4}$ Junlin Han ${ }^{3,5}$<br>Tong Zhang ${ }^{1}$ Sabine Süsstrunk ${ }^{1}$ Filippos Kokkinos ${ }^{3}$<br>${ }^{1}$ School of Computer and Communication Sciences, EPFL<br>${ }^{2}$ National and Kapodistrian University of Athens<br>${ }^{3}$ Meta GenAI<br>${ }^{4}$ University College London<br>${ }^{5}$ University of Oxford</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Benchmark Overview. (a) We present a benchmark for Large Vision-Language Models (LVLMs) consisting of 20 diverse visual grid reasoning puzzles (see supplementary material for complete table of per-puzzle examples and descriptions). (b) We evaluate state-of-the-art LVLMs, including closed-source models such as GPT-4o [38] and Gemini [53], open-source models like Llama 3.2 [16], and recently released reasoning models such as Gemini-Thinking, on various aspects, including perception, overall puzzle-solving, and cell-level rule-following. Additionally, to explore potential approaches for improving LVLMs' puzzle-solving abilities, we examine posttraining techniques, including (c) Solution Supervised Fine-Tuning (S-SFT) and (d) Reasoning Supervised Fine-Tuning (R-SFT), where we train on thought trajectories of a predefined solver. (Best viewed on a screen when zoomed-in)</p>
<h1>Abstract</h1>
<p>Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to engage in structured reasoning - an essential skill for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles ${ }^{1}$. VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, highlighting fundamental limitations in their puzzlesolving capabilities. Most importantly, through systematic experiments, we identify and analyze key factors influencing LVLMs' puzzle-solving performance, including the number of clues, grid size, and rule complexity. Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. We will release VGRP-Bench to facilitate further research on LVLMs for complex, real-world problem-solving. Project page: https://yufan-ren.com/subpage/VGRPBench/.</p>
<h2>1. Introduction</h2>
<p>As Large Language Models (LLMs) advance rapidly [12, $21,46,50,55]$, researchers are extending their capabilities to multimodal tasks, leading to the rise of Large VisionLanguage Models (LVLMs) [5, 16, 36, 63, 69]. While LVLMs demonstrate success in some perception tasks, they often face challenges in strategic planning, especially in visual games that require a combination of perception and multi-step reasoning [39, 59, 66].</p>
<p>Among the visual games, grid-like reasoning puzzles, e.g., Sudoku, Futoshiki, and Thermometers, Fig. 1, are renowned for their simple rules yet challenging solutions. They have gained widespread popularity, even being featured in annual world championships [60]. Beyond entertainment, grid puzzles also serve as structured reasoning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1. VGRP-Bench offers a large puzzle collection for LVLM benchmarking, providing a comprehensive evaluation of state-of-the-art LVLMs across different dimensions, such as perception, rule adherence, and overall puzzle-solving, across different difficulty levels. We also investigate post-training strategies to enhance LVLMs' puzzle-solving performance.
tasks that require logical deduction, constraint satisfaction, and combinatorial search-skills that are fundamental to real-world problem-solving in domains such as robotic path planning [68], automated logistics scheduling [52], and embodied AI control [64]. Their well-defined rules and inherent complexity make them ideal for testing AI system's ability to process structured visual information and adhere to logical constraints. Nevertheless, despite their potential as benchmarks for visual reasoning, there are underused for evaluating LVLMs in existing research.</p>
<p>To address this gap, we introduce the Visual Grid Reasoning Puzzle Benchmark (VGRP-Bench), the largest visual puzzle benchmark to date in terms of puzzle variety and complexity, featuring 20 diverse customizable puzzles that emphasize grid-based visual reasoning and form a taxonomy of rules, attributes, and patterns (Fig. 3). We draw inspiration from popular reasoning puzzles [42-44], and design this benchmark with different levels of difficulty, easy ( ), medium ( ), and hard ( depending on the grid size, the required number of reasoning steps, and the size of the decision space. We conduct extensive experiments evaluating state-of-the-art LVLMs, including their reasoning counterparts, Fig. 5. With our benchmark, we assess several aspects of LVLMs including perception, rule adherence, and overall puzzle-solving capabilities. To separate reasoning and perception, we additionally provide a text version of all puzzles. Through evaluations, we observe that our benchmark poses a huge challenge for most LVLMs, even at the easy level. For instance, GPT-4o fails to solve a simple $4 \times 4$ Sudoku consistently, even in the text-only version of the game ( $&lt;30 \%$ solving rate). We summarize several common failure cases, such as the inability to localize a number on a grid and to correctly keep track of a reasoning process. Moreover, we investigate factors that might impact an LVLM's performance, such as the difficulty level, the grid size, the number of clues, and the rules involved in a puzzle.</p>
<p>Beyond benchmarking off-the-shelf models following other game benchmark papers, we investigate whether posttraining techniques can enhance LVLMs' puzzle-solving abilities (Tab. 1). Specifically, we explore two post-training</p>
<p>strategies, including Solution Supervised Fine-Tuning (SSFT) and Reasoning SFT (R-SFT). In S-SFT, we finetune LVLMs on final solutions, typically represented as nested lists indicating the board’s final state. In R-SFT, inspired by human and algorithmic approaches to puzzle solving [10, 13] such as step-by-step reasoning and process-of-elimination via rule-based deduction, we construct an SFT dataset by recording a solver’s stepwise reasoning trajectory. We then fine-tune the LVLM on this dataset. We observe significant improvement in puzzle solving at the easy level, while fine-tuned models still struggle at the medium and hard levels. Additionally, recognizing the risk of overfitting to the puzzles used for finetuning, we examine the generalization capabilities of models trained with each approach in our benchmark.</p>
<p>In summary, we present a novel, customizable LVLM benchmark tailored for visual reasoning puzzles and conduct a systematic evaluation of LVLMs, as shown in Tab. 1. Our key contributions are as follows:</p>
<ul>
<li>We introduce a large LVLM customizable grid-based reasoning benchmark with systematic evaluation protocols structured around a taxonomy of diverse visual clues and rules.</li>
<li>We conduct extensive experiments on state-of-the-art closed-source and open-source LVLMs using our benchmark, including fine-grained evaluations such as cell-level perception and step-wise rule understanding.</li>
<li>We summarize common failure cases of LVLMs in puzzle solving and provide detailed ablation studies on various factors that impact an LVLM’s puzzle solving, such as difficulty level, number of clues, and rules involved.</li>
<li>To gain deeper insights into the challenges faced by LVLMs in puzzle solving, we explore two post-training strategies: Solution SFT and Reasoning SFT.</li>
</ul>
<h2>2 Related Works</h2>
<h3>2.1 General LLM/LVLM Benchmarks</h3>
<p>The advanced capabilities of Large Language Models (LLMs) [1, 2, 53, 54] and Large Vision-Language Models (LVLMs) [30–32, 35] have inspired extensive research on benchmarking their capabilities. Prominent benchmarks like SuperGLUE [55], MMLU [21], and BigBench [50], evaluate general language understanding and multitasking text-based capabilities. Domain-specific benchmarks evaluate specialized competencies such as coding [3, 37] and mathematics [12, 22]. Notable early examples include Science QA [34], VizWiz [8], and VQAv2 [19]. Specific domains, such as image captioning, are represented by works such as [29]. More recent efforts [67], such as MMBench [33], EMMA [20], and SEED-Bench [27], offer comprehensive evaluations of multimodal reasoning and perception. BLINK [18] focuses on visual perception tasks</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Result Summary on Easy Level. Puzzle-solving rate of state-of-the-art chat LVLMs on easy-level puzzles associated with each rule. Please refer to the experiment section for detailed result analysis. Note that this plot’s score ranges from 0 to 45%, instead of 100%. (Best viewed on a screen when zoomed in)</p>
<p>that humans can solve in an instant. LMEvalKit [15] unifies model comparisons across various benchmarks.</p>
<p>Our VGRP-Bench differs from other benchmarks by focusing on reasoning puzzles, a special challenge to LVLMs that requires combining perception and decision making with multi-step reasoning.</p>
<h3>2.2 LLM/LVLM Game Benchmarks</h3>
<p>Challenging games have long been regarded as milestones of machine intelligence as exemplified by Deep Blue [24] and AlphaGo [49]. Classical benchmarks, such as Atari [48] and the Arcade Learning Environment [7], have played a crucial role in developing reinforcement learning algorithms and improving agent capabilities. Given the natural language capabilities of LLMs, researchers have introduced benchmarks where LLM agents interact within game environments [40, 61]. [9, 23, 45, 51, 57] investigate LLMs’ performance in agent-based and collaborative game environments, emphasizing interaction and teamwork skills.</p>
<p>Several recent studies benchmark LVLMs on visual games. ING-VP [66] shows that LVLMs still struggle with easy games. [59] proposes a benchmark with fine-grained evaluation. BALROG [39] measures LVLM games like MiniHack and NetHack. [17] proposed a puzzle RL environment, and benchmark several RL algorithms. ZeroBench [47] proposes a benchmark in which current LVLMs struggle to achieve meaningful accuracy. A concurrent work, [56], created a visual benchmark by scraping existing puzzles from online sources, resulting in a dataset of 949 instances of puzzles.</p>
<p>VGRP-Bench distinguishes itself by focusing on reasoning puzzles, employing customizable puzzle generators, and systematically evaluating models from inference to post-training techniques.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Benchmark Games: Primitives and Sample Questions. we systematically define puzzle primitives, including conditions, constraints, variables, and states, to establish a unified framework for inference and evaluation (left). This benchmark includes tasks designed to evaluate the reasoning, rule-following, and perception capabilities of state-of-the-art LVLMs. (Best viewed on a screen when zoomed in)</p>
<h2>3. VGRP-Bench: The Benchmark</h2>
<p>This section is organized as follows: we first present our benchmark in Sec. 3.1, along with its evaluation protocol in Sec. 3.2 and taxonomy in Sec. 3.3. In addition to benchmarking off-the-shelf models, we investigate the challenges faced by existing LVLMs in solving visual puzzles and propose strategies to address these limitations. Specifically, we use two fine-tuning strategies, Solution Supervised Fine-Tuning (S-SFT) and Reasoning SFT (R-SFT), as described in detail in Sec. 3.4.</p>
<h3>3.1. Grid-Like Visual Reasoning Puzzles</h3>
<p><strong>Puzzle Selection.</strong> To form this benchmark, we select visual puzzle games based on the following criteria: requiring multi-step reasoning for decision-making and rule validation, incorporating a diverse range of visual clues, rules and interaction methods, and ultimately contributing to a structured taxonomy (Fig. 4). For example, vanilla Sudoku is purely numerical and relies on repetition-based constraints, while Trees-and-Tents demands pattern recognition, relational reasoning between trees and tents, and checking 1-to-1 matching. In contrast, Thermometers relies heavily on understanding and applying physical-world rules, e.g., thermometers must be filled starting from their base.</p>
<p>Puzzle Primitives. To ensure consistency across different puzzles and facilitate future integration of new ones, we design the benchmark around four core primitives—variables, states, constraints, and conditions—to provide a unified structure, as depicted in Fig. 4 left. Variables <em>V</em> and States <em>S</em>. Each puzzle consists of a set of variables, <em>V</em> = {<em>v</em><sub><em>i</em></sub>}<em>v</em><sub><em>i</em>−1</sub>, representing cells or elements requiring value assignments. For example, a 4 × 4 <em>Sudoku</em> grid comprises 16 variables, with each variable taking a value from the set of possible values {1, 2, 3, 4}. The set of states <em>S</em> = {<em>s</em><sub><em>i</em></sub>}<em>v</em><sub><em>i</em>−1</sub> represents the current value assignments of the variables. Constraints. Constraints <em>C</em> = {<em>c</em><sub><em>j</em></sub>}<em>v</em><sub><em>j</em>−1</sub> define rules for valid puzzle state configurations. For instance, in <em>Sudoku</em>, constraints enforce the non-repetition of values in each row, column, and block. In <em>Trees and Tents</em>, constraints enforce a bijective mapping between trees and tents while adhering to row and column sums. Conditions. Conditions correspond to preset values or clues that define the puzzle's starting state. Examples include predefined digits that act as initial clues in <em>Sudoku</em> or row and column constraints given as clues in <em>Thermometers</em>.</p>
<h3>3.2. Evaluation Protocol</h3>
<p>Our benchmark evaluates LVLM performance across several capabilities, including perception, rule-following, and reasoning tasks at multiple granular levels, and on difficulty</p>
<p><sup>2</sup>Here, Sudoku serves as an example of puzzles that could be easily converted to text, owing to its widespread popularity, while Trees-and-</p>
<p>Tents and Thermometers represent puzzles harder to convert to text.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Diverse Rules and Visual Patterns in VGRP-Bench. Our benchmark includes a diverse set of rules, such as counting and mathematical calculations, and also exhibits diversity in visual patterns, encompassing text, numerical values, and objects such as trees. We highlight puzzles that are easy or difficult to convert into text.</p>
<p>levels, as illustrated in the right column of Fig. 4. Specifically, at the puzzle-solving level, we assess overall perception accuracy and puzzle-solving success rate by evaluating the LVLM's holistic understanding of the board and its ability to generate a correct solution. Moreover, we provide additional evaluations at finer levels of granularity, including evaluations at the cell and step level.</p>
<h3>3.3. Puzzle Rule/Capability Taxonomy</h3>
<p>We create a taxonomy of rule/capabilities required to solve the puzzles in our benchmark, and visualize the prominent ones in Fig. 4, as one puzzle might require multiple capabilities like counting, a basic rule in most puzzles. For example, Killer-Sudoku, Kakuro, Kakurasu, and Renzoku require mathematical calculations involving addition and subtraction. Trees-and-Tents, requiring the LVLM to understand bijective matching of trees and tents, is an example the matching rule of associating spatially or semantically relevant components. Other rules and capabilities are numerical comparison, understanding procedural order (unidirectionality) and putting connected components together.</p>
<h3>3.4. Post-Training Techniques</h3>
<p>Beyond assessing off-the-shelf LVLMs, we would like to take a step further to explore potential approaches to boost their performance. In this subsection, we utilize two post-training methods to tune a pretrained LVLM, i.e., Solution Supervised Fine-Tuning (S-SFT) and Reasoning Supervised Fine-Tuning (R-SFT).</p>
<p>S-SFT. A baseline is to use Supervised Fine-Tuning. Here, we adopt two strategies. First, we adopt a naive SFT for supervision of the LVLM to generate solutions. More specifically, we first convert the solution into a JSON-formatted text file, "{"answer": [[1, 2, 3, 4], [3, 4, 1, 2], [2, 1, 4, 3], [4, 3, 2, 1]]}". During training, we provide a text puzzle description as prompt and a screenshot of the puzzle as input. Then we use the predefined solution as supervision for the model.</p>
<p>R-SFT. We introduce a SFT data creation method specific for puzzle solving. Inspired by human and algorithmic puzzle solving that feature step-by-step reasoning and per-cell rule violation checking, we propose to conduct supervised Fine-Tuning (SFT) on synthetic trajectories. In this way, we would like to supervise LVLMs to imitate step-by-step reasoning, in a similar manner to how a predefined solver solves these puzzles. To generate thought trajectories, we define the reasoning process as a trajectory through states. A Trajectory, T = {s_i}_i=1, encodes key intermediate states encountered during puzzle solving. Each state s_t captures variable assignments and potential values for unassigned variables. To avoid the inefficiency of starting from a random cell, Depth-First Search (DFS) with process-of-elimination is employed, enabling systematic exploration and backtracking upon failure states. For instance, in a 4×4 Sudoku with 12 missing values, a random start often leads to excessive branching, producing trajectories that exceed the model's output window.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Off-the-Shelf LVLMs on Level-Easy with CoT. We report both correct perception rate and puzzle-solving rate evaluations with closed-source / open-source and reasoning / chat models. Please refer to supplementary for additional evaluations such as finer granularity evaluations and other difficulty levels, e.g., medium and hard (Puzzle-solving in hatched bars and best viewed on a screen when zoomed in)</p>
<h2>4. Experiments</h2>
<h3>4.1. Implementation Details</h3>
<p>We benchmark several state-of-the-art LVLMs. For accessibility purposes, we include both closed-source and open-source models like Gemini-Pro [53] and LlaVA-OneVision-7B [28] respectively. To assess different types of models, we include both chat LVLMs and reasoning LVLMs ${ }^{3}$. For</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>evaluation, we launch 5 independent inference runs, with each run containing 20 instances, resulting in a total of 100 samples. We report the overall mean correctness and standard deviation across all sample runs. For post-training, we use Llama 3.2 Vision Instruct as the base model and conduct training on a single node equipped with 8 A100 GPUs. We ensure that the training and test splits contain no overlapping puzzles in terms of input or solution. Please refer to supplementary for more implementation details.</p>
<h3>4.2 Off-the-Shelf LVLMs Evaluation</h3>
<p>We present the overall perception and puzzle-solving results in Fig. 5, where all LVLMs struggle with puzzlesolving, achieving a success rate below 80%. Additional granularity and evaluation results are discussed below, and the complete evaluation on all puzzles can be found in the supplementary material. More specifically, regarding perception, most closed-source models, except for Claude, achieve less than 50% accuracy. Among open-source models, Qwen2.5-72B performs the best. Hitori exhibits the highest perception accuracy among all puzzles, suggesting that LVLMs struggle with grids containing missing cells. Secondly, in terms of puzzle-solving, though all models struggle, closed-source models generally outperform open-source ones. We also observe that larger models tend to perform better; for example, GPT-4o outperforms GPT-4o-mini. For reasoning models, we find that Gemini-2.0-Thinking performs well, whereas Qwen-QVQ underperforms compared to Qwen2.5-72B, potentially because Qwen-QVQ is a preview version.</p>
<p>Cell-Level Evaluation. We provide cell-level perception evaluation in Fig. 6. Similar to overall perception, closed-source models—particularly Claude and Gemini 2.0-Flash—generally achieve the highest performance. Interestingly, we notice cases when querying the LVLM for the entire board yields the correct answer, whereas querying a specific cell results in an incorrect response. This phenomenon mirrors previously observed failures in LVLMs, such as their struggles with counting tasks like "How many R's are in the word Strawberry" [62].</p>
<p>Step-Level Rule-Following Evaluation. Claude consistently achieves the highest performance, whereas LlaVA performs the worst among all models. Among the four puzzles shown in Fig. 7, Sudoku attains the highest accuracy, aligning with the intuition that it is a widely recognized puzzle with relatively simple and well-defined rules compared to the others.</p>
<p>Text Puzzles Evaluation. To understand the reasoning challenges in the text domain, we present the results of off-the-shelf models using text input in Fig. 8. Notably, while this setting eliminates vision-related losses, the puzzles remain challenging for LVLMs.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Cell-level Perception Accuracy at Level-Easy (Best viewed on a screen when zoomed in)</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Step-Level Rule-Following Accuracy at Level-Easy (Best viewed on a screen when zoomed in)</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Performance of Text Version Puzzles on Level-Easy. For the text version of puzzles, the puzzle-solving rate increases significantly compared to the vision-based setting, highlighting the challenge of visual perception in our benchmark. (Best viewed on a screen when zoomed in)</p>
<p>Puzzle Taxonomy Analysis. The diversity of puzzles and rule types in our benchmark enables analysis through the lens of puzzle taxonomy, making it a key differentiator from other existing benchmarks. Each category includes at least two puzzles. For example, both Field-Explore and Trees-and-Tents require matching and pairing components. We</p>
<p>present results aggregated by puzzle taxonomy in Fig. 2.
Effect of Difficulty Level. As difficulty increases, reflected in larger grids and more steps required to complete the puzzle-accuracy declines in both perception and puzzlesolving (Fig. 10). Notably, at the medium difficulty level with Thermometers, all LVLMs achieve a perception accuracy below $5 \%$ and fail to solve the puzzles completely. Performance further deteriorates at the hard difficulty level, indicating significant limitations in handling complex puzzles.
Effect of Clue Number. Intuitively, providing more clues simplifies the puzzles, leading to improved performance. This trend is evident in Fig. 9, where we also observe a corresponding increase in perception accuracy.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Results with Different Number of Clues on LevelEasy (3. When more clues are provided (to the right), puzzles become easier, resulting in a higher puzzle-solving rate. (Best viewed on a screen when zoomed in)</p>
<p>Common Failure Patterns. Off-the-shelf chat models exhibit several common failure cases. For instance, chat LVLMs often struggle to localize values on a grid, misinterpreting sequences like [<em>, 2, </em>, ] as [, <em>, 2, </em>]. Additionally, they frequently misunderstand the roles of different components, such as mistaking a cage clue for a board number in Killer Sudoku, and they tend to repeat responses. Extensive sample outputs and common failure cases are provided in the supplementary material.</p>
<h3>4.3. Post-Training Evaluation</h3>
<p>We compare the pre-trained Llama 3.2 model with its finetuned versions after S-SFT and R-SFT in Fig. 11, with additional details provided in the supplementary material. First, we observe that both S-SFT and R-SFT significantly enhance performance, as the pre-trained model initially fails to produce any correct answers. This suggests that generalization to new puzzle settings is feasible. Comparing S-SFT and R-SFT, their effectiveness varies across puzzles: S-SFT outperforms R-SFT in some cases, whereas R-SFT excels in others such as Aquarium. We hypothesize that this is because R-SFT receives more supervision but is also more susceptible to compounding errors in long reasoning trajec-
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Off-the-Shelf LVLMs on Level-Medium (top row) and Hard (bottom row) with CoT. (Best viewed on a screen when zoomed in)
tories. We provide an evaluation on cross-puzzle generalization in the supplementary material.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Comparing S-SFT and R-SFT on Level-Easy Both S-SFT and R-SFT significantly improve the pretrained model's performance in perception and puzzle-solving, with RSFT achieves slightly better results in a few puzzles such as Binairo, while being lower in puzzles like Field-Explore. (Puzzlesolving in hatched and best viewed on a screen when zoomed in)</p>
<h2>5. Limitations and Future Work</h2>
<p>Due to the high computational cost of fine-tuning large models (e.g., 70B parameter models), our SFT experiments are limited to smaller 11B models. Future research could explore inference-time strategies, including Monte Carlo Tree Search [49]. Another promising direction is to enhance puzzle-solving performance by integrating RL with outcome-based reward models. We report preliminary findings in the supplementary material.</p>
<h2>6. Conclusion</h2>
<p>In this work, we have introduced VGRP-Bench, a large visual grid puzzle benchmark with various setting, including difficulty levels and diversified puzzle rules, and systematic evaluation. We evaluated off-the-shelf LVLMs on our VGRP-Bench showing their inability of puzzle solving. Furthermore, we explore post-training for improving LVLM performance, revealing significant improvement on the trained puzzle but also a lack of generalization to unseen ones. We hope this benchmark inspires future research and advances LVLM studies for complex, real-world tasks.</p>
<h2>References</h2>
<p>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3
[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. 3
[3] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. Multi-lingual evaluation of code generation models. 2022. 3
[4] Stefano Baccianella. JSON Repair - A python module to repair invalid JSON, commonly used to parse the output of LLMs, 2024. 3
[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2
[6] Evan Becker and Stefano Soatto. Cycles of thought: Measuring llm confidence through stable explanations. arXiv preprint arXiv:2406.03441, 2024. 4
[7] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013. 3
[8] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333-342, 2010. 3
[9] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023. 3
[10] Eric C Chi and Kenneth Lange. Techniques for solving sudoku puzzles. arXiv preprint arXiv:1203.2295, 2012. 3
[11] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. 5
[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 2, 3
[13] Carlos F Daganzo. Minuet: A method to solve sudoku puzzles by hand. arXiv preprint arXiv:1812.06778, 2018. 3
[14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 6
[15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACMMM, pages 11198-11201, 2024. 3
[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 2
[17] Benjamin Estermann, Luca Lanzendörfer, Yannick Niedermayr, and Roger Wattenhofer. Puzzles: A benchmark for neural algorithmic reasoning. NIPS, 37:127059-127098, 2025. 3
[18] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 3
[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, pages 6904-6913, 2017. 3
[20] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. 3
[21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 2, 3
[22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 3
[23] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. 3
[24] Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated the world chess champion. Princeton University Press, 2022. 3
[25] Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Transactions of the Association for Computational Linguistics, 12:1417-1440, 2024. 4
[26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 3
[27] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 3
[28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In $E C C V$, pages 740-755. Springer, 2014. 3
[30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 3
[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
[32] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3
[33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, pages 216-233. Springer, 2025. 3
[34] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NIPS, 35: 2507-2521, 2022. 3
[35] Meta AI Research. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Technical report, Meta AI, 2024. [Online; accessed 10-Jan-2025]. 3
[36] Cade Metz. Openai unveils new ai model with advanced math and science capabilities. The New York Times, 2024. 2
[37] Jain Naman, Han King, Gu Alex, Li Wen-Ding, Yan Fanjia, Zhang Tianjun, Wang Sida, Solar-Lezama Armando, Sen Koushik, and Stoica Ion. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint, 2024. 3
[38] OpenAI. Hello gpt-4o, 2024. Accessed: 2024-12-20. 1
[39] Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. 2, 3
[40] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 1-22, 2023. 3
[41] Patrick Pérez, Michel Gangnet, and Andrew Blake. Poisson image editing. TOG, 22(3):313-318, 2003. 3
[42] Puzzle Battleships. Battleships - online puzzle game. https://www.puzzle-battleships.com/. Accessed: 2025-01-17. 2
[43] Puzzlemix. Free puzzles to play online. https://www. puzzlemix.com/menu.php. Accessed: 2025-01-17.</p>
<p>[44] Puzzler Media. Online puzzles, brain teasers and games. https://www.puzzler.com/online-puzzles. Accessed: 2025-01-17. 2
[45] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6(3), 2023. 3
[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2
[47] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. 3
[48] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020. 3
[49] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. 3, 8
[50] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià GarrigaAlonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 2, 3
[51] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. 3
[52] Paul Mingzheng Tang, Kenji Kah Hoe Leong, Nowshad Shaik, and Hoong Chuin Lau. Automated conversion of static to dynamic scheduler via natural language. arXiv preprint arXiv:2405.06697, 2024. 2
[53] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 3, 6
[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3
[55] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for generalpurpose language understanding systems. NIPS, 32, 2019. 2,3
[56] Clinton J Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, and Dan Hendrycks. Enigmaeval: A benchmark of long multimodal reasoning challenges. arXiv preprint arXiv:2502.08859, 2025. 3
[57] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 3
[58] Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, and Ying Wei. Mitigating the language mismatch and repetition issues in llm-based machine translation via model editing. arXiv preprint arXiv:2410.07054, 2024. 4
[59] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? In The Thirteenth International Conference on Learning Representations, 2025. 2, 3
[60] Wikipedia contributors. World puzzle championship - Wikipedia, the free encyclopedia. https : / /en.wikipedia.org/wiki/World_Puzzle_ Championship, 2024. [Online; accessed 6-Dec-2024]. 2
[61] Yue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023. 3
[62] Nan Xu and Xuezhe Ma. Llm the genius paradox: A linguistic and math expert's struggle with simple word-based counting problems. arXiv preprint arXiv:2410.14166, 2024. 7
[63] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multimodal instruction-tuning dataset, framework, and benchmark. NIPS, 36, 2024. 2
[64] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. 2
[65] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In NIPS, 2024. 3
[66] Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, and Ge Zhang. Ing-vp: Mllms cannot play easy vision-based games yet. arXiv preprint arXiv:2410.06555, 2024. 2, 3
[67] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. arXiv preprint arXiv:2406.11775, 2024. 3
[68] Hongyou Zhou, Ingmar Schubert, Marc Toussaint, and Oegur S Oguz. Spatial reasoning via deep vision models for robotic sequential manipulation. In IROS, pages 1132811335. IEEE, 2023. 2
[69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2</p>
<h1>Contents</h1>
<ol>
<li>Introduction ..... 2</li>
<li>Related Works ..... 3
2.1. General LLM/LVLM Benchmarks ..... 3
2.2. LLM/LVLM Game Benchmarks ..... 3</li>
<li>VGRP-Bench: The Benchmark ..... 4
3.1. Grid-Like Visual Reasoning Puzzles ..... 4
3.2. Evaluation Protocol ..... 4
3.3. Puzzle Rule/Capability Taxonomy ..... 5
3.4. Post-Training Techniques ..... 5</li>
<li>Experiments ..... 6
4.1. Implementation Details ..... 6
4.2. Off-the-Shelf LVLMs Evaluation ..... 7
4.3. Post-Training Evaluation ..... 8</li>
<li>Limitations and Future Work ..... 8</li>
<li>Conclusion ..... 9</li>
<li>Versions of Models Used ..... 3</li>
<li>Additional Evaluation Details ..... 3</li>
<li>Additional Implementation details ..... 3
10Image Augmentations ..... 3
11Common Mistakes of LVLMs in Puzzle Solving ..... 4
11.1Common Mistakes in LVLMs' Perception ..... 4
11.2Common Mistakes in Pre-Trained LVLMs' Puzzle-Solving ..... 4
12Generalization Capability of SFT Models ..... 5
13Reinforcement Learning with Text Input ..... 5
Visualizations ..... 6
Per-Puzzle Examples of Easy, Medium, and Hard Levels ..... 6
Visualization: Per-Puzzle Examples and Query Templates ..... 9
Results ..... 13
Medium and Hard Level Overall Evaluation of Off-the-Shelf Models (w/ CoT) ..... 13
Easy, Medium and Hard Overall Level Evaluation of Off-the-Shelf Models (w/o CoT) ..... 15
Easy, Medium and Hard Level Cell-Level Perception Evaluation of Off-the-Shelf Models ..... 18
Easy, Medium and Hard Level Rule Following Evaluation of Off-the-Shelf Models ..... 21
Easy Level Overall Evaluation of Off-the-Shelf Models v.s. Clue Number (w/ CoT) ..... 24
Easy Level Cell-Level Perception Evaluation of Off-the-Shelf Models v.s. Clue Number ..... 25
Easy and Medium Level Overall Evaluation of Off-the-Shelf Models (w/ CoT, Text Version) ..... 26
Easy, Medium, and Hard Level Overall Evaluation of SFT Models ..... 28</li>
</ol>
<p>Qualitative Studies ..... 31
Sudoku - Easy - W/ CoT - Vision Input ..... 31
Sudoku - Easy - W/ CoT - Text Input ..... 43
Battle-Ships - Easy - W/ CoT - Vision Input ..... 53
Field-Explore - Medium - W/ CoT - Vision Input ..... 64
Llama 3.2 Instruction Vision after Reasoning-SFT (Successful Example) ..... 81
Llama 3.2 Instruction Vision after Reasoning-SFT (Failure Example) ..... 82
Reinforcement Learning Training Process with Text Input ..... 83</p>
<h1>7. Versions of Models Used</h1>
<p>Since off-the-shelf models may exist in multiple versions even under the same name, we provide the specific version numbers for the models used, where applicable, in the list below.</p>
<ul>
<li>Gemini 2.0 Flash Thinking: gemini-2.0-flash-thinking-exp-01-21</li>
<li>QVQ-72B: Qwen/QVQ-72B-Preview</li>
<li>Claude 3.5: claude-3-5-sonnet-20241022</li>
<li>Gemini 2.0 Flash: gemini-2.0-flash</li>
<li>Gemini 1.5 Pro: gemini-1.5-pro</li>
<li>Gemini 1.5 Flash: gemini-1.5-flash</li>
<li>GPT-4o: gpt-4o-2024-08-06</li>
<li>GPT-4o-mini: gpt-4o-mini-2024-07-18</li>
<li>Qwen2.5-VL-72B-it: Qwen/Qwen2.5-VL-72B-Instruct</li>
<li>Qwen2.5-VL-7B-it: Qwen/Qwen2.5-VL-7B-Instruct</li>
<li>Llama 3.2 90B it: Llama-3.2-90B-Vision-Instruct</li>
<li>Llama 3.2 11B it: Llama-3.2-11B-Vision-Instruct</li>
<li>Qwen2-VL-7B-it: Qwen/Qwen2-VL-7B-Instruct</li>
<li>Qwen2-VL-72B-it: Qwen/Qwen2-VL-72B-Instruct</li>
<li>llava-onevision-7B: llava-hf/llava-onevision-qwen2-7b-ov-hf</li>
<li>llava-mistral-7B: llava-hf/llava-v1.6-mistral-7b-hf</li>
</ul>
<h2>8. Additional Evaluation Details</h2>
<p>We observed that many off-the-shelf models-particularly open-source, small-scale ones such as Llama 3.2 11B—do not strictly adhere to the required output format. For example, their outputs frequently omit closing quotes or inconsistently alternate between single and double quotes, rendering them unsuitable for JSON parsing. Furthermore, these LVLMs often fail to follow instructions for representing empty cells. Instead of using the designated symbols (e.g., "0", "", ".", or ".") to denote an empty cell, they sometimes default to using "*". In the chain-of-thought (CoT) setting, the final answer may not appear at the end of the sentence, further complicating extraction via regular expressions. Since our primary focus is on assessing puzzle-solving capabilities, rather than outright rejecting non-compliant responses, we have implemented two targeted post-processing strategies to address these issues. First, we employ the json-repair package [4] to repair broken JSON outputs, addressing issues such as incorrect escape characters and inconsistent usage of single and double quotes. Second, we utilize an LLM—specifically GPT-4o—as an output formatter to standardize the outputs into a unified format.</p>
<h2>9. Additional Implementation details</h2>
<p>We provide each model's version id in Supp.7. For the closed-source models, we directly use their official API calls to query. For the open-source models, we deploy them locally using the vLLM framework[26] before querying. For a fair comparison, none of the closed-source LVLMs dynamically construct a solver to solve the puzzles. Instead, they rely solely on vision-language modeling capabilities to solve puzzles. For dataset creation, we compile a dataset of 100,000 puzzles per puzzle type and split it into training and validation sets, ensuring there are no duplicated conditions or solutions within or across splits. Note that we do not enforce unique solutions for the games, as doing so would significantly reduce the number of possible conditions and increase computational overhead during game generation. For both puzzle-solution and reasoning fine-tuning, we use LLaMa 3.2 Vision Instruct as the base model and employ one node with 8 A100 GPUs, using a batch size of 4 on each GPU. Training for 5 epochs with the llama-recipe code base and default fine-tuning settings for LVLMs takes approximately 24 hours. The training process follows a causal modeling paradigm, masking the input question to focus supervision solely on the answer predictions. Number of clues, considering that puzzles typically become more challenging with fewer clues. For reveal-based games, such as Sudoku, at the easy level we randomly select $25 \%$ to $75 \%$ of cells as hints. For the medium level, we randomly select $25 \%$ to $50 \%$ as clues. For the hard level, we randomly select $15 \%$ to $40 \%$ as clues.</p>
<h2>10. Image Augmentations</h2>
<p>In our framework, we strategically employ two light-weight augmentation techniques to enhance model generalization across diverse visual inputs: (1) randomized affine transforms for geometric variation simulation, and (2) Poisson blending [41] for seamless texture integration.This streamlined augmentation approach effectively bridges the domain gap between training</p>
<p>samples and real-world scenarios while preserving the pre-trained model's inherent visual understanding capabilities. As demonstrated in Table 2, our method achieves realistic image transformations with significantly fewer artifacts compared to conventional augmentation strategies.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Table 2. Examples of Image Augmentations</p>
<h1>11. Common Mistakes of LVLMs in Puzzle Solving</h1>
<h3>11.1. Common Mistakes in LVLMs' Perception</h3>
<p>Grid Layout Misunderstanding. As shown in Tab. 10, GPT-4o fails to accurately perceive the Sudoku board layout-for instance, it erroneously places a " 2 " in the first row where the cell should remain empty. Although LVLMs generally recognize numerical values, they often misplace them; for example, they might confuse a sequence intended as [<em>, 2, </em>, ] with [, <em>, 2, </em>]. Notably, performance improves as more hints are provided, as observed in Sudoku tasks, suggesting that LVLMs struggle to detect board gridlines, which are essential for deducing the correct board organization.
Misinterpretation of Component Roles. In visual puzzles, distinct components serve unique functions. For example, in Killer-Sudoku, the grid incorporates both cell numbers and cage sums. However, LVLMs frequently misinterpret cage sums as cell values, even when differences in font size and color are present.
Difficulty Understanding Complex Visual Components. In puzzles like Thermometers and Battle-Ships, we notice that LVLMs sometimes fail to comprehend complex visual components. For example, LVLMs may erroneously perceive an empty cell in Thermometers as filled, and they often struggle to discern the distinct roles of different ship segments in Battle-Ships. Rejection of answering. We noticed that sometimes a model responds with a message such as "I'm sorry, I can't view or process images directly. Could you please describe the puzzle to me in text form?" to avoid answering, even though it sometimes does provide an answer. We hypothesize that the criteria for providing or rejecting an answer may depend on the model's internal confidence level [6].</p>
<h3>11.2. Common Mistakes in Pre-Trained LVLMs' Puzzle-Solving</h3>
<p>Cell-by-Cell Solving without Prioritizing. LVLMs often solve puzzles in a strictly sequential manner, overlooking constraints that emerge in later steps and failing to exploit the fact that some cells, with fewer possible options, are easier to resolve. For example, as demonstrated by Gemini-2.0-Flash in Tab. 11, the LVLM does not begin with the final cell in the second row, which has only one option.
Failure to Store Previous Actions. LVLMs frequently fail to update their understanding of the puzzle state based on previous actions. An example is an LVLM places a number at one cell. However, when making later decisions, it just ignored the previous actions it take.
Inability to Detect Error and Backtrack [25]. As is shown in the GPT-4o-Mini example in text version sudoku in Tab. 11, the LVLM finally outputs [2,1,4,2] as the last row, where a clear violation of duplication of number " 2 " exist. However, GPT-4o-Mini replies it as an answer. Another example is Qwen2.5-72B and Qwen2.5-7B in the same table, we noticed that model outputs a possibly valid board but does not obey the initial condition.
Repetitive Generation. Repetitive output is a common issue [58], especially in the LLaVA and Qwen families of models. For example, as shown in Tab. 10, the model generates excessively repetitive sequences, such as repeating "perception" followed by more than 50 " $[*]$ " symbols. We also notice the repetitive generation issue frequently in the reasoning models QVQ, that it reason many steps until running out of the maximum context length, as shown in Tab. 10.</p>
<h1>12. Generalization Capability of SFT Models</h1>
<p>Here, we analyze how an LVLM with SFT on one puzzle generalizes to another. In Fig.12, we use four puzzles-Sudoku, Odd-Even Sudoku, Renzoku, and Aquarium-with increasingly different rules. Notably, despite the fact that the Llama model trained on different puzzles performs well on the same puzzle, their performance on other puzzles is significantly lower. When the rules are similar, e.g., Sudoku and Odd-Even Sudoku, the puzzle-solving rate under generalization remains high, while evaluating a Sudoku-trained model on Aquarium yields a zero success rate. We notice similar situation for both S-SFT and R-SFT. Some recent work has also discussed the generalization limitations of Supervised Fine-Tuning [11, 65].
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12. Generalization Evaluation of SFT Models on the Level Easy (). (Best viewed on a screen when zoomed in)</p>
<h2>13. Reinforcement Learning with Text Input</h2>
<p>PPO stabilizes training by incorporating a clipping mechanism into the policy ratio, which prevents excessive deviations from the previous policy, where $r_{t}=\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)}$ is the policy ratio.</p>
<p>$$
\mathbb{E}\left[\min \left(r_{t} \hat{A}<em t="t">{t}, \operatorname{clip}\left(r</em>\right)\right]
$$}, 1-\epsilon, 1+\epsilon\right) \hat{A}_{t</p>
<p>GRPO extends this framework by optimizing over groups of trajectories and regularizing updates with an explicit KL penalty, where $r_{i, t}$ compares token probabilities under the updated and old policies, conditioned on $q$ and prior outputs.</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}\left[\frac{1}{G} \sum_{\epsilon=1}^{G} \frac{1}{\left|o_{i}\right|} \sum_{\epsilon=1}^{\left|o_{i}\right|} \min \left(r_{i, t} \hat{A}<em i_="i," t="t">{i, t}, \operatorname{clip}\left(r</em>}, 1-\epsilon, 1+\epsilon\right) \hat{A<em _mathrm_KL="\mathrm{KL">{i, t}\right)\right] \
&amp; -\beta D</em>\right)
\end{aligned}
$$}}\left(\pi_{\theta} | \pi_{\mathrm{ref}</p>
<p>To ensure stable training, GRPO calculate the advantage by normalizing reward with all rollouts:</p>
<p>$$
\hat{A}<em i="i">{i, t}=\widetilde{r}</em>
$$}=\frac{r_{i}-\operatorname{mean}(\mathbf{r})}{\operatorname{std}(\mathbf{r})</p>
<p>The total reward consists of: success reward $r_{\text {succ }}$ for generating the correct solution, and format reward $r_{\text {fmt }}$ for producing a structured, extractable output, where $\lambda_{\text {succ }}$ and $\lambda_{\text {fmt }}$ balance the two components. We provide the training loss curve on page 83 .</p>
<p>$$
r_{i}=\lambda_{\text {succ }} r_{\text {succ }}+\lambda_{\text {fmt }} r_{\text {fmt }}
$$</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Table 3. Per-Puzzle Sample Screenshots of Level Easy</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Table 4. Per-Puzzle Sample Screenshots of Level Medium</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Table 5. Per-Puzzle Sample Screenshots of Level Hard ⑤. Note that some games do not have a hard level due to constraints imposed by the game rules. For example, Vanilla Sudoku's size can only be $4 \times 4$ or $9 \times 9$, and the next possible grid size is $16 \times 16$, which is too large.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Sample Screenshot</th>
<th style="text-align: center;">Description and Queries</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. Aquarium</td>
<td style="text-align: center;"><img alt="img-16.jpeg" src="img-16.jpeg" /></td>
<td style="text-align: center;">Rule: You are an Aquarium puzzle player. You need to fill the aquariums with water up to a certain level or leave it empty. The numbers on the sides indicate how many filled (water) cells must be in each row and column. Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;">2. Battle-Ships</td>
<td style="text-align: center;"><img alt="img-17.jpeg" src="img-17.jpeg" /></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ what is at the cell ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from {water, empty}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"><img alt="img-18.jpeg" src="img-18.jpeg" /></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell and * $</em>$ * indicates a filled cell) and your solution (where * $<em>$ * indicates a filled cell and * $</em>$ * indicates an empty cell) in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"><img alt="img-19.jpeg" src="img-19.jpeg" /></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ is it valid to assign the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with value {value}? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell and * $</em>$ * indicates a filled cell), your step-by-step reasoning, and your solution (where * $<em>$ * indicates a filled cell and * $</em>$ * indicates an empty cell) in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ think: {your step-by-step reasoning}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;">3. Binairo</td>
<td style="text-align: center;"><img alt="img-20.jpeg" src="img-20.jpeg" /></td>
<td style="text-align: center;">Rule: You are a Battle-Ships player. You need to place ships in a grid based on row and column hints. The hints indicate how many ship cells are in each row and column. The numbers of each size ship are given. Ships cannot touch each other, even diagonally. Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ Given the current game state, what is at position ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from: {ship, empty, unknown}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell) and your solution (where * $</em>$ * indicates a ship cell and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, answer: {solution as a 2D array} }</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ Given the current game state, is it valid to assign cell ( ${\mathrm{row}},{\mathrm{col}})$ with value {value}? Respond with: valid or invalid.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell), your step-by-step reasoning, and your solution (where * $</em>$ * indicates a ship cell and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, think: {your step-by-step reasoning}, answer: {solution as a 2D array} }</td>
</tr>
<tr>
<td style="text-align: center;">4. Colored-Sudoku</td>
<td style="text-align: center;"><img alt="img-21.jpeg" src="img-21.jpeg" /></td>
<td style="text-align: center;">Rule: You are a Binairo player. You have to fill a grid with white (w) and black (b) pieces. No more than two circles of the same color can be adjacent (horizontally and vertically). Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ what is the value of the cell at ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from ${\mathrm{b}, \mathrm{w}$, empty $}$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with ${\mathbf{b}, \mathbf{w}}$ ? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ think: {your step-by-step reasoning}, $\backslash n$ answer: {solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;">5. Field-Explorer</td>
<td style="text-align: center;"><img alt="img-22.jpeg" src="img-22.jpeg" /></td>
<td style="text-align: center;">Rule: You are a Colored-Sudoku player. You have to enter a numerical digit from 1 through N in each cell of a NxN grid, $\backslash n$ The rule is to make sure unique numbers in each row, column, and within cells of the same color. Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ what is the value of the cell at ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${\mathbf{R a l e}}$ is it valid to fill the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with value {value}? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ think: {your step-by-step reasoning}, $\backslash n$ answer: {solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule: You are a Field-Explore player. You need to identify mine locations in a grid based on revealed numbers. Each revealed number indicates how many mines are adjacent to that cell (including diagonals). Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ Given the current game state, what is in the cell at ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from {mine, number, hidden}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates hidden cells, "s" indicates a mine, and numbers represent revealed counts) and your solution (where * $</em>$ * indicates a mine and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, answer: {solution as a 2D array} }</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ Given the current game state, is it valid to assign the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with {value}? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates hidden cells, "s" indicates a mine, and numbers represent revealed counts), your step-by-step reasoning, and your solution (where * $</em>$ * indicates a mine and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, think: {your step-by-step reasoning}, answer: {solution as a 2D array} }</td>
</tr>
</tbody>
</table>
<p>Table 6. Per-Puzzle Sample Screenshots and Query Templates. Part 1. (1st - 5th) Note that ${\cdot}$ represents variable values that depend on each query and the specific game. Additionally, some quotation marks, e.g., " $\cdot$ ", are omitted in the query template for clarity. To clearly distinguish between 0 -indexing and 1 -indexing, we explicitly require indexing in the query. Action validity is assessed solely based on the presence of immediate rule violations without considering any long $\mathbf{0 r m}$ effects.</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Description and Queries
Rule: You are a Futoshiki player. You have to enter a numerical digit from 1 through N in each cell of an NxN grid. The rules are: unique numbers in each row and column; inequality signs between cells must be respected (for example, $&lt;$ means left number is smaller, $&gt;$ means left number is larger). Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${$ row $}, {\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}.
Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Hitori player. You need to shade some cells in the grid such that no number appears more than once in each row and column among unshaded cells. The rules are: shaded cells cannot be adjacent; all unshaded cells must be connected. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to shade the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${$ valid, invalid $}$.
Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot, your step-by-step reasoning, and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ think: ${$ your step-by-step reasoning $}$, $\backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Jigsaw-Sudoku player. You have to enter a numerical digit from 1 through N in each cell of a NxN grid. The rules are: unique numbers in each row, column, and within cells of the same region. Each region is a connected group of cells. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ Given the current game state in the screenshot, what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. { perception: {current state of the grid as a 2D array}, answer: {solution as a 2D array} }</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ Given the current game state in the screenshot, is it valid to fill the cell at ( ${$ row $}$, ${\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}
Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. { perception: {current state of the grid as a 2D array}, think: {your step-by-step reasoning}, answer: {solution as a 2D array} }</p>
<p>Rule: You are a Kakurasu puzzle player. You need to shade some cells in a grid where the sum of the weights of selected cells in each row and column matches the given clues. The weights increase from left to right (for rows) and top to bottom (for columns), starting from 1. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ ? Response in a formate of (number, number).
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to shade the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${$ valid, invalid $}$.
Perception and Chain-of-Thought reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Kakuro player. You have to fill in the grid with numbers ( 1 to N ) such that each row and column adds up to the specified sum. The rules are: (1) adjacent numbers should not be the same. (2) numbers add up to the given sum for each row and column. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array}, $\backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${$ row $}, {\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}
Perception and Chain-of-Thought reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array}, $\backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Kakuro player. You have to fill in the grid with numbers ( 1 to N ) such that each row and column adds up to the specified sum. The rules are: (1) adjacent numbers should not be the same. (2) numbers add up to the given sum for each row and column. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array}, $\backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${$ row $}, {\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}
Perception and Chain-of-Thought reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2D array}, $\backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</p>
<p>Table 7. Per-Puzzle Sample Screenshots and Query Templates. Part 2 (6th - 10th). Note that there is a special case that in selection-based games, e.g., Thermometers, where a set of cell is selected as the answer. A cell being empty could mean both undefined and deliberately leaving empty. To distinguish these cases, we typically use two notations, such as "*" for undefined cells and "e" for the deliberately empty cells in the query and SFT dataset creation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ In the reasoning model category, we include Gemini-2.0-Thinking and Qwen-QVQ, as other reasoning models are either lacking vision capabilities, e.g., DeepSeek [14], or only accessible to high-tier users. Due to the rate limit in Gemini-2.0-Thinking, we only evaluate puzzle-solving with&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
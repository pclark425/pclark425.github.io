<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1050 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1050</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1050</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-247245026</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2102.08370v2.pdf" target="_blank">Quantifying the effects of environment and population diversity in multi-agent reinforcement learning</a></p>
                <p><strong>Paper Abstract:</strong> Generalization is a major challenge for multi-agent reinforcement learning. How well does an agent perform when placed in novel environments and in interactions with new co-players? In this paper, we investigate and quantify the relationship between generalization and diversity in the multi-agent domain. Across the range of multi-agent environments considered here, procedurally generating training levels significantly improves agent performance on held-out levels. However, agent performance on the specific levels used in training sometimes declines as a result. To better understand the effects of co-player variation, our experiments introduce a new environment-agnostic measure of behavioral diversity. Results demonstrate that population size and intrinsic motivation are both effective methods of generating greater population diversity. In turn, training with a diverse set of co-players strengthens agent performance in some (but not all) cases.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1050.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1050.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HarvestPatch V-MPO agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>V-MPO trained agents in HarvestPatch environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated multi-agent embodied policies trained with on-policy V-MPO in a 35×35 gridworld social-dilemma (apple-harvest) environment; experiments vary environment-instance diversity (L) and population size (N) and measure rewards and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying the effects of environment and population diversity in multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>V-MPO agent (optionally SVO-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy learned with V-MPO (on-policy MPO variant), optionally augmented with Social Value Orientation (SVO) intrinsic-motivation; learned from egocentric visual observations using a ResNet + LSTM architecture; trained via distributed arenas until episodic rewards converge.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual/gridworld multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HarvestPatch</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>35×35 gridworld with apple patches; apples regrow dependent on local patch density and regrowth radius r (r ∈ [3,7]); social-dilemma: individual greedy harvesting yields immediate +1 reward per apple but depletes patches; players can tag others (tag-out mechanic) and see partial egocentric observations. Procedural generation varies number of patches p ∈ [1,14], patch radius, and apple density d ∈ [0.90,1.00].</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Grid size (35×35), number of apple patches p (1–14), patch radius r (3–7), apple density d (0.90–1.00), existence of tagging mechanic, partially observable egocentric view; complexity arises from social-dilemma dynamics and sparse/regrowth-dependent rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (35×35 grid, multi-agent social dilemma with sparse/temporal dependency via regrowth; variable by procedural params)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; held-out test set of 100 levels; also population size N ∈ {1,2,4,8} and intrinsic-motivation distributions (SVO identical/diverse/none).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varied from low (L=1) to high (L=10,000); population variation low to high via N up to 8 (EAV increases with N).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Individual episodic reward (sum of +1 per apple harvested); additionally expected action variation (EAV) for population diversity and generalization gap (train vs test reward difference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: population-size experiment (training-level reward, mean over runs): N=1: 191.6 (±32.8); N=2: 190.5 (±32.1); N=4: 186.7 (±28.4); N=8: 184.2 (±36.4). Environment-diversity experiment (generalization): numeric values reported in main tables but trend: test performance ↑ with L, training performance slightly ↓ with L; generalization gap approaches ~0 by L=1e3.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly analyzes interactions: increasing environment variation (L) improves test (held-out) performance and reduces the generalization gap, but often reduces training-set performance/variance; population variation (larger N, heterogeneous SVO) increases behavioral diversity (EAV) and in some environments strengthens performance (but not in HarvestPatch). The paper notes trade-offs (generalization vs training performance) and environment-dependent patterns (e.g., HarvestPatch shows high variance at L=1).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Procedural level generation (domain randomization over L), single-population or population play (N agents sampled for episodes), optional intrinsic-motivation (SVO) to diversify behavior; V-MPO optimization with PopArt normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Increasing L from 1 → 1e1 → 1e2 → 1e3 → 1e4 increased held-out test rewards and reduced the generalization gap; variance in training-set performance declines with increasing L; generalization effect significant (ANOVA reported). However, training-level performance can decline slightly as L increases.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained until episodic rewards converged; experiments run for multiple independent runs (10 runs for L experiments, 5 runs for N experiments); exact number of environment interactions/steps not reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Procedural environment diversity substantially improves held-out generalization and reduces the train-test gap; population size and heterogeneous intrinsic motivations increase behavioral diversity (measured by expected action variation), but increased population diversity did not significantly change individual rewards in HarvestPatch (i.e., behavior diversity does not always produce better task rewards in social-dilemma setting). Trade-off observed: higher environment variation can reduce training-set performance while improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying the effects of environment and population diversity in multi-agent reinforcement learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1050.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1050.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrafficNav V-MPO agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>V-MPO trained agents in Traffic Navigation environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated multi-agent agents trained with V-MPO in an 8-player coordination gridworld where agents navigate to goals while avoiding collisions; experiments vary procedural level diversity and population size and measure rewards and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying the effects of environment and population diversity in multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>V-MPO agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>On-policy V-MPO trained policies with visual egocentric inputs and goal-offset observation; trained in arenas with episodes sampled from procedurally generated levels; objective maximizes discounted extrinsic reward (+1 per goal reached, −1 per collision).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual/gridworld multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Traffic Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>8-player gridworld (level size 10×10 to 20×20) with blocking walls creating corridors; each player is assigned successive goal edge cells and earns +1 for reaching goals and −1 for collisions; procedural generation varies openness, wall counts and level size producing routable mazes/corridors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Level width/height (10–20 → 100–400 cells), number of walls, openness (proportion of non-wall cells), corridor complexity (path constraints), number of simultaneous players (n=8), collision dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (variable level sizes and corridor complexity; 8-agent coordination increases systemic complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; held-out test set of 100 levels; population size N ∈ {1,2,4,8} tested.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varied from low (L=1) to high (L=10,000).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Individual episodic reward (per-agent cumulative reward: +1 per reached goal, −1 per collision); generalization gap (train minus test reward).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>From environment-diversity experiments (means ± std): L=1 Train: 65.5 (±7.9), Test: 6.3 (±7.1), Gen gap: 59.2 (±8.3); L=1e1 Train:58.7 (±2.5), Test:28.4 (±3.6); L=1e2 Train:59.8 (±1.2), Test:51.4 (±2.5); L=1e3 Train:58.7 (±3.5), Test:58.5 (±1.5); L=1e4 Train:59.0 (±0.7), Test:58.9 (±0.9).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — environment variation (increasing L) strongly improves held-out test performance and reduces the generalization gap, with near-equal train/test performance by L≈1e3. Population diversity (N) increases behavioral EAV but population size did not significantly affect individual rewards in Traffic Navigation (no performance gain from larger populations).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Procedural generation of levels (domain randomization) across L; single-population training (population play with N varied); V-MPO with PopArt normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Test performance increases and generalization gap decreases monotonically with L; by L≥1e3 train and test rewards are similar (generalization gap ~0). Population size (N) increased EAV but did not translate to significant reward improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained until convergence; multiple independent runs (10 runs for L experiments). Exact environment-step counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Procedural environment diversity markedly improves generalization for coordination tasks; increasing environment diversity reduces variance and the train-test gap, with diminishing returns beyond ~1e3 unique levels. Population diversity (larger N) increases behavioral heterogeneity but does not necessarily improve task reward in Traffic Navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying the effects of environment and population diversity in multi-agent reinforcement learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1050.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1050.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Overcooked V-MPO agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>V-MPO trained agents in Overcooked environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-player common-payoff cooking task (tomato soup production) in procedurally generated kitchens; agents trained with V-MPO show strong dependency on both environment diversity and co-player diversity for held-out performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying the effects of environment and population diversity in multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>V-MPO agent (2-player)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>On-policy V-MPO agents with visual egocentric inputs and interact action semantics; trained to maximize shared extrinsic reward (+20 per delivered soup, +1 per tomato deposited); occasionally trained in populations to study co-player diversity effects.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual/gridworld cooperative multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Overcooked</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2-player kitchen-inspired gridworld with cooking pots, ingredient/dish stations, counters and delivery stations; tasks require multi-step cooperative sequences (deposit 3 tomatoes, wait 20 steps cooking, pick up soup with dish, deliver); procedural generation varies solution path length and openness (counter placement), producing kitchens of varying coordination difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Estimated solution path length (movement steps for produce & delivery), openness (proportion of non-counter cells), number and placement of pots/stations, required multi-step coordination (sequential dependency with 20-step cook time).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium–high (sequential multi-step tasks with coordination timing constraints and spatial constraints); varies with procedural parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; population size N ∈ {1,2,4,8}; held-out test set of 100 levels.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low to high via L values; population variation increases with N.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Individual reward per player (shared rewards assigned on delivery; reported as per-player episodic reward); generalization gap (train vs test).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Environment-diversity experiments (means ± std): L=1 Train: 359.7 (±103.3), Test: 0.4 (±0.3), Gen gap: 359.2 (±103.2); L=1e1 Train:345.5 (±36.7), Test:2.8 (±1.4); L=1e2 Train:351.8 (±24.4), Test:103.0 (±16.3); L=1e3 Train:296.6 (±23.3), Test:244.6 (±20.8); L=1e4 Train:284.4 (±13.4), Test:274.0 (±15.4). Population-size experiments: larger N increased behavioral diversity (EAV) and produced stronger agents in cross-play — significant improvement from N=1 to N=2, diminishing returns afterward.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — increasing environment variation (L) strongly increases held-out performance and reduces the generalization gap, but training-level performance can decrease as L increases; population variation (increasing N) increases behavioral diversity and improves held-out performance and cross-play success in Overcooked.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Procedural level generation across L, population play with varying N, V-MPO optimization with PopArt normalization; some experiments use heterogeneous SVO for behavioral diversification.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Held-out performance increases dramatically with L; very low test performance when L=1 (near-zero), rising to near training performance by L≈1e3; population diversity (N) beneficial — biggest improvement from single-agent self-play (N=1) to N=2, then diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained until episodic rewards converged; environment-diversity experiments run for 10 independent runs (per L), population experiments had more runs (20 for Overcooked population-size experiments); exact environment-step counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Procedural environment diversity is crucial for generalization in cooperative sequential tasks; training on many unique levels (L≥1e3) reduces the generalization gap and yields high held-out performance, whereas single-level training leads to brittle policies that fail on new kitchens. Population diversity (larger N and heterogeneous SVO) increases behavioral diversity and improves cross-play performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying the effects of environment and population diversity in multi-agent reinforcement learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1050.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1050.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CtF V-MPO agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>V-MPO trained agents in Capture the Flag environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-player competitive/team-based Capture the Flag (gridworld) implemented in DeepMind Lab2D; agents trained with V-MPO and evaluated across environment diversity (L) and population diversity (N) with Elo/win-rate metrics and EAV.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying the effects of environment and population diversity in multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>V-MPO agent (team-play)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>On-policy V-MPO agents with PopArt-normalized critic, LSTM memory and egocentric observations plus boolean flag-held signals; rewarded for captures, returns and tagging; trained in populations and evaluated via win rates and Elo ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual/gridworld competitive multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Capture the Flag (gridworld)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Gridworld environment with impassable walls partitioning rooms and corridors; two-team (red/blue), flags spawn in bases; players have health, tagging beam (cooldown), and respawn; procedural generation creates symmetric levels with varied crow/path distances, path complexity, openness and map size.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Map geometry (rooms/corridors), crow distance and path distance between flags, path complexity (crow/path ratio), openness (proportion non-wall), map size; team coordination and sparse capture rewards make learning complex.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high in some generated levels (large maps, sparse rewards, complex navigation and strategic play); variable across procedural parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; held-out test set of 100 levels; population sizes N ∈ {1,2,4,8,16}.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varied; L increases from 1 to 10k produce large variation; population N extended to 16 to examine effect on EAV.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Team win rate (paired 2v2 on a level) on training levels and held-out test levels; Elo rating computed over pairwise matchings (100 matches per pairing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Environment-diversity experiments (means ± std): L=1 Train: 213.6 (±165.4), Test:3.0 (±2.1), Gen gap: 210.6 (±164.0); L=1e1 Train:222.1 (±65.2), Test:45.0 (±20.3); L=1e2 Train:174.4 (±19.4), Test:128.8 (±20.3); L=1e3 Train:159.0 (±47.6), Test:147.7 (±23.3); L=1e4 Train:161.7 (±20.9), Test:150.5 (±21.5). Population-diversity: EAV by N: N=1:0.00, N=2:0.19 (±0.03), N=4:0.34 (±0.01), N=8:0.44 (±0.01), N=16:0.48 (±0.01). Larger N improved cross-play performance (big jump N=1→2, diminishing returns thereafter).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — increasing environment variation (L) consistently strengthens held-out performance (Elo ratings increase monotonically with L), but training-level performance can be highest at intermediate L (1e2) and decline above that; population diversity (increased N) increases behavioral diversity (EAV) and improves performance (win-rate/Elo) in Capture the Flag, with largest improvement from N=1 to N=2.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Procedural generation of symmetric maps with varied structural features; population play with N up to 16; V-MPO with PopArt and Elo-based evaluation for skill; some experiments use SVO for intrinsic motivation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Held-out performance (win-rate and Elo) increases monotonically with L; generalization gap shrinks with higher L and is small by L≈1e3. Training on a limited set (L=1) yields brittle strategies with high variance. Population diversity improves robustness and cross-play performance in this competitive domain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained until convergence; environment-diversity experiments run with nine independent runs for Capture the Flag (others used 10); exact environment-step counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Capture the Flag shows strong sensitivity to environment diversity: larger L yields much better held-out performance and higher Elo; population diversity (larger N) increases behavioral diversity and improves team performance, particularly shifting markedly from single-population self-play (N=1) to N=2. There is an environment-dependent trade-off where training-level skill may peak at intermediate L but held-out skill improves with greater L.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying the effects of environment and population diversity in multi-agent reinforcement learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-level performance in 3D multiplayer games with population-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Quantifying generalization in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Leveraging procedural generation to benchmark reinforcement learning <em>(Rating: 2)</em></li>
                <li>Paired open-ended trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions <em>(Rating: 2)</em></li>
                <li>Obstacle tower: A generalization challenge in vision, control, and planning <em>(Rating: 1)</em></li>
                <li>Open problems in cooperative AI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1050",
    "paper_id": "paper-247245026",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "HarvestPatch V-MPO agents",
            "name_full": "V-MPO trained agents in HarvestPatch environment",
            "brief_description": "Simulated multi-agent embodied policies trained with on-policy V-MPO in a 35×35 gridworld social-dilemma (apple-harvest) environment; experiments vary environment-instance diversity (L) and population size (N) and measure rewards and generalization.",
            "citation_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "V-MPO agent (optionally SVO-augmented)",
            "agent_description": "Policy learned with V-MPO (on-policy MPO variant), optionally augmented with Social Value Orientation (SVO) intrinsic-motivation; learned from egocentric visual observations using a ResNet + LSTM architecture; trained via distributed arenas until episodic rewards converge.",
            "agent_type": "simulated agent (virtual/gridworld multi-agent)",
            "environment_name": "HarvestPatch",
            "environment_description": "35×35 gridworld with apple patches; apples regrow dependent on local patch density and regrowth radius r (r ∈ [3,7]); social-dilemma: individual greedy harvesting yields immediate +1 reward per apple but depletes patches; players can tag others (tag-out mechanic) and see partial egocentric observations. Procedural generation varies number of patches p ∈ [1,14], patch radius, and apple density d ∈ [0.90,1.00].",
            "complexity_measure": "Grid size (35×35), number of apple patches p (1–14), patch radius r (3–7), apple density d (0.90–1.00), existence of tagging mechanic, partially observable egocentric view; complexity arises from social-dilemma dynamics and sparse/regrowth-dependent rewards.",
            "complexity_level": "medium (35×35 grid, multi-agent social dilemma with sparse/temporal dependency via regrowth; variable by procedural params)",
            "variation_measure": "Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; held-out test set of 100 levels; also population size N ∈ {1,2,4,8} and intrinsic-motivation distributions (SVO identical/diverse/none).",
            "variation_level": "varied from low (L=1) to high (L=10,000); population variation low to high via N up to 8 (EAV increases with N).",
            "performance_metric": "Individual episodic reward (sum of +1 per apple harvested); additionally expected action variation (EAV) for population diversity and generalization gap (train vs test reward difference).",
            "performance_value": "Example: population-size experiment (training-level reward, mean over runs): N=1: 191.6 (±32.8); N=2: 190.5 (±32.1); N=4: 186.7 (±28.4); N=8: 184.2 (±36.4). Environment-diversity experiment (generalization): numeric values reported in main tables but trend: test performance ↑ with L, training performance slightly ↓ with L; generalization gap approaches ~0 by L=1e3.",
            "complexity_variation_relationship": "Yes — the paper explicitly analyzes interactions: increasing environment variation (L) improves test (held-out) performance and reduces the generalization gap, but often reduces training-set performance/variance; population variation (larger N, heterogeneous SVO) increases behavioral diversity (EAV) and in some environments strengthens performance (but not in HarvestPatch). The paper notes trade-offs (generalization vs training performance) and environment-dependent patterns (e.g., HarvestPatch shows high variance at L=1).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Procedural level generation (domain randomization over L), single-population or population play (N agents sampled for episodes), optional intrinsic-motivation (SVO) to diversify behavior; V-MPO optimization with PopArt normalization.",
            "generalization_tested": true,
            "generalization_results": "Increasing L from 1 → 1e1 → 1e2 → 1e3 → 1e4 increased held-out test rewards and reduced the generalization gap; variance in training-set performance declines with increasing L; generalization effect significant (ANOVA reported). However, training-level performance can decline slightly as L increases.",
            "sample_efficiency": "Trained until episodic rewards converged; experiments run for multiple independent runs (10 runs for L experiments, 5 runs for N experiments); exact number of environment interactions/steps not reported in main text.",
            "key_findings": "Procedural environment diversity substantially improves held-out generalization and reduces the train-test gap; population size and heterogeneous intrinsic motivations increase behavioral diversity (measured by expected action variation), but increased population diversity did not significantly change individual rewards in HarvestPatch (i.e., behavior diversity does not always produce better task rewards in social-dilemma setting). Trade-off observed: higher environment variation can reduce training-set performance while improving generalization.",
            "uuid": "e1050.0",
            "source_info": {
                "paper_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "TrafficNav V-MPO agents",
            "name_full": "V-MPO trained agents in Traffic Navigation environment",
            "brief_description": "Simulated multi-agent agents trained with V-MPO in an 8-player coordination gridworld where agents navigate to goals while avoiding collisions; experiments vary procedural level diversity and population size and measure rewards and generalization.",
            "citation_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "V-MPO agent",
            "agent_description": "On-policy V-MPO trained policies with visual egocentric inputs and goal-offset observation; trained in arenas with episodes sampled from procedurally generated levels; objective maximizes discounted extrinsic reward (+1 per goal reached, −1 per collision).",
            "agent_type": "simulated agent (virtual/gridworld multi-agent)",
            "environment_name": "Traffic Navigation",
            "environment_description": "8-player gridworld (level size 10×10 to 20×20) with blocking walls creating corridors; each player is assigned successive goal edge cells and earns +1 for reaching goals and −1 for collisions; procedural generation varies openness, wall counts and level size producing routable mazes/corridors.",
            "complexity_measure": "Level width/height (10–20 → 100–400 cells), number of walls, openness (proportion of non-wall cells), corridor complexity (path constraints), number of simultaneous players (n=8), collision dynamics.",
            "complexity_level": "medium (variable level sizes and corridor complexity; 8-agent coordination increases systemic complexity).",
            "variation_measure": "Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; held-out test set of 100 levels; population size N ∈ {1,2,4,8} tested.",
            "variation_level": "varied from low (L=1) to high (L=10,000).",
            "performance_metric": "Individual episodic reward (per-agent cumulative reward: +1 per reached goal, −1 per collision); generalization gap (train minus test reward).",
            "performance_value": "From environment-diversity experiments (means ± std): L=1 Train: 65.5 (±7.9), Test: 6.3 (±7.1), Gen gap: 59.2 (±8.3); L=1e1 Train:58.7 (±2.5), Test:28.4 (±3.6); L=1e2 Train:59.8 (±1.2), Test:51.4 (±2.5); L=1e3 Train:58.7 (±3.5), Test:58.5 (±1.5); L=1e4 Train:59.0 (±0.7), Test:58.9 (±0.9).",
            "complexity_variation_relationship": "Yes — environment variation (increasing L) strongly improves held-out test performance and reduces the generalization gap, with near-equal train/test performance by L≈1e3. Population diversity (N) increases behavioral EAV but population size did not significantly affect individual rewards in Traffic Navigation (no performance gain from larger populations).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Procedural generation of levels (domain randomization) across L; single-population training (population play with N varied); V-MPO with PopArt normalization.",
            "generalization_tested": true,
            "generalization_results": "Test performance increases and generalization gap decreases monotonically with L; by L≥1e3 train and test rewards are similar (generalization gap ~0). Population size (N) increased EAV but did not translate to significant reward improvements.",
            "sample_efficiency": "Trained until convergence; multiple independent runs (10 runs for L experiments). Exact environment-step counts not provided.",
            "key_findings": "Procedural environment diversity markedly improves generalization for coordination tasks; increasing environment diversity reduces variance and the train-test gap, with diminishing returns beyond ~1e3 unique levels. Population diversity (larger N) increases behavioral heterogeneity but does not necessarily improve task reward in Traffic Navigation.",
            "uuid": "e1050.1",
            "source_info": {
                "paper_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Overcooked V-MPO agents",
            "name_full": "V-MPO trained agents in Overcooked environment",
            "brief_description": "Two-player common-payoff cooking task (tomato soup production) in procedurally generated kitchens; agents trained with V-MPO show strong dependency on both environment diversity and co-player diversity for held-out performance.",
            "citation_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "V-MPO agent (2-player)",
            "agent_description": "On-policy V-MPO agents with visual egocentric inputs and interact action semantics; trained to maximize shared extrinsic reward (+20 per delivered soup, +1 per tomato deposited); occasionally trained in populations to study co-player diversity effects.",
            "agent_type": "simulated agent (virtual/gridworld cooperative multi-agent)",
            "environment_name": "Overcooked",
            "environment_description": "2-player kitchen-inspired gridworld with cooking pots, ingredient/dish stations, counters and delivery stations; tasks require multi-step cooperative sequences (deposit 3 tomatoes, wait 20 steps cooking, pick up soup with dish, deliver); procedural generation varies solution path length and openness (counter placement), producing kitchens of varying coordination difficulty.",
            "complexity_measure": "Estimated solution path length (movement steps for produce & delivery), openness (proportion of non-counter cells), number and placement of pots/stations, required multi-step coordination (sequential dependency with 20-step cook time).",
            "complexity_level": "medium–high (sequential multi-step tasks with coordination timing constraints and spatial constraints); varies with procedural parameters.",
            "variation_measure": "Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; population size N ∈ {1,2,4,8}; held-out test set of 100 levels.",
            "variation_level": "low to high via L values; population variation increases with N.",
            "performance_metric": "Individual reward per player (shared rewards assigned on delivery; reported as per-player episodic reward); generalization gap (train vs test).",
            "performance_value": "Environment-diversity experiments (means ± std): L=1 Train: 359.7 (±103.3), Test: 0.4 (±0.3), Gen gap: 359.2 (±103.2); L=1e1 Train:345.5 (±36.7), Test:2.8 (±1.4); L=1e2 Train:351.8 (±24.4), Test:103.0 (±16.3); L=1e3 Train:296.6 (±23.3), Test:244.6 (±20.8); L=1e4 Train:284.4 (±13.4), Test:274.0 (±15.4). Population-size experiments: larger N increased behavioral diversity (EAV) and produced stronger agents in cross-play — significant improvement from N=1 to N=2, diminishing returns afterward.",
            "complexity_variation_relationship": "Yes — increasing environment variation (L) strongly increases held-out performance and reduces the generalization gap, but training-level performance can decrease as L increases; population variation (increasing N) increases behavioral diversity and improves held-out performance and cross-play success in Overcooked.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Procedural level generation across L, population play with varying N, V-MPO optimization with PopArt normalization; some experiments use heterogeneous SVO for behavioral diversification.",
            "generalization_tested": true,
            "generalization_results": "Held-out performance increases dramatically with L; very low test performance when L=1 (near-zero), rising to near training performance by L≈1e3; population diversity (N) beneficial — biggest improvement from single-agent self-play (N=1) to N=2, then diminishing returns.",
            "sample_efficiency": "Trained until episodic rewards converged; environment-diversity experiments run for 10 independent runs (per L), population experiments had more runs (20 for Overcooked population-size experiments); exact environment-step counts not reported.",
            "key_findings": "Procedural environment diversity is crucial for generalization in cooperative sequential tasks; training on many unique levels (L≥1e3) reduces the generalization gap and yields high held-out performance, whereas single-level training leads to brittle policies that fail on new kitchens. Population diversity (larger N and heterogeneous SVO) increases behavioral diversity and improves cross-play performance.",
            "uuid": "e1050.2",
            "source_info": {
                "paper_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "CtF V-MPO agents",
            "name_full": "V-MPO trained agents in Capture the Flag environment",
            "brief_description": "Four-player competitive/team-based Capture the Flag (gridworld) implemented in DeepMind Lab2D; agents trained with V-MPO and evaluated across environment diversity (L) and population diversity (N) with Elo/win-rate metrics and EAV.",
            "citation_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "V-MPO agent (team-play)",
            "agent_description": "On-policy V-MPO agents with PopArt-normalized critic, LSTM memory and egocentric observations plus boolean flag-held signals; rewarded for captures, returns and tagging; trained in populations and evaluated via win rates and Elo ratings.",
            "agent_type": "simulated agent (virtual/gridworld competitive multi-agent)",
            "environment_name": "Capture the Flag (gridworld)",
            "environment_description": "Gridworld environment with impassable walls partitioning rooms and corridors; two-team (red/blue), flags spawn in bases; players have health, tagging beam (cooldown), and respawn; procedural generation creates symmetric levels with varied crow/path distances, path complexity, openness and map size.",
            "complexity_measure": "Map geometry (rooms/corridors), crow distance and path distance between flags, path complexity (crow/path ratio), openness (proportion non-wall), map size; team coordination and sparse capture rewards make learning complex.",
            "complexity_level": "high in some generated levels (large maps, sparse rewards, complex navigation and strategic play); variable across procedural parameters.",
            "variation_measure": "Number of unique training levels L ∈ {1, 1e1, 1e2, 1e3, 1e4}; held-out test set of 100 levels; population sizes N ∈ {1,2,4,8,16}.",
            "variation_level": "varied; L increases from 1 to 10k produce large variation; population N extended to 16 to examine effect on EAV.",
            "performance_metric": "Team win rate (paired 2v2 on a level) on training levels and held-out test levels; Elo rating computed over pairwise matchings (100 matches per pairing).",
            "performance_value": "Environment-diversity experiments (means ± std): L=1 Train: 213.6 (±165.4), Test:3.0 (±2.1), Gen gap: 210.6 (±164.0); L=1e1 Train:222.1 (±65.2), Test:45.0 (±20.3); L=1e2 Train:174.4 (±19.4), Test:128.8 (±20.3); L=1e3 Train:159.0 (±47.6), Test:147.7 (±23.3); L=1e4 Train:161.7 (±20.9), Test:150.5 (±21.5). Population-diversity: EAV by N: N=1:0.00, N=2:0.19 (±0.03), N=4:0.34 (±0.01), N=8:0.44 (±0.01), N=16:0.48 (±0.01). Larger N improved cross-play performance (big jump N=1→2, diminishing returns thereafter).",
            "complexity_variation_relationship": "Yes — increasing environment variation (L) consistently strengthens held-out performance (Elo ratings increase monotonically with L), but training-level performance can be highest at intermediate L (1e2) and decline above that; population diversity (increased N) increases behavioral diversity (EAV) and improves performance (win-rate/Elo) in Capture the Flag, with largest improvement from N=1 to N=2.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Procedural generation of symmetric maps with varied structural features; population play with N up to 16; V-MPO with PopArt and Elo-based evaluation for skill; some experiments use SVO for intrinsic motivation baselines.",
            "generalization_tested": true,
            "generalization_results": "Held-out performance (win-rate and Elo) increases monotonically with L; generalization gap shrinks with higher L and is small by L≈1e3. Training on a limited set (L=1) yields brittle strategies with high variance. Population diversity improves robustness and cross-play performance in this competitive domain.",
            "sample_efficiency": "Trained until convergence; environment-diversity experiments run with nine independent runs for Capture the Flag (others used 10); exact environment-step counts not provided.",
            "key_findings": "Capture the Flag shows strong sensitivity to environment diversity: larger L yields much better held-out performance and higher Elo; population diversity (larger N) increases behavioral diversity and improves team performance, particularly shifting markedly from single-population self-play (N=1) to N=2. There is an environment-dependent trade-off where training-level skill may peak at intermediate L but held-out skill improves with greater L.",
            "uuid": "e1050.3",
            "source_info": {
                "paper_title": "Quantifying the effects of environment and population diversity in multi-agent reinforcement learning",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning",
            "rating": 2,
            "sanitized_title": "humanlevel_performance_in_3d_multiplayer_games_with_populationbased_reinforcement_learning"
        },
        {
            "paper_title": "Quantifying generalization in reinforcement learning",
            "rating": 2,
            "sanitized_title": "quantifying_generalization_in_reinforcement_learning"
        },
        {
            "paper_title": "Leveraging procedural generation to benchmark reinforcement learning",
            "rating": 2,
            "sanitized_title": "leveraging_procedural_generation_to_benchmark_reinforcement_learning"
        },
        {
            "paper_title": "Paired open-ended trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions",
            "rating": 2,
            "sanitized_title": "paired_openended_trailblazer_poet_endlessly_generating_increasingly_complex_and_diverse_learning_environments_and_their_solutions"
        },
        {
            "paper_title": "Obstacle tower: A generalization challenge in vision, control, and planning",
            "rating": 1,
            "sanitized_title": "obstacle_tower_a_generalization_challenge_in_vision_control_and_planning"
        },
        {
            "paper_title": "Open problems in cooperative AI",
            "rating": 1,
            "sanitized_title": "open_problems_in_cooperative_ai"
        }
    ],
    "cost": 0.01844775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Quantifying the effects of environment and population diversity in multi-agent reinforcement learning
4 Mar 2022</p>
<p>Kevin R Mckee kevinrmckee@deepmind.com 
Joel Z Leibo 
Charlie Beattie 
Richard Everett reverett@deepmind.com 
Quantifying the effects of environment and population diversity in multi-agent reinforcement learning
4 Mar 2022A6EA14174FD635CBF923D5CCF19A0FB4arXiv:2102.08370v2[cs.MA]Received: 30 Jul 2021 / Accepted: 05 Feb 2022Machine learningDeep reinforcement learningMulti-agentDiversity *equal contribution
Generalization is a major challenge for multiagent reinforcement learning.How well does an agent perform when placed in novel environments and in interactions with new co-players?In this paper, we investigate and quantify the relationship between generalization and diversity in the multi-agent domain.Across the range of multi-agent environments considered here, procedurally generating training levels significantly improves agent performance on held-out levels.However, agent performance on the specific levels used in training sometimes declines as a result.To better understand the effects of co-player variation, our experiments introduce a new environment-agnostic measure of behavioral diversity.Results demonstrate that population size and intrinsic motivation are both effective methods of generating greater population diversity.In turn, training with a diverse set of co-players strengthens agent performance in some (but not all) cases.</p>
<p>Introduction</p>
<p>An emerging theme in single-agent reinforcement learning research is the effect of environment diversity on learning and generalization [26,27,45].Reinforcement learning agents are typically trained and tested on a single level, which produces high performance and brittle generalization.Such overfitting stems from agents' capacity to memorize a mapping from environmental states observed in training to specific actions [48].Singleagent research has counteracted and alleviated overfitting by incorporating environment diversity into training.For example, procedural generation can be used to produce larger sets of training levels and thereby encourage policy generality [5,6].</p>
<p>In multi-agent settings, the tendency of agents to overfit to their co-players is another large challenge to generalization [31].Generalization performance tends to be more robust when agents train with a heterogeneous set of co-players.Prior studies have induced policy generality through population-based training [3,24], policy ensembles [35], the application of diverse leagues of game opponents [44], and the diversification of architectures or hyperparameters for the agents within the population [21,36].</p>
<p>Of course, the environment is still a major component of multi-agent reinforcement learning.In multiagent games, an agent's learning is shaped by both the other co-players and the environment [34].Despite this structure, only a handful of studies have explicitly assessed the effects of environmental variation on multiagent learning.Jaderberg et al. [24] developed agents for Capture the Flag that were capable of responding to a variety of opponents and match conditions.They argued that this generalizability was produced in part by the use of procedurally generated levels during training.Other multi-agent experiments using procedurally generated levels (e.g., [14,32]) stop short of rigorously measuring generalization.It thus remains an open question whether procedural generation of training levels benefits generalization in multi-agent learning.</p>
<p>Here we build from prior research and rigorously characterize the effects of environment and population diversity on multi-agent reinforcement learning.Specifi- cally, we use procedural generation and population play to investigate performance and generalization in four distinct multi-agent environments drawn from prior studies: HarvestPatch, Traffic Navigation, Overcooked, and Capture the Flag.These experiments make three contributions to multi-agent reinforcement learning research:</p>
<ol>
<li>Agents trained with greater environment diversity exhibit stronger generalization to new levels.However, in some environments and with certain coplayers, these improvements come at the expense of performance on an agent's training set.2. Expected action variation-a new, domain-agnostic metric introduced here-can be used to assess behavioral diversity in a population.3. Behavioral diversity tends to increase with population size, and in some (but not all) environments is associated with increases in performance and generalization.</li>
</ol>
<p>Environments</p>
<p>Markov games and multi-agent reinforcement learning</p>
<p>This paper aims to explore the influence of diversity on agent behavior and generalization in n-player Markov games [34].A partially observable Markov game M is played by n players within a finite set of states S. The game is parameterized by an observation function O : S × {1, . . ., n} → R d , sets of available actions for each player A 1 , . . ., A n , and a stochastic transition function T : S × A 1 × • • • × A n → ∆(S), mapping from joint actions at each state to the set of discrete probability distributions over states.Each player i independently experiences the game and receives its own observation o i = O(s, i).The observations of the n players in the game can be represented jointly as o = (o 1 , . . ., o n ).Following this notation, we can also refer to the vector of player actions a = (a 1 , . . ., a n ) ∈ A 1 , . . ., A n for convenience.Each agent i independently learns a behavior policy π(a i |o i ) based on its observation o i and its extrinsic reward r i (s, a).Agent i learns a policy which maximizes a long-term γ-discounted payoff defined as:
V πi (s 0 ) = E ∞ t=0 γ t U i (s t , o t , a t )| a t ∼ π t , s t+1 ∼ T (s t , a t ) (1)
where U i (s t , o t , a t ) is the utility function for agent i.In the absence of reward sharing [23] or instrinsic motivation [22,40], the utility function maps directly to the extrinsic reward provided by the environment.</p>
<p>A key source of diversity in Markov games is the environment itself.To this end, we train agents on distributions of environment levels produced by procedural generators.Our investigation explores four distinct environments drawn from prior studies: HarvestPatch (a mixed-motive game), Traffic Navigation (a coordination game), Overcooked (a common-payoff game), and Capture the Flag (a competitive game).The following subsections provide an overview of the game rules for each of these games.All environments were implemented with the open-source engine DeepMind Lab2D [2].Full details on the environments and the procedural generation methods are available in the Appendix A.</p>
<p>HarvestPatch</p>
<p>HarvestPatch [36] (Figure 1a) is a mixed-motive game, played by n = 6 players in the experiments here (see also [22,25,30,38]).</p>
<p>Players inhabit a gridworld environment containing harvestable apples.Players can harvest apples by moving over them, receiving a small reward for each apple collected (+1 reward).Apples regrow after being harvested at a rate determined by the number of unharvested apples within the regrowth radius r.An apple cannot regrow if there are no apples within its radius.This property induces a social dilemma for the players.The group as a whole will perform better if its members are abstentious in their apple consumption, but in the short term individuals can always do better by harvesting greedily.</p>
<p>Levels are arranged with patches of apples scattered throughout the environment in varying densities.Every step, players can either stand still, move around the level, or fire a short tag-out beam.If another player is hit by the beam, they are removed from play for a number of steps.They also observe a partial egocentric window of the environment.</p>
<p>Traffic Navigation</p>
<p>Traffic Navigation [33] (Figure 1b) is a coordination game, played by n = 8 players.</p>
<p>Players are placed at the edges of a gridworld environment and tasked with reaching specific goal locations within the environment.When a player reaches their goal, they receive a reward and a new goal location.If they collide with another player, they receive a negative reward.Consequently, each player's objective is to reach their goal locations as fast as possible while avoiding collisions with other players.</p>
<p>To make coordinated navigation more challenging, blocking walls are scattered throughout the environment, creating narrow paths which limit the number of players that can pass at a time.On each step of the game, players can either stand still or move around the level.Players observe both a small egocentric window of the environment and their relative offset to their current goal location.</p>
<p>Overcooked</p>
<p>Overcooked [3] (Figure 1c) is a common-payoff game, played in the experiments here by n = 2 players (see also [4,29,47]).</p>
<p>Players are placed in a kitchen-inspired gridworld environment and tasked with cooking as many dishes of tomato soup as possible.Cooking a dish is a sequential task: players must deposit three tomatoes into a cooking pot, let the tomatoes cook, remove the cooked soup with a dish, and then deliver the dish.Both players receive a reward upon the delivery of a plated dish.</p>
<p>Environment levels contain multiple cooking pots and stations.Players can coordinate their actions to maximize their shared reward.On each step, players can stand still, move around the level, or interact with the entity the object are facing (e.g., pick up tomato, place tomato onto counter, or deliver soup).Players observe a partial egocentric window of the level.</p>
<p>Capture the Flag</p>
<p>Capture the Flag (Figure 1d) is a competitive game.Jaderberg et al. [24] studied Capture the Flag using the Quake engine.Here, we implement a gridworld version of Capture the Flag played by n = 4 players.</p>
<p>Players are split into red and blue teams and compete to capture the opposing team's flag by strategically navigating, evading, and tagging members of the opposing team.The team that captures the greater number of flags by the end of the episode wins.</p>
<p>Walls partition environment levels into rooms and corridors, generating strategic spaces for players to navigate and exploit to gain an advantage over the other team.On each step, players can stand still, move around the level, or fire a tag-out beam.If another player is hit by the tag-out beam three times, they are removed from play for a set number of steps.Each player observes a partial egocentric window oriented in the direction they are facing, as well as whether each of the two flags is held by its opposing team.</p>
<p>Agents</p>
<p>We use a distributed, asynchronous framework for training, deploying a set of "arenas" to train each population of N reinforcement learning agents.Arenas run in parallel; each arena instantiates a copy of the environment, running one episode at a time.To begin an episode, an arena selects a population i of size N i with an associated set of L i training levels.The arena samples one level l from the population's training set and n agents from the population (with replacement).The episode lasts T steps, with the resulting trajectories used by the sampled agents to update their weights.Agents are trained until episodic rewards converge.After training ends, we run various evaluation experiments with agents sampled after the convergence point.</p>
<p>For the learning algorithm of our agents, we use V-MPO [41], an on-policy variant of Maximum a Posteriori Policy Optimization (MPO).In later experiments, we additionally endow these agents with the Social Value Orientation (SVO) component, encoding an intrinsic motivation to maintain certain group distributions of reward [36].These augmented agents act as a baseline for our behavioral diversity analysis, following suggestions from prior research that imposing variation in important hyperparameters can lead to greater population diversity.More details on the algorithm (including hyperparameters) are available in Appendix B.</p>
<p>Methods</p>
<p>Investigating Environment Diversity</p>
<p>To assess how environment diversity (i.e., the number of unique levels encountered during training) affects an agent's ability to generalize, we follow single-agent work on quantifying agent generalization in procedurally generated environments [5,6,48].</p>
<p>Specifically, we train multiple populations of N = 1 agents with different sets of training levels.We procedurally generate training levels in sets of size L ∈ {1, 1e1, 1e2, 1e3, 1e4}, where each training set is a subset of any larger sets.We also procedurally generate a separate test set containing 100 held-out levels.These held-out levels are not played by agents during training.For each training set of size L, we launch ten independent training runs and train each population until their rewards converge.</p>
<p>Generalization Gap Following prior work, we compare the performance of populations on the levels from their training set with their performance on the 100 heldout test levels.We focus on the size of the generalization gap, defined as the absolute difference between the population's performance on the test-set levels and training-set levels.</p>
<p>Cross-Play Evaluation</p>
<p>We also assess population performance through cross-play evaluation-that is, by evaluating agents in groups formed from two different training populations.We evaluate populations in cross-play both on the level(s) in their training set and on the held-out test levels.Specifically, for every pair of populations A and B, the training-level evaluation places agents sampled from population A (e.g., trained on L = 1 level) with agents sampled from population B (e.g., trained on L = 1e1 levels) in a level from the intersection of the populations' training sets.The heldout evaluation similarly samples agents from populations A and B, but uses a level not found in either of the populations' training sets.</p>
<p>As each environment requires a different number of players, we group agents from populations A and B as shown in Table 1.For HarvestPatch, Traffic Navigation, and Overcooked, we report the individual rewards achieved by the agents sampled from population A. For Capture the Flag, we analogously report the win rate for the agents from population A.  [10,18].Prior multi-agent projects have largely focused on two-player zero-sum games [1,37].In these environments, the diversity of a set of agent strategies can be directly estimated from the empirical payoff matrix, rather than behavioral trajectories.</p>
<p>Given the varied environments used in our experiments (and particularly the cooperative and competitive natures of their payoff structures), we draw inspiration from the former approach, focusing on heterogeneity in agent behavior.This paper uses the term "population diversity" to refer to variation in the set of potential co-player policies that a new individual joining a population might face [36].High policy diversity maximizes coverage over the set of all possible behaviors in an environment (including potentially suboptimal or useless behaviors) [15], while low policy diversity × 100 (a) To assess expected action variation, each agent in a population is prompted multiple times with a number of agent states.The probabilistic action outputs for each state are recorded.Here, an agent (Agent 1) is prompted 100 times with a state (State 1) from HarvestPatch.The process will be repeated for both other states and other agents in the population.consistently maps a given state to the same behavior, regardless of the agents involved.</p>
<p>In multi-agent research, the increasing prevalence of population play and population-based training make population size a commonly tuned feature of experimental design.Prior studies suggest that larger population sizes can increase policy diversity [8,39].We directly examine how varying population size affects diversity by training agents in populations of size N ∈ {1, 2, 4, 8} for each environment.For Capture the Flag experiments, we additionally train populations with N = 16.</p>
<p>Expected Action Variation Researchers should ideally be able to estimate population diversity in a task-agnostic manner, but in practice diversity is often evaluated using specific knowledge about the environment (e.g., unit composition in StarCraft II [44]).</p>
<p>To address this challenge, we introduce a new method of measuring behavioral diversity in a population that we call expected action variation (EAV; Algorithm 1 in Appendix C.1). Intuitively, this metric captures the probability that two agents sampled at random from the population will select different actions when provided the same state (Figure 2).At a high level, we compute expected action variation by simulating a number of rollouts for each policy in the population and calculating the total variational distance between the resulting action distributions.One of the key advantages of this metric is that it can be naïvely applied to a set of stochastic policies generated in any environment.</p>
<p>Expected action variation ranges in value from 0 to 1: a value of 0 indicates that the population is behaviorally homogeneous (all agents select the same action for any given agent state), whereas a value of 1 indi-cates that the population is maximally behaviorally diverse (all agents select different actions for any given agent state).An expected action variation of 0.5 indicates that if two agents are sampled at random from the population and provided a representative state, they are just as likely to select the same action as they are to select different actions.</p>
<p>This procedure is designed to help compare diversity across populations and to reason about the way a focal agent's experience of the game might change as a function of which co-players are encountered.Expected action variation is affected by stochasticity in policies, since such stochasticity can affect the state transitions that a focal agent experiences.Expected action variation is not intended to test whether the behavioral diversity of a population is significantly different from zero (or from 1), since such a difference could emerge for trivial reasons.</p>
<p>We leverage expected action variation to assess the effect of population size on behavioral diversity.We also include additional baselines to help explore the dynamics of co-player diversity.Specifically, we train several N = 4 populations parameterized with an intrinsic motivation module [40] on L = 1e3 levels.In particular, we use the SVO component to motivate agents in these populations to maintain target distributions of reward [36].Each population is parameterized with either a homogeneous or heterogeneous distribution of SVO targets (see Appendix B.2.3).</p>
<p>Cross-Play Evaluation</p>
<p>We employ a cross-play evaluation procedure to measure the performance resulting from varying population sizes, following Section 4.1.Specifically, we group agents sampled from populations A and B and then evaluate group performance on a level  We use the same grouping and reporting procedure as before.</p>
<p>Quantifying Performance</p>
<p>For the majority of our environments, we quantify and analyze the individual rewards earned by the agents.In Capture the Flag, we evaluate agents in team competition.Consequently, we record the result of each match from which we calculate win rates and skill ratings.To estimate each population's skill, we use the Elo rating system [13], an evaluation metric commonly used in games such as chess (see Appendix C.2 for details).</p>
<p>Statistical Analysis</p>
<p>In our experiments, we launch multiple independent training runs for each value of L and each value of N being investigated.Critically, we match the training sets of these independent runs across values of L and N .For example, the first run of the N = 1 HarvestPatch experiment trains on the exact same training set as the first runs of the N ∈ {2, 4, 8} experiments.Similarly, the second runs for each of the N = 1 to N = 8 experiments use the same training set, and so on.This allows us to avoid confounding the effects of N with those of L and vice versa.</p>
<p>For our statistical analyses, we primarily leverage the Analysis of Variance (ANOVA) method [16].The ANOVA allows us to test whether changing the value of an independent variable (e.g., environment diversity) significantly affects the value of a specified dependent variable (e.g., individual reward).Each ANOVA is summarized with an F -statistic and a p-value.In cases where we repeat ANOVAs for each environment, we apply a Holm-Bonferroni correction to control the probability of false positives [20].</p>
<p>Results</p>
<p>Environment Diversity</p>
<p>To begin, we assess how environment diversity (i.e., the number of unique levels used for training) affects generalization.Agents are trained on L ∈ {1, 1e1, 1e2, 1e3, 1e4} levels in populations of size N = 1.</p>
<p>Generalization Gap Analysis</p>
<p>As shown in Figure 3, for all environments generalization improves as the number of levels used for training increases.Performance on the test set increases as L increases, while performance on the training set tends to decrease with greater values of L (Figure 3, top row).</p>
<p>Performance on the training set experiences a minor decrease from low to high values of L. In contrast, Fig. 4: Top row: Cross-play evaluation of agent performance for each environment, using levels drawn from their training set.Bottom row: Cross-play evaluation of agent performance for each environment, using held-out test levels.Result: Environment diversity exerts strong effects on agent performance, though the exact pattern varies substantially across environments.the variance in training-set performance declines considerably as environment diversity increases.The variance in training-set performance is notably large for HarvestPatch and Capture the Flag when L = 1.This variability likely results from the wide distribution of possible rewards in the generated levels (e.g., due to varying apple density in HarvestPatch or map size in Capture the Flag).For Capture the Flag, the observed variance may also stem from the inherent difficulty of learning navigation behaviors on a singular large level where the rewards are sparse (i.e., without the availability of a natural curriculum).</p>
<p>To avoid ecological fallacy [17], we directly quantify and analyze the generalization gap with the procedure outlined in Section 4.1.As environment diversity increases, the generalization gap between the training and test sets decreases substantially (Figure 3, bottom row).The trend is sizeable, materializing even in the shift from L = 1 to L = 1e1.Across the environments considered here, the generalization gap approaches zero around values of L = 1e3.A set of ANOVAs confirm that L has a statistically significant effect on generalization in HarvestPatch, F (4, 45) = 4.8, p = 2.5 × 10 −3 , Traffic Navigation, F (4, 45) = 314.9,p = 1.1 × 10 −31 , Overcooked, F (4, 45) = 106.8,p = 6.9 × 10 −22 , and Capture the Flag, F (4, 40) = 12.8, p = 1.6 × 10 −6 (pvalues adjusted for multiple comparisons with a Holm-Bonferroni correction).</p>
<p>Cross-Play Evaluation</p>
<p>Next, we conduct cross-play evaluations of all populations following the procedure outlined in Section 4.1.We separately evaluate populations on the single level included in the training set for all populations (Figure 4, top row) and the held-out test levels (Figure 4, bottom row).</p>
<p>Overall, the effects of environment diversity vary substantially across environments.</p>
<p>HarvestPatch</p>
<p>We observe the highest level of performance for the agents playing with a group trained on a large number of levels (column L = 1e4) after themselves training on a small number of levels (row L = 1).In contrast, the worst-performing agents play with a group trained on a small number of levels (column L = 1) after themselves training on a large number of levels (row L = 1e4).These patterns emerge in both the training-level and test-level evaluations.</p>
<p>Traffic Navigation Agents perform equally well on their training level across all values for their training set size and for the training set size of the group's other members.In contrast, when evaluating agents on held-out test levels, an agent's performance strongly depends on how many levels the agent and its group were trained on.Average rewards increase monotonically from column L = 1 to column L = 1e4, and increase nearmonotonically from row L = 1 to row L = 1e4.Navigation appears more successful with increasing experience of various level layouts and with increasingly experienced groupmates.</p>
<p>Overcooked In the held-out evaluations, we observe a consistent improvement in rewards earned from the bottom left (L = 1 grouped with L = 1) to the top right (L = 1e4 grouped with L = 1e4).An agent benefits both from playing with a partner with diverse training and from itself training with environment diversity.</p>
<p>A different pattern emerges in the training-level evaluation.Team performance generally decreases when one of the two agents trains on just L = 1 levels.However, when both agents train on L = 1, they collaborate fairly effectively.The highest scores occur at the intermediate values L = 1e2 and L = 1e3, rather than at L = 1e4.Population skill on training levels declines with increasing environment diversity.</p>
<p>Capture the Flag Team performance is closely tied to environment diversity.A team's odds of winning are quite low when they train on a smaller level set than the opposing team, and the win rate tends to jump considerably as soon as a team's training set is larger than their opponents'.However, echoing the results in Overcooked, agents trained on an intermediate level-set size achieve the highest performance on training levels.Population skill actually decreases above L = 1e2 on these levels (Table 2, middle column).In contrast, in held-out evaluation, environment diversity consistently strengthens performance; Elo ratings monotonically increase as L increases (Table 2, right column).</p>
<p>Population Diversity</p>
<p>We next delve into the effects of population diversity on agent policies and performance.Agents are trained in populations of size N ∈ {1, 2, 4, 8} on L = 1 levels.In Capture the Flag, a set of additional populations are trained with size N = 16.</p>
<p>Expected Action Variation Analysis</p>
<p>We investigate the behavioral diversity of each population by calculating their expected action variation (see Section 4.2).As shown in Figure 5, population size positively associates with behavioral diversity among agents trained in each environment.A set of ANOVAs confirm that N has a statistically significant effect on expected action variation in HarvestPatch, F   Intrinsic Motivation and Behavioral Diversity Prior studies demonstrate that parameterizing an agent population with heterogeneous levels of intrinsic motivation can induce behavioral diversity, as measured through task-specific, hard-coded metrics [36].These agent populations benefited from the resulting diversity in social dilemma tasks, including HarvestPatch.We run an experiment to confirm that this behavioral diversity can be detected through the measurement of expected action variation.Following prior work on HarvestPatch, we endow several N = 4 populations with SVO, an in-trinsic motivation for maintaining a target distribution of reward among group members, and then train them on L = 1e3 levels.We parameterize these populations with either a homogeneous or heterogeneous distribution of SVO (see Appendix B.2.3 for details).As seen in Figure 6, populations with heterogeneous intrinsic motivation exhibit significantly greater behavioral diversity than populations without intrinsic motivation, p = 4.9 × 10 −4 .In contrast, behavioral diversity does not differ significantly between populations of agents lacking intrinsic motivation and those parameterized with homogeneous intrinsic motivation, p = 0.99.These results help baseline the diversity induced by increasing population size and demonstrate that expected action variation can be used to assess established sources of behavioral heterogeneity.</p>
<p>Cross-Play Evaluation</p>
<p>We next conduct a cross-play evaluation of all populations following the procedure outlined in Section 4.2.As before, we test whether observed patterns are statistically significant using a set of ANOVAs (with a Holm-Bonferroni correction to account for multiple comparisons).</p>
<p>Population diversity does not significantly affect agent rewards for HarvestPatch, F (3, 16) = 0.06, p = 1.0 (Figure 7a), and Traffic Navigation, F (3, 16) = 0.22, p = 1.0 (Figure 7b).Agents trained through self-play (N = 1) perform equivalently to agents from the largest, most diverse populations (N = 8).Training in a diverse population thus appears to neither advantage nor disadvantage agents in these environments.</p>
<p>In contrast, agents trained in diverse populations outperform those trained in lower-variation populations for Overcooked, F (3, 76) = 5.2, p = 7.7 × 10 −3 (Figure 7c) and Capture the Flag (Figure 7d).For both environments, we observe a substantial jump in performance from N = 1 to N = 2 and diminishing increases thereafter.The diminishing returns of diversity resemble the relationship between environment diversity and performance observed for Overcooked and Capture the Flag in Section 5.1.2.</p>
<p>Discussion</p>
<p>In summary, this paper makes several contributions to multi-agent reinforcement learning research.Our experiments extend single-agent findings on environment diversity and policy generalization to the multi-agent domain.We find that applying a small amount of environment diversity can lead to a substantial improvement in the generality of agents.However, this generalization reduces performance on agents' training set for certain environments and co-players.</p>
<p>The expected action variation metric demonstrates how population size and the diversification of agent hyperparameters can influence behavioral diversity.As with environmental diversity, we find that training with a diverse set of co-players strengthens agent performance in some (but not all) cases.</p>
<p>Expected action variation measures population diversity by estimating the heterogeneity in a population's policy distribution.As recognized by hierarchical and options-based frameworks [42], the mapping of lower-level actions to higher-level strategic outcomes is imperfect; in some states, different actions may lead to identical outcomes.Higher levels of expected action variation may capture greater strategic diversity.Nonetheless, future work could aim to directly measure variation in a population's strategy set.</p>
<p>These findings may prove useful for the expanding field of human-agent cooperation research.Human behavior is notoriously variable [7,12].Interindividual differences in behavior can be a major difficulty for agents intended to interact with humans [11].This variance thus presents a major challenge stymying the development of human-compatible reinforcement learning agents.Improving the generalizability of our agents could advance progress toward human compatibility, especially for cooperative domains [9].</p>
<p>Future work could seek to develop more sophisticated approaches for quantifying diversity.For example, here we use the "number of unique levels" metric as a proxy of environment diversity, and therefore increased L leads to monotonically increasing environment diversity.However, these levels may be unique in ways which are irrelevant to the agents.Scaling existing approaches to these settings, such as those that study how the environment influences agent behaviour [46], may help determine which features correspond to meaningful diversity.</p>
<p>The experiments presented here employ a rigorous statistical approach to test the consistency and significance of the effects in question.Consequently, they help scope the benefits of environment and population diversity for multi-agent reinforcement learning.Overall, we hope that these findings can improve the design of future multi-agent studies, leading to more generalized agents.</p>
<p>A Environments</p>
<p>A.1 HarvestPatch</p>
<p>A.1.1 Gameplay</p>
<p>Players are placed in a 35 × 35 gridworld environment containing a number of apples.Players can harvest apples by moving over them, receiving +1 reward for each apple collected.Apples regrow after being harvested.The rate of apple regrowth is determined by the number of unharvested apples within the regrowth radius r.An apple cannot regrow if there are no apples within its radius r or if a player is standing on its cell.This property induces a social dilemma for the players.The group as a whole will perform better if its members are abstentious in their apple consumption, but in the short term individuals can always do better by harvesting greedily.</p>
<p>In addition to basic movement actions, players can use a beam to tag out other players.Players are tagged out for 50 steps after being struck by another group member's tagging beam, and then are respawned at a random location within the environment.The ability to tag other players can be used to reduce the effective group size and mitigate the intensity of the social dilemma [38].There are no direct reward penalties for tagging or being tagged; any reward penalties experienced by the agents are indirectly imposed through opportunity costs.</p>
<p>Observations Players observe an egocentric view of the environment (Figure A1).</p>
<p>Actions Players can take one of the following eight actions each step: The use tag beam action has a cooldown of four steps for each player.If the player tries to use the tag action during this cooldown period, the outcome is equivalent to the no-op action.Players who are hit by the beam are tagged out for 50 steps, after which they are respawned in a random location.</p>
<p>Rewards Players receive +1 reward for each apple they harvest.(Players can harvest an edible apple by moving into its cell.)Players do not receive reward in any other way.Notably, neither using the use tag beam action nor being hit by the tagging beam yields any reward.</p>
<p>Apple regrowth probabilities In HarvestPatch, apples grow in "patches".Each apple in a patch is within r distance of the other apples in the patch, and further than r away from apples in all other patches.Based on the respawn rule described previously, on each step of an episode, harvested apples have a probability of regrowing based on the number of other non-harvested apples in their patch.These probabilities are as follows:</p>
<p>Crucially, when the patch has been depleted (i.e., the number of apples in patch is zero), the apples in that patch cannot regrow for the rest of the episode.A.4 Capture the Flag</p>
<p>A.4.1 Gameplay</p>
<p>Capture the Flag is played in a gridworld environment segmented by impassable walls and containing two bases.Players are divided into two teams and tasked with capturing the opposing team's flag while simultaneously defending their own flag.Flags spawn within team bases.Players can capture the opposing team's flag by first moving onto it (picking it up) and then returning it to their own base, while their own flag is there.Players observe an egocentric window around themselves.On each step, a player can move around the environment and fire a tag-out beam in the direction they are facing.At the beginning of an episode, players start with three units of health.A player's health is reduced by one each time it is tagged by a member of the opposing team.Upon reaching zero health, the player is tagged out for 20 steps.After 20 steps, the tagged-out player respawns at their base with three health.Players are rewarded for flag captures and returns, as well as for tagging opposing players.</p>
<p>Observations Players observe an egocentric view of the environment (see Figure A11), as well as a boolean value for each flag indicating whether it is being held by the opposing team.The use tag beam action has a cooldown of three steps for each player.If the player tries to use the action during this cooldown period, the outcome is equivalent to the no-op action.Players on the opposing team who are hit by the beam have their health points reduced by one and are tagged out when their health points are reduced to zero.Tagged players are respawned back at their teams' base with three units of health after 20 steps.</p>
<p>Rewards Players are rewarded following the Quake III Arena points system presented in Jaderberg et al. [24]: Table A2: Event-based rewards given to players in Capture the Flag, following [24].</p>
<p>While tagged out, a player receives all-black visual observations.Players can still receive reward from their teammate capturing the flag while tagged out.Algorithm 3 approximate policy dists Approximate action-policy distributions 1: Input: Set of populations of interest P = {P 0 , . . ., P k }, number of action samples R, state pool S 2: for all P ∈ P do 3:</p>
<p>for all A ∈ P do 4: for all (s, l) ∈ S do 5:</p>
<p>hist A,(s,l) ← 0 6:</p>
<p>for i = 1 : R do 7:</p>
<p>a ∼ π A (s, l) 8:</p>
<p>hist A,(s,l) (a) ← hist A,(s,l) (a) + 1 9:</p>
<p>end for 10:</p>
<p>policy dists A,(s,l) ← hist A,(s,l) /R 11:</p>
<p>end for 12:</p>
<p>end for 13: end for 14: Return: policy dists Algorithm 4 intra population variation Compute normalized total variation distance between all empirical action-probability distributions</p>
<p>C.2 Calculating Elo rating</p>
<p>To calculate the Elo rating of each trained population, we evaluate every possible pairing of trained populations against each another with 100 matches per pairing.For each of these 100 matches, two agents are randomly sampled from the first population to form the red team, and two from the second population to form the blue team (sampling with replacement).</p>
<p>After these matches are completed, we iteratively calculate the Elo rating of each population using the following procedure on each match result (looping over all match results until convergence):</p>
<p>Algorithm 5 Update Elo rating 1: Input: Step size of Elo rating update K, population i with Elo rating r i and match score s i , population j with Elo rating r j and match score s j 2: s ← (sign(s i , s j ) + 1)/2 3: s elo ← 1/(1 + 10 (r j −r i )/400 ) 4: r i ← r i + K(s − s elo ) 5: r j ← r j − K(s − s elo ) 6: return r i , r j where we initialised the rating of each population to 1000 and set K = 2.</p>
<p>D Additional Results</p>
<p>This section presents additional results and statistical analyses supporting the results in the main text.</p>
<p>For each ANOVA we conducted, we compare the categorical levels of the independent variable in pairs and apply Tukey's honestly significant differences (HSD) method to evaluate which pairs differ significantly [43].Tukey's method adjusts the raw p-values to account for the increased probability of false positives when running multiple independent statistical tests.</p>
<p>D.1 Environment Diversity</p>
<p>D.1.1 Generalization Gap Analysis</p>
<p>Fig. 1 :
1
Fig. 1: We investigate the influence of environment and population diversity on agent performance across four distinct n-player Markov games: (a) HarvestPatch (a six-player mixed-motive game), (b) Traffic Navigation (an eight-player coordination game), (c) Overcooked (a two-player common-payoff game), and (d) Capture the Flag (a four-player team-competition game).</p>
<p>The action outputs are then compared for each pair of agents in the population.Here we see an example set of action outputs from Agent 1 and another agent over three states.For a population comprising these two agents, the computed expected action variation is 0.68.</p>
<p>Fig. 2 :
2
Fig. 2: For each population, we calculate expected action variation (EAV), a new measure of behavioral diversity.The exact procedure for calculating this measure is detailed in the Appendix C.1.</p>
<p>(a) HarvestPatch.(b) Traffic Navigation.(c) Overcooked.(d) Capture the Flag.</p>
<p>Fig. 3 :
3
Fig. 3: Top row: Effect of training set size L on group performance on train vs. test levels for each environment.Error bands reflect 95% confidence intervals calculated over 10 independent runs (nine for Capture the Flag).Bottom row: Effect of training set size L on the generalization gap between training and test levels for each environment.Error bars correspond to 95% confidence intervals calculated over 10 independent runs (nine for Capture the Flag).Result: As environment diversity increases, test performance tends to improve.Training performance and the generalization gap experience concomitant decreases.</p>
<p>(a) HarvestPatch: Reward of one row player when grouped with five column players.(b) Traffic Navigation: Reward of one row player when grouped with seven column players.(c) Overcooked: Reward of one row player when paired with one column player.(d) Capture the Flag: Win rate of two row players versus two column players.</p>
<p>(3,16) = 367.7,p = 1.7 × 10 −14 , Traffic Navigation, F (3, 16) = 70.5, p = 1.9 × 10 −9 , Overcooked, F (3, 16) = 126.7,p = 4.6 ×</p>
<p>Fig. 5 :
5
Fig. 5: Effect of population size N on behavioral diversity, as measured by expected action variation.Error bars represent 95% confidence intervals calculated over five independent runs.Result: Increasing population size induces greater behavioral diversity.</p>
<p>Fig. 6 :
6
Fig.6: Effect of variation in intrinsic motivation on behavioral diversity on HarvestPatch.Error bands reflect 95% confidence intervals calculated over five independent runs.Result: Populations with a heterogeneous distribution of intrinsic motivation exhibit significantly greater behavioral diversity than populations with no intrinsic motivation or with a homogeneous distribution.</p>
<p>Fig. 7 :
7
Fig. 7: Effect of population size N on agent performance.Error bars indicate 95% confidence intervals calculated over five independent runs (20 for Overcooked).Result: Training population size has no influence on the rewards of agents for HarvestPatch and Traffic Navigation.For Overcooked and Capture the Flag, larger populations produced stronger agents.The increase in performance is especially salient moving from N = 1 to N = 2, with diminishing returns as N increases further.</p>
<p>1 . 3 . 4 . 5 . 6 . 7 . 8 .
1345678
No-op: The player stays in the same position.2. Move forward: Moves the player forwards one cell.Move backward: Moves the player backwards one cell.Move left: Moves the player left one cell.Move right: Moves the player right one cell.Turn left: Rotates the player 90 degrees anti-clockwise.Turn right: Rotates the player 90 degrees clockwise.Use tag beam: Fires a short yellow beam forwards from the player.The beam is three cells wide and is projected three cells forwards.</p>
<p>(a) Example observation.(b) Observation dimensions.(c) Beam structure.</p>
<p>Fig</p>
<p>Fig. A1: (a) Example observation for HarvestPatch.Players observe an 88 × 88 × 3 egocentric view of the environment (i.e., 11 × 11 cells with 8 × 8 × 3 sprites in each cell).(b) The observation window is offset from the player such that they can always see one cell behind them, five cells either side, and nine cells in front.(c) The beam is three cells wide and extends three cells forward from the player (until blocked by players or walls).</p>
<p>Fig. A2 :
A2
Fig. A2: Distribution over environmental features, alongside an example level at the minimum, median, and maximum of these distributions.</p>
<p>(a) Example observation.(b) Observation dimensions.(c) Beam structure.</p>
<p>Fig. A11 : 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 .
A1112345678
Fig. A11: (a) Example observation for Capture the Flag.Players observe an 88 × 88 × 3 egocentric view of the environment (i.e., 11 × 11 cells with 8 × 8 × 3 sprites in each cell).(b) The observation window is offset from the player such that they can always see one cell behind them, five cells either side, and nine cells in their facing direction.(c) The beam is one cell wide and extends infinitely forward from the player (until it hits either a wall or a player).</p>
<p>Fig. A12 :
A12
Fig.A12: Distribution over environmental features, alongside an example level at the minimum, median, and maximum of these distributions.</p>
<p>Fig. A13 :
A13
Fig. A13: Example levels procedurally generated for the Capture the Flag environment.</p>
<p>Fig. A14 :
A14
Fig. A14: Training curves for V-MPO on Capture the Flag with PopArt normalization enabled (True) and disabled (False).Results averaged over 25 independent runs (five runs per L ∈ {1, 1e1, 1e2, 1e3, 1e4}).Error bands reflect ±1 standard deviation.</p>
<p>Table 1 :
1
Number of agents sampled from populationsA and B for cross-play evaluation in each environment.
Population:EnvironmentABHarvestPatch15Traffic Navigation17Overcooked11Capture the Flag224.2 Investigating Population DiversityWe run a second set of experiments to investigate howpopulation diversity affects generalization. Measuringand optimizing for agent diversity are established chal-lenges in reinforcement learning research. In single-agentdomains, diversity is often estimated over behavioraltrajectories or state-action distributions</p>
<p>1 :
1
Input: Population of interest P , state pool S, action-probability distributions for agents policy dists 2: paired ← {} 3: TVD ← 0 4: for all A 1 ∈ P do 5:for all A 2 ∈ P do 6:ifA 1 = A 2 and ¬({{A 1 , A 2 }} ∩ paired) then
7:for all (s, l) ∈ S do8:tvd ←|policy dists A 1 ,(s,l) − policy dists A 2 ,(s,l) |9:norm tvd ← norm tvd + tvd/|S|10:end for11:paired ← paired ∪ {{A 1 , A 2 }}12:end if13:end for14: end for15: Return: norm tvd/|paired|</p>
<p>Table A5 :
A5
Full HarvestPatch results from Figure3a: Performance metrics for environment diversity experiments.Mean values (and standard deviations, reported in parentheses) are calculated over 10 independent runs.
LaL bMean difference Adjusted p-value11e141.35.0 × 10 −211e252.17.2 × 10 −311e351.28.6 × 10 −311e453.45.6 × 10 −31e1 1e210.70.951e1 1e39.80.961e1 1e412.00.921e2 1e3−0.91.001e2 1e41.31.001e3 1e42.21.00</p>
<p>Table A6 :
A6
Pairwise comparisons of generalization gaps between level set sizes L ∈ {1, 1e2, 1e3, 1e4}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with L a resulted in a higher generalization gap than training with L b .
Traffic NavigationIndividual reward on:Gen. gapLTrain levels Test levels (train − test)165.5 (7.9)6.3 (7.1)59.2 (8.3)1e158.7 (2.5)28.4 (3.6)30.3 (3.9)1e259.8 (1.2)51.4 (2.5)8.4 (2.3)1e358.7 (3.5)58.5 (1.5)0.2 (3.5)1e459.0 (0.7)58.9 (0.9)0.1 (0.6)</p>
<p>Table A7 :
A7
Full Traffic Navigation results from Figure3b: Performance metrics for environment diversity experiments.Mean values (and standard deviations, reported in parentheses) are calculated over 10 independent runs.
LaL bMean difference Adjusted p-value11e128.92.4 × 10 −1311e250.82.4 × 10 −1311e359.02.4 × 10 −1311e459.12.4 × 10 −131e1 1e221.96.2 × 10 −131e1 1e330.12.4 × 10 −131e1 1e430.22.4 × 10 −131e2 1e38.21.8 × 10 −31e2 1e48.31.5 × 10 −31e3 1e40.11.00</p>
<p>Table A8 :
A8
Pairwise comparisons of generalization gaps between level set sizes L ∈ {1, 1e2, 1e3, 1e4}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with L a resulted in a higher generalization gap than training with L b .
OvercookedIndividual reward on:Gen. gapLTrain levelsTest levels(train − test)1359.7 (103.3)0.4 (0.3)359.2 (103.2)1e1345.5 (36.7)2.8 (1.4)342.8 (36.1)1e2351.8 (24.4)103.0 (16.3)248.8 (16.4)1e3296.6 (23.3)244.6 (20.8)52.0 (9.6)1e4284.4 (13.4)274.0 (15.4)10.4 (9.3)</p>
<p>Table A9 :
A9
Full Overcooked results from Figure3c: Performance metrics for environment diversity experiments.Mean values (and standard deviations, reported in parentheses) are calculated over 10 independent runs.
LaL bMean difference Adjusted p-value11e116.50.9511e2110.41.0 × 10 −411e3307.22.4 × 10 −1311e4348.82.4 × 10 −131e1 1e294.01.1 × 10 −31e1 1e3290.82.4 × 10 −131e1 1e4332.42.4 × 10 −131e2 1e3196.82.1 × 10 −101e2 1e4238.48.6 × 10 −131e3 1e441.60.35</p>
<p>Table A10 :
A10
Pairwise comparisons of generalization gaps between level set sizes L ∈ {1, 1e2, 1e3, 1e4}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with L a resulted in a higher generalization gap than training with L b .
Capture the FlagIndividual reward on:Gen. gapLTrain levelsTest levels(train − test)1213.6 (165.4)3.0 (2.1)210.6 (164.0)1e1222.1 (65.2)45.0 (20.3)177.1 (59.2)1e2174.4 (19.4)128.8 (20.3)45.6 (10.3)1e3159.0 (47.6)147.7 (23.3)11.3 (38.1)1e4161.7 (20.9)150.5 (21.5)11.2 (5.5)</p>
<p>Table A11 :
A11
Full Capture the Flag results from Figure3d: Performance metrics for environment diversity experiments.Mean values (and standard deviations, reported in parentheses) are calculated over nine independent runs.
LaL bMean difference Adjusted p-value11e133.50.9011e2165.07.6 × 10 −411e3199.34.5 × 10 −511e4199.44.4 × 10 −51e1 1e2131.51.0 × 10 −21e1 1e3165.77.2 × 10 −41e1 1e4165.97.1 × 10 −41e2 1e334.30.891e2 1e434.40.891e3 1e40.21.0</p>
<p>Table A12 :
A12
Pairwise comparisons of generalization gaps between level set sizes L ∈ {1, 1e2, 1e3, 1e4}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with L a resulted in a higher generalization gap than training with L b .D.1.2Cross-PlayEvaluationFig. A15: Win matrices corresponding to the Elo ratings presented in Table2: Each win matrix contains the win rates of populations trained on the row value of L over those trained on the column value of L.
Capture the Flag(a) Win matrix on training level.(b) matrix on test set.D.2 Population DiversityD.2.1 Expected Action VariationAll environmentsExpected Action VariationNHarvestPatch Traffic Navigation Overcooked Capture the Flag10.00 (0.00)0.00 (0.00)0.00 (0.00)0.00 (0.00)20.20 (0.03)0.08 (0.03)0.15 (0.03)0.19 (0.03)40.36 (0.04)0.13 (0.02)0.36 (0.05)0.34 (0.01)80.49 (0.01)0.18 (0.01)0.54 (0.07)0.44 (0.01)16---0.48 (0.01)</p>
<p>Table A13 :
A13
Expected action variation for population diversity experiments in each environment.Mean values (and standard deviations, reported in parentheses) are calculated over five independent runs.
HarvestPatchNaN bMean difference Adjusted p-value12−0.205.4 × 10 −914−0.366.8 × 10 −1318−0.492.4 × 10 −1424−0.176.7 × 10 −828−0.291.5 × 10 −1148−0.132.8 × 10 −6</p>
<p>Table A14 :
A14
Pairwise comparisons of expected action variation between population sizes N ∈ {1, 2, 4, 8}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with N a resulted in higher behavioral diversity than training with N b .
Traffic NavigationNaN bMean difference Adjusted p-value12−0.081.1 × 10 −414−0.131.8 × 10 −718−0.181.3 × 10 −924−0.056.1 × 10 −328−0.103.1 × 10 −648−0.054.4 × 10 −3</p>
<p>Table A15 :
A15
Pairwise comparisons of expected action variation between population sizes N ∈ {1, 2, 4, 8}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with N a resulted in higher behavioral diversity than training with N b .
OvercookedNaN bMean difference Adjusted p-value12−0.155.1 × 10 −414−0.361.1 × 10 −818−0.542.1 × 10 −1124−0.201.9 × 10 −528−0.393.3 × 10 −948−0.187.3 × 10 −5</p>
<p>Table A16 :
A16
Pairwise comparisons of expected action variation between population sizes N ∈ {1, 2, 4, 8}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with N a resulted in higher behavioral diversity than training with N b .
Capture the FlagNaN bMean difference Adjusted p-value12−0.192.7 × 10 −1214−0.341.9 × 10 −1418−0.441.9 × 10 −14116−0.481.9 × 10 −1424−0.151.4 × 10 −1028−0.262.8 × 10 −14216−0.302.0 × 10 −1448−0.109.1 × 10 −8416−0.143.6 × 10 −10816−0.041.5 × 10 −2</p>
<p>Table A17 :
A17
Pairwise comparisons of expected action variation between population sizes N ∈ {1, 2, 4, 8, 16}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with N a resulted in higher behavioral diversity than training with N b .
Intrinsic Motivation and Behavioral DiversitySocial Value Orientation Expected Action VariationNone0.30 (0.02)Identical0.30 (0.03)Diverse0.37 (0.01)</p>
<p>Table A18 :
A18
Expected action variation for various distributions of SVO in HarvestPatch.Experiments are run with N = 4 and L = 1e3.Mean values (and standard deviations, reported in parentheses) are calculated over five independent runs.
SVOaSVO bMean difference Adjusted p-valueNoneIdentical0.000.99NoneDiverse−0.076.2 × 10 −4IdenticalDiverse−0.074.9 × 10 −4</p>
<p>Table A19 :
A19
Pairwise comparisons of expected action variation between different population distributions of SVO, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with SVO a resulted in higher behavioral diversity than training with SVO b .
D.2.2 Cross-Play EvaluationHarvestPatchIndividual reward on:NTraining level1191.6 (32.8)2190.5 (32.1)4186.7 (28.4)8184.2 (36.4)</p>
<p>Table A20 :
A20
Full results from Figure 7a: Performance metrics for population diversity experiments in HarvestPatch.Mean values (and standard deviations, reported in parentheses) are calculated over five independent runs.
NaN bMean difference Adjusted p-value121.11.00144.91.00187.40.98243.81.00286.30.99482.51.00</p>
<p>Table A21 :
A21
Pairwise comparisons of agent performance between population sizes N ∈ {1, 2, 4, 8}, calculated with Tukey's HSD method.Positive "Mean difference" values indicate that training with N a resulted in higher performance than training with N b .</p>
<p>. Place p points randomly, ensuring a distance of at least
× r p between all points. 3. Around each point, assign apples in radius r p with probability d per apple.
. Place 10 player spawn points in random non-assigned cells.Distribution over environmental featuresFigure A2 presents the distribution of apple counts in HarvestPatch levels, alongside visualizations of the levels at the minimum, median, and maximum of this distribution. Similarly, Figure A3 presents an example level for each of the various values of the patch radius parameter r.
AcknowledgementsWe thank Ian Gemp, Edgar Duéñez-Guzmán, and Thore Graepel for their support and feedback during the preparation of this manuscript.We are also indebted to Mary Cassin for designing and creating the sprite art for the DeepMind Lab2D implementation of Overcooked.Number ofRegrowth apples in patch probability 0 P = 0 1 P = 0.001 2 P = 0.005 3+ P = 0.025TableA1: Regrowth rates for apples in HarvestPatch.A.1.2 Additional detailsThe HarvestPatch environment instantiates an intertemporal social dilemma[22].A group can sustainably harvest from the environment by abstaining from fully depleting the apple patches.This strategy supplies the group with an indefinite stream of reward over time.However, for the group to reap the benefits of a sustainable harvesting strategy, every group member must abstain from depleting apple patches.In contrast, an individual is immediately and unilaterally guaranteed reward for eating an "endangered apple" (the last unharvested apple remaining in a given patch) if it acts greedily.Overall, there is a strong tension between the short-term individual temptation to maximize reward through unsustainable behavior and the long-term strategy of maximizing group welfare by acting sustainably.A.1.3 Procedural generationProcedure To generate levels for HarvestPatch, we use the following procedure given a number of patches p ∈[1,14]with a patch radius r p ∈[3,7]and density d ∈ [0.90, 1.00]:1. Generate a 35 × 35 area.A.2 Traffic NavigationA.2.1 GameplayEight players are placed along the edges of a gridworld environment.The goal for each player is to navigate through the level to their given "goal location" while avoiding collisions with other players.Goal locations are randomly sampled from the available edge cells in the game.More than one player can have the same goal at any one time.Upon a player reaching their given goal, they receive +1 reward and are assigned a new goal location.Levels created by the procedural generator can contain large open-spaces (making navigation and coordination easy), as well as small corridors (requiring additional coordination to avoid collisions with other players).While the number of players remains fixed (at eight), the size of the levels ranges from 10 × 10 (100 cells in total) to 20 × 20 (400 cells in total).For the smaller levels, while a players may chart a short path to their goal, the tight proximity of other players increases the likelihood of colliding into another player.In large levels, the goals are further apart, on expectation necessitating further travel for players to receive their reward.However, due to the larger size of the environment, the likelihood of players encountering one another is reduced.Observations Players observe an egocentric view of the environment (FigureA5) as well as their relative offset to the current goal location.Rewards Players receive +1 reward when they successfully occupy the same cell as their goal location.They receive −1 reward whenever they collide into another player (regardless of whether they or the other player caused the collision).A.2.2 Procedural generationProcedure To generate levels for Traffic Navigation, we use the following procedure:Distribution over environmental features To demonstrate the diversity of the procedurally generated levels for Traffic Navigation, this section introduces four descriptive features, presents their distributions from 100,000 samples, and visualizes the minimum, medium, and maximum level of each distribution.The two features are as follows:1. Openness: The proportion of non-wall cells in the level.Number of Walls:The number of walls in the level.A.3 OvercookedA.3.1 GameplayPlayers are placed in a gridworld environment containing cooking pots, dispensing stations (tomatoes or dishes), delivery stations, and empty counters.Players can move around and interact with these objects.By sequencing certain object interactions, players can pick up and deposit items.Each player (and each counter) can only hold one item at a time.The objective of each episode is for the players to deliver as many tomato soup dishes as possible through delivery stations.In order to create a tomato soup dish, players must pickup tomatoes and deposit them into the cooking pot.Once there are three tomatoes in the cooking pot, it begins to cook for 20 steps.After 20 steps, the soup is fully cooked.Cooking progress for a dish is tracked by a loading bar overlaying the cooking pot.The loading bar increments over the cooking time and then turns green when the dish is ready for collection.When the tomato soup is ready for collection, a player holding an empty dish can interact with the cooking pot to pick up the soup.The player can then deliver the soup by interacting with a delivery station while holding the completed dish.A successful delivery rewards both players and removes the dish from the game.Observations Players observe an egocentric view of the environment (FigureA8).The outcome of the Interact action depends on the current item held by the player (none, empty dish, tomato, or soup), as well as the type of object which they are facing (counter, cooking pot, tomato station, dish station, or delivery station).Depending on these two conditions, the player will either deposit the held item to the object or pick up an item from the object.Rewards Players receive a shared +20 reward for every soup they deliver through the delivery station.As a result, they are incentivized to create and deliver as many soups as possible within an episode.To scaffold learning, players also receive +1 reward each time they deposit a tomato into the cooking pot.A.3.2 Procedural generationProcedure To generate levels for Overcooked, we use the following procedure:Distribution over environmental features To demonstrate the diversity of the procedurally generated levels for Overcooked, this section introduces two descriptive features, presents their distributions from 100,000 samples, and visualizes the minimum, medium, and maximum level of each distribution.The two features are as follows:1. Solution's Estimated Path Length: The estimated number of required movement actions between the kitchen objects to create and delivery one tomato soup.2. Openness: The proportion of non-counter cells in the level.B AgentsB.1 Training and evaluationWe use a distributed asynchronous training framework consisting of "arenas" that run in parallel to train populations of reinforcement learning agents.In TablesA3 and Table A4B.2 V-MPOB.2.1 Architecture and hyperparametersWe used the following architecture for V-MPO[41].The agent's visual observations were first processed through a 3-section ResNet used in[19].Each section consisted of a convolution and 3 × 3 max-pooling operation (stride 2), followed by residual blocks of size 2 (i.e., a convolution followed by a ReLU nonlinearity, repeated twice, and a skip connection from the input residual block input to the output).The entire stack was passed through one more ReLU nonlinearity.All convolutions had a kernel size of 3 and a stride of 1.The number of channels in each section was(16,32,32).The resulting output was then concatenated with the previous action and reward of the agent (along with any extra observations in Capture the Flag and Traffic Navigation), and processed by a single-layer MLP with 256 hidden units.This was followed by a single-layer LSTM with 256 hidden units (unrolled for 100 steps), and then a separate single-layer MLP with 256 hidden units to produce the action distribution.For the critic, we used a single-layer MLP with 256 hidden units followed by PopArt normalization (ablated in Section B.2.2).To train the agent, we used a discount factor of 0.99, batch sizes of 16, and the Adam optimizer ([28]; learning rate of 0.0001).We configured V-MPO with a target network update period of 100, k = 0.5, and an epsilon temperature of 0.1.For PopArt normalization, we used a scale lower bound of 1e−2, an upper bound of 1e6, and a learning rate of 1e−3.B.2.2 PopArt normalizationPopArt normalization is typically used in multi-task settings[19].While we did not investigate multi-task settings in this work, we found that including PopArt (as in[41]) led to improved performance on the procedurally generated environments considered here.In FigureA14, we show the reward curves across training on both the training and test levels with PopArt normalization enabled (PopArt = True) and disabled (PopArt = False).As can be seen, PopArt consistently leads to a higher reward and therefore more performant agents.Consequently, we used PopArt for all agents.B.2.3 Social Value OrientationFor the "identical" populations of Social Value Orentation agents, we set the target reward angle θ hyperparameter with an identical distribution across agents: θ ∈ {45C MethodsC.1 Expected action variationFor reproducibility, we include pseudocode to calculate expected action variation (Algorithm 1).Starting from a set of k populations of interest trained on a shared set of levels L, we simulate a number of episodes within each population to generate a representative pool of agent states (Algorithm 2).We then approximate action-policy distributions over these states for each agent in each population (Algorithm 3).Finally, for each population, we compare the action-policy distributions between each pair of agents within the population to calculate that population's expected action variation (Algorithm 4).Because the environments that we consider in this paper use discrete action spaces, Algorithm 4 uses total variation distance to estimate pairwise divergence between action-policy distributions.For environments with continuous action spaces, pairwise divergence could be estimated with earth mover's distance instead.Algorithm 1 Calculate expected action variation for multiple populations
Open-ended learning in symmetric zero-sum games. D Balduzzi, M Garnelo, Y Bachrach, W Czarnecki, J Perolat, M Jaderberg, T Graepel, International Conference on Machine Learning. PMLR2019</p>
<p>. C Beattie, T Köppe, E A Duéñez-Guzmán, J Z Leibo, arXiv:2011.070272020DeepMind Lab2DarXiv preprint</p>
<p>On the utility of learning about humans for human-AI coordination. M Carroll, R Shah, M K Ho, T Griffiths, S Seshia, P Abbeel, A Dragan, Advances in Neural Information Processing Systems. 2019</p>
<p>Investigating partner diversification methods in cooperative multi-agent deep reinforcement learning. R Charakorn, P Manoonpong, N Dilokthanakul, International Conference on Neural Information Processing. Springer2020</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, arXiv:1912.015882019arXiv preprint</p>
<p>Quantifying generalization in reinforcement learning. K Cobbe, O Klimov, C Hesse, T Kim, J Schulman, International Conference on Machine Learning. 2019</p>
<p>The two disciplines of scientific psychology. L J Cronbach, American Psychologist. 12116711957</p>
<p>Real world games look like spinning tops. W M Czarnecki, G Gidel, B Tracey, K Tuyls, S Omidshafiei, D Balduzzi, M Jaderberg, Advances in Neural Information Processing Systems. 332020</p>
<p>A Dafoe, E Hughes, Y Bachrach, T Collins, K R Mckee, J Z Leibo, K Larson, T Graepel, arXiv:2012.08630Open problems in cooperative AI. 2020arXiv preprint</p>
<p>Diversityaugmented intrinsic motivation for deep reinforcement learning. T Dai, Y Du, M Fang, A A Bharath, Neurocomputing. 4682022</p>
<p>Individual differences in human-computer interaction. D E Egan, Handbook of Human-Computer Interaction. Elsevier1988</p>
<p>Intraindividual variability in affect: Reliability, validity, and personality correlates. M Eid, E Diener, Journal of Personality and Social Psychology. 7646621999</p>
<p>The Rating of Chessplayers, Past and Present. A E Elo, 1978Arco Publishing</p>
<p>Optimising worlds to evaluate and influence reinforcement learning agents. R Everett, A Cobb, A Markham, S Roberts, Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems. the 18th International Conference on Autonomous Agents and Multiagent Systems2019International Foundation for Autonomous Agents and Multiagent Systems</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. 2019</p>
<p>R A Fisher, Statistical Methods for Research Workers. Oliver &amp; Boyd1928</p>
<p>Ecological inference and the ecological fallacy. D A Freedman, International Encyclopedia of the Social &amp; Behavioral Sciences. 61999</p>
<p>Reinforcement learning with deep energy-based policies. T Haarnoja, H Tang, P Abbeel, S Levine, International Conference on Machine Learning. PMLR2017</p>
<p>Multi-task deep reinforcement learning with PopArt. M Hessel, H Soyer, L Espeholt, W Czarnecki, S Schmitt, H Van Hasselt, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>A simple sequentially rejective multiple test procedure. S Holm, Scandinavian Journal of Statistics. 1979</p>
<p>H Hu, A Lerer, A Peysakhovich, J Foerster, arXiv:2003.02979Other-play' for zero-shot coordination. 2020arXiv preprint</p>
<p>Inequity aversion improves cooperation in intertemporal social dilemmas. E Hughes, J Z Leibo, M Phillips, K Tuyls, E Dueñez-Guzman, A G Castañeda, I Dunning, T Zhu, K R Mc-Kee, R Koster, H Roff, T Graepel, Advances in Neural Information Processing Systems. 2018</p>
<p>Reward redistribution mechanisms in multi-agent reinforcement learning. A Ibrahim, A Jitani, D Piracha, D Precup, Adaptive Learning Agents Workshop at the International Conference on Autonomous Agents and Multiagent Systems. 2020</p>
<p>Human-level performance in 3D multiplayer games with population-based reinforcement learning. M Jaderberg, W M Czarnecki, I Dunning, L Marris, G Lever, A G Castañeda, C Beattie, N C Rabinowitz, A S Morcos, A Ruderman, N Sonnerat, T Green, L Deason, J Z Leibo, D Silver, D Hassabis, K Kavukcuoglu, T Graepel, Science. 36464432019</p>
<p>Intrinsic social motivation via causal influence in multi-agent RL. N Jaques, A Lazaridou, E Hughes, C Gulcehre, P A Ortega, D Strouse, J Z Leibo, N De Freitas, International Conference on Learning Representations. 2019</p>
<p>A Juliani, A Khalifa, V P Berges, J Harper, E Teng, H Henry, A Crespi, J Togelius, D Lange, arXiv:1902.01378Obstacle tower: A generalization challenge in vision, control, and planning. 2019arXiv preprint</p>
<p>Illuminating generalization in deep reinforcement learning through procedural level generation. N Justesen, R R Torrado, P Bontrager, A Khalifa, J Togelius, S Risi, arXiv:1806.107292018arXiv preprint</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>P Knott, M Carroll, S Devlin, K Ciosek, K Hofmann, A Dragan, R Shah, arXiv:2101.05507Evaluating the robustness of collaborative agents. 2021arXiv preprint</p>
<p>J Kramár, N Rabinowitz, T Eccles, A Tacchetti, arXiv:2004.07625Should I tear down this wall? Optimizing social metrics by evaluating novel actions. 2020arXiv preprint</p>
<p>A unified game-theoretic approach to multiagent reinforcement learning. M Lanctot, V Zambaldi, A Gruslys, A Lazaridou, K Tuyls, J Pérolat, D Silver, T Graepel, Advances in Neural Information Processing Systems. 2017</p>
<p>Malthusian reinforcement learning. J Z Leibo, J Perolat, E Hughes, S Wheelwright, A H Marblestone, E Duéñez-Guzmán, P Sunehag, I Dunning, T Graepel, Proceedings of the 18th International Conference on Autonomous Agents and Multiagent Systems. the 18th International Conference on Autonomous Agents and Multiagent SystemsInternational Foundation for Autonomous Agents and Multiagent Systems2019</p>
<p>Learning existing social conventions via observationally augmented self-play. A Lerer, A Peysakhovich, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. the 2019 AAAI/ACM Conference on AI, Ethics, and Society2019</p>
<p>Markov games as a framework for multiagent reinforcement learning. M L Littman, Machine Learning Proceedings 1994. Elsevier1994</p>
<p>Multi-agent actor-critic for mixed cooperativecompetitive environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, Advances in Neural Information Processing Systems. 2017</p>
<p>Social diversity and social preferences in mixed-motive reinforcement learning. K R Mckee, I Gemp, B Mcwilliams, E A Duéñez-Guzmán, E Hughes, J Z Leibo, Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems. the 19th International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems2020</p>
<p>N P Nieves, Y Yang, O Slumbers, D H Mguni, Y Wen, J Wang, arXiv:2103.07927Modelling behavioural diversity for learning in open-ended games. 2021arXiv preprint</p>
<p>A multi-agent reinforcement learning model of common-pool resource appropriation. J Perolat, J Z Leibo, V Zambaldi, C Beattie, K Tuyls, T Graepel, Advances in Neural Information Processing Systems. 2017</p>
<p>R Sanjaya, J Wang, Y Yang, arXiv:2110.11737Measuring the nontransitivity in chess. 2021arXiv preprint</p>
<p>Intrinsically motivated reinforcement learning. S P Singh, A G Barto, N Chentanez, Advances in Neural Information Processing Systems. 2005</p>
<p>H F Song, A Abdolmaleki, J T Springenberg, A Clark, H Soyer, J W Rae, S Noury, A Ahuja, S Liu, D Tirumala, N Heess, D Belov, M Riedmiller, M M Botvinick, arXiv:1909.12238V-MPO: On-policy maximum a posteriori policy optimization for discrete and continuous control. 2019arXiv preprint</p>
<p>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial Intelligence. 1121-21999</p>
<p>Comparing individual means in the analysis of variance. J W Tukey, Biometrics. 1949</p>
<p>Grandmaster level in StarCraft II using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, J Oh, D Horgan, M Kroiss, I Danihelka, A Huang, L Sifre, T Cai, J P Agapiou, M Jaderberg, A S Vezhnevets, R Leblond, T Pohlen, V Dalibard, D Budden, Y Sulsky, J Molloy, T L Paine, C Gulcehre, Z Wang, T Pfaff, Y Wu, R Ring, D Yogatama, D Wünsch, K Mckinney, O Smith, T Schaul, T Lillicrap, K Kavukcuoglu, D Hassabis, C Apps, D Silver, Nature. 57577822019</p>
<p>Paired open-ended trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions. R Wang, J Lehman, J Clune, K O Stanley, arXiv:1901.017532019arXiv preprint</p>
<p>Enhanced POET: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. R Wang, J Lehman, A Rawal, J Zhi, Y Li, J Clune, K Stanley, International Conference on Machine Learning. PMLR2020</p>
<p>Too many cooks: Bayesian inference for coordinating multi-agent collaboration. R E Wang, S A Wu, J A Evans, J B Tenenbaum, D C Parkes, M Kleiman-Weiner, Cooperative AI Workshop at the Conference on Neural Information Processing Systems. 2020</p>
<p>C Zhang, O Vinyals, R Munos, S Bengio, arXiv:1804.06893A study on overfitting in deep reinforcement learning. 2018arXiv preprint</p>
<p>Add walls along the outer edge of this area. </p>
<p>Remove two neighbouring walls for every player, adding a player spawn point at each newly available cell and adding those cells as available goal locations. </p>
<p>Create a random number of 3 × 3 wall blocks in the available area, ensuring that a majority of the blocks are more than two cells away from each other. </p>
<p>Randomly scatter a number of individual walls within the available area. </p>
<p>Check that the level is solvable: all goals are reachable from all starting locations. </p>
<p>Generate an area with a width. 4, 9</p>
<p>Add counters along the outer edge of this area. </p>
<p>Create a random number of counters within the available area, either as a block of counters in the middle or randomly scattered throughout the area. </p>
<p>For each object in {cooking pot, tomato station, dish station, delivery station}, convert one to three counters into that object. </p>
<p>Place two player spawn points in random non-assigned cells. </p>
<p>Check that the level is solvable: tomato soups can be created and delivered by both players. </p>
<p>Procedural generation Procedure To generate levels for Capture the Flag, we adapt the procedural generation procedure introduced in [24]: 1. Generate an area with an odd width. A.4.29, 15</p>
<p>Create random sized rectangular rooms within this area. </p>
<p>Fill the space between rooms using the backtracking-maze algorithm. to produce corridors</p>
<p>Remove deadends and horseshoes in the maze. </p>
<p>Make the level symmetric by taking the left-half of the level and concatenating it with its 180 • -rotated self. The flags and base on the right side of the level correspond to the blue team</p>
<p>Check that the level is solvable: both flags can be captured and returned by each team, with the extra constraint that the flags must be at least six cells apart. </p>
<p>Distribution over environmental features To demonstrate the diversity of the procedurally generated levels for Capture the Flag, this section introduces four descriptive features, presents their distributions from 100,000 samples, and visualizes the minimum, medium, and maximum level of each distribution. The four features are as follows</p>
<p>Crow Distance: The euclidean distance between the red flag and blue flag. </p>
<p>Path Distance: The path distance between the red flag and blue flag. </p>
<p>Path Complexity: The crow distance divided by the path distance. </p>
<p>Openness: The proportion of non-wall cells in the level. </p>
<p>end for 8: Return: EAV P Algorithm 2 get pool Generate representative pool of states for policy comparison 1: Input: Set of populations of interest P = {P 0 , . . . , P k }, shared set of training levels L, number of episodes to simulate E, number of states to draw from each episode J, number of players for. P , L , E , J , ) ; P , R , S) ; P , S , P , S , P k }, shared set of training levels L, number of episodes to simulate E, number of states to draw from each episode J, number of players for each episode n, number of action samples R 2: S ← get pool. 43: policy dists P,S ← approximate policy dists. each episode n 2: S ← {} 3: for all P ∈ P do 4: for e = 1 : E do 5: A ← {} 6: for i = 1 : n do 7: A i ∼ P 8: A ← A ∪ A i 9: end for 10: l ∼ L 11: {(s, a, r, s )t} ← sim( A, l) 12: T = |{(s, a, r, s )t}| 13: for j = 1 : J do 14: t ∼ {1, . . . , T } 15: S ← S ∪ {(st, l)} 16: end for 17: end for 18: end for 19: Return: S</p>            </div>
        </div>

    </div>
</body>
</html>
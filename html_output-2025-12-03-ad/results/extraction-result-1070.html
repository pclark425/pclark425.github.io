<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1070 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1070</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1070</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-3cb1d5ea1128f684cf68f59e1e04739fe9aea2ed</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3cb1d5ea1128f684cf68f59e1e04739fe9aea2ed" target="_blank">Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner</a></p>
                <p><strong>Paper Venue:</strong> Paladyn J. Behav. Robotics</p>
                <p><strong>Paper TL;DR:</strong> It is shown that SGIM-ACTS learns significantly more effciently than using single learning strategies, and coherently selects the best strategy with respect to the chosen outcome, taking advantage of the available teachers (with different levels of skills).</p>
                <p><strong>Paper Abstract:</strong> We present an active learning architecture that allows a robot to actively learn which data collection strategy is most efficient for acquiring motor skills to achieve multiple outcomes, and generalise over its experience to achieve new outcomes. The robot explores its environment both via interactive learning and goal-babbling. It learns at the same time when, who and what to actively imitate from several available teachers, and learns when not to use social guidance but use active goal-oriented self-exploration. This is formalised in the framework of life-long strategic learning.The proposed architecture, called Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy (SGIM-ACTS), relies on hierarchical active decisions of what and how to learn driven by empirical evaluation of learning progress for each learning strategy. We illustrate with an experiment where a simulated robot learns to control its arm for realising two kinds of different outcomes. It has to choose actively and hierarchically at each learning episode: 1) what to learn: which outcome is most interesting to select as a goal to focus on for goal-directed exploration; 2) how to learn: which data collection strategy to use among self-exploration, mimicry and emulation; 3) once he has decided when and what to imitate by choosing mimicry or emulation, then he has to choose who to imitate, from a set of different teachers. We show that SGIM-ACTS learns significantly more effciently than using single learning strategies, and coherently selects the best strategy with respect to the chosen outcome, taking advantage of the available teachers (with different levels of skills).</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1070.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1070.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGIM-ACTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical active learning architecture for embodied agents that actively selects which outcome to learn, which data-collection strategy (intrinsic exploration, mimicry, emulation) to use, and which teacher to query, driven by empirical competence progress estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SGIM-ACTS learner (simulated 1-DOF arm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated robot arm controlled by 14-dimensional motor primitives that builds inverse models L^{-1} via a hierarchical learner combining intrinsic-motivation (goal-babbling / SAGG-RIAC), mimicry and emulation; strategy selection is driven by local competence progress (gamma) and an interest map over outcome subregions and strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (simulated robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Throwing-and-placing task with wall obstacle</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 1-DOF arm holds and releases a ball. The outcome space T is composite: T1 = throwing outcomes (distance x and max height h, 2D) and T2 = placing outcomes (final arm angle phi with low velocity, 1D). Environment includes an obstacle wall at x=10 causing possible elastic collisions. The mapping from 14-D policy parameters to outcomes is highly redundant and continuous; placing outcomes are more difficult given acceleration-level control and the need for low release velocity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by: dimensionality of policy space (14 continuous parameters), composite outcome space dimensionalities (1D placing + 2D throwing), redundancy of M:Pi->T (many-to-one), physical dynamics including gravity and elastic collision with a wall, and task-specific control difficulty (placing requires velocity constraint). Specific numeric elements: policy dim = 14, outcome dims = {1,2}, obstacle at x=10.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (continuous high-dimensional policy space with redundant mapping to composite continuous outcome spaces and additional physical dynamics / obstacle)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation comes from: multiple outcome types (two types, infinite continuous instances within each), multiple teacher demonstration sets (3 teachers with different expertise and one with a correspondence problem), and continuous distribution of goals (benchmark set evenly distributed). Number of teachers = 3; outcome instances = effectively infinite (continuous).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (continuous outcome variability and heterogenous teacher demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean error J over an independent benchmark set (distance between desired outcome tau and achieved outcome M(L^{-1}(tau))); competence gamma = J(tau, M(L^{-1}(tau))) and competence progress (prog) computed from gamma1 and gamma2 over episodes. Evaluation performed every 1000 actions on benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SGIM-ACTS produced the lowest mean error curves for both throwing and placing compared to baselines over 8000 actions (exact numeric error values not reported in text); evaluated every 1000 actions over 8000 total interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs: (1) environment complexity (high-dimensional redundant policy-to-outcome mapping and composite outcomes) makes exploration difficult and benefits from demonstrations that bias policy-space exploration; (2) variation in demonstrations/teachers affects which strategy is best (e.g., a teacher expert for throwing is useless for placing, and correspondence problems make mimicry ineffective); (3) mimicry tends to reduce exploration (bad for generalisation in highly variable tasks), while emulation and intrinsic exploration help generalise but differ in goal selection. Overall, the method balances complexity and variation by selecting strategies that produce maximal competence progress per region: demonstrations can mitigate complexity by biasing exploration but if teacher variation is poor (wrong expertise or correspondence problems) social guidance can hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Hierarchical active learning with meta-level strategy selection (modes: random exploration, interest-driven exploitation, local refinement), combining intrinsic motivation (SAGG-RIAC goal-babbling), mimicry, and emulation; episodic interactive queries to teachers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — evaluated on an independent benchmark set uniformly distributed over the (continuous) outcome space; SGIM-ACTS generalised better to unseen goals across both outcome types than random, SAGG-RIAC alone, and pure socially guided strategies, achieving lower mean benchmark error curves.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Total interactions per run = 8000 actions; evaluations every 1000 actions. SGIM-ACTS converged faster (faster error reduction) than baselines in the experiments, but absolute numeric sample-efficiency numbers (e.g., error at specific action counts) are provided only graphically, not as exact table values in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SGIM-ACTS: (1) learns to select which outcome types and which strategy/teacher to use, leading to better and faster overall learning in a composite continuous outcome space; (2) autonomous exploration increases over time (agent becomes more autonomous as competence grows); (3) mimicry can be detrimental when teachers have correspondence problems or wrong expertise because it reduces exploration; (4) demonstrations bias exploration beneficially when teacher and learner embodiments are compatible and teachers are expert for the targeted outcomes; (5) the hierarchical competence-progress-driven strategy selection efficiently handles both environment complexity and variation among teachers/outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner', 'publication_date_yy_mm': '2012-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1070.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1070.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGG-RIAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsically motivated goal-babbling algorithm that self-generates goals where competence progress is maximal and performs goal-directed policy optimisation to learn inverse models in continuous spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active learning of inverse models with intrinsically motivated goal exploration in robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGG-RIAC autonomous learner (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An autonomous intrinsic-motivation-driven agent that selects goals based on local competence progress and performs goal-directed exploration / policy optimisation (used here as the intrinsic-motivation-only baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (simulated robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same throwing-and-placing task</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same composite outcome environment (1D placing, 2D throwing) with obstacle; controlled by 14-D motor primitives. Intrinsic-only exploration relies on goal-babbling without taking demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same as SGIM-ACTS: policy dim = 14, outcome dims {1,2}, redundant mapping, dynamics with gravity and wall obstacle.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation in outcomes is continuous (infinite goal instances); no social teacher variation used in this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (continuous outcome variability but no teacher heterogeneity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean benchmark error (J) across outcomes and competence progress metrics during learning; evaluated every 1000 actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SAGG-RIAC performs well on throwing outcomes (because throwing maps are easier under acceleration control) but is outperformed by SGIM-ACTS, especially on placing outcomes. Exact numeric values are given only in plotted curves, not in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed indirectly: intrinsic exploration alone handles high outcome variability but can be less efficient than mixed strategies when demonstrations could bias exploration usefully; intrinsic strategy excels for easier subproblems (throwing) in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic motivation / goal-babbling (SAGG-RIAC) only; no teacher interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalises well for throwing outcomes; less effective than SGIM-ACTS for placing outcomes due to difficulty of placement under acceleration control.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>8000 interactions per run; converges reasonably on throwing outcomes but slower/poorer on placing outcomes compared to SGIM-ACTS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intrinsic goal-babbling is effective for subproblems that match the control encoding (throwing under acceleration control) but benefits from social guidance or mixed strategies for more complex outcome types like placing that require different control affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner', 'publication_date_yy_mm': '2012-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1070.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1070.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RandomExpl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random exploration baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that samples policy parameters uniformly at random across the 14-dimensional policy parameter space to produce outcomes and collect data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random exploration agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent selects policies uniformly at random in Pi and collects resulting outcomes; no intrinsic motivation or social learning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (simulated robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same throwing-and-placing task</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same environment as above with composite outcome spaces and wall obstacle; no strategy selection — pure random sampling of motor primitive parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Policy dim = 14; outcome dims {1,2}; redundancy and continuous dynamics complicate mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Outcome variation is high (continuous), but the agent does not exploit structure nor teachers.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean benchmark error over independent outcome set (J).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: performs worse than SGIM-ACTS and SAGG-RIAC overall; some baseline competence on throwing due to easy mapping but poor on placing. No exact numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Not explicitly analysed, but demonstrates that unguided exploration struggles in high-dimensional, composite outcome spaces; shows need for structured strategies to handle complexity/variation trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment random sampling (no curriculum or adaptive strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalisation poor compared to active learners; baseline error remains higher across benchmark evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>8000 interactions; very sample-inefficient compared to SGIM-ACTS and SAGG-RIAC.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random exploration is insufficient in composite high-dimensional embodied tasks and highlights the value of intrinsic motivation and socially-guided active strategies for sample-efficient learning and generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner', 'publication_date_yy_mm': '2012-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1070.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1070.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mimicry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mimicry strategy (policy-space demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A social learning strategy where the learner requests a teacher demonstration of a policy trajectory and attempts to reproduce (copy) the teacher's policy, with small local variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mimicry strategy (used by learner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learner requests a teacher demonstration (trajectory ζ_d, outcome τ_d), identifies closest reproducible policy parameters θ_d minimizing distance to ζ_d, and executes small stochastic variations about θ_d for a short number of trials; competence is evaluated locally.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>strategy used by simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same throwing-and-placing task with teacher demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Learner receives policy demonstrations from one of 3 teachers. One teacher has correspondence problem causing parameter misinterpretation; teacher expertise varies across throwing vs placing.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Effectiveness depends on correspondence between teacher and learner embodiment, and on policy encoding (14-dim motor primitives). Complexity arises when teacher demonstrations fall into small subspaces and imitation reduces exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>variable (can be low-local but globally limiting)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Teacher variation (3 teachers with different specialisations; one with correspondence problem) and outcome variation within demonstrations; mimicry tends to reduce outcome variation explored because demonstrations sampled from teacher datasets are localized.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (limited by teacher dataset coverage and demonstration selection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Local competence improvement (gamma change) and mean benchmark error after mimicry-driven episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: mimicry often produced little learning progress (flat error curves) in this experiment — especially when teacher had correspondence problems or was expert in different outcome type; mimicry of teacher 2 (expert in placing) helped placing, but mimicry overall was less effective than emulation or intrinsic methods for general learning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper notes that mimicry biases policy-space exploration to local subspaces (helpful if teacher demonstrations match desired outcomes and embodiment) but can hinder global exploration and generalisation when teacher variation is insufficient or when correspondence problems exist.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Interactive imitation learning via demonstration requests (learner-initiated) combined with limited local random variation around demonstrated policy parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Poor generalisation in many conditions due to limited exploration; effective only when teacher demonstrations match task and embodiment closely.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Uses short-duration episodes per demonstration; however, because mimicry reduces exploration breadth, it was not sample-efficient for global learning in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mimicry can accelerate local acquisition near demonstration points but tends to limit global exploration; correspondence problems in teacher demonstrations can make mimicry ineffective or misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner', 'publication_date_yy_mm': '2012-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1070.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1070.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emulation strategy (outcome imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A social learning strategy where the learner observes a teacher's demonstrated outcome τ_d and uses its own policy repertoire and goal-directed optimisation to achieve the same outcome, without trying to copy the demonstrated policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Emulation strategy (used by learner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learner requests demonstration (ζ_d, τ_d) and sets τ_g := τ_d, then performs goal-directed policy optimisation (local models + Nelder-Mead + random exploration) to achieve the demonstrated outcome using its own motor repertoire.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>strategy used by simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same throwing-and-placing task with teacher demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Learner uses teacher-provided outcomes (not policies) to guide goal selection; multiple teachers provide outcomes with varying expertise, and one teacher has a correspondence problem affecting policy-level mimicry but not outcome-level emulation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Depends on outcome complexity (1D vs 2D), and mapping complexity from 14-D policies to outcomes; emulation leverages outcome variation in demonstrations to focus exploration on outcome subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation due to teacher outcome demonstrations and continuous outcome instances; emulation can steer outcome-space exploration (can both hinder or encourage exploration of some subspaces depending on demonstrated outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean benchmark error and competence progress (gamma) for goals set to demonstrated outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: emulation of appropriate teachers (e.g., teacher 2 for placing) reduces error for those outcome types; emulation is more robust than mimicry when correspondence problems exist because it uses outcome parametrisation which is shared between teacher and learner.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Emulation mitigates correspondence problems and can exploit teacher-provided outcome variation to focus learning; however, if teacher demonstrations are biased (limited variation) emulation may bias outcome exploration accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Interactive emulation: learner requests outcome demonstrations and performs goal-directed policy optimisation to reach the demonstrated outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Emulation improved performance for outcomes where teachers were expert; it generalized better than mimicry in presence of correspondence problems because it relied on outcome-space parametrisation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>More sample-efficient than mimicry in problematic correspondence cases because it focuses on outcomes rather than trying to reproduce policy trajectories; exact sample counts are within the 8000-action runs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Emulation leverages demonstrations at outcome level to guide exploration and is robust to teacher-learner embodiment mismatches that break mimicry; it can both improve learning speed in targeted outcome subspaces and bias exploration depending on teacher demonstration coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner', 'publication_date_yy_mm': '2012-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Bootstrapping intrinsically motivated learning with human demonstrations <em>(Rating: 2)</em></li>
                <li>The Strategic Student Approach for Life-Long Exploration and Learning <em>(Rating: 2)</em></li>
                <li>Properties for efficient demonstrations to a socially guided intrinsically motivated learner <em>(Rating: 1)</em></li>
                <li>Exploration in model-based reinforcement learning by empirically estimating learning progress <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1070",
    "paper_id": "paper-3cb1d5ea1128f684cf68f59e1e04739fe9aea2ed",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "SGIM-ACTS",
            "name_full": "Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy",
            "brief_description": "A hierarchical active learning architecture for embodied agents that actively selects which outcome to learn, which data-collection strategy (intrinsic exploration, mimicry, emulation) to use, and which teacher to query, driven by empirical competence progress estimates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SGIM-ACTS learner (simulated 1-DOF arm)",
            "agent_description": "A simulated robot arm controlled by 14-dimensional motor primitives that builds inverse models L^{-1} via a hierarchical learner combining intrinsic-motivation (goal-babbling / SAGG-RIAC), mimicry and emulation; strategy selection is driven by local competence progress (gamma) and an interest map over outcome subregions and strategies.",
            "agent_type": "simulated agent (simulated robotic arm)",
            "environment_name": "Throwing-and-placing task with wall obstacle",
            "environment_description": "A 1-DOF arm holds and releases a ball. The outcome space T is composite: T1 = throwing outcomes (distance x and max height h, 2D) and T2 = placing outcomes (final arm angle phi with low velocity, 1D). Environment includes an obstacle wall at x=10 causing possible elastic collisions. The mapping from 14-D policy parameters to outcomes is highly redundant and continuous; placing outcomes are more difficult given acceleration-level control and the need for low release velocity.",
            "complexity_measure": "Characterized by: dimensionality of policy space (14 continuous parameters), composite outcome space dimensionalities (1D placing + 2D throwing), redundancy of M:Pi-&gt;T (many-to-one), physical dynamics including gravity and elastic collision with a wall, and task-specific control difficulty (placing requires velocity constraint). Specific numeric elements: policy dim = 14, outcome dims = {1,2}, obstacle at x=10.",
            "complexity_level": "high (continuous high-dimensional policy space with redundant mapping to composite continuous outcome spaces and additional physical dynamics / obstacle)",
            "variation_measure": "Variation comes from: multiple outcome types (two types, infinite continuous instances within each), multiple teacher demonstration sets (3 teachers with different expertise and one with a correspondence problem), and continuous distribution of goals (benchmark set evenly distributed). Number of teachers = 3; outcome instances = effectively infinite (continuous).",
            "variation_level": "high (continuous outcome variability and heterogenous teacher demonstrations)",
            "performance_metric": "Mean error J over an independent benchmark set (distance between desired outcome tau and achieved outcome M(L^{-1}(tau))); competence gamma = J(tau, M(L^{-1}(tau))) and competence progress (prog) computed from gamma1 and gamma2 over episodes. Evaluation performed every 1000 actions on benchmark.",
            "performance_value": "Qualitative: SGIM-ACTS produced the lowest mean error curves for both throwing and placing compared to baselines over 8000 actions (exact numeric error values not reported in text); evaluated every 1000 actions over 8000 total interactions.",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs: (1) environment complexity (high-dimensional redundant policy-to-outcome mapping and composite outcomes) makes exploration difficult and benefits from demonstrations that bias policy-space exploration; (2) variation in demonstrations/teachers affects which strategy is best (e.g., a teacher expert for throwing is useless for placing, and correspondence problems make mimicry ineffective); (3) mimicry tends to reduce exploration (bad for generalisation in highly variable tasks), while emulation and intrinsic exploration help generalise but differ in goal selection. Overall, the method balances complexity and variation by selecting strategies that produce maximal competence progress per region: demonstrations can mitigate complexity by biasing exploration but if teacher variation is poor (wrong expertise or correspondence problems) social guidance can hurt.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Hierarchical active learning with meta-level strategy selection (modes: random exploration, interest-driven exploitation, local refinement), combining intrinsic motivation (SAGG-RIAC goal-babbling), mimicry, and emulation; episodic interactive queries to teachers.",
            "generalization_tested": true,
            "generalization_results": "Yes — evaluated on an independent benchmark set uniformly distributed over the (continuous) outcome space; SGIM-ACTS generalised better to unseen goals across both outcome types than random, SAGG-RIAC alone, and pure socially guided strategies, achieving lower mean benchmark error curves.",
            "sample_efficiency": "Total interactions per run = 8000 actions; evaluations every 1000 actions. SGIM-ACTS converged faster (faster error reduction) than baselines in the experiments, but absolute numeric sample-efficiency numbers (e.g., error at specific action counts) are provided only graphically, not as exact table values in text.",
            "key_findings": "SGIM-ACTS: (1) learns to select which outcome types and which strategy/teacher to use, leading to better and faster overall learning in a composite continuous outcome space; (2) autonomous exploration increases over time (agent becomes more autonomous as competence grows); (3) mimicry can be detrimental when teachers have correspondence problems or wrong expertise because it reduces exploration; (4) demonstrations bias exploration beneficially when teacher and learner embodiments are compatible and teachers are expert for the targeted outcomes; (5) the hierarchical competence-progress-driven strategy selection efficiently handles both environment complexity and variation among teachers/outcomes.",
            "uuid": "e1070.0",
            "source_info": {
                "paper_title": "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner",
                "publication_date_yy_mm": "2012-09"
            }
        },
        {
            "name_short": "SAGG-RIAC",
            "name_full": "Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC)",
            "brief_description": "An intrinsically motivated goal-babbling algorithm that self-generates goals where competence progress is maximal and performs goal-directed policy optimisation to learn inverse models in continuous spaces.",
            "citation_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "mention_or_use": "use",
            "agent_name": "SAGG-RIAC autonomous learner (baseline)",
            "agent_description": "An autonomous intrinsic-motivation-driven agent that selects goals based on local competence progress and performs goal-directed exploration / policy optimisation (used here as the intrinsic-motivation-only baseline).",
            "agent_type": "simulated agent (simulated robotic arm)",
            "environment_name": "Same throwing-and-placing task",
            "environment_description": "Same composite outcome environment (1D placing, 2D throwing) with obstacle; controlled by 14-D motor primitives. Intrinsic-only exploration relies on goal-babbling without taking demonstrations.",
            "complexity_measure": "Same as SGIM-ACTS: policy dim = 14, outcome dims {1,2}, redundant mapping, dynamics with gravity and wall obstacle.",
            "complexity_level": "high",
            "variation_measure": "Variation in outcomes is continuous (infinite goal instances); no social teacher variation used in this baseline.",
            "variation_level": "medium-high (continuous outcome variability but no teacher heterogeneity)",
            "performance_metric": "Mean benchmark error (J) across outcomes and competence progress metrics during learning; evaluated every 1000 actions.",
            "performance_value": "Qualitative: SAGG-RIAC performs well on throwing outcomes (because throwing maps are easier under acceleration control) but is outperformed by SGIM-ACTS, especially on placing outcomes. Exact numeric values are given only in plotted curves, not in text.",
            "complexity_variation_relationship": "Discussed indirectly: intrinsic exploration alone handles high outcome variability but can be less efficient than mixed strategies when demonstrations could bias exploration usefully; intrinsic strategy excels for easier subproblems (throwing) in this setup.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic motivation / goal-babbling (SAGG-RIAC) only; no teacher interactions.",
            "generalization_tested": true,
            "generalization_results": "Generalises well for throwing outcomes; less effective than SGIM-ACTS for placing outcomes due to difficulty of placement under acceleration control.",
            "sample_efficiency": "8000 interactions per run; converges reasonably on throwing outcomes but slower/poorer on placing outcomes compared to SGIM-ACTS.",
            "key_findings": "Intrinsic goal-babbling is effective for subproblems that match the control encoding (throwing under acceleration control) but benefits from social guidance or mixed strategies for more complex outcome types like placing that require different control affordances.",
            "uuid": "e1070.1",
            "source_info": {
                "paper_title": "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner",
                "publication_date_yy_mm": "2012-09"
            }
        },
        {
            "name_short": "RandomExpl",
            "name_full": "Random exploration baseline",
            "brief_description": "A baseline agent that samples policy parameters uniformly at random across the 14-dimensional policy parameter space to produce outcomes and collect data.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random exploration agent (baseline)",
            "agent_description": "Agent selects policies uniformly at random in Pi and collects resulting outcomes; no intrinsic motivation or social learning.",
            "agent_type": "simulated agent (simulated robotic arm)",
            "environment_name": "Same throwing-and-placing task",
            "environment_description": "Same environment as above with composite outcome spaces and wall obstacle; no strategy selection — pure random sampling of motor primitive parameters.",
            "complexity_measure": "Policy dim = 14; outcome dims {1,2}; redundancy and continuous dynamics complicate mapping.",
            "complexity_level": "high",
            "variation_measure": "Outcome variation is high (continuous), but the agent does not exploit structure nor teachers.",
            "variation_level": "high",
            "performance_metric": "Mean benchmark error over independent outcome set (J).",
            "performance_value": "Qualitative: performs worse than SGIM-ACTS and SAGG-RIAC overall; some baseline competence on throwing due to easy mapping but poor on placing. No exact numbers in text.",
            "complexity_variation_relationship": "Not explicitly analysed, but demonstrates that unguided exploration struggles in high-dimensional, composite outcome spaces; shows need for structured strategies to handle complexity/variation trade-offs.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment random sampling (no curriculum or adaptive strategy).",
            "generalization_tested": true,
            "generalization_results": "Generalisation poor compared to active learners; baseline error remains higher across benchmark evaluations.",
            "sample_efficiency": "8000 interactions; very sample-inefficient compared to SGIM-ACTS and SAGG-RIAC.",
            "key_findings": "Random exploration is insufficient in composite high-dimensional embodied tasks and highlights the value of intrinsic motivation and socially-guided active strategies for sample-efficient learning and generalisation.",
            "uuid": "e1070.2",
            "source_info": {
                "paper_title": "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner",
                "publication_date_yy_mm": "2012-09"
            }
        },
        {
            "name_short": "Mimicry",
            "name_full": "Mimicry strategy (policy-space demonstrations)",
            "brief_description": "A social learning strategy where the learner requests a teacher demonstration of a policy trajectory and attempts to reproduce (copy) the teacher's policy, with small local variations.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Mimicry strategy (used by learner)",
            "agent_description": "Learner requests a teacher demonstration (trajectory ζ_d, outcome τ_d), identifies closest reproducible policy parameters θ_d minimizing distance to ζ_d, and executes small stochastic variations about θ_d for a short number of trials; competence is evaluated locally.",
            "agent_type": "strategy used by simulated agent",
            "environment_name": "Same throwing-and-placing task with teacher demonstrations",
            "environment_description": "Learner receives policy demonstrations from one of 3 teachers. One teacher has correspondence problem causing parameter misinterpretation; teacher expertise varies across throwing vs placing.",
            "complexity_measure": "Effectiveness depends on correspondence between teacher and learner embodiment, and on policy encoding (14-dim motor primitives). Complexity arises when teacher demonstrations fall into small subspaces and imitation reduces exploration.",
            "complexity_level": "variable (can be low-local but globally limiting)",
            "variation_measure": "Teacher variation (3 teachers with different specialisations; one with correspondence problem) and outcome variation within demonstrations; mimicry tends to reduce outcome variation explored because demonstrations sampled from teacher datasets are localized.",
            "variation_level": "medium (limited by teacher dataset coverage and demonstration selection)",
            "performance_metric": "Local competence improvement (gamma change) and mean benchmark error after mimicry-driven episodes.",
            "performance_value": "Qualitative: mimicry often produced little learning progress (flat error curves) in this experiment — especially when teacher had correspondence problems or was expert in different outcome type; mimicry of teacher 2 (expert in placing) helped placing, but mimicry overall was less effective than emulation or intrinsic methods for general learning.",
            "complexity_variation_relationship": "Paper notes that mimicry biases policy-space exploration to local subspaces (helpful if teacher demonstrations match desired outcomes and embodiment) but can hinder global exploration and generalisation when teacher variation is insufficient or when correspondence problems exist.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Interactive imitation learning via demonstration requests (learner-initiated) combined with limited local random variation around demonstrated policy parameters.",
            "generalization_tested": true,
            "generalization_results": "Poor generalisation in many conditions due to limited exploration; effective only when teacher demonstrations match task and embodiment closely.",
            "sample_efficiency": "Uses short-duration episodes per demonstration; however, because mimicry reduces exploration breadth, it was not sample-efficient for global learning in this setup.",
            "key_findings": "Mimicry can accelerate local acquisition near demonstration points but tends to limit global exploration; correspondence problems in teacher demonstrations can make mimicry ineffective or misleading.",
            "uuid": "e1070.3",
            "source_info": {
                "paper_title": "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner",
                "publication_date_yy_mm": "2012-09"
            }
        },
        {
            "name_short": "Emulation",
            "name_full": "Emulation strategy (outcome imitation)",
            "brief_description": "A social learning strategy where the learner observes a teacher's demonstrated outcome τ_d and uses its own policy repertoire and goal-directed optimisation to achieve the same outcome, without trying to copy the demonstrated policy.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Emulation strategy (used by learner)",
            "agent_description": "Learner requests demonstration (ζ_d, τ_d) and sets τ_g := τ_d, then performs goal-directed policy optimisation (local models + Nelder-Mead + random exploration) to achieve the demonstrated outcome using its own motor repertoire.",
            "agent_type": "strategy used by simulated agent",
            "environment_name": "Same throwing-and-placing task with teacher demonstrations",
            "environment_description": "Learner uses teacher-provided outcomes (not policies) to guide goal selection; multiple teachers provide outcomes with varying expertise, and one teacher has a correspondence problem affecting policy-level mimicry but not outcome-level emulation.",
            "complexity_measure": "Depends on outcome complexity (1D vs 2D), and mapping complexity from 14-D policies to outcomes; emulation leverages outcome variation in demonstrations to focus exploration on outcome subspaces.",
            "complexity_level": "high",
            "variation_measure": "Variation due to teacher outcome demonstrations and continuous outcome instances; emulation can steer outcome-space exploration (can both hinder or encourage exploration of some subspaces depending on demonstrated outcomes).",
            "variation_level": "high",
            "performance_metric": "Mean benchmark error and competence progress (gamma) for goals set to demonstrated outcomes.",
            "performance_value": "Qualitative: emulation of appropriate teachers (e.g., teacher 2 for placing) reduces error for those outcome types; emulation is more robust than mimicry when correspondence problems exist because it uses outcome parametrisation which is shared between teacher and learner.",
            "complexity_variation_relationship": "Emulation mitigates correspondence problems and can exploit teacher-provided outcome variation to focus learning; however, if teacher demonstrations are biased (limited variation) emulation may bias outcome exploration accordingly.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Interactive emulation: learner requests outcome demonstrations and performs goal-directed policy optimisation to reach the demonstrated outcomes.",
            "generalization_tested": true,
            "generalization_results": "Emulation improved performance for outcomes where teachers were expert; it generalized better than mimicry in presence of correspondence problems because it relied on outcome-space parametrisation.",
            "sample_efficiency": "More sample-efficient than mimicry in problematic correspondence cases because it focuses on outcomes rather than trying to reproduce policy trajectories; exact sample counts are within the 8000-action runs.",
            "key_findings": "Emulation leverages demonstrations at outcome level to guide exploration and is robust to teacher-learner embodiment mismatches that break mimicry; it can both improve learning speed in targeted outcome subspaces and bias exploration depending on teacher demonstration coverage.",
            "uuid": "e1070.4",
            "source_info": {
                "paper_title": "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner",
                "publication_date_yy_mm": "2012-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2
        },
        {
            "paper_title": "Bootstrapping intrinsically motivated learning with human demonstrations",
            "rating": 2
        },
        {
            "paper_title": "The Strategic Student Approach for Life-Long Exploration and Learning",
            "rating": 2
        },
        {
            "paper_title": "Properties for efficient demonstrations to a socially guided intrinsically motivated learner",
            "rating": 1
        },
        {
            "paper_title": "Exploration in model-based reinforcement learning by empirically estimating learning progress",
            "rating": 1
        }
    ],
    "cost": 0.015198,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Active Choice of Teachers, Learning Strategies and Goals for a Socially Guided Intrinsic Motivation Learner</h1>
<p>Sao Mai Nguyen ${ }^{1 *}$<br>Pierre-Yves Oudeyer ${ }^{11}$<br>1 Flowers Team, INRIA and ENSTA ParisTech, France 200 avenue de la Vieille Tour, 33405<br>Tolence Cedex, France</p>
<p>Received 15-12-2012
Accepted 27-03-2013</p>
<h4>Abstract</h4>
<p>We present an active learning architecture that allows a robot to actively learn which data collection strategy is most efficient for acquiring motor skills to achieve multiple outcomes, and generalise over its experience to achieve new outcomes. The robot explores its environment both via interactive learning and goal-babbling. It learns at the same time when, who and what to actively imitate from several available teachers, and learns when not to use social guidance but use active goal-oriented self-exploration. This is formalised in the framework of life-long strategic learning. The proposed architecture, called Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy (SGIM-ACTS), relies on hierarchical active decisions of what and how to learn driven by empirical evaluation of learning progress for each learning strategy. We illustrate with an experiment where a simulated robot learns to control its arm for realising two kinds of different outcomes. It has to choose actively and hierarchically at each learning episode: 1) what to learn: which outcome is most interesting to select as a goal to focus on for goal-directed exploration; 2) how to learn: which data collection strategy to use among self-exploration, mimicry and emulation; 3) once he has decided when and what to imitate by choosing mimicry or emulation, then he has to choose who to imitate, from a set of different teachers. We show that SGIM-ACTS learns significantly more efficiently than using single learning strategies, and coherently selects the best strategy with respect to the chosen outcome, taking advantage of the available teachers (with different levels of skills).</p>
<h2>Keywords</h2>
<p>strategic learner $\cdot$ imitation learning $\cdot$ mimicry $\cdot$ emulation $\cdot$ artificial curiosity $\cdot$ intrinsic motivation $\cdot$ interactive learner $\cdot$ active learning $\cdot$ goal babbling $\cdot$ robot skill learning.</p>
<h2>1. Strategic Active Learning for Life-Long Acquisition of Multiple Skills</h2>
<p>Life-long learning by robots to acquire multiple skills in unstructured environments poses challenges of not only predicting the consequences or outcomes of their actions on the environment, but also learning the causal effectiveness of their actions for varied outcomes. The set of outcomes can be in large and high-dimensional sensorimotor spaces, while the physical embedding of robots allows only limited time for collecting training data. The learning agent has to decide for instance in which order he should focus on learning how to achieve the different outcomes, how much time he can spend to learn to achieve an outcome or which data collection strategy to use for learning to achieve a given outcome.</p>
<h3>1.1. Active Learning for Producing Varied Outcomes with Multiple Data Collection Strategies</h3>
<p>These questions can be formalised under the notion of strategic learning [27].
*E-mail: nguyenmai at gmail.com
${ }^{\dagger}$ E-mail: pierre-yves.oudeyer at inria.fr
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. An arm, described by its angle $\phi$, is controlled by a motor primitive with 14 continuous parameters (taking bounded values) that determine the evolution of its acceleration $\dot{\phi}$. A ball is held by the arm and then released at the end of the motion. The objective of the robot is to learn the mapping between the parameters of the motor primitive and two types of outcomes he can produce: a ball thrown at distance $x$ and height $h$, or a ball placed at the arm tip at angle $\phi$ with velocity smaller than $\left|v_{\text {max }}\right|$.</p>
<p>One perspective is learning to achieve varied outcomes. It aims at selecting which outcome to spend time on. A typical classification was proposed in $[35,36]$ where active learning methods improved the overall quality of the learning. In sequential problems as in robotics, producing an outcome has been modelled as a local predictive forward model [33], an option [7], or a region in a parameterised goal/option space [6]. In these works each sampling of an outcome entails a cost. The learning agent has to decide which outcome to explore/observe next. However, most studies using this perspective do not consider several strategies.
Another perspective is learning how to learn, by making explicit the choice and dependence of the learning performance on the method.</p>
<p>For instance, [5] selects among different learning strategies depending on the results for different outcomes. However most studies using this perspective consider only a single outcome.
Indeed, these works have not addressed the learning of both how to learn and what to learn, to select at the same time which outcome to spend time on, and which learning method to use. Only [27] studies the framework of these questions, and only examined a toy example with discrete and finite number of states, outcomes and strategies. In initial work to address learning for varied outcomes with multiple methods, we proposed the Socially Guided Intrinsic Motivation by Demonstration (SGIM-D) algorithm which uses both:
socially guided exploration, especially programming by demonstration [8], and
intrinsically motivated exploration, which are active learning algorithms based on measures of the evolution of the learning performance [32]
to reach goals in a continuous outcome space, in the case of a complex and continuous environment. High-dimensional environments can be handled by SGIM-D, designed for multiple outcomes in a continuous outcome space. In [29], SGIM-D learned to manipulate a fishing rod with a 6 -dof arm, i.e. to place the float on the surface of the water, which is described as a 2 d continuous outcome space. The robotic arm was controlled by a motor primitive with 24 continuous parameters that determine the trajectory of its joint positions. The robot learned which action $a$ to perform for a given goal position on the surface of the water $g_{g}$, where the hook should reach when falling into the water. However, the outcomes considered belonged to only one type of outcomes. Moreover, although SGIM-D has 2 learning strategies, it is a passive learner which only imitates when the teacher decides to give a demonstration. SGIM-D does not learn which method enables it to perform best.
In this paper, we address these two limitations. We study how a learning agent can achieve varied outcomes in structured continuous outcome spaces, even with outcomes of different types, and how he can learn for those various outcomes which strategy to adopt among 1) active self-exploration, 2) emulation of a teacher actively selected among available teachers, 3) mimicry of an actively selected teacher. We propose an algorithm for actively choosing the appropriate strategy, among several strategies.</p>
<h3>1.2. Formalisation</h3>
<p>Let us consider an agent learning motor skills, i.e. the mapping between an outcome space and a policy space. As an illustration, let us imagine the agent learning how to play tennis, He maps how the ball behaves (outcome) with respect to the movement of his racket (policy). He thus learns a forward model $M$ to predict where the ball bounces given the movement of his racket. More importantly, he builds an inverse model $L^{-1}$ to control his racket in order to make the ball bounce at a desired position. A good player knows which outcomes are feasible and knows at least one policy to produce any possible outcome: he can place the ball anywhere on the court. Ideally, he builds an inverse model $L^{-1}$ such that $M\left(L^{-1}\right)$ is identity.
More formally, we define an outcome space which may comprise of outcomes of different types and different dimensionalities. For tennis, outcomes can be the bouncing positions, spin angles ... We only assume that they can be parameterised by parameters $\tau \in T$ and that we can define a distance measure $J$ on $T \times T$. A policy $\pi_{0}$ is described by motor primitives parameterised by $\theta \in \Pi$. Its outcome is $M(\theta)$, where the mapping $M: \Pi \rightarrow T$ describes the environment.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Representation of the problem. The environment can evolve to an outcome state $\tau$ by means of the learner's policy of parameter $\theta$ or the teacher's actions $\mathcal{C}$. The learner and the teacher have a priori different policy spaces. The learner estimates $L^{-1}: T \mapsto \Pi$. By emulation or mimicry, the learner can take advantage of the demonstrations $\left(\mathcal{C} \times \mu\right)$ of the teacher to improve its estimation $L^{-1}$.
For the tennis player, the policy controls the movement of his arm and racket and $M$ represents the physical equations for the ball trajectory. The performance of a policy $\pi_{0}$ at completing an outcome $\tau$ is measured by the distance between $\tau$ and the outcome of $\pi_{0}: J(\tau, M(\theta))$. The agent focuses on learning the inverse model and builds its estimate $L^{-1}: T \rightarrow \Pi$. We note that $M^{-1}$, the inverse of $M$ might not be a function as $M$ might be redundant, whereas our learner builds a function $L^{-1}$ that finds at least one adequate policy to complete every outcome $\tau$. In sum, it endeavours to minimise with respect to $L^{-1}$ :</p>
<p>$$
I=\int_{\tau \in T} P(\tau) J\left(\tau, M\left(L^{-1}(\tau)\right)\right) d \tau
$$</p>
<p>where $P(\tau)$ is a probability density distribution over $T$. A priori unknown to the learner, $P(\tau)$ can describe the probability of $\tau$ occurring or the reachable space or a region of interest.
We assume that $T$ can be partitioned into subspaces where the outcomes are related, and in these subspaces our parametrisation allows a smooth variation of $\tau \mapsto J(\tau, M(\theta)), \forall \theta$ with respect to $\tau$ most of the time. This partition, initially unknown to the agent, needs to be learned. Note that we have described our method without specifying a particular choice of policy representation, learning algorithm, action or outcome space properties. These designs can indeed be decided according to the application at hand. In particular, outcomes can be of different types and dimensionalities. In this case, we note $T_{i}$ the subspaces of $T$ corresponding to the different types of outcome and $T=U T_{i}$.</p>
<h3>1.3. Our Approach</h3>
<p>To solve the problem formalised above, we propose a system, called Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy (SGIM-ACTS) that allows an online interactive learning of inverse models in continuous high-dimensional robotic sensorimotor spaces with multiple teachers, and learning strategies. SGIM-ACTS learns various outcomes with different types of outcomes, and generalises from sampled data to continuous sets of outcomes.
Technically, we adopt a method of generalisation of policies for new outcomes similar to $[15,16]$. Whereas in their approaches the algorithms use a pool of examples given by the teacher preset from the beginning of the experiment to learn outcomes specified by the engineer of the</p>
<p>robot, in a batch learning method; in our case, the SGIM-ACTS algorithm decides by itself which outcomes it needs to learn more to better generalise for the whole outcome space, like in $[6,7,33]$. Moreover, SGIM-ACTS actively requests the teacher's demonstrations online, by choosing online the best learning strategy, similarly to [5], except that we do not learn with a discrete outcome space for a classification problem, but with a continuous outcome space. SGIM-ACTS also interacts with several teachers and uses several social learning methods, in an interactive learning approach.
Our active learning approach is inspired by:</p>
<ul>
<li>intrinsic motivation in psychology [38] which triggers spontaneous exploration and curiosity in humans, which recently led to novel robotic and machine active learning methods which outperform traditional active learning methods $[6,24]$</li>
<li>teleological learning [14] which considers actions as goaloriented, and recently led to efficient goal babbling methods in robotics $[6,37]$</li>
<li>psychological theories for socially guided learning [12, 16, 42], as detailed in the next section.</li>
</ul>
<p>After this formal description of our approach, we analyse our point of view on social guidance in Section 2. Then, we detail the proposed algorithm SGIM-ACTS in Section 3, before testing it on a problem to learn how to throw and place a ball (fig. 1) in Section 4.</p>
<h2>12. Social Guidance</h2>
<h3>2.1. Interactive Learning</h3>
<p>An interactive learner who not only listens to the teacher, but actively requests for the information it needs and when it needs help, has been shown to be a fundamental aspect of social learning [13, 31, 40]. Under the interactive learning approach, the robot can combine programming by demonstration, learning by exploration and tutor guidance. Several works in interactive learning have considered extra reinforcement signals [41], action requests [17, 25] or disambiguation among actions [13]. In [10] the comparison of a robot that has the option to ask the user for feedback, to the passive, shows a better accuracy and fewer demonstrations. Therefore, requesting demonstrations when it is needed can lessen the dependence on the teacher and reduce the quantity of required demonstrations. This approach is the most beneficial to the learner, for the information arrives as it needs it, and to the teacher who no longer needs to monitor the learning process.
For an agent learning motor skills, i.e. the mapping between policies and outcomes, let us examine the type of social guidance that a learner can get as reviewed in $[3,8,26,39]$ with respect to: what, how, when and who [16]. In this section, we note $\boldsymbol{s i}_{\boldsymbol{i} \boldsymbol{t}}$ the information flow from the human to the robot.</p>
<h3>2.2. What?</h3>
<p>Let us examine the target of the information given by the teacher, or mathematically speaking, the space on which he operates. This can be either the policy or outcome spaces, or combinations of them.</p>
<h3>2.2.1. Policy Space</h3>
<p>Many social learning studies target the policy parameter space $\Pi$. For instance, in programming by demonstration (LbD), $\boldsymbol{s i}_{\boldsymbol{i} \boldsymbol{t}}$ shows the right policy to perform in order to reach a given goal. As an illustration, when
teaching how to play tennis, your coach could show you how to hit a backhand by a demonstration, or even by taking your hand and directing your movement. This approach relates to two levels of social learning: mimicry, in which the learner copies the policies of others without an appreciation of their purpose, and imitation, in which the learner reproduces the policies and the changes in the environment, as formalised in [12, 26, 43]. The literature often considers that targeting the policy space is the most directive and efficient method. However, it relies on the human teacher's expertise, which bears limitations such as ambiguity, imprecision, under-optimality or the correspondence problem.</p>
<h3>2.2.2. Outcome Space</h3>
<p>The second kind of information is about possible outcomes $\boldsymbol{\tau} \in \boldsymbol{\Gamma}$, and is related to goal-directed exploration, where the learner focuses on discovering different outcomes instead of different ways of entailing the same outcome. Psychologically speaking, this case pertains to the simulation level of social learning, where the observer witnesses someone produce a result on an object, but then employs his own policy repertoire to reproduce the result, as formalised in [12, 26, 28, 43]. During our tennis training, your coach could ask you to hit with the ball the right corner of the court, wherever you received the ball, whichever shot you use. Goal-directed approaches allow the teacher to reset goal outcomes [1], to request the execution of outcomes [40] or to label outcomes [40, 41]. The learner can infer from the demonstrations the goal outcome by positional and force profiles to iron and open doors [21], or by using inverse reinforcement learning [23]. This approach is essential to learn multiple outcomes, and all the more interesting as it is inspired by psychological behaviours [14, 42, 43]. The drawback is that the learning needs the actions repertoire to be large enough to be used to reach various goals, before it improves.
As we want the learner to accomplish not only a single outcome but to be efficient on a large variety of goals, we choose to bootstrap its learning with information targeting the outcome space. Furthermore, we also want the learning process to benefit from the social interaction early. So that the learner builds its action repertoire quickly, we choose to target the policy parameter space $\Pi$ too.</p>
<h3>2.3. When?</h3>
<p>The timing of the interaction varies with respect to its general activity during the whole learning process. The rhythm of social interaction varies considerably among studies of social learning:</p>
<ul>
<li>At a fixed frequency: In classical imitation learning, the learner uses a demonstration to improve its learning at every policy it performs $[1,2,11]$. This solution is ill-adapted to the teacher's availability or the needs of the learner who requires more support in difficult situations.</li>
<li>Beginning of learning: A limited number of examples are given to initialise the learning, as a basic behaviours repertoire [1, 2], or a sample behaviour to be optimised [20, 34]. The learner is endowed with some basic competence before self-exploration. Nevertheless, if the interactions are restricted to the beginning, the learner could face difficulties adapting to changes in the environment.</li>
<li>At the teacher's initiative: The teacher alone decides when he interacts with the robot [40], by for instance giving corrections when seeing errors [10, 19]. Nevertheless, it still is time consuming as he needs to monitor the robot's errors to give adequate information to the learner.</li>
</ul>
<p>At the learner's initiative: The interactive learner can request for the teacher's help in an ambiguous [10, 13] or unknown [40] situation, or only reproduces the observations when the observed outcome matches its goal during goal-based imitation or mimicking [11]. This approach is the most beneficial to the learner, for the information arrives as it needs them, and the teacher needs not monitor the process.</p>
<p>These 4 types can be classified into 2 larger groups:</p>
<ul>
<li>batch learning, where the data provided to the learner is decided before the learning phase, and is given independently of the learning progress, generally in the beginning of the learning phase.</li>
<li>interactive learning, where the user interacts with the incrementally learning robot, either at the teacher's or the learner's initiative.</li>
</ul>
<h3>2.4. Who?</h3>
<p>While most social guidance studies only consider a single teacher, in natural environments, a household robot in reality interacts with several users. Moreover, being able to request help to different experts is also an efficient way to address the problem of the reliability of the teacher. Imitation learning studies often rely on the quality of the demonstrations, whereas in reality a teacher can be performant for some outcomes but not for others. Demonstrations can be ambiguous, unsuccessful or suboptimal in certain areas. Like students who learn from different teachers who are experts in the different topics of a curriculum, a robot learner should be able to determine its best teacher for the different outcomes it wants to achieve.
In this work, we consider the possibility of a learner to observe and imitate from several teachers, as much like a child in a natural environment would observe and imitate several adults in his surrounding throughout his development. In this case, choosing whom to imitate, recognising who is the expert in the outcomes we need to make progress, constitutes an important strategy choice.</p>
<h3>2.5. Actively Learning When, Who and What to Imitate</h3>
<p>For the model and experiments presented below, our choice of social guidance among this listing of social learning is:</p>
<p>What: We opted for an information flow targeting both policy and outcome spaces, to enable the biggest progress for the learner. It can imitate to reproduce either a demonstrated policy or outcome. Therefore, our learner can decide whether to mimic and emulate by learning what is the most interesting information.</p>
<p>When: Interactive learning at the learner's initiative seems the most natural interaction approach, the most efficient for learning and less costly for the teacher than if he would have to monitor the learner's progress to adapt his demonstrations. The robot has to learn when it is useful to imitate.</p>
<p>Who: Interactive learning where the learner can choose who to interact with and to whom to ask for help, is an important strategy choice in learning.</p>
<h2>Algorithm 1 SGIM-ACTS</h2>
<p>Input: the different strategies $\sigma_{1}, \ldots \sigma_{n}$.
Initialization: partition of outcome space $\mathcal{R} \leftarrow$ singleton $T$
Initialization: episodic memory (collection of produced outcomes) Memo $\leftarrow$ empty
loop
$\tau_{i}, \sigma \leftarrow$ Select Goal Outcome and Strategy $(\mathcal{R})$
if $\sigma \leftarrow$ Mimic teacher $i$ strategy then
$\left(\zeta_{i}, \tau_{i}\right) \leftarrow$ ask and observe demonstration to teacher $i$.
$\gamma_{1} \leftarrow$ Competence for $\tau_{g}$
Memo $\leftarrow$ Mimic Action $\left(\zeta_{g}\right)$
Update $L^{-1}$ with collected data Memo
$\gamma_{2} \leftarrow$ Competence for $\tau_{g}$
else if $\sigma=$ Emulate teacher $i$ strategy then
$\left(\zeta_{i}, \tau_{i}\right) \leftarrow$ ask and observe demonstration to teacher $i$.
Emulation: $\tau_{g} \leftarrow \tau_{g}$
$\gamma_{1} \leftarrow$ Competence for $\tau_{g}$
Memo $\leftarrow$ Goal-Directed Policy Optimisation $\left(\tau_{g}\right)$
Update $L^{-1}$ with collected data Memo
$\gamma_{2} \leftarrow$ Competence for $\tau_{g}$
else
$\sigma=$ Intrinsic Motivation strategy
$\tau_{g} \leftarrow \tau_{i}$
$\gamma_{1} \leftarrow$ Competence for $\tau_{g}$
Memo $\leftarrow$ Goal-Directed Policy Optimisation $\left(\tau_{g}\right)$
Update $L^{-1}$ with collected data Memo
$\gamma_{2} \leftarrow$ Competence for $\tau_{g}$
end if
$n b A \leftarrow$ number of new episodes in Memo
$\operatorname{prog} \leftarrow 2\left(\operatorname{sig}\left(\sigma_{p}+\frac{\sigma_{p}-\sigma_{1}}{\left(\zeta_{1}\right) \cdot n b A}\right)-1\right)$
$\mathcal{R} \leftarrow \quad$ Update Outcome and Strategy Interest Mapping
$(\mathcal{R}$, Memo, $\tau_{g}$, prog, $\sigma)$
end loop
Thus, it learns to answer the four main questions of imitation learning: "what, how, when and who to imitate" $[9,16]$ at the same time. We address active learning for varied outcomes with multiple strategies, multiple teachers, with a structured continuous outcome space (embedding sub-spaces with different properties). The strategies we consider are autonomous self-exploration, emulation and mimicking, by interactive learning with several teachers. Hereafter we describe the design of our SGIM-ACTS (Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy) algorithm. Then we show through an illustration experiment that SGIM-ACTS efficiently learns to realise different types of outcomes in continuous outcome spaces, and it coherently selects the right teacher to learn from.</p>
<h2>3. Algorithm Description</h2>
<p>In this section, we describe the SGIM-ACTS architecture by giving a behavioural outline in Section 3.1, before describing its general structure in Section 3.2. We then detail the different functions in sections 3.3 and 3.4. The overall architecture is summarised in Algorithm 1 and is illustrated in Fig. 3 .</p>
<h3>3.1. Architecture Outline</h3>
<p>SGIM-ACTS is an architecture that merges intrinsically motivated selfexploration with interactive learning as socially guided exploration. In the latter case, a teacher performs an observed trajectory $\zeta$ which</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Time flow chart of SGIM-ACTS, which combines Intrinsic Motivation and Mimicking and Emulation into 3 layers that pertain to the strategy, the outcome space and the policy space exploration respectively.</p>
<p>achieves an outcome τ<sub>d</sub>. Note that the observed trajectory might be impossible for the learner to re-execute, and he can only approach it best with a policy π<sub>θ</sub><sup>τ</sup>. The agent learns to achieve different types of outcomes by actively choosing which outcomes to focus on and set as goals, which data collection strategy to adopt and to which teacher to ask for help. It learns local inverse and forward models in complex, redundant and continuous spaces.</p>
<p>SGIM-ACTS learns by episodes during which it actively chooses simultaneously an outcome τ<sub>g</sub> ∈ T to reach and a learning strategy with a specific teacher (cf. 3.4.3). Its choice σ is selected between: intrinsically motivated exploration, mimicry from teacher 1, emulation of teacher 1, mimicry from teacher 2, emulation of teacher 2 ...</p>
<p>In an episode under a mimicking strategy (fig. 3), our SGIM-ACTS learner actively self-generates a goal τ<sub>g</sub> where its competence improvement is maximal (cf. 3.4.3). The SGIM-ACTS learner explores preferentially goal outcomes easy to reach and where it makes progress the fastest. The selected teacher answers its request with a demonstration (ζ<sub>d</sub>, τ<sub>d</sub>) to produce an outcome τ<sub>d</sub> that is closest to τ<sub>g</sub> (cf. 3.3.1). The robot mimics the teacher to reproduce ζ<sub>d</sub>, for a fixed duration, by performing policies π<sub>θ</sub> which are small variations of an approximation of ζ<sub>d</sub>.</p>
<p>In an episode under an emulation strategy (fig. 3), our SGIM-ACTS learner observes from the selected teacher a demonstration (ζ<sub>d</sub>, τ<sub>d</sub>). It tries different policies using goal-directed optimisation algorithms to approach the observed outcome τ<sub>d</sub>, without taking into account the demonstrated policy ζ<sub>d</sub>. It re-uses and optimises its policy repertoire built through its past autonomous and socially guided explorations (cf. 3.3.2). The episode ends after a fixed duration.</p>
<p>In an episode under the intrinsic motivation strategy (fig. 3), it explores autonomously following the SAGG-RIAC algorithm [6]. It actively self-generates a goal τ<sub>g</sub> where its competence improvement is maximal (cf. 3.4.3), as in the mimicking strategy. Then, it explores which policy π<sub>θ</sub> can achieve τ<sub>g</sub> best. It tries different policies to approach the self-determined outcome τ<sub>g</sub>, as in the emulation strategy (cf. 3.3.2). The episode ends after a fixed duration. The intrinsic motivation and emulation strategies differ mainly by the way the goal outcome is chosen. An extensive study of the role of these different learning strategies can be found in [30]. Thus the mimicry exploration increases the learner's policy repertoire on which to build up emulation and self-exploration, while biasing the policy space exploration. Demonstrations with structured policy sets, similar policy shapes, bias the policy space exploration to interesting subspaces, that allow the robot to overcome high-dimensionality and redundancy issues and interpolate to generalise in continuous outcome spaces. With emulation learning, the teacher influences the exploration of the outcome space. He can hinder the exploration of subspaces attracting the learner's attention to other subspaces. On the contrary, he can encourage their exploration by making demonstrations in those subspaces. Self-exploration is essential to build up on these demonstrations to overcome correspondence problems and collect more data to acquire better precision according to the embodiment of the robot.</p>
<p>This behavioural description of SGIM-ACTS is followed in the next section by the description of its architecture.</p>
<h3>3.2. Hierarchical Structure</h3>
<p>SGIM-ACTS improves its estimation L<sup>−1</sup> to minimise I = ∫<sub>r</sub> P(r|I|τ, M|L<sup>−1</sup>(r)|I|dτ by exploring with the different strategies the outcome and policy spaces. Its architecture is separated into three levels:</p>
<ul>
<li>A Strategy Exploration level which decides actively which learning strategy to use between intrinsic motivation, emulation and mimicry, and which teacher to ask for demonstrations (Select Goal Outcome and Strategy). To motivate its choice, it maps T in terms of interest level for each strategy (Outcome and Strategy Interest Mapping) to keep track which strategy and which subspace of T leads to the best learning progress.</li>
<li>An Outcome Space Exploration level which minimises I by exploring T. It decides actively which outcome τ<sub>g</sub> to focus on, to minimise I(τ<sub>g</sub>, M|L<sup>−1</sup>(τ<sub>g</sub>)|I), according to the adopted strategy. In the case of an emulation strategy, it sets the observed outcome of the demonstration τ<sub>d</sub> as a goal. In the case of mimicry and intrinsic motivation strategies, it self-determines a goal τ<sub>g</sub> selected by the Select Goal Outcome and Strategy function.</li>
<li>A Policy Space Exploration level which explores the policy parameters space Π to improve its estimation of I and estimate the inverse mapping L<sup>−1</sup>(τ<sub>g</sub>). With the mimicry learning strategy, it mimics the demonstrated trajectory ζ<sub>d</sub> by the chosen teacher to estimate I around that locality (Mimicry). With the emulation and autonomous exploration strategy, the Goal-Directed Policy Optimisation function</li>
</ul>
<p>minimises $J\left(\tau_{g}, M(\theta)\right)$ with respect to $\theta$. It attempts to reach the goals $\tau_{g}$ set by the Strategy and Outcome Space Exploration level, and gets a better estimate of $J$ that it can use later on to reach other goals. It finally returns to the Strategy and Outcome Space Exploration level the measure of competence progress for reaching $\tau_{g}$ or $\tau_{d}$.</p>
<p>The exploration in the three levels is the key to the robustness of SGIMACTS in high dimensional policy spaces.</p>
<h3>3.3. Policy Space Exploration</h3>
<h3>3.3.1. Mimicry</h3>
<p>This function tries to mimic a demonstration $\left(\zeta_{d}, \tau_{d}\right)$ with policy parameters $\theta_{o e}=\theta_{d}+\theta_{o \text { end }}$ with a random movement parameter variation $\left|\theta_{\text {read }}\right|&lt;\epsilon$ and $\pi_{\theta_{d}}$ is the closest policy to reproduce $\zeta_{d}$. $\theta_{d}$ is computed by minimising over $\theta$ the distance between $\zeta_{d}$ and the motor primitives $\pi_{\theta}$. This function thus makes an estimate of $J\left(\tau_{d}, M(\theta)\right)$ in the locality of $\theta_{d}$. After a short fixed number of times, SGIMACTS computes its competence at reaching the goal $\tau_{d}$.</p>
<h3>3.3.2. Goal-Directed Policy Optimisation</h3>
<p>This function searches for policies $\pi_{\theta}$ that guide the system toward the goal $\tau_{g}$ by 1) building local models of $J$ during exploration that can be re-used for later goals and 2) updating its estimated inverse model $L^{-1}$. In the experiments below, exploration mixes local optimisation with the Nelder-Mead simplex algorithm [22] and global random exploration to avoid local minima. The measures are used to build memory-based local direct and inverse models, using interpolation and more specifically locally weighted learning with a gaussian kernel such as presented in [4].</p>
<h3>3.4. Strategy and Outcome Space Exploration</h3>
<h3>3.4.1. Emulation</h3>
<p>In the emulation strategy, the learner explores outcomes $\tau_{d}$ that he observed from the demonstrations: $\tau_{g} \leftarrow \tau_{d}$. The learner tries to achieve $\tau_{d}$ by goal-oriented policy optimisation, which allows data collection and updating of $L^{-1}$.</p>
<h3>3.4.2. Outcome and Strategy Interest Mapping</h3>
<p>$T$ is partitioned according to interest levels. We note $\mathcal{R}=\left{R_{i}, T=\right.$ $\left.\cup_{i} R_{i}\right}$ a partition of $T$. For each outcome $\tau$ explored with strategy $\sigma$, the learner evaluates its competence progress, where competence measure assesses how close it can reach $\tau: \gamma=J\left(\tau, M\left(L^{-1}(\tau)\right)\right)$. A high value of $\gamma$ means a good competence at reaching the goal $g_{g}$ by strategy $\sigma$.
For each episode, it can compute its competence for the goal outcome at the beginning of the episode $\gamma_{1}$ and the end of the episode $\gamma_{2}$ after trying $n b A$ movements and measure its competence progress:</p>
<p>$$
\operatorname{prog}=2\left(s i g\left(\sigma_{p} * \frac{\gamma_{1}-\gamma_{2}}{\left|T_{i}\right| \cdot n b A}\right)-1\right) \text { with } \operatorname{sig}(x)=\frac{e^{x}+e^{-x}}{2}
$$</p>
<p>where $\sigma_{p}$ is a constant and $\left|T_{i}\right|$ is the size of the subspace $T_{i}$.
$T$ is partitioned so as to maximally discriminate areas according to their competence progress, as described in Algorithm 2 and [6]. For each strategy $\sigma$, we define a cost $\kappa(\sigma)$, which are weights for the computation of the interest of each region of the outcome space. $\kappa(\sigma)$ represents the preference of the teachers to help the robot or not, or the</p>
<p>Algorithm $2[\mathcal{R}]=$ Update Outcome and Strategy Interest Mapping( $\mathcal{R}, \operatorname{Memo}, \tau_{g}, \operatorname{progress}<em i="i">{g}, \sigma)$
input: $\mathcal{R}$ : set of regions $R</em>(\sigma)$ for each strategy $\sigma$.
input: $\tau_{g}$, progress $}$, and corresponding interest $R_{i<em i="i">{g}$ : goal outcome of the episode and its progress measure.
input: Memo: the set of all observed outcomes during the episode and their progress measures $\left(\tau</em>} \text { progress <em _Max="{Max" _text="\text">{i}\right)$.
input: $\sigma$ : strategy and teacher used during the episode.
parameter: $g</em>$ : the maximal number of elements inside a region.
parameter: $\delta$ : a time window used to compute the interest.
for all $\left(\tau\right.$, progress $\left.\left.\left.\in\left{\operatorname{Memo},\left(\tau_{g}, \operatorname{progress}}<em i="i">{g}\right)\right}\right} \right\rvert\, d\right)$
Find the region $R</em>$.
Add progress in $R_{i}(\sigma)$, the list of competence progress measures of experiments $\tau \in R_{i}$, with strategy $\sigma$.
Compute the new value of competence progress of $R_{i}(\sigma)$ :}, \in \mathcal{R}$ such that $\tau \in R_{i</p>
<p>$$
\operatorname{interest}<em n="n">{\theta</em>}}(\sigma)=\frac{\operatorname{mean<em i="R_{i">{\left.\left.\left.\right|</em>
$$} i- \delta}^{\left|R_{i}\right.}\right\rvert\right.}^{\left|R_{i}\right.} \text { progress }_{i}}{\epsilon(\sigma)</p>
<p>if $\left|R_{i}(\sigma)\right|&gt;g_{\text {max }}$ then
$\mathcal{R} \leftarrow \operatorname{Split} R_{i}$.
end if
end for
return $\mathcal{R}$
cost in time and energy ... of each strategy, and in this study $\kappa(\sigma)$ are set to arbitrary constant values.
We compute the interest as the local competence progress, over a sliding time window of the $\delta$ most recent goals attempted inside $R_{i}$ with strategy $\sigma$ which builds the list of competence progress measures $R_{i}(\sigma)=\left{\right.$ progress $\left.<em i="i">{1}, \ldots\right.$ progress $\left.\left.R</em>(\sigma)\right]\right}$.</p>
<p>$$
\operatorname{interest}<em k="k">{\theta</em>}}(\sigma)=\frac{\operatorname{mean<em i="R_{i">{\left.\left.\left.\right|</em>
$$} \mid \sigma}\right] \mid}^{\left|R_{i}(\sigma)\right|} \cdot \delta \text { progress }_{k}}{\kappa(\sigma)</p>
<p>The partition of $T$ is done recursively and so as to maximally discriminate areas according to their levels of interest. A split is triggered once a number of outcomes $g_{\text {max }}$ has been attempted inside $R_{i}$, with the same strategy $\sigma$. The split separates areas of different interest levels and different reaching difficulties. The split of a region $R_{i}$, into $R_{i+1}$ and $R_{i+2}$ is done by selecting among $m$ randomly generated splits, a split dimension $j \in|T|$ and then a position $v_{j}$ (we suppose that $R_{i}, \subset T_{i} \subset T$ with $T_{i}$ a n-dimensional space) such that:</p>
<ul>
<li>All the $\tau \in R_{i+1}$ have a |th component smaller than $v$ |;</li>
<li>All the $\tau \in R_{i+2}$ have a |th component higher than $v$ |;</li>
<li>It maximises the quantity $\operatorname{Qual}(j, v j)=\left|R_{i+1}\right| \cdot\left|R_{i+2}\right|$ $\left.\mid \operatorname{interest}<em i_1="i+1">{\theta</em>} \mid(\sigma)}\right|-\operatorname{interest<em i_2="i+2">{\theta</em>$.}}(\sigma) \mid$, where $\left|R_{i}\right|$ is the size of the region $R_{i</li>
</ul>
<h3>3.4.3. Select Goal Outcome and Strategy</h3>
<p>In order to balance exploitation and exploration, the next goal outcome and strategy are selected according to one of the 3 modes, chosen stochastically with respectively probabilities p1, p2 and p3:</p>
<ul>
<li>mode 1: choose $\sigma$ and $\tau \in T$ randomly. It ensures a minimum of exploration of the full strategy and outcome spaces.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. The selection of outcome and strategy is based on a partition of the outcome space with respect to different competence progress levels. We illustrate with the case of an outcome space of 3 different types of outcomes. $T 1 \subset \mathbb{R}^{2}, T 2 \subset \mathbb{R}$ and $T 3 \subset \mathbb{R}^{3}$. $T$ is partitioned in regions $R_{t}$ to which are associated measures of competences $\gamma$ for each strategy. The "Select Goal Outcome and Strategy" function chooses the (region, strategy) pair that makes the most competence progress.
$\cdot$ mode 2: choose the region $R_{n}(\sigma)$ and thus the strategy $\sigma$ with a probability proportional to its interest value interest $R_{n}(\sigma)$ :
$P_{n}(\sigma)=\frac{\operatorname{interest}<em t="t">{R</em>}}(\sigma)-\min \left(\text { interest <em t="t">{R</em>}}\right)}{\sum_{s=1}^{\left|R_{n}\right|} \operatorname{interest<em t="t">{R</em>}}(\sigma)-\min \left(\text { interest <em t="t">{R</em>$
A outcome $\tau$ is then generated randomly inside $R_{n}$. This mode uses exploitation to choose the region with highest interest measure.
$\cdot$ mode 3: the strategy and regions are selected like in mode 2, but the outcome $\tau \in R_{n}$ is generated close to the already experimented one which received the lowest competence estimation. This mode also uses exploitation to choose the best outcome and strategy with respect to interest measures.}}\right)</p>
<p>We illustrate in the following Section this hierarchical algorithm through an illustration example where a robot learns to throw a ball or to place it at different angles with 7 strategies: intrinsically motivated exploration, mimicry from 3 teachers and emulation from 3 teachers.</p>
<h2>4. Throwing and Placing a Ball</h2>
<h3>4.1. Experimental Setup</h3>
<p>In our simulated experimental setup, we have a 1 degree-of-freedom arm place a ball at different angles or throw the ball by controlling its angular acceleration $\ddot{\varphi}$ (fig. 1). The time evolution of its angular acceleration is described a motor primitives determined by 14 parameters, $\Pi \subset \mathbb{R}^{14}$ as described in 4.1.1. The outcome space is composed of 2 types of outcomes $T=T 1 \cup T 2$, that we detail in 4.1.2 and 4.1.3.</p>
<h3>4.1.1. Policy Parameter Space</h3>
<p>Starting from angle $\phi=0$, the robot can control its angular acceleration $\ddot{\varphi}$. Its movement is parameterised by $\left(\dot{\varphi}<em 1="1">{1}, t</em>}, \ldots \dot{\varphi<em 7="7">{7}, t</em>(t)$ as a piecewise constant function. The policy parameter space is arbitrarily set to a 14 dimensional space.}\right)$ which defines the acceleration of the arm for the 7 durations $t_{i}$. It thus defines $\dot{\varphi</p>
<h3>4.1.2. Throwing Outcomes</h3>
<p>The first type of outcomes is the different distance $x$ and height $h$ at which the ball $B$ can be thrown. $T 1={(\mathrm{x}, h)}$ is of dimension 2. The ball, initially in the robot's hand is first accelerated by the robot arm, and then automatically released:
at position $\overrightarrow{O B}<em t="0">{t=0}$ which is the position of the tip of the arm,
with velocity $\frac{d \overrightarrow{O B}}{d t}</em>$ which magnitude is the velocity of the arm, and which direction is the tangent of the arm movement.</p>
<p>Then, the ball falls under gravity force, described by the equation:</p>
<p>$$
\overrightarrow{O B}<em t="0">{t}=\frac{g}{2} \cdot t^{2}+\frac{d \overrightarrow{O B}}{d t}</em>
$$} \cdot t+\overrightarrow{O B}_{t=0</p>
<p>where $g$ is the gravity force. $x$ is therefore computed for $t_{\text {impact }}$, the time when the ball touches the ground, or in other words the solution to the 2nd polynomial equation:</p>
<p>$$
\frac{-g}{2} \cdot t^{2}+\frac{d z}{d t}<em t="0">{t=0} \cdot t+z</em>=0
$$</p>
<p>The maximum height is also directly computed by equation:</p>
<p>$$
h=z_{t=0}+\frac{\left(\frac{d \overrightarrow{O B}}{d t_{t=0}}\right)^{2}}{2 g}
$$</p>
<p>To make the throwing less trivial, we also added a wall as an obstacle at $x=10$. The ball can bounce on the wall using an immobile wall model and elastic collision.</p>
<h3>4.1.3. Placing Outcomes</h3>
<p>The second type of outcomes is placing a ball at different angles $\phi$. Therefore $T 2$ is of dimension 1. To achieve an outcome in $T 2$, the robot has to stop its arm in a direction $\phi$ before releasing the ball, i.e. it learns to reach $\phi$ at a small velocity $|v|&lt;\left|v_{\text {max }}\right|$.
Any policy would move the arm to a final angle $\phi$, but to "place" the ball at an angle, it also needs to reach a velocity smaller than $\left|v_{\text {max }}\right|$. Therefore placing a ball is difficult.
The robot learns which arm movement it needs to perform to either place at a given angle $\phi$ or to throw a ball at a given height and distance. Mathematically speaking, it learns highly redundant mappings between a 14-dimensional policy space and a union of a 1D and a 2D continuous outcome spaces.
In our experimental setup, the outcome space is thus the union of two continuous spaces of different dimensionalities, related to throwing and placing skills, which makes it complex because of the continuous and</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Comparison of several learning algorithms
composite nature of the space. The complexity of the placing of the ball depends on the physics of the body and on the structure of motor commands. We choose to control the robot by angular acceleration to emphasise the difference in the ease of control between the "throwing outcomes" which require rather a velocity control, and the "placing outcomes" which require rather a position control. Given the motor control by acceleration and the encoding of motor primitives, the placing outcomes are thus more difficult to achieve than the throwing outcomes.</p>
<h3>4.2. Several Teachers and Strategies</h3>
<p>We create simulated teachers by building 3 demonstration sets from which to pick a random demonstration when asked by the learner :</p>
<ul>
<li>teacher 1 has learned how to throw a ball with SAGG-RIAC. The teacher 1 has the same motor primitives encoding as the learner, and the robot observes from the demonstrated trajectories directly the demonstrated $\left(\dot{\phi}<em 1="1">{1}, t</em>}, \ldots \dot{\phi<em 7="7">{7}, t</em>\right)$.</li>
<li>teacher 2 is an expert in placing, programmed by an explicit equation to place at any angle with a null velocity. The teacher 2 too has the same motor primitives encoding as the learner, and the robot observes from the demonstrated trajectories directly the demonstrated $\left(\dot{\phi}<em 1="1">{1}, t</em>}, \ldots \dot{\phi<em 7="7">{7}, t</em>\right)$.</li>
<li>teacher 3 is an expert in placing, except that in this case the learner faces correspondence problems and misinterprets the two parameters $\dot{\phi}<em 7="7">{k}$ and $\dot{\phi}</em>$ as the opposite values. In this experiment, we do not attempt to solve this correspondence problem. We also note that while the learner has issues mimicking teacher 3, he has no issues emulating teacher 3, as the outcome space parametrisation is the same.</li>
</ul>
<p>Therefore in our experiment, the interactive learner can choose between 7 strategies : SAGG-RIAC autonomous exploration, emulation of each of the 3 teachers or mimicry of each of the 3 teachers.</p>
<h3>4.3. Comparison of Learning Algorithms</h3>
<p>To assess the efficiency of SGIM-ACTS, we decide to compare the performance of several learning algorithms (fig. 5):</p>
<ul>
<li>Random exploration : throughout the experiment, the robot learns by picking policy parameters randomly. It explores randomly the policy parameter space $\Pi$.</li>
<li>SAGG-RIAC : throughout the experiment, the robot uses active goal-babbling to explore autonomously, without taking into account any demonstration by the teacher, and is driven by intrinsic motivation.</li>
<li>mimicry : at a regular frequency, the learner determines a goal $\tau_{g}$ where learning progress is maximal, and requests to the chosen teacher a demonstration. The teacher selects
<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
</ul>
<p>Figure 6. Mean error for the different learning algorithms averaged over the two sub outcome spaces (final variance value $\Delta$ is indicated in the legend)
among his data set a demonstration $\left[\zeta_{d}, \tau_{d}\right]$ so that $\tau_{d}=$ $\arg \min <em g="g">{\tau \in[D e m a S e r]}\left|\tau</em>$ by repeating the movement with small variations.
emulation : at a regular frequency, the learner determines a goal $\tau_{g}$ where is learning progress is maximal, and requests to the chosen teacher a demonstration. The teacher selects among his data set a demonstration $\left[\zeta_{d}, \tau_{d}\right]$ so that $\tau_{d}=$ $\arg \min }-\tau\right|$. The learner mimics the demonstrated policy $\zeta_{d<em g="g">{\tau \in[D e m a S e r]}\left|\tau</em>$.
- SGIM-ACTS : interactive learning where the robot learns by actively choosing between intrinsic motivation strategy or one of the social learning strategies with the chosen teacher: mimicking or emulation.}-\tau\right|$. The learner tries to reproduce the outcome $\tau_{d</p>
<p>We run simulations with the following parameters. The costs of all socially guided strategies $\boldsymbol{\kappa}(\boldsymbol{\sigma})$ are set to 2 , and the cost of intrinsic motivation is set to 1 . The probabilities for the different modes of selecting a region of the outcome space and a strategy are: $\mathrm{p} 1=0.05, \mathrm{p} 2=0.7$ and $\mathrm{p} 3=0.25$. Other parameters are $\boldsymbol{\epsilon}=0.05, \boldsymbol{g}<em p="p">{\max }=10, \sigma</em>=0.01$.
For each experiment, we let the robot perform 8000 actions in total, and evaluate its performance every 1000 actions, by requiring the system to produce outcomes from a benchmark set that is evenly distributed in the outcome space and independent from the learning data.}=1000$ and $v_{\max </p>
<h3>4.4. Results</h3>
<p>The comparison of these four learning algorithms in Fig. 6 shows that SGIM-ACTS decreases its cumulative error for both placing and throwing. It performs better than autonomous exploration by random or intrinsic motivation, and better than any socially guided exploration with any teacher. Fig. 7 details that SGIM-ACTS error rate for both placing and throwing is low. For throwing, SGIM-ACTS performs the best in terms of error rate and speed because it could find the right strategy. We also note that random exploration and SAGG-RIAC also perform well</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Mean error for the different learning algorithms for each of the throwing outcomes and placing outcomes separately. The legend is the same as in Fig. 6.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Strategy chosen by by SGIM-ACTS through time: percentage of times each strategy is chosen for several runs of the experiment.
for solving the 2nd degree polynomial equation (5) to achieve throwing outcomes. While mimicking and emulating teacher 1 decreases the error as expected, mimicking and emulating a teacher who is expert in another kind of outcomes and is bad in that outcome leaves a high error rate. For placing, SGIM-ACTS makes less error than all other algorithms. Indeed, as we expected, mimicking the teacher 2, and emulating teachers 2 and 3 enhances low error rates, while mimicking a teacher with correspondence problem (teacher 3) or an expert on an-
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Outcome chosen by SGIM-ACTS through time: percentage of times each kind of outcome is chosen for several runs of the experiment.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Consistency in the choice of outcome, teacher and strategy: percentage of times each strategy, teacher and outcome are chosen over all the history of the robot.
other outcome (teacher 1) gives poor result. We also note that for both outcomes, mimicry does not lead to important learning progress, and the error curve is almost flat. This is due to the lack of exploration which leads the learner to ask demonstrations for outcomes only in a small subspace.
Indeed, we see in Fig. 8 which illustrates the percentage times each strategy is chosen by SGIM-ACTS with respect to time, that mimicry of teacher 3, which lacks efficiency because of the correspondence problem, is seldom chosen by SGIM-ACTS. Mimicry and emulation of teacher 1 is also little used because autonomous learning learns quickly throwing outcomes. Teachers 2 and 3 are exactly the same with respect to the outcomes they demonstrate, and are emulated in the same proportion. This figure also shows that the more the learner cumulates knowledge, the more autonomous he grows : his percentage of autonomous learning increases steadily.
Not only does he choose the right strategies, but also the right outcome to concentrate on. Fig. 9 shows that he concentrates in the end more on placing, which are more difficult.
Finally, Fig. 10 shows the percentage of times over all the experiments where he chooses at the same time each outcome type, a strategy and a teacher. We can see that for the placing outcomes, he seldom requests help from the teacher 1, as he learns that teacher 1 does not</p>
<p>know how to place the ball. Likewise, because of the correspondence problems, he does not mimic teacher 3 . But he learns that mimicking teacher 2 and emulating teachers 2 and 3 are useful for placing outcomes. For the throwing outcomes, he uses slightly more the autonomous exploration strategy, as he can learn efficiently by himself. The high percentage for the other strategies is due to the fact that the throwing outcomes are easy to learn, therefore are learned in the beginning when a lot of sampling of all possible strategies is carried out. SGIM-ACTS is therefore consistent in its choice of outcomes, data collection strategies and teachers.</p>
<h2>5. Conclusion and Discussion</h2>
<p>We presented the SGIM-ACTS (Socially Guided Intrinsic Motivation with Active Choice of Teacher and Strategy) algorithm that efficiently and actively combines autonomous self-exploration and interactive learning, to address the learning of multiple outcomes, with outcomes of different types, and with different data collection strategies. In particular, it learns actively to decide on the fundamental questions of programming by demonstration: what and how to learn; but also what, how, when and who to imitate. This interactive learner decides efficiently and coherently whether to use social guidance. It learns when to ask for demonstration, what kind of demonstrations (action to mimic or outcome to emulate) and who to ask for demonstrations, among the available teachers. Its hierarchical architecture bears three levels. The lower level explores the policy parameters space to build skills for determined goal outcomes. The upper level explores the outcome space to evaluate for which outcomes he makes the best progress. A meta-level actively chooses the outcome and data collection strategy that leads to the best competence progress. We showed through our illustration example that SGIM-ACTS can focus on the outcome where it learns the most, while choosing the most appropriate associated data collection strategy. The active learner can explore efficiently a composite and continuous outcome space to be able to generalise for new outcomes of the outcome spaces.
SGIM-ACTS has been shown an efficient method for learning with multiple teachers and multiple outcome types. The number of outcomes used in the experiment is infinite, with a continuous outcome space that is made of 2 types of outcomes, but all the formalism and framework is in principle scalable to a higher number of types of outcomes. Likewise, the method should apply to domestic or industrial robots who usually interact with a finite number of teachers. Even in the case of correspondence problems, the system still takes advantage of the demonstrations to bias its exploration of the outcome space. When the discrepancies between the teacher and the learner are small, demonstrations advantageously bias the exploration of the outcome space, as argued in [30]. Future work should test SGIM-ACTS on more complex environments, and with real physical robots and everyday human users. It would also be interesting to compare the outcomes selected by our system to developmental behavioural studies, and highlight developmental trajectories.</p>
<h2>Acknowledgement</h2>
<p>This work was supported by the French ANR program (ANR 2010 BLAN 0216 01) through Project MACSi, as well by ERC Starting Grant EXPLORERS 240007.</p>
<h2>References</h2>
<p>[1] Brenna D. Argall, B. Browning, and Manuela Veloso. Learning robot motion control with demonstration and advice-operators. In In Proceedings IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 399-404. IEEE, September 2008.
[2] Brenna D. Argall, B. Browning, and Manuela Veloso. Teacher feedback to scaffold and refine demonstrated motion primitives on a mobile robot. Robotics and Autonomous Systems, 59(34):243-255, 2011.
[3] Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5):469 - 483, 2009.
[4] C.G. Atkeson, Moore Andrew, and Schaal Stefan. Locally weighted learning. AI Review, 11:11-73, April 1997.
[5] Y. Baram, R. El-Yaniv, and K. Luz. Online choice of active learning algorithms. The Journal of Machine Learning Research,, 5:255-291, 2004.
[6] Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.
[7] Andrew G. Barto, S. Singh, and N Chentanez. Intrinsically motivated learning of hierarchical collections of skills. In ICDE International Conference on Developmental Learning, pages 112-119, 2004.
[8] Aude Billard, Sylvain Calinon, Ruediger Dillmann, and Stefan Schaal. Handbook of Robotics, chapter Robot Programming by Demonstration. Number 59. MIT Press, 2007.
[9] Cynthia Breszeal and B. Scassellati. Robots that imitate humans. Trends in Cognitive Sciences, 6(11):481-487, 2002.
[10] Maya Cakmak, C. Chao, and Andrea L. Thomaz. Designing interactions for robot active learners. Autonomous Mental Development, IEEE Transactions on, 2(2):108-118, 2010.
[11] Maya Cakmak, Nick DePalma, Andrea L. Thomaz, and Rosa Arriaga. Effects of social exploration mechanisms on robot learning. In The 18th IEEE International Symposium on Robot and Human Interactive Communication, 2009. RO-MAN 2009., pages 128-134. IEEE, 2009.
[12] J. Call and M. Carpenter. Imitation in animals and artifacts, chapter Three sources of information in social learning, pages 211-228. Cambridge, MA: MIT Press., 2002.
[13] Sonia Chernova and Manuela Veloso. Interactive policy learning through confidence-based autonomy. Journal of Artificial Intelligence Research, 34, 2009.
[14] Gergely Csibra. Teleological and referential understanding of action in infancy. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 358(1431):447, 2003.
[15] B.C. da Silva, G. Konidaris, and Andrew G. Barto. Learning parameterized skills. In 29th International Conference on Machine Learning (ICML 2012), 2012.
[16] Kerstin Dautenhahn and Chrystopher L. Nehaniv. Imitation in Animals and Artifacts. MIT Press, 2002.
[17] Daniel H Grollman and Odest Chadwicke" Jenkins. Incremental learning of subtasks from unsegmented demonstration. In Intelligent Robots and Systems IROS 2010 IEEERSJ International Conference on, pages 261-266, 2010.
[18] Jens Kober, Andreas Wilhelm, Ethan Oztop, and Jan Peters. Reinforcement learning to adjust parametrized motor primitives to new situations. Autonomous Robots, pages 1-19, 2012.</p>
<p>10.1007/s10514-012-9290-3.
[19] N Koenig, L Takayama, and M Matariq. Communication and knowledge sharing in human-robot interaction and learning from demonstration. Neural Netw, 23(8-9):1104-1112, Oct-Nov 2010.
[20] Petar Kormushev, Sylvain Calinon, and Darwin G. Caldwell. Robot motor skill coordination with EM-based reinforcement learning. In Proc. IEEE/RSJ Intl Conf. on Intelligent Robots and Systems (IROS), pages 3232-3237, Taipei, Taiwan, October 2010.
[21] Petar Kormushev, Sylvain Calinon, and Darwin G. Caldwell. Imitation learning of positional and force skills demonstrated via kinesthetic teaching and haptic input. Advanced Robotics, 25(5):581-603, 2011.
[22] J. C. Lagarias, J. A. Reeds, M. H. Wright, and P. E. Wright. Convergence properties of the nelder-mead simplex method in low dimensions. SIAM Journal of Optimization, 9(1):112-147, 1998.
[23] Manuel Lopes, Thomas Cederborg, and Pierre-Yves Oudeyer. Simultaneous acquisition of task and feedback models. Development and Learning (ICDL), 2011 IEEE International Conference on, pages $1-7,2011$.
[24] Manuel Lopes, Tobias Lang, Marc Toussaint, Pierre-Yves Oudeyer, et al. Exploration in model-based reinforcement learning by empirically estimating learning progress. In Neural Information Processing Systems (NIPS), 2012.
[25] Manuel Lopes, Francisco Melo, and Luis Montesano. Active learning for reward estimation in inverse reinforcement learning. Machine Learning and Knowledge Discovery in Databases, pages 31-46, 2009.
[26] Manuel Lopes, Francisco Melo, Luis Montesano, and Jose Santos-Victor. From Motor to Interaction Learning in Robots, chapter Abstraction Levels for Robotic Imitation: Overview and Computational Approaches. Springer, 2009.
[27] Manuel Lopes and Pierre-Yves Oudeyer. The Strategic Student Approach for Life-Long Exploration and Learning. In IEEE Conference on Development and Learning / EpiRob, San Diego, États-Unis, November 2012.
[28] Chrystopher L Nehaniv and Kerstin Dautenhahn. Imitation and Social Learning in Robots, Humans and Animals: Behavioural, Social and Communicative Dimensions. Cambridge Univ. Press, Cambridge, March 2007.
[29] Sao Mai Nguyen, Adrien Baranes, and Pierre-Yves Oudeyer. Bootstrapping intrinsically motivated learning with human demonstrations. In IEEE International Conference on Development and Learning, Frankfurt, Germany, 2011.
[30] Sao Mai Nguyen and Pierre-Yves Oudeyer. Properties for efficient demonstrations to a socially guided intrinsically motivated learner. In 21st IEEE International Symposium on Robot and Human</p>
<p>Interactive Communication, 2012.
[31] M.N. Nicolescu and M.J. Mataric. Natural methods for robot task learning: Instructive demonstrations, generalization and practice. In Proceedings of the second international joint conference on Autonomous agents and multiagent systems, pages 241-248. ACM, 2003.
[32] Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in Neurorobotics, 2007.
[33] Pierre-Yves Oudeyer, Frederic Kaplan, and Verena Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265-286, 2007.
[34] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural Networks, 21(4):682-697, 2008.
[35] G. Qi, X. Hua, Y. Rui, J. Tang, and H. Zhang. Two-dimensional active learning for image classification. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008.
[36] Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. Multi-task active learning for linguistic annotations. In Annual Meeting of the Association for Computational Linguistics (ACL). Citeseer, 2008.
[37] Matthias Rolf and Jochen J Steil. Goal babbling: a new concept for early sensorimotor exploration. pages 40-43, Osaka, 11/2012 2012. IEEE.
[38] Richard M. Ryan and Edward L. Deci. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary Educational Psychology, 25(1):54 - 67, 2000.
[39] Stefan Schaal, A ljspeert, and Aude Billard. Computational approaches to motor learning by imitation. Philosophical transactions of the Royal Society of London. Series B, Biological sciences, 358(1431), 032003.
[40] Andrea L. Thomaz. Socially Guided Machine Learning. PhD thesis, MIT, 52006.
[41] Andrea L. Thomaz and Cynthia Breazeal. Experiments in socially guided exploration: Lessons learned in building robots that learn with and without human teachers. Connection Science, 20 Special Issue on Social Learning in Embodied Agents(2-3):91-110, 2008.
[42] M. Tomasello and M. Carpenter. Shared intentionality. Developmental Science, 10(1):121-125, 2007.
[43] Andrew Whiten. Primate culture and social learning. Cognitive Science, 24(3):477-508, 2000.</p>            </div>
        </div>

    </div>
</body>
</html>
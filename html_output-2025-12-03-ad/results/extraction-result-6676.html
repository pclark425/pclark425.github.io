<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6676 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6676</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6676</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-cfc22be4649a8ec692ddf71688a8b52a416a3da6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cfc22be4649a8ec692ddf71688a8b52a416a3da6" target="_blank">What Can You Do with a Rock? Affordance Extraction via Word Embeddings</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This paper applies a method for affordance extraction via word embeddings trained on a Wikipedia corpus to a reinforcement learning agent in a text-only environment and shows that affordance-based action selection improves performance most of the time.</p>
                <p><strong>Paper Abstract:</strong> Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6676.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6676.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Affordance Q-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Affordance-based Q-learning agent (word2vec affordance extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Q-learning agent that uses word2vec-derived affordance vectors to reduce verb and noun action spaces per state in text-based adventure games, selecting affordant verbs and manipulable nouns to speed exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>What Can You Do with a Rock? Affordance Extraction via Word Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Affordance-based Q-learner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Q-learning agent whose state is a hash of the game's textual observation; actions are verb+object pairs. Uses word2vec embeddings (trained on Wikipedia with POS tags) to (a) produce an affordance vector from canonical verb-noun exemplars to query affordant verbs for a given noun and (b) project nouns to identify manipulable objects, thereby reducing action/object search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Infocom Z-machine suite (50 text-based adventure games, including Zork)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (tabular Q-learning; alpha=0.1, gamma=0.95)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Change in game score (score delta), number of games with nonzero score, peak performance expressed as % of possible game score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline agent (1000-verb manipulation list) scored on 12/49 games, peak performance 20.84% of possible score. Affordance-based reductions: verb-space reduction scored on 18/49 games with peak 57.44%; object reduction peak 23.90%; combined verb+object reduction peak 56.84%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>The paper compares affordance-based verb/object reductions against several alternative reduction strategies (random selection, ConceptNet CapableOf, co-occurrence counts, intrinsic rewards). Affordance-based verb selection improved performance on most evaluated games (improved on 13/18 games shown) while other reduction methods sometimes pruned essential actions. No ablation was performed on any explicit memory component because the agent did not incorporate one.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Agent maintained no explicit inventory representation or game-score representation; could not construct prepositional commands; did not model physical or temporal dependencies (possession or past actions do not persist as symbolic memory); affordance extraction is sensitive to polysemy (e.g., 'apple' dominated by the company). Also did not implement explicit memory mechanisms or item-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use word embeddings as a common-sense knowledge base and average multiple canonical exemplars to form affordance vectors; use exemplar differences (analogy queries) to extract verbs for nouns and to project nouns for manipulability. The paper contains no explicit recommendations about adding or designing memory modules for text-game agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can You Do with a Rock? Affordance Extraction via Word Embeddings', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6676.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6676.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Narasimhan LSTM-DQN (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language understanding for textbased games using deep reinforcement learning (Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) that passes the game's textual output through an LSTM to obtain a state representation and uses a DQN to select actions; reported to work in small discrete environments but to degrade as environment complexity grows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for textbased games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>What Can You Do with a Rock? Affordance Extraction via Word Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM + DQN agent (as described by Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described briefly in related work: game text is processed by an LSTM to produce a learned state representation (embedding), which is fed to a Deep Q-Network to estimate Q-values over actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM encoder + DQN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>LSTM hidden state / learned state embedding derived from the sequence of recent game text</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Implicit recurrent update via LSTM cell dynamics (hidden and cell state updates as text is processed)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>No explicit retrieval; the current LSTM hidden state (embedding) is used directly by the DQN to select actions</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (Deep Q-Network with LSTM-based state encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Q-values and overall game reward; paper reports qualitative behavior and reward trends (no numeric metrics quoted here by this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>According to this paper's brief summary, the LSTM+DQN approach 'appeared to work well within a small discrete environment with reliable state action pairs', but as environment complexity and alphabet grew, 'the clarity of Q-values broke down and left them with a negative overall reward.' No numerical values are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Mentioned scaling issues: as the size/complexity of the environment and action/state alphabet increased, the learned Q-values became unclear and performance degraded (negative overall reward). The paper provides no further memory ablations or detailed analysis of the LSTM memory mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>None in this paper — the reference is only cited briefly; no explicit guidance about memory design beyond the architecture description (LSTM for state encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can You Do with a Rock? Affordance Extraction via Word Embeddings', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for textbased games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Long short-term memory <em>(Rating: 1)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Human-level AI's killer application: Interactive computer games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6676",
    "paper_id": "paper-cfc22be4649a8ec692ddf71688a8b52a416a3da6",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "Affordance Q-Agent",
            "name_full": "Affordance-based Q-learning agent (word2vec affordance extraction)",
            "brief_description": "A Q-learning agent that uses word2vec-derived affordance vectors to reduce verb and noun action spaces per state in text-based adventure games, selecting affordant verbs and manipulable nouns to speed exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
            "agent_name": "Affordance-based Q-learner",
            "agent_description": "Q-learning agent whose state is a hash of the game's textual observation; actions are verb+object pairs. Uses word2vec embeddings (trained on Wikipedia with POS tags) to (a) produce an affordance vector from canonical verb-noun exemplars to query affordant verbs for a given noun and (b) project nouns to identify manipulable objects, thereby reducing action/object search spaces.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Infocom Z-machine suite (50 text-based adventure games, including Zork)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Reinforcement learning (tabular Q-learning; alpha=0.1, gamma=0.95)",
            "evaluation_metric": "Change in game score (score delta), number of games with nonzero score, peak performance expressed as % of possible game score",
            "performance_with_memory": null,
            "performance_without_memory": "Baseline agent (1000-verb manipulation list) scored on 12/49 games, peak performance 20.84% of possible score. Affordance-based reductions: verb-space reduction scored on 18/49 games with peak 57.44%; object reduction peak 23.90%; combined verb+object reduction peak 56.84%.",
            "has_comparative_results": false,
            "ablation_findings": "The paper compares affordance-based verb/object reductions against several alternative reduction strategies (random selection, ConceptNet CapableOf, co-occurrence counts, intrinsic rewards). Affordance-based verb selection improved performance on most evaluated games (improved on 13/18 games shown) while other reduction methods sometimes pruned essential actions. No ablation was performed on any explicit memory component because the agent did not incorporate one.",
            "reported_limitations": "Agent maintained no explicit inventory representation or game-score representation; could not construct prepositional commands; did not model physical or temporal dependencies (possession or past actions do not persist as symbolic memory); affordance extraction is sensitive to polysemy (e.g., 'apple' dominated by the company). Also did not implement explicit memory mechanisms or item-tracking.",
            "best_practices_recommendations": "Use word embeddings as a common-sense knowledge base and average multiple canonical exemplars to form affordance vectors; use exemplar differences (analogy queries) to extract verbs for nouns and to project nouns for manipulability. The paper contains no explicit recommendations about adding or designing memory modules for text-game agents.",
            "uuid": "e6676.0",
            "source_info": {
                "paper_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Narasimhan LSTM-DQN (mentioned)",
            "name_full": "Language understanding for textbased games using deep reinforcement learning (Narasimhan et al., 2015)",
            "brief_description": "Prior work (cited) that passes the game's textual output through an LSTM to obtain a state representation and uses a DQN to select actions; reported to work in small discrete environments but to degrade as environment complexity grows.",
            "citation_title": "Language understanding for textbased games using deep reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
            "agent_name": "LSTM + DQN agent (as described by Narasimhan et al., 2015)",
            "agent_description": "Described briefly in related work: game text is processed by an LSTM to produce a learned state representation (embedding), which is fed to a Deep Q-Network to estimate Q-values over actions.",
            "model_name": "LSTM encoder + DQN",
            "model_size": null,
            "benchmark_name": null,
            "memory_used": true,
            "memory_type": "recurrent hidden state",
            "memory_representation": "LSTM hidden state / learned state embedding derived from the sequence of recent game text",
            "memory_update_mechanism": "Implicit recurrent update via LSTM cell dynamics (hidden and cell state updates as text is processed)",
            "memory_retrieval_method": "No explicit retrieval; the current LSTM hidden state (embedding) is used directly by the DQN to select actions",
            "training_method": "Reinforcement learning (Deep Q-Network with LSTM-based state encoder)",
            "evaluation_metric": "Q-values and overall game reward; paper reports qualitative behavior and reward trends (no numeric metrics quoted here by this paper)",
            "performance_with_memory": "According to this paper's brief summary, the LSTM+DQN approach 'appeared to work well within a small discrete environment with reliable state action pairs', but as environment complexity and alphabet grew, 'the clarity of Q-values broke down and left them with a negative overall reward.' No numerical values are provided in this paper.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Mentioned scaling issues: as the size/complexity of the environment and action/state alphabet increased, the learned Q-values became unclear and performance degraded (negative overall reward). The paper provides no further memory ablations or detailed analysis of the LSTM memory mechanism.",
            "best_practices_recommendations": "None in this paper — the reference is only cited briefly; no explicit guidance about memory design beyond the architecture description (LSTM for state encoding).",
            "uuid": "e6676.1",
            "source_info": {
                "paper_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for textbased games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Long short-term memory",
            "rating": 1
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Human-level AI's killer application: Interactive computer games",
            "rating": 1
        }
    ],
    "cost": 0.011886,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Can You Do with a Rock? Affordance Extraction via Word Embeddings</h1>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch and David Wingate<br>Brigham Young University<br>nfulda@byu.edu, daniel_ricks@byu.edu, murdoch@byu.edu, wingated@byu.edu</p>
<h4>Abstract</h4>
<p>Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a tagged Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance in most cases. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.</p>
<h2>1 Introduction</h2>
<p>The physical world is filled with constraints. You can open a door, but only if it isn't locked. You can douse a fire, but only if a fire is present. You can throw a rock or drop a rock or even, under certain circumstances, converse with a rock, but you cannot traverse it, enumerate it, or impeach it. The term affordances [Gibson, 1977] refers to the subset of possible actions which are feasible in a given situation. Human beings detect these affordances automatically, often subconsciously, but it is not uncommon for autonomous learning agents to attempt impossible or even ridiculous actions, thus wasting effort on futile behaviors.</p>
<p>This paper presents a method for affordance extraction based on the copiously available linguistic information in online corpora. Word embeddings trained using Wikipedia articles are treated as a common sense knowledge base that encodes (among other things) object-specific affordances. Because knowledge is represented as vectors, the knowledge base can be queried using linear algebra. This somewhat counterintuitive notion - the idea that words can be manipulated mathematically - creates a theoretical bridge between the frustrating realities of real-world systems and the im-
mense wealth of common sense knowledge implicitly encoded in online corpora.</p>
<p>We apply our technique to a text-based environment and show that a priori knowledge provided by affordance extraction greatly speeds learning. Specifically, we reduce the agent's search space by (a) identifying actions afforded by a given object; and (b) discriminating objects that can be grasped, lifted and manipulated from objects which can merely be observed. Because the agent explores only those actions which 'make sense', it is able to discover valuable behaviors more quickly than a comparable agent using a brute force approach. Critically, the affordance agent is demonstrably able to eliminate extraneous actions without (in most cases) discarding beneficial ones.</p>
<p>All the experiments in this paper were performed using a Wikipedia corpus that had been preprocessed to append part-of-speech tagging to each word.</p>
<h2>2 Related Work</h2>
<p>Our research relies heavily on word2vec [Mikolov et al., 2013a], an algorithm that encodes individual words based on the contexts in which they tend to appear. Earlier work has shown that word vectors trained using this method contain intriguing semantic properties, including structured representations of gender and geography [Mikolov et al., 2013b; 2013c]. The (by now) archetypal example of such properties is represented by the algebraic expression vector['king'] vector $[$ 'man'] + vector $[$ 'woman'] $=$ vector $[$ 'queen'].</p>
<p>Researchers have leveraged these properties for diverse applications including sentence- and paragraph-level encoding [Kiros et al., 2015; Le and Mikolov, 2014], image categorization [Frome et al., 2013], bidirectional retrieval [Karpathy et al., 2014], semantic segmentation [Socher et al., 2011], biomedical document retrieval [Brokos et al., 2016], and the alignment of movie scripts to their corresponding source texts [Zhu et al., 2015]. Our work is most similar to [Zhu et al., 2014]; however, rather than using a Markov Logic Network to build an explicit knowledge base, we instead rely on the semantic structure implicitly encoded in skip-grams.</p>
<p>Affordance detection, a topic of rising importance in our increasingly technological society, has been attempted and/or accomplished using visual characteristics [Song et al., 2011; 2015], haptic data [Navarro et al., 2012], visuomotor simulation [Schenck et al., 2012; 2016], repeated real-world ex-</p>
<p>perimentation [Montesano et al., 2007; Stoytchev, 2008], and knowledge base representations [Zhu et al., 2014].</p>
<p>In 2001 [Laird and van Lent, 2001] identified text-based adventure games as a step toward general problem solving. The same year at AAAI, Mark DePristo and Robert Zubek unveiled a hybrid system for text-based game play [Arkin, 1998], which operated on hand-crafted logic trees combined with a secondary sensory system used for goal selection. The handcrafted logic worked well, but goal selection broke down and became cluttered due to the scale of the environment. Perhaps most notably, in 2015 [Narasimhan et al., 2015] designed an agent which passed the text output of the game through an LSTM [Hochreiter and Schmidhuber, 1997] to find a state representation, then used a DQN [Mnih et al., 2015] to select a Q-valued action. This approach appeared to work well within a small discrete environment with reliable state action pairs, but as the complexity and alphabet of the environment grew, the clarity of Q-values broke down and left them with a negative overall reward. Our work, in contrast, is able to find meaningful state action pairs even in complex environments with many possible actions.</p>
<h2>3 Wikipedia as a Common Sense Knowledge Base</h2>
<p>Google 'knowledge base', and you'll get a list of hand-crafted systems, both commercial and academic, with strict constraints on encoding methods. These highly-structured, often node-based solutions are successful at a wide variety of tasks including topic gisting [Liu and Singh, 2004], affordance detection [Zhu et al., 2014] and general reasoning [Russ et al., 2011]. Traditional knowledge bases are human-interpretable, closely tied to high-level human cognitive functions, and able to encode complex relationships compactly and effectively.</p>
<p>It may seem strange, then, to treat Wikipedia as a knowledge base. When compared with curated solutions like ConceptNet [Liu and Singh, 2004], Cyc [Matuszek et al., 2006], and WordNet [Miller, 1995], its contents are largely unstructured, polluted by irrelevant data, and prone to user error. When used as a training corpus for the word2vec algorithm, however, Wikipedia becomes more tractable. The word vectors create a compact representation of the knowledge base and, as observed by [Bolukbasi et al., 2016a] and [Bolukbasi et al., 2016b], can even encode relationships about which a human author is not consciously cognizant. Perhaps most notably, Wikipedia and other online corpora are constantly updated in response to new developments and new human insight; hence, they do not require explicit maintenance.</p>
<p>However: in order to leverage the semantic structure implicitly encoded within Wikipedia, we must be able to interpret the resulting word vectors. Significant semantic relationships are not readily apparent from the raw word vectors or from their PCA reduction. In order to extract useful information, the database must be queried through a mathematical process. For example, in Figure 1 a dot product is used to project gendered terms onto the space defined by vector $[$ 'king'] - vector $[$ queen'] and vector $[$ woman $]-$ vector $[$ 'man']. In such a projection, the mathematical relationship between the words is readily apparent. Masculine
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Word vectors projected into the space defined by vector ['king'] - vector ['queen'] and vector ['woman'] vector ['man']. In this projection, masculine and feminine terms are linearly separable.
and feminine terms become linearly separable, making it easy to distinguish instances of each group.</p>
<p>These relationships can be leveraged to detect affordances, and thus reduce the agent's search space. In its most general interpretation, the adjective affordant describes the set of actions which are physically possible under given conditions. In the following subsections, however, we use it in the more restricted sense of actions which seem reasonable. For example, it is physically possible to eat a pencil, but it does not 'make sense' to do so.</p>
<h3>3.1 Verb/Noun Affordances</h3>
<p>So how do you teach an algorithm what 'makes sense'? We address this challenge through an example-based query. First we provide a canonical set of verb/noun pairs which illustrate the relationship we desire to extract from the knowledge base. Then we query the database using the analogy format presented by [Mikolov et al., 2013a]. Using their terminology, the analogy sing:song::[?]:[x] encodes the following question: If the affordant verb for 'song' is 'sing', then what is the affordant verb for [x]?</p>
<p>In theory, a single canonical example is sufficient to perform a query. However, experience has shown that results are better when multiple canonical values are averaged.</p>
<p>More formally, let $W$ be the set of all English-language word vectors in our agent's vocabulary. Further, let $N=$ $\left{\vec{n}<em j="j">{1}, \ldots, \vec{n}</em>}\right}, N \subset W$ be the set of all nouns in $W$ and let $V=\left{\vec{v<em k="k">{1}, \ldots, \vec{v}</em>\right}, V \subset W$ be the set of all verbs in $W$.</p>
<p>Let $C=\left{\left(\vec{v}<em 1="1">{1}, \vec{n}</em>}\right), \ldots,\left(\vec{v<em m="m">{m}, \vec{n}</em>}\right)\right}$ represent a set of canonical verb/noun pairs used by our algorithm. We use $C$ to define an affordance vector $\vec{a}=1 / m \sum_{i}\left(\vec{v<em i="i">{i}-\vec{n}</em>\right)$, which can be thought of as the distance and direction within the embedding space which encodes affordant behavior.</p>
<p>In our experiments we used the following verb/noun pairs as our canonical set:
['sing song', 'drink water', 'read book', 'eat food',</p>
<p>| Our algorithm | Co-occurrence | Concept Net |
| :-- | :-- | :-- | :-- |
| vanquish | impale | have | die | kill harm |
| duel | battle | make | cut | parry fence |
| unsheath | behead | kill fight | strike thrust |
| summon | wield | move use | slash injure |
| overpower | cloak | destroy be look cool cut |  |</p>
<p>Figure 2: Verb associations for the noun 'sword' using three different methods: (1) Affordance detection using word vectors extracted from Wikipedia, as described in this section, (2) Strict co-occurrence counts using a Wikipedia corpus and a co-occurrence window of 9 words, (3) Results generated using ConceptNet's CapableOf relationship.
'wear coat', 'drive car', 'ride horse', 'give gift', 'attack enemy', 'say word', 'open door', 'climb tree', 'heal wound', 'cure disease', 'paint picture']</p>
<p>We describe a verb/noun pair $(\vec{v}, \vec{n})$ as affordant to the extent that $\vec{n}+\vec{a}=\vec{v}$. Therefore, a typical knowledge base query would return the $n$ closest verbs $\left{\vec{v}<em c="c" n="n">{c 1}, \ldots, \vec{v}</em>$}\right}$ to the point $\vec{n}+\vec{a</p>
<p>For example, using the canonical set listed above and a set of pre-trained word vectors, a query using $\vec{n}=$ vector['sword'] returns the following:
['vanquish', 'duel', 'unsheathe', 'wield', 'summon', 'behead', 'battle', 'impale', 'overpower', 'cloak']
Intuitively, this query process produces verbs which answer the question, 'What should you do with an [x]?'. For example, when word vectors are trained on a Wikipedia corpus with part-of-speech tagging, the five most affordant verbs to the noun 'horse' are {'gallop', 'ride', 'race', 'horse', 'outrun'}, and the top five results for 'king' are {'dethrone', 'disobey', 'depose', 'reign', 'abdicate'}.</p>
<p>The resulting lists are surprisingly logical, especially given the unstructured nature of the Wikipedia corpus from which the vector embeddings were extracted. Subjective examination suggests that affordances extracted using Wikipedia are at least as relevant as those produced by more traditional methods (see Figure 2).</p>
<p>It is worth noting that our algorithm is not resilient to polysemy, and behaves unpredictably when multiple interpretations exist for a given word. For example, the verb 'eat' is highly affordant with respect to most food items, but the twelve most salient results for 'apple' are {'apple', 'package', 'program', 'release', 'sync', 'buy', 'outsell', 'download', 'install', 'reinstall', 'uninstall', 'reboot'}. In this case, 'Apple, the software company' is more strongly represented in the corpus than 'apple, the fruit'.</p>
<h3>3.2 Identifying Graspable Objects</h3>
<p>Finding a verb that matches a given noun is useful. But an autonomous agent is often confronted with more than one object at a time. How should it determine which object to manipulate, or whether any of the objects are manipulable? Pencils, pillows, and coffee mugs are easy to grasp and lift, but the same cannot be said of shadows, boulders, or holograms.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Word vectors projected into the space defined by vector['forest'] - vector['tree'] and vector['mountain'] vector['pebble']. Small, manipulable objects appear in the lowerleft corner of the graph. Large, abstract, or background objects appear in the upper right. An object's manipulability can be roughly estimated by measuring its location along either of the defining axes.</p>
<p>To identify affordant nouns - i.e. nouns that can be manipulated in a meaningful way - we again utilize analogies based on canonical examples. In this section, we describe a noun as affordant to the extent that it can be pushed, pulled, grasped, transported, or transformed. After all, it would not make much sense to lift a sunset or unlock a cliff.</p>
<p>We begin by defining canonical affordance vectors $\vec{a}<em 1="1" x="x">{x}=$ $\vec{n}</em>}-\vec{n<em y="y">{x 2}$ and $\vec{a}</em>}=\vec{n<em 2="2" y="y">{y 1}-\vec{n}</em>}$ for each axis of the affordant vector space. Then, for each object $\vec{o<em o__i="o_{i" x="x">{i}$ under consideration, we generate a pair of projections $\vec{p}</em>}}=\vec{o<em x="x">{i} \operatorname{dot} \vec{a}</em>}$ and $\vec{p<em i="i" y="y">{o</em>}}=$ $\vec{o<em y="y">{i} \operatorname{dot} \vec{a}</em>$.</p>
<p>The results of such a projection can be seen in Figure 3. This query is distinct from those described in section 3.1 because, instead of using analogies to test the relationships between nouns and verbs, we are instead locating a noun on the spectrum defined by two other words.</p>
<p>In our experiments, we used a single canonical vector, vector['forest'] - vector['tree'], to distinguish between nouns of different classes. Potentially affordant nouns were projected onto this line of manipulability, with the words whose projection lay closest to 'tree' being selected for further experimentation.</p>
<p>Critical to this approach is the insight that canonical word vectors are most effective when they are thought of as exemplars rather than as descriptors. For example, vector['forest'] - vector['tree'] and vector['building'] - vector['brick'] function reasonably well as projections for identifying manipulable items, whereas vector['big'] - vector['small'] is utterly ineffective.</p>
<h2>4 Test Environment: A World Made of Words</h2>
<p>In this paper, we test our ideas in the challenging world of text-based adventure gaming. Text-based adventure games offer an unrestricted, free-form interface: the player is presented with a block of text describing a situation, and must</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Noun Selection With Affordance Detection
    state \(=\) game response to last command
    manipulable_nouns \(=-\{ \}\)
    for each word \(w \in\) state do
        if \(w\) is a noun then
            if \(w\) is manipulable then
                add \(w\) to manipulable_nouns
            end if
        end if
    end for
    noun \(=\) a randomly selected noun from manipulable_nouns
Algorithm 2 Verb Selection With Analogy Reduction
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">:</span><span class="w"> </span><span class="nv">navigation_verbs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&#39;</span><span class="nv">north</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">south</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">east</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">west</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">northeast</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">southeast</span><span class="o">&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="o">&#39;</span><span class="nv">southeast</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">northwest</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">up</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="k">do</span><span class="nv">wn</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">enter</span><span class="o">&#39;</span><span class="p">]</span>
<span class="w">    </span><span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="nv">manipulation_verbs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">list</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="mi">1000</span><span class="w"> </span><span class="nv">most</span><span class="w"> </span><span class="nv">common</span><span class="w"> </span><span class="nv">verbs</span>
<span class="w">    </span><span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="nv">essential_manipulation_verbs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&#39;</span><span class="nv">get</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">drop</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">push</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">pull</span><span class="o">&#39;</span><span class="p">,</span><span class="w"> </span><span class="o">&#39;</span><span class="nv">open</span><span class="o">&#39;</span><span class="p">,</span>
<span class="w">        </span><span class="o">&#39;</span><span class="nv">close</span><span class="o">&#39;</span><span class="p">]</span>
<span class="w">    </span><span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="nv">affordant_verbs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">verbs</span><span class="w"> </span><span class="nv">returned</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">Word2vec</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">match</span><span class="w"> </span><span class="nv">noun</span>
<span class="w">    </span><span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="nv">affordant_verbs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">affordant_verbs</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="p">$</span>\<span class="nv">triangleleft</span><span class="p">$</span>
<span class="w">        </span><span class="nv">manipulation_verbs</span>
<span class="w">    </span><span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="nv">final_verbs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">navigation_verbs</span><span class="w"> </span>\<span class="p">(</span>\<span class="nv">cup</span>\<span class="p">)</span><span class="w"> </span><span class="nv">affordant_verbs</span><span class="w"> </span>\<span class="p">(</span>\<span class="nv">cup</span>\<span class="p">)</span>
<span class="w">        </span><span class="nv">essential_manipulation_verbs</span>
<span class="w">    </span><span class="mi">7</span><span class="o">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="nv">operatorname</span><span class="p">{</span><span class="nv">verb</span><span class="p">}</span><span class="o">=</span>\<span class="p">)</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">randomly</span><span class="w"> </span><span class="nv">selected</span><span class="w"> </span><span class="nv">verb</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nv">final_verbs</span>
</code></pre></div>

<p>respond with a written phrase. Typical actions include commands such as: 'examine wallet', 'eat apple', or 'light campfire with matches'. The game engine parses this response and produces a new block of text. The resulting interactions, although syntactically simple, provide a fertile research environment for natural language processing and human/computer interaction. Game players must identify objects that are manipulable and apply appropriate actions to those objects in order to make progress.</p>
<p>In these games, the learning agent faces a frustrating dichotomy: its action set must be large enough to accommodate any situation it encounters, and yet each additional action increases the size of its search space. A brute force approach to such scenarios is frequently futile, and yet factorization, function approximation, and other search space reduction techniques bring the risk of data loss. We desire an agent that is able to clearly perceive all its options, and yet applies only that subset which is likely to produce results.</p>
<p>In other words, we want an agent that explores the game world the same way a human does: by trying only those actions that 'make sense'. In the following sections, we show that affordance-based action selection provides a meaningful first step towards this goal.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Sample text from the adventure game Zork. Player responses follow a single angle bracket.</p>
<h3>4.1 Learning Algorithm</h3>
<p>Our agent utilizes Q-learning [Watkins and Dayan, 1992], a reinforcement learning algorithm which attempts to maximize expected discounted reward. Q-values are updated according to the equation</p>
<p>$$
\Delta Q_{\pi}(s, a)=\alpha\left(R(s, a)+\gamma \max <em _pi="\pi">{a} Q</em>(s, a)\right)
$$}\left(s^{\prime}, a\right)-Q_{\pi</p>
<p>where $Q_{\pi}(s, a)$ is the expected cumulative reward for performing action $a$ in observed state $s$ and following the policy $\pi$ thereafter, $\alpha$ is the learning rate, $\gamma$ is the discount factor, and $s^{\prime}$ is the new state observation after performing action $a$. All experiments in this paper used $\alpha=0.1$ and $\gamma=0.95$</p>
<p>The agent's state representation is encoded as a hash of the text provided by the game engine. Actions are comprised of verb/object pairs:</p>
<p>$$
a=v+{ }^{\prime}+o, v \epsilon V, o \epsilon O
$$</p>
<p>where $V$ is the set of all English-language verbs and $O$ is the set of all English-language nouns. To enable the agent to distinguish between state transitions and merely informational feedback, the agent executes a 'look' command every second iteration and assumes that the resulting game text represents its new state. Some games append a summary of actions taken and points earned in response to each 'look' command. To prevent this from obfuscating the state space, we stripped all numerals from the game text prior to hashing.</p>
<p>Given that the English language contains at least 20,000 verbs and 100,000 nouns in active use, a naive application of Q-learning is intractable. Some form of action-space reduction must be used. For our baseline comparison, we use an agent with a vocabulary consisting of the 1000 most common verbs in Wikipedia plus an 11-word navigation list and a 6-word essential manipulation list as depicted in Algorithm 2. The navigation list contains words which, by convention, are used to navigate through text-based games. The essential manipulation list contains words which, again by convention, are generally applicable to all in-game objects.</p>
<p>The baseline agent does not use a fixed noun vocabulary. Instead, it extracts nouns from the game text using part-ofspeech tags. To facilitate game interactions, the baseline agent augments its noun list using adjectives that precede them. For example, if the game text consisted of 'You see a red pill and a blue pill', then the agent's noun list for that state would be ['pill', 'red pill', 'blue pill']. (And its next action is hopefully 'swallow red pill').</p>
<p>In Sections 5.1 and 5.2 the baseline agent is contrasted with an agent using affordance extraction to reduce its manipulation list from 1000 verbs to a mere 30 verbs for each state, and to reduce its object list to a maximum of 15 nouns per state. We compare our approach to other search space reduction techniques and show that the a priori knowledge provided by affordance extraction enables the agent to achieve results which cannot be paralleled through brute force methods. All agents used epsilon-greedy exploration with a decaying epsilon.</p>
<p>The purpose of our research was to test the value of affordance-based search space reduction. Therefore, we did not add augmentations to address some of the more challenging aspects of text-based adventure games. Specifically, the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Learning trajectories for eighteen Z-machine games. No agent scored any points on the remaining 31 games. Average of 10 data runs. *In zork3, standard smoothing obscured small, sporadic positive rewards obtained by our reduction algorithms.
agent maintained no representation of items carried in inventory or of the game score achieved thus far. The agent was also not given the ability to construct prepositional commands such as 'put book on shelf' or 'slay dragon with sword'. Our algorithm does not account for physical or temporal dependencies in which the possession of an item or the execution of a specific action in the past affect the outcome of the current behaviors.</p>
<h2>5 Results</h2>
<p>We tested our agent on a suite of 50 text-based adventure games compatible with Infocom's Z-machine. These games represent a wide variety of situations, ranging from business scenarios like 'Detective' to complex fictional worlds like 'Zork: The Underground Empire'. The core objective of the games vary widely. In some games, the player's goal is simply to survive, while in others specific items must be acquired. Significantly, the games provide little or no information about the agent's goals or which actions might provide reward.</p>
<p>During training, the agent interacted with the game engine for 1000 epochs with 1000 training steps per epoch. On each training step, the agent received a reward corresponding to the change in game score. After each epoch the game was restarted and the game score reset, but the agent retained its learned Q-values. Some games, notably zork3, allowed the player to 'die' as play advanced, triggering a negative reward and prematurely terminating the training epoch.</p>
<p>One game experienced a scoring malfunction and was excluded from subsequent analysis; however, our verb-space reduction method obtained a score identical to the baseline agent's in the discarded partial results.</p>
<p>Our affordance-based search space reduction algorithms enabled the agent to score points on 18/49 games, with a peak performance (expressed as a percentage of possible game
score) of $57.44 \%$ for verb space reduction, $23.90 \%$ for object space reduction, and $56.84 \%$ when both methods were combined. The baseline agent (see Sec. 4.1) scored points on 12/49 games, with a peak performance of $20.84 \%$. Peak performance is defined as the maximum score achieved over all epochs, a metric that expresses the agent's ability to comb through the search space and discover areas of high reward.</p>
<p>Figures 5 and 7 show the performance of our reduction techniques when compared to the baseline. Affordancebased search space reduction improved overall performance on 13/18 games, and decreased performance on only 2 games.</p>
<p>Examination of the 31 games in which no agent scored points (and which are correspondingly not depicted in Figures 5 and 7) revealed three prevalent failure modes: (1) The game required prepositional commands such as 'look at machine' or 'give dagger to wizard', (2) The game provided points only after an unusually complex sequence of events, (3) The game required the user to infer the proper term for manipulable objects. (For example, the game might describe 'something shiny' at the bottom of a lake, but required the agent to 'get shiny object'.) Our test framework was not designed to address these issues, and hence did not score points on those games. A fourth failure mode (4) might be the absence of a game-critical verb within the 1000-word manipulation list. However, this did not occur in our coarse examination of games that failed.</p>
<h3>5.1 Alternate Reduction Methods</h3>
<p>We compared our affordance-based reduction technique with four other approaches that seemed intuitively applicable to the test domain. Results are shown in Figure 7.</p>
<p>Intrinsic rewards: This approach guides the agent's exploration of the search space by allotting a small reward each time a new state is attained. The agent is thus encouraged to traverse multiple game locations, enabling possibilities for</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Affordant selection</th>
<th style="text-align: left;">Random selection</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">decorate glass</td>
<td style="text-align: left;">continue quantity</td>
</tr>
<tr>
<td style="text-align: left;">open window</td>
<td style="text-align: left;">break sack</td>
</tr>
<tr>
<td style="text-align: left;">add table</td>
<td style="text-align: left;">result window</td>
</tr>
<tr>
<td style="text-align: left;">generate quantity</td>
<td style="text-align: left;">stay table</td>
</tr>
<tr>
<td style="text-align: left;">ring window</td>
<td style="text-align: left;">build table</td>
</tr>
<tr>
<td style="text-align: left;">weld glass</td>
<td style="text-align: left;">end house</td>
</tr>
<tr>
<td style="text-align: left;">travel passage</td>
<td style="text-align: left;">remain quantity</td>
</tr>
<tr>
<td style="text-align: left;">climb staircase</td>
<td style="text-align: left;">discuss glass</td>
</tr>
<tr>
<td style="text-align: left;">jump table</td>
<td style="text-align: left;">passage</td>
</tr>
</tbody>
</table>
<p>Figure 6: Sample exploration actions produced by a Q-learner with and without affordance detection. The random agent used nouns extracted from game text and a verb list comprising the 200 most common verbs in Wikipedia.
higher reward. We call these awards intrinsic because they are tied to the agent's assessment of its progress rather than to external events.</p>
<p>Random reduction: When applying search space reductions one must always ask: 'Did improvements result from my specific choice of reduced space, or would any reduction be equally effective?' We address this question by randomly selecting 30 manipulation verbs to use during each epoch.</p>
<p>ConceptNet reduction: In this approach we used ConceptNet's CapableOf relation to obtain a list of verbs relevant to the current object. We then reduced the agent's manipulation list to include only words that were also in ConceptNet's word list (effectively taking the intersection of the two lists).</p>
<p>Co-occurrence reduction: In this method, we populated a co-occurrence dictionary prior to training using the 1000 most common verbs and 30,000 most common nouns in Wikipedia. The dictionary tracked the number of times each verb/noun pair occurred within a 9-word radius of each other. During game play, the agent's manipulation list was reduced to include only words which exceeded a low threshold (cooccurrences $&gt;3$ ).</p>
<p>Figure 7 shows the performance of these four algorithms, along with a baseline learner using a 1000-word manipulation list, on the sixteen games presented in Figure 5. Affordancebased verb selection improved performance in most games, but the other reduction techniques fell prey to a classic danger: they pruned precisely those actions which were essential to obtain reward.</p>
<h3>5.2 Fixed-length Vocabularies vs. Free-form Learning</h3>
<p>An interesting question arises from our research. What if, rather than beginning with a 1000-word vocabulary, the agent was free to search the entire English-language verb space?</p>
<p>A traditional learning agent could not do this: the space of possible verbs is too large. However, the Wikipedia knowledge base opens new opportunities. Using the action selection mechanism described in Section 4.1, we allowed the agent to construct its own manipulation list for each state (see Section 3.1). The top 15 responses were unioned with the agent's navigation and essential manipulation lists, with actions selected randomly from that set.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Five verb space reduction techniques compared over 100 exploration epochs. Average of 5 data runs. Results were normalized for each game based on the maximum reward achieved by any agent.</p>
<p>A sampling of the agent's behavior is displayed in Figure 6 , along with comparable action selections from the baseline agent described in Section 4.1. The free-form learner is able to produce actions that seem, not only reasonable, but also rather inventive when considered in the context of the game environment. We believe that further research in this direction may enable the development of one-shot learning for textbased adventure games.</p>
<h2>6 Conclusion</h2>
<p>The common sense knowledge implicitly encoded within Wikipedia and other online corpora opens new opportunities for autonomous agents. In this paper we have shown that previously intractable search spaces can be efficiently navigated when word embeddings are used to identify contextdependent affordances. We have also shown that, in the domain of text-based adventure games, this approach is superior to several other intuitive methods.</p>
<p>We emphasize that, although our initial experiments have been restricted to text-based environments, the underlying principles apply to any domain in which mappings can be formed between words and objects. Steady advances in the fields of object recognition and semantic segmentation, combined with improved precision in robotic systems, suggests that our methods are readily applicable to systems including self-driving cars, domestic robots, and UAVs.</p>
<h2>Acknowledgements</h2>
<p>Our experiments were run using Autoplay: a learning environment for interactive fiction (https://github.com/ danielricks/autoplay). We thank Nvidia, the Center for Unmanned Aircraft Systems, and Analog Devices, Inc. for their generous support.</p>
<h2>References</h2>
<p>[Arkin, 1998] Ronald C. Arkin. Behavior-Based Robotics. MIT Press, 1998.
[Bolukbasi et al., 2016a] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, NIPS, pages 4349-4357. Curran Associates, Inc., 2016.
[Bolukbasi et al., 2016b] Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. Quantifying and reducing stereotypes in word embeddings. CoRR, abs/1606.06121, 2016.
[Brokos et al., 2016] Georgios-Ioannis Brokos, Prodromos Malakasiotis, and Ion Androutsopoulos. Using centroids of word embeddings and word mover's distance for biomedical document retrieval in question answering. CoRR, abs/1608.03905, 2016.
[Frome et al., 2013] Andrea Frome, Greg S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In In NIPS, 2013.
[Gibson, 1977] James J. Gibson. The theory of affordances. In Robert Shaw and John Bransford, editors, Perceiving, Acting, and Knowing. 1977.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.
[Karpathy et al., 2014] Andrej Karpathy, Armand Joulin, and Li Fei-fei. Deep fragment embeddings for bidirectional image sentence mapping. In In arXiv:1406.5679, 2014.
[Kiros et al., 2015] Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Skip-thought vectors. CoRR, abs/1506.06726, 2015.
[Laird and van Lent, 2001] John E. Laird and Michael van Lent. Human-level AI's killer application: Interactive computer games. AI Magazine, 22(2):15-26, 2001.
[Le and Mikolov, 2014] Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. CoRR, abs/1405.4053, 2014.
[Liu and Singh, 2004] H. Liu and P. Singh. Conceptnet - a practical commonsense reasoning tool-kit. BT Technology Journal, 22(4):211-226, 2004.
[Matuszek et al., 2006] Cynthia Matuszek, John Cabral, Michael Witbrock, and John Deoliveira. An introduction to the syntax and content of cyc. In Proceedings of the 2006 AAAI Spring Symposium on Formalizing and Compiling Background Knowledge and Its Applications to Knowledge Representation and Question Answering, pages 44-49, 2006.
[Mikolov et al., 2013a] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.
[Mikolov et al., 2013b] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 3111-3119. Curran Associates, Inc., 2013.
[Mikolov et al., 2013c] Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. Association for Computational Linguistics, May 2013.
[Miller, 1995] George A. Miller. Wordnet: A lexical database for english. Commun. ACM, 38(11):39-41, November 1995.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[Montesano et al., 2007] L. Montesano, M. Lopes, A. Bernardino, and J. Santos-Victor. Modeling affordances using bayesian networks. In 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4102-4107, Oct 2007.
[Narasimhan et al., 2015] Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for textbased games using deep reinforcement learning. CoRR, abs/1506.08941, 2015.
[Navarro et al., 2012] Stefan Escaida Navarro, Nicolas Gorges, Heinz Wörn, Julian Schill, Tamim Asfour, and Rüdiger Dillmann. Haptic object recognition for multi-fingered robot hands. In 2012 IEEE Haptics Symposium (HAPTICS), pages 497-502. IEEE, 2012.
[Russ et al., 2011] Thomas A Russ, Cartic Ramakrishnan, Eduard H Hovy, Mihail Bota, and Gully APC Burns. Knowledge engineering tools for reasoning with scientific observations and interpretations: a neural connectivity use case. BMC bioinformatics, 12(1):351, 2011.
[Schenck et al., 2012] Wolfram Schenck, Hendrik Hasenbein, and Ralf Möller. Detecting affordances by mental imagery. In Alessandro G. Di Nuovo, Vivian M. de la Cruz, and Davide Marocco, editors, Proceedings of the SAB Workshop on "Artificial Mental Imagery", pages 15-18, Odense (Danmark), 2012.
[Schenck et al., 2016] Wolfram Schenck, Hendrik Hasenbein, and Ralf Möller. Detecting affordances by visuomotor simulation. arXiv preprint arXiv:1611.00274, 2016.
[Socher et al., 2011] Richard Socher, Cliff C. Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. ICML, pages 129 -136, 2011.
[Song et al., 2011] Hyun Oh Song, Mario Fritz, Chunhui Gu, and Trevor Darrell. Visual grasp affordances from appearance-based cues. In ICCV Workshops, pages 998-1005. IEEE, 2011.
[Song et al., 2015] Hyun Oh Song, Mario Fritz, Daniel Goehring, and Trevor Darrell. Learning to detect visual grasp affordance. In IEEE Transactions on Automation Science and Engineering (TASE), 2015.
[Stoytchev, 2008] Alexander Stoytchev. Learning the Affordances of Tools Using a Behavior-Grounded Approach, pages 140-158. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008.
[Watkins and Dayan, 1992] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
[Zhu et al., 2014] Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about object affordances in a knowledge base representation. In ECCV, 2014.
[Zhu et al., 2015] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. CoRR, abs/1506.06724, 2015.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2507 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2507</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2507</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-222208930</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.04153v2.pdf" target="_blank">Olympus: a benchmarking framework for noisy optimization and experiment planning</a></p>
                <p><strong>Paper Abstract:</strong> Research challenges encountered across science, engineering, and economics can frequently be formulated as optimization tasks. In chemistry and materials science, recent growth in laboratory digitization and automation has sparked interest in optimization-guided autonomous discovery and closed-loop experimentation. Experiment planning strategies based on off-the-shelf optimization algorithms can be employed in fully autonomous research platforms to achieve desired experimentation goals with the minimum number of trials. However, the experiment planning strategy that is most suitable to a scientific discovery task is a priori unknown while rigorous comparisons of different strategies are highly time and resource demanding. As optimization algorithms are typically benchmarked on low-dimensional synthetic functions, it is unclear how their performance would translate to noisy, higher-dimensional experimental tasks encountered in chemistry and materials science. We introduce Olympus, a software package that provides a consistent and easy-to-use framework for benchmarking optimization algorithms against realistic experiments emulated via probabilistic deep-learning models. Olympus includes a collection of experimentally derived benchmark sets from chemistry and materials science and a suite of experiment planning strategies that can be easily accessed via a user-friendly python interface. Furthermore, Olympus facilitates the integration, testing, and sharing of custom algorithms and user-defined datasets. In brief, Olympus mitigates the barriers associated with benchmarking optimization algorithms on realistic experimental scenarios, promoting data sharing and the creation of a standard framework for evaluating the performance of experiment planning strategies</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2507.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2507.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Olympus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Olympus benchmarking framework for noisy optimization and experiment planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular Python framework that provides (i) interfaces to 18 experiment planners, (ii) analytically defined noisy surfaces, (iii) probabilistic emulators trained on experimental datasets, and (iv) tools to run automated benchmarking campaigns and baselines to evaluate experiment-planning strategies under realistic noisy conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Olympus</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Software package providing a standardized interface (Planner class) to a suite of experiment planners (Bayesian optimization, evolutionary strategies, grid/quasi-random sampling, heuristics, gradient methods, etc.), noisified analytical surfaces, and probabilistic emulators (BNNs and deterministic NNs) trained on experimental datasets; it enables inexpensive emulation of experiments (<1 ms per evaluation for trained emulators), automated benchmarking (including random-search baselines), result storage/plotting, and community dataset sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experiment planning and autonomous discovery in chemistry and materials science (reaction optimization, materials/property optimization, color mixing, HPLC calibration, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Provides interfaces for planners to propose next experiments; allocation is performed by the chosen planner (e.g., acquisition-function maximization for Bayesian optimization, population updates for evolutionary strategies, predetermined sampling for grid/quasi-random), and Olympus orchestrates execution against emulators or real experiments and records budgets and outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not prescriptive; the framework records algorithm runtimes and notes algorithmic scaling with number of samples and parameter dimensionality. The paper reports emulator inference times (<1 ms per evaluation on a standard laptop) and typical experimental runtimes (>10 minutes) to argue compute is usually smaller than lab cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not intrinsic to the framework; individual planners accessed through Olympus may use metrics such as Expected Improvement (EI) or model uncertainty. Olympus provides probabilistic emulators (BNNs) to enable planners to compute information-aware acquisition values.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Delegated to the selected planner (e.g., acquisition functions in Bayesian optimization balance exploration vs exploitation; evolutionary methods maintain a population; grid/quasi-random sampling prioritizes coverage). Olympus provides the infrastructure to run and compare these mechanisms under identical emulated/noisy conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Olympus itself supplies tools (quasi-random sequences, population-based planners, ability to add noise, and custom planners) that support diversity mechanisms implemented inside planners; it does not impose a single diversity scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments budgets (user-specified), downloadable random-search baselines (100 campaigns × 10,000 evaluations each) and arbitrary per-campaign iteration limits. The paper recommends evaluating planners at budgets [1,3,10,30,100,300,1000,3000,10000].</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Olympus executes planners for a specified number of iterations/evaluations and records best-observed values; it supports multiple independent campaigns to obtain statistics and comparison against baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best-achieved property value (objective), tracked as the key metric over budgeted experiments; ranking-preserving metrics (Spearman ρ) are used to validate emulator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary reported baseline metrics: average best objective vs number of evaluations over 100 independent runs; emulator fidelity metrics include Spearman's rank correlation >0.90 and inference times <1 ms; datasets sizes (66–1,386 data points) and parameter dimensionalities (3–6) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random search baseline (100 independent campaigns, 10,000 evaluations each) is provided and downloadable via Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Olympus itself is a benchmarking tool; it reports that on some emulators (photo pce10, photo wf3, colormix bob, colormix n9, snar, hplc n9) random search reaches near-optimal values quickly, whereas other emulators (alkox, fullerenes, nbenzylation, suzuki) remain challenging and random search fails to reach asymptotic optima even at 10,000 evaluations. No single planner is declared uniformly best.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not claimed by the framework itself; Olympus provides infrastructure to measure efficiency gains of planners relative to baselines but does not state universal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Olympus emphasizes that planner selection affects required experimental budgets and that computational cost and algorithmic scaling are sometimes neglected; it notes ill-chosen planners can increase experimental budget requirements by up to an order of magnitude (cited). The framework enables empirical tradeoff analysis between experiment cost, algorithm runtime, and information efficiency by enabling repeatable emulated campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>The paper recommends empirical benchmarking on realistic, noisy emulators to select planners suited to a task, noting that (i) planner computational cost usually is small relative to experiment duration for typical chemistry/materials tasks (<10 parameters and >10 min per experiment), and (ii) choice of planner should account for problem dimensionality, noise, and desired sample-efficiency — no single universally optimal allocation strategy exists.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2507.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BayesianOpt (GPyOpt / Phoenics / HyperOpt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization frameworks (examples: GPyOpt, Phoenics, HyperOpt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential model-based optimization methods that build a surrogate (e.g., Gaussian process, mixture of kernels, tree-structured Parzen estimator) and select new experiments by maximizing an acquisition function that trades off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian optimization (GPyOpt / Phoenics / HyperOpt)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Surrogate-model-based sequential optimization: surrogate models (GPs, mixture kernels, TPEs) are trained on observed data; acquisition functions (e.g., Expected Improvement (EI), EI_MCMC variants) compute utility for candidate points accounting for predicted mean and uncertainty; next experiments are chosen by maximizing the acquisition function, and the surrogate is updated with new observations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box experiment planning and automated discovery in chemistry and materials science (reaction optimization, materials properties), hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments sequentially by maximizing an acquisition function that encodes expected utility (improvement, uncertainty reduction), thereby prioritizing points likely to improve objectives or reduce model uncertainty; implementations in Olympus include GPyOpt (GP with MCMC), Phoenics (mixture of Gaussian kernels with BO for chemistry), and HyperOpt (TPE).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Paper does not prescribe a single metric; it notes algorithmic computational scaling depends on number of samples and dimensionality (GPs scale cubically with sample count in naive implementations); Olympus notes typical experimental runtimes (>10 min) render computational cost often secondary, and gives emulator inference times (<1 ms).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions such as Expected Improvement (EI) and acquisition types labeled EI_MCMC are used; these evaluate expected improvement (a form of expected utility) and implicitly trade off mean prediction versus uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition function optimization (EI, EI_MCMC, etc.) balances exploitation (high surrogate mean) and exploration (high predictive uncertainty); model uncertainty (e.g., via GP posterior or BNN predictive samples) informs the exploration term.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly enforced in the standard BO implementations described; diversity arises implicitly from exploration induced by uncertainty-aware acquisition functions. Olympus also allows batch/parallel queries (through planners) but explicit diversity-promoting strategies are not described in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-evaluations budgets (user-specified iteration counts); Olympus evaluates BO at various budget levels (e.g., 10–100–1000 evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BO is run sequentially until the predefined number of iterations is exhausted; acquisition may be optimized under per-step computational budgets but the paper does not detail acquisition-cost-aware modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best-observed objective value (e.g., yield, degradation metric, distance-to-target color) over the run; improvement over baseline is used to judge breakthrough potential.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported as a single number across datasets in the paper for BO specifically; Olympus provides tools to measure best-achieved property vs number of evaluations and to compare across planners and against random baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared within Olympus to Random Search baseline and other planners in the provided suite (evolutionary methods, grid/quasi-random, heuristics, gradient-based), though specific per-planner numeric comparisons are left to users to run.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>The paper does not provide numeric BO-vs-baseline aggregates; it highlights that some emulated tasks are easy for random search while others are hard and that ill-chosen planners can require an order-of-magnitude more experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified generically in the paper; empirical gains are intended to be measured per-emulator within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper discusses that BO's computational overhead and algorithmic scaling vary per method but generally are small relative to lab experiment durations in typical use cases; it encourages benchmarking to understand tradeoffs between compute demand and sample-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommends using acquisition-function based planners for sample-efficient sequential allocation in noisy experimental contexts, but stresses that suitability depends on problem structure, noise, and dimensionality and must be empirically benchmarked.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2507.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BNN emulators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian neural network based experiment emulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Probabilistic emulators (BNNs) trained on experimental datasets to cheaply emulate noisy experimental response surfaces, returning predictive distributions conditioned on input parameters to enable large-scale simulated campaigns and acquisition computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian neural network emulators</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Feedforward fully connected Bayesian neural networks model weight/bias distributions and are trained (with cross-validation) on 80% of the data (20% test) to emulate experiment response and noise; BNNs return predictive samples enabling uncertainty-aware planners to compute acquisition values; inference is typically <1 ms per query.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Emulation of experimental response surfaces for chemistry and materials science datasets (reaction yields, photodegradation, color mixing, HPLC, etc.) to benchmark experiment planners.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not an allocation algorithm itself — it supports allocation by providing cheap, stochastic responses with calibrated uncertainty so planners can trade off information gain vs experiment cost in simulated campaigns. By enabling many low-cost queries, BNN emulators shift allocation decisions from physical experiments to fast in-silico evaluations for planner benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Emulator inference time (<1 ms per evaluation on a standard laptop) and training overhead (cross-validation and training, not specifically timed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>BNN predictive distributions enable computation of acquisition metrics (e.g., Expected Improvement) by planners; the emulator provides uncertainty estimates necessary for expected-utility computations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not applicable to emulator itself; enables planners that implement acquisition-based exploration/exploitation trade-offs by providing uncertainty-aware predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Emulators do not directly promote diversity; they facilitate planner strategies that may do so by providing many low-cost evaluations to evaluate diverse candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Used to study fixed-number-of-evaluations budgets for planners in simulation; emulators themselves are inexpensive so budgets reflect experimental budgets rather than emulator cost.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Because emulator evaluations are cheap, Olympus uses emulators to run many independent campaigns (e.g., 100 campaigns of 10,000 evaluations) yielding statistical baselines for budget-restricted comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same as for planners: best-observed target property in emulated runs; emulators enable estimation of probability-of-discovery curves vs budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Emulator fidelity measured by Spearman's rank correlation (>0.90 for all BNN emulators on both training and test sets), R^2 and RMSE reported per emulator in figures; inference latency ~<1 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not an optimizer; BNN emulators are used to compare planners against Random Search baseline and against each other.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not applicable to emulators; they provide the ground on which planner performance vs baseline is measured.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables orders-of-magnitude cheaper evaluation vs physical experiments (ms vs minutes), facilitating large-scale benchmarking that would be infeasible with real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper emphasizes that emulators allow separating algorithmic-computation tradeoffs from experimental resource costs and permit exploration of planner behavior over large budgets to analyze sample-efficiency and robustness under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>By enabling rapid, noisy emulation, BNNs let practitioners empirically determine which planners allocate experimental budgets most effectively for a specific experimental surface and noise regime.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2507.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform random search baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A naive experiment-planning strategy that samples parameter configurations uniformly at random within specified parameter bounds and is used in the paper as a baseline to quantify planner efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Random search (uniform)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each step, a parameter vector is generated by sampling each parameter independently from a uniform distribution over its allowed range; no information from previous observations is used to select future evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Baseline experiment planning for optimization tasks in chemistry and materials science emulated in Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate budget uniformly at random across parameter space, independent draws per evaluation; used to build a baseline distribution of best-achieved values vs number of evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Minimal compute per sample; computational cost mainly the cost of evaluating the emulator or experiment; the paper reports running 100 independent random-search campaigns of 10,000 evaluations each for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>None — random search does not attempt to maximize information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration (no exploitation), since it never uses past data to focus sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Random sampling ensures statistical coverage but does not actively enforce structured diversity beyond randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-evaluations budgets; baseline executed for 10,000 evaluations per campaign (100 campaigns) and additionally recommended comparison budgets [1,3,10,30,100,300,1000,3000,10000].</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Simple termination after predefined number of draws; performance tracked as best-observed value vs budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best-observed objective value over the run; distribution of best values over independent runs is used to gauge difficulty of an emulator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Average best value vs evaluations (solid blue lines in paper figures) and boxplots of distribution over 100 runs; identification of emulators where random search quickly finds near-optimal values vs those where it does not.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the baseline to compare all other planners within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>By definition, random search is baseline; paper reports that on some emulators random search reaches near-optimal values quickly while on others it fails even at 10,000 evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not applicable; other planners are expected to outperform random search in sample-efficiency, but specific numeric improvements are dataset- and planner-dependent and not universally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Random search highlights the intrinsic difficulty of tasks and provides a lower bound on performance; paper suggests the magnitude by which planners outperform random search is a proxy for planner efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Random search performance identifies which emulators are inherently easy vs hard; the paper uses this insight to motivate selection and benchmarking of more sophisticated allocation strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2507.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evolutionary algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Population-based evolutionary strategies (Genetic algorithms, CMA-ES, Particle Swarm, Differential Evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Heuristic population-based optimizers that evolve a set of candidate solutions using selection, mutation, recombination, and population update rules to explore and exploit the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evolutionary algorithms (Genetic, CMA-ES, PSO, Differential Evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintain a population of candidate parameter vectors evaluated on the objective; new candidates are generated by variation operators (mutation, crossover, covariance adaptation) and selected based on fitness; many variants (CMA-ES, PSO, Differential Evolution) provide different heuristics for mutation and adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Global optimization for experiment planning in chemistry/materials when gradient information is unavailable or surfaces are multi-modal and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate a batch of evaluations per generation across population members; selection biases future allocations toward higher-fitness regions while variation maintains exploration; population size and number of generations set the budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost proportional to population size × evaluations per generation; additional overhead for internal adaptation steps (e.g., covariance updates in CMA-ES). Paper notes algorithms differ in runtime and memory scaling with sample number and dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicit — selection is fitness-driven rather than calculated via explicit expected information metrics; implicit information collected via population fitness landscape sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration is maintained by stochastic variation (mutation, recombination) and population diversity; exploitation obtained by selection pressure toward better-performing individuals and adaptation mechanisms (e.g., CMA-ES covariance adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Built-in via population diversity, mutation and recombination operators; CMA-ES adapts search distribution to maintain effective exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational/evaluation budget specified via population size and number of generations or total evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Practically handled by tuning population size and number of iterations; users must trade off per-generation breadth vs depth given evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best-found fitness (objective) across population across generations; convergence behavior and ability to escape local minima are proxies for breakthrough capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not quantitatively compared in aggregate across datasets in the paper; included among Olympus planners to be benchmarked against the random baseline and other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random search baseline, Bayesian optimization, grid/quasi-random, heuristics, gradient-based methods within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported in aggregate; performance is dataset- and hyperparameter-dependent and intended to be evaluated within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Depends on implementation and tuning; the paper notes evolutionary strategies can be competitive especially in multi-modal landscapes but may require more evaluations compared to model-based BO in some contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes algorithmic runtime and memory scaling differs across methods and can affect applicability, but for typical experimental runtimes (>10 min) compute overhead from evolutionary algorithms is often not limiting.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Population-based methods provide an explicit mechanism to maintain hypothesis diversity while allocating evaluations, but optimal allocation (population vs iterations) must be chosen per problem and can be explored empirically using Olympus.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2507.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grid / Quasi-random</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grid-like searches and low-discrepancy sequences (Grid, Latin Hypercube, Sobol)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deterministic or quasi-random schemes that sample the parameter space to ensure coverage (full factorial grids, Latin Hypercube, Sobol sequences), trading off exponential scaling in dimensionality for reliable global coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Grid-like sampling and low-discrepancy sequences</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Predefine a set of parameter points to be evaluated (grid), or generate quasi-random low-discrepancy samples (Latin Hypercube, Sobol) that more uniformly cover high-dimensional spaces; random search is also included as a degenerate 'grid-like' strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design and initial exploration for low-dimensional parameter spaces in chemistry/materials; initialization strategy for other planners.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments by iterating through the predefined sampling plan; does not use past observations to adapt allocation (except that planners can wrap these samplers as initialization phases).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost is number of evaluations times per-evaluation cost; full grid scales exponentially with dimensionality, making cost explode with more parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly used; design aims to maximize space-filling coverage rather than expected information gain measured by mutual information or EI.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration via predetermined coverage; low-discrepancy sequences approximate uniform coverage and thereby increase diversity of sampled hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: low-discrepancy sequences (Latin Hypercube, Sobol) are used to promote even coverage and diversity across the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-evaluations; user's choice of sequence length or grid density defines budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget determines sampling density; in high dimensions, selective use of low-discrepancy sampling is recommended to achieve coverage with limited budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best-observed objective found in sampled design; coverage increases chance to sample high-performing regions but does not actively seek breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not universally reported; within Olympus grid/quasi-random methods serve as comparators to adaptive planners and can be evaluated at the same evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against adaptive planners and random search within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Performance depends on problem dimensionality and surface structure; paper notes grid-like searches can identify global optima reliably in low dimensions but cost scales poorly with dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Low-discrepancy sequences improve coverage efficiency relative to naive full grids at limited budgets; exact gains depend on dimensionality and objective landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights exponential cost scaling for grid methods vs improved global coverage; recommends low-discrepancy sequences or random search for higher-dimensional (>~5) problems.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For low-dimensional experimental design, planned, space-filling allocations (grid or quasi-random) can be effective; for higher dimensions, randomized or adaptive strategies are generally preferred due to budget scaling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2507.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic local/global methods (Snobfit, Simplex, Basin Hopping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Heuristic optimization strategies combining local and global search heuristics, designed to be robust to noise and to practical experimental constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Heuristic optimizers (Snobfit, Nelder–Mead Simplex, Basin Hopping)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Snobfit combines local and global search tailored for noisy evaluations; Simplex (Nelder–Mead) uses simplex geometry for downhill moves (local); Basin Hopping alternates local minimizations with stochastic global moves to escape basins. They are derivative-free and intended for practical optimization under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Noisy black-box experimental optimization where gradient information is unreliable and robustness to noise is important (chemical reaction optimization, materials experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Propose new evaluations based on local heuristics and global branching (Snobfit), simplex reflections/expansions/contractions (Simplex), or global random jumps followed by local minimization (Basin Hopping); allocation emphasizes iteratively improving local candidates while occasionally exploring globally.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost proportional to number of proposed evaluations and overhead of local-fit steps; Snobfit is designed with practical computational efficiency considerations for noisy experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly framed as information-theoretic; strategies are heuristic and focused on improving objective under noisy measurements rather than maximizing formal information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Hybrid mechanisms: Simplex is local-exploitative; Basin Hopping alternates exploration (random hops) and exploitation (local minimization); Snobfit explicitly mixes local and global probing to handle noise and multimodality.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via global moves or branching heuristics (Basin Hopping, Snobfit) which promote visiting diverse basins; Simplex lacks explicit diversity promotion.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-evaluations or termination criteria based on convergence; users set iteration limits.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Algorithms proceed until evaluation budget exhausted or convergence; they do not typically optimize allocation across multiple objectives or costs beyond their heuristic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best-found objective; robustness to noise and ability to escape local minima are key to discovering high-impact solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively in the paper and illustrated in example plots (e.g., Fig. 3 showing Random Search vs Simplex on an emulator), but no universal numeric performance claims across all datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random search and other planners within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Example (Fig. 3) shows Simplex outperforming Random Search on the fullerenes emulator in average best-measurement over 100 iterations in that case; general performance is task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Task dependent; heuristics can be more efficient than random search in low-noise, locally-smooth landscapes, but may be trapped by multimodality.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Heuristic methods offer lower computational overhead and practical robustness to noise but may sacrifice global exploration; hybrid methods (Snobfit, Basin Hopping) aim to balance these tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Hybrid local-global heuristics are recommended when noise and local structure dominate, but their efficacy should be benchmarked against adaptive, model-based planners using tools like Olympus.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2507.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2507.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sherpa (Bandit/Early-stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sherpa toolkit (hyperparameter optimization with bandit/early-stopping schemes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python toolkit for hyperparameter tuning that includes Bayesian optimization, evolutionary approaches, and Bandit/Early-stopping schemes to allocate computational resources adaptively across candidate trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sherpa (hyperparameter optimization with bandit/early-stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Orchestration toolkit for hyperparameter optimization that implements multiple optimization algorithms and includes resource-allocation strategies like bandit/early-stopping (e.g., successive halving) that adaptively terminate poor trials to reallocate compute to promising candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter optimization of machine learning models; cited as related work demonstrating resource-allocation decisions that trade computational cost vs expected benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Adaptive allocation via bandit/early-stopping schemes: allocate partial resources to many trials, evaluate intermediate performance, and stop underperforming trials early to concentrate resources on promising ones.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitly measured as wall-clock or compute resources per trial; Sherpa orchestrates training-evaluation cycles and can reduce total compute by early termination.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not specified in the Olympus text; bandit schemes use intermediate performance as a proxy for future improvement rather than formal information measures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration via launching many short trials; exploitation via early-stopping to divert resources to better-performing trials.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit: by initially exploring many configurations, Sherpa preserves diversity early on; early-stopping prunes unpromising parts of the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational resource budget (time/compute) and number of trials; Sherpa is intended to optimize under such constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses bandit-style methods and early-stopping to reallocate limited compute resources away from poor trials toward promising ones.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified in Olympus; in hyperparameter contexts this would be final model performance; in experimental contexts analogous metrics would be objective improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in the Olympus paper; Sherpa is mentioned as a related toolkit with adaptive resource allocation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned alongside Optuna and others as related work; not benchmarked within Olympus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not discussed in Olympus; referenced to illustrate types of resource-allocation schemes (bandit/early-stopping) used elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in Olympus; early-stopping and bandit schemes are generally known to reduce compute by terminating poor trials early, potentially large savings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Sherpa exemplifies approaches that trade-off upfront broad exploration (many short runs) against focused exploitation (long runs for promising candidates) to reduce compute while retaining discovery potential.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Not provided by Olympus for Sherpa; referenced to underline that different application areas use adaptive compute-allocation mechanisms that could be relevant for experiment planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Phoenics: A Bayesian optimizer for chemistry <em>(Rating: 2)</em></li>
                <li>GPyOpt: A Bayesian Optimization Framework in Python <em>(Rating: 2)</em></li>
                <li>HyperOpt: Algorithms for hyper-parameter optimization <em>(Rating: 2)</em></li>
                <li>Snobfit - stable noisy optimization by branch and fit <em>(Rating: 2)</em></li>
                <li>Snobfit-stable noisy optimization by branch and fit <em>(Rating: 1)</em></li>
                <li>A Bayesian experimental autonomous researcher for mechanical design <em>(Rating: 2)</em></li>
                <li>Accelerated search for materials with targeted properties by adaptive design <em>(Rating: 2)</em></li>
                <li>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2507",
    "paper_id": "paper-222208930",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Olympus",
            "name_full": "Olympus benchmarking framework for noisy optimization and experiment planning",
            "brief_description": "A modular Python framework that provides (i) interfaces to 18 experiment planners, (ii) analytically defined noisy surfaces, (iii) probabilistic emulators trained on experimental datasets, and (iv) tools to run automated benchmarking campaigns and baselines to evaluate experiment-planning strategies under realistic noisy conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Olympus",
            "system_description": "Software package providing a standardized interface (Planner class) to a suite of experiment planners (Bayesian optimization, evolutionary strategies, grid/quasi-random sampling, heuristics, gradient methods, etc.), noisified analytical surfaces, and probabilistic emulators (BNNs and deterministic NNs) trained on experimental datasets; it enables inexpensive emulation of experiments (&lt;1 ms per evaluation for trained emulators), automated benchmarking (including random-search baselines), result storage/plotting, and community dataset sharing.",
            "application_domain": "Experiment planning and autonomous discovery in chemistry and materials science (reaction optimization, materials/property optimization, color mixing, HPLC calibration, etc.)",
            "resource_allocation_strategy": "Provides interfaces for planners to propose next experiments; allocation is performed by the chosen planner (e.g., acquisition-function maximization for Bayesian optimization, population updates for evolutionary strategies, predetermined sampling for grid/quasi-random), and Olympus orchestrates execution against emulators or real experiments and records budgets and outcomes.",
            "computational_cost_metric": "Not prescriptive; the framework records algorithm runtimes and notes algorithmic scaling with number of samples and parameter dimensionality. The paper reports emulator inference times (&lt;1 ms per evaluation on a standard laptop) and typical experimental runtimes (&gt;10 minutes) to argue compute is usually smaller than lab cost.",
            "information_gain_metric": "Not intrinsic to the framework; individual planners accessed through Olympus may use metrics such as Expected Improvement (EI) or model uncertainty. Olympus provides probabilistic emulators (BNNs) to enable planners to compute information-aware acquisition values.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Delegated to the selected planner (e.g., acquisition functions in Bayesian optimization balance exploration vs exploitation; evolutionary methods maintain a population; grid/quasi-random sampling prioritizes coverage). Olympus provides the infrastructure to run and compare these mechanisms under identical emulated/noisy conditions.",
            "diversity_mechanism": "Olympus itself supplies tools (quasi-random sequences, population-based planners, ability to add noise, and custom planners) that support diversity mechanisms implemented inside planners; it does not impose a single diversity scheme.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed-number-of-experiments budgets (user-specified), downloadable random-search baselines (100 campaigns × 10,000 evaluations each) and arbitrary per-campaign iteration limits. The paper recommends evaluating planners at budgets [1,3,10,30,100,300,1000,3000,10000].",
            "budget_constraint_handling": "Olympus executes planners for a specified number of iterations/evaluations and records best-observed values; it supports multiple independent campaigns to obtain statistics and comparison against baselines.",
            "breakthrough_discovery_metric": "Best-achieved property value (objective), tracked as the key metric over budgeted experiments; ranking-preserving metrics (Spearman ρ) are used to validate emulator fidelity.",
            "performance_metrics": "Primary reported baseline metrics: average best objective vs number of evaluations over 100 independent runs; emulator fidelity metrics include Spearman's rank correlation &gt;0.90 and inference times &lt;1 ms; datasets sizes (66–1,386 data points) and parameter dimensionalities (3–6) are reported.",
            "comparison_baseline": "Random search baseline (100 independent campaigns, 10,000 evaluations each) is provided and downloadable via Olympus.",
            "performance_vs_baseline": "Olympus itself is a benchmarking tool; it reports that on some emulators (photo pce10, photo wf3, colormix bob, colormix n9, snar, hplc n9) random search reaches near-optimal values quickly, whereas other emulators (alkox, fullerenes, nbenzylation, suzuki) remain challenging and random search fails to reach asymptotic optima even at 10,000 evaluations. No single planner is declared uniformly best.",
            "efficiency_gain": "Not claimed by the framework itself; Olympus provides infrastructure to measure efficiency gains of planners relative to baselines but does not state universal gains.",
            "tradeoff_analysis": "Olympus emphasizes that planner selection affects required experimental budgets and that computational cost and algorithmic scaling are sometimes neglected; it notes ill-chosen planners can increase experimental budget requirements by up to an order of magnitude (cited). The framework enables empirical tradeoff analysis between experiment cost, algorithm runtime, and information efficiency by enabling repeatable emulated campaigns.",
            "optimal_allocation_findings": "The paper recommends empirical benchmarking on realistic, noisy emulators to select planners suited to a task, noting that (i) planner computational cost usually is small relative to experiment duration for typical chemistry/materials tasks (&lt;10 parameters and &gt;10 min per experiment), and (ii) choice of planner should account for problem dimensionality, noise, and desired sample-efficiency — no single universally optimal allocation strategy exists.",
            "uuid": "e2507.0"
        },
        {
            "name_short": "BayesianOpt (GPyOpt / Phoenics / HyperOpt)",
            "name_full": "Bayesian optimization frameworks (examples: GPyOpt, Phoenics, HyperOpt)",
            "brief_description": "Sequential model-based optimization methods that build a surrogate (e.g., Gaussian process, mixture of kernels, tree-structured Parzen estimator) and select new experiments by maximizing an acquisition function that trades off exploration and exploitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian optimization (GPyOpt / Phoenics / HyperOpt)",
            "system_description": "Surrogate-model-based sequential optimization: surrogate models (GPs, mixture kernels, TPEs) are trained on observed data; acquisition functions (e.g., Expected Improvement (EI), EI_MCMC variants) compute utility for candidate points accounting for predicted mean and uncertainty; next experiments are chosen by maximizing the acquisition function, and the surrogate is updated with new observations.",
            "application_domain": "Black-box experiment planning and automated discovery in chemistry and materials science (reaction optimization, materials properties), hyperparameter tuning.",
            "resource_allocation_strategy": "Allocate experiments sequentially by maximizing an acquisition function that encodes expected utility (improvement, uncertainty reduction), thereby prioritizing points likely to improve objectives or reduce model uncertainty; implementations in Olympus include GPyOpt (GP with MCMC), Phoenics (mixture of Gaussian kernels with BO for chemistry), and HyperOpt (TPE).",
            "computational_cost_metric": "Paper does not prescribe a single metric; it notes algorithmic computational scaling depends on number of samples and dimensionality (GPs scale cubically with sample count in naive implementations); Olympus notes typical experimental runtimes (&gt;10 min) render computational cost often secondary, and gives emulator inference times (&lt;1 ms).",
            "information_gain_metric": "Acquisition functions such as Expected Improvement (EI) and acquisition types labeled EI_MCMC are used; these evaluate expected improvement (a form of expected utility) and implicitly trade off mean prediction versus uncertainty.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition function optimization (EI, EI_MCMC, etc.) balances exploitation (high surrogate mean) and exploration (high predictive uncertainty); model uncertainty (e.g., via GP posterior or BNN predictive samples) informs the exploration term.",
            "diversity_mechanism": "Not explicitly enforced in the standard BO implementations described; diversity arises implicitly from exploration induced by uncertainty-aware acquisition functions. Olympus also allows batch/parallel queries (through planners) but explicit diversity-promoting strategies are not described in detail.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-evaluations budgets (user-specified iteration counts); Olympus evaluates BO at various budget levels (e.g., 10–100–1000 evaluations).",
            "budget_constraint_handling": "BO is run sequentially until the predefined number of iterations is exhausted; acquisition may be optimized under per-step computational budgets but the paper does not detail acquisition-cost-aware modifications.",
            "breakthrough_discovery_metric": "Best-observed objective value (e.g., yield, degradation metric, distance-to-target color) over the run; improvement over baseline is used to judge breakthrough potential.",
            "performance_metrics": "Not reported as a single number across datasets in the paper for BO specifically; Olympus provides tools to measure best-achieved property vs number of evaluations and to compare across planners and against random baselines.",
            "comparison_baseline": "Compared within Olympus to Random Search baseline and other planners in the provided suite (evolutionary methods, grid/quasi-random, heuristics, gradient-based), though specific per-planner numeric comparisons are left to users to run.",
            "performance_vs_baseline": "The paper does not provide numeric BO-vs-baseline aggregates; it highlights that some emulated tasks are easy for random search while others are hard and that ill-chosen planners can require an order-of-magnitude more experiments.",
            "efficiency_gain": "Not quantified generically in the paper; empirical gains are intended to be measured per-emulator within Olympus.",
            "tradeoff_analysis": "The paper discusses that BO's computational overhead and algorithmic scaling vary per method but generally are small relative to lab experiment durations in typical use cases; it encourages benchmarking to understand tradeoffs between compute demand and sample-efficiency.",
            "optimal_allocation_findings": "Recommends using acquisition-function based planners for sample-efficient sequential allocation in noisy experimental contexts, but stresses that suitability depends on problem structure, noise, and dimensionality and must be empirically benchmarked.",
            "uuid": "e2507.1"
        },
        {
            "name_short": "BNN emulators",
            "name_full": "Bayesian neural network based experiment emulators",
            "brief_description": "Probabilistic emulators (BNNs) trained on experimental datasets to cheaply emulate noisy experimental response surfaces, returning predictive distributions conditioned on input parameters to enable large-scale simulated campaigns and acquisition computations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian neural network emulators",
            "system_description": "Feedforward fully connected Bayesian neural networks model weight/bias distributions and are trained (with cross-validation) on 80% of the data (20% test) to emulate experiment response and noise; BNNs return predictive samples enabling uncertainty-aware planners to compute acquisition values; inference is typically &lt;1 ms per query.",
            "application_domain": "Emulation of experimental response surfaces for chemistry and materials science datasets (reaction yields, photodegradation, color mixing, HPLC, etc.) to benchmark experiment planners.",
            "resource_allocation_strategy": "Not an allocation algorithm itself — it supports allocation by providing cheap, stochastic responses with calibrated uncertainty so planners can trade off information gain vs experiment cost in simulated campaigns. By enabling many low-cost queries, BNN emulators shift allocation decisions from physical experiments to fast in-silico evaluations for planner benchmarking.",
            "computational_cost_metric": "Emulator inference time (&lt;1 ms per evaluation on a standard laptop) and training overhead (cross-validation and training, not specifically timed in paper).",
            "information_gain_metric": "BNN predictive distributions enable computation of acquisition metrics (e.g., Expected Improvement) by planners; the emulator provides uncertainty estimates necessary for expected-utility computations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not applicable to emulator itself; enables planners that implement acquisition-based exploration/exploitation trade-offs by providing uncertainty-aware predictions.",
            "diversity_mechanism": "Emulators do not directly promote diversity; they facilitate planner strategies that may do so by providing many low-cost evaluations to evaluate diverse candidates.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Used to study fixed-number-of-evaluations budgets for planners in simulation; emulators themselves are inexpensive so budgets reflect experimental budgets rather than emulator cost.",
            "budget_constraint_handling": "Because emulator evaluations are cheap, Olympus uses emulators to run many independent campaigns (e.g., 100 campaigns of 10,000 evaluations) yielding statistical baselines for budget-restricted comparisons.",
            "breakthrough_discovery_metric": "Same as for planners: best-observed target property in emulated runs; emulators enable estimation of probability-of-discovery curves vs budget.",
            "performance_metrics": "Emulator fidelity measured by Spearman's rank correlation (&gt;0.90 for all BNN emulators on both training and test sets), R^2 and RMSE reported per emulator in figures; inference latency ~&lt;1 ms.",
            "comparison_baseline": "Not an optimizer; BNN emulators are used to compare planners against Random Search baseline and against each other.",
            "performance_vs_baseline": "Not applicable to emulators; they provide the ground on which planner performance vs baseline is measured.",
            "efficiency_gain": "Enables orders-of-magnitude cheaper evaluation vs physical experiments (ms vs minutes), facilitating large-scale benchmarking that would be infeasible with real experiments.",
            "tradeoff_analysis": "Paper emphasizes that emulators allow separating algorithmic-computation tradeoffs from experimental resource costs and permit exploration of planner behavior over large budgets to analyze sample-efficiency and robustness under noise.",
            "optimal_allocation_findings": "By enabling rapid, noisy emulation, BNNs let practitioners empirically determine which planners allocate experimental budgets most effectively for a specific experimental surface and noise regime.",
            "uuid": "e2507.2"
        },
        {
            "name_short": "Random Search",
            "name_full": "Uniform random search baseline",
            "brief_description": "A naive experiment-planning strategy that samples parameter configurations uniformly at random within specified parameter bounds and is used in the paper as a baseline to quantify planner efficiency.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Random search (uniform)",
            "system_description": "At each step, a parameter vector is generated by sampling each parameter independently from a uniform distribution over its allowed range; no information from previous observations is used to select future evaluations.",
            "application_domain": "Baseline experiment planning for optimization tasks in chemistry and materials science emulated in Olympus.",
            "resource_allocation_strategy": "Allocate budget uniformly at random across parameter space, independent draws per evaluation; used to build a baseline distribution of best-achieved values vs number of evaluations.",
            "computational_cost_metric": "Minimal compute per sample; computational cost mainly the cost of evaluating the emulator or experiment; the paper reports running 100 independent random-search campaigns of 10,000 evaluations each for baselines.",
            "information_gain_metric": "None — random search does not attempt to maximize information gain.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploration (no exploitation), since it never uses past data to focus sampling.",
            "diversity_mechanism": "Random sampling ensures statistical coverage but does not actively enforce structured diversity beyond randomness.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-evaluations budgets; baseline executed for 10,000 evaluations per campaign (100 campaigns) and additionally recommended comparison budgets [1,3,10,30,100,300,1000,3000,10000].",
            "budget_constraint_handling": "Simple termination after predefined number of draws; performance tracked as best-observed value vs budget.",
            "breakthrough_discovery_metric": "Best-observed objective value over the run; distribution of best values over independent runs is used to gauge difficulty of an emulator.",
            "performance_metrics": "Average best value vs evaluations (solid blue lines in paper figures) and boxplots of distribution over 100 runs; identification of emulators where random search quickly finds near-optimal values vs those where it does not.",
            "comparison_baseline": "Used as the baseline to compare all other planners within Olympus.",
            "performance_vs_baseline": "By definition, random search is baseline; paper reports that on some emulators random search reaches near-optimal values quickly while on others it fails even at 10,000 evaluations.",
            "efficiency_gain": "Not applicable; other planners are expected to outperform random search in sample-efficiency, but specific numeric improvements are dataset- and planner-dependent and not universally reported.",
            "tradeoff_analysis": "Random search highlights the intrinsic difficulty of tasks and provides a lower bound on performance; paper suggests the magnitude by which planners outperform random search is a proxy for planner efficiency.",
            "optimal_allocation_findings": "Random search performance identifies which emulators are inherently easy vs hard; the paper uses this insight to motivate selection and benchmarking of more sophisticated allocation strategies.",
            "uuid": "e2507.3"
        },
        {
            "name_short": "Evolutionary algorithms",
            "name_full": "Population-based evolutionary strategies (Genetic algorithms, CMA-ES, Particle Swarm, Differential Evolution)",
            "brief_description": "Heuristic population-based optimizers that evolve a set of candidate solutions using selection, mutation, recombination, and population update rules to explore and exploit the search space.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Evolutionary algorithms (Genetic, CMA-ES, PSO, Differential Evolution)",
            "system_description": "Maintain a population of candidate parameter vectors evaluated on the objective; new candidates are generated by variation operators (mutation, crossover, covariance adaptation) and selected based on fitness; many variants (CMA-ES, PSO, Differential Evolution) provide different heuristics for mutation and adaptation.",
            "application_domain": "Global optimization for experiment planning in chemistry/materials when gradient information is unavailable or surfaces are multi-modal and noisy.",
            "resource_allocation_strategy": "Allocate a batch of evaluations per generation across population members; selection biases future allocations toward higher-fitness regions while variation maintains exploration; population size and number of generations set the budget.",
            "computational_cost_metric": "Cost proportional to population size × evaluations per generation; additional overhead for internal adaptation steps (e.g., covariance updates in CMA-ES). Paper notes algorithms differ in runtime and memory scaling with sample number and dimensionality.",
            "information_gain_metric": "Not explicit — selection is fitness-driven rather than calculated via explicit expected information metrics; implicit information collected via population fitness landscape sampling.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration is maintained by stochastic variation (mutation, recombination) and population diversity; exploitation obtained by selection pressure toward better-performing individuals and adaptation mechanisms (e.g., CMA-ES covariance adaptation).",
            "diversity_mechanism": "Built-in via population diversity, mutation and recombination operators; CMA-ES adapts search distribution to maintain effective exploration.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed computational/evaluation budget specified via population size and number of generations or total evaluations.",
            "budget_constraint_handling": "Practically handled by tuning population size and number of iterations; users must trade off per-generation breadth vs depth given evaluation budget.",
            "breakthrough_discovery_metric": "Best-found fitness (objective) across population across generations; convergence behavior and ability to escape local minima are proxies for breakthrough capability.",
            "performance_metrics": "Not quantitatively compared in aggregate across datasets in the paper; included among Olympus planners to be benchmarked against the random baseline and other methods.",
            "comparison_baseline": "Random search baseline, Bayesian optimization, grid/quasi-random, heuristics, gradient-based methods within Olympus.",
            "performance_vs_baseline": "Not reported in aggregate; performance is dataset- and hyperparameter-dependent and intended to be evaluated within Olympus.",
            "efficiency_gain": "Depends on implementation and tuning; the paper notes evolutionary strategies can be competitive especially in multi-modal landscapes but may require more evaluations compared to model-based BO in some contexts.",
            "tradeoff_analysis": "Paper notes algorithmic runtime and memory scaling differs across methods and can affect applicability, but for typical experimental runtimes (&gt;10 min) compute overhead from evolutionary algorithms is often not limiting.",
            "optimal_allocation_findings": "Population-based methods provide an explicit mechanism to maintain hypothesis diversity while allocating evaluations, but optimal allocation (population vs iterations) must be chosen per problem and can be explored empirically using Olympus.",
            "uuid": "e2507.4"
        },
        {
            "name_short": "Grid / Quasi-random",
            "name_full": "Grid-like searches and low-discrepancy sequences (Grid, Latin Hypercube, Sobol)",
            "brief_description": "Deterministic or quasi-random schemes that sample the parameter space to ensure coverage (full factorial grids, Latin Hypercube, Sobol sequences), trading off exponential scaling in dimensionality for reliable global coverage.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Grid-like sampling and low-discrepancy sequences",
            "system_description": "Predefine a set of parameter points to be evaluated (grid), or generate quasi-random low-discrepancy samples (Latin Hypercube, Sobol) that more uniformly cover high-dimensional spaces; random search is also included as a degenerate 'grid-like' strategy.",
            "application_domain": "Experimental design and initial exploration for low-dimensional parameter spaces in chemistry/materials; initialization strategy for other planners.",
            "resource_allocation_strategy": "Allocate experiments by iterating through the predefined sampling plan; does not use past observations to adapt allocation (except that planners can wrap these samplers as initialization phases).",
            "computational_cost_metric": "Cost is number of evaluations times per-evaluation cost; full grid scales exponentially with dimensionality, making cost explode with more parameters.",
            "information_gain_metric": "Not explicitly used; design aims to maximize space-filling coverage rather than expected information gain measured by mutual information or EI.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploration via predetermined coverage; low-discrepancy sequences approximate uniform coverage and thereby increase diversity of sampled hypotheses.",
            "diversity_mechanism": "Explicit: low-discrepancy sequences (Latin Hypercube, Sobol) are used to promote even coverage and diversity across the domain.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-evaluations; user's choice of sequence length or grid density defines budget.",
            "budget_constraint_handling": "Budget determines sampling density; in high dimensions, selective use of low-discrepancy sampling is recommended to achieve coverage with limited budgets.",
            "breakthrough_discovery_metric": "Best-observed objective found in sampled design; coverage increases chance to sample high-performing regions but does not actively seek breakthroughs.",
            "performance_metrics": "Not universally reported; within Olympus grid/quasi-random methods serve as comparators to adaptive planners and can be evaluated at the same evaluation budgets.",
            "comparison_baseline": "Compared against adaptive planners and random search within Olympus.",
            "performance_vs_baseline": "Performance depends on problem dimensionality and surface structure; paper notes grid-like searches can identify global optima reliably in low dimensions but cost scales poorly with dimensionality.",
            "efficiency_gain": "Low-discrepancy sequences improve coverage efficiency relative to naive full grids at limited budgets; exact gains depend on dimensionality and objective landscape.",
            "tradeoff_analysis": "Paper highlights exponential cost scaling for grid methods vs improved global coverage; recommends low-discrepancy sequences or random search for higher-dimensional (&gt;~5) problems.",
            "optimal_allocation_findings": "For low-dimensional experimental design, planned, space-filling allocations (grid or quasi-random) can be effective; for higher dimensions, randomized or adaptive strategies are generally preferred due to budget scaling.",
            "uuid": "e2507.5"
        },
        {
            "name_short": "Heuristic optimizers",
            "name_full": "Heuristic local/global methods (Snobfit, Simplex, Basin Hopping)",
            "brief_description": "Heuristic optimization strategies combining local and global search heuristics, designed to be robust to noise and to practical experimental constraints.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Heuristic optimizers (Snobfit, Nelder–Mead Simplex, Basin Hopping)",
            "system_description": "Snobfit combines local and global search tailored for noisy evaluations; Simplex (Nelder–Mead) uses simplex geometry for downhill moves (local); Basin Hopping alternates local minimizations with stochastic global moves to escape basins. They are derivative-free and intended for practical optimization under noise.",
            "application_domain": "Noisy black-box experimental optimization where gradient information is unreliable and robustness to noise is important (chemical reaction optimization, materials experiments).",
            "resource_allocation_strategy": "Propose new evaluations based on local heuristics and global branching (Snobfit), simplex reflections/expansions/contractions (Simplex), or global random jumps followed by local minimization (Basin Hopping); allocation emphasizes iteratively improving local candidates while occasionally exploring globally.",
            "computational_cost_metric": "Cost proportional to number of proposed evaluations and overhead of local-fit steps; Snobfit is designed with practical computational efficiency considerations for noisy experiments.",
            "information_gain_metric": "Not explicitly framed as information-theoretic; strategies are heuristic and focused on improving objective under noisy measurements rather than maximizing formal information gain.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Hybrid mechanisms: Simplex is local-exploitative; Basin Hopping alternates exploration (random hops) and exploitation (local minimization); Snobfit explicitly mixes local and global probing to handle noise and multimodality.",
            "diversity_mechanism": "Implicit via global moves or branching heuristics (Basin Hopping, Snobfit) which promote visiting diverse basins; Simplex lacks explicit diversity promotion.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-evaluations or termination criteria based on convergence; users set iteration limits.",
            "budget_constraint_handling": "Algorithms proceed until evaluation budget exhausted or convergence; they do not typically optimize allocation across multiple objectives or costs beyond their heuristic rules.",
            "breakthrough_discovery_metric": "Best-found objective; robustness to noise and ability to escape local minima are key to discovering high-impact solutions.",
            "performance_metrics": "Reported qualitatively in the paper and illustrated in example plots (e.g., Fig. 3 showing Random Search vs Simplex on an emulator), but no universal numeric performance claims across all datasets.",
            "comparison_baseline": "Random search and other planners within Olympus.",
            "performance_vs_baseline": "Example (Fig. 3) shows Simplex outperforming Random Search on the fullerenes emulator in average best-measurement over 100 iterations in that case; general performance is task-dependent.",
            "efficiency_gain": "Task dependent; heuristics can be more efficient than random search in low-noise, locally-smooth landscapes, but may be trapped by multimodality.",
            "tradeoff_analysis": "Heuristic methods offer lower computational overhead and practical robustness to noise but may sacrifice global exploration; hybrid methods (Snobfit, Basin Hopping) aim to balance these tradeoffs.",
            "optimal_allocation_findings": "Hybrid local-global heuristics are recommended when noise and local structure dominate, but their efficacy should be benchmarked against adaptive, model-based planners using tools like Olympus.",
            "uuid": "e2507.6"
        },
        {
            "name_short": "Sherpa (Bandit/Early-stopping)",
            "name_full": "Sherpa toolkit (hyperparameter optimization with bandit/early-stopping schemes)",
            "brief_description": "A Python toolkit for hyperparameter tuning that includes Bayesian optimization, evolutionary approaches, and Bandit/Early-stopping schemes to allocate computational resources adaptively across candidate trials.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Sherpa (hyperparameter optimization with bandit/early-stopping)",
            "system_description": "Orchestration toolkit for hyperparameter optimization that implements multiple optimization algorithms and includes resource-allocation strategies like bandit/early-stopping (e.g., successive halving) that adaptively terminate poor trials to reallocate compute to promising candidates.",
            "application_domain": "Hyperparameter optimization of machine learning models; cited as related work demonstrating resource-allocation decisions that trade computational cost vs expected benefit.",
            "resource_allocation_strategy": "Adaptive allocation via bandit/early-stopping schemes: allocate partial resources to many trials, evaluate intermediate performance, and stop underperforming trials early to concentrate resources on promising ones.",
            "computational_cost_metric": "Implicitly measured as wall-clock or compute resources per trial; Sherpa orchestrates training-evaluation cycles and can reduce total compute by early termination.",
            "information_gain_metric": "Not specified in the Olympus text; bandit schemes use intermediate performance as a proxy for future improvement rather than formal information measures.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration via launching many short trials; exploitation via early-stopping to divert resources to better-performing trials.",
            "diversity_mechanism": "Implicit: by initially exploring many configurations, Sherpa preserves diversity early on; early-stopping prunes unpromising parts of the search space.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Computational resource budget (time/compute) and number of trials; Sherpa is intended to optimize under such constraints.",
            "budget_constraint_handling": "Uses bandit-style methods and early-stopping to reallocate limited compute resources away from poor trials toward promising ones.",
            "breakthrough_discovery_metric": "Not specified in Olympus; in hyperparameter contexts this would be final model performance; in experimental contexts analogous metrics would be objective improvement.",
            "performance_metrics": "Not provided in the Olympus paper; Sherpa is mentioned as a related toolkit with adaptive resource allocation capabilities.",
            "comparison_baseline": "Mentioned alongside Optuna and others as related work; not benchmarked within Olympus.",
            "performance_vs_baseline": "Not discussed in Olympus; referenced to illustrate types of resource-allocation schemes (bandit/early-stopping) used elsewhere.",
            "efficiency_gain": "Not quantified in Olympus; early-stopping and bandit schemes are generally known to reduce compute by terminating poor trials early, potentially large savings.",
            "tradeoff_analysis": "Sherpa exemplifies approaches that trade-off upfront broad exploration (many short runs) against focused exploitation (long runs for promising candidates) to reduce compute while retaining discovery potential.",
            "optimal_allocation_findings": "Not provided by Olympus for Sherpa; referenced to underline that different application areas use adaptive compute-allocation mechanisms that could be relevant for experiment planning.",
            "uuid": "e2507.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Phoenics: A Bayesian optimizer for chemistry",
            "rating": 2,
            "sanitized_title": "phoenics_a_bayesian_optimizer_for_chemistry"
        },
        {
            "paper_title": "GPyOpt: A Bayesian Optimization Framework in Python",
            "rating": 2,
            "sanitized_title": "gpyopt_a_bayesian_optimization_framework_in_python"
        },
        {
            "paper_title": "HyperOpt: Algorithms for hyper-parameter optimization",
            "rating": 2,
            "sanitized_title": "hyperopt_algorithms_for_hyperparameter_optimization"
        },
        {
            "paper_title": "Snobfit - stable noisy optimization by branch and fit",
            "rating": 2,
            "sanitized_title": "snobfit_stable_noisy_optimization_by_branch_and_fit"
        },
        {
            "paper_title": "Snobfit-stable noisy optimization by branch and fit",
            "rating": 1,
            "sanitized_title": "snobfitstable_noisy_optimization_by_branch_and_fit"
        },
        {
            "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "rating": 2,
            "sanitized_title": "a_bayesian_experimental_autonomous_researcher_for_mechanical_design"
        },
        {
            "paper_title": "Accelerated search for materials with targeted properties by adaptive design",
            "rating": 2,
            "sanitized_title": "accelerated_search_for_materials_with_targeted_properties_by_adaptive_design"
        },
        {
            "paper_title": "Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories",
            "rating": 2,
            "sanitized_title": "chimera_enabling_hierarchy_based_multiobjective_optimization_for_selfdriving_laboratories"
        }
    ],
    "cost": 0.023679999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Olympus: a benchmarking framework for noisy optimization and experiment planning</p>
<p>Florian Häse 
Department of Chemistry and Chemical Biology
Harvard University
02138CambridgeMassachusettsUSA</p>
<p>Vector Institute for Artificial Intelligence
M5S 1M1TorontoONCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Department of Chemistry
Chemical Physics Theory Group
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Atinary Technologies Sàrl
1006LausanneVDSwitzerland</p>
<p>Matteo Aldeghi 
Vector Institute for Artificial Intelligence
M5S 1M1TorontoONCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Department of Chemistry
Chemical Physics Theory Group
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Riley J Hickman 
Department of Computer Science
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Department of Chemistry
Chemical Physics Theory Group
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Loïc M Roch 
Vector Institute for Artificial Intelligence
M5S 1M1TorontoONCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Department of Chemistry
Chemical Physics Theory Group
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Atinary Technologies Sàrl
1006LausanneVDSwitzerland</p>
<p>Melodie Christensen 
Process Research and Development, Merck &amp; Co., Inc
RahwayNJUSA</p>
<p>Department of Chemistry
The University of British Columbia
V6T 1Z1VancouverCanada</p>
<p>Elena Liles 
Department of Chemistry
The University of British Columbia
V6T 1Z1VancouverCanada</p>
<p>Jason E Hein 
Department of Chemistry
The University of British Columbia
V6T 1Z1VancouverCanada</p>
<p>Alán Aspuru-Guzik 
Vector Institute for Artificial Intelligence
M5S 1M1TorontoONCanada</p>
<p>Department of Computer Science
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Department of Chemistry
Chemical Physics Theory Group
University of Toronto
M5S 3H6TorontoONCanada</p>
<p>Lebovic Fellow
Canadian Institute for Advanced Research
M5G 1Z8TorontoOntarioCanada</p>
<p>Olympus: a benchmarking framework for noisy optimization and experiment planning
(Dated: March 31, 2021)
Research challenges encountered across science, engineering, and economics can frequently be formulated as optimization tasks. In chemistry and materials science, recent growth in laboratory digitization and automation has sparked interest in optimization-guided autonomous discovery and closed-loop experimentation. Experiment planning strategies based on off-the-shelf optimization algorithms can be employed in fully autonomous research platforms to achieve desired experimentation goals with the minimum number of trials. However, the experiment planning strategy that is most suitable to a scientific discovery task is a priori unknown while rigorous comparisons of different strategies are highly time and resource demanding. As optimization algorithms are typically benchmarked on low-dimensional synthetic functions, it is unclear how their performance would translate to noisy, higher-dimensional experimental tasks encountered in chemistry and materials science. We introduce Olympus, a software package that provides a consistent and easy-to-use framework for benchmarking optimization algorithms against realistic experiments emulated via probabilistic deep-learning models. Olympus includes a collection of experimentally derived benchmark sets from chemistry and materials science and a suite of experiment planning strategies that can be easily accessed via a user-friendly python interface. Furthermore, Olympus facilitates the integration, testing, and sharing of custom algorithms and user-defined datasets. In brief, Olympus mitigates the barriers associated with benchmarking optimization algorithms on realistic experimental scenarios, promoting data sharing and the creation of a standard framework for evaluating the performance of experiment planning strategies. * These authors contributed equally † alan@aspuru.com</p>
<p>I. INTRODUCTION</p>
<p>Optimization tasks are ubiquitous across science, engineering, and economics. They typically involve the identification of specific choices for controllable parameters under which a system of interest yields a desired response. The development of efficient strategies that lead to the discovery of such optimal parameter choices is of significant importance and has long been of interest to many scientific communities. Selecting an appropriate optimization strategy for a problem with unknown structure is non-trivial given that a single, overall superior strategy does not exist. 1,2 Specifically, the qualities of a single optimization strategy including convergence, computational demand, or requirements on the function to be optimized, could be ideal for some applications but render the same strategy inapplicable to other tasks. Understanding the challenges of optimization tasks in specific domains and the behavior of different algorithms for such tasks is essential to the development of efficient search strategies that are suitable to the considered application. Empirical assessments of the performance of different optimization strategies on realistic and domainrelevant scenarios is thus of paramount practical relevance.</p>
<p>One aspect where optimization has recently gained increased attention is the digitization of scientific discovery with autonomous platforms. [3][4][5] The emergence of ever more sophisticated and reliable automated experimentation equipment in chemistry and materials science over the last decades has increasingly allowed for formulation of scientific discovery as an optimization task. [6][7][8] In this formulation, compositions of candidate materials and processing conditions to fabricate multi-component materials are optimized to reach desired goals with respect to the physical and chemical properties of the synthesized material. Key missions in these fields relate to the discovery of functional molecules and advanced materials to tackle societal challenges such as climate change, renewable energy, sustainability, or clean water, which can be directly approached by modifying the structures and compositions of candidate materials to optimize their physical and chemical properties. 9 Automated instrumentation is now being combined arXiv:2010.04153v2 [stat.ML] 30 Mar 2021 with data-driven optimization strategies to enable autonomous molecule and materials development in self-driving laboratories. 10 Autonomous experimentation leverages these data-driven strategies to suggest molecules or materials candidates that are synthesized and characterized by robotic platforms, [10][11][12][13] with realtime feedback on the suggested candidates being collected in the form of physical or chemical measurements. In this vision, the experimentation process requires minimal human intervention once the experimental campaign has been defined. The integration of algorithmic experiment planners with robotic hardware into an autonomous platform has already been shown to substantially lower the development costs of organic photovoltaic materials, 14 identify novel chemical reactions, 15 yield unexpected findings for thin film technologies, 16 the discovery of photocatalysts for hydrogen production from water, 17 and mechanical design, 18 amongst other applications. Several different optimization strategies have already been used for automated scientific discovery. While some of these optimization algorithms have been designed for broad applicability across general optimization tasks, other approaches have been developed with the more specific goal of planning laboratory experiments and are based on assumptions about the expected experimental response surfaces. For example, Design of experiments (DoE) constitutes a frequently employed strategy to identify optimal conditions for chemical reactions, 19,20 where the system of interest is probed on a grid of different parameter choices. Chemical reactions have also been optimized with the Snobfit algorithm, [21][22][23] variants of the Simplex method, [24][25][26] or even gradientbased strategies. 26 Bayesian optimization frameworks have been demonstrated on materials science applications, most often realized using Gaussian processes [27][28][29] or random forests. 30 While the experiment planning strategies deployed in the aforementioned examples enabled autonomous workflows, it is not clear whether they are the most efficient ones for the considered task. In fact, it has recently been reported that ill-chosen planners can increase the budget requirements for scientific discovery in the context of materials science by up to an order of magnitude. 31 Without comprehensive benchmarks, availability and ease-of-use might be the primary considerations behind the choice of experiment planning strategy, while other factors such as the speed of convergence or the computational demand are neglected. The lack of the ability to evaluate the effectiveness of different experiment planning strategies thus poses a major obstacle to the development of autonomous research platforms.</p>
<p>To resolve this challenge, we propose to benchmark experiment planning strategies on probabilistic models. These models can emulate noisy experimental responses after being trained on experimental data, as previously demonstrated in the context of multi-objective optimization with autonomous research platforms. 32 In particu-lar, we suggest to use Bayesian neural networks (BNNs) due to their robustness, scalability and non-local generalization capabilities. The outcome (e.g., reaction yield, solubility, etc.) of a specific set of experimental parameters (e.g., concentration, temperature, etc.) can be emulated by drawing a predictive sample from the BNN, conditioned on these parameters. This approach provides a viable avenue to benchmarking experiment planning algorithms in the presence of noise and on realistic, experimentally-derived response surfaces.</p>
<p>Following this idea of emulating experimental response based on real data, we introduce Olympus, a comprehensive software package that provides the possibility to probe the performance of experiment planning strategies on emulated experimental surfaces collected from experiments in chemistry and materials science. Olympus implements a common interface to 18 different experiment planning strategies and thus simplifies the implementation of closed-loop autonomous workflows. Olympus further provides a collection of 10 experimental datasets for which emulators have been trained to serve as a standard set of benchmarks, and a collection of 23 analytical surfaces which can be modulated by different sources of stochastic noise. An automated benchmarking process that determines the most efficient planner for a given application is available. As such, Olympus provides the means to run comprehensive comparisons of novel optimization algorithms and planning strategies to existing ones, allowing to identify the strengths and limitations of individual tools for various scientific discovery tasks. Its capacity to construct probabilistic approximations to experimental surfaces from collected data, modeling both the expected response and the noise modulations, makes Olympus a realistic benchmark suite without the need for excessive and resource demanding experimentation.</p>
<p>In the following, we summarize the datasets and emulators available through Olympus as well as the experiment planning strategies for which intuitive yet flexible interfaces have been implemented. We further highlight the application programming interface of Olympus, demonstrate how individual planners can be accessed and comprehensive benchmarks constructed with only a few lines of code. We conclude by providing a performance baseline comprised of a uniform random search and invite the community to develop and demonstrate more efficient experiment planning strategies on the Olympus benchmarks.</p>
<p>II. BACKGROUND AND RELATED WORKS</p>
<p>While the goal of an optimization task is usually well defined, the setting in which this task is approached might differ from application to application. Thus, the applicability of optimization strategies to certain tasks can be assessed based on multiple criteria, which are designed to highlight strengths and shortcomings of individual strategies on the considered application.  48 and Moses 49 offer benchmarking functionalities for de novo molecular design. These examples provide datasets which aim to model realistic abstractions of the targeted applications on which optimization algorithms can be benchmarked. Yet, the requirements of comprehensive benchmarking frameworks go beyond realistic use cases and also include: (i) intuitive interfaces to interact with these datasets, (ii) interfaces to established algorithms to benchmark, (iii) tools to store and analyze collected results, and (iv) the flexibility to allow the community to extend the framework with additional datasets and algorithms.</p>
<p>Tab. I reports a set of currently available benchmarking toolkits for different applications. Coco is a platform for the systematic comparison of real-parameter global optimizers. 33 It provides benchmark function testbeds, experimentation templates which are easy to parallelize, and tools for processing and visualizing data generated by one or several optimizers. Coco focuses on runtime as the central performance measure and optimization tasks on continuous domains with dimensionalities beyond those typically encountered in chemistry and materials science. The OpenAI Gym offers a series of environments and tasks to test reinforcement learning algorithms. 34 Sherpa is a Python toolkit for hyperparameter tuning of machine learning models. 35 As such, Sherpa offers the automated optimization of hyperparameters via a choice of hyperparameter optimization algorithms including Bayesian optimization, evolutionary approaches and Bandit/Early-stopping schemes. Sherpa orchestrates the entire optimization process and results can be visualized in a comprehensive dashboard. However, Sherpa does not provide synthetic or noisy benchmark cases. Optuna is another toolkit that focuses on the optimization of hyperparameters for machine learning models. 36 In contrast to Sherpa, Optuna implements a define-by-run interface for a dynamic construction of search spaces. However, it also does not provide benchmark cases. Pygmo is a library for massively parallel optimization, which provides a unified interface to a number of gradient and heuristic based optimization algorithms, as well as to synthetic benchmark problems. Pygmo also provides algorithms and benchmarks for constrained and multi-objective optimization problems.</p>
<p>The aforementioned software packages have been developed with ML applications in mind. Summit, however, provides a selection of chemically motivated virtual benchmarks and a selection of experiment planning strategies. Although the application space of Summit is heavily focused on reaction optimization, it targets a realistic modeling of its use cases via physical and statistical models. In contrast, Olympus is tailored to the needs of optimization in a broader range of experimental disciplines, including self-driving laboratories and autonomous experimentation workflows. Specifically, it constitutes a framework to assess the algorithmic performance of data-driven experiment planning strategies in the context of autonomous experimentation for chemistry and materials science. It targets optimization tasks in chemistry and materials science, where the number of parameters to optimize is typically smaller than 10. To serve this purpose, Olympus provides interfaces to optimization algorithms commonly used for experiment planning tasks and offers interfaces to noisy emulators of experimental optimization tasks. In addition, the benchmaking capabilities of Olympus are open to be extended by the community who can contribute their own datasets (see Sec. IV.F).</p>
<p>III. PACKAGE OVERVIEW</p>
<p>Olympus is a modular software package that allows user interactions at different levels and can be used for data-driven experimentation as well as benchmarking experiment planning strategies. With this modularity, Olympus allows for both beginner and expert use and enables performing several optimization and benchmarking tasks in a few lines of code. Some common usecase scenarios are detailed in Sec. IV, including (i) the High-level overview of Olympus and its four core modules: (i) planners, which provide interfaces to common or custom optimization algorithms for experiment planning, (ii) surfaces, which constitute standardized interfaces to established synthetic benchmark surfaces, (iii) emulators, which describe a set of ML models trained to reproduce experimentally derived response surfaces encountered in chemistry and materials science, and (iv) datasets, which form a collection of experimental campaigns. All four core modules offer the possibility to implement and add custom methods and data.</p>
<p>use of different experiment planners for an autonomous workflow, (ii) benchmarking an experiment planner on an emulator, and (iii) constructing an emulator from a user-provided dataset. At the heart of Olympus are four modules, planners, surfaces, datasets, and emulators, which are highlighted conceptually in this section and in Fig. 1.</p>
<p>The planners module (see Sec. III.A) provides a consistent interface to 18 different experiment planning strategies via its core Planner class. Olympus translates a standardized access protocol to the interfaces of individual planners, making it easy to switch the experiment planning strategy of an autonomous workflow. This module also provides the basis for integrating customized algorithms into the package. Available planners are listed in Table II. The surfaces module provides a set of synthetic response surfaces, which are functions commonly used to evaluate and compare the performance of optimization algorithms. Similar to the planners module, a convenient Surface class allows to easily retrieve the desired analytical surface. Available surfaces are listed in Table IV. While these surfaces return deterministic function evaluations by default, it is possible to pass a noise object that results in stochastic evaluations, as shown in Sec. IV.B.</p>
<p>The datasets module in Olympus offers 10 core experimentally derived datasets from chemistry and materials science. These datasets vary in size and represent optimization tasks with dimensionalities from 3 to 6. The core class of this module is Dataset, which allows for retrieval and manipulation of the desired dataset. Available datasets are listed in Table III. Users can also load their own dataset, which can then be used to benchmark experiment planning strategies for the specific problem of interest. Furthermore, users can share their datasets with the commu-nity by uploading it to the Olympus repository of userprovided datasets at https://github.com/aspuru-guzikgroup/olympus datasets. Any user can then download these additional datasets to be used in Olympus via the same interface used for the core datasets.</p>
<p>The emulators module provides access to probabilistic models trained on the core Olympus datasets, reproducing the experimental responses of the corresponding experiments. With its Emulator class, this module also offers a high-level interface for the training of such probabilistic models on user-provided datasets. In this spirit, emulators constitute stochastic response surfaces resembling those encountered in real-life applications, thus allowing to benchmark experiment planning strategies on close-to-reality optimization tasks.</p>
<p>A. Summary of included planners</p>
<p>This section details the types of experiment planning strategies and algorithms available in Olympus and listed in Table II. More information about each specific planner can be found on the online documentation.</p>
<p>Gradient approaches use derivative information (gradient or Hessian) at the current point to determine the location of the next point to be evaluated. Such strategies are efficient on convex optimization problems, but are not guaranteed to find the global optimum on non-convex surfaces. 75,76 Most gradient-based approaches condition both the stepping direction and the step size on the local gradient. The numerical approximation of gradients generally poses a challenge in the context of experimentation where the response surface is subject to noise. Nevertheless, gradient-based search strategies have been reported for the optimization of some chemical processes. 77 Grid-like searches constitute a more common approach to experiment planning. [68][69][70] These strategies define a set of selected parameter points in the parameter space to be evaluated at the start of the optimization campaign. At every step of the campaign, the next point to be evaluated is chosen deterministically. Although grid-like searches mitigate the locality issue of gradient approaches and can reliably identify global optima, their cost scales exponentially with the dimensionality of the parameter space. Alternatives to standard full grid approaches involve the use of low discrepancy sequences, such as Latin Hypercube or Sobol Sequence, to sample more effectively high dimensional spaces. The discrepancy of a sequence is considered to be low if the proportion of points falling into an arbitrary subset of the considered parameter domain is roughly proportional to the measure of this subset. Low discrepancy sequences are also known as quasi-random sequences and are commonly used to finding characteristic functions of probability density functions, higher-order moments of statistical distributions, and integration and sampling of high-dimensional deterministic functions. Random Search reduces the correlation between consecutive proposals even further and has been shown to be particularly effective in higherdimensional search spaces. 52,78,79 Evolutionary algorithms are population and heuristicbased approaches inspired by biological evolution. [80][81][82][83][84] Each individual in the population represent a point in the search space, and their fitness corresponds to the objective evaluated at that point. Evolutionary strategies, like CMA-ES, Particle Swarms, and Differential Evolution, evolve a population of candidate solutions simultaneously and generate new candidates based on some heuristics. The population is frequently updated, with better candidates replacing worse performing candidates. 85 Genetic algorithms constitute a subclass of evolutionary strategies which mimic mechanisms such as reproduction, mutation, recombination, and selection to iteratively improve the fitness of a population.</p>
<p>Other heuristic-based approaches are not inspired by biological evolution specifically. For example, Basin Hopping is a two-step approach that uses both local and global searches and is inspired by the energy landscape of atom clusters. 73 Snobfit too combines both local and global approaches and the strategy was designed with the goal of addressing a number of practical challenges. 72 Finally, the Simplex algorithm by Nelder and Mead exploits the geometry of simplices to define an update rule that proposes new points in a downhill direction. 74 Bayesian optimization methods are sequential, modelbased approaches for the global optimization of black-box functions. [86][87][88] The function to be optimized is approximated by a surrogate model that is refined as more data is collected. Based on this model, an acquisition function that evaluates the utility of candidate points can be defined, leading to the balanced exploration and exploitation of the search space of interest. Similar to evolutionary strategies, no gradient information is required and they can be used for the global optimization of blackbox functions. What distinguishes Bayesian optimization approaches are primarily the surrogate model and acquisition functions used. GPyOpt uses a Gaussian process to model the objective function, 50 Phoenics adopts a mixture of Gaussian kernels, 54 and HyperOpt uses a tree-structured Parzen estimator. [51][52][53] We note that the algorithms available in Olympus present different computational scalings with respect to the number of samples collected and the dimensional-ity of the optimization domain. These algorithmic and implementation aspects result in different runtimes and memory requirements, which ultimately affect the applicability of each algorithm to different problems. However, given the typical problem dimensionalities (&lt; 10 parameters) and runtimes (&gt; 10 min per experiment) encountered experimentally, the computational cost of any of the algorithms above described is not expected to be of significant importance in autonomous workflows.</p>
<p>B. Summary of included datasets</p>
<p>Olympus ships with a total of ten core datasets collected from experiments spanning chemistry and materials science. The datasets have either been collected from the literature or were generated in-house. With these datasets, Olympus can construct experiment emulators using probabilistic machine learning models, notably BNNs, to emulate the overall response surface of the considered experiment for an arbitrary choice of parameter values (see Sec. III.C for details). As such, the provided datasets constitute the basis for realistic benchmarks of experiment planning strategies. Table III summarizes core information about each dataset and further details are provided in the supplementary information (see Sec. VII.A). All datasets are collected from experimental campaigns with three to six independently controllable parameters, one property of interest, and contain from a few tens to more than 1,000 data samples. Five datasets are related to the optimization of organic chemistry reactions, one is derived from the calibration of analytical chemistry instrumentation, two address the identification of polymer blends of photovoltaic materials with favorable photodegradation properties, and two are related to the identification of the colorant mixture displaying a chosen target color. This core set of datasets can be extended by community datasets contributed from individual research groups. Details are provided in Sec. IV.F.</p>
<p>C. Summary of included emulators</p>
<p>The experiment emulators offered through Olympus provide a core functionality to benchmarking data-driven experiment planning strategies: the opportunity to query the response of a quasi-experimental surfaces inexpensively within milliseconds. Balancing robustness and prediction accuracy on the data-scarce datasets reported in Sec. III.B, we construct Olympus emulators from feedforward fully connected Bayesian neural networks (BNNs). BNNs constitute probabilistic machine learning models which, contrary to standard neural network, define a distribution of possible target values conditioned on the input features. To this end, the conventional weight and bias parameters of standard neural networks are modeled as distributions themselves and the BNN is trained via Bayesian inference. While, in theory, weights and biases can be modeled by any valid distribution, in practice the distributions are often explicitly modeled via a set of parameters, such as the location and scale of a normal distribution. This approximation can greatly accelerate inference computations and make the training of a BNN overall computationally tractable. In addition to probabilistic, BNN-based emulators, we also provide deterministic, NN-based ones. These emulators return the same target value given a set of input features.</p>
<p>Emulators are trained on 80 % of the data and tested on 20 % using a random split. The training set is furthermore split into training and validation sets for cross-validation. By default, 5 folds are used, but users can choose how many folds to use when creating their own emulators. The test set is used to probe the generalizability of the emulator. Model performances on both training and test sets are shown in Fig. 2 for BNN-based emulators, and in Fig. 10 for NN-based emulators. Emulators are constructed with different choices for hyperparameters, including the number of layers, the number of neurons per layer, activation functions, and others, which can be accessed directly through the emulator objects. Activation functions for the output layer have been chosen to satisfy physical constraints, such as positivity of the property of interest. Other hyperparameters have been manually selected to achieve promising prediction accuracies. We define emulators as accurate if they achieve a Spearman's rank correlation coefficient above 0.90 for both training and test sets, given that a monotonic relationship between predicted and measured values preserves the relative ranking of all extrema. Typical evaluations of trained emulators take less than 1 ms on a standard laptop. This cheap experiment emulation approach enable the large-scale querying of experimental responses and the rigorous benchmarking of data-driven experiment planning strategies.</p>
<p>D. Summary of included analytical surfaces</p>
<p>In addition to experimentally-derived benchmarks, Olympus provides a suite of analytical functions traditionally used to evaluate optimization algorithms (Table IV). These functions include 11 analytic and smooth functions, such as the Branin and Rosenbrock functions, 5 piece-wise constant functions, and 6 Gaussian mixture model functions derived from a parent Gaussian mixture generator. This generator takes a number of dimensions as argument, and draws random means and covariances. By default, a full covariance matrix is drawn, but a diagonal matrix can also be requested. By fixing the random seed, the Gaussian mixture generator creates reproducible surfaces. In fact, the six Gaussian mixture models available have been obtained by fixing the random seed of each mountain-named surface (e.g. Everest)  to the height of the respective mountain peak in meters (e.g. 8848).</p>
<p>For all analytical surfaces in Olympus, it is possible to specify noise to be added to the evaluations. In such a way, the output of these toy surfaces will be stochastic. A few commonly used noise functions, like Gaussian, uniform, and gamma-distributed noise, are already implemented and readily available in Olympus. However, custom types of noise can also be defined by the users and provided to the surface of interest, which will then return noisy evaluations. Note, that noise modulations are currently only supported for experimental responses. However, realistic experiments might also be subject to noise on the preparation of experimental parameters, such as the dispensing of desired amounts of chemicals or controlling the temperature in an experimental setup. In Olympus, users may add noise to the experimental parameters by taking advantage of the Planners interface.</p>
<p>Tab. IV summarizes the synthetic benchmark functions available in Olympus. Further details as well as illustrations of the surfaces are reported in the supplementary information (see Sec. VII.C).</p>
<p>IV. USING OLYMPUS</p>
<p>In this section, we detail the usage of Olympus on selected applications and use cases. A detailed documentation of the package is provided on GitHub. 92</p>
<p>A. Installation and dependencies</p>
<p>Olympus is available for download on GitHub 92 and can be installed via pip and conda.  The installation requires Python 3.6+ with support for numpy and pandas. However, to access specific features of the package, such as running an emulator, using specific experiment planners, or plotting the results of completed campaigns, the installation of additional packages might be required. Details are provided in the documentation. 92</p>
<p>B. Evaluate analytical surfaces</p>
<p>The analytical surfaces in Olympus can be accessed via the olympus.surfaces module or the Surface function, with the latter loading a surface with default argument. from olympus.surfaces import Michalewicz surface = Michalewicz(param_dim=2, m=12) # or, to load with default arguments from olympus import Surface surface = Surface("Michalewicz")</p>
<p>The above example defines a surface with deterministic output. However, noise can be added to have a surface instance that returns stochastic evaluations. from olympus.surfaces import Dejong from olympus.noises import GaussianNoise noise = GaussianNoise(scale=0.5) surface = Dejong(param_dim=2, noise=noise) Surfaces can then be evaluated sequentially or in batches as follows. </p>
<p>C. Run a simulated campaign</p>
<p>The datasets available in Olympus can be accessed via the Dataset class, using the keyword associated with each dataset.</p>
<h1>load an Olympus dataset from olympus import Dataset dataset = Dataset("snar") Neural Network (NN) or Bayesian Neural Network (BNN) based emulators are already available in Olympus for all datasets provided. However, the user also has the freedom to train new emulators by customising the models provided in the olympus.models module. from olympus import Emulator emulator = Emulator(dataset="snar", model="BayesNeuralNet") # or customize the model from olympus.models import BayesNeuralNet model = BayesNeuralNet(hidden_depth=4, out_act="sigmoid") emulator = Emulator(dataset="snar", model=model) All algorithms described in the previous section can easily be accessed from the olympus.planners module or via the Planner function. While the former allows the user to choose specific settings for each planner, the latter loads them with default arguments. from olympus.planners import Gpyopt planner = Gpyopt(goal="minimize", model_type="GP_MCMC", acquisition_type="EI_MCMC") # or, to load with default arguments: from olympus import Planner planner = Planner("Gpyopt", goal="minimize") Once a planning algorithm and an emulator have been defined, it is possible to start a simulated optimization campaign using the optimize method. emulator = Emulator(dataset="snar", model="BayesNeuralNet") planner = Planner("Phoenics", goal="minimize") campaign = planner.optimize(emulator=emulator, num_iter=50)</h1>
<p>D. Train custom emulator</p>
<p>With Olympus you can create an Emulator in order to generate a custom emulated response surface for a new dataset. For instance, if you have data for a chemical reaction of interest, for which you would like to optimize the yield, you can load the dataset from a table as follows.</p>
<h1>load a custom dataset from olympus import Dataset import pandas as pd mydata = pd.from_csv("mydata.csv") dataset = Dataset(data=mydata, target_ids=['yield'])</h1>
<p>After this step, you can load one of the available models from the olympus.models module and pass it to a new Emulator instance, which will allow you to cross-validate and train the emulator. Users can override default model hyperparameters by passing custom values as arguments to the olympus.models module. Once you obtain an emulator with satisfactory performance, you can save it. The ask method takes advantage of the param space attribute present in CustomPlanner, which is a list of dictionaries defining the parameter space over which to optimize. In addition, params and values contain the parameters and associated merits for all previous observations, respectively. These attributes will be needed for any algorithm in which the set of parameters proposed depend on the previous observations. Finally, note ask returns a ParameterVector object, which can be instantiated with an array or dictionary of parameters.</p>
<p>In the above example, an init method is not specified. This is because the following default one is inherited from CustomPlanner. def <strong>init</strong>(self, goal='minimize'):</p>
<p>AbstractPlanner.<strong>init</strong>(**locals())</p>
<p>If you would like to initialize your own Planner with more options, you can expand upon the above init method. Note it is required to keep the argument goal and to initialise AbstractPlanner as above. A tutorial on the creation of custom Planner classes with further details on possible customization is available as part of the online documentation.</p>
<p>F. Download/upload community datasets</p>
<p>In addition to the set of core datasets distributed with Olympus, we allow users to share their own datasets with the community. These additional datasets are stored on GitHub and provide an extended set of benchmarks built by the autonomous experimentation community. Olympus provides intuitive command line tools to upload and download these datasets. For instance, to download the excitonics dataset and make it available to your local Olympus installation:</p>
<blockquote>
<blockquote>
<p>olympus download -n excitonics</p>
</blockquote>
</blockquote>
<p>After the download, the excitonics dataset will be available and you will be able to load it in the same way as the core datasets.</p>
<p>from olympus import Dataset dataset = Dataset("excitonics") Note that, for community-provided datasets, trained Emulator instances are not readily available, such that you will need to train the relevant Emulator.</p>
<p>from olympus import Dataset from olympus.models import NeuralNet dataset = Dataset("excitonics") model = NeuralNet(hidden_depth=3, out_act="relu") emulator = Emulator(dataset=dataset, model=model) emulator.train() &gt;&gt;&gt; ...</p>
<p>emulator.save("excitonics_nn_emulator")</p>
<p>If you have a dataset that you think would be a useful benchmark for the community, you can upload it to the Olympus pool of datasets using the Olympus command line tools as follows.</p>
<blockquote>
<blockquote>
<p>olympus upload -n <dataset_name> -p <dataset_path></p>
</blockquote>
</blockquote>
<p>G. Plotting benchmark results</p>
<p>Results collected in several campaigns can be plotted automatically via a comprehensive plotting interface. Plots are generated from a Database object, like the one generated by Olympus when running a benchmark campaign. The following example illustrates the generation of a plot that illustrates the results of the executed benchmark. The generated plot is shown in Fig. 3 </p>
<p>V. A RANDOM SEARCH BASELINE</p>
<p>The promise of data-driven strategies to identify desired parameter choices for experimental setups in closedloop workflows is based on their capacity to condition design choices on feedback collected from previous experiments. The performance of a data-driven experiment planning strategies can, for example, be quantified via the number of experiments required to locate parameter values which yield the desired experimental outcomes. In this paragraph, we aim to provide a baseline for the performance of data-driven experiment planning strategies which can indicate the degree of difficulty that each constructed emulator poses to a planner.</p>
<p>We construct the baseline by probing the performance of the random search strategy on each of the emulators. Random search as an experiment planning approach can be considered to be a naive strategy as it does not leverage any feedback collected in previous experiments. Parameter choices for future experiments are generated by drawing random samples from uniform distributions supported within the allowed ranges of each of the parameters. As such, the suggested parameter values are independent from one another and are not influenced by any past measurements. Data-driven strategies for experiment planning that do condition their design choices on previous feedback are therefore expected to outperform the random search baseline. The magnitude by which random search is outperformed can be used as a proxy to quantify the efficiency of the planner for each emulator.</p>
<p>The provided baseline consists of 100 independent campaigns with 10,000 emulator evaluations per campaign for each of the emulators. Note that results from random baselines are not shipped with the software package and need to be downloaded separately. Random baselines are available on Github 92 and can be downloaded from there or via the Olympus command line interface.</p>
<h1>download random baseline &gt;&gt; olympus baseline --get</h1>
<p>All parameters are generated with the random search planner. The results of these baseline calculations can be accessed through Olympus as follows # load the baseline from olympus import Baseline base = Baseline() summary = base.get('snar', kind='summary') campaigns = base.get('snar', kind='campaigns') database = base.get('snar', kind='db')</p>
<p>While the full traces of the random search baselines are available through Olympus, we suggest to compare the achieved feedback after a specified set of emulator evaluations. We propose to use [1, 3, 10, 30, 100, 300, 1000, 3000, 10000]. This choice is inspired by the fact that most experimental campaigns reported for autonomous experimentation platforms are limited to about 100 experiments. This set of evaluation numbers allows to estimate the performance of each planner in the regime of little data (∼10 evaluations), medium data (∼100 evaluations), abundant data (∼1,000 evaluations) and asymptotic behavior (∼10,000 evaluations).</p>
<p>The results of random search against all core Olympus datasets are illustrated in Fig. 4. Based on these results, we can identify a subset of the emulated surfaces for which random search reaches near-optimal property values in a small number of evaluations. This subset includes photobleaching pce10, photobleaching wf3, colormix bob, colormix n9, snar, and hplc n9. Given that random search does not leverage any feedback from collected measurements for future decisions, these emulated surfaces might be considered to be the simpler cases for a more sophisticated experiment planner. The remaining surfaces, however, including alkox, fullerenes, nbenzylation, and suzuki, might pose a bigger challenge to experiment planners given that asymptotic property values are only achieved after a significant number of random evaluations or not even reached after 10,000 evaluations. Numerical values for the baseline are available through the Olympus package. 92 We hope that the results from this random search can serve as a baseline to compare the performance of different experiment planning strategies such as those already included in Olympus, but also new strategies developed by the community. </p>
<p>VI. CONCLUSION</p>
<p>Standardized and challenging benchmarks are necessary to facilitate precise comparison between different approaches and allow scientific and technological advances to be quantified. Widely used benchmark sets like MNIST and CIFAR-10/100, 39,40 which are comprised of images of hand-written digits and various objects and animals, respectively, have allowed to measure constant advances in machine vision, providing clear feedback to the community on the most promising research directions. MoleculeNet, a collection of quantum mechanical, physical, biophysical and physiological molecular properties, provides a similar example in the field of chemistry and biophysics. 41 Olympus constitutes an orthogonal set of benchmarks, with a focus on optimization and experiment planning in chemistry and materials science, as opposed to prediction. It provides a framework with the potential to spark and streamline the development of powerful algorithms and data-driven approaches aimed at efficient experiment planning. To this end, Olympus also provides intuitive interfaces to a variety of experiment planning strategies to simplify their implementation, deployment, and testing in autonomous discovery workflows. With every user being able to supply their own datasets through our standardized interfaces, Olympus also encourages the free exchange of data across the community and promotes the establishment of standard, reproducible optimization challenges. In summary, Olympus provides a unified framework for the deployment and testing of experiment planning strategies. We thus invite the community to take advantage of Olympus in the implementation and testing of novel approaches to autonomous workflows, as well as to share experimental data that can prove valuable in moving this exciting new field forward. In this section we provide a brief summary of each dataset available in Olympus, along with the parameters, objectives and optimization goal.</p>
<p>Alkoxylation</p>
<p>This dataset contains 104 measurements on biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. Stock solution were prepared daily and stored over crushed ice. Additional dilutions were done as required using sodium phosphate buffer and immediately discarded after use. The assays were initiated by the addition of Cgr AlcOx and H 2 O 2 to a well-mixed HPLC vial containing all other reaction components. The initial rate was obtained by fitting the concentration of aldehyde to a linear function and reporting the slope. Conversion was calculated by fitting the percent conversion of aldehyde to a linear function and reporting the value at twenty minutes.    </p>
<p>Buckminsterfullerene adducts</p>
<p>This dataset is based on the reported production of o-xylenyl adducts of Buckminsterfullerenes (Fig. 6). 90 Three process conditions can be varied to maximize the mole fraction of the desired products X 1 and X 2 . The conditions are temperature, reaction time and the ratio of sultine to C 60 . Experiments were based on a fully factorial design with three factors and six levels, totalling 246 samples. </p>
<p>Parameter</p>
<p>Kind Range Description Objective reaction time continuous [3,31] reaction </p>
<p>HPLC</p>
<p>This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. 89 The dataset includes 1,386 samples with six parameters and one objective. This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. 91 The undesired product is the tertiary amine shown in Fig. 7. This dataset includes 73 samples with four parameters and one objective.   This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. 14 The dataset includes 1040 samples with four parameters and one objective.  This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. 14 The dataset includes 1040 samples with four parameters and one objective.  This dataset reports the environmental factor (E-factor) for the nucleophilic aromatic substitution (S N Ar) reaction shown in Fig. 8. 91 The E-factor is defined as the ratio of the mass of waste to the mass of product. The dataset includes 66 samples with four parameters and one objective.  High-throughput reactions were carried out on the palladium-catalyzed Suzuki cross-coupling between 2bromophenyltetrazole and an electron-deficient aryl boronate (see Fig. 9). Cross-couplings of aryl halide electrophiles bearing non-protected ortho tetrazole substituents are typically carried out under harsh conditions due to the metalchelating nature of the tetrazole moiety, but we have found that the use of electron-rich bidentate phosphines, such as dtbpf, in alcohol solvents, facilitate milder reaction conditions. A wide range of continuous factors were explored in the microscale optimization run, including reaction temperature, Pd(dtbpf)Cl 2 loading, and equivalents of base. This resulted in product yields spanning from 2 to 97 mol %, generating a robust data set for modeling. Parameters and ranges are summarized in Tab. XIV Kind Range Description Objective temperature continuous [75,90] temperature The datasets in Olympus can be emulated using probabilistic Bayesian neural network (BNN) models or deterministic neural network (NN) models. Fig. 2 and Fig. 10 show the correlation between predicted and measured target data points for BNN and NN models, respectively. All emulators display a Spearman's rank coefficient above 0.90, for both training and test sets. Train/test splits were performed at random, but using a fixed random seed for reproducibility; 80% of the data was used for training and 20% for testing. Hyperparameter optimization was performed manually using 5-fold cross validation. The details of all hyperparameters used in these models are stored in the respective Emulator objects that can be loaded from Olympus. Performance on the training set (80% of data; blue markers) is shown in the top-left corner of each plot and on blue background; performance on the test set (20% of data; pink markers) is shown in the bottom-right corner of each plot and on pink background. R 2 is the coefficient of determination, ρ is the Spearman's rank correlation coefficient, and RMSE is the root-mean-square error. Emulators trained on datasets introduced in this study are indicated with *. Fig. 11 illustrates the analytical benchmark surfaces available in Olympus. With the exception of Branin, which is restricted to a two-dimensional input space, all other surfaces may be defined in any dimension. Note that all surfaces operate on the unit hypercube, X ∈ [0, 1] d . Olympus internally scales the inputs to be in agreement with</p>
<p>C. Description of surfaces</p>
<p>FIG. 2 .
2Parity plots with experimental versus predicted target values for all emulators based on Bayesian Neural Network models. Performance on the training set (80 % of data; blue markers) is shown in the top-left corner of each plot and on blue background; performance on the test set (20 % of data; pink markers) is shown in the bottom-right corner of each plot and on pink background. R 2 is the coefficient of determination, ρ is the Spearman's rank correlation coefficient, and RMSE is the root-mean-square error. Emulators trained on datasets introduced in this work are marked with *.</p>
<p>().plot_from_db(olymp.database) Further details on the capabilities and the usage of the Plotter module are reported in the online documentation. 92 FIG. 3. Plot generated with the Plotter module within Olympus illustrating the average best measurements collected during 5 independent runs with Random Search and Simplex over 100 iterations on the fullerenes emulator.</p>
<p>FIG. 4 .
4Performance of random search on all ten emulated surfaces in Olympus. We illustrate the best achieved property values up to the specified number of evaluations for all ten datasets. Solid blue lines show the average best values over 100 independent executions of the optimization starting from different random seeds. Box plots show the distribution of these 100 values at a few specific number of evaluations. Results obtained for emulators trained on datasets introduced in this work are marked with *.</p>
<p>ACKNOWLEDGMENTS
The authors acknowledge generous support from Natural Resources Canada (NRCAN). F.H. acknowledges financial support from the Herchel Smith Graduate Fellowship and the Jacques-Emile Dubois Student Dissertation Fellowship. M.A. is supported by a Postdoctoral Fellowship of the Vector Institute. R.J.H. gratefully acknowledges the Natural Sciences and Engineering Research Council of Canada (NSERC) for provision of the Postgraduate Scholarships-Doctoral Program (PGSD3-534584-2019). This work relates to Department of Navy award (N00014-19-1-2134) issued by the Office of Naval Research. This work was supported by the Defense Advanced Research Projects Agency under the Accelerated Molecular Discovery Program under Cooperative Agreement No. HR00111920027 dated August 1, 2019. The content of the information presented in this work does not necessarily reflect the position or the policy of the Government. A.A.G. would like to thank Dr. Anders Frøseth for his support. All computations reported in this paper were completed on the computing clusters of the Vector Institute and the Odyssey cluster supported by the FAS Division of Science, Research Computing Group at Harvard University. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.</p>
<p>FIG. 5 .
5Scheme of the AlcOx assay.</p>
<p>FIG. 6 .
6Synthesis of o-xylenyl C60 adducts of varying order via (a) in situ conversion of sultine (1,4-dihydro-2,3-benzoxathiin 3oxide) to o-quinodimethane (oQDM) followed by (b) successive attachments of oQDM to C60 (X0) by Diels-Alder cycloadditions. Reproduced from Walker et al. 90 under a CC BY 3.0 license.</p>
<p>FIG. 7 .
7Scheme of the N-benzylation reaction. The desired secondary amine is shown in green, while the undesired tertiary amine is shown in red.</p>
<p>FIG. 8 .
8Scheme of the SNAr reaction. The desired ortho product is shown in green, while the undesired para and bis adduct products are shown in red. 60.0, 140.0] temperature of the reactor [Celsius]</p>
<p>FIG. 9 .
9Scheme of the Suzuki reactionParameter</p>
<p>FIG. 10 .
10Parity plots with experimental versus predicted target values for all emulators based on Neural Network models.</p>
<p>TABLE I. Feature comparison of different benchmarking tools. This work introduces Olympus, which targets benchmark applications related to autonomous scientific discovery in chemistry and materials science. † limited capability. benchmarks and software packages have been introduced for different applications and at various levels of accessibility. Prominent examples in the field of machine learning include the MNIST 39 and CIFAR-10 40 datasets for image recognition, which have been made available through a selection of libraries and interfaces. In the field of chemistry, comprehensive collections of datasets such as MoleculeNet 41 or the QMx series 42-47 serve similar purposes. Frameworks such as GuacaMolToolkit 
Optimizer 
interfaces </p>
<p>Synthetic 
benchmarks </p>
<p>Noisy 
surfaces </p>
<p>Emulated 
experiments </p>
<p>Visuali-
zations </p>
<p>Community 
contributions </p>
<p>Primary purpose </p>
<p>Coco 33 
no 
yes 
yes 
no 
yes 
no 
Continuous optimization 
OpenAI Gym 34 
no 
yes 
yes 
no 
yes 
no 
Reinforcement learning 
Sherpa 35 
yes 
no 
no 
no 
yes 
no 
Hyperparameter optimization 
Optuna 36 
yes 
no 
no 
no 
yes 
no 
Hyperparameter optimization 
Pygmo 37 
yes 
yes 
no 
no 
yes  † 
no 
Parallel optimization 
Summit 38 
yes 
yes 
yes 
yes 
no 
no 
Experiment planning </p>
<p>Olympus 
yes 
yes 
yes 
yes 
yes 
yes 
Experiment planning </p>
<p>Several </p>
<p>TABLE II. List of algorithms available in Olympus. Convergence is categorized into global (converges to global optimum), global* (does not necessarily converge to global optimum but can overcome local minima), local (does not necessarily converge to global optimum and does not overcome local minima).Planner 
Strategy 
Convergence Derivative-Free 
GPyOpt 50 
Bayesian 
global 
yes 
HyperOpt 51-53 
Bayesian 
global 
yes 
Phoenics 54 
Bayesian 
global 
yes </p>
<p>Genetic 55 
Evolutionary global<em> 
yes 
CMA-ES 56,57 
Evolutionary global</em> 
yes 
Particle Swarms 58,59 
Evolutionary global<em> 
yes 
Differential Evolution 60 Evolutionary global</em> 
yes </p>
<p>Steepest Descent 61,62 
Gradient-based local 
no 
Conjugate Gradient 62,63 Gradient-based local 
no 
LBFGS 64-66 
Gradient-based local 
no 
SLSQP 67 
Gradient-based local 
no </p>
<p>Grid Search 68-70 
Grid-like 
global 
yes 
Latin Hypercube 68-70 
Grid-like 
global 
yes 
Sobol Sequence 71 
Grid-like 
global 
yes 
Random Search 
Grid-like 
global 
yes </p>
<p>Snobfit 72 
Heuristic 
global 
yes 
Basin Hopping 73 
Heuristic 
global 
yes 
Simplex 74 
Heuristic 
local 
yes </p>
<p>TABLE III. List of the core Olympus datasets. Datasets contributed in this work are marked with * .Olympus label Topic 
Discipline </p>
<h1>data points # parameters</h1>
<p>alkox  *<br />
Alkoxylation reaction 
Organic chemistry 
208 
4 
colors bob 89 Colorant mixture with 3D printed robot Colorimetry 
241 
5 
colors n9 89 
Colorant mixture with commercial robot Colorimetry 
102 
3 
fullerenes 90 Synthesis of o-xylenyl C60 adducts 
Organic chemistry 
246 
3 
hplc 89 
Calibration of an automated HPLC 
Analytical chemistry 
1,386 
6 
benzylation 91 N-benzylation reaction 
Organic chemistry 
73 
4 
photo pce10 14 Photostability of organic photovoltaics 
with PCE10 polymers </p>
<p>Materials science 
1,040 
4 </p>
<p>photo wf3 14 
Photostability of organic photovoltaics 
with WF3 polymers </p>
<p>Materials science 
1,040 
4 </p>
<p>snar 91 
Nucleophilic aromatic substitution 
Organic chemistry 
66 
4 
suzuki  *<br />
Carbon-carbon cross-coupling reaction 
Organic chemistry 
247 
4 </p>
<p>Non-convex AckleyPath, Branin, Levy, Michalewicz, Rastrigin, Rosenbrock, Schwefel, StyblinskiTangType 
Property 
Surfaces 
Continuous 
Convex 
Dejong, HyperEllipsoid, Zakharov 
Piece-wise constant Convex 
LinearFunnel, NarrowFunnel 
Non-convex DiscreteAckley, DiscreteDoubleWell, DiscreteMichalewicz </p>
<p>Mixture model 
Non-convex GaussianMixture, Denali, Everest, K2, Kilimanjaro, Matterhorn, MontBlanc </p>
<p>TABLE IV .
IVAvailable analytical surfaces in Olympus.# Option 1 (recommended): installation via pip </p>
<blockquote>
<blockquote>
<p>pip install olymp </p>
</blockquote>
</blockquote>
<h1>Option 2: installation via anaconda</h1>
<blockquote>
<blockquote>
<p>conda install -c conda-forge olymp </p>
</blockquote>
</blockquote>
<h1>Option 3: installation from source</h1>
<blockquote>
<blockquote>
<p>git clone https://github.com/aspuru-guzik-group/ 
olympus.git 
cd olympus 
python setup.py develop </p>
</blockquote>
</blockquote>
<p>Olympus allows you to create a Planner implementing your own custom algorithm, such that you can test it against established methods on a set of challenging benchmarks. To create such Planner you just need to inherit from the CustomPlanner class and implement the ask method. This method should return the next query point based on the algorithm's strategy. For instance, a random sampler can be implemented as follows.from olympus.models import BayesNeuralNet 
mymodel = BayesNeuralNet(hidden_depth=4, 
out_act="sigmoid") 
emulator = Emulator(dataset=mydataset, 
model=mymodel) 
emulator.cross_validate() </p>
<blockquote>
<blockquote>
<blockquote>
<p>... </p>
</blockquote>
</blockquote>
</blockquote>
<p>emulator.train() </p>
<blockquote>
<blockquote>
<blockquote>
<p>... </p>
</blockquote>
</blockquote>
</blockquote>
<p>emulator.save("my_new_emulator") </p>
<p>E. Test your planning algorithm </p>
<p>from olympus.planners import CustomPlanner 
from olympus import ParameterVector as PV 
import numpy as np 
class RandomSampler(CustomPlanner): 
def _ask(self): 
new_params = [] 
for param in self._param_space: 
new_param = np.random.uniform( 
low=param['domain'][0], 
high=param['domain'][1]) 
new_params.append(new_param) 
return PV(array=new_params, 
param_space=self.param_space) </p>
<p>TABLE V. Parameter space of the alkoxylation dataset 2. Color mixing BOBThis dataset consists of colors prepared by mixing varying amounts of 5 colored dyes (red, orange, yellow, blue, green). The parameters represent fractions of each dye used in a mixture. The target is the normalized green-like RGB value [0.16, 0.56, 0.28]. This dataset consists of 241 measurements performed using the Bayesian Optimized Bartender (BOB).89 Parameter 
Kind 
Range 
Description 
Objective 
Catalase 
continuous [0.05, 1.0] concentration [µM] </p>
<p>conversion ↑ 
Horseradish peroxidase continuous [0.5, 10.0] concentration [µM] 
Alcohol oxidase 
continuous [2.0, 8.0] concentration [nM] 
pH 
continuous [6.0, 8.0] -log(H + ) [ml/min] </p>
<p>Parameter 
Kind 
Range Description 
Objective 
red 
continuous [0, 1] 
amount of red </p>
<p>distance to green ↓ </p>
<p>orange 
continuous [0, 1] amount of orange 
yellow 
continuous [0, 1] amount of yellow 
blue 
continuous [0, 1] 
amount of blue 
green 
continuous [0, 1] amount of green </p>
<p>TABLE VI .
VIParameter space of the color BOB dataset3. Color mixing N9This dataset consists of colors prepared by mixing 3 types of dyes (red, green and blue). The target is the normalized green-like RGB value [0.16, 0.56, 0.28]. This dataset consists of 102 measurements performed with an N9 robotic arm from North Robotics.Parameter 
Kind 
Range Description 
Objective 
red 
continuous [0, 1] amount of red 
distance to green ↓ 
green 
continuous [0, 1] amount of green 
blue 
continuous [0, 1] amount of blue </p>
<p>TABLE VII .
VIIParameter space of the color N9 dataset</p>
<p>time in flow reactor [min] mole fraction of X1 + X2 ↑ sultine conc continuous [1.5, 6.0] relative concentration of sultine to C60 temperature continuous [100, 150] temperature of the reaction [deg Celsius] TABLE VIII. Parameter space of the Buckminsterfullerene dataset</p>
<p>Parameter Kind Range Description Objective flow rate continuous [0.2, 0.4] flow rate [mL/min] impurity yield ↓ ratio continuous [1, 5] benzyl bromide equivalents w.r.t. the methylbenzylamine reagent solvent continuous [0.5, 1.0] solvent (CHCl3) equivalents w.r.t. stock solution of reagent temperature continuous [110, 150] reaction temperature [°C]</p>
<p>TABLE X .
XParameter space of the N-benzylation dataset 7. Photobleaching PCE10</p>
<p>TABLE XI .
XIParameter space of the photobleaching PCE10 dataset8. Photobleaching WF3 </p>
<p>TABLE XII .
XIIParameter space of the photobleaching WF3 dataset 9. SnAr reaction</p>
<p>TABLE XIII .
XIIIParameter space of the SNAr dataset10. Suzuki reaction</p>
<p>of the reaction [°C] TABLE XIV. Parameter space of the Suzuki dataset B. Description of emulatorsyield ↑ 
Pd mol 
continuous [0.5, 5.0] 
loading of Pd catalyst [mol %] 
ArBpin 
continuous [1.0, 1.8] equivalents of pinacolyl boronate ester coupling partner 
K3PO4 
continuous [1.5, 3] 
equivalents of tripotassium phosphate </p>
<p>SurfaceDomainDefault parameters15]n.a.
No free lunch theorems for optimization. D H Wolpert, W G Macready, IEEE Transactions on Evolutionary Computation. 167D. H. Wolpert and W. G. Macready, No free lunch theo- rems for optimization, IEEE Transactions on Evolution- ary Computation 1, 67 (1997).</p>
<p>Optimization with randomized search heuristics-the (a)nfl theorem, realistic scenarios, and difficult functions. S Droste, T Jansen, I Wegener, natural Computing. 287S. Droste, T. Jansen, and I. Wegener, Optimization with randomized search heuristics-the (a)nfl theorem, real- istic scenarios, and difficult functions, Theoretical Com- puter Science 287, 131 (2002), natural Computing.</p>
<p>Digitizing chemistry using the chemical processing unit: From synthesis to discovery. L Wilbraham, S H M Mehr, L Cronin, Accounts of Chemical Research. 54253L. Wilbraham, S. H. M. Mehr, and L. Cronin, Digitizing chemistry using the chemical processing unit: From syn- thesis to discovery, Accounts of Chemical Research 54, 253 (2021).</p>
<p>Machine learning based approaches to accelerate energy materials discovery and optimization. D Krishnamurthy, H Weiland, A Farimani, E Antono, J Green, V Viswanathan, ACS Energy Letters. 4187D. Krishnamurthy, H. Weiland, A. Barati Farimani, E. Antono, J. Green, and V. Viswanathan, Machine learning based approaches to accelerate energy materi- als discovery and optimization, ACS Energy Letters 4, 187 (2018).</p>
<p>The matter simulation (r) evolution. A Aspuru-Guzik, R Lindh, M Reiher, ACS central science. 4144A. Aspuru-Guzik, R. Lindh, and M. Reiher, The mat- ter simulation (r) evolution, ACS central science 4, 144 (2018).</p>
<p>Discovery and optimization of materials using evolutionary approaches, Chemical reviews. T C Le, D A Winkler, 1166107T. C. Le and D. A. Winkler, Discovery and optimization of materials using evolutionary approaches, Chemical re- views 116, 6107 (2016).</p>
<p>Automatic discovery and optimization of chemical processes. C Houben, A A Lapkin, Current opinion in chemical engineering. 91C. Houben and A. A. Lapkin, Automatic discovery and optimization of chemical processes, Current opinion in chemical engineering 9, 1 (2015).</p>
<p>A combinatorial approach to the discovery and optimization of luminescent materials. E Danielson, J H Golden, E W Mcfarland, C M Reaves, W H Weinberg, X Di Wu, Nature. 389944E. Danielson, J. H. Golden, E. W. McFarland, C. M. Reaves, W. H. Weinberg, and X. Di Wu, A combinatorial approach to the discovery and optimization of lumines- cent materials, Nature 389, 944 (1997).</p>
<p>Accelerating the discovery of materials for clean energy in the era of smart automation. D P Tabor, L M Roch, S K Saikin, C Kreisbeck, D Sheberla, J H Montoya, S Dwaraknath, M Aykol, C Ortiz, H Tribukait, Nature Reviews Materials. 35D. P. Tabor, L. M. Roch, S. K. Saikin, C. Kreisbeck, D. Sheberla, J. H. Montoya, S. Dwaraknath, M. Aykol, C. Ortiz, H. Tribukait, et al., Accelerating the discovery of materials for clean energy in the era of smart automa- tion, Nature Reviews Materials 3, 5 (2018).</p>
<p>Nextgeneration experimentation with self-driving laboratories. F Häse, L M Roch, A Aspuru-Guzik, Trends in Chemistry. F. Häse, L. M. Roch, and A. Aspuru-Guzik, Next- generation experimentation with self-driving laborato- ries, Trends in Chemistry (2019).</p>
<p>Progress and prospects for accelerating materials science with automated and autonomous workflows. H S Stein, J M Gregoire, Chemical Science. 109640H. S. Stein and J. M. Gregoire, Progress and prospects for accelerating materials science with automated and au- tonomous workflows, Chemical Science 10, 9640 (2019).</p>
<p>Autonomous discovery in the chemical sciences part i: Progress, Angewandte Chemie International Edition. C W Coley, N S Eyke, K F Jensen, 5922858C. W. Coley, N. S. Eyke, and K. F. Jensen, Autonomous discovery in the chemical sciences part i: Progress, Ange- wandte Chemie International Edition 59, 22858 (2020).</p>
<p>Autonomous discovery in the chemical sciences part ii: Outlook, Angewandte Chemie International Edition. C W Coley, N S Eyke, K F Jensen, 5923414C. W. Coley, N. S. Eyke, and K. F. Jensen, Autonomous discovery in the chemical sciences part ii: Outlook, Ange- wandte Chemie International Edition 59, 23414 (2020).</p>
<p>Beyond ternary opv: High-throughput experimentation and self-driving laboratories optimize multicomponent systems. S Langner, F Häse, J Perea, T Stubhan, J Hauch, L Roch, T Heumueller, A Aspuru-Guzik, C Brabec, Advanced Materials. 1907801S. Langner, F. Häse, J. Perea, T. Stubhan, J. Hauch, L. Roch, T. Heumueller, A. Aspuru-Guzik, and C. Brabec, Beyond ternary opv: High-throughput ex- perimentation and self-driving laboratories optimize mul- ticomponent systems, Advanced Materials , 1907801 (2020).</p>
<p>Controlling an organic synthesis robot with machine learning to search for new reactivity. J M Granda, L Donina, V Dragone, D.-L Long, L Cronin, Nature. 559377J. M. Granda, L. Donina, V. Dragone, D.-L. Long, and L. Cronin, Controlling an organic synthesis robot with machine learning to search for new reactivity, Nature 559, 377 (2018).</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. B P Macleod, F G L Parlane, T D Morrissey, F Häse, L M Roch, K E Dettelbach, R Moreira, L P E Yunker, M B Rooney, J R Deeth, V Lai, G J Ng, H Situ, R H Zhang, M S Elliott, T H Haley, D J Dvorak, A Aspuru-Guzik, J E Hein, C P Berlinguette, Science Advances. 68867B. P. MacLeod, F. G. L. Parlane, T. D. Morrissey, F. Häse, L. M. Roch, K. E. Dettelbach, R. Moreira, L. P. E. Yunker, M. B. Rooney, J. R. Deeth, V. Lai, G. J. Ng, H. Situ, R. H. Zhang, M. S. Elliott, T. H. Haley, D. J. Dvorak, A. Aspuru-Guzik, J. E. Hein, and C. P. Berlinguette, Self-driving laboratory for acceler- ated discovery of thin-film materials, Science Advances 6, eaaz8867 (2020).</p>
<p>. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, A mobile robotic chemist. 583237NatureB. Burger, P. M. Maffettone, V. V. Gusev, C. M. Aitchi- son, Y. Bai, X. Wang, X. Li, B. M. Alston, B. Li, R. Clowes, et al., A mobile robotic chemist, Nature 583, 237 (2020).</p>
<p>A bayesian experimental autonomous researcher for mechanical design. A E Gongora, B Xu, W Perry, C Okoye, P Riley, K G Reyes, E F Morgan, K A Brown, Science Advances. 61708A. E. Gongora, B. Xu, W. Perry, C. Okoye, P. Riley, K. G. Reyes, E. F. Morgan, and K. A. Brown, A bayesian experimental autonomous researcher for mechanical de- sign, Science Advances 6, eaaz1708 (2020).</p>
<p>Suzuki-miyaura cross-coupling optimization enabled by automated feedback. B J Reizman, Y.-M Wang, S L Buchwald, K F Jensen, Reaction chemistry &amp; engineering. 1658B. J. Reizman, Y.-M. Wang, S. L. Buchwald, and K. F. Jensen, Suzuki-miyaura cross-coupling optimization en- abled by automated feedback, Reaction chemistry &amp; en- gineering 1, 658 (2016).</p>
<p>Integration of enabling methods for the automated flow preparation of piperazine-2-carboxamide. R J Ingham, C Battilocchio, J M Hawkins, S V Ley, Beilstein journal of organic chemistry. 10641R. J. Ingham, C. Battilocchio, J. M. Hawkins, and S. V. Ley, Integration of enabling methods for the automated flow preparation of piperazine-2-carboxamide, Beilstein journal of organic chemistry 10, 641 (2014).</p>
<p>Intelligent routes to the controlled synthesis of nanoparticles. S Krishnadasan, R Brown, A Demello, J Demello, Lab on a Chip. 71434S. Krishnadasan, R. Brown, A. Demello, and J. Demello, Intelligent routes to the controlled synthesis of nanopar- ticles, Lab on a Chip 7, 1434 (2007).</p>
<p>Tuning reaction products by constrained optimisation. B E Walker, J H Bannock, A M Nightingale, J C Demello, Reaction Chemistry &amp; Engineering. 2785B. E. Walker, J. H. Bannock, A. M. Nightingale, and J. C. deMello, Tuning reaction products by constrained optimisation, Reaction Chemistry &amp; Engineering 2, 785 (2017).</p>
<p>Reconfigurable system for automated optimization of diverse chemical reactions. A.-C Bédard, A Adamo, K C Aroh, M G Russell, A A Bedermann, J Torosian, B Yue, K F Jensen, T F Jamison, Science. 3611220A.-C. Bédard, A. Adamo, K. C. Aroh, M. G. Russell, A. A. Bedermann, J. Torosian, B. Yue, K. F. Jensen, and T. F. Jamison, Reconfigurable system for automated optimization of diverse chemical reactions, Science 361, 1220 (2018).</p>
<p>A novel internet-based reaction monitoring, control and autonomous self-optimization platform for chemical synthesis. D E Fitzpatrick, C Battilocchio, S V Ley, Organic Process Research &amp; Development. 20386D. E. Fitzpatrick, C. Battilocchio, and S. V. Ley, A novel internet-based reaction monitoring, control and au- tonomous self-optimization platform for chemical synthe- sis, Organic Process Research &amp; Development 20, 386 (2016).</p>
<p>An autonomous selfoptimizing flow reactor for the synthesis of natural product carpanone. D Cortés-Borda, E Wimmer, B Gouilleux, E Barré, N Oger, L Goulamaly, L Peault, B Charrier, C Truchet, P Giraudeau, The Journal of organic chemistry. 8314286D. Cortés-Borda, E. Wimmer, B. Gouilleux, E. Barré, N. Oger, L. Goulamaly, L. Peault, B. Charrier, C. Truchet, P. Giraudeau, et al., An autonomous self- optimizing flow reactor for the synthesis of natural prod- uct carpanone, The Journal of organic chemistry 83, 14286 (2018).</p>
<p>An automated microfluidic system for online optimization in chemical synthesis, Organic process research &amp; development. J P Mcmullen, K F Jensen, 141169J. P. McMullen and K. F. Jensen, An automated mi- crofluidic system for online optimization in chemical syn- thesis, Organic process research &amp; development 14, 1169 (2010).</p>
<p>Accelerated search for materials with targeted properties by adaptive design. D Xue, P V Balachandran, J Hogden, J Theiler, D Xue, T Lookman, Nature communications. 71D. Xue, P. V. Balachandran, J. Hogden, J. Theiler, D. Xue, and T. Lookman, Accelerated search for materi- als with targeted properties by adaptive design, Nature communications 7, 1 (2016).</p>
<p>A kriging-based approach to autonomous experimentation with applications to X-ray scattering. M M Noack, K G Yager, M Fukuto, G S Doerk, R Li, J A Sethian, Scientific reports. 91M. M. Noack, K. G. Yager, M. Fukuto, G. S. Doerk, R. Li, and J. A. Sethian, A kriging-based approach to autonomous experimentation with applications to X-ray scattering, Scientific reports 9, 1 (2019).</p>
<p>Fast machine-learning online optimization of ultra-cold-atom experiments. P B Wigley, P J Everitt, A Van Den Hengel, J Bastian, M A Sooriyabandara, G D Mcdonald, K S Hardman, C Quinlivan, P Manju, C C Kuhn, Scientific reports. 625890P. B. Wigley, P. J. Everitt, A. van den Hengel, J. Bastian, M. A. Sooriyabandara, G. D. McDonald, K. S. Hard- man, C. Quinlivan, P. Manju, C. C. Kuhn, et al., Fast machine-learning online optimization of ultra-cold-atom experiments, Scientific reports 6, 25890 (2016).</p>
<p>Autonomy in materials research: a case study in carbon nanotube growth. P Nikolaev, D Hooper, F Webber, R Rao, K Decker, M Krein, J Poleski, R Barto, B Maruyama, Computational Materials. 21P. Nikolaev, D. Hooper, F. Webber, R. Rao, K. Decker, M. Krein, J. Poleski, R. Barto, and B. Maruyama, Auton- omy in materials research: a case study in carbon nan- otube growth, npj Computational Materials 2, 1 (2016).</p>
<p>Benchmarking the acceleration of materials discovery by sequential learning. B Rohr, H S Stein, D Guevarra, Y Wang, J A Haber, M Aykol, S K Suram, J M Gregoire, Chemical Science. 112696B. Rohr, H. S. Stein, D. Guevarra, Y. Wang, J. A. Haber, M. Aykol, S. K. Suram, and J. M. Gregoire, Benchmark- ing the acceleration of materials discovery by sequential learning, Chemical Science 11, 2696 (2020).</p>
<p>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories. F Häse, L M Roch, A Aspuru-Guzik, Chemical science. 97642F. Häse, L. M. Roch, and A. Aspuru-Guzik, Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories, Chemical science 9, 7642 (2018).</p>
<p>O Elhara, K Varelas, D Nguyen, T Tusar, D Brockhoff, N Hansen, A Auger, Coco , arXiv:1903.06396The large scale black-box optimization benchmarking (bbob-largescale) test suite. arXiv preprintO. Elhara, K. Varelas, D. Nguyen, T. Tusar, D. Brock- hoff, N. Hansen, and A. Auger, Coco: The large scale black-box optimization benchmarking (bbob-largescale) test suite, arXiv preprint arXiv:1903.06396 (2019).</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540Openai gym. arXiv preprintG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, Openai gym, arXiv preprint arXiv:1606.01540 (2016).</p>
<p>L Hertel, J Collado, P Sadowski, J Ott, P Baldi, arXiv:2005.04048Sherpa: Robust hyperparameter optimization for machine learning. arXiv preprintL. Hertel, J. Collado, P. Sadowski, J. Ott, and P. Baldi, Sherpa: Robust hyperparameter optimization for ma- chine learning, arXiv preprint arXiv:2005.04048 (2020).</p>
<p>Optuna: A next-generation hyperparameter optimization framework. T Akiba, S Sano, T Yanase, T Ohta, M Koyama, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningT. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, Optuna: A next-generation hyperparameter optimiza- tion framework, in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Dis- covery &amp; Data Mining (2019) pp. 2623-2631.</p>
<p>A parallel global multiobjective framework for optimization: pagmo. F Biscani, D Izzo, Journal of Open Source Software. 52338F. Biscani and D. Izzo, A parallel global multiobjective framework for optimization: pagmo, Journal of Open Source Software 5, 2338 (2020).</p>
<p>Summit: Benchmarking machine learning methods for reaction optimisation. K C Felton, J G Rittig, A A Lapkin, Chemistry-Methods. 1116K. C. Felton, J. G. Rittig, and A. A. Lapkin, Summit: Benchmarking machine learning methods for reaction op- timisation, Chemistry-Methods 1, 116 (2021).</p>
<p>Mnist handwritten digit database. Y Lecun, C Cortes, C Burges, ATT Labs [OnlineY. LeCun, C. Cortes, and C. Burges, Mnist hand- written digit database, ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2 (2010).</p>
<p>Learning multiple layers of features from tiny images. A Krizhevsky, Univerity of TorontoTech. Rep.A. Krizhevsky, Learning multiple layers of features from tiny images, Tech. Rep. (Univerity of Toronto, 2009).</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chemical science. 9513Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Ge- niesse, A. S. Pappu, K. Leswing, and V. Pande, Molecu- lenet: a benchmark for molecular machine learning, Chemical science 9, 513 (2018).</p>
<p>970 million druglike small molecules for virtual screening in the chemical universe database gdb-13. L C Blum, J.-L Reymond, Journal of the American Chemical Society. 131L. C. Blum and J.-L. Reymond, 970 million druglike small molecules for virtual screening in the chemical uni- verse database gdb-13, Journal of the American Chemical Society 131, 8732 (2009).</p>
<p>Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17. L Ruddigkeit, R Van Deursen, L C Blum, J.-L Reymond, Journal of Chemical Information and Modeling. 522864L. Ruddigkeit, R. van Deursen, L. C. Blum, and J.- L. Reymond, Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17, Journal of Chemical Information and Modeling 52, 2864 (2012).</p>
<p>Fast and accurate modeling of molecular atomization energies with machine learning. M Rupp, A Tkatchenko, K.-R Müller, O A Von Lilienfeld, 10.1103/PhysRevLett.108.058301Phys. Rev. Lett. 10858301M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. von Lilienfeld, Fast and accurate modeling of molecular atom- ization energies with machine learning, Phys. Rev. Lett. 108, 058301 (2012).</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. R Ramakrishnan, P O Dral, M Rupp, O A Von Lilienfeld, 10.1038/sdata.2014.22Scientific Data. 1140022R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld, Quantum chemistry structures and properties of 134 kilo molecules, Scientific Data 1, 140022 (2014).</p>
<p>Electronic spectra from tddft and machine learning in chemical space. R Ramakrishnan, M Hartmann, E Tapavicza, O A Von Lilienfeld, The Journal of Chemical Physics. 14384111R. Ramakrishnan, M. Hartmann, E. Tapavicza, and O. A. von Lilienfeld, Electronic spectra from tddft and machine learning in chemical space, The Journal of Chemical Physics 143, 084111 (2015).</p>
<p>Dataset's chemical diversity limits the generalizability of machine learning predictions. M Glavatskikh, J Leguy, G Hunault, T Cauchy, B. Da Mota, Journal of Cheminformatics. 1169M. Glavatskikh, J. Leguy, G. Hunault, T. Cauchy, and B. Da Mota, Dataset's chemical diversity limits the gen- eralizability of machine learning predictions, Journal of Cheminformatics 11, 69 (2019).</p>
<p>Guacamol: Benchmarking models for de novo molecular design. N Brown, M Fiscato, M H Segler, A C Vaucher, Journal of Chemical Information and Modeling. 591096N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher, Guacamol: Benchmarking models for de novo molecular design, Journal of Chemical Information and Modeling 59, 1096 (2019).</p>
<p>D Polykovskiy, A Zhebrak, B Sanchez-Lengeling, S Golovanov, O Tatanov, S Belyaev, R Kurbanov, A Artamonov, V Aladinskiy, M Veselov, A Kadurin, S Nikolenko, A Aspuru-Guzik, A Zhavoronkov, arXiv:1811.12823Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. arXiv preprintD. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kur- banov, A. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin, S. Nikolenko, A. Aspuru-Guzik, and A. Zha- voronkov, Molecular Sets (MOSES): A Benchmark- ing Platform for Molecular Generation Models, arXiv preprint arXiv:1811.12823 (2018).</p>
<p>Gpyopt: A bayesian optimization framework in python. The GPyOpt authors. The GPyOpt authors, Gpyopt: A bayesian opti- mization framework in python, http://github.com/ SheffieldML/GPyOpt (2016).</p>
<p>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. J S Bergstra, D Yamins, D D Cox, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningPMLR28J. S. Bergstra, D. Yamins, and D. D. Cox, Making a sci- ence of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures, in Pro- ceedings of the 30th International Conference on Machine Learning, Vol. 28 (PMLR, 2013) pp. 115-123.</p>
<p>Random search for hyperparameter optimization. J S Bergstra, Y Bengio, Journal of Machine Learning Research. 13281J. S. Bergstra and Y. Bengio, Random search for hyper- parameter optimization, Journal of Machine Learning Research 13, 281 (2012).</p>
<p>Algorithms for hyper-parameter optimization. J S Bergstra, R Bardenet, Y Bengio, B Kégl, Advances in neural information processing systems. J. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, Algorithms for hyper-parameter optimization, in Ad- vances in neural information processing systems (2011) pp. 2546-2554.</p>
<p>F Häse, L M Roch, C Kreisbeck, A Aspuru-Guzik, Phoenics: A bayesian optimizer for chemistry. 41134F. Häse, L. M. Roch, C. Kreisbeck, and A. Aspuru-Guzik, Phoenics: A bayesian optimizer for chemistry, ACS cen- tral science 4, 1134 (2018).</p>
<p>Deap: Evolutionary algorithms made easy. F.-A Fortin, F.-M De Rainville, M.-A Gardner, M Parizeau, C Gagné, Journal of Machine Learning Research. 132171F.-A. Fortin, F.-M. De Rainville, M.-A. Gardner, M. Parizeau, and C. Gagné, Deap: Evolutionary algo- rithms made easy, Journal of Machine Learning Research 13, 2171 (2012).</p>
<p>Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). N Hansen, S D Müller, P Koumoutsakos, Evolutionary computation. 111N. Hansen, S. D. Müller, and P. Koumoutsakos, Reducing the time complexity of the derandomized evolution strat- egy with covariance matrix adaptation (cma-es), Evolu- tionary computation 11, 1 (2003).</p>
<p>Completely derandomized self-adaptation in evolution strategies. N Hansen, A Ostermeier, Evolutionary computation. 9159N. Hansen and A. Ostermeier, Completely derandomized self-adaptation in evolution strategies, Evolutionary com- putation 9, 159 (2001).</p>
<p>A new optimizer using particle swarm theory. R Eberhart, J Kennedy, MHS'95. Proceedings of the Sixth International Symposium on Micro Machine and Human Science. IeeeR. Eberhart and J. Kennedy, A new optimizer using par- ticle swarm theory, in MHS'95. Proceedings of the Sixth International Symposium on Micro Machine and Human Science (Ieee, 1995) pp. 39-43.</p>
<p>A modified particle swarm optimizer. Y Shi, R Eberhart, 1998 IEEE international conference on evolutionary computation proceedings. IEEE world congress on computational intelligence (Cat. No. 98TH8360). IEEEY. Shi and R. Eberhart, A modified particle swarm op- timizer, in 1998 IEEE international conference on evolu- tionary computation proceedings. IEEE world congress on computational intelligence (Cat. No. 98TH8360) (IEEE, 1998) pp. 69-73.</p>
<p>Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. R Storn, K Price, Journal of global optimization. 11341R. Storn and K. Price, Differential evolution-a simple and efficient heuristic for global optimization over con- tinuous spaces, Journal of global optimization 11, 341 (1997).</p>
<p>The method of steepest descent for nonlinear minimization problems. H B Curry, Quarterly of Applied Mathematics. 2258H. B. Curry, The method of steepest descent for non- linear minimization problems, Quarterly of Applied Mathematics 2, 258 (1944).</p>
<p>Nonsymmetric preconditioning for conjugate gradient and steepest descent methods. H Bouwmeester, A Dougherty, A V Knyazev, Procedia Computer Science. 51276H. Bouwmeester, A. Dougherty, and A. V. Knyazev, Non- symmetric preconditioning for conjugate gradient and steepest descent methods, Procedia Computer Science 51, 276 (2015).</p>
<p>Methods of conjugate gradients for solving linear systems. M R Hestenes, E Stiefel, Journal of research of the National Bureau of Standards. 49409M. R. Hestenes, E. Stiefel, et al., Methods of conjugate gradients for solving linear systems, Journal of research of the National Bureau of Standards 49, 409 (1952).</p>
<p>Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale boundconstrained optimization. C Zhu, R H Byrd, P Lu, J Nocedal, ACM Transactions on Mathematical Software (TOMS). 23550C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal, Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound- constrained optimization, ACM Transactions on Mathe- matical Software (TOMS) 23, 550 (1997).</p>
<p>A limited memory algorithm for bound constrained optimization. R H Byrd, P Lu, J Nocedal, C Zhu, SIAM Journal on scientific computing. 161190R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, A limited memory algorithm for bound constrained optimization, SIAM Journal on scientific computing 16, 1190 (1995).</p>
<p>Sequential quadratic programming, Numerical optimization. J Nocedal, S J Wright, 529J. Nocedal and S. J. Wright, Sequential quadratic pro- gramming, Numerical optimization , 529 (2006).</p>
<p>A software package for sequential quadratic programming. D Kraft, Tech. Rep. (Wiss. Berichtswesen d. DFVLRD. Kraft, A software package for sequential quadratic pro- gramming, Tech. Rep. (Wiss. Berichtswesen d. DFVLR, 1988).</p>
<p>M J Anderson, P J Whitcomb, DOE simplified: practical tools for effective experimentation. CRC PressM. J. Anderson and P. J. Whitcomb, DOE simplified: practical tools for effective experimentation (CRC Press, 2016).</p>
<p>G E P Box, J S Hunter, W G Hunter, Statistics for experimenters: design, innovation and discovery. Wiley2G. E. P. Box, J. S. Hunter, and W. G. Hunter, Statis- tics for experimenters: design, innovation and discovery, Vol. 2 (Wiley, 2005).</p>
<p>The design of experiments. R A Fisher, Oliver and BoydEdinburghR. A. Fisher, The design of experiments (Oliver and Boyd; Edinburgh;</p>
<p>. London, London, 1937).</p>
<p>On the distribution of points in a cube and the approximate evaluation of integrals. I M Sobol, Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki. 7784I. M. Sobol', On the distribution of points in a cube and the approximate evaluation of integrals, Zhurnal Vy- chislitel'noi Matematiki i Matematicheskoi Fiziki 7, 784 (1967).</p>
<p>Snobfit-stable noisy optimization by branch and fit. W Huyer, A Neumaier, ACM Transactions on Mathematical Software (TOMS). 351W. Huyer and A. Neumaier, Snobfit-stable noisy opti- mization by branch and fit, ACM Transactions on Math- ematical Software (TOMS) 35, 1 (2008).</p>
<p>Global optimization by basinhopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms. D J Wales, J P Doye, The Journal of Physical Chemistry A. 1015111D. J. Wales and J. P. Doye, Global optimization by basin- hopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms, The Journal of Phys- ical Chemistry A 101, 5111 (1997).</p>
<p>A simplex method for function minimization. J A Nelder, R Mead, The computer journal. 7308J. A. Nelder and R. Mead, A simplex method for function minimization, The computer journal 7, 308 (1965).</p>
<p>S Bubeck, Convex optimization: Algorithms and complexity, Foundations and Trends in Machine Learning. 8231S. Bubeck et al., Convex optimization: Algorithms and complexity, Foundations and Trends in Machine Learning 8, 231 (2015).</p>
<p>S Boyd, S P Boyd, L Vandenberghe, Convex optimization. Cambridge university pressS. Boyd, S. P. Boyd, and L. Vandenberghe, Convex opti- mization (Cambridge university press, 2004).</p>
<p>Chemical process optimization using newton-like methods. A Lucia, J Xu, Computers &amp; chemical engineering. 14119A. Lucia and J. Xu, Chemical process optimization using newton-like methods, Computers &amp; chemical engineering 14, 119 (1990).</p>
<p>Convergence of a random optimization method for constrained optimization problems. N Baba, Journal of Optimization Theory and Applications. 33451N. Baba, Convergence of a random optimization method for constrained optimization problems, Journal of Opti- mization Theory and Applications 33, 451 (1981).</p>
<p>Random optimization, Automation and Remote control. J Matyas, 26246J. Matyas, Random optimization, Automation and Re- mote control 26, 246 (1965).</p>
<p>I Rechenberg, Evolutionsstrategien , Simulationsmethoden in der Medizin und Biologie. SpringerI. Rechenberg, Evolutionsstrategien, in Simulationsmeth- oden in der Medizin und Biologie (Springer, 1978) pp. 83-114.</p>
<p>Evolutionsstrategien für die numerische optimierung. H.-P Schwefel, Numerische Optimierung von Computer-Modellen mittels der Evolutionsstrategie. SpringerH.-P. Schwefel, Evolutionsstrategien für die numerische optimierung, in Numerische Optimierung von Computer- Modellen mittels der Evolutionsstrategie (Springer, 1977) pp. 123-176.</p>
<p>Genetic algorithms in search, optimization and machine learning. G Zames, N Ajlouni, N Ajlouni, N Ajlouni, J Holland, W Hills, D Goldberg, Information Technology Journal. 3301G. Zames, N. Ajlouni, N. Ajlouni, N. Ajlouni, J. Hol- land, W. Hills, and D. Goldberg, Genetic algorithms in search, optimization and machine learning., Information Technology Journal 3, 301 (1981).</p>
<p>Genetic programming: on the programming of computers by means of natural selection. J R Koza, J R Koza, MIT press1J. R. Koza and J. R. Koza, Genetic programming: on the programming of computers by means of natural selection, Vol. 1 (MIT press, 1992).</p>
<p>M Srinivas, L M Patnaik, Genetic algorithms: A survey. 2717M. Srinivas and L. M. Patnaik, Genetic algorithms: A survey, computer 27, 17 (1994).</p>
<p>Natural evolution strategies. D Wierstra, T Schaul, T Glasmachers, Y Sun, J Peters, J Schmidhuber, The Journal of Machine Learning Research. 15949D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Pe- ters, and J. Schmidhuber, Natural evolution strategies, The Journal of Machine Learning Research 15, 949 (2014).</p>
<p>J Mockus, Bayesian approach to global optimization: theory and applications. Springer Science &amp; Business Media37J. Mockus, Bayesian approach to global optimization: theory and applications, Vol. 37 (Springer Science &amp; Busi- ness Media, 2012).</p>
<p>The application of bayesian methods for seeking the extremum. J Mockus, V Tiesis, A Zilinskas, Towards global optimization. 22J. Mockus, V. Tiesis, and A. Zilinskas, The application of bayesian methods for seeking the extremum, Towards global optimization 2, 2 (1978).</p>
<p>On bayesian methods for seeking the extremum. J Močkus, Optimization techniques IFIP technical conference. SpringerJ. Močkus, On bayesian methods for seeking the ex- tremum, in Optimization techniques IFIP technical con- ference (Springer, 1975) pp. 400-404.</p>
<p>L M Roch, F Häse, C Kreisbeck, T Tamayo-Mendoza, L P E Yunker, J E Hein, A Aspuru-Guzik, Chemos: An orchestration software to democratize autonomous discovery. 151L. M. Roch, F. Häse, C. Kreisbeck, T. Tamayo-Mendoza, L. P. E. Yunker, J. E. Hein, and A. Aspuru-Guzik, Chemos: An orchestration software to democratize au- tonomous discovery, PLOS ONE 15, 1 (2020).</p>
<p>Tuning reaction products by constrained optimisation. B Walker, J Bannock, N A M , J Demello, React. Chem. Eng. 785B. Walker, J. Bannock, N. A.M., and J. deMello, Tun- ing reaction products by constrained optimisation, React. Chem. Eng. , 785 (2017).</p>
<p>Machine learning meets continuous flow chemistry: Automated optimization towards the pareto front of multiple objectives. A Schweidtmann, A Clayton, N Holmes, E Bradford, R Bourne, A Lapkin, Chem. Eng. J. 177A. Schweidtmann, A. Clayton, N. Holmes, E. Bradford, R. Bourne, and A. Lapkin, Machine learning meets con- tinuous flow chemistry: Automated optimization towards the pareto front of multiple objectives, Chem. Eng. J , 177 (2018).</p>
<p>Olympus: a benchmarking framework for noisy optimization and experiment planning. The Olympus authors. The Olympus authors, Olympus: a benchmarking framework for noisy optimization and experiment planning, https://github.com/aspuru-guzik-group/ olympus (2020).</p>
<p>the untransformed domains typically employed for these analytical functions (Table XV). the untransformed domains typically employed for these analytical functions (Table XV).</p>
<p>Analytical surfaces available in Olympus. FIG. 11. Analytical surfaces available in Olympus.</p>            </div>
        </div>

    </div>
</body>
</html>
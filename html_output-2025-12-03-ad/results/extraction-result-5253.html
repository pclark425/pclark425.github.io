<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5253 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5253</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5253</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265216566</p>
                <p><strong>Paper Title:</strong> WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding</p>
                <p><strong>Paper Abstract:</strong> Graphs are widely used to model interconnected entities and improve downstream predictions in various real-world applications. However, real-world graphs nowadays are often associated with complex attributes on multiple types of nodes and even links that are hard to model uniformly, while the widely used graph neural networks (GNNs) often require sufficient training toward specific downstream predictions to achieve strong performance. In this work, we take a fundamentally different approach than GNNs, to simultaneously achieve deep joint modeling of complex attributes and flexible structures of real-world graphs and obtain unsupervised generic graph representations that are not limited to specific downstream predictions. Our framework, built on a natural integration of language models (LMs) and random walks (RWs), is straightforward, powerful and data-efficient. Specifically, we first perform attributed RWs on the graph and design an automated program to compose roughly meaningful textual sequences directly from the attributed RWs; then we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM, which encapsulates both attribute semantics and graph structures. In our experiments, we evaluate the learned node embeddings towards different downstream prediction tasks on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods. We believe this work opens a door for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5253.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5253.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts attributed graphs into natural-language-like sequences by textualizing node/edge attributes and then sampling attributed random walks; these RW-derived textual sequences are used to fine-tune a pre-trained language model (DistilRoBERTa) via masked language modeling to produce node/edge embeddings that encode both attribute semantics and graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attributed Random-Walk-based Textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two-stage textualization: (1) Entity-level: a rule-based function P(·) concatenates attribute names, attribute values and optionally entity IDs into short natural-language phrases (e.g., a patient with {age:35, sex:female, pid:P246} -> "<A 35-year-old female patient P246>"). (2) Walk-level: generate N attributed random walks; for each walk append the sequence P(v0), P(e1), P(v1), ..., P(eL), P(vL) (a sequence of length 2L+1). Random walk transitions use uniform selection among outgoing edges and walks terminate with probability α. The resulting corpus of textual walk sequences is used to fine-tune a pre-trained LM with MLM (mask ratio m, typical m=0.15).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed heterogeneous graphs (nodes and edges with attributes), knowledge graphs / HINs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves original node and (simple) edge attribute text verbatim; captures local/meso-scale topology via random-walk co-occurrence statistics; produces roughly natural-language sentences that LMs can comprehend; unsupervised (no downstream labels needed); parallelizable RW generation; hyperparameters (number of walks N, termination probability α, mask ratio m) control information vs. cost; expressivity depends on attribute textualness and walk coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, few-shot node classification, graph-level classification (MUTAG), KG completion on Freebase and FB15K-237</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Macro-F1, Micro-F1 (node classification); AUC and MRR (link prediction). Representative reported results: PubMed node classification Macro-F1 60.42%, Micro-F1 62.33%; MIMIC-III node classification Macro-F1 75.16%, Micro-F1 77.89%; PubMed link prediction AUC 85.65%, MRR 94.16%; MIMIC-III link prediction AUC 82.15%, MRR 92.78%. Few-shot examples: 1-shot PubMed Macro-F1 28.09% (WalkLM).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to RW-based baselines (Metapath2Vec, HIN2Vec), relation-learning methods (ConvE, ComplEx), supervised HGNNs (RGCN, HAN, HGT) and unsupervised HGNNs (HeCo, SHGP) and LM-only baselines, WalkLM achieves the best overall performance in the paper's experiments. Reported aggregate improvements include ~138.59% average gain on PubMed over the second-best and ~39.37% on MIMIC-III; link prediction average gain ~5.97% over second-best. The paper stresses that LM-only methods capture attributes well but lack structural information, while RW-only methods lack rich attribute modeling; WalkLM combines both.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires meaningful textual attributes (node names/descriptions) to be effective; current edge handling assumes simple textual relation names—edges with rich non-textual attributes are not fully addressed; rule-based concatenation can produce noisy/ungrammatical text; random-walk sequences capture local structure but may miss global structural patterns (depends on N and α); LM fine-tuning is computationally expensive; performance sensitive to choice of base LM and hyperparameters (N, α, mask ratio m).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5253.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5253.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RW+NNLM (Goikoetxea et al. 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random walks and neural network language models on knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier approach that generates random-walk sequences on a knowledge base and trains neural language models on those sequences to learn word/entity representations; focuses on co-occurrence statistics from walks rather than textual attributes or relation semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Random walks and neural network language models on knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Random-walk-based linearization to token sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Perform random walks over the graph/KG to produce sequences of node identifiers or entity tokens and train neural language models (word/entity embedding models) on those sequences to learn representations based on walk co-occurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Captures local topology and node proximity via walk co-occurrence; computationally efficient and parallelizable; does not textualize attributes and typically ignores edge type semantics; lower expressivity for heterogeneous attributed graphs compared to attribute-aware textualization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Learning entity/word representations (original work aimed at embeddings); in this paper referenced as antecedent work rather than re-evaluated on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper notes this prior method ignores rich relational information between nodes and cannot learn richer topological/semantic information; WalkLM differs by textualizing attributes and leveraging modern pre-trained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not model edge semantics/attributes; limited ability to capture heterogeneous attribute semantics; token-based encoders used historically are less able to extract rich semantic features than modern pre-trained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5253.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5253.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimKGC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimKGC: Simple contrastive knowledge graph completion with pre-trained language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based KG completion approach that uses pre-trained LMs (BERT-style) with a contrastive learning objective to encode entities/relations from their textual descriptions and perform KG completion/link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SimKGC: Simple contrastive knowledge graph completion with pre-trained language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-based entity/relation encoding for contrastive KG completion</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent entities and relations by textual strings (names/descriptions), encode them with a pre-trained LM (often a bi-encoder), and train with contrastive losses and negative sampling to increase semantic similarity for correct triples; does not use random-walk sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / attributed graphs with textual metadata</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LM pretraining to capture semantic similarity from textual metadata; effective when good textual descriptions are available; structural/topological information is weak unless encoded indirectly via text or contrastive negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification and link prediction / KG completion on datasets including PubMed, MIMIC-III, Freebase, FB15K-237</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Macro-F1, Micro-F1, AUC, MRR. Example reported numbers (from this paper's tables): PubMed node classification Macro-F1 21.97%, Micro-F1 30.83%; MIMIC-III node classification Macro-F1 51.62%, Micro-F1 58.50%; PubMed link prediction AUC 79.62%, MRR 91.43%; MIMIC-III AUC 67.73%, MRR 84.86%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>SimKGC is competitive on semantic tasks and often ranks second in the paper's comparisons; however, WalkLM—which integrates RW-based structural signals with textual attribute sequences—outperforms SimKGC in combined semantic+structural evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on availability and quality of textual descriptions; limited explicit encoding of graph topology beyond what can be inferred from text and contrastive negatives; performance sensitive to negative sampling and contrastive design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5253.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5253.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM baselines (XRoBERTa, GPT-2, DistilRoBERTa) applied to textualized node attributes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines where each node's attributes (names/descriptions) are textualized into short phrases and each entity is encoded independently by a pre-trained LM (e.g., XRoBERTa, GPT-2, DistilRoBERTa) to produce embeddings, without constructing walk sequences that encode topology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attribute-level textualization with direct LM encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each graph entity, convert available attributes into a single natural-language string (e.g., via rule-based concatenation) or use name/description metadata, then feed each string separately into a pre-trained LM and extract the final hidden representation as the entity embedding (no explicit random-walk sequence concatenation).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs, knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves attribute semantics per-entity; simple and computationally cheaper than sequence-corpus LM fine-tuning on large RW corpora; does not directly encode neighborhood or relation co-occurrence information, limiting structural expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification and link prediction (used as baselines in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported examples from paper: LM (DRoBERTa) — PubMed node classification Macro-F1 58.29%, Micro-F1 60.57%; MIMIC-III Macro-F1 66.25%, Micro-F1 70.14%; PubMed link prediction AUC 60.97%, MRR 83.00%; MIMIC-III AUC 51.44%, MRR 75.09%. (Other LM variants XRoBERTa and GPT-2 also reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>LM-only methods learn attribute semantics well and outperform many graph-only baselines on attribute-rich node classification, but perform worse on link prediction than RW-based or relation-learning methods. Hybrid approaches (LM+RGCN, LM+HGT) improve performance, but WalkLM's graph-aware LM fine-tuning gives the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lack of neighborhood/topology encoding (poor link prediction); sensitivity to pretrained LM choice and pretraining corpus; may require combination with graph models or RW-based corpora to capture structural information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5253.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5253.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wikidata-textualization (KG extension)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikidata-based textualization of KG entities/relations (Appendix A.1 experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental variant for KG datasets where entities are matched to Wikidata to extract names/descriptions as textual node features and original relation labels are used as edge text; these textual features are used to form inputs to LMs (with or without RW sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Metadata-driven textualization (names/descriptions + relation text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Map KG entities to Wikidata, pull name and description fields and use these strings as textualized node attributes; treat original KG relation labels as textual edge attributes. These textual items can be used directly as per-entity LM inputs or integrated into RW-based sequences for LM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase, FB15K-237)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages external textual metadata to enrich semantic content; straightforward where mapping exists; coverage and quality dependent on external metadata completeness; retains textual relation labels but may still require structural modeling via walks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification and link prediction on Freebase and FB15K-237 (reported in Appendix A.1 / Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (WalkLM on KG datasets): Freebase node classification Macro-F1 55.01%, Micro-F1 71.36%; Freebase link prediction AUC 95.65%, MRR 98.45% (numbers correspond to WalkLM using metadata-driven textualization and RW corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Using external descriptions improves LM-based approaches; SimKGC and other text-based KG methods are competitive, but WalkLM combining RW structure with metadata textualization obtains top results in the paper's KG experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires entity disambiguation/matching to external KBs (Wikidata); metadata may be absent or noisy for many entities; mapping process is an extra preprocessing dependency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Random walks and neural network language models on knowledge bases <em>(Rating: 2)</em></li>
                <li>SimKGC: Simple contrastive knowledge graph completion with pre-trained language models <em>(Rating: 2)</em></li>
                <li>KG-BERT: Bert for knowledge graph completion <em>(Rating: 2)</em></li>
                <li>Kepler: A unified model for knowledge embedding and pre-trained language representation <em>(Rating: 2)</em></li>
                <li>Deep bidirectional language-knowledge graph pretraining <em>(Rating: 1)</em></li>
                <li>Jaket: Joint pre-training of knowledge graph and language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5253",
    "paper_id": "paper-265216566",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "WalkLM",
            "name_full": "WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
            "brief_description": "A method that converts attributed graphs into natural-language-like sequences by textualizing node/edge attributes and then sampling attributed random walks; these RW-derived textual sequences are used to fine-tune a pre-trained language model (DistilRoBERTa) via masked language modeling to produce node/edge embeddings that encode both attribute semantics and graph structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Attributed Random-Walk-based Textualization",
            "representation_description": "Two-stage textualization: (1) Entity-level: a rule-based function P(·) concatenates attribute names, attribute values and optionally entity IDs into short natural-language phrases (e.g., a patient with {age:35, sex:female, pid:P246} -&gt; \"&lt;A 35-year-old female patient P246&gt;\"). (2) Walk-level: generate N attributed random walks; for each walk append the sequence P(v0), P(e1), P(v1), ..., P(eL), P(vL) (a sequence of length 2L+1). Random walk transitions use uniform selection among outgoing edges and walks terminate with probability α. The resulting corpus of textual walk sequences is used to fine-tune a pre-trained LM with MLM (mask ratio m, typical m=0.15).",
            "graph_type": "Attributed heterogeneous graphs (nodes and edges with attributes), knowledge graphs / HINs",
            "representation_properties": "Preserves original node and (simple) edge attribute text verbatim; captures local/meso-scale topology via random-walk co-occurrence statistics; produces roughly natural-language sentences that LMs can comprehend; unsupervised (no downstream labels needed); parallelizable RW generation; hyperparameters (number of walks N, termination probability α, mask ratio m) control information vs. cost; expressivity depends on attribute textualness and walk coverage.",
            "evaluation_task": "Node classification, link prediction, few-shot node classification, graph-level classification (MUTAG), KG completion on Freebase and FB15K-237",
            "performance_metrics": "Macro-F1, Micro-F1 (node classification); AUC and MRR (link prediction). Representative reported results: PubMed node classification Macro-F1 60.42%, Micro-F1 62.33%; MIMIC-III node classification Macro-F1 75.16%, Micro-F1 77.89%; PubMed link prediction AUC 85.65%, MRR 94.16%; MIMIC-III link prediction AUC 82.15%, MRR 92.78%. Few-shot examples: 1-shot PubMed Macro-F1 28.09% (WalkLM).",
            "comparison_to_other_representations": "Compared to RW-based baselines (Metapath2Vec, HIN2Vec), relation-learning methods (ConvE, ComplEx), supervised HGNNs (RGCN, HAN, HGT) and unsupervised HGNNs (HeCo, SHGP) and LM-only baselines, WalkLM achieves the best overall performance in the paper's experiments. Reported aggregate improvements include ~138.59% average gain on PubMed over the second-best and ~39.37% on MIMIC-III; link prediction average gain ~5.97% over second-best. The paper stresses that LM-only methods capture attributes well but lack structural information, while RW-only methods lack rich attribute modeling; WalkLM combines both.",
            "limitations_or_challenges": "Requires meaningful textual attributes (node names/descriptions) to be effective; current edge handling assumes simple textual relation names—edges with rich non-textual attributes are not fully addressed; rule-based concatenation can produce noisy/ungrammatical text; random-walk sequences capture local structure but may miss global structural patterns (depends on N and α); LM fine-tuning is computationally expensive; performance sensitive to choice of base LM and hyperparameters (N, α, mask ratio m).",
            "uuid": "e5253.0",
            "source_info": {
                "paper_title": "WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "RW+NNLM (Goikoetxea et al. 2015)",
            "name_full": "Random walks and neural network language models on knowledge bases",
            "brief_description": "An earlier approach that generates random-walk sequences on a knowledge base and trains neural language models on those sequences to learn word/entity representations; focuses on co-occurrence statistics from walks rather than textual attributes or relation semantics.",
            "citation_title": "Random walks and neural network language models on knowledge bases",
            "mention_or_use": "mention",
            "representation_name": "Random-walk-based linearization to token sequences",
            "representation_description": "Perform random walks over the graph/KG to produce sequences of node identifiers or entity tokens and train neural language models (word/entity embedding models) on those sequences to learn representations based on walk co-occurrence.",
            "graph_type": "Knowledge graphs / knowledge bases",
            "representation_properties": "Captures local topology and node proximity via walk co-occurrence; computationally efficient and parallelizable; does not textualize attributes and typically ignores edge type semantics; lower expressivity for heterogeneous attributed graphs compared to attribute-aware textualization.",
            "evaluation_task": "Learning entity/word representations (original work aimed at embeddings); in this paper referenced as antecedent work rather than re-evaluated on downstream tasks.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Paper notes this prior method ignores rich relational information between nodes and cannot learn richer topological/semantic information; WalkLM differs by textualizing attributes and leveraging modern pre-trained LMs.",
            "limitations_or_challenges": "Does not model edge semantics/attributes; limited ability to capture heterogeneous attribute semantics; token-based encoders used historically are less able to extract rich semantic features than modern pre-trained LMs.",
            "uuid": "e5253.1",
            "source_info": {
                "paper_title": "WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SimKGC",
            "name_full": "SimKGC: Simple contrastive knowledge graph completion with pre-trained language models",
            "brief_description": "A text-based KG completion approach that uses pre-trained LMs (BERT-style) with a contrastive learning objective to encode entities/relations from their textual descriptions and perform KG completion/link prediction.",
            "citation_title": "SimKGC: Simple contrastive knowledge graph completion with pre-trained language models",
            "mention_or_use": "use",
            "representation_name": "Text-based entity/relation encoding for contrastive KG completion",
            "representation_description": "Represent entities and relations by textual strings (names/descriptions), encode them with a pre-trained LM (often a bi-encoder), and train with contrastive losses and negative sampling to increase semantic similarity for correct triples; does not use random-walk sequence generation.",
            "graph_type": "Knowledge graphs / attributed graphs with textual metadata",
            "representation_properties": "Leverages LM pretraining to capture semantic similarity from textual metadata; effective when good textual descriptions are available; structural/topological information is weak unless encoded indirectly via text or contrastive negatives.",
            "evaluation_task": "Node classification and link prediction / KG completion on datasets including PubMed, MIMIC-III, Freebase, FB15K-237",
            "performance_metrics": "Macro-F1, Micro-F1, AUC, MRR. Example reported numbers (from this paper's tables): PubMed node classification Macro-F1 21.97%, Micro-F1 30.83%; MIMIC-III node classification Macro-F1 51.62%, Micro-F1 58.50%; PubMed link prediction AUC 79.62%, MRR 91.43%; MIMIC-III AUC 67.73%, MRR 84.86%.",
            "comparison_to_other_representations": "SimKGC is competitive on semantic tasks and often ranks second in the paper's comparisons; however, WalkLM—which integrates RW-based structural signals with textual attribute sequences—outperforms SimKGC in combined semantic+structural evaluations.",
            "limitations_or_challenges": "Depends on availability and quality of textual descriptions; limited explicit encoding of graph topology beyond what can be inferred from text and contrastive negatives; performance sensitive to negative sampling and contrastive design.",
            "uuid": "e5253.2",
            "source_info": {
                "paper_title": "WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LM-baselines",
            "name_full": "LM baselines (XRoBERTa, GPT-2, DistilRoBERTa) applied to textualized node attributes",
            "brief_description": "Baselines where each node's attributes (names/descriptions) are textualized into short phrases and each entity is encoded independently by a pre-trained LM (e.g., XRoBERTa, GPT-2, DistilRoBERTa) to produce embeddings, without constructing walk sequences that encode topology.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Attribute-level textualization with direct LM encoding",
            "representation_description": "For each graph entity, convert available attributes into a single natural-language string (e.g., via rule-based concatenation) or use name/description metadata, then feed each string separately into a pre-trained LM and extract the final hidden representation as the entity embedding (no explicit random-walk sequence concatenation).",
            "graph_type": "Attributed graphs, knowledge graphs",
            "representation_properties": "Preserves attribute semantics per-entity; simple and computationally cheaper than sequence-corpus LM fine-tuning on large RW corpora; does not directly encode neighborhood or relation co-occurrence information, limiting structural expressivity.",
            "evaluation_task": "Node classification and link prediction (used as baselines in experiments)",
            "performance_metrics": "Reported examples from paper: LM (DRoBERTa) — PubMed node classification Macro-F1 58.29%, Micro-F1 60.57%; MIMIC-III Macro-F1 66.25%, Micro-F1 70.14%; PubMed link prediction AUC 60.97%, MRR 83.00%; MIMIC-III AUC 51.44%, MRR 75.09%. (Other LM variants XRoBERTa and GPT-2 also reported.)",
            "comparison_to_other_representations": "LM-only methods learn attribute semantics well and outperform many graph-only baselines on attribute-rich node classification, but perform worse on link prediction than RW-based or relation-learning methods. Hybrid approaches (LM+RGCN, LM+HGT) improve performance, but WalkLM's graph-aware LM fine-tuning gives the largest gains.",
            "limitations_or_challenges": "Lack of neighborhood/topology encoding (poor link prediction); sensitivity to pretrained LM choice and pretraining corpus; may require combination with graph models or RW-based corpora to capture structural information.",
            "uuid": "e5253.3",
            "source_info": {
                "paper_title": "WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Wikidata-textualization (KG extension)",
            "name_full": "Wikidata-based textualization of KG entities/relations (Appendix A.1 experiment)",
            "brief_description": "An experimental variant for KG datasets where entities are matched to Wikidata to extract names/descriptions as textual node features and original relation labels are used as edge text; these textual features are used to form inputs to LMs (with or without RW sequences).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Metadata-driven textualization (names/descriptions + relation text)",
            "representation_description": "Map KG entities to Wikidata, pull name and description fields and use these strings as textualized node attributes; treat original KG relation labels as textual edge attributes. These textual items can be used directly as per-entity LM inputs or integrated into RW-based sequences for LM fine-tuning.",
            "graph_type": "Knowledge graphs (Freebase, FB15K-237)",
            "representation_properties": "Leverages external textual metadata to enrich semantic content; straightforward where mapping exists; coverage and quality dependent on external metadata completeness; retains textual relation labels but may still require structural modeling via walks.",
            "evaluation_task": "Node classification and link prediction on Freebase and FB15K-237 (reported in Appendix A.1 / Table 4)",
            "performance_metrics": "Reported (WalkLM on KG datasets): Freebase node classification Macro-F1 55.01%, Micro-F1 71.36%; Freebase link prediction AUC 95.65%, MRR 98.45% (numbers correspond to WalkLM using metadata-driven textualization and RW corpora).",
            "comparison_to_other_representations": "Using external descriptions improves LM-based approaches; SimKGC and other text-based KG methods are competitive, but WalkLM combining RW structure with metadata textualization obtains top results in the paper's KG experiments.",
            "limitations_or_challenges": "Requires entity disambiguation/matching to external KBs (Wikidata); metadata may be absent or noisy for many entities; mapping process is an extra preprocessing dependency.",
            "uuid": "e5253.4",
            "source_info": {
                "paper_title": "WalkLM : A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Random walks and neural network language models on knowledge bases",
            "rating": 2,
            "sanitized_title": "random_walks_and_neural_network_language_models_on_knowledge_bases"
        },
        {
            "paper_title": "SimKGC: Simple contrastive knowledge graph completion with pre-trained language models",
            "rating": 2,
            "sanitized_title": "simkgc_simple_contrastive_knowledge_graph_completion_with_pretrained_language_models"
        },
        {
            "paper_title": "KG-BERT: Bert for knowledge graph completion",
            "rating": 2,
            "sanitized_title": "kgbert_bert_for_knowledge_graph_completion"
        },
        {
            "paper_title": "Kepler: A unified model for knowledge embedding and pre-trained language representation",
            "rating": 2,
            "sanitized_title": "kepler_a_unified_model_for_knowledge_embedding_and_pretrained_language_representation"
        },
        {
            "paper_title": "Deep bidirectional language-knowledge graph pretraining",
            "rating": 1,
            "sanitized_title": "deep_bidirectional_languageknowledge_graph_pretraining"
        },
        {
            "paper_title": "Jaket: Joint pre-training of knowledge graph and language understanding",
            "rating": 1,
            "sanitized_title": "jaket_joint_pretraining_of_knowledge_graph_and_language_understanding"
        }
    ],
    "cost": 0.02158625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding</p>
<p>Yanchao Tan yctan@fzu.edu.cn 
Zihao Zhou 
Hang Lv 
Weiming Liu 
Carl Yang j.carlyang@emory.edu </p>
<p>College of Computer and Data Science
Fuzhou University Fuzhou
China</p>
<p>College of Computer and Data Science
Fuzhou University Fuzhou
China</p>
<p>College of Computer and Data Science
Fuzhou University Fuzhou
China</p>
<p>College of Computer Science
Zhejiang University Hangzhou
China</p>
<p>Department of Computer Science
Emory University Atlanta
United States</p>
<p>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding
2FFFEE9A03C91D87D780CEB4892360A1
Graphs are widely used to model interconnected entities and improve downstream predictions in various real-world applications.However, real-world graphs nowadays are often associated with complex attributes on multiple types of nodes and even links that are hard to model uniformly, while the widely used graph neural networks (GNNs) often require sufficient training toward specific downstream predictions to achieve strong performance.In this work, we take a fundamentally different approach than GNNs, to simultaneously achieve deep joint modeling of complex attributes and flexible structures of real-world graphs and obtain unsupervised generic graph representations that are not limited to specific downstream predictions.Our framework, built on a natural integration of language models (LMs) and random walks (RWs), is straightforward, powerful and data-efficient.Specifically, we first perform attributed RWs on the graph and design an automated program to compose roughly meaningful textual sequences directly from the attributed RWs; then we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM, which encapsulates both attribute semantics and graph structures.In our experiments, we evaluate the learned node embeddings towards different downstream prediction tasks on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods.We believe this work opens a door for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs.</p>
<p>Introduction</p>
<p>Graphs are widely used to model interconnected entities, and they are critical in enhancing downstream predictions in various real-world applications.Nowadays, real-world graphs are often associated with complex attributes on multiple types of nodes and even links [27,28], and modeling such real-world graphs is non-trivial.For example, in the schema of a clinical attributed graph constructed from MIMIC-III2 data (shown in Figure 1(a)), there are multiple types of nodes such as patients, visits and diseases, each with their own attributes such as age, sex, time and name; there are also multiple types of links associated with different meanings.Such complex heterogeneous attributes of nodes and links attributes can hardly be modeled in a uniform space.</p>
<p>Although the widely used graph neural networks (GNNs) have shown remarkable successes in the modeling of attributed graphs for various downstream applications [5,14,53,53], the representation learning (a.k.a.embedding) of GNNs often requires sufficient training toward specific downstream predictions to achieve strong performance [4,76].While unsupervised training has also been explored for GNNs [23,63], the generic performances of unsupervised GNN embeddings across different downstream tasks are still unsatisfactory (as we will also show in our experiments).Consequently, it is important to devise a general-purpose graph embedding method to simultaneously understand the complex node/link attributes and incorporate the flexible graph structures in an unsupervised manner.</p>
<p>However, two obstacles stand in the way of achieving this goal.First, the nature of the attributes can be intricate and diverse, thus understanding their semantics in a uniform space is non-trivial.Second, the graph structures need to be accurately captured and incorporated into the embedding space, which is not straightforward due to the inherent flexibility and potential complexity of entity relations.</p>
<p>To address these issues, in this work, we take a fundamentally different approach than GNNs, named WalkLM, which is a uniform framework to obtain unsupervised generic graph representations that are not limited to specific downstream tasks.To this end, we first draw inspiration from the recent successes of language models (LMs), and propose to leverage LMs as uniform attribute models that can capture the intricate semantics of complex heterogeneous attributes of nodes and links.Secondly, we propose to leverage the classic tool of random walks (RWs) on graphs which have been shown effective in capturing flexible graph topological structures by various studies [8,10,13,21,38].Specifically, we first generate attributed RWs on the graph (e.g., 1 has → 2 . . .for → 5 in Figure 1(b)), and design an automated textualization program to compose roughly meaningful textual sequences directly from the attributed RWs.As shown in Figure 1(c), the composed text is a mapping from the attributed RW in Figure 1(b), where a uniform automatic program firstly textualize different types of nodes (in different colors) by properly concatenating the nodes with the names and values of different attributes, and then textualize the whole RWs by concatenating the nodes and links.Furthermore, we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM.The learned embeddings encapsulate both attribute semantics and graph structures, and can be flexibly utilized for arbitrary downstream tasks.</p>
<p>In our experiments, we take the node embeddings and evaluate them towards different downstream prediction tasks (e.g., node classification and link prediction) on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods (e.g., WalkLM achieves an average of 207.01%improvement over the state-of-the-art baselines ranging from existing RW-based graph embedding methods to popular unsupervised GNN modes regarding both Macro-F1 and Micro-F1 metrics).We believe this work paves the way for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs.</p>
<p>Related Work</p>
<p>Graph Representation Learning.In recent years, a plethora of representation learning techniques are proposed for graphs [3,15,55,58,74].In this work, we focus on the objective of learning embedding vectors for each node that characterizes its topological (and semantic) information in the graph.Among existing node embedding methods, many have analyzed and utilized the great promise of random walks (RWs) in capturing the topological structures of graphs [8,13,21,38].However, the above methods ignore the abundant attribute information surrounding the nodes and edges [32].In recent studies, Graph neural networks (GNNs) for learning node representations through aggregating information from neighboring nodes on graphs [14,24,62].However, most existing GNNs are established in a supervised learning setting, which requires abundant task-specific labeled data that may not be available in real-world applications [4,76], and the embeddings they learn are not generalizable across different downstream tasks [70].Although some studies tried to reduce the labeling effort by pre-training an expressive GNN model on unlabeled data with self-supervision methods (e.g., contrastive learning) [19,22,75], their performances in specific downstream tasks still rely much on the properly chosen self-supervision tasks and attribute encoders [47,73]-that is, there still lack a uniform framework for generic unsupervised attributed graph representation learning.</p>
<p>Language Models (LMs).With the massive corpora and powerful computation resources for pretraining, modern language models (LMs) derive various families [34].These LMs can be grouped into: (1) auto-regressive LMs (e.g., GPT [40] and GPT-2/3 [1,41]), (2) masked LMs (e.g., BERT [7], RoBERTa [31], and XLNet [64]), and (3) encoder-decoder LMs (e.g., BART [26] and T5 [42]).LMs have been intensively studied by NLP researchers for various language-related tasks [16,29,43].In our work, we innovatively utilize LMs as uniform attribute models for nodes and links in graphs for the first time.Note that, our work also readily generalizes to recent large language models (LLMs) (e.g., InstructGPT [36], ChatGPT and GPT-4 [35]) via appropriate parameter-efficient training approaches (e.g., LoRA [18] and prompt-tuning [25,30]).However, those are orthogonal to the innovations in this work, for which we leave the exploration as immediate future works.</p>
<p>LMs with Knowledge Graph (KG).</p>
<p>In recent studies, combining LMs with KG has been widely applied in various real-world applications [37].Among existing methods, many have proposed to enhance LMs with KG for significantly improve the performance of LMs in accessing domain-specific knowledge [46,56,67,69], and the others proposed to harness the power of LMs for addressing KG-related tasks [65,66].However, the above methods fail to effectively combine the rich semantic information of graphs with global topological information.Furthermore, as closest to us, [12] proposed to combine random walks and neural network language models to produce new word representations.However, it ignores the rich relational information between nodes and thus fails to learn richer topological information.Moreover, compared with modern LMs with extensive prior knowledge, the text-based encoder used in [12,51] fails to extract richer semantic information.</p>
<p>Preliminaries</p>
<p>Problem Formulation</p>
<p>Given an attributed graph G and multiple downstream tasks τ i (e.g., node classification τ 1 and link prediction τ 2 ), the goal of WalkLM is to sufficiently model information in G and improve task performances on τ i .Specifically, we denote the graph as G = (V, E, Φ, Ψ), where each node v i ∈ V is associated with attributes Φ(v i ), and each edge e i ∈ E is associated with attributes Ψ(e i ).</p>
<p>To fully exploit both the semantic information in Φ and Ψ, and the structural information in V and E, we first design attributed random walks (RWs) based automated textualization program on G, where we can transform the attributed graph to the meaningful textual sequences W = {W i } N i=1 .Then, we fine-tune a graph-aware language model (LM) using the RW-based textual sequences W, find the optimized parameters Θ of the LM, and extract embedding vectors from the LM.Finally, we apply these embeddings to τ 1 and τ 2 for performance evaluation.</p>
<p>Masked Language Modeling</p>
<p>Masked Language Modeling (MLM) is a widely-used and highly effective technique for training large-scale language models [7].In the MLM task, 15% of the language tokens are randomly chosen for prediction.If the i-th token in a sequence W is chosen, it can be replaced by (1) the [MASK] token 80% of the time, (2) a random token 10% of the time, and (3) the unchanged i-th token 10% of the time.Then, token t i will be used to predict the original token with following cross-entropy loss [7,59,60]:
L M LM = − 1 |X| ∑ X∈X ∑ ti∈M log p (t i |T /i ) ,(1)
where X is a set of training examples, M is the prediction set of the masked token, T /i = {t 1 , . . ., t i−1 , t i+1 , . . ., t L } is the surrounding token set of t i , and
|T /i | = L − 1.</p>
<p>Methodology</p>
<p>In this section, we present the proposed method WalkLM, which comprises two major components.The first component, the attributed random walk (RW) based textualization program, captures both topological structures and attribute information of the graph and composes corresponding text automatically.The second component, the graph-aware language model (LM) fine-tuning, leverages pre-trained LM to encode the complex semantics along with the graph structure.The overall model architecture is shown in Figure 2.</p>
<p>Attributed RW-based Textualization Program</p>
<p>To model node/link attributes, traditional machine learning algorithms require a standard process of vectorization, which transforms different types of attributes into categorical or numerical features as model input.However, such a vectorization process removes the actual semantic meanings of the attributes, and it cannot unify different types of attributes (e.g., ages, sexes, time, etc.) in the same space.Inspired by the recent successes of LMs, we find it promising to leverage pre-trained LMs to understand the intricate semantics of complex heterogeneous attributes [29,34].The key idea is to perform textualization instead of vectorization, that is, to transform different types of attributes into texts, which can then be modeled by the LMs in a uniform space.Therefore, we first design the following process for the textualization of individual entities in the graph (i.e., nodes and edges).</p>
<p>Entity-level Textualization.Inspired by a wide range of NLP tasks that leverage prompts to construct informative rule templates, we propose to textualize attributed graph entities via a rule-based program function P(⋅) that automatically concatenates attribute values with the corresponding attribute names, as well as attributes and the corresponding entity id.For example, as shown in Figure 1, for a patient node v i with attributes Φ(v i ) = {age ∶ 35, sex ∶ female, pid ∶ P246}, the texualization program will convert it into P(v i ) = &lt; A 35-year-old female patient P246 &gt;.For edges, in this work, we only consider simple relational edges such as has and including, which are already texts, but our framework is readily extensible to edges with more complex attributes.</p>
<p>For attributed graphs, after modeling the complex attributes of individual entities, the next challenge would be to model the flexible graph topological structures.To this end, we propose to utilize the powerful and efficient tool of RWs, as the foundation for the textualization of graphs.Specifically, we design the following process:</p>
<p>Walk-level Textualization.We first initiate an attributed RW W by randomly selecting a node v 0 and attaching the textualized node information P(v 0 ) to W .Then, we extend W by randomly selecting an edge e 1 starting from v 0 as in a standard RW with a uniform probability as 1 divided by the number of out-edges of v 0 , with its terminating node v 1 , and appending the corresponding texts P(e 1 ) and P(v 1 ) to W .We can keep adding edges and nodes until the random walk terminates, such as based on a termination probability α.The final textual sequence W = {P(v 0 ), P(e 1 ), P(v 1 ), . . ., P(v L−1 ), P(e L ), P(v L )}, which corresponds to an actual attributed random walk with the length of 2L + 1 on the graph, will be a roughly meaningful sentence, such as the one shown in Figure 1(c).</p>
<p>After performing the RW for N times, we can obtain N attributed RWs as W = {W i } N i=1 , which constitutes a graph-aware corpus for training LMs without any downstream task supervision.</p>
<p>Discussion.We believe that the above proposed attributed RW-based textualization program is effective in capturing both complex attribute information and flexible topological structures of the graph in an unsupervised manner.Such ability arises from two critical properties: (1) Random walks are known to be capable of providing characteristic graph traits and reconstructing network proximity of nodes [21,33].To be specific, it has been proven that the distribution of random walks starting at a particular node, which can be estimated with sufficient numbers of random walks, can sufficiently preserve the subgraph structure around the node.This means a sufficient amount of attributed RWs from different nodes can well reflect the topological structures of graphs.(2) Our textualization program completely preserves the attributes of nodes and edges, as well as the whole RWs, and it presents such information as meaningful texts, which is natural for LMs to comprehend.Moreover, RWs are known to be efficient and highly parallelizable, where numerous threads can run simultaneously to generate large amounts of RWs [9].Note that, we only need to perform the rule-based textualization once for every node during the pre-processing stage, which is also efficient and highly amenable to parallelization.</p>
<p>Graph-Aware LM Fine-Tuning</p>
<p>Despite the robust generalizability of LMs, fine-tuning remains a necessary step [57,68], which allows the general LM to adapt its broad language capabilities to the specificities of the different attributed graphs.</p>
<p>As one of the mainstream language modeling techniques, masked language modeling (MLM) is proven to sufficiently utilize textual semantic information for further fine-tuning LMs [39,77].To achieve the balance between effectiveness and efficiency, we propose a graph-aware LM finetuning mechanism with knowledge distillation [2,17,44].Specifically, we adopt a general LM DistilRoBERTa (abbr.DRoBERTa) 3 as our starting point for fine-tuning, where RoBERTa is a widely used successor of BERT [7].Note that, DRoBERTa can further reduce the size of the original RoBERTa model by 40% and achieve 60% faster training while retaining 97% capacity of RoBERTa's language understanding [44].Then, we feed the attributed RW W ∈ W to the LM tokenizer and obtain the corresponding token list
T = {t 1 , t 2 , . . . , t K , &lt; MASK &gt; 1 , &lt; MASK &gt; 2 , . . . , &lt; MASK &gt; |M| },
where t i denotes the unmasked token, and &lt; MASK &gt; i denotes the token chosen for prediction.In this way, we create a training example X i = ⟨W, T ⟩ ∈ X for fine-tuning, where X = {X 1 , X 2 , . . ., X N } is the whole training dataset.We adopt the cross-entropy loss as the fine-tuning objective [7,44,59], which is formulated as follows:
L F T (Θ) = − 1 |X| ∑ Xi∈X ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ∑ t * k ∈M log exp(Sim(t k , t * k )) ∑ t∈V exp(Sim(t k , t)) ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ , (2)
where Θ is the learnable parameters of our graph-aware LM, M is the ground-truth set of the masked token, V is the token vocabulary, t k is the prediction token, t * k is the ground-truth token, and Sim(t i , t j ) is the similarity scoring function between t i and t j .</p>
<p>After obtaining the fine-tuned LM based on MLM, we can extract generic graph embeddings (e.g., node embeddings based on node name).For example, we can access the representation of disease embedding in Figure 1(c) via extracting the embedding of Epistaxis.</p>
<p>Complexity Analysis. The time complexity of fine-tuning is O(Iter
⋅ |N | ⋅ l 2 avg ⋅ d),
where Iter is the number of iterations of training, N is the number of training examples, l avg is the average length of input textual sequences for the LM and d is the dimension of embedding.We infuse global graph structure knowledge into the LM to distinguish similar positions instead of negative sampling, making it possible to fine-tune more efficiently [59].</p>
<p>Model Extension.Our framework is a fundamental approach to integrate LMs and RWs for generic attributed graph embedding, which can choose different LMs according to different tasks and domains (shown in our experiments in Sec.5.4) and generalize to recent large language models (LLMs) (e.g., InstructGPT [36], ChatGPT and GPT-4 [35]) via appropriate parameter-efficient training approaches such as LoRA [18].A full exploration of different LMs is orthogonal to the main contributions in this work, which is left as future work.</p>
<p>Datasets Extension.Our method introduces the novel process of textualization, which converts general attributed graphs into text-like sequence data.This process allows us to leverage the capabilities of pre-trained language models for graph representation learning.Note that, our method only requires some meaningful attributes on the graphs, which are available in most real-world graphs such as biological networks, social networks, and knowledge graphs.Some preliminary experimental results of graph classification and KG-related datasets are shown in Appendix A.1 and Appendix A.2.</p>
<p>Various Downstream Tasks</p>
<p>In this work, we focus on node embeddings since they are most commonly studied for graph representation learning, and it is straightforward to extract node embeddings from the fine-tuned LM based on node names (or node IDs if the node has no meaningful name).However, WalkLM can also easily generate edge embeddings, by adding edge names (e.g., relation names) or edge IDs to the textualization process, and even obtain subgraph/graph embeddings via appropriate embedding aggregation mechanisms.The extracted embeddings can be directly used as fixed feature vectors to train downstream prediction models for tasks such as node classification or link prediction.Alternatively, these embeddings can also serve as initialization for more learnable embeddings in complex neural network models, which can be further updated according to the specific requirements of the downstream task.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Datasets.We conduct extensive experiments on two real-world datasets, PubMed4 and MIMIC-III 5 .PubMed contains a graph of genes, diseases, chemicals, and species.The nodes and edges are extracted according to [61].A relatively small fraction of diseases are grouped into eight categories.MIMIC-III contains a graph of diseases, patients, and visits, where nodes and relations are extracted from clinical records.Diseases are classified into nineteen categories according to ICD-9-CM 6 .The detailed statistics are shown in Table 1.Competitors.We compare our proposed WalkLM with ten graph-oriented baselines that are designed for heterogeneous information networks (HINs) or knowledge graphs (KG), which can handle different types of nodes and edges.We divided them into four groups as follows:</p>
<p>(1) RW-based methods: Metapath2Vec (abbr.M2V) [8] proposes to use user-defined meta-paths as guidance, so as to learn node embeddings on HINs.HIN2Vec [10] carries out multiple prediction training tasks jointly based on a target set of relations to learn node embeddings and meta-paths on HINs.</p>
<p>(2) Relation learning-based methods: ConvE [6] proposes to use 2D convolution over embeddings and multiple layers of non-linear features to model KGs.ComplEx [49] handles a large number of binary relations using complex-valued embeddings on KGs.SimKGC [52] proposes to elicit the implicitly stored knowledge from BERT and designs a text-based contrastive learning mechanism for knowledge graph completion.</p>
<p>(3) Supervised heterogeneous graph neural networks (HGNNs): RGCN [45] proposes to apply GCN to model HINs or KGs.HAN [53] proposes to learn the importance between a node and its meta-path based neighbors on HINs.HGT [20] proposes to use each edge's type to parameterize the transformer-based self-attention architecture on HINs.For the above supervised HGNNs, we use link prediction loss introduced in GraphSAGE [14] to achieve unsupervised learning (i.e., without any node labels), following existing studies on HINs [11,61].</p>
<p>(4) Unsupervised HGNNs: HeCo [54] proposes a co-contrastive learning mechanism for HGNNs.SHGP [63] designs a self-supervised pre-training method for HGNNs.</p>
<p>Settings.We mainly compare ten algorithms under the setting of unsupervised graph learning.The full code for this work is available 7 .All the models are optimized through the Adam optimizer and As shown in Table 2, our proposed WalkLM has superior performance, indicating the importance of leveraging both semantic and structural information in attributed graphs.WalkLM achieves 138.59% performance gains on PubMed over the second-best performance on average while achieving 39.37% average performance gains on MIMIC-III.Specifically, SimKGC achieves second-best performance by effectively employing text-based contrastive learning, which leverages BERT to capture a rich set of semantic information.Compared with SimKGC, WalkLM can effectively combine the complex semantic and graph structure information of attributed graphs, so as to accurately model the complex attributes of nodes.Although the HGNNs can naturally model attributes, their unsupervised training mechanisms likely do not align well with the downstream prediction task of node classification.</p>
<p>Results in the Few-shot Setting.Since one key challenge of node classification lies in the generalizability and adaptability of models [63], we design a few-shot setting to evaluate models in extending knowledge to unseen scenarios and adapting to new tasks with limited training data.</p>
<p>As shown in Table 3, our framework can stay strong with a small size of training data, where we win a 275.45% performance gain over the second-best performance on average.Through the novel textualization process that converts general attributed graphs into text-like sequence data, our proposed WalkLM can leverage the capabilities of modern language models for graph representation learning.With the extensive pre-training of LMs on broad text corpora, WalkLM can easily understand meaningful node attributes given a new graph, while the random walk strategy further allows it to capture graph structures.Consequently, WalkLM maintains superior performance even in the few-shot setting.Similar to the main results in Table 2, the ranking of baselines is fluctuating across datasets, where ConvE and M2V continue to exhibit promising performance.Note that ComplEx and HIN2Vec exhibit notable improvements in this setting, likely because HIN2Vec can also learn node representations based on random walks and ComplEx can capture fine interactions through complex-valued vectors, thus being more capable of capturing comprehensive node information before supervision.</p>
<p>Link Prediction</p>
<p>For link prediction, we train all models with the randomly selected 80% links and evaluate towards the 20% held-out links.We use the Hadamard function to construct feature vectors for node pairs and train a two-layer MLP classifier on the 80% training links.We evaluate WalkLM with AUC (area under the ROC curve) and MRR (mean reciprocal rank).Note that, HAN cannot predict links on MIMIC-III for its restriction to embed only one type of node at a time, and thus it cannot predict links between different types of nodes on MIMIC-III [53].As shown in Table 2, our fine-tuned WalkLM demonstrates outstanding performance in uncovering latent associations among nodes in attributed graphs.In general, WalkLM outperforms all ten baselines with an average of 5.97% performance gain over the second-best performance, showing that our proposed framework can learn accurate edge representation for link prediction.As a RW-based method, M2V can effectively employ meta-path-guided random walks to capture topological information and trace meta-path to understand relations between nodes.As relation-learning methods, ConvE and ComplEx design different deep neural models to evaluate triplets.ConvE can achieve good performance by using convolutions over embeddings to mine relations between entities.ComplEx can capture relations via complex-valued embeddings, so as to better represent the complex inherent relations among entities.Compare to M2V, ConvE, and ComplEX, WalkLM can effectively capture the complex relations by providing characteristic graph traits and reconstructing network proximity of nodes that inherit from RWs.On the other hand, the unsupervised HGNNs, especially HeCo and SHGP, perform rather poorly, again because their training mechanisms are not aligned with the link prediction task.Such observation is consistent with the results in the recent work [72], showing the heterogeneous approaches that only preserve certain-type entities fail to capture accurate representations for all kinds of nodes.</p>
<p>Ablation Studies</p>
<p>To better understand our proposed techniques, we closely study our framework by selecting different LMs and varying the graph-aware LM fine-tuning mechanism.</p>
<p>Compared with the graph-based baselines, the LM-based models (e.g., LM (XRoBERTa8 ), LM (GPT-2 9 ), and LM (DRoBERTa10 )) are able to learn accurate and rich node attributes.However, it is difficult for them to mine the relations between nodes in the attributed graph, where all of them perform worse than the RW-based and relation learning-based methods in the link prediction task.Considering the overall performance of the above three LMs on two different tasks and the goal of learning graph embedding, we choose LM (DRoBERTa) as our starting point for fine-tuning.Furthermore, we show that LM can further effectively integrate with existing heterogeneous graph algorithms, such as LM + RGCN and LM + HGT, resulting in a notable performance enhancement over their individual methods.Compared with LM + RGCN and LM + HGT, our proposed graph-aware LM fine-tuning can achieve the largest improvement gains based on the chosen LM (DRoBERTa) in both node classification and link prediction tasks, showing the effectiveness of capturing topological information together with semantics in modeling attributed graphs.The detailed analysis of the ablation studies is shown in Appendix A.3.</p>
<p>Hyper-parameter Studies</p>
<p>In this subsection, we investigate the model sensitivity on the number of sampled walks N and the termination probability α, which are the major hyper-parameters in WalkLM.For the space limitation, we show results on PubMed in Figure 3 and the results on MIMIC-III in Appendix A.4.Overall, WalkLM is not sensitive to the two hyper-parameters, where its performance increases slowly with N and α.Note that, too small N or large α can cause the textualization data |C| to lose sufficient information, while too large N or small α lead to extensive |C| and increase computational costs for fine-tuning.Setting N around 3 × 10 5 and α around 0.05 seems appropriate to generate sufficient textual sequences, which can achieve a good balance of performance and efficiency.We additionally investigate the model's sensitivity to the quantity of language model masking samples, aiming to elucidate the parameter's influence on the performance of downstream tasks.The detailed experimental results are shown in Appendix A.4.</p>
<p>Visualization</p>
<p>For an intuitive comparison, we visualize the embedding space of different types of nodes which are learned by M2V, ConvE, HGT, and our WalkLM, respectively.Specifically, we select gene/disease nodes on PubMed and patient/visit/disease nodes on MIMIC-III.The embeddings are further transformed into the 2-dimensional Euclidean space via the t-SNE algorithm [50].The nodes and links are both colored according to their types.</p>
<p>As shown in Figure 4, M2V, ConvE, and HGT have blurred boundaries and even overlaps between different types of nodes, which are hard to distinguish.Our WalkLM shows the clearest boundaries between different types of nodes and the best within-type compactness, which indicate it can automatically organize heterogeneous nodes in a uniform space.Moreover, by connecting different types of nodes according to the relations in the data, we find that WalkLM can provide more discriminate distributions for different types of relations than others.The visualizations clearly demonstrate the advantages of WalkLM in capturing both attribute semantics and topological structures on graphs.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel uniform language model fine-tuning framework for attributed graph embedding.The proposed WalkLM consists of two key modules, which encapsulate both attribute semantics and graph structures and obtain unsupervised generic graph representations.We conduct extensive experiments to demonstrate the superior effectiveness of WalkLM against state-of-the-art baselines.For future work, it is intriguing to further design more sophisticated techniques and empirical evaluations toward the leverage of LMs and generalize our work to modern LLMs.</p>
<p>Acknowledgments</p>
<p>This</p>
<p>A Appendix</p>
<p>A.1 Experiments based on KG datasets Datasets.We conduct extensive experiments on two new real-world KG datasets, i.e., Freebase11 and FB15K-237 12 .Freebase contains a graph of books, films, sports, and locations.The nodes and edges are extracted according to [61].A large portion of books are labeled into eight genres of literature.</p>
<p>Each labeled book has only one label.FB15K-237 is a standard dataset in the knowledge graph community, which contains 310,116 triples with 14,541 entities and 237 relation types.Since we did not manually label the nodes, we only predicted whether a is correct or not on this dataset.We matched the entities with Wikidata entities and obtained metadata from Wikidata, and constructed a rough attribute graph dataset by using the names and descriptions of the nodes as textualized features of the nodes, and directly applying their original relationship text as the edge textualized attributes.</p>
<p>Node Classification.As shown in</p>
<p>A.2 Graph-level Classification</p>
<p>Compared with node or edge classification, aggregating node embeddings for graph-level classification needs more context information.Furthermore, graph-level classification presents its own set of challenges, which require holistic capturing of graph structures and often do not rely much on attributes.Therefore, it is difficult to find a universal representation learning approach that solves all different levels of graph mining tasks.Technically, adapting our method to graph-level classification necessitates some subtle decisions to make (such as whether to include graph ID as a virtual node).We've conducted a preliminary analysis on aggregating our learned node embeddings for graph-level tasks.Specifically, we adopt a widely-used MUTAG 13 dataset and use mean accuracy as the metric [48,71].The results on the popular MUTAG dataset are listed in Table 5.Although the findings are encouraging and show the potential of WalkLM, further studies are still needed to establish a clear advantage of our approach over SOTA graph classification baselines.</p>
<p>A.3 Detailed Ablation Studies</p>
<p>From Table 6, we have the following observations: (1) Compared with the graph-based baselines, the LM-based models (e.g., LM (XRoBERTa), LM (GPT-2), and LM (DRoBERTa)) are able to learn accurate and rich node attributes, leading to superior performance in node classification.For the PubMed dataset distributed on 8 classes, LM (XRoBERTa), LM (GPT-2), and LM (DRoBERTa) achieve 63.78%, 135.04%, and 130.89% performance gains over the second-best performance on (2) The choice of LMs can affect the performance of fine-tuning.Due to different pre-training corpora, LM (XRoBERTa) performs worse than LM (DRoBERTa) in most cases.Moreover, LM (GPT-2) achieves an average of 3.30% improvement over LM (DRoBERTa) in node classification, while LM (DRoBERTa) achieves an average of 7.08% improvement over LM (GPT-2) in link prediction.</p>
<p>Considering the overall performance of the above three LMs on two different tasks and the goal of learning graph embedding, we choose LM (DRoBERTa) as our starting point for fine-tuning.</p>
<p>(</p>
<p>A.4 Detailed Hyper-parameter Studies</p>
<p>We show the results of the model sensitivity on the number of sampled walks N and the termination probability α on MIMIC-III in Figure 5. Overall, WalkLM is not sensitive to the two hyper-parameters, where its performance increases slowly with N and α.Note that, setting N around 3 × 10 5 and α around 0.05 seems appropriate to generate sufficient textual sequences and limit computational costs  for fine-tuning, which can achieve a good balance of performance and efficiency.Furthermore, for the ratio of masked samples m, the specific results are listed in Table 7. Overall, WalkLM is sensitive to m, where the optimal value across different tasks is 0.15, which is consistent with the empirical selection in our paper and the previous work [7,31,40,60] (b) N on link prediction (a) N on node classification (d)  on link prediction (c)  on node classification</p>
<p>Figure 1 :
1
Figure 1: A toy example of the transformation from a real-world attributed graph to the composed text.(a) is a schema of a real-world attributed graph on MIMIC-III that delineates how nodes (e.g, patients), edges (e.g, has between patients and visits), and the associated attributes (e.g., age) are organized and interconnected.(b) is attributed random walk for capturing structural information and can be composed to text in (c).</p>
<p>Figure 2 :
2
Figure 2: The overall framework of WalkLM: (a) An attributed random walk (RW) based textualization program obtains attributed RWs and composes roughly meaningful textual sequences directly from the attributed RWs.(b) A pre-trained general LM is fine-tuned in a graph-aware fashion using the generated textual sequences to produce generic graph embeddings.(c) The learned graph embeddings can be applied to various downstream tasks.</p>
<p>(</p>
<p>b) N on link prediction (a) N on node classification (d)  on link prediction (c)  on node classification</p>
<p>Figure 3 :
3
Figure 3: Analysis of the number of sampled walks N and the termination probability α.</p>
<p>Figure 4 :
4
Figure 4: Visualization of different types of node embeddings on PubMed and MIMIC-III.</p>
<p>Figure 5 :
5
Figure 5: Analysis of the number of sampled walks N and the termination probability α.</p>
<p>Table 1 :
1
Statistics of the datasets.
Dataset#attribute type #node type #node #link type# link#label type #label nodePubMed8463,10910244,9868454MIMIC-III10332,2674559,290194880</p>
<p>Table 2 :
2
Different downstream task results (%) with the corresponding std (±) on two datasets.The best performances are in bold and the second runners are shaded in gray, where * denotes a significant improvement according to the Wilcoxon signed-rank significance test.
TaskNode ClassificationLink PredictionDatasetPubMedMIMIC-IIIPubMedMIMIC-IIIMetricMacro-F1 Micro-F1 Macro-F1 Micro-F1AUCMRRAUCMRRM2V15.3520.2719.6929.2474.5389.5875.0588.32(±1.27)(±3.01)(±0.62)(±1.57)(±3.79) (±2.05) (±0.41) (±0.23)HIN2Vec11.5718.9219.1228.0574.2190.5673.4688.10(±1.23)(±2.78)(±1.32)(±1.44)(±5.49) (±1.06) (±0.41) (±0.14)ConvE16.0619.1624.4432.8976.4892.2769.5684.88(±3.69)(±4.00)(±1.28)(±0.86)(±4.31) (±0.57) (±0.36) (±0.25)ComplEx13.9318.279.8221.3979.8191.7963.8681.40(±2.59)(±4.12)(±0.56)(±3.12)(±0.97) (±0.48) (±0.42) (±0.40)SimKGC21.9730.8351.6258.5079.6291.4367.7384.86(±3.51)(±3.10)(±1.81)(±1.52)(±2.72) (±0.48) (±1.69) (±0.54)RGCN12.5018.507.1914.5572.0888.2057.3173.91(±2.36)(±1.41)(±0.77)(±3.25)(±1.13) (±0.47) (±0.71) (±0.57)HAN15.2916.956.9814.7370.5787.89--(±2.87)(±2.71)(±0.58)(±1.69)(±1.58) (±0.62)--HGT11.9820.128.0317.7977.2489.6364.0181.54(±2.23)(±3.89)(±0.87)(±0.83)(±3.50) (±0.84) (±0.36) (±0.56)HeCo10.3218.0110.7815.2665.0483.2953.1371.81(±1.12)(±0.87)(±0.41)(±1.52)(±1.26) (±0.72) (±0.47) (±0.35)SHGP10.8019.2811.3417.4468.2285.3454.4972.58(±3.03)(±0.91)(±1.29)(±1.49)(±2.71) (±0.48) (±0.33) (±0.24)LM40.1044.7154.5161.2760.2084.2351.2174.22(XRoBERTa) (±4.62)(±3.68)(±1.50)(±1.22)(±2.78) (±1.71) (±0.17) (±0.26)LM59.4361.5370.2672.6751.7180.5450.6672.36(GPT-2)(±4.73)(±3.43)(±1.43)(±0.90)(±3.67) (±2.49) (±0.74) (±0.86)LM58.2960.5766.2570.1460.9783.0051.4475.09(DRoBERTa) (±2.44)(±2.11)(±1.60)(±1.52)(±2.98) (±0.40) (±0.14) (±0.29)LM13.8322.7014.3224.5972.3588.8658.6278.78+RGCN(±0.73)(±3.25)(±0.87)(±1.17)(±4.34) (±1.46) (±0.50) (±0.10)LM12.8121.7910.4920.5782.9789.9865.0182.28+HGT(±1.22)(±3.54)(±0.41)(±0.97)(±3.91) (±0.88) (±0.20) (±0.30)WalkLM60.42<em>62.33</em>75.16<em>77.89</em>85.65<em>94.16</em>82.15<em>92.78</em>(±2.62)(±3.13)(±0.93)(±0.70)(±3.28) (±0.37) (±0.67) (±0.68)</p>
<p>Table 3 :
3
Node classification results (%) in the few-shot setting with Macro-F1 (abbr.Ma-F1) and Micro-F1 (abbr.Mi-F1) metrics.The hyper-parameters of baselines are chosen carefully based on either grid search or their official source codes.For all the methods, we use a five-fold cross-validation for a more reliable evaluation of the model's performance.All the experiments are performed with two NVIDIA GTX 3090 Ti GPUs.For link prediction, we deploy the Large Margin Nearest Neighbor (LMNN) technique based on the embeddings generated by our WalkLM.Then we construct feature vectors for edges.For node classification, we train a separate one-layer MLP classifier based on the learned embeddings on 80% of the labeled nodes and predict the remaining 20%.All the methods are trained in an unsupervised manner without classification labels.We evaluate WalkLM with Macro-F1 (across all labels) and Micro-F1 (across all nodes).
DatasetPubMedMIMIC-IIISetting1 shot3 shot5 shot1 shot3 shot5 shotMetricMa-F1Mi-F1Ma-F1Mi-F1Ma-F1Mi-F1Ma-F1Mi-F1Ma-F1Mi-F1Ma-F1Mi-F1ComplEx9.3112.5110.3213.2610.1215.942.825.292.003.033.879.26M2V9.8613.4210.2712.5612.9714.985.838.723.915.113.404.49ConvE13.2313.458.8410.9311.2513.535.856.755.617.246.317.69RGCN9.3411.028.5710.5810.8413.434.975.825.436.285.225.73HIN2Vec8.4610.549.0412.7910.9617.395.7210.334.905.723.574.91SHGP8.9412.799.1211.7310.5315.144.126.355.366.584.475.34WalkLM28.09<em>30.94</em>32.11<em>35.35</em>35.41<em>37.68</em>23.33<em>27.96</em>34.19<em>40.49</em>41.12<em>46.83</em>the learning rate is searched in [1e-4, 1e-2]. 5.2 Node ClassificationMain Results.</p>
<p>work was supported in part by the National Natural Science Foundation of China (No. 6230071268).Carl Yang was supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913.</p>
<p>Table 4
4
, our proposed WalkLM has superior performance, indicating the importance of leveraging both semantic and structural information in attributed graphs.WalkLM achieves 40.24% performance gains on Freebase over the second-best performance on average.Specifically, as a text-based Knowledge graph completion method, SimKGC can effectively employ text-based contrastive learning to capture a rich set of semantic information.Compared with SimKGC, WalkLM can effectively combine the complex semantic and graph structure information of attributed graphs, so as to accurately model the complex attributes of nodes.Link Prediction.We evaluate WalkLM with AUC and MRR.As shown in Table4, our fine-tuned WalkLM demonstrates outstanding performance in uncovering latent associations among nodes in attributed graphs.In general, WalkLM outperforms all ten baselines with an average of 2.05% performance gain over the second-best performance, showing that our proposed framework can learn accurate edge representation for link prediction.ComplEx, and ConvE consistently demonstrate promising performance by effectively capturing generic node representations.However, as a textbased Knowledge graph completion method, SimKGC can sometimes outperform others in terms of the MRR metric, where SimKGC can enhance semantic similarity between nodes through contrastive learning based on bi-encoder architecture and three types of negatives.Compared with ConvE, ComplEX, and SimKGC, WalkLM can effectively capture the complex relations by providing textbased semantic traits of characteristic graph and reconstructing network proximity of nodes that inherit from RWs.</p>
<p>Table 4 :
4
Different downstream task results (%) with the corresponding std (±) on two KG datasets.The best performances are in bold and the second runners are shaded in gray, where * denotes a significant improvement according to the Wilcoxon signed-rank significance test.
TaskNode ClassificationLink PredictionDatasetFreebaseFreebaseFB15K-237MetricMacro-F1Micro-F1AUCMRRAUCMRRM2V25.74±1.1250.25±2.5780.68±1.8188.97±0.9390.35±0.5096.78±0.19HIN2Vec15.56±1.0743.67±2.1280.04±3.0190.90 ±0.8779.68±0.8392.85±0.40ConvE25.13±1.8349.31±3.4588.14±1.0393.57±0.4292.88±0.4297.57±0.15ComplEx20.25±1.6249.43±3.5784.01±1.4391.46±0.5695.03±0.3597.88±0.22SimKGC35.88±0.8756.12±0.4587.33±1.5194.21±0.3493.80±0.3197.62±0.30RGCN15.37±1.5445.86±1.0382.75±0.8991.52±0.6485.88±0.3589.84±0.19HAN14.25±1.7739.30±2.1880.73±1.3791.61±0.3482.06±0.5389.31±0.89HGT19.97±1.3447.99±2.5681.94±1.8489.65±0.4387.41±0.6994.62±0.34HeCo23.95±1.4548.62±1.1379.32±0.8687.40±0.3278.82±0.3790.41±0.23SHGP13.83±1.2739.07±1.3978.37±1.7785.52±0.6978.56±0.3389.84±0.21XRoBERTa48.10±2.0167.95±0.9773.94±1.6288.17±0.9175.62±0.7291.10±0.71GPT-249.24±2.1268.28±1.3760.45±2.4383.29±1.8768.87±1.2185.23±1.73DRoBERTa51.76±1.2469.51±0.7379.22±1.8591.21±1.1784.15±0.6393.39±0.39LM+RGCN28.38±0.6353.37±2.2783.63±1.8196.38±0.6787.72±0.5094.47±0.46LM+HGT20.79±0.6748.73±3.1383.09±1.2389.79±0.3588.18±0.6194.85±0.27WalkLM55.01±2.67<em> 71.36±1.53</em> 92.11±2.24<em> 96.54±0.56</em> 95.65±0.18<em> 98.45±0.33</em></p>
<p>Table 5 :
5
Accuracy results (%) of graph-level classification on MUTAG.
DatasetMUTAGModelHIN2Vec ConvE ComplEx LM (DRoBERTa) WalkLM w/o. graph-ID WalkLMAccuracy78.7277.6478.6979.2379.7781.39*average, respectively. For the MIMIC-III dataset on the total 19 classes, LM (XRoBERTa), LM(GPT-2), and LM (DRoBERTa) achieve 5.17%, 30.17%, and 20.12% average performance gainscompared to the second-best performance, respectively.</p>
<p>) Furthermore, LM can further effectively integrate with existing heterogeneous graph algorithms, resulting in a notable performance enhancement over their individual methods.Specifically, compared with RGCN, LM + RGCN achieves an average of 50.38% improvement in node classification, and achieves up to 6.59% improvements in link prediction.Compared with HGT, LM + HGT achieves up to 30.64% improvements in node classification and 7.42% improvements in link prediction.(4)Compared with LM + RGCN and LM + HGT, our proposed graph-aware LM fine-tuning can achieve the largest improvement gains based on the chosen LM (DRoBERTa) in both node classification and link prediction tasks, showing the effectiveness of capturing topological information together with semantics in modeling attributed graphs.Specifically, our WalkLM outperforms the chosen LM (DRoBERTa) by up to 13.45% in node classification.In Particular, our WalkLM achieves up to 59.70% improvements in link prediction, which demonstrates our WalkLM can better learn accurate edge representation for link prediction by the graph-aware LM fine-tuning.</p>
<p>Table 6 :
6
The detailed ablation results (%) with the corresponding std (±) on two datasets.The best performances are in bold and the second runners are shaded in gray, where * denotes a significant improvement according to the Wilcoxon signed-rank significance test.
TaskNode ClassificationLink PredictionDatasetPubMedMIMIC-IIIPubMedMIMIC-IIIMetricMacro-F1Micro-F1Macro-F1Micro-F1AUCMRRAUCMRRM2V15.3520.2719.6929.2474.5389.5875.0588.32(±1.27)(±3.01)(±0.62)(±1.57)(±3.79)(±2.05)(±0.41)(±0.23)HIN2Vec11.5718.9219.1228.0574.2190.5673.4688.10(±1.23)(±2.78)(±1.32)(±1.44)(±5.49)(±1.06)(±0.41)(±0.14)ConvE16.0619.1624.4432.8976.4892.2769.5684.88(±3.69)(±4.00)(±1.28)(±0.86)(±4.31)(±0.57)(±0.36)(±0.25)ComplEx13.9318.279.8221.3979.8191.7963.8681.40(±2.59)(±0.56)(±3.12)(±0.97)(±0.48)(±0.42)(±0.40)SimKGC21.9730.8351.6258.5079.6291.4367.7384.86(±3.51)(±3.10)(±1.81)(±1.52)(±2.72)(±0.48)(±1.69)(±0.54)RGCN12.5018.507.1914.5572.0888.2057.3173.91(±2.36)(±1.41)(±0.77)(±3.25)(±1.13)(±0.47)(±0.71)(±0.57)HAN15.2916.956.9814.7370.5787.89--(±2.87)(±2.71)(±0.58)(±1.69)(±1.58)(±0.62)--HGT11.9820.128.0317.7977.2489.6364.0181.54(±2.23)(±3.89)(±0.87)(±0.83)(±3.50)(±0.84)(±0.36)(±0.56)HeCo10.3218.0110.7815.2665.0483.2953.1371.81(±1.12)(±0.87)(±0.41)(±1.52)(±1.26)(±0.72)(±0.47)(±0.35)SHGP10.8019.2811.3417.4468.2285.3454.4972.58(±3.03)(±0.91)(±1.29)(±1.49)(±2.71)(±0.48)(±0.33)(±0.24)LM40.1044.7154.5161.2760.2084.2351.2174.22(XRoBERTa) (±4.62)(±3.68)(±1.50)(±1.22)(±2.78)(±1.71)(±0.17)(±0.26)LM59.4361.5370.2672.6751.7180.5450.6672.36(GPT-2)(±4.73)(±3.43)(±1.43)(±0.90)(±3.67)(±2.49)(±0.74)(±0.86)LM58.2960.5766.2570.1460.9783.0051.4475.09(DRoBERTa) (±2.44)(±2.11)(±1.60)(±1.52)(±2.98)(±0.40)(±0.14)(±0.29)LM13.8322.7014.3224.5972.3588.8658.6278.78+RGCN(±0.73)(±3.25)(±0.87)(±1.17)(±4.34)(±1.46)(±0.50)(±0.10)LM12.8121.7910.4920.5782.9789.9865.0182.28+HGT(±1.22)(±3.54)(±0.41)(±0.97)(±3.91)(±0.88)(±0.20)(±0.30)WalkLM60.42<em>62.33</em>75.16<em>77.89</em>85.65<em>94.16</em>82.15<em>92.78</em>(±2.62)(±3.13)(±0.93)(±0.70)(±3.28)(±0.37)(±0.67)(±0.68)</p>
<p>Table 7 :
7
Different downstream task results (%) with ratio of masked samples m on PubMed.
TaskNode ClassificationLink PredictionMetricMacro-F1 Micro-F1AUCMRRm = 0.0552.9756.3383.1693.47m = 0.1560.42<em>62.33</em>85.65<em> 94.16</em>m = 0.2553.8056.0982.9293.75m = 0.3552.2255.6182.3892.72
https://physionet.org/content/mimiciii/1.4/
https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation
https://pubmed.ncbi.nlm.nih.gov/
https://physionet.org/content/mimiciii/1.4/
https://www.cdc.gov/nchs/icd/icd9cm.htm
https://github.com/Melinda315/WalkLM
https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr
https://github.com/openai/gpt-2
https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation
http://www.freebase.com/
https://paperswithcode.com/dataset/fb15k-237
https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Model compression. Cristian Buciluǎ, Rich Caruana, Alexandru Niculescu-Mizil, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. the 12th ACM SIGKDD international conference on Knowledge discovery and data mining2006</p>
<p>A comprehensive survey of graph embedding: Problems, techniques, and applications. Hongyun Cai, Vincent W Zheng, Kevin Chen, -Chuan Chang, 10.1109/TKDE.2018.2807452IEEE Trans. Knowl. Data Eng. 3092018</p>
<p>Graph-wise common latent factor extraction for unsupervised graph representation learning. Thilini Cooray, Ngai-Man Cheung, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Adaptive graph encoder for attributed graph embedding. Ganqu Cui, Jie Zhou, Cheng Yang, Zhiyuan Liu, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>Convolutional 2d knowledge graph embeddings. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>metapath2vec: Scalable representation learning for heterogeneous networks. Yuxiao Dong, Nitesh V Chawla, Ananthram Swami, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>Pixie: A system for recommending 3+ billion items to 200+ million users in real-time. Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, Jure Leskovec, Proceedings of the 2018 world wide web conference. the 2018 world wide web conference2018</p>
<p>Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning. Tao-Yang Fu, Wang-Chien Lee, Zhen Lei, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge Management2017</p>
<p>Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding. Xinyu Fu, Jiani Zhang, Ziqiao Meng, Irwin King, Proceedings of The Web Conference 2020. The Web Conference 20202020</p>
<p>Random walks and neural network language models on knowledge bases. Josu Goikoetxea, Aitor Soroa, Eneko Agirre, Proceedings of the 2015 conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies. the 2015 conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies2015</p>
<p>node2vec: Scalable feature learning for networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining2016</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730Advances in neural information processing systems</p>
<p>Representation learning on graphs: Methods and applications. William L Hamilton, Rex Ying, Jure Leskovec, IEEE Data Eng. Bull. 4032017</p>
<p>A survey on recent approaches for natural language processing in low-resource scenarios. A Michael, Lukas Hedderich, Heike Lange, Jannik Adel, Dietrich Strötgen, Klakow, 10.18653/v1/2021.naacl-main.201Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2015arXiv preprint</p>
<p>Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2021</p>
<p>Gpt-gnn: Generative pre-training of graph neural networks. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Heterogeneous graph transformer. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun, Proceedings of the web conference 2020. the web conference 20202020</p>
<p>Anonymous walk embeddings. Sergey Ivanov, Evgeny Burnaev, International conference on machine learning. PMLR2018</p>
<p>Pre-training on large-scale heterogeneous graph. Xunqiang Jiang, Tianrui Jia, Yuan Fang, Chuan Shi, Zhe Lin, Hui Wang, Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining. the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining2021</p>
<p>Multi-task self-supervised graph neural networks enable stronger task generalization. Mingxuan Ju, Tong Zhao, Qianlong Wen, Wenhao Yu, Neil Shah, Yanfang Ye, Chuxu Zhang, International Conference on Learning Representations (ICLR). 2023</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, International Conference on Learning Representations. 2017</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBart2020</p>
<p>Graph filter-based multi-view attributed graph clustering. Zhiping Lin, Zhao Kang, IJCAI. 2021</p>
<p>Self-supervised consensus representation learning for attributed graph. Changshu Liu, Liangjian Wen, Zhao Kang, Guangchun Luo, Ling Tian, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on Multimedia2021</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, arXiv:2110.076022021arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Hinormer: Representation learning on heterogeneous information networks with graph transformer. Qiheng Mao, Zemin Liu, Chenghao Liu, Jianling Sun, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Reconstructing markov processes from independent and anonymous experiments. Silvio Micali, Zeyuan Allen, Zhu , Discrete Applied Mathematics. 2002016</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran, Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, Dan Roth, arXiv:2111.012432021arXiv preprint</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.083022023arXiv preprint</p>
<p>Deepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>Pre-trained models for natural language processing: A survey. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, Science China Technological Sciences. 63102020</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>A primer on contrastive pretraining in language processing: Methods, lessons learned, and perspectives. Nils Rethmeier, Isabelle Augenstein, ACM Computing Surveys. 55102023</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.011082019arXiv preprint</p>
<p>Modeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, The Semantic Web: 15th International Conference. Heraklion, Crete, GreeceSpringer2018. June 3-7, 2018. 201815</p>
<p>Exploiting structured knowledge in text via graph-guided representation learning. Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, Weizhu Chen, arXiv:2004.142242020arXiv preprint</p>
<p>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, Xin Wang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>S2gae: Self-supervised graph autoencoders are generalizable learners with graph masking. Qiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, Xia Hu, Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. the Sixteenth ACM International Conference on Web Search and Data Mining2023</p>
<p>Complex embeddings for simple link prediction. Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, Guillaume Bouchard, International conference on machine learning. PMLR2016</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Haoyu Wang, Vivek Kulkarni, William Yang, Wang , arXiv:1811.00147Dolores: deep contextualized knowledge graph embeddings. 2018arXiv preprint</p>
<p>Simkgc: Simple contrastive knowledge graph completion with pre-trained language models. Liang Wang, Wei Zhao, Zhuoyu Wei, Jingming Liu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Heterogeneous graph attention network. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, Philip S Yu, The world wide web conference. 2019</p>
<p>Self-supervised heterogeneous graph neural network with co-contrastive learning. Xiao Wang, Nian Liu, Hui Han, Chuan Shi, Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining. the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining2021</p>
<p>A survey on heterogeneous graph embedding: Methods, techniques, applications and sources. Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, Philip S Yu, 10.1109/TBDATA.2022.3177455IEEE Trans. Big Data. 922023</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Climatebert: A pretrained language model for climate-related text. Nicolas Webersinke, Mathias Kraus, Julia Bingler, Markus Leippold, Available at SSRN. 2022</p>
<p>A comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S Yu, 10.1109/TNNLS.2020.2978386IEEE Trans. Neural Networks Learn. Syst. 3212021</p>
<p>Taxoprompt: A prompt-based generation method with taxonomic context for self-supervised taxonomy expansion. Hongyuan Xu, Yunong Chen, Zichen Liu, Yanlong Wen, Xiaojie Yuan, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-222022</p>
<p>Clinical-bert: Vision-language pre-training for radiograph diagnosis and reports generation. Bin Yan, Mingtao Pei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Heterogeneous network representation learning: A unified framework with survey and benchmark. Carl Yang, Yuxin Xiao, Yu Zhang, Yizhou Sun, Jiawei Han, IEEE Transactions on Knowledge and Data Engineering. 34102020</p>
<p>Relation learning on social networks with multi-modal graph edge variational autoencoders. Carl Yang, Jieyu Zhang, Haonan Wang, Sha Li, Myungwan Kim, Matt Walker, Yiou Xiao, Jiawei Han, Proceedings of the 13th International Conference on Web Search and Data Mining. the 13th International Conference on Web Search and Data Mining2020</p>
<p>Self-supervised heterogeneous graph pre-training based on structural clustering. Yaming Yang, Ziyu Guan, Zhe Wang, Wei Zhao, Cai Xu, Weigang Lu, Jianbin Huang, Conference on Neural Information Processing Systems. 2022</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in neural information processing systems. 322019</p>
<p>Kg-bert: Bert for knowledge graph completion. Liang Yao, Chengsheng Mao, Yuan Luo, arXiv:1909.031932019arXiv preprint</p>
<p>Exploring large language models for knowledge graph completion. Liang Yao, Jiazhen Peng, Chengsheng Mao, Yuan Luo, arXiv:2308.139162023arXiv preprint</p>
<p>Deep bidirectional language-knowledge graph pretraining. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, Jure Leskovec, Advances in Neural Information Processing Systems. 202235</p>
<p>Differentially private fine-tuning of language models. Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Gautam Huseyin A Inan, Janardhan Kamath, Yin Tat Kulkarni, Andre Lee, Lukas Manoel, Wutschitz, International Conference on Learning Representations. 2022</p>
<p>Jaket: Joint pre-training of knowledge graph and language understanding. Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 201831</p>
<p>An end-to-end deep learning architecture for graph classification. Muhan Zhang, Zhicheng Cui, Marion Neumann, Yixin Chen, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>A simple meta-path-free framework for heterogeneous network embedding. Rui Zhang, Arthur Zimek, Peter Schneider-Kamp, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge Management2022</p>
<p>Structure pretraining and prompt tuning for knowledge graph transfer. Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing Xu, Wenting Song, Huajun Chen, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun, Graph neural networks: A review of methods and applications. AI open. 20201</p>
<p>Transfer learning of graph neural networks with ego-graph information maximization. Qi Zhu, Carl Yang, Yidan Xu, Haonan Wang, Chao Zhang, Jiawei Han, Advances in Neural Information Processing Systems. 202134</p>
<p>Graph contrastive learning with adaptive augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Proceedings of the Web Conference 2021. the Web Conference 20212021</p>
<p>Causal language model aided sequential decoding with natural redundancy. Zhaorui Zhu, Hongyi Yu, Caiyao Shen, Jianping Du, Zhixiang Shen, Zhenyu Wang, IEEE Transactions on Communications. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
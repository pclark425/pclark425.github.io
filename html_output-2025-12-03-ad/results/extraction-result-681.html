<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-681 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-681</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-681</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-085b360d3c08aaf997f45a78e27f2629f5625205</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/085b360d3c08aaf997f45a78e27f2629f5625205" target="_blank">Translation Artifacts in Cross-lingual Transfer Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is shown that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon, and the state-of-the-art in XNLI for the translate-test and zero-shot approaches is improved.</p>
                <p><strong>Paper Abstract:</strong> Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e681.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e681.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Train/Test Type Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between training data type (original vs translated) and test data type in cross-lingual pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies a systematic discrepancy where models are trained on original (non-translated) English data but evaluated on translated test sets (human- or machine-translated), producing large performance differences that are not due to cross-lingual generalization but to differences in data provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>cross-lingual experimental pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental setup for cross-lingual transfer (Translate-Test, Translate-Train, Zero-Shot) involving training on English data (original / back-translated / machine-translated) and evaluating on test sets that are original, human translated, or machine translated.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset description and evaluation protocol in benchmark papers (e.g., XNLI description)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data preparation and evaluation pipeline (translation/back-translation scripts, training/evaluation scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>data provenance mismatch (training/test data-type mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Benchmarks and common experimental descriptions treat per-language accuracy differences as a measure of cross-lingual generalization, but in practice train/test pipelines mix original, human-translated and machine-translated text. This creates a gap between the conceptual description (train on English, test on target language) and the effective data distribution seen by the model (original vs translated), causing the measured cross-lingual gap to conflate language transfer with original-vs-translated generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / dataset creation / evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>controlled experiments comparing models trained on original English, back-translated English, and machine-translated training sets and evaluated on original, human-translated, and machine-translated test sets (ablation-style comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of task accuracy across variants (XNLI dev/test accuracy deltas); reported numeric differences (e.g., average improvements when training on back-translated data vs original), plus class-distribution analyses and stress tests</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial — training on back-translated or machine-translated data yields large gains when evaluating on translated test sets (Translate-Test: up to ~4.6 points avg; paper reports improving XNLI state-of-the-art for Translate-Test by 4.3 points and Zero-Shot by 2.8 points). Also, bias-correction (unbiasing class logits) improved ORIG Translate-Test Roberta by +2.9 points, indicating evaluation was hindered by the mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in multilingual benchmarks: the paper notes most existing multilingual datasets (e.g., XNLI, PAWS-X, XQuAD, MLQA) are created through translation, so this mismatch is pervasive across commonly used cross-lingual evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implicit experimental assumptions and dataset-creation practices: translations (human or machine) change surface statistics; experiments and benchmark descriptions often omit or under-specify the implications of mixing original and translated data.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Create consistency between train and test data types (e.g., use back-translation to make training data resemble translated test sets), adjust classifier biases to match test class distributions (unbiasing), or design benchmarks with original annotations in all languages or consistent document-level translations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: back-translation (BT-XX) and machine-translating training data (MT-XX) consistently outperform Orig on translated test sets (best-case avg improvement ~4.6 points); unbiasing yielded +2.9 points for Roberta (Translate-Test, ORIG) but had smaller effects for BT/MT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / cross-lingual transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Translation Artifacts in Cross-lingual Transfer Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e681.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e681.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Independent-Component Translation Artifact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Artifacts caused by independently translating components of instances (e.g., premise and hypothesis translated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Translating different parts of a single example independently (such as premise and hypothesis in NLI) alters superficial patterns (e.g., lexical overlap) that models exploit, causing models trained on original data to behave differently on translated test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>dataset translation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The data-generation step that translates textual components (sentences, premises/hypotheses, question/context) either independently or with document-level context using human or machine translation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset creation description / annotation protocol (how translation was done — sentence-level vs document-level)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>translation scripts and data-parallelization procedures (machine-translation / back-translation pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / inconsistent translation procedure</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many datasets (e.g., XNLI) translate premise and hypothesis independently; natural-language descriptions or uses of these datasets often treat translated data as equivalent to original data, but independent translation reduces lexical overlap and other surface cues that models rely on, producing a mismatch between assumed data properties and actual data properties.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / dataset creation (translation granularity)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>behavioral analyses: stress tests targeted at phenomena such as lexical overlap and negation, output class distribution comparisons, and controlled training variants (BT/MT) that simulate independent translations</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>stress-test performance (competence vs distraction suites), output class distribution shifts (underprediction of entailment when overlap reduced), and task accuracy differences across training variants and test types; qualitative and quantitative comparisons reported (e.g., BT/MT models are less reliant on lexical overlap according to distraction test results).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Changes model behavior and evaluation: BT-FI and MT-FI models are less biased by lexical overlap and resist certain shallow heuristics, leading to better performance on translated/distraction datasets but sometimes worse on competence tests; this shifts measured accuracy and can both help or hurt depending on task and translation procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Relevant when datasets use sentence-level translation; XNLI used independent sentence translation for premises and hypotheses, while some QA datasets used document-level translation (so effect less pronounced there).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Translation inconsistency due to lack of context during translation (sentence-level translation) and differing translation procedures (sentence vs document level); implicit assumption that translations preserve dataset surface statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use document-level translation or provide translators with associated context when translating related components, or mirror translation procedure in training data (back-translation or machine-translating training data) so the model sees similar inconsistencies during training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in practice: BT-XX and MT-XX reduce the susceptibility to shallow patterns and improve performance on translated/distraction evaluations; however, effectiveness depends on task and how translations were created (e.g., QA less affected when contexts/questions were translated together).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / dataset creation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Translation Artifacts in Cross-lingual Transfer Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e681.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e681.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Misattributed Mechanism (XLDA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misattribution of improvements to data augmentation (as in XLDA) versus translation artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper challenges prior claims (Singh et al., XLDA) that replacing segments with translations helps via data augmentation, showing instead that the gains can arise from altering translation-induced dataset artifacts rather than simple augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>XLDA: Cross-lingual data augmentation for natural language inference and question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>cross-lingual data augmentation technique</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Method that replaces parts of training examples with translations in different languages to increase apparent training diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>prior paper's claimed mechanism in method description (data augmentation explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data augmentation pipeline (mixing translated segments per-example)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>interpretation mismatch / incorrect causal attribution</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior natural-language explanation attributed performance gains to increased data diversity (augmentation). The current paper shows that identical-size training sets (no new unique sentences) that differ only in translation provenance still change evaluation results, indicating the true mechanism is dataset artifact modification (reduction of spurious surface patterns) rather than classic augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>method rationale / interpretation of empirical improvements</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>controlled experimental design that prevents data augmentation effects (keeps same unique sentences) while varying translation provenance (back-translation, MT), and comparing results to show improvements persist without augmenting unique sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of accuracy across controlled training variants where number of unique sentences is held constant; observation that BT/MT variants outperform ORIG only on translated test sets indicates artifact-driven effect.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reinterprets prior results: methods previously credited to augmentation should be analyzed as altering dataset artifacts; this changes how one would design or trust such methods and has implications for conclusions in earlier literature.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Potentially common wherever multi-lingual augmentation was used and not analyzed for translation-provenance effects; the paper specifically calls out XLDA as an example.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Conflation of observed performance gains with the wrong causal explanation in textual descriptions; insufficient experimental controls in prior work to rule out alternative explanations (translation artifacts).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Run controlled experiments that isolate augmentation effects from translation-provenance effects (e.g., keep unique sentence count constant), analyze performance separately on original vs translated test sets, and explicitly test artifact hypotheses (stress tests / distribution matching).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Clarifying experiments are effective at revealing the true mechanism; the paper's own experiments show that controlling for augmentation still reveals translation-artifact effects and explain prior gains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / empirical methods</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Translation Artifacts in Cross-lingual Transfer Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e681.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e681.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset Documentation Ambiguity (XNLI labels)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguity in dataset documentation vs actual dataset labeling procedure (XNLI additional labels and gold standard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that XNLI's documentation is unclear about how additional labels were collected and used; by inspecting the dataset the authors found the original English label was retained as gold standard, making the extra labels irrelevant for evaluation, a discrepancy between description and practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>XNLI: Evaluating cross-lingual sentence representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>multilingual benchmark dataset (XNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>XNLI is a parallel multilingual NLI development/test set created by professional translation of English-annotated premises and hypotheses into multiple languages; it contains multiple labels per example but retains the original English label as gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset documentation / paper description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>dataset labeling and packaging (annotation metadata and evaluation script)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>documentation vs dataset content mismatch / ambiguous specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Conneau et al.'s description of XNLI does not make explicit that the original English label is kept as gold standard despite collecting additional labels; this discrepancy can mislead users about whether the dataset uses consensus/multi-label gold standards or the original label.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset documentation / evaluation specification</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual inspection of the released dataset and evaluation protocol (authors verified by inspecting the dataset files and evaluation behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative confirmation (checking label fields and evaluation protocol); no numeric metric required beyond noting irrelevance of additional labels for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate: may lead to misunderstandings about evaluation (e.g., whether multilingual annotations provide alternate gold labels); does not itself change accuracy numbers but affects correct interpretation of evaluation setup.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Specific to XNLI as discussed; similar documentation ambiguities may exist in other datasets but paper only documents XNLI's case.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incomplete or ambiguous dataset description in the original dataset paper, and insufficient clarity about how multiple annotations are aggregated or used in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Recommend clearer dataset documentation that states which label is used as gold standard and how multiple annotations are handled; users should inspect dataset release artifacts rather than rely solely on paper prose.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Clarification is straightforward and effective; the authors validated the dataset behavior by inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>dataset curation / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Translation Artifacts in Cross-lingual Transfer Learning', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>XLDA: Cross-lingual data augmentation for natural language inference and question answering <em>(Rating: 2)</em></li>
                <li>XNLI: Evaluating cross-lingual sentence representations <em>(Rating: 2)</em></li>
                <li>Annotation artifacts in natural language inference data <em>(Rating: 2)</em></li>
                <li>Understanding back-translation at scale <em>(Rating: 2)</em></li>
                <li>On the features of translationese <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-681",
    "paper_id": "paper-085b360d3c08aaf997f45a78e27f2629f5625205",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Train/Test Type Mismatch",
            "name_full": "Mismatch between training data type (original vs translated) and test data type in cross-lingual pipelines",
            "brief_description": "The paper identifies a systematic discrepancy where models are trained on original (non-translated) English data but evaluated on translated test sets (human- or machine-translated), producing large performance differences that are not due to cross-lingual generalization but to differences in data provenance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "cross-lingual experimental pipeline",
            "system_description": "Experimental setup for cross-lingual transfer (Translate-Test, Translate-Train, Zero-Shot) involving training on English data (original / back-translated / machine-translated) and evaluating on test sets that are original, human translated, or machine translated.",
            "nl_description_type": "dataset description and evaluation protocol in benchmark papers (e.g., XNLI description)",
            "code_implementation_type": "data preparation and evaluation pipeline (translation/back-translation scripts, training/evaluation scripts)",
            "gap_type": "data provenance mismatch (training/test data-type mismatch)",
            "gap_description": "Benchmarks and common experimental descriptions treat per-language accuracy differences as a measure of cross-lingual generalization, but in practice train/test pipelines mix original, human-translated and machine-translated text. This creates a gap between the conceptual description (train on English, test on target language) and the effective data distribution seen by the model (original vs translated), causing the measured cross-lingual gap to conflate language transfer with original-vs-translated generalization.",
            "gap_location": "data preprocessing / dataset creation / evaluation protocol",
            "detection_method": "controlled experiments comparing models trained on original English, back-translated English, and machine-translated training sets and evaluated on original, human-translated, and machine-translated test sets (ablation-style comparison)",
            "measurement_method": "comparison of task accuracy across variants (XNLI dev/test accuracy deltas); reported numeric differences (e.g., average improvements when training on back-translated data vs original), plus class-distribution analyses and stress tests",
            "impact_on_results": "Substantial — training on back-translated or machine-translated data yields large gains when evaluating on translated test sets (Translate-Test: up to ~4.6 points avg; paper reports improving XNLI state-of-the-art for Translate-Test by 4.3 points and Zero-Shot by 2.8 points). Also, bias-correction (unbiasing class logits) improved ORIG Translate-Test Roberta by +2.9 points, indicating evaluation was hindered by the mismatch.",
            "frequency_or_prevalence": "High in multilingual benchmarks: the paper notes most existing multilingual datasets (e.g., XNLI, PAWS-X, XQuAD, MLQA) are created through translation, so this mismatch is pervasive across commonly used cross-lingual evaluations.",
            "root_cause": "Implicit experimental assumptions and dataset-creation practices: translations (human or machine) change surface statistics; experiments and benchmark descriptions often omit or under-specify the implications of mixing original and translated data.",
            "mitigation_approach": "Create consistency between train and test data types (e.g., use back-translation to make training data resemble translated test sets), adjust classifier biases to match test class distributions (unbiasing), or design benchmarks with original annotations in all languages or consistent document-level translations.",
            "mitigation_effectiveness": "Effective: back-translation (BT-XX) and machine-translating training data (MT-XX) consistently outperform Orig on translated test sets (best-case avg improvement ~4.6 points); unbiasing yielded +2.9 points for Roberta (Translate-Test, ORIG) but had smaller effects for BT/MT variants.",
            "domain_or_field": "natural language processing / cross-lingual transfer learning",
            "reproducibility_impact": true,
            "uuid": "e681.0",
            "source_info": {
                "paper_title": "Translation Artifacts in Cross-lingual Transfer Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Independent-Component Translation Artifact",
            "name_full": "Artifacts caused by independently translating components of instances (e.g., premise and hypothesis translated separately)",
            "brief_description": "Translating different parts of a single example independently (such as premise and hypothesis in NLI) alters superficial patterns (e.g., lexical overlap) that models exploit, causing models trained on original data to behave differently on translated test sets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "dataset translation pipeline",
            "system_description": "The data-generation step that translates textual components (sentences, premises/hypotheses, question/context) either independently or with document-level context using human or machine translation.",
            "nl_description_type": "dataset creation description / annotation protocol (how translation was done — sentence-level vs document-level)",
            "code_implementation_type": "translation scripts and data-parallelization procedures (machine-translation / back-translation pipelines)",
            "gap_type": "incomplete specification / inconsistent translation procedure",
            "gap_description": "Many datasets (e.g., XNLI) translate premise and hypothesis independently; natural-language descriptions or uses of these datasets often treat translated data as equivalent to original data, but independent translation reduces lexical overlap and other surface cues that models rely on, producing a mismatch between assumed data properties and actual data properties.",
            "gap_location": "data preprocessing / dataset creation (translation granularity)",
            "detection_method": "behavioral analyses: stress tests targeted at phenomena such as lexical overlap and negation, output class distribution comparisons, and controlled training variants (BT/MT) that simulate independent translations",
            "measurement_method": "stress-test performance (competence vs distraction suites), output class distribution shifts (underprediction of entailment when overlap reduced), and task accuracy differences across training variants and test types; qualitative and quantitative comparisons reported (e.g., BT/MT models are less reliant on lexical overlap according to distraction test results).",
            "impact_on_results": "Changes model behavior and evaluation: BT-FI and MT-FI models are less biased by lexical overlap and resist certain shallow heuristics, leading to better performance on translated/distraction datasets but sometimes worse on competence tests; this shifts measured accuracy and can both help or hurt depending on task and translation procedure.",
            "frequency_or_prevalence": "Relevant when datasets use sentence-level translation; XNLI used independent sentence translation for premises and hypotheses, while some QA datasets used document-level translation (so effect less pronounced there).",
            "root_cause": "Translation inconsistency due to lack of context during translation (sentence-level translation) and differing translation procedures (sentence vs document level); implicit assumption that translations preserve dataset surface statistics.",
            "mitigation_approach": "Use document-level translation or provide translators with associated context when translating related components, or mirror translation procedure in training data (back-translation or machine-translating training data) so the model sees similar inconsistencies during training.",
            "mitigation_effectiveness": "Effective in practice: BT-XX and MT-XX reduce the susceptibility to shallow patterns and improve performance on translated/distraction evaluations; however, effectiveness depends on task and how translations were created (e.g., QA less affected when contexts/questions were translated together).",
            "domain_or_field": "natural language processing / dataset creation",
            "reproducibility_impact": true,
            "uuid": "e681.1",
            "source_info": {
                "paper_title": "Translation Artifacts in Cross-lingual Transfer Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Misattributed Mechanism (XLDA)",
            "name_full": "Misattribution of improvements to data augmentation (as in XLDA) versus translation artifacts",
            "brief_description": "The paper challenges prior claims (Singh et al., XLDA) that replacing segments with translations helps via data augmentation, showing instead that the gains can arise from altering translation-induced dataset artifacts rather than simple augmentation.",
            "citation_title": "XLDA: Cross-lingual data augmentation for natural language inference and question answering",
            "mention_or_use": "mention",
            "system_name": "cross-lingual data augmentation technique",
            "system_description": "Method that replaces parts of training examples with translations in different languages to increase apparent training diversity.",
            "nl_description_type": "prior paper's claimed mechanism in method description (data augmentation explanation)",
            "code_implementation_type": "data augmentation pipeline (mixing translated segments per-example)",
            "gap_type": "interpretation mismatch / incorrect causal attribution",
            "gap_description": "Prior natural-language explanation attributed performance gains to increased data diversity (augmentation). The current paper shows that identical-size training sets (no new unique sentences) that differ only in translation provenance still change evaluation results, indicating the true mechanism is dataset artifact modification (reduction of spurious surface patterns) rather than classic augmentation.",
            "gap_location": "method rationale / interpretation of empirical improvements",
            "detection_method": "controlled experimental design that prevents data augmentation effects (keeps same unique sentences) while varying translation provenance (back-translation, MT), and comparing results to show improvements persist without augmenting unique sentences.",
            "measurement_method": "comparison of accuracy across controlled training variants where number of unique sentences is held constant; observation that BT/MT variants outperform ORIG only on translated test sets indicates artifact-driven effect.",
            "impact_on_results": "Reinterprets prior results: methods previously credited to augmentation should be analyzed as altering dataset artifacts; this changes how one would design or trust such methods and has implications for conclusions in earlier literature.",
            "frequency_or_prevalence": "Potentially common wherever multi-lingual augmentation was used and not analyzed for translation-provenance effects; the paper specifically calls out XLDA as an example.",
            "root_cause": "Conflation of observed performance gains with the wrong causal explanation in textual descriptions; insufficient experimental controls in prior work to rule out alternative explanations (translation artifacts).",
            "mitigation_approach": "Run controlled experiments that isolate augmentation effects from translation-provenance effects (e.g., keep unique sentence count constant), analyze performance separately on original vs translated test sets, and explicitly test artifact hypotheses (stress tests / distribution matching).",
            "mitigation_effectiveness": "Clarifying experiments are effective at revealing the true mechanism; the paper's own experiments show that controlling for augmentation still reveals translation-artifact effects and explain prior gains.",
            "domain_or_field": "natural language processing / empirical methods",
            "reproducibility_impact": true,
            "uuid": "e681.2",
            "source_info": {
                "paper_title": "Translation Artifacts in Cross-lingual Transfer Learning",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Dataset Documentation Ambiguity (XNLI labels)",
            "name_full": "Ambiguity in dataset documentation vs actual dataset labeling procedure (XNLI additional labels and gold standard)",
            "brief_description": "The paper notes that XNLI's documentation is unclear about how additional labels were collected and used; by inspecting the dataset the authors found the original English label was retained as gold standard, making the extra labels irrelevant for evaluation, a discrepancy between description and practice.",
            "citation_title": "XNLI: Evaluating cross-lingual sentence representations",
            "mention_or_use": "mention",
            "system_name": "multilingual benchmark dataset (XNLI)",
            "system_description": "XNLI is a parallel multilingual NLI development/test set created by professional translation of English-annotated premises and hypotheses into multiple languages; it contains multiple labels per example but retains the original English label as gold standard.",
            "nl_description_type": "dataset documentation / paper description",
            "code_implementation_type": "dataset labeling and packaging (annotation metadata and evaluation script)",
            "gap_type": "documentation vs dataset content mismatch / ambiguous specification",
            "gap_description": "Conneau et al.'s description of XNLI does not make explicit that the original English label is kept as gold standard despite collecting additional labels; this discrepancy can mislead users about whether the dataset uses consensus/multi-label gold standards or the original label.",
            "gap_location": "dataset documentation / evaluation specification",
            "detection_method": "manual inspection of the released dataset and evaluation protocol (authors verified by inspecting the dataset files and evaluation behavior).",
            "measurement_method": "qualitative confirmation (checking label fields and evaluation protocol); no numeric metric required beyond noting irrelevance of additional labels for evaluation.",
            "impact_on_results": "Moderate: may lead to misunderstandings about evaluation (e.g., whether multilingual annotations provide alternate gold labels); does not itself change accuracy numbers but affects correct interpretation of evaluation setup.",
            "frequency_or_prevalence": "Specific to XNLI as discussed; similar documentation ambiguities may exist in other datasets but paper only documents XNLI's case.",
            "root_cause": "Incomplete or ambiguous dataset description in the original dataset paper, and insufficient clarity about how multiple annotations are aggregated or used in evaluation.",
            "mitigation_approach": "Recommend clearer dataset documentation that states which label is used as gold standard and how multiple annotations are handled; users should inspect dataset release artifacts rather than rely solely on paper prose.",
            "mitigation_effectiveness": "Clarification is straightforward and effective; the authors validated the dataset behavior by inspection.",
            "domain_or_field": "dataset curation / NLP evaluation",
            "reproducibility_impact": true,
            "uuid": "e681.3",
            "source_info": {
                "paper_title": "Translation Artifacts in Cross-lingual Transfer Learning",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "XLDA: Cross-lingual data augmentation for natural language inference and question answering",
            "rating": 2
        },
        {
            "paper_title": "XNLI: Evaluating cross-lingual sentence representations",
            "rating": 2
        },
        {
            "paper_title": "Annotation artifacts in natural language inference data",
            "rating": 2
        },
        {
            "paper_title": "Understanding back-translation at scale",
            "rating": 2
        },
        {
            "paper_title": "On the features of translationese",
            "rating": 1
        }
    ],
    "cost": 0.014499999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Translation Artifacts in Cross-lingual Transfer Learning</h1>
<p>Mikel Artetxe, Gorka Labaka, Eneko Agirre<br>HiTZ Center<br>University of the Basque Country (UPV/EHU)<br>{mikel.artetxe, gorka.labaka,e.agirre}@ehu.eus</p>
<h4>Abstract</h4>
<p>Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.</p>
<h2>1 Introduction</h2>
<p>While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018; Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training.</p>
<p>Closely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (Translate-Test), machine translating the training set into each target language and training the models on their respective languages (Translate-Train), or using English data to finetune a multilingual model that is then transferred to the rest of languages (Zero-Shot).</p>
<p>The dataset creation and transfer procedures described above result in a mixture of original, ${ }^{1}$ human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: Translate-Test systems are trained on original data and evaluated on machine translated test sets, Zero-Shot systems are trained on original data and evaluated on human translated test sets, and Translate-Train systems are trained on machine translated data and evaluated on human translated test sets.</p>
<p>Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the Translate-Test and Zero-Shot approaches in cross-lingual Natural Language Inference (NLI). While improvements brought by machine translation have previously been attributed to data augmentation (Singh et al., 2019), we reject this hypothesis and show that the phenomenon is only present in translated test sets, but not in original ones. Instead, our analysis reveals that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>this behavior is caused by subtle artifacts arising from the translation process itself. In particular, we show that translating different parts of each instance separately (e.g., the premise and the hypothesis in NLI) can alter superficial patterns in the data (e.g., the degree of lexical overlap between them), which severely affects the generalization ability of current models. Based on the gained insights, we improve the state-of-the-art in XNLI, and show that some previous findings need to be reconsidered in the light of this phenomenon.</p>
<h2>2 Related work</h2>
<p>Cross-lingual transfer learning. Current crosslingual models work by pre-training multilingual representations using some form of language modeling, which are then fine-tuned on the relevant task and transferred to different languages. Some authors leverage parallel data to that end (Conneau and Lample, 2019; Huang et al., 2019), but training a model akin to BERT (Devlin et al., 2019) on the combination of monolingual corpora in multiple languages is also effective (Conneau et al., 2020). Closely related to our work, Singh et al. (2019) showed that replacing segments of the training data with their translation during fine-tuning is helpful. However, they attribute this behavior to a data augmentation effect, which we believe should be reconsidered given the new evidence we provide.</p>
<p>Multilingual benchmarks. Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 2019) for adversarial paraphrase identification, and XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it.</p>
<p>Annotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain
strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, McCoy et al. (2019) showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. Several authors have worked on adversarial datasets to diagnose these issues and provide a more challenging benchmark (Naik et al., 2018; Glockner et al., 2018; Nie et al., 2020). Besides NLI, other tasks like QA have also been found to be susceptible to annotation artifacts (Jia and Liang, 2017; Kaushik and Lipton, 2018). While previous work has focused on the monolingual scenario, we show that translation can interfere with these artifacts in multilingual settings.</p>
<p>Translationese. Translated texts are known to have unique features like simplification, explicitation, normalization and interference, which are refer to as translationese (Volansky et al., 2013). This phenomenon has been reported to have a notable impact in machine translation evaluation (Zhang and Toral, 2019; Graham et al., 2019). For instance, back-translation brings large BLEU gains for reversed test sets (i.e., when translationese is on the source side and original text is used as reference), but its effect diminishes in the natural direction (Edunov et al., 2020). While connected, the phenomenon we analyze is different in that it arises from translation inconsistencies due to the lack of context, and affects cross-lingual transfer learning rather than machine translation.</p>
<h2>3 Experimental design</h2>
<p>Our goal is to analyze the effect of both human and machine translation in cross-lingual models. For that purpose, the core idea of our work is to (i) use machine translation to either translate the training set into other languages, or generate English paraphrases of it through back-translation, and (ii) evaluate the resulting systems on original, human translated and machine translated test sets in comparison with systems trained on original data. We next describe the models used in our experiments (§3.1), the specific training variants explored (§3.2), and the evaluation procedure followed (§3.3).</p>
<h3>3.1 Models and transfer methods</h3>
<p>We experiment with two models that are representative of the state-of-the-art in monolingual and</p>
<p>cross-lingual pre-training: (i) Roberta (Liu et al., 2019), which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R (Conneau et al., 2020), which is a multilingual extension of the former pre-trained on 100 languages. In both cases, we use the large models released by the authors under the fairseq repository. ${ }^{2}$ As discussed next, we explore different variants of the training set to fine-tune each model on different tasks. At test time, we try both machine translating the test set into English (Translate-Test) and, in the case of XLM-R, using the actual test set in the target language (Zero-Shot).</p>
<h3>3.2 Training variants</h3>
<p>We try 3 variants of each training set to fine-tune our models: (i) the original one in English (Orig), (ii) an English paraphrase of it generated through back-translation using Spanish or Finnish as pivot (BT-ES and BT-FI), and (iii) a machine translated version in Spanish or Finnish (MT-ES and MT-FI). For sentences occurring multiple times in the training set (e.g., premises repeated for multiple hypotheses), we use the exact same translation for all occurrences, as our goal is to understand the inherent effect of translation rather than its potential application as a data augmentation method.</p>
<p>In order to train the machine translation systems for MT-XX and BT-XX, we use the big Transformer model (Vaswani et al., 2017) with the same settings as Ott et al. (2018) and SentencePiece tokenization (Kudo and Richardson, 2018) with a joint vocabulary of 32 k subwords. For English-Spanish, we train for 10 epochs on all parallel data from WMT 2013 (Bojar et al., 2013) and ParaCrawl v5.0 (Esplà et al., 2019). For English-Finnish, we train for 40 epochs on Europarl and Wiki Titles from WMT 2019 (Barrault et al., 2019), ParaCrawl v5.0, and DGT, EUbookshop and TildeMODEL from OPUS (Tiedemann, 2012). In both cases, we remove sentences longer than 250 tokens, with a source/target ratio exceeding 1.5 , or for which langid.py (Lui and Baldwin, 2012) predicts a different language, resulting in a final corpus size of 48 M and 7 M sentence pairs, respectively. We use sampling decoding with a temperature of 0.5 for inference, which produces more diverse translations than beam search (Edunov et al., 2018) and performed better in our preliminary experiments.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.3 Tasks and evaluation procedure</h3>
<p>We use the following tasks for our experiments:
Natural Language Inference (NLI). Given a premise and a hypothesis, the task is to determine whether there is an entailment, neutral or contradiction relation between them. We fine-tune our models on MultiNLI (Williams et al., 2018) for 10 epochs using the same settings as Liu et al. (2019). In most of our experiments, we evaluate on XNLI (Conneau et al., 2018), which comprises 2490 development and 5010 test instances in 15 languages. These were originally annotated in English, and the resulting premises and hypotheses were independently translated into the rest of the languages by professional translators. For the Translate-Test approach, we use the machine translated versions from the authors. Following Conneau et al. (2020), we select the best epoch checkpoint according to the average accuracy in the development set.</p>
<p>Question Answering (QA). Given a context paragraph and a question, the task is to identify the span answering the question in the context. We fine-tune our models on SQuAD v1.1 (Rajpurkar et al., 2016) for 2 epochs using the same settings as Liu et al. (2019), and report test results for the last epoch. We use two datasets for evaluation: XQuAD (Artetxe et al., 2020), a subset of the SQuAD development set translated into 10 other languages, and MLQA (Lewis et al., 2020) a dataset consisting of parallel context paragraphs plus the corresponding questions annotated in English and translated into 6 other languages. In both cases, the translation was done by professional translators at the document level (i.e., when translating a question, the text answering it was also shown). For our BT-XX and MT-XX variants, we translate the context paragraph and the questions independently, and map the answer spans using the same procedure as Carrino et al. (2020). ${ }^{3}$ For the Translate-Test approach, we use the official machine translated versions of MLQA, run inference over them, and map the predicted answer spans back to the target language. ${ }^{4}$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">fr</th>
<th style="text-align: center;">es</th>
<th style="text-align: center;">de</th>
<th style="text-align: center;">el</th>
<th style="text-align: center;">bg</th>
<th style="text-align: center;">ru</th>
<th style="text-align: center;">tr</th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">vi</th>
<th style="text-align: center;">th</th>
<th style="text-align: center;">zh</th>
<th style="text-align: center;">hi</th>
<th style="text-align: center;">sw</th>
<th style="text-align: center;">ur</th>
<th style="text-align: center;">avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test set machine translated into English (TRANSLATE-TEST)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Roberta</td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">77.7 a0.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">81.7 a0.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">82.3 a0.2</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R</td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">77.9 a0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">80.8 a0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">81.3 a0.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-ES</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">80.0 a0.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-FI</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">80.5 a0.3</td>
</tr>
<tr>
<td style="text-align: center;">Test set in target language (ZERO-SHOT)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">XLM-R</td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">81.0 a0.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">83.1 a0.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">83.2 a0.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-ES</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">82.9 a0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-FI</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">83.4 a0.2</td>
</tr>
</tbody>
</table>
<p>Table 1: XNLI dev results (acc). BT-XX and MT-XX consistently outperform ORIG in all cases.</p>
<p>Both for NLI and QA, we run each system 5 times with different random seeds and report the average results. Space permitting, we also report the standard deviation across the 5 runs. In our result tables, we use an underline to highlight the best result within each block, and boldface to highlight the best overall result.</p>
<h2>4 NLI experiments</h2>
<p>We next discuss our main results in the XNLI development set ( $\S 4.1, \S 4.2$ ), run additional experiments to better understand the behavior of our different variants ( $\S 4.3, \S 4.4, \S 4.5$ ), and compare our results to previous work in the XNLI test set (§4.6).</p>
<h3>4.1 Translate-Test results</h3>
<p>We start by analyzing XNLI development results for Translate-Test. Recall that, in this approach, the test set is machine translated into English, but training is typically done on original English data. Our BT-ES and BT-FI variants close this gap by training on a machine translated English version of the training set generated through back-translation. As shown in Table 1, this brings substantial gains for both Roberta and XLM-R, with an average improvement of 4.6 points in the best case. Quite remarkably, MT-ES and MT-FI also outperform ORIG by a substantial margin, and are only 0.8 points below their BT-ES and BT-FI counterparts. Recall that, for these two systems, training is done in machine translated Spanish or Finnish, while inference is done in machine translated English. This shows that the loss of performance when generalizing
from original data to machine translated data is substantially larger than the loss of performance when generalizing from one language to another.</p>
<h3>4.2 Zero-Shot results</h3>
<p>We next analyze the results for the Zero-Shot approach. In this case, inference is done in the test set in each target language which, in the case of XNLI, was human translated from English. As such, different from the Translate-Test approach, neither training on original data (Orig) nor training on machine translated data (BT-XX and MT-XX) makes use of the exact same type of text that the system is exposed to at test time. However, as shown in Table 1, both BT-XX and MT-XX outperform Orig by approximately 2 points, which suggests that our (back-)translated versions of the training set are more similar to the human translated test sets than the original one. This also provides a new perspective on the Translate-Train approach, which was reported to outperform Orig in previous work (Conneau and Lample, 2019): while the original motivation was to train the model on the same language that it is tested on, our results show that machine translating the training set is beneficial even when the target language is different.</p>
<h3>4.3 Original vs. translated test sets</h3>
<p>So as to understand whether the improvements observed so far are limited to translated test sets or apply more generally, we conduct additional experiments comparing translated test sets to original ones. However, to the best of our knowledge, all</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XNLI dev</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Our dataset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">OR <br> (en)</td>
<td style="text-align: center;">HT <br> (es)</td>
<td style="text-align: center;">OR <br> (es)</td>
<td style="text-align: center;">HT <br> (en)</td>
</tr>
<tr>
<td style="text-align: center;">RobERTA</td>
<td style="text-align: center;">ORIG</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">80.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">80.5</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R</td>
<td style="text-align: center;">ORIG</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">78.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">78.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-ES</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">78.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-FI</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">78.3</td>
</tr>
</tbody>
</table>
<p>Table 2: NLI results on original (OR), human translated (HT) and machine translated (MT) sets (acc). BT-XX and MT-XX outperform OrIG in translated sets, but do not get any clear improvement in original ones.
existing non-English NLI benchmarks were created through translation. For that reason, we build a new test set that mimics XNLI, but is annotated in Spanish rather than English. We first collect the premises from a filtered version of CommonCrawl (Buck et al., 2014), taking a subset of 5 websites that represent a diverse set of genres: a newspaper, an economy forum, a celebrity magazine, a literature blog, and a consumer magazine. We then ask native Spanish annotators to generate an entailment, a neutral and a contradiction hypothesis for each premise. ${ }^{5}$ We collect a total of 2490 examples using this procedure, which is the same size as the XNLI development set. Finally, we create a human translated and a machine translated English version of the dataset using professional translators from Gengo and our machine translation system described in $\S 3.2,{ }^{6}$ respectively. We report results for the best epoch checkpoint on each set.</p>
<p>As shown in Table 2, both BT-XX and MT-XX clearly outperform Orig in all test sets created through translation, which is consistent with our previous results. In contrast, the best results on the original English set are obtained by ORIG, and neither BT-XX nor MT-XX obtain any clear improvement on the one in Spanish either. ${ }^{7}$ This confirms that the underlying phenomenon is limited to translated test sets. In addition, it is worth mentioning that the results for the machine translated test set in English are slightly better than those for the human</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: NLI Stress Test results (combined matched \&amp; mismatched acc). $\mathrm{AT}=$ antonymy, $\mathrm{NR}=$ numerical reasoning, $\mathrm{WO}=$ word overlap, $\mathrm{NG}=$ negation, $\mathrm{LN}=$ length mismatch, $\mathrm{SE}=$ spelling error. BT-FI and MT-FI are considerably weaker than ORIG in the competence test, but substantially stronger in the distraction test.
translated one, which suggests that the difficulty of the task does not only depend on the translation quality. Finally, it is also interesting that MT-ES is only marginally better than MT-FI in both Spanish test sets, even if it corresponds to the TranslateTrain approach, whereas MT-FI needs to ZERO-SHOT transfer from Finnish into Spanish. This reinforces the idea that it is training on translated data rather than training on the target language that is key in Translate-Train.</p>
<h3>4.4 Stress tests</h3>
<p>In order to better understand how systems trained on original and translated data differ, we run additional experiments on the NLI Stress Tests (Naik et al., 2018), which were designed to test the robustness of NLI models to specific linguistic phenomena in English. The benchmark consists of a competence test, which evaluates the ability to understand antonymy relation and perform numerical reasoning, a distraction test, which evaluates the robustness to shallow patterns like lexical overlap and the presence of negation words, and a noise test, which evaluates robustness to spelling errors. Just as with previous experiments, we report results for the best epoch checkpoint in each test set.</p>
<p>As shown in Table 3, Orig outperforms BT-FI and MT-FI on the competence test by a large margin, but the opposite is true on the distraction test. ${ }^{8}$ In particular, our results show that BT-FI and MT-FI are less reliant on lexical overlap and the presence of negative words. This feels intuitive, as translating the premise and hypothesis independently-as BT-FI and MT-FI do-is likely to reduce the lexical overlap between them. More generally, the trans-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>lation process can alter similar superficial patterns in the data, which NLI models are sensitive to (§2). This would explain why the resulting models have a different behavior on different stress tests.</p>
<h3>4.5 Output class distribution</h3>
<p>With the aim to understand the effect of the previous phenomenon in cross-lingual settings, we look at the output class distribution of our different models in the XNLI development set. As shown in Table 4, the predictions of all systems are close to the true class distribution in the case of English. Nevertheless, ORIG is strongly biased for the rest of languages, and tends to underpredict entailment and overpredict neutral. This can again be attributed to the fact that the English test set is original, whereas the rest are human translated. In particular, it is well-known that NLI models tend to predict entailment when there is a high lexical overlap between the premise and the hypothesis (§2). However, the degree of overlap will be smaller in the human translated test sets given that the premise and the hypothesis were translated independently, which explains why entailment is underpredicted. In contrast, BT-FI and MT-FI are exposed to the exact same phenomenon during training, which explains why they are not that heavily affected.</p>
<p>So as to measure the impact of this phenomenon, we explore a simple approach to correct this bias: having fine-tuned each model, we adjust the bias term added to the logit of each class so the model predictions match the true class distribution for each language. ${ }^{9}$ As shown in Table 5, this brings large improvements for ORIG, but is less effective for BT-FI and MT-FI. ${ }^{10}$ This shows that the performance of ORIG was considerably hindered by this bias, which BT-FI and MT-FI effectively mitigate.</p>
<h3>4.6 Comparison with the state-of-the-art</h3>
<p>So as to put our results into perspective, we compare our best variant to previous work on the XNLI test set. As shown in Table 6, our method improves the state-of-the-art for both the Translate-Test and the Zero-Shot approaches by 4.3 and 2.8 points,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Output class distribution on XNLI dev. All systems are close to the true distribution in English, but ORIG is biased toward neu and con in the transfer languages. BT-FI and MT-FI alleviate this issue.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">base</td>
<td style="text-align: center;">Unbias</td>
<td style="text-align: center;">$+\Delta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RobertA <br> (translate-test)</td>
<td style="text-align: center;">ORIG</td>
<td style="text-align: center;">77.7 a0.6</td>
<td style="text-align: center;">80.6 a0.2</td>
<td style="text-align: center;">2.9 a0.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">82.3 a0.2</td>
<td style="text-align: center;">82.8 a0.1</td>
<td style="text-align: center;">0.4 a0.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">XLM-R <br> (zero-shot)</td>
<td style="text-align: center;">ORIG</td>
<td style="text-align: center;">81.0 a0.2</td>
<td style="text-align: center;">82.4 a0.2</td>
<td style="text-align: center;">1.4 a0.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">83.2 a0.1</td>
<td style="text-align: center;">83.3 a0.1</td>
<td style="text-align: center;">0.1 a0.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-FI</td>
<td style="text-align: center;">83.4 a0.2</td>
<td style="text-align: center;">83.8 a0.1</td>
<td style="text-align: center;">0.4 a0.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: XNLI dev results with class distribution unbiasing (average acc across all languages). Adjusting the bias term of the classifier to match the true class distribution brings large improvements for ORIG, but is less effective for BT-FI and MT-FI.
respectively. It also obtains the best overall results published to date, with the additional advantage that the previous state-of-the-art required a machine translation system between English and each of the 14 target languages, whereas our method uses a single machine translation system between English and Finnish (which is not one of the target languages). While the main goal of our work is not to design better cross-lingual models, but to analyze their behavior in connection to translation, this shows that the phenomenon under study is highly relevant, to the extent that it can be exploited to improve the state-of-the-art.</p>
<h2>5 QA experiments</h2>
<p>So as to understand whether our previous findings apply to other tasks besides NLI, we run additional experiments on QA. As shown in Table 7, BT-FI and BT-ES do indeed outperform ORIG for the Translate-Test approach on MLQA. The improvement is modest, but very consistent across different languages, models and runs. The results for MT-ES and MT-FI are less conclusive, presumably because mapping the answer spans across languages might introduce some noise. In contrast, we do not ob-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">fr</th>
<th style="text-align: center;">es</th>
<th style="text-align: center;">de</th>
<th style="text-align: center;">el</th>
<th style="text-align: center;">bg</th>
<th style="text-align: center;">ru</th>
<th style="text-align: center;">tr</th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">vi</th>
<th style="text-align: center;">th</th>
<th style="text-align: center;">zh</th>
<th style="text-align: center;">hi</th>
<th style="text-align: center;">sw</th>
<th style="text-align: center;">ur</th>
<th style="text-align: center;">avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-tune an English model and machine translate the test set into English (TRANSLATE-TEST)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT (Devlin et al., 2019)</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: center;">Roberta (Liu et al., 2019)</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">77.8</td>
</tr>
<tr>
<td style="text-align: center;">Proposed (Roberta - BT-FI)</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: center;">+ Unbiasing (tuned in dev)</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">82.1</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune a multilingual model on all machine translated training sets (TRANSLATE-TRAIN-ALL)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Unicoder (Huang et al., 2019)</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R (Conneau et al., 2020)</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune a multilingual model on the English training set (ZERO-SHOT)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mBERT (Devlin et al., 2019)</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">66.3</td>
</tr>
<tr>
<td style="text-align: center;">XLM (Conneau and Lample, 2019)</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">75.1</td>
</tr>
<tr>
<td style="text-align: center;">Unicoder (Huang et al., 2019)</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">75.4</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R (Conneau et al., 2020)</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">80.1</td>
</tr>
<tr>
<td style="text-align: center;">Proposed (XLM-R - MT-FI)</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">82.4</td>
</tr>
<tr>
<td style="text-align: center;">+ Unbiasing (tuned in dev)</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">82.9</td>
</tr>
</tbody>
</table>
<p>Table 6: XNLI test results (acc). Results for other methods are taken from their respective papers or, if not provided, from Conneau et al. (2020). For those with multiple variants, we select the one with the best results.
serve any clear improvement for the Zero-Shot approach on this dataset. Our XQuAD results in Table 8 are more positive, but still inconclusive.</p>
<p>These results can partly be explained by the translation procedure used to create the different benchmarks: the premises and hypotheses of XNLI were translated independently, whereas the questions and context paragraphs of XQuAD were translated together. Similarly, MLQA made use of parallel contexts, and translators were shown the sentence containing each answer when translating the corresponding question. As a result, one can expect both QA benchmarks to have more consistent translations than XNLI, which would in turn diminish this phenomenon. In contrast, the questions and context paragraphs are independently translated when using machine translation, which explains why BT-ES and BT-FI outperform Orid for the Translate-Test approach. We conclude that the translation artifacts revealed by our analysis are not exclusive to NLI, as they also show up on QA for the Translate-Test approach, but their actual impact can be highly dependent on the translation procedure used and the nature of the task.</p>
<h2>6 Discussion</h2>
<p>Our analysis prompts to reconsider previous findings in cross-lingual transfer learning as follows:</p>
<p>The cross-lingual transfer gap on XNLI was overestimated. Given the parallel nature of XNLI, accuracy differences across languages are commonly interpreted as the loss of performance
when generalizing from English to the rest of languages. However, our work shows that there is another factor that can have a much larger impact: the loss of performance when generalizing from original to translated data. Our results suggest that the real cross-lingual generalization ability of XLMR is considerably better than what the accuracy numbers in XNLI reflect.</p>
<p>Overcoming the cross-lingual gap is not what makes Translate-Train work. The original motivation for Translate-Train was to train the model on the same language it is tested on. However, we show that it is training on translated data, rather than training on the target language, that is key for this approach to outperform ZERO-SHOT as reported by previous authors.</p>
<p>Improvements previously attributed to data augmentation should be reconsidered. The method by Singh et al. (2019) combines machine translated premises and hypotheses in different languages (§2), resulting in an effect similar to BT-XX and MT-XX. As such, we believe that this method should be analyzed from the point of view of dataset artifacts rather than data augmentation, as the authors do. ${ }^{11}$ From this perspective, having the premise and the hypotheses in different languages can reduce the superficial patterns between them, which would explain why this approach is better than using examples in a single language.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">es</th>
<th style="text-align: center;">de</th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">vi</th>
<th style="text-align: center;">zh</th>
<th style="text-align: center;">hi</th>
<th style="text-align: center;">avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test set machine translated into English (TRANSLATE-TEST)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RobeRTA</td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">84.7 / 71.4</td>
<td style="text-align: center;">70.1 / 49.7</td>
<td style="text-align: center;">60.5 / 41.2</td>
<td style="text-align: center;">55.7 / 32.5</td>
<td style="text-align: center;">65.6 / 40.8</td>
<td style="text-align: center;">53.5 / 26.0</td>
<td style="text-align: center;">42.7 / 20.7</td>
<td style="text-align: center;">61.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">84.4 / 71.2</td>
<td style="text-align: center;">70.9 / 50.7</td>
<td style="text-align: center;">61.0 / 41.6</td>
<td style="text-align: center;">56.5 / 33.3</td>
<td style="text-align: center;">66.7 / 41.8</td>
<td style="text-align: center;">54.4 / 27.1</td>
<td style="text-align: center;">43.0 / 21.1</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">83.8 / 70.4</td>
<td style="text-align: center;">70.3 / 50.1</td>
<td style="text-align: center;">61.1 / 41.9</td>
<td style="text-align: center;">56.5 / 33.4</td>
<td style="text-align: center;">66.8 / 42.1</td>
<td style="text-align: center;">54.9 / 27.5</td>
<td style="text-align: center;">42.8 / 21.3</td>
<td style="text-align: center;">62.3</td>
</tr>
<tr>
<td style="text-align: center;">XLM-R</td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">84.1 / 71.0</td>
<td style="text-align: center;">69.9 / 49.2</td>
<td style="text-align: center;">60.8 / 42.5</td>
<td style="text-align: center;">55.2 / 31.8</td>
<td style="text-align: center;">65.4 / 40.6</td>
<td style="text-align: center;">54.3 / 27.8</td>
<td style="text-align: center;">43.6 / 21.3</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">83.8 / 70.8</td>
<td style="text-align: center;">70.5 / 50.0</td>
<td style="text-align: center;">61.4 / 43.5</td>
<td style="text-align: center;">56.1 / 33.1</td>
<td style="text-align: center;">66.5 / 41.6</td>
<td style="text-align: center;">55.4 / 29.0</td>
<td style="text-align: center;">44.0 / 22.2</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">82.7 / 69.6</td>
<td style="text-align: center;">70.0 / 49.7</td>
<td style="text-align: center;">61.1 / 43.3</td>
<td style="text-align: center;">56.0 / 33.1</td>
<td style="text-align: center;">66.2 / 41.5</td>
<td style="text-align: center;">55.6 / 29.2</td>
<td style="text-align: center;">43.7 / 22.0</td>
<td style="text-align: center;">62.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-ES</td>
<td style="text-align: center;">83.4 / 69.7</td>
<td style="text-align: center;">70.0 / 49.1</td>
<td style="text-align: center;">61.0 / 42.7</td>
<td style="text-align: center;">55.6 / 32.2</td>
<td style="text-align: center;">65.9 / 40.9</td>
<td style="text-align: center;">54.9 / 28.1</td>
<td style="text-align: center;">43.9 / 21.6</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-FI</td>
<td style="text-align: center;">82.6 / 69.0</td>
<td style="text-align: center;">69.7 / 48.6</td>
<td style="text-align: center;">61.0 / 42.8</td>
<td style="text-align: center;">55.7 / 32.3</td>
<td style="text-align: center;">65.8 / 40.9</td>
<td style="text-align: center;">54.8 / 27.9</td>
<td style="text-align: center;">43.9 / 21.6</td>
<td style="text-align: center;">61.9</td>
</tr>
</tbody>
</table>
<p>Table 7: MLQA test results (F1 / exact match).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">en</th>
<th style="text-align: center;">es</th>
<th style="text-align: center;">de</th>
<th style="text-align: center;">el</th>
<th style="text-align: center;">ru</th>
<th style="text-align: center;">tr</th>
<th style="text-align: center;">ar</th>
<th style="text-align: center;">vi</th>
<th style="text-align: center;">th</th>
<th style="text-align: center;">zh</th>
<th style="text-align: center;">hi</th>
<th style="text-align: center;">avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">XLM-R <br> (zero-shoot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-ES</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT-FI</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-ES</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MT-FI</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: XQuAD results (F1). Results for the exact match metric are similar.</p>
<p>The potential of Translate-Test was underestimated. The previous best results for TranslateTest on XNLI lagged behind the state-of-the-art by 4.6 points. Our work reduces this gap to only 0.8 points by addressing the underlying translation artifacts. The reason why Translate-Test is more severely affected by this phenomenon is twofold: (i) the effect is doubled by first using human translation to create the test set and then machine translation to translate it back to English, and (ii) Translate-Train was inadvertently mitigating this issue (see above), but equivalent techniques were never applied to Translate-Test.</p>
<p>Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. ${ }^{12}$ In any case, this is a more general issue that falls beyond the scope of</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.</p>
<h2>7 Conclusions</h2>
<p>In this paper, we have shown that both human and machine translation can alter superficial patterns in data, which requires reconsidering previous findings in cross-lingual transfer learning. Based on the gained insights, we have improved the state-of-theart in XNLI for the Translate-Test and Zero-Shot approaches by a substantial margin. Finally, we have shown that the phenomenon is not specific to NLI but also affects QA, although it is less pronounced there thanks to the translation procedure used in the corresponding benchmarks. So as to facilitate similar studies in the future, we release</p>
<p>our NLI dataset, ${ }^{13}$ which, unlike previous benchmarks, was annotated in a non-English language and human translated into English.</p>
<h2>Acknowledgments</h2>
<p>We thank Nora Aranberri and Uxoa Iñurrieta for helpful discussion during the development of this work, as well as the rest of our colleagues from the IXA group that worked as annotators for our NLI dataset.</p>
<p>This research was partially funded by a Facebook Fellowship, the Basque Government excellence research group (IT1343-19), the Spanish MINECO (UnsupMT TIN2017-91692-EXP MCIU/AEI/FEDER, UE), Project BigKnowledge (Ayudas Fundación BBVA a equipos de investigación científica 2018), and the NVIDIA GPU grant program.</p>
<p>This research is supported via the BETTER Program contract #2019-19051600006 (ODNI, IARPA activity). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623-4637. Association for Computational Linguistics.</p>
<p>Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1-61, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ondřej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Translation, pages 1-44, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Christian Buck, Kenneth Heafield, and Bas van Ooyen. 2014. N-gram counts and language models from the Common Crawl. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 3579-3584, Reykjavik, Iceland. European Language Resources Association (ELRA).</p>
<p>Casimiro Pio Carrino, Marta R. Costa-jussà, and José A. R. Fonollosa. 2020. Automatic Spanish translation of the SQuAD dataset for multi-lingual question answering. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5515-5523, Marseille, France. European Language Resources Association.</p>
<p>Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470.</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451. Association for Computational Linguistics.</p>
<p>Alexis Conneau and Guillaume Lample. 2019. Crosslingual language model pretraining. In Advances in Neural Information Processing Systems 32, pages 7059-7069.</p>
<p>Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),</p>
<p>pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644-648, Atlanta, Georgia. Association for Computational Linguistics.</p>
<p>Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489-500, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Sergey Edunov, Myle Ott, Marc'Aurelio Ranzato, and Michael Auli. 2020. On the evaluation of machine translation systems trained with back-translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 28362846. Association for Computational Linguistics.</p>
<p>Miquel Esplà, Mikel Forcada, Gema Ramírez-Sánchez, and Hieu Hoang. 2019. ParaCrawl: Web-scale parallel corpora for the languages of the EU. In Proceedings of Machine Translation Summit XVII Volume 2: Translator, Project and User Tracks, pages 118-119, Dublin, Ireland. European Association for Machine Translation.</p>
<p>Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650-655, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Yvette Graham, Barry Haddow, and Philipp Koehn. 2019. Translationese in machine translation evaluation. arXiv preprint arXiv:1906.09833.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization. arXiv preprint arXiv:2003.11080.</p>
<p>Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019.</p>
<p>Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2485-2494, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? A critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5010-5015, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 73157330. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 25-30, Jeju Island, Korea. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340-2353, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901. Association for Computational Linguistics.</p>
<p>Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 1-9, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1946-1958, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. 2019. WikiMatrix: Mining 135m parallel sentences in 1620 language pairs from Wikipedia. arXiv preprint arXiv:1907.05791.</p>
<p>Holger Schwenk and Xian Li. 2018. A corpus for multilingual document classification in eight languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Jasdeep Singh, Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2019. XLDA: Cross-lingual data augmentation for natural language inference and question answering. arXiv preprint arXiv:1905.11471.</p>
<p>Jörg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 2214-2218, Istanbul, Turkey. European Language Resources Association (ELRA).</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30, pages 5998-6008.</p>
<p>Vered Volansky, Noam Ordan, and Shuly Wintner. 2013. On the features of translationese. Digital Scholarship in the Humanities, 30(1):98-118.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 36873692, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Mike Zhang and Antonio Toral. 2019. The effect of translationese in machine translation test sets. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 7381, Florence, Italy. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ https://github.com/artetxem/esxnli&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ We observe similar trends for BT-ES and MT-ES, but omit these results for conciseness.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
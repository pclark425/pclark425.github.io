<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7348 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7348</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7348</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-267782758</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14268v1.pdf" target="_blank">Can Large Language Models Detect Misinformation in Scientific News Reporting?</a></p>
                <p><strong>Paper Abstract:</strong> Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database. Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press. For each of these architectures, we use several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting. We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7348.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7348.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B (generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama2 7B (used to generate synthetic scientific news articles)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as a text-based simulator to generate both 'True' and 'Convincing False' scientific news articles grounded on CORD-19 abstracts via a jailbreak prompt; bulk generation engine for the LLM-generated portion of the SciNews dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Detect Misinformation in Scientific News Reporting?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base LLM (used with jailbreak prompt to bypass alignment restrictions)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / COVID-19 scientific news reporting (science communication)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate news-article-style summaries from scientific abstracts, producing both truthful and deliberately misleading (convincing false) news articles for dataset creation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Jailbreak prompt that frames the request as a classroom exercise (instructor asks model to produce a 'True Article' and a 'Convincing False Article' based on a provided abstract); no-chain-of-thought for generation described.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Qualitative manual review; ROUGE-2 used elsewhere for summary filtering (see experimental_conditions). No formal automatic metric for generation fidelity reported beyond manual assessment of fabricated content.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>N/A (generation task; quantitative generation-quality metrics not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model alignment / guardrails (necessitated jailbreak prompts to produce misinformation)', 'Model size (7B) and resulting quality vs larger models', 'Prompt engineering (jailbreak framing influenced ability to produce falsehoods)', 'Selection of high-quality scientific abstracts (most-cited CORD-19 papers) as seeds for generation']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Generated 1,000 samples with Llama2-7B using the designed jailbreak prompt; curated seed abstracts from CORD-19 (highly cited journals); post-generation human manual filtering and quality control by researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Generation sometimes fabricated details even in intended 'True' summaries (imagined journals, made-up scientists); produced a range of falsehood types (numeric changes, exaggeration, reversal of claims); no formal evaluation of generation fidelity provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Detect Misinformation in Scientific News Reporting?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7348.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7348.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (used to generate synthetic scientific news articles)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as a secondary generator to create LLM-generated news articles (both true and convincing false) from selected CORD-19 abstracts via jailbreak-style prompts; smaller share of generated dataset due to cost considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Detect Misinformation in Scientific News Reporting?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (commonly referenced as ChatGPT/GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned chat model (used with jailbreak prompt for generation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / COVID-19 scientific news reporting</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate news-style 'True' and 'Convincing False' articles from scientific abstracts to populate dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Jailbreak prompt similar to that used with Llama2-7B (classroom framing requesting true and convincing false articles).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Qualitative manual review; no numeric generation-quality metric reported for GPT-3.5 generation itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>N/A (generation task).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model alignment / guardrails (required jailbreak prompt to elicit false articles)', 'Cost (smaller number of samples generated with GPT-3.5 due to cost)', 'Prompt engineering (jailbreak needed to elicit misinformation)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Generated 200 LLM-generated news articles using GPT-3.5; seed abstracts from curated CORD-19 set; manual review and filtering applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>As with Llama2-7B, GPT-3.5 could fabricate details even in 'True' outputs; no automated fidelity scoring reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Detect Misinformation in Scientific News Reporting?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7348.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7348.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (used to detect scientific misinformation in news reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluated as an LLM-based classifier in three architectures (SERIf, SIf, D2I) to decide whether a news article is reliable or unreliable given paired scientific abstracts/evidence, using multiple prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Detect Misinformation in Scientific News Reporting?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned chat model (used directly for inference and prompting-based modules)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / COVID-19 scientific news reporting (classification of news veracity relative to scientific evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based inference/classification: determine whether a news article is 'reliable' or 'unreliable' relative to associated scientific abstracts; produce justifications/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, few-shot (two examples: one reliable + reasoning, one unreliable + reasoning), and Chain-of-Thought (CoT) prompts that use defined dimensions of scientific validity (Alignment, Causation confusion, Accuracy, Generalization, Contextual Fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, Precision, Recall, F1 (per-architecture and per-prompt reported). ROUGE-2 used elsewhere for summary filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Representative reported results (SERIf architecture): overall accuracy ~75.53% with CoT prompting (GPT-3.5, SERIf CoT overall in Table 2). Representative SIf result: human-written accuracy = 82.50% and F1 = 83.50% (SIf, GPT-3.5, zero-shot as reported in paper text/Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random-guess baseline ~50% (implied); Llama2 models acted as weaker baselines (near-random performance). Also comparisons across architectures (D2I much worse) serve as internal baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy (CoT generally outperforms zero-shot; few-shot showed limited improvement)', 'Architecture (SIf often matched/surpassed SERIf; D2I underperformed)', 'Type of article (LLM-generated misinformation harder to detect than human-written)', 'Model family (GPT performed substantially better than Llama2)', 'Summarization module presence (improves performance vs raw input)', 'Temperature setting (experiments used temperature=0 for predictability)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Temperature set to 0; evaluated SERIf, SIf, and D2I architectures; evidence corpus drawn from CORD-19 with up to three matched abstracts per news article; evidence retrieval implemented via BM25 (Vespa) and optional sentence-level retrieval via LLM; dataset size 2,400 news articles (balanced human/LLM-generated and reliable/unreliable).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower precision or recall trade-offs depending on prompt; CoT sometimes increased precision but decreased recall (particularly on human-authored SIf); difficulty in detecting LLM-generated falsehoods (high recall with low precision trend, tendency to classify news as True); hallucinations in abstractive summaries and generated articles leading to noisy evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Detect Misinformation in Scientific News Reporting?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7348.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7348.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used to detect scientific misinformation in news reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as a high-performing LLM for the SERIf, SIf, and D2I architectures and tested with zero-shot, few-shot, and Chain-of-Thought prompting; outperformed Llama2 models and often outperformed GPT-3.5 in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Detect Misinformation in Scientific News Reporting?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (GPT-4 family; larger than GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned chat model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / COVID-19 scientific news reporting</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Classify news articles as reliable/unreliable with justification relative to scientific abstracts; produce chain-of-thought style explanations guided by scientific-validity dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, few-shot (two examples), Chain-of-Thought (CoT) incorporating dimensions of scientific validity (Alignment, Causation confusion, Accuracy, Generalization, Contextual Fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, Precision, Recall, F1 (per-architecture and per-prompt reported).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Representative reported results: SERIf with CoT: overall accuracy reported as ~77.96% (GPT-4, SERIf, CoT, from Table 2). SIf zero-shot human-written accuracy ~80.00% (Table 3). Across architectures GPT-4 outperformed Llama2 and often matched or surpassed GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Llama2-7B/13B performances near chance (≈50–58% depending on prompt) served as weaker baselines; random baseline ≈50%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy (CoT improved performance notably, especially on LLM-generated data)', 'Architecture (SIf performed best on human-written articles; D2I performed poorly)', 'Type of article (LLM-generated harder to detect)', 'Summarization module importance', 'Model family / pretraining (GPT models stronger due to broader training/usage)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Temperature set to 0; architectures SERIf, SIf, D2I tested; evidence corpus = 7,087 paragraph-level evidence items from CORD-19; sentence-level retrieval when used was LLM-assisted; ROUGE-2 used to filter abstractive summaries (threshold 0.4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although GPT-4 performed best among evaluated models, detection of LLM-generated misinformation remained substantially more difficult; few-shot prompting offered limited improvements; D2I architecture underperformed indicating that raw feeding of article+abstracts is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Detect Misinformation in Scientific News Reporting?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7348.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7348.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B (detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama2 13B (used for misinformation detection experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluated as a larger Llama2 variant for the SERIf architecture and prompting strategies; performance was substantially weaker than GPT-family models and often near chance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Detect Misinformation in Scientific News Reporting?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base LLM (not reported as instruction-fine-tuned in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / COVID-19 scientific news reporting</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Classify news articles as reliable/unreliable given abstracts/evidence under SERIf (and possibly other) architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot, few-shot, and Chain-of-Thought evaluated (same CoT that encodes scientific-validity dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, Precision, Recall, F1.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Representative reported result (SERIf CoT): overall accuracy ~71.60% (Table 2 shows CoT overall ~71.60), substantially lower than GPT variants and only modestly above chance in many configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Random ≈50%; GPT models (GPT-3.5 / GPT-4) used as stronger baselines (≈75–82% in best settings).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model family and training data (Llama2 underperformed relative to GPT, possibly due to less widespread fine-tuning/crowdsourced improvement)', 'Prompting strategy (CoT improved performance but still lagged)', 'Architecture (summarization and SIf improved results vs D2I)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Temperature=0; evaluated across prompt strategies and architectures; evidence corpus from CORD-19; BM25 via Vespa used for initial abstract matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance barely above random guessing in many configurations; Llama2 models achieved low precision/recall relative to GPT; suggests limitations from training/data/fine-tuning differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Detect Misinformation in Scientific News Reporting?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7348.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7348.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization+ROUGE-2 filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstractive summarization module with ROUGE-2 filtering for summary validity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A summarization pipeline (extractive followed by abstractive using LLMs) was used to condense news articles; ROUGE-2 similarity between abstractive summary and original abstract was used to filter poor/invalid 'True' summaries (threshold ROUGE-2 > 0.4).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Detect Misinformation in Scientific News Reporting?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-based summarizer (not tied to a single external model name in text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM used for extractive-abstractive summarization</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Science news summarization for biomedical/COVID-19 articles</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Produce concise abstractive summaries of news articles to feed inference modules; filter to ensure fidelity to source abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Extractive-abstractive two-step summarization where extractive summary seeds the abstractive LLM; quality checked via ROUGE-2 thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-2 similarity used for automatic filtering; manual annotation used Krippendorff's alpha on aspects of summary quality (alpha: extractive quality 0.53, abstractive quality 0.82, hallucination detection 0.91, extractive/abstractive comparison 0.94).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Filtering criterion: keep abstractive summaries with ROUGE-2 > 0.4. Manual evaluation: when ROUGE-2 > 0.4, likelihood of an invalid 'true' summary was 2%; when ROUGE-2 ≤ 0.4, likelihood of invalid true summary rose to 30%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>N/A (summary-filtering component rather than classifier baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['ROUGE-2 similarity correlates with summary validity', 'Abstractive hallucinations exist despite extractive seed', 'Human annotation agreement used to evaluate summary quality']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>200 abstractions randomly selected for human annotation; summaries retained only if ROUGE-2 > 0.4 to reduce invalid 'true' summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Abstractive summarizer sometimes hallucinated (generated imagined journals or quotes); ROUGE-2 thresholding mitigates but does not eliminate these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Detect Misinformation in Scientific News Reporting?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Autodan: Generating stealthy jailbreak prompts on aligned large language models <em>(Rating: 2)</em></li>
                <li>Jailbreaking chatgpt via prompt engineering: An empirical study <em>(Rating: 2)</em></li>
                <li>Can LLM-Generated Misinformation Be Detected? <em>(Rating: 2)</em></li>
                <li>Scientific Fact-Checking: A Survey of Resources and Approaches <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7348",
    "paper_id": "paper-267782758",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "Llama2-7B (generation)",
            "name_full": "Llama2 7B (used to generate synthetic scientific news articles)",
            "brief_description": "Used as a text-based simulator to generate both 'True' and 'Convincing False' scientific news articles grounded on CORD-19 abstracts via a jailbreak prompt; bulk generation engine for the LLM-generated portion of the SciNews dataset.",
            "citation_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_size": "7B",
            "model_type": "base LLM (used with jailbreak prompt to bypass alignment restrictions)",
            "scientific_domain": "Biomedical / COVID-19 scientific news reporting (science communication)",
            "simulation_task_description": "Generate news-article-style summaries from scientific abstracts, producing both truthful and deliberately misleading (convincing false) news articles for dataset creation.",
            "prompting_strategy": "Jailbreak prompt that frames the request as a classroom exercise (instructor asks model to produce a 'True Article' and a 'Convincing False Article' based on a provided abstract); no-chain-of-thought for generation described.",
            "evaluation_metric": "Qualitative manual review; ROUGE-2 used elsewhere for summary filtering (see experimental_conditions). No formal automatic metric for generation fidelity reported beyond manual assessment of fabricated content.",
            "reported_accuracy": "N/A (generation task; quantitative generation-quality metrics not reported).",
            "baseline_accuracy": "N/A",
            "factors_reported": [
                "Model alignment / guardrails (necessitated jailbreak prompts to produce misinformation)",
                "Model size (7B) and resulting quality vs larger models",
                "Prompt engineering (jailbreak framing influenced ability to produce falsehoods)",
                "Selection of high-quality scientific abstracts (most-cited CORD-19 papers) as seeds for generation"
            ],
            "experimental_conditions": "Generated 1,000 samples with Llama2-7B using the designed jailbreak prompt; curated seed abstracts from CORD-19 (highly cited journals); post-generation human manual filtering and quality control by researchers.",
            "limitations_or_failure_modes": "Generation sometimes fabricated details even in intended 'True' summaries (imagined journals, made-up scientists); produced a range of falsehood types (numeric changes, exaggeration, reversal of claims); no formal evaluation of generation fidelity provided.",
            "uuid": "e7348.0",
            "source_info": {
                "paper_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5 (generation)",
            "name_full": "GPT-3.5 (used to generate synthetic scientific news articles)",
            "brief_description": "Used as a secondary generator to create LLM-generated news articles (both true and convincing false) from selected CORD-19 abstracts via jailbreak-style prompts; smaller share of generated dataset due to cost considerations.",
            "citation_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": "not specified in paper (commonly referenced as ChatGPT/GPT-3.5 family)",
            "model_type": "instruction-tuned chat model (used with jailbreak prompt for generation)",
            "scientific_domain": "Biomedical / COVID-19 scientific news reporting",
            "simulation_task_description": "Generate news-style 'True' and 'Convincing False' articles from scientific abstracts to populate dataset.",
            "prompting_strategy": "Jailbreak prompt similar to that used with Llama2-7B (classroom framing requesting true and convincing false articles).",
            "evaluation_metric": "Qualitative manual review; no numeric generation-quality metric reported for GPT-3.5 generation itself.",
            "reported_accuracy": "N/A (generation task).",
            "baseline_accuracy": "N/A",
            "factors_reported": [
                "Model alignment / guardrails (required jailbreak prompt to elicit false articles)",
                "Cost (smaller number of samples generated with GPT-3.5 due to cost)",
                "Prompt engineering (jailbreak needed to elicit misinformation)"
            ],
            "experimental_conditions": "Generated 200 LLM-generated news articles using GPT-3.5; seed abstracts from curated CORD-19 set; manual review and filtering applied.",
            "limitations_or_failure_modes": "As with Llama2-7B, GPT-3.5 could fabricate details even in 'True' outputs; no automated fidelity scoring reported.",
            "uuid": "e7348.1",
            "source_info": {
                "paper_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5 (detection)",
            "name_full": "GPT-3.5 (used to detect scientific misinformation in news reporting)",
            "brief_description": "Evaluated as an LLM-based classifier in three architectures (SERIf, SIf, D2I) to decide whether a news article is reliable or unreliable given paired scientific abstracts/evidence, using multiple prompting strategies.",
            "citation_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": "not specified in paper (GPT-3.5 family)",
            "model_type": "instruction-tuned chat model (used directly for inference and prompting-based modules)",
            "scientific_domain": "Biomedical / COVID-19 scientific news reporting (classification of news veracity relative to scientific evidence)",
            "simulation_task_description": "Text-based inference/classification: determine whether a news article is 'reliable' or 'unreliable' relative to associated scientific abstracts; produce justifications/explanations.",
            "prompting_strategy": "Zero-shot, few-shot (two examples: one reliable + reasoning, one unreliable + reasoning), and Chain-of-Thought (CoT) prompts that use defined dimensions of scientific validity (Alignment, Causation confusion, Accuracy, Generalization, Contextual Fidelity).",
            "evaluation_metric": "Accuracy, Precision, Recall, F1 (per-architecture and per-prompt reported). ROUGE-2 used elsewhere for summary filtering.",
            "reported_accuracy": "Representative reported results (SERIf architecture): overall accuracy ~75.53% with CoT prompting (GPT-3.5, SERIf CoT overall in Table 2). Representative SIf result: human-written accuracy = 82.50% and F1 = 83.50% (SIf, GPT-3.5, zero-shot as reported in paper text/Table 3).",
            "baseline_accuracy": "Random-guess baseline ~50% (implied); Llama2 models acted as weaker baselines (near-random performance). Also comparisons across architectures (D2I much worse) serve as internal baselines.",
            "factors_reported": [
                "Prompting strategy (CoT generally outperforms zero-shot; few-shot showed limited improvement)",
                "Architecture (SIf often matched/surpassed SERIf; D2I underperformed)",
                "Type of article (LLM-generated misinformation harder to detect than human-written)",
                "Model family (GPT performed substantially better than Llama2)",
                "Summarization module presence (improves performance vs raw input)",
                "Temperature setting (experiments used temperature=0 for predictability)"
            ],
            "experimental_conditions": "Temperature set to 0; evaluated SERIf, SIf, and D2I architectures; evidence corpus drawn from CORD-19 with up to three matched abstracts per news article; evidence retrieval implemented via BM25 (Vespa) and optional sentence-level retrieval via LLM; dataset size 2,400 news articles (balanced human/LLM-generated and reliable/unreliable).",
            "limitations_or_failure_modes": "Lower precision or recall trade-offs depending on prompt; CoT sometimes increased precision but decreased recall (particularly on human-authored SIf); difficulty in detecting LLM-generated falsehoods (high recall with low precision trend, tendency to classify news as True); hallucinations in abstractive summaries and generated articles leading to noisy evidence.",
            "uuid": "e7348.2",
            "source_info": {
                "paper_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (detection)",
            "name_full": "GPT-4 (used to detect scientific misinformation in news reporting)",
            "brief_description": "Used as a high-performing LLM for the SERIf, SIf, and D2I architectures and tested with zero-shot, few-shot, and Chain-of-Thought prompting; outperformed Llama2 models and often outperformed GPT-3.5 in the reported experiments.",
            "citation_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not specified in paper (GPT-4 family; larger than GPT-3.5)",
            "model_type": "instruction-tuned chat model",
            "scientific_domain": "Biomedical / COVID-19 scientific news reporting",
            "simulation_task_description": "Classify news articles as reliable/unreliable with justification relative to scientific abstracts; produce chain-of-thought style explanations guided by scientific-validity dimensions.",
            "prompting_strategy": "Zero-shot, few-shot (two examples), Chain-of-Thought (CoT) incorporating dimensions of scientific validity (Alignment, Causation confusion, Accuracy, Generalization, Contextual Fidelity).",
            "evaluation_metric": "Accuracy, Precision, Recall, F1 (per-architecture and per-prompt reported).",
            "reported_accuracy": "Representative reported results: SERIf with CoT: overall accuracy reported as ~77.96% (GPT-4, SERIf, CoT, from Table 2). SIf zero-shot human-written accuracy ~80.00% (Table 3). Across architectures GPT-4 outperformed Llama2 and often matched or surpassed GPT-3.5.",
            "baseline_accuracy": "Llama2-7B/13B performances near chance (≈50–58% depending on prompt) served as weaker baselines; random baseline ≈50%.",
            "factors_reported": [
                "Prompting strategy (CoT improved performance notably, especially on LLM-generated data)",
                "Architecture (SIf performed best on human-written articles; D2I performed poorly)",
                "Type of article (LLM-generated harder to detect)",
                "Summarization module importance",
                "Model family / pretraining (GPT models stronger due to broader training/usage)"
            ],
            "experimental_conditions": "Temperature set to 0; architectures SERIf, SIf, D2I tested; evidence corpus = 7,087 paragraph-level evidence items from CORD-19; sentence-level retrieval when used was LLM-assisted; ROUGE-2 used to filter abstractive summaries (threshold 0.4).",
            "limitations_or_failure_modes": "Although GPT-4 performed best among evaluated models, detection of LLM-generated misinformation remained substantially more difficult; few-shot prompting offered limited improvements; D2I architecture underperformed indicating that raw feeding of article+abstracts is insufficient.",
            "uuid": "e7348.3",
            "source_info": {
                "paper_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama2-13B (detection)",
            "name_full": "Llama2 13B (used for misinformation detection experiments)",
            "brief_description": "Evaluated as a larger Llama2 variant for the SERIf architecture and prompting strategies; performance was substantially weaker than GPT-family models and often near chance.",
            "citation_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
            "mention_or_use": "use",
            "model_name": "Llama2-13B",
            "model_size": "13B",
            "model_type": "base LLM (not reported as instruction-fine-tuned in paper)",
            "scientific_domain": "Biomedical / COVID-19 scientific news reporting",
            "simulation_task_description": "Classify news articles as reliable/unreliable given abstracts/evidence under SERIf (and possibly other) architectures.",
            "prompting_strategy": "Zero-shot, few-shot, and Chain-of-Thought evaluated (same CoT that encodes scientific-validity dimensions).",
            "evaluation_metric": "Accuracy, Precision, Recall, F1.",
            "reported_accuracy": "Representative reported result (SERIf CoT): overall accuracy ~71.60% (Table 2 shows CoT overall ~71.60), substantially lower than GPT variants and only modestly above chance in many configurations.",
            "baseline_accuracy": "Random ≈50%; GPT models (GPT-3.5 / GPT-4) used as stronger baselines (≈75–82% in best settings).",
            "factors_reported": [
                "Model family and training data (Llama2 underperformed relative to GPT, possibly due to less widespread fine-tuning/crowdsourced improvement)",
                "Prompting strategy (CoT improved performance but still lagged)",
                "Architecture (summarization and SIf improved results vs D2I)"
            ],
            "experimental_conditions": "Temperature=0; evaluated across prompt strategies and architectures; evidence corpus from CORD-19; BM25 via Vespa used for initial abstract matching.",
            "limitations_or_failure_modes": "Performance barely above random guessing in many configurations; Llama2 models achieved low precision/recall relative to GPT; suggests limitations from training/data/fine-tuning differences.",
            "uuid": "e7348.4",
            "source_info": {
                "paper_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Summarization+ROUGE-2 filter",
            "name_full": "Abstractive summarization module with ROUGE-2 filtering for summary validity",
            "brief_description": "A summarization pipeline (extractive followed by abstractive using LLMs) was used to condense news articles; ROUGE-2 similarity between abstractive summary and original abstract was used to filter poor/invalid 'True' summaries (threshold ROUGE-2 &gt; 0.4).",
            "citation_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
            "mention_or_use": "use",
            "model_name": "LLM-based summarizer (not tied to a single external model name in text)",
            "model_size": null,
            "model_type": "LLM used for extractive-abstractive summarization",
            "scientific_domain": "Science news summarization for biomedical/COVID-19 articles",
            "simulation_task_description": "Produce concise abstractive summaries of news articles to feed inference modules; filter to ensure fidelity to source abstracts.",
            "prompting_strategy": "Extractive-abstractive two-step summarization where extractive summary seeds the abstractive LLM; quality checked via ROUGE-2 thresholding.",
            "evaluation_metric": "ROUGE-2 similarity used for automatic filtering; manual annotation used Krippendorff's alpha on aspects of summary quality (alpha: extractive quality 0.53, abstractive quality 0.82, hallucination detection 0.91, extractive/abstractive comparison 0.94).",
            "reported_accuracy": "Filtering criterion: keep abstractive summaries with ROUGE-2 &gt; 0.4. Manual evaluation: when ROUGE-2 &gt; 0.4, likelihood of an invalid 'true' summary was 2%; when ROUGE-2 ≤ 0.4, likelihood of invalid true summary rose to 30%.",
            "baseline_accuracy": "N/A (summary-filtering component rather than classifier baseline).",
            "factors_reported": [
                "ROUGE-2 similarity correlates with summary validity",
                "Abstractive hallucinations exist despite extractive seed",
                "Human annotation agreement used to evaluate summary quality"
            ],
            "experimental_conditions": "200 abstractions randomly selected for human annotation; summaries retained only if ROUGE-2 &gt; 0.4 to reduce invalid 'true' summaries.",
            "limitations_or_failure_modes": "Abstractive summarizer sometimes hallucinated (generated imagined journals or quotes); ROUGE-2 thresholding mitigates but does not eliminate these failures.",
            "uuid": "e7348.5",
            "source_info": {
                "paper_title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "rating": 2,
            "sanitized_title": "autodan_generating_stealthy_jailbreak_prompts_on_aligned_large_language_models"
        },
        {
            "paper_title": "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "rating": 2,
            "sanitized_title": "jailbreaking_chatgpt_via_prompt_engineering_an_empirical_study"
        },
        {
            "paper_title": "Can LLM-Generated Misinformation Be Detected?",
            "rating": 2,
            "sanitized_title": "can_llmgenerated_misinformation_be_detected"
        },
        {
            "paper_title": "Scientific Fact-Checking: A Survey of Resources and Approaches",
            "rating": 1,
            "sanitized_title": "scientific_factchecking_a_survey_of_resources_and_approaches"
        }
    ],
    "cost": 0.015788499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Detect Misinformation in Scientific News Reporting?
22 Feb 2024</p>
<p>Yupeng Cao 
Aishwarya Muralidharan Nair anair9@stevens.edu 
Nastaran Jamalipour Soofi 
K P Subbalakshmi 
John R Wullert IIjwullert@peratonlabs.com 
Chumki Basu cbasu@peratonlabs.com 
David Shallcross dshallcross@peratonlabs.com 
Elyon Eyimife eeyimife@stevens.edu 
Ja- Malipour Nastaran 
K P Soofi 
John R Subbalakshmi 
Chumki Wullert II
David Basu 
2024 Shallcross </p>
<p>Stevens Institute of Technology Hoboken
NJUSA</p>
<p>Stevens Institute of Technology Hoboken
NJUSA</p>
<p>Stevens Institute of Technology Hoboken
NJUSA</p>
<p>Stevens Institute of Technology Hoboken
NJUSA</p>
<p>Stevens Institute of Technology Hoboken
NJUSA</p>
<p>Peraton Labs Basking Ridge
NJUSA</p>
<p>Peraton Labs Basking Ridge
NJUSA</p>
<p>Peraton Labs Basking Ridge
NJUSA</p>
<p>Conference acronym 'XX
03-05, 2018June, WoodstockNY</p>
<p>Can Large Language Models Detect Misinformation in Scientific News Reporting?
22 Feb 20247BE2C445C24A95D65411FC71E4B909BAarXiv:2402.14268v1[cs.CL]Misinformation in Scientific ReportingLarge Language ModelsAI-generated MisinformationExplainability
Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic.Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence.Most research on the validity of scientific reporting treats this problem as a claim verification challenge.In doing so, significant expert human effort is required to generate appropriate claims.Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available.The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting.To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database, a comprehensive collection of COVID-19 and historical coronavirus research.Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles.Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation.We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press.Our first architecture, SERIf, uses three modules: Summarization, Evidence Retrieval, and Inference to classify the news article as fake or true; the second architecture,</p>
<p>INTRODUCTION</p>
<p>Scientific information is communicated to the non-expert audience via popular press (news articles) and online platforms like blogs, social media posts, etc.Studies have shown that news with scientific-sounding content is trusted more than other types [31].Therefore, any misinformation in the scientific domain can cause significant public risk as was evidenced during the recent COVID-19 pandemic [19].Other examples include the emergence of vaccination hesitancy [3,27], eroded trust in health institutions [28], and the amplification of public fear and anxiety [21,37].Therefore, it is imperative to identify misinformation in scientific news reporting.</p>
<p>Several websites are maintained by science reporters (Health News Review1 ) and scientists (Science Feedback2 ) to track scientific misinformation in the media.Although such manual debunking is important, the sheer volume of scientific news can make this task unscalable.</p>
<p>Natural language processing (NLP) based approaches have consequently started to emerge to deal with this problem.These methods typically involve language analysis, like detecting exaggeration [46] and certainty [22] and fact-checking [12] and claim verification [24].Several claim verification datasets have also been developed for this problem [36,42] and a method for modeling information change from scientific article to scientific reporting has also been proposed [48].</p>
<p>While these works have laid the foundation to address this problem, the area is still in its nascence.Several challenges remain unaddressed: 1) there are no existing taxonomies to define dimensions of the scientific validity of scientific news articles that can be used in automated methods to detect misinformation in scientific news; 2) all existing datasets for scientific fact-checking relies on explicit claim generation from the news articles before it can be compared to the scientific articles for misinformation detection.This can be a cumbersome process in real-life scenarios where it would be potentially necessary for expert human involvement to first generate claims from the scientific article and 3) there is no generalized architecture that can detect scientific misinformation without an explicit claim generation step.</p>
<p>In response to the aforementioned limitations, and because of the quantum leap in performance improvement offered by large language models (LLMs) in downstream NLP tasks, we formulate the following research questions:</p>
<p>• RQ1: Can LLMs be used to define a general architecture to detect misinformation in scientific news reporting in simulated real-life scenarios without the need for explicit claims?• RQ2: Is it feasible to define dimensions along which the scientific validity of the news article can be measured and aid in the creation of effective prompts for these architectures?• RQ3: Do these architectures possess the capability to provide explanations for their decision-making processes?</p>
<p>To answer the above questions, we first create a novel dataset, SciNews 3 , comprised of scientific news and related scientific articles.Given the rising trend in LLM-generated content in both legitimate reporting and misinformation, this dataset contains an equal number of LLM-generated and human-written articles.Our dataset construction method does away with extensive human annotation which can prove to be very expensive.The dataset construction pipeline is flexible enough to allow continuous updates with emerging news articles and scientific articles.</p>
<p>We then propose three architectures using LLMs to automatically detect false representations of scientific findings in the popular press without explicit claim generation.The first architecture, SERIf, uses three modules: Summarization, Evidence Retrieval, and Inference to classify the news article as fake or true; the second architecture, SIf, bypasses the explicit evidence retrieval module while keeping the other two, and the third, direct-to-inference architecture, D2I, adopts a direct-to-inference approach, dispensing with both summarization and explicit evidence retrieval.For each of these architectures, we employ several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting.We test these architectures using several state-of-the-art LLMs, including GPT (versions GPT-3.5 and GPT-4) and Llama2 (models with 7B and 13B).</p>
<p>Our extensive experiments conducted on the SciNews dataset have yielded significant findings: 1) we discovered that false scientific content generated by large language models (LLMs) is more challenging to detect compared to misinformation written by humans.2) the SIf architecture arises capable of matching, and in some cases, surpassing the performance of the SERIf architecture.This suggests the feasibility of using LLMs to develop a more generalized architecture for detecting scientific misinformation more appropriate for real-life situations.3) We make the first attempt to define dimensions of scientific validity and incorporate them into the Chain-of-Thought prompt.This novel addition has been instrumental in enhancing the LLMs' ability to make more accurate predictions.4) LLMs can generate plausible explanations for their decision-making process, which can help users understand the model's judgment of news veracity.</p>
<p>RELATED WORK</p>
<p>As mentioned earlier, scientific misinformation detection is still in its nascence and while related to misinformation detection in general, it is a harder problem since the language characteristics of the scientific communication is different from the formal format of scientific publications.The problem of scientific misinformation is related to two other concepts in NLP, including: 1) fact-checking (claim verification) 2) scientific language analysis.However, none of these approaches can singly capture the complexity of scientific misinformation and so far, there has not been any attempt to systematically capture the ways in which scientific misinformation can occur and then to use that to detect scientific misinformation.In this work, we first define some dimensions of scientific validity and then harness the power of LLMs to design general architectures to analyze scientific news for misinformation.</p>
<p>Fact-Checking</p>
<p>Automatic fact-checking, which assesses the truthfulness of claims made in the text [13,38], has been extensively studied across various domains, including common knowledge verification [36], political topics [45], COVID-19 [29], E-commercial [55], biology [9].When thinking about scientific misrepresentation in popular media, it is natural to think of the veracity of scientific findings.It is therefore possible to cast the scientific misinformation as a fact-checking problem or claim verification problem.Several researchers have taken this approach to defect misinformation in the scientific domain [11,[39][40][41][42][43]49].These works typically construct claims from the existing scientific literature by manually reformulated scientific findings and then the constructed claims are verified by utilizing pre-existing knowledge resources.However, most of these works rely on human resources to identify and extract appropriate claims for verification.Furthermore, these artificially constructed claims may not accurately represent the complexity and nuance of claims encountered in real-world scenarios.</p>
<p>Scientific Language Analysis</p>
<p>Scholarly document processing has garnered considerable attention in recent years, reflecting a growing interest in the analysis and interpretation of scientific literature [6].Of particular relevance to our research are tasks that track the change of scientific information from published literature to social press.This includes investigating writing strategies employed in science communication [2,33], detecting changes in certainty [22] and exaggeration detection [47].Furthermore, the automatic detection of semantic similarities between scientific texts and their paraphrases represents an alternative approach for analyzing scientific content [18,23].However, rather than use the typical metrics for measuring semantic similarity, we propose to use the inherent knowledge in pre-trained LLMs for this task.</p>
<p>Large Language Model in Misinformation</p>
<p>LLMs have consistently demonstrated the ability to generate text on par with human authors [51,53].This has led to their widespread use by professionals in generating legitimate real news stories.Unfortunately, they have also been used to generate misinformation [5,7,56] and often at a much larger scale than is humanly possible.While falsehoods crafted by LLMs prove challenging for humans to detect, compared to human-generated ones [7], several studies have illustrated the feasibility of identifying LLM-generated text using [5,34].Motivated by these studies, we include a balanced set of LLM-generated scientific articles, both fake and true, in the SciNews dataset.</p>
<p>SCINEWS DATASET CONSTRUCTION</p>
<p>The first part of this work is to create a dataset, SciNews, of scientific news articles and associated technical scientific publications.The SciNews Dataset contains 2,400 news articles which are labeled: Reliable or Unreliable depending on whether the article represents scientific fact truthfully or not, respectively.The dataset contains 1,200 articles in each category to keep it balanced.Each article is systematically paired with up to three pertinent scientific abstracts from the CORD-19 repository.The CORD-19 is a comprehensive resource of over 1 million scholarly articles, including over 300,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses [44].</p>
<p>Given the growing trend in using Large Language Models (LLMs) to generate legitimate as well as fake news stories [7,56], we include LLM-generated news articles in the dataset.The SciNews dataset contains an equal number (1,200) of human-written and LLM-generated news articles in each category (reliable and unreliable).The statistics of the SciNews dataset are presented in Table 1.We show the construction of SciNews in Figure 1 and describe the construction of the dataset below.</p>
<p>Human-Written News Articles</p>
<p>To gather human-written news articles, we searched for content containing scientific information in existing misinformation datasets and known websites.[8], COVID19-FNIR [30], and COVID-Rumor [10], which are labeled datasets containing human written news articles on COVID-19 from January 2020 through May 2021.</p>
<p>Our search within these datasets commenced with a predefined set of scientific keywords: {scientist, investigating, study finds, experts say, experts recommend}.Using these, we filtered the data to yield 1,190 candidate news pieces.Next, we manually reviewed each candidate to sift out articles without scientific content or irrelevant to COVID-19.The reason we eliminated articles that are irrelevant to COVID-19 was because we will be including scientific data from the CORD-19 as evidence.This process resulted in 223 news articles: 130 reliable and 93 unreliable.</p>
<p>Web-Based Collection.</p>
<p>In order to expand the dataset to cover the latest discussions on COVID-19, we crawled both credible and dubious websites for data.To collect unreliable data, we referred to Wiki Fake News Website List4 , crawled the listed sites for articles, and manually verified the content.We eliminated articles exhibiting blatant discrimination or prejudice and conspicuous propaganda devoid of substantial scientific dialogue.This process yielded 507 unreliable articles that contained discussions pertaining to COVID-19 and were grounded in a scientific context.</p>
<p>For reliable data, we restricted the range of sources for the news articles to a set of educational press sites and other well-regarded news websites.Appendix A.1 lists all the educational press sites.The full list of known trustworthy websites we consulted is included in Appendix A. 2 The target news articles were collected by web crawling, anchoring our search with our set of scientific keywords augmented by two topical ones: COVID-19 and Vaccine.Each article was reviewed by the same set of annotators, ensuring a direct correlation with the referenced research papers.The content is scraped from the web to extract body text, title, and other data needed for data construction.We gathered 470 reliable articles from varied reputable sources in this way.This process gave us a combined total of 1,200 human-written news articles (600 reliable, 600 unreliable) spanning from January 2020 to October 2023.</p>
<p>LLM-Generated News Articles</p>
<p>Generating true articles using LLMs is fairly straightforward.However, since most LLMs come with guardrails to protect them from misuse, we used a jailbreak strategy to generate scientific misinformation.The step-by-step process to generate news articles using LLMs is described below.</p>
<p>Selecting Scientific</p>
<p>Abstracts.First, we curate a collection of abstracts from the CORD-19 database.These abstracts served as the foundational resource for the subsequent generation process.The CORD-19 database contains more than 1,000,000 articles in the medical field.Typically, most widely distributed science articles tend to get more media attention.Hence, we focused on the most frequently cited papers from CORD-19.The CORD-19 organizes its articles using seven principal elements including {title, abstract, doi, PubMed ID, PMCID, JSON file ID, and XML ID}.We start our curation with papers that have all 7 elements.We then confine our focus to post-January 2020, subsequently filtering out off-topic data using the keyword set: {COVID-19, Corvarius, and Vaccine}.To further ensure that generated articles in our dataset are of high quality, we narrowed our selection to articles published in well-regarded journals spanning a spectrum from basic science (examples include 'Cell' and 'Nature') to medicine (such as the 'British Medical Journal').The comprehensive list of these journals is detailed in Appendix A.3.From this refined pool, we handpicked the abstracts of over 2,000 highly cited articles, thereby forming a rich and diverse foundation for our dataset.</p>
<p>Jailbreak</p>
<p>Prompts.We utilized the curated collection of abstracts as guiding resources for LLMs to generate both reliable and unreliable scientific articles.To mitigate the risk of generating harmful content, LLMs are subjected to an alignment process, complemented by the setup of predefined prompts serving as security measures [1,26,35].Therefore, requests for LLMs to generate fake messages are usually denied.In light of these constraints and inspired by the Jailbreak Attacks [16,17,20,32], we designed a 'Jailbreak' prompt to enable the LLMs to generate fake newsoriented scientific articles that were both informative and contextually aligned with the provided abstracts.The designed jailbreak prompt is illustrated in Figure 2. Given selected scientific abstract resources, we simulated a scenario where we acted as the instructor of a science class, prompting the large language model (LLM) to create two types of articles: a 'True Article' and a 'Convincing False Article'.The objective was to use these articles as teaching tools to help students discern between authentic scientific knowledge and fabricated information.By including both true and false LLM-created content we can ensure that the systems trained on our dataset to detect false content are not simply detecting LLM-generated content, but would be adept at distinguishing between true and false content.The generated articles are generally in the style of a news article, with many including an explicit title, to mimic human-generated scientific news.In consideration of the cost implications associated with different LLMs 56 , we primarily utilized Llama2-7B for generating the bulk of data samples, supplemented by a smaller set from GPT-3.5.After filtering (see subsection 3.4), this approach led to the creation of 1,000 data samples from Llama2-7b and 200 from GPT-3.5, reflecting a balance between quality and resource optimization.</p>
<p>Evidence Corpus Creation</p>
<p>To augment the constructed dataset, we matched as many as three scientific abstracts per news article as evidence resources.For both Human-Written and LLM-generated News Articles, we employed Vespa 7 to identify relevant abstracts from CORD-19 based on BM25 scoring for each article.While most articles were matched with three corresponding abstracts, a few could only be paired with two or even just one.This led to the creation of a fixed evidence corpus comprising 7,087 pieces of paragraph-level evidence.While this evidence corpus remains static at this juncture, its design allows for future expansion.</p>
<p>Quality Control</p>
<p>The quality control team consists of 4 graduate students and 5 senior researchers with a background in NLP.For the Human-Written News Article subset and LLM-Generated subset, the team used different strategies to examine the data quality.</p>
<p>For the collection of human-written news articles from various sources, we referred to the guidelines outlined in [25].Based on its principles and our specific needs, we developed an instruction guide, which can be found in Appendix A.5.To ensure uniformity and understanding of the task, all team members thoroughly reviewed this guide.An additional layer of quality assurance involved crosschecking the collected data among team members.This step was implemented to mitigate any potential biases and to guarantee that the data aligned with our collection criteria.</p>
<p>Regarding the LLM-generated articles, team members manually assessed the generated content.When instructed to do so, the LLM generated many types of falsehoods and often provided explanations of them, even though it was not prompted to explain.The falsehoods included features such as changing quantitative data (e.g., altering numeric percentages and statistical certainty levels), exaggeration (e.g., adding "superhuman strength" to the list of benefits), and omitting key information to support alternate conclusions.In other cases, the model generated text that completely reversed the claims in the original abstract.Even the True summaries included fabrications in some cases, with the model occasionally citing an imagined journal or generating quotes from made-up scientists that were in keeping with the original abstract content.Our sampling and manual review revealed that in some cases, the fabrications in the True summaries altered the overall validity of the summary.In such cases, we observed significant linguistic differences between the original abstract and the true summaries.Manual evaluation on two samples of 50 documents showed that when the ROUGE-2 similarity [15] between the abstract and true summary exceeded 0.4, the likelihood of an invalid true summary was 2% while when the ROUGE score was 0.4 or</p>
<p>DIMENSIONS OF SCIENTIFIC VALIDITY AND PROPOSED ARCHITECTURES</p>
<p>In order to develop automated methods to detect scientific misinformation in real-world situations, we first develop dimensions of scientific validity.These will later be used in the chain-of-thought prompts for the LLMs.</p>
<p>Dimensions of Scientific Validity</p>
<p>We define the following dimensions of scientific validity in the reporting.We note that this is not an exhaustive list of ways in which scientific validity may be compromised, however, to the best of our knowledge, this is the first attempt to systematically define directions of scientific validity in science news reporting for the design of automated science misinformation detection.</p>
<p>(1) Alignment: The news paragraph may show different levels of alignment with the evidence sentences.Alignment in this case is defined as news and evidence representing the same meaning about one scientific content.(2) Causation confusion: The news article may confuse correlations presented in the scientific literature as causation.This could be one dimension in which the scientific validity is compromised.(3) Accuracy: This refers to how accurately the news item describes the scientific findings quantitatively and qualitatively (4) Generalization: This refers to overgeneralization or oversimplification of the findings reported in the scientific literature.(5) Contextual Fidelity: Does the news article retain the broader context of the scientific finding?</p>
<p>Proposed Architectures</p>
<p>Conceptually, we may think of the process of automatically detecting scientific misreporting (or mis/disinformation in science news reporting) "in the wild" as comprising of three elements: (1) understanding the gist of the news article;</p>
<p>(2) comparing it to relevant information from scientific articles and (3) inferring if the news is reliable or unreliable.To this end, we propose three architectures with varying degrees of granularity.These architectures use LLMs and several prompting strategies for the different modules.Note that we do not require a separate claim generation module in any of the architectures.These architectures are depicted in Fig. 3.</p>
<p>In order to account for potential differences in performance between different prompting strategies and LLMs, each of these architectures are tested against multiple prompting strategies and LLMs and the results are described in Section 5.</p>
<p>4.2.1</p>
<p>The SERIf Architecture.The first proposed architecture, Summarization Evidence Retrieval Inference (SERIf), treats each of the conceptual elements above, as a separate module.Hence, it contains three modules: 1) Summarization; 2) Evidence Retrieval and 3) Inference.The summarization module distills the key information from the news articles, thereby streamlining the analysis process.The Evidence Retrieval module is responsible for identifying and extracting sentences from the scientific articles in our dataset that may validate or contradict the statements in the news article.This process aids in gathering relevant contextual evidence for further analysis.The Inference module categorizes the news articles into two reliable or unreliable, based on the evidence from the scientific dataset.</p>
<p>Summarization module.</p>
<p>Inspired by the recent success of text summarization [54] using LLMs, we take the 'Extractive -Abstractive' two-step summarization strategy [52] to construct a summary of the news article.The extractive summarization process summarizes the article by identifying and concatenating the most salient sentences from it, ensuring that the extracted summaries are consistent with the original text.The resulting summaries serve as a foundation for the abstractive summarization which uses a generative approach to create a more concise and cohesive summary.By synthesizing the extractive and abstractive approaches, the module ensures a balance between accuracy (adherence to the original text) and brevity (conciseness and essence of the content), thereby providing an effective and reliable summarization for further analysis in the misinformation detection process.</p>
<p>Formally, for a document composed of  sentences, the extractive summarization process creates an extractive summary,   , consisting of  ≪  sentences.Then, the LLM, , creates an abstractive summary using a query,  and   as input:   =  (,   ).</p>
<p>As a sanity check, to verify the quality of the summary, we randomly selected 200 samples for annotation by two graduate students with a background in NLP, and evaluated the summaries using four criteria: 1) quality of extractive summary (High vs Low); 2) quality of abstractive summary (High vs Low); 3) presence of hallucination in abstractive summary (Yes vs No); 4) comparison of Extractive and Abstractive Summaries.We used the Krippendorff's alpha score to evaluate the agreement between the annotations [14].The alpha scores for the four aspects were 0.53, 0.82, 0.91, and 0.94, respectively.These values suggest that there is strong agreement that the abstractive summaries effectively encapsulate the core information of the original texts and maintain a high degree of consistency.Consequently, we have decided to use the abstractive summaries in subsequent steps of our analysis.</p>
<p>Sentence-level Evidence</p>
<p>Retrieval.The process of key evidence selection in our The evidence retrieval module involves extracting key sentences from scientific articles that may support or refute the claims of the news article.This task bears a resemblance to paragraph retrieval but operates at a finer granularity.It essentially constitutes a semantic matching challenge, where each sentence within a paragraph undergoes a comparison against a specific statement query.The objective is to pinpoint the most relevant evidence interval within these sentences.</p>
<p>A critical step in refining this process was pre-defining our evidence corpus using CORD-19.This strategic choice significantly narrows down the search space to a manageable scope, allowing for efficient traversal through all relevant paragraphs to locate key evidence.We use an LLM to enhance the effectiveness and accuracy of our sentence selection process.</p>
<p>Given an abstractive summary,   , and candidate scientific abstract, , we select sentences   from : {  } =  (  , ), where {  } is a set that contains all relevant and important sentences selected by LLM based on semantics.</p>
<p>Inference Module.</p>
<p>The Inference Module is dedicated to assessing the the veracity of the summarized news paragraph using the set of retrieved evidence sentences using the abstractive summary,   , and the selected evidence sentence set {  }.Thus, the inference module produces a binary output (reliable or unreliable) for each &lt;   , {  } &gt; pair.</p>
<p>4.2.5</p>
<p>The SIf Architecture.In this architecture, we remove the evidence retrieval module from the previously described SERIf architecture.In the SIf architecture, the Summarization module works exactly as described in Section 4.2.1.The LLM in the Inference module is now directly prompted to decide whether the given news summary is trustworthy or not and to provide justifications based on the paired scientific abstracts from the evidence corpus in the SciNews dataset.</p>
<p>4.2.6</p>
<p>The Direct to Inference (D2I) Architecture.In the third architecture, there is no summarization module or explicit evidence retrieval module.Instead, the LLM is directly fed the scientific article, and the corresponding scientific abstracts and prompted to determine whether the news item is trustworthy with justifications.</p>
<p>When viewed from the perspective of identifying scientific misinformation "in the wild", the D2I is the architecture that does little in the way of processing and the SERIf architecture involves the most processing.In other words, the SERIf requires engineering each aspect of the elements of scientific misinformation separately, the D2I architecture requires very little engineering and the SIf falls between these two.However, as noted earlier, none of these architectures expect an explicit set of claims to be generated from the news article for misinformation detection.</p>
<p>Prompt Strategies.</p>
<p>A key factor in the performance of an LLM-based task is prompt engineering.Several kinds of prompting strategies have been used in various applications with varying degrees of success in specific tasks.</p>
<p>• Zero-shot prompting: LLMs are presented with a task without any prior specific training or examples related to that task [4].We test the performance of LLM on scientific misinformation detection by zero-shot prompts directly using the pre-existing knowledge of the LLMs.• Few-shot prompting: In contrast to zero-shot prompting, where LLMs are presented with tasks without prior training or examples, few-shot prompting involves furnishing LLMs with a concise set of examples prior to task execution [4].This approach is designed to provide the model with essential context, thereby augmenting its capability for tasks like detecting scientific misinformation.In our methodology, we supply the LLMs with two carefully selected examples: one deemed 'reliable' with accompanying reasoning, and another labeled 'unreliable'.This strategy is intended to better equip the LLMs to discern and categorize information accurately in the execution of the task.• Chain-of-thought prompting (CoT): CoT prompting involves structuring prompts to elicit a step-by-step reasoning process, effectively emulating the cognitive process humans employ in solving complex problems [50].In our approach, we used the dimensions of scientific validity defined in Section 4.1 to design multiple CoT prompts to guide the LLMs.This methodology not only aids the LLMs in systematically dissecting and assessing factual content but also aligns their reasoning process with structured, human-like analytical methods.</p>
<p>RESULTS AND DISCUSSIONS</p>
<p>We used GPT-3.5,GPT-4, Llama2-7B, and Llama2-13B in our architectures with the temperature set to 0. This setting ensures that the LLMs generate responses with the highest predictability, thereby maintaining uniformity across the experiments.The performance of each of the proposed architectures, using each of the above LLMs is measured using accuracy, precision, recall, and F1-score.These results are displayed in Tables 2 and 3. We note immediately that there is a significant difference between the performances of GPT and Llama2, with both GPT-3.5 and GPT-4 beating Llama2-7B/13B.All Llama2-7B/13B models achieved an accuracy score barely above random guessing.This may be due to the greater popularity of GPT, which may have made the crowdsourced, ongoing training of the GPT models stronger than those of Llama2.</p>
<p>Human-Written vs LLM-Generated Misinformation</p>
<p>Table 2 records the results of our experiments on the SERIf architecture and Table 3 shows the performance of the SIf and D2I architectures.From these tables, we note that there is a significant difference in misinformation detection accuracy and F1 scores for the human-written articles and LLM-generated articles.Across all architectures, it is consistently more challenging to identify LLMgenerated scientific misinformation compared to human-authored misinformation.This is evidenced by high recall scores paired with low precision scores, indicating a tendency of the detectors to classify news as True.Such a trend highlights the difficulty in discerning false information within LLM-generated scientific news.This notable discrepancy emphasizes the complexity and challenge inherent in detecting misinformation in LLM-generated content.It aligns with similar findings in non-scientific misinformation domains [7].These results also raise concerns about the potential misuse of LLMs and underscore the importance of advancing our detection methodologies to keep pace with the evolving capabilities of LLMs, considering their implications for public safety.</p>
<p>Comparison Across Architectures (RQ1)</p>
<p>The comparative analysis revealed a significant performance superiority of GPT over Llama2.Therefore, GPT was selected for further evaluation of the various architectures proposed in this work.According to the result presented in Table 3, it is evident that the SIf architecture yields the highest accuracy in processing humanauthored articles, achieving an impressive 82.50% in accuracy and 83.50% in F1 score.The encouraging results, even in the absence of the 'sentence-level evidence retrieval' module, suggest the potential to develop more flexible and generalized scientific misinformation detection models.In stark contrast, the performances of the D2I models were notably subpar.This disparity indicates that merely supplying a Large Language Model (LLM) with a news article coupled with scientific articles does not suffice to guarantee effective performance.Furthermore, our results highlight the importance of the 'summarization' module.By distilling key statements from the news, this module minimizes the impact of extraneous information, thereby enhancing the LLM's ability to generate more accurate predictions.</p>
<p>Comparison Across Prompting Strategies (RQ2)</p>
<p>Observing the overall performance from Table 2 and 3, the results reveal a significant trend: the CoT prompting generally outperforms the zero-shot approach.Notably, for LLM-generated data, the CoT prompt markedly enhances detection capabilities.This suggests that our defined dimensions of scientific validity effectively aid LLMs in making more accurate predictions.However, an interesting observation in the SIf architecture for human-authored content is that, while the CoT prompt led to higher precision, it resulted in lower recall compared to the zero-shot approach.This indicates that our scientific validity dimensions sometimes prompt the LLM to apply more stringent criteria in its judgments.</p>
<p>On the other hand, the few-shot setting did not significantly improve performance, implying that despite providing two wellcrafted examples (one positive and one negative), it is challenging for LLMs to extract substantial features from the provided cases.This not only highlights the complexity of scientific misinformation detection but also underscores the intricate nature of the potential scientific misinformation data involved.</p>
<p>Explainability Study (RQ3)</p>
<p>To assess the explainability of the proposed architectures, we prompt the LLMs to not only classify the news articles as reliable or unreliable, but also to explain the reasoning behind these classifications.An example of the prompt and the result are presented in Table 4.The response illustrates the LLM's ability to the content of the news article against corresponding evidence and articulates the differences, leading to explanations that are comprehensible to users.This feature is crucial as it aids individuals in discerning the veracity of scientific content, enhancing their understanding of what constitutes true versus false science.We provide the complete example in the appendix C.</p>
<p>CONCLUSIONS</p>
<p>In this paper, we explore LLMs for identifying unreliable scientific news 'in the wild.' We developed the SciNews dataset, comprising equal parts human-written and LLM-generated news articles, paired with related scientific articles for validation.We established dimensions of scientific validity specific to news misinformation and proposed three LLM-based architectures for detecting untrustworthy scientific news.Our tests across various LLMs and prompting strategies yielded key insights: 1) the effectiveness of prompting strategies in adhering to scientific validity, 2) the ability of LLMs to detect human-written misinformation with over 80% accuracy using suitable architectures and prompts, 3) the greater challenge in identifying LLM-generated misinformation, and 4) the capacity of LLMs to provide rationales for their classifications.</p>
<p>A.5 An Example of LLM-Generated Data Sample</p>
<p>Figure 4 showcases a Llama2-7B-generated LLM article example.The 'True Article' mirrors the original scientific abstracts accurately, whereas the 'Convincing False' modifies key details, including experimental effects and scientific conclusions.This indicates that even LLMs with smaller parameters, such as llama2-7B, are capable of producing scientific disinformation.Fabricating and exaggerating medical research findings could lead to public complacency in pandemic situations, resulting in greater harm.This example not only validates our jailbreak prompt approach but also underscores the public safety risks associated with LLMs.</p>
<p>B A SUMMARY OF PROMPT EXAMPLES</p>
<p>In our experiments, we employed various prompt strategies: zeroshot, few-shot, and Chain-of-Thought (CoT).For the different architectures, the prompt remained consistent across all strategies.Table 5: An example of the Zero-Shot prompt.</p>
<p>System Message: As a Fact Checker, your role involves analyzing a news paragraph and several evidence sentences provided by the user.The user will present a news paragraph.Following this, the user will present evidence sentences.Your task is to determine the factual accuracy of the news story based on these evidence sentences.To justify your conclusion, select and reference specific phrases or sentences from both the news story and the evidence provided.</p>
<p>User Message: I will give you one news paragraph and several relevant sentences.Please help me determine if these sentences support or refute the news point of view.Finally, please answer using one word 'refute' or 'support' and give reasons.Please provide the final output in JSON format containing the following two keys: prediction and reason.Table 6: An example of the Few-Shot prompt.</p>
<p>System Message: As a Fact Checker, your role involves analyzing a news paragraph and several evidence sentences provided by the user.The user will present a news paragraph.Carefully read this paragraph to understand its central claim.Following this, the user will present evidence sentences.These sentences may either support or refute the news paragraph's central claim.Your task is to determine the factual accuracy of the news story based on these evidence sentences.Are they supporting or contradicting the news?To justify your conclusion, select and reference specific phrases or sentences from both the news story and the evidence provided.Table 7: An example of the Chain-of-Thought (CoT) prompt involves integrating our defined dimensions of scientific validity into the thought process, guiding the LLM toward making decisions.</p>
<p>System Message: You are a Fact Checker.The user will present a new paragraph.Following this, the user will present evidence paragraphs.These sentences may either support or refute the news paragraph's central claim.Your task is to determine the factual accuracy of the news story based on these evidence paragraphs.Make a final prediction and provide a comprehensive explanation step by step based on the following: Alignment Check: examine the evidence for alignment with the news paragraph Causation confusion: evaluate if the news paragraph confuses correlation with causation Accuracy: verify quantitative and qualitative accuracy in the news paragraph compared to evidence Generalization: assess if the news paragraph overgeneralizes or oversimplifies findings from evidence sentences Contextual Fidelity: consider the broader context surrounding the news and evidence.</p>
<p>User Message: I will give you one news paragraph and relevant evidence corpus.Please help me determine if these paragraphs support or refute the news point of view.Finally, please answer using one word 'refute' or 'support' and give reasons.Please provide the final output in JSON format containing the following two keys: prediction and reason.</p>
<p>C MORE DETAILS OF EXPLAINABILITY STUDY</p>
<p>The example in figure 5 is from SERIf architecture under the CoT prompt and GPT-4.It shows how the 'sentence-level evidence retrieval' module effectively identifies relevant statements from the evidence corpus, detects contradictions between the original text and evidence, and makes accurate predictions during the 'Inference' phase based on the predefined dimensions of scientific validity.Such effective explanations enhance understanding of the reasoning process.</p>
<p>Figure 1 :
1
Figure 1: The dataset construction process: ① utilizing publicly available datasets as well as web resources to collect humanwritten scientific news related to COVID-19 (Subsection 3.1), ② selecting abstracts from CORD-19 as resources to guide LLMs to generate articles using jailbreak prompt (Subsection 3.2), ③ the dataset is augmented with evidence corpus drawn from CORD-19 (Subsection 3.3).</p>
<p>Figure 2 :
2
Figure 2: Schematic of the designed jailbreak prompt.</p>
<p>Figure 3 :
3
Figure 3: Proposed Architectures.SERIf includes all three modules: Summarization, Sentence-level Evidence Retrieval, and Inference Module.SIf bypasses the evidence retrieval module while keeping the other two.D2I removes both the summarization and the explicit evidence retrieval module.</p>
<p>Figure 4 :
4
Figure 4: An example of LLM-generated articles by using JailBreak prompt.</p>
<p>User Message: Task: Analyze the following news paragraph and several relevant sentences to determine their relationship.Example 1: {One positive example with label and reason.}Example 2: {One negative example with label and reason.}Now analyze the following: {News Paragraph} and {Evidence Corpus} Instructions: Decide if the relevant sentences 'support' or 'refute' the point of view of the news paragraph.Provide your answer in one word -either 'support' or 'refute'.Then, explain your reasoning in a few sentences.Output: format your response as JSON with two keys: prediction and reason.</p>
<p>Figure 5 :
5
Figure 5: An example of explainability study.</p>
<p>Table 1 :
1
Distribution of article labels in SciNews Dataset.
Human-Written LLM-Generated TotalReliable6006001200Unreliable6006001200Total1200120024003.1.1 Leveraging Publicly Available Dataset. We leveraged MM-CoVaR</p>
<p>Table 2 :
2
Performance results of our proposed SERIf architecture using different LLMs and prompt strategies.
ModelsPrompt StrategyHuman-Written Accuracy Precision RecallF1LLM-Generated Accuracy Precision RecallF1Overall Accuracy Precision RecallF1Zero-Shot74.2574.9572.8373.8766.7560.8893.6673.8070.5067.9283.2373.84GPT-3.5Few-Shot70.0071.6466.6068.9568.1462.1292.8274.4369.0766.8879.7170.49CoT76.6776.2077.6776.9266.7560.6695.3374.1471.7168.4386.5075.53Zero-Shot77.3376.4879.0077.7270.2562.9198.6776.8373.7969.7088.3477.26GPT-4Few-Shot75.0874.6076.0075.3070.1762.8598.3276.6872.6368.7387.1675.99CoT79.5876.9085.0080.7267.2560.5099.3375.2073.4268.7092.1777.96Zero-Shot56.0053.2498.3369.1051.1750.6096.8066.5053.8951.9297.5767.80LLAMA2-7BFew-Shot54.7558.2093.3071.7052.0051.0097.3067.0052.3854.6095.3069.35CoT56.8359.2097.8073.7051.5850.8096.8066.7054.2155.0097.3070.20Zero-Shot57.3359.5098.5074.2053.6757.1097.0071.9055.5058.3097.7573.05LLAMA2-13BFew-Shot56.9159.5095.8073.4052.3351.2097.5067.2054.6255.3596.6570.30CoT58.0059.9099.0074.6055.0852.7098.3068.6056.5456.3098.6571.60</p>
<p>Table 3 :
3
Performance results of our proposed SIf and D2I architectures using different LLMs and prompt strategies.
Models ArchPrompt StrategyHuman-Written Accuracy Precision RecallF1LLM-Generated Accuracy Precision RecallF1Overall Accuracy Precision RecallF1Zero-Shot82.5080.0086.5083.5062.0057.0099.0072.0072.2568.5089.1377.75SIfFew-Shot78.6779.7676.8378.2765.3359.8992.8372.8172.0069.8384.8375.54GPT3.5CoT76.0879.8873.6775.6870.1765.1286.8374.4373.1372.5080.2575.06Zero-Shot66.6066.5564.3365.4263.4257.8598.3372.9865.0162.2081.3369.20D2IFew-Shot65.6063.3067.3365.2562.7563.8097.5377.1064.1863.5581.3371.18CoT77.1769.5096.8380.9164.0857.6399.6573.0370.6363.5798.2476.97Zero-Shot80.0080.0079.0080.0071.0064.0098.0077.0075.5072.0088.5078.50SIfFew-Shot70.0875.9158.8366.2971.7564.1798.5077.7170.9270.0478.6772.00GPT-4CoT73.9282.3860.8369.9871.0865.7987.8375.2372.5074.0974.3372.61Zero-Shot68.0866.8072.0069.3065.0059.0098.5073.8066.5062.9085.2573.80D2IFew-Shot70.0071.4066.7069.0068.1462.2092.8274.5069.0766.8079.7171.75CoT78.0884.6068.6775.8072.0065.2098.5078.3075.0474.9083.5677.05</p>
<p>Table 4 :
4
Example of prompt and response for generating explanations.Decide if the evidence 'support' or 'refute' the point of view of the news paragraph.Provide your prediction and explain your reasoning in a few sentences.
Response: Refute. The evidence provided does not support the claimmade in the news paragraph. While the evidence mentionsneurological complaints reported by vaccinated individuals, they donot provide any evidence that suggests COVID-19 mRNA vaccinescause extensive brain damage. Additionally, the fact that differentvaccines produced different profiles of complications does notnecessarily indicate brain damage. The study's authors calling forfurther investigations does not confirm the claim either.
Prompt:</p>
<p>https://www.healthnewsreview.org/
https://science.feedback.org/
We will make our dataset fully accessible to the research community
https://en.wikipedia.org/wiki/List_of_fake_news_websites
https://openai.com/pricing
llama.meta.com
https://cord19.vespa.ai/ below, the likelihood of an invalid true summary was 30%. Thus, we filtered the data set to only accept summaries with ROUGE-2 scores above 0.4.
ACKNOWLEDGMENTSTo Robert, for the bagels and explaining CMYK and color spaces.We collected data from the following educational press sites:YaleNews, Yale School of Medicine Latest News, Boston University -University News, Boston College BC News, University of Washington School of Medicine Newsroom, Regenstrief Institute News, University of South Carolina In the News, University of Utah Unews, Colorado State University Source, University of Kansas Medicine Center News, University of Michigan News, University of Nebraska Medicine News &amp; Events, University of Maryland School of Medicine News, Stanford News, Stanford Medicine News Center, University of Mississippi Medical Center News Stories, Washington University School of Medicine in St. Louis News, Center for Education Policy Research at Harvard University News, Johns Hopkins Bloomberg School of Public Health Articles &amp; News Releases, University of Missouri School of Medicine News, University of Hawaii News, The Ohio State University Wexner Medical Center Press Releases, Oregon State University Newsroom, University of Minnesota News and Events, Emory University Emory News Center, Tufts Now, University of Kentucky College of Medicine News, University of Calgary UCALGORY News, Texas AM Today, Duke Today, North Carolina State University NC State News, Vanderbilt University Research News, University of Toronto U of T News, McMaster University Daily News, University of Virginia UVAToday, University of New Hampshire Newsroom, Rutgers University -Rutgers Today, UT Southwestern Research Labs News, University of Houston UH Newsroom, University of Oxford News, Queen Mary University of London Queen Mary News, University of York News, The BMJ News, JAMA Health Medical News, Nature News, Allen Institute News, National Institutes of Health News and Events.A MATERIALS TO SCINEWS DATASET CONSTRUCTION A.1 Educational Press SitesA.2 News SourcesWe extracted data from the following news websites: CNBC, The Washington Post, The Atlantic, CNN, NPR, BBC, Forbes, USA Today, Bloomberg, Daily Mail, CBC, News Medical, ABC News, CBS News, The Economic Times, and OHSU News.Although these sources are trustworthy on the whole, there can still be some biased content.Our team double-checked the content.Only after a rigorous verification process was an article deemed suitable for inclusion in our dataset.A.3 Journal ListAcademic articles in journals with good reputations are more likely to attract attention and be widely disseminated.Therefore, we select abstracts of high-quality articles from the CORD-19 database based on the following list to be used as resources for LLM-generated articles:Nature, Science, British Medical Journal, Journal of Medical Virology, BMC Medicine, Blood, Nature Cell Biology.A.4 Review Guidance for Human-Written ArticlesTo ensure that our dataset covers only scientific-related content and does not include politics, economics, etc., we apply the following guidance to check each collected human-written article:The article can include:• After reading through the title and body text, the main content is the discussion of scientific discoveries or scientific progress.• The title contains obvious scientific vocabulary: such as investigating, study finds, scientist, experts say, and 'experts recommend'.• The title reads as a scientifically relevant conclusion or discussion: • The main body content is some news summary or news paraphrase.The article cannot include:• Live News Style Title.• Explicit political information.• Contains other information such as finance and marketing in the title.• If First-person pronouns appear in the title, it should be noted that it is not a science-related discussion.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Writing strategies for science communication: Data and computational analysis. Tal August, Lauren Kim, Katharina Reinecke, Noah A Smith, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Annalise Baines, Muhammad Ittefaq, Mauryne Abwao, # Scamdemic,# Plandemic, or# Scaredemic: what Parler social media platform tells us about COVID-19 vaccine. 2021. 20219421</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Truth, lies, and automation: How language models could change disinformation. Ben Buchanan, Andrew Lohn, Micah Musser, 2021Center for Security and Emerging Technology</p>
<p>Overview of the first workshop on scholarly document processing (SDP). Muthu Kumar Chandrasekaran, Guy Feigenblat, Dayne Freitag, Tirthankar Ghosal, Eduard Hovy, Philipp Mayr, Michal Shmueli-Scheuer, Anita De Waard, Proceedings of the first workshop on scholarly document processing. the first workshop on scholarly document processing2020</p>
<p>. Canyu Chen, Kai Shu, arXiv:2309.137882023. 2023Can LLM-Generated Misinformation Be Detected? arXiv preprint</p>
<p>MMCoVaR: multimodal COVID-19 vaccine focused data repository for fake news detection and a baseline architecture for classification. Mingxuan Chen, Xinqiao Chu, Subbalakshmi, Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining2021</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022. 2022arXiv preprint</p>
<p>A COVID-19 rumor dataset. Mingxi Cheng, Songli Wang, Xiaofeng Yan, Tianqi Yang, Wenshuo Wang, Zehao Huang, Xiongye Xiao, Shahin Nazarian, Paul Bogdan, Frontiers in Psychology. 126448012021. 2021</p>
<p>Climate-fever: A dataset for verification of real-world climate claims. Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, Markus Leippold, arXiv:2012.006142020. 2020arXiv preprint</p>
<p>A survey on automated fact-checking. Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos, Transactions of the Association for Computational Linguistics. 102022. 2022</p>
<p>A survey on automated fact-checking. Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos, Transactions of the Association for Computational Linguistics. 102022. 2022</p>
<p>Answering the call for a standard reliability measure for coding data. F Andrew, Klaus Hayes, Krippendorff, Communication methods and measures. 12007. 2007</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Autodan: Generating stealthy jailbreak prompts on aligned large language models. Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao, arXiv:2310.044512023. 2023arXiv preprint</p>
<p>Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Yang Liu, arXiv:2305.13860Jailbreaking chatgpt via prompt engineering: An empirical study. 2023. 2023arXiv preprint</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Dan S Weld, arXiv:1911.02782S2ORC: The semantic scholar open research corpus. 2019. 2019arXiv preprint</p>
<p>Leveraging media and health communication strategies to overcome the COVID-19 infodemic. Nour Mheidly, Jawad Fares, Journal of public health policy. 412020. 2020</p>
<p>. Zvi Mowshowitz, n. d.</p>
<p>. Jailbreaking chatgpt on release day. </p>
<p>The danger of misinformation in the COVID-19 crisis. Taylor Nelson, Nicole Kagan, Claire Critchlow, Alan Hillard, Albert Hsu, Missouri Medicine. 1175102020. 2020</p>
<p>Measuring sentence-level and aspect-level (un) certainty in science communications. Jiaxin Pei, David Jurgens, arXiv:2109.147762021. 2021arXiv preprint</p>
<p>Semeval-2023 task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. Jakub Piskorski, Nicolas Stefanovitch, Giovanni Da San, Preslav Martino, Nakov, Proceedings of the 17th International Workshop on Semantic Evaluation. the 17th International Workshop on Semantic Evaluation2023SemEval-2023</p>
<p>Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin, arXiv:2010.11930Scientific claim verification with VerT5erini. 2020. 2020arXiv preprint</p>
<p>Natural Language Annotation for Machine Learning: A guide to corpus-building for applications. James Pustejovsky, Amber Stubbs, 2012O'Reilly Media, Inc</p>
<p>Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson, arXiv:2310.03693Fine-tuning aligned language models compromises safety, even when users do not intend to!. 2023. 2023arXiv preprint</p>
<p>Covid-19 vaccination hesitancy. Mohammad S Razai, A R Umar, Katja Chaudhry, Linda Doerholt, Azeem Bauld, Majeed, Bmj. 3732021. 2021</p>
<p>Public trust, misinformation and COVID-19 vaccination willingness in Latin America and the Caribbean: today's key challenges. The Lancet Regional Health. Alfonso J Rodriguez-Morales , Oscar H Franco, Americas. 32021. 2021</p>
<p>COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic. Arkadiy Saakyan, Tuhin Chakrabarty, Smaranda Muresan, arXiv:2106.037942021. 2021arXiv preprint</p>
<p>COVID-19 fake news infodemic research dataset (COVID19-FNIR dataset). Julio A Saenz, Sindhu Reddy Kalathur, Diksha Gopal, Shukla, IEEE Dataport. 2021. 2021</p>
<p>Can science literacy help individuals identify misinformation in everyday life?. J Aviv, Ayelet Sharon, Baram-Tsabari, Science Education. 1042020. 2020</p>
<p>Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang, arXiv:2308.03825Characterizing and evaluating in-the-wild jailbreak prompts on large language models. 2023. 2023arXiv preprint</p>
<p>A corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in communication. Chenhao Tan, Lillian Lee, arXiv:1405.14392014. 2014arXiv preprint</p>
<p>The science of detecting llm-generated texts. Ruixiang Tang, Yu-Neng Chuang, Xia Hu, arXiv:2303.072052023. 2023arXiv preprint</p>
<p>Bing conversational experiences and image creator terms. Bing Termsofusebing, 2023. 2023</p>
<p>FEVER: a Large-scale Dataset for Fact Extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, NAACL-HLT. 2018</p>
<p>Examining the impact of sharing COVID-19 misinformation online on mental health. Gaurav Verma, Ankur Bhardwaj, Talayeh Aledavood, Munmun De Choudhury, Srijan Kumar, Scientific Reports. 1280452022. 2022</p>
<p>Fact checking: Task definition and dataset construction. Andreas Vlachos, Sebastian Riedel, Proceedings of the ACL 2014 workshop on language technologies and computational social science. the ACL 2014 workshop on language technologies and computational social science2014</p>
<p>Juraj Vladika, Florian Matthes, arXiv:2305.16859Scientific Fact-Checking: A Survey of Resources and Approaches. 2023. 2023arXiv preprint</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, arXiv:2004.149742020. 2020arXiv preprint</p>
<p>David Wadden, Kyle Lo, arXiv:2107.08188Overview and insights from the SCIVER shared task on scientific claim verification. 2021. 2021arXiv preprint</p>
<p>David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi, arXiv:2210.13777SciFact-open: Towards open-domain scientific claim verification. 2022. 2022arXiv preprint</p>
<p>Gengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram, Melanie Subbiah, Kathleen Mckeown, COVID-19Check-COVID: Fact-Checking. 2023</p>
<p>arXiv:2305.18265News Claims with Scientific Evidence. 2023arXiv preprint</p>
<p>Cord-19: The covid-19 open research dataset. Lucy Lu, Wang , Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Douglas Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Kinney, ArXiv. 2020. 2020</p>
<p>William Yang, Wang , arXiv:1705.00648liar, liar pants on fire": A new benchmark dataset for fake news detection. 2017. 2017arXiv preprint</p>
<p>Semi-supervised exaggeration detection of health science press releases. Dustin Wright, Isabelle Augenstein, arXiv:2108.134932021. 2021arXiv preprint</p>
<p>Semi-supervised exaggeration detection of health science press releases. Dustin Wright, Isabelle Augenstein, arXiv:2108.134932021. 2021arXiv preprint</p>
<p>Modeling information change in science communication with semantically matched paraphrases. Dustin Wright, Jiaxin Pei, David Jurgens, Isabelle Augenstein, arXiv:2210.130012022. 2022arXiv preprint</p>
<p>Generating scientific for zero-shot scientific fact checking. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu, Wang , arXiv:2203.129902022. 2022arXiv preprint</p>
<p>Evidenceaware fake news detection with neural networks. Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, Liang Wang, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Exploring the limits of chatgpt for query or aspect-based text summarization. Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, Wei Cheng, arXiv:2302.080812023. 2023arXiv preprint</p>
<p>Extractive summarization via chatgpt for faithful summary generation. Haopeng Zhang, Xiao Liu, Jiawei Zhang, arXiv:2304.041932023. 2023arXiv preprint</p>
<p>Benchmarking Large Language Models for News Summarization. Zhang, Ladhak, Durmus, Liang, Mckeown, Hashimoto, arXiv:2301.138482023. 2023arXiv preprint</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, arXiv:2301.13848Benchmarking large language models for news summarization. 2023. 2023arXiv preprint</p>
<p>AnswerFact: Fact checking in product question answering. Wenxuan Zhang, Yang Deng, Jing Ma, Wai Lam, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions. Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, Munmun De Choudhury, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6418 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6418</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6418</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-267617098</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06332v2.pdf" target="_blank">InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learning which shows the possibility of using LEAN as a unified platform for solving and proving in math. Our models, codes, and data are released at \url{https://github.com/InternLM/InternLM-Math}.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6418.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6418.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM2-Math-7B (SFT COT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM2-Math-7B (Supervised Fine-Tuned for Math with Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter InternLM2-derived model continued-pretrained on math corpora and supervised-fine-tuned on a mixture of chain-of-thought, code-interpreter, reward-modeling and formal-logic data to improve arithmetic and multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-7B (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq (InternLM/InternLM2 family; unified seq2seq format used for tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued pretraining on CC-retrieved math text, domain-specific math corpora (web pages, arXiv, forums, books), and synthetic numerical-operation data (arithmetic, exponentiation, trig, etc.); SFT mixture includes MetaMath chain-of-thought, GSM8K augmentations, LEAN formalization pairs, code-interpreter examples, reward-modeling labels, and data-augmenter examples.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (primary), MATH, Hungary math exam, MathBench-ZH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic word problems (grade-school arithmetic), algebraic reasoning, assorted math tasks (prime checking, Game-of-24)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems presented to the model; zero-shot chain-of-thought prompt format for many SFT evaluations; some few-shot ICL used in pretrain evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school level (GSM8K) up to high-school olympiad-style (MATH levels), includes primary/middle/high school items in MathBench-ZH</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot chain-of-thought (COT) in SFT evaluation; majority-voting / self-consistency and few-shot ICL used in other evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (greedy or majority-voting depending on experiment); reranking uses PRM/ORM pass@K style</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K (zero-shot COT, greedy): 78.1% accuracy; MATH (zero-shot COT, greedy): 34.6% accuracy (reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No neuron-level/activation probes reported; analyses are at behavioral and data-level: authors observe 'calculation hallucination' in COT (equations written with immediate incorrect/eager equalities), remedied by rewriting/matching equations in SFT; ablations show MetaMath is crucial for general reasoning; LEAN/formal data is sparse and benefits from multitask SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Calculation hallucination (models emitting compact equations with incorrect immediate results), sensitivity to prompt formatting (different lead-in strings affect performance), occasional false-positive correct outcomes with incorrect processes (outcome-correct but process-incorrect), verbosity/repetition due to matched-equals rewriting, code-switching when Chinese/English SFT imbalance exists.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Improves with scale to 20B (see companion entry). SFT yields large gains over base checkpoints at same size; model is sensitive to SFT mixture composition (MetaMath inclusion critical).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6418.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6418.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM2-Math-20B (SFT COT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM2-Math-20B (Supervised Fine-Tuned for Math with Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 20B-parameter InternLM2-derived model with the same training and SFT recipe as the 7B model but scaled to 20B, showing improved arithmetic and multi-step reasoning performance compared to 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-20B (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq (InternLM/InternLM2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same continued pretraining sources as 7B (CC math retrieval, domain-specific corpora, synthetic numerical ops); SFT includes MetaMath, GSM8K augmentations, LEAN, code-interpreter examples, reward-modeling and augmenter data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, Hungary math exam, MiniF2F (formal)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step word problems, higher-difficulty MATH items, formal proof translation and solving via LEAN</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (COT) and formal LEAN statements for formal tasks</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to high-school/competition levels (MATH and MiniF2F)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot chain-of-thought; PRM/ORM reranking and code-assisted methods used in evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (greedy / reranked pass@K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K (zero-shot COT): 82.6% accuracy; MATH (zero-shot COT greedy): 37.7% (greedy) and PRM K=100 reranking on MATH(500) yields 50.0% for the 20B RM-reranked setting (Table 6). Base MiniF2F (ICL) pretrain achieves 29.5–30.3 depending on checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Similar behavioral-level analyses as for 7B: PRM generally outperforms ORM and majority voting; LEAN used as a verifier can check calculations but not full logical correctness; LEAN advantage in reranking shrinks with larger model size.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same as 7B but with lower relative benefit from LEAN as reward model; false-positive outcome correctness persists in harder MATH examples; PRM effectiveness limited by SFT/PRM format confusion and imbalance between positive/negative process examples.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Clear upward trend from 7B to 20B (e.g., GSM8K 78.1% → 82.6%; MATH 34.6% → 37.7%); RM reranking gains present for both sizes but relative benefits (e.g., LEAN-as-RM) diminish with scale; pretraining token-wise gains plateau after ~80B tokens and may degrade by 200B.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6418.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6418.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RICO (Reasoning Interleaved with Coding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning Interleaved with Coding (RICO) - iterative code-assisted reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-interpreter prompting/training design where the LLM alternates between symbolic reasoning and program generation/execution in multiple rounds (tool-calling protocol) so the model sees execution results during reasoning to solve complex calculations and multi-step math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-7B / 20B using RICO</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq with tool-call/code-interpreter integration</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 20B evaluated (same code-interpreter SFT used for both)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SFT data for code interpreter uses iterative data update and hard-example mining; initial data from ToRA-70B and GPT-4-turbo supplemented by model self-sampling on GSM8K and MATH train sets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (code-assisted evaluations shown in Table 9)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic that requires heavy numeric computation, program-of-thought style problems, algebra, and other tasks benefitting from exact computation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; model generates code (Python) interleaved with textual reasoning; code is executed and results fed back to model in multiple rounds until answer is produced.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to high-school numeric problems; complex calculation-heavy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tool-augmented prompting: RICO (iterative program generation + execution), code-interpreter protocol, training with executed-code supervision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Code-interpreter results: InternLM2-Math-7B (RICO) — GSM8K 79.4%, MATH 50.9% (Table 9). Larger models further improve.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral evidence that iterative execution and interleaving reasoning with execution yields better exploitation of model's reasoning capabilities than single-shot program-of-thought; training uses iterative self-training and GPT-4 to fill hard examples; no internal neuron/attention mechanistic analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Dependence on correct code generation and execution environment; format/style mismatches between data sources (ToRA vs desired format) require conversion; still sensitive to SFT and prompt formats; does not eliminate reasoning errors that are logical rather than computational.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>RICO benefits at both 7B and 20B; 20B models outperform 7B on code-assisted tasks; code-interpreter SFT and iterative data mining reduce reliance on external GPT-4 but performance still correlates with model scale and SFT quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6418.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6418.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEAN formal solving / LEAN as RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using LEAN (Lean theorem prover) as solver/verifier and reward model (LEAN-RM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Translate natural-language problems or COT processes to LEAN 3 formal statements and proofs; use LEAN to execute/verify computations and to rerank candidate chains-of-thought based on whether LEAN code compiles/executes to the same answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-7B/20B (using LEAN3 interface)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq with formal-language generation capability</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 20B evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SFT includes distilled GPT-4-generated LEAN3 solutions for GSM8K train set, formalization pairs, and MathLib-derived examples; LEAN is relatively sparse in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (solved via LEAN), MiniF2F (formal theorem proving)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formalization of natural-language math problems; formal proof generation and verification; computation via LEAN for arithmetic problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Model generates comments + LEAN 3 code; code compiled/executed by LEAN to produce numerical answer or proof check</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level word problems for LEAN solving; MiniF2F includes more advanced formal problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Generate LEAN code from natural-language prompt (COT→LEAN); use LEAN execution results as verification/reranking (LEAN-RM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (whether LEAN compilation/execution yields correct numeric answer or proof completion)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K solved via LEAN: InternLM2-Math-7B 70.4% (LEAN) vs COT 78.1%; InternLM2-Math-20B 73.9% (LEAN) vs COT 82.6% (Table 8). MiniF2F ICL formal performance: base models achieve ~30.3% on MiniF2F test without fine-tuning (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>LEAN can check/execute arithmetic computations reliably when formalization is correct, but it cannot verify higher-level natural-language logical reasoning unless fully formalized; authors ablate sensitivity to LEAN SFT data size and find MetaMath helps: grammar learned from GSM8K-LEAN and reasoning from MetaMath, so multi-task SFT stabilizes LEAN performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>LEAN-specific semantic mismatches: integer-division and natural-number typing (a/b semantics), subtraction with natural-number type constraints (a - b = 0 when a:N < b:N), and LEAN3 vs LEAN4 differences; LEAN checks calculations but not informal logic if formalization is incorrect; LEAN data sparsity causes brittleness without auxiliary SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>LEAN solving accuracy improves with model size but lags behind COT; LEAN-as-reward-model gives larger relative gains for 7B than 20B (advantage diminishes with scale); LEAN performance sensitive to SFT dataset composition and amount.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6418.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6418.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward Modeling (ORM / PRM / LEAN-RM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Outcome Reward Models (ORM), Process Reward Models (PRM) and LEAN-based Reward Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unified seq2seq reward-model training that supports reranking candidate reasoning paths using outcome checks (ORM), process-based checks (PRM), or LEAN-execution verification (LEAN-RM). PRM tends to outperform ORM empirically in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-7B/20B used both as solver and as reward-model reranker</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq trained verifier + policy in unified format</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 20B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SFT reward-model data includes ORM labeled pairs, PRM-style process labels generated via model self-sampling and filtering, and LEAN-verified COT→LEAN pairs for LEAN-RM.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (tests use reranking on sampled candidates; MATH(500) reported in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Reranking of multiple candidate solution traces to select the correct arithmetic/logic solution</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple sampled chain-of-thought outputs per problem; reranking by verifier score (seq2seq output) using ORM, PRM or LEAN-based verification</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K and MATH range (grade-school to high-school)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Generate K candidate traces (sampling temperature 0.7), then rerank with PRM/ORM/LEAN-RM; majority-voting and oracle baselines reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy after reranking (pass@K-like / majority-vote baselines compared)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On MATH(500): InternLM-MATH-7B PRM K=100 → 47.0%; InternLM-MATH-20B PRM K=100 → 50.0%; ORM and majority-voting yield lower scores (Table 6). LEAN-RM gives substantial gains on GSM8K for 7B but smaller gains for 20B.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral finding: PRM > ORM > majority voting, consistent with prior literature; LEAN-RM provides strong computational verification but cannot check informal logical correctness. Authors note limited PRM gains may stem from SFT/PRM format confusion and imbalance in positive vs negative processes. No mechanistic interpretability (e.g., probe of verifier internals) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Reranker limited by quality/diversity of sampled candidates; oracle gap remains large; PRM performance constrained by annotation/mix issues; LEAN-RM cannot detect logical errors not captured in formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Reranking helps at both sizes; absolute performance increases with model size but sizable headroom remains between reranking and oracle; LEAN-as-RM advantage reduces with larger base models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6418.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6418.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining & Synthetic Numerical Data / Scratchpad mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining with math corpora and synthetic numerical-operation data, plus scratchpad-style COT rewriting to reduce calculation hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Continuation pretraining on math-specific corpora and a small synthetic numeric dataset improves numeric ability; authors also synthesize scratchpad-like calculation data and apply post-processing to COT chains to avoid 'calculation hallucination' by rewriting chained equations into stepwise calculable steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-Math-Base (7B/20B pre-trained checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq continued-pretrained models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 20B base models (pretrained to up to 125B continued training tokens; 20B early-stop at 80B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>125B tokens of continued pretraining across CC-retrieved math (20B tokens), domain-specific math data (~11B tokens), and synthetic numerical data (~0.2B tokens across arithmetic, exponentiation, trig, polynomials), with deduplication and decontamination against MATH test set.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ICL evaluations on GSM8K and MATH (pretrain-only ICL shown in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic operations, numerical calculation capability, few-shot/few-query ICL arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot ICL prompts; synthetic numeric templates in pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Numeric operations up to 10-digit sampling in synthetic data; GSM8K and MATH ranges</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>In-context learning (few-shot/majority voting) for pretrain evaluations; scratchpad-like chain-of-thought templates included in SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Majority-voting accuracy for ICL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pretrain ICL: InternLM2-Math-Base-7B MATH ~21.5% (MAJ@K in Table 2); pretraining beyond 80B tokens yields diminishing returns and sometimes degradation (Table 12 shows ICL performance peaks near 80B tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors find synthetic numeric data helps baseline numeric operations but SFT format can induce 'calculation hallucination' (models produce equations like '(12+17)^3 = 24389' inline). They mitigate by matching and rewriting such equations into explicit calculatable step sequences in COT; no low-level mechanistic probing performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Calculation hallucination; overfitting to template styles if synthetic templates are not diverse; pretraining too long (beyond ~80B tokens) may degrade ICL/SFT performance; data contamination/de-duplication needed to avoid test leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Pretraining improvements saturate around 80B continued tokens; synthetic numeric data by itself yields limited gains without appropriate SFT; careful SFT composition required to translate pretraining gains into downstream math performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6418.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6418.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observed Failure Modes (summary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical failure modes observed for arithmetic/math reasoning in InternLM-Math experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A consolidated summary of the main numeric/arithmetic failure modes reported in the paper: calculation hallucination, false-positive correct outcomes, formalization brittleness, prompt sensitivity, and SFT-format/PRM limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across InternLM2-Math 7B & 20B experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>seq2seq (observational across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 20B (observations apply to both)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Failure modes arise despite pretraining on math corpora and synthetic numerical data; SFT data composition influences some failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, MiniF2F, Game-of-24, prime-check sets (where failures were annotated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic, counting, multi-step reasoning, formalization and verification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural language and formal (LEAN) formats; also code-interpreter assisted formats</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Failure modes more prevalent at higher difficulty levels (MATH level 4–5) and in counting/integer-output tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Issues manifest across COT, ICL, and code-interpreter prompting; prompt sensitivity noted</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>N/A (qualitative failure catalogue), process-correctness vs outcome-correctness evaluated on sampled MATH predictions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports manual inspection across 25 MATH examples where model answered correctly but process was often incorrect; many false positives occur on integer or single-digit answers; no single numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analysis: false-positive 'correct answers with incorrect reasoning' increases with problem difficulty; PRM/LEAN used to address process correctness but gaps remain; no interpretability at representation/activation level provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>1) Calculation hallucination (equations with immediate/wrong equals chains). 2) False-positive outcomes where answer matches but reasoning is wrong (especially counting/integer results). 3) LEAN semantic brittleness (integer division, nat typing). 4) Sensitivity to prompt wording and SFT format. 5) PRM format confusion and imbalance yielding limited PRM gains. 6) Repetition due to '='-matching rewriting in COT.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Some failure modes reduce with model scale (e.g., accuracy improves 7B→20B), but others (false positives, prompt sensitivity, LEAN brittleness) persist and require SFT/data-format solutions rather than pure scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations <em>(Rating: 2)</em></li>
                <li>Tora: A tool-integrated reasoning agent for mathematical problem solving <em>(Rating: 2)</em></li>
                <li>MiniF2F: a cross-system benchmark for formal olympiad-level mathematics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6418",
    "paper_id": "paper-267617098",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "InternLM2-Math-7B (SFT COT)",
            "name_full": "InternLM2-Math-7B (Supervised Fine-Tuned for Math with Chain-of-Thought)",
            "brief_description": "A 7B-parameter InternLM2-derived model continued-pretrained on math corpora and supervised-fine-tuned on a mixture of chain-of-thought, code-interpreter, reward-modeling and formal-logic data to improve arithmetic and multi-step problem solving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-7B (SFT)",
            "model_family": "seq2seq (InternLM/InternLM2 family; unified seq2seq format used for tasks)",
            "model_size": "7B",
            "training_data_description": "Continued pretraining on CC-retrieved math text, domain-specific math corpora (web pages, arXiv, forums, books), and synthetic numerical-operation data (arithmetic, exponentiation, trig, etc.); SFT mixture includes MetaMath chain-of-thought, GSM8K augmentations, LEAN formalization pairs, code-interpreter examples, reward-modeling labels, and data-augmenter examples.",
            "benchmark_name": "GSM8K (primary), MATH, Hungary math exam, MathBench-ZH",
            "task_type": "Multi-step arithmetic word problems (grade-school arithmetic), algebraic reasoning, assorted math tasks (prime checking, Game-of-24)",
            "problem_format": "Natural-language word problems presented to the model; zero-shot chain-of-thought prompt format for many SFT evaluations; some few-shot ICL used in pretrain evaluation",
            "difficulty_level": "Grade-school level (GSM8K) up to high-school olympiad-style (MATH levels), includes primary/middle/high school items in MathBench-ZH",
            "prompting_method": "Zero-shot chain-of-thought (COT) in SFT evaluation; majority-voting / self-consistency and few-shot ICL used in other evaluations",
            "performance_metric": "Accuracy (greedy or majority-voting depending on experiment); reranking uses PRM/ORM pass@K style",
            "performance_value": "GSM8K (zero-shot COT, greedy): 78.1% accuracy; MATH (zero-shot COT, greedy): 34.6% accuracy (reported in Table 5).",
            "internal_analysis": "No neuron-level/activation probes reported; analyses are at behavioral and data-level: authors observe 'calculation hallucination' in COT (equations written with immediate incorrect/eager equalities), remedied by rewriting/matching equations in SFT; ablations show MetaMath is crucial for general reasoning; LEAN/formal data is sparse and benefits from multitask SFT.",
            "failure_modes": "Calculation hallucination (models emitting compact equations with incorrect immediate results), sensitivity to prompt formatting (different lead-in strings affect performance), occasional false-positive correct outcomes with incorrect processes (outcome-correct but process-incorrect), verbosity/repetition due to matched-equals rewriting, code-switching when Chinese/English SFT imbalance exists.",
            "scaling_trend": "Improves with scale to 20B (see companion entry). SFT yields large gains over base checkpoints at same size; model is sensitive to SFT mixture composition (MetaMath inclusion critical).",
            "uuid": "e6418.0",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InternLM2-Math-20B (SFT COT)",
            "name_full": "InternLM2-Math-20B (Supervised Fine-Tuned for Math with Chain-of-Thought)",
            "brief_description": "A 20B-parameter InternLM2-derived model with the same training and SFT recipe as the 7B model but scaled to 20B, showing improved arithmetic and multi-step reasoning performance compared to 7B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-20B (SFT)",
            "model_family": "seq2seq (InternLM/InternLM2 family)",
            "model_size": "20B",
            "training_data_description": "Same continued pretraining sources as 7B (CC math retrieval, domain-specific corpora, synthetic numerical ops); SFT includes MetaMath, GSM8K augmentations, LEAN, code-interpreter examples, reward-modeling and augmenter data.",
            "benchmark_name": "GSM8K, MATH, Hungary math exam, MiniF2F (formal)",
            "task_type": "Multi-step word problems, higher-difficulty MATH items, formal proof translation and solving via LEAN",
            "problem_format": "Natural-language word problems (COT) and formal LEAN statements for formal tasks",
            "difficulty_level": "Grade-school to high-school/competition levels (MATH and MiniF2F)",
            "prompting_method": "Zero-shot chain-of-thought; PRM/ORM reranking and code-assisted methods used in evaluations",
            "performance_metric": "Accuracy (greedy / reranked pass@K)",
            "performance_value": "GSM8K (zero-shot COT): 82.6% accuracy; MATH (zero-shot COT greedy): 37.7% (greedy) and PRM K=100 reranking on MATH(500) yields 50.0% for the 20B RM-reranked setting (Table 6). Base MiniF2F (ICL) pretrain achieves 29.5–30.3 depending on checkpoint.",
            "internal_analysis": "Similar behavioral-level analyses as for 7B: PRM generally outperforms ORM and majority voting; LEAN used as a verifier can check calculations but not full logical correctness; LEAN advantage in reranking shrinks with larger model size.",
            "failure_modes": "Same as 7B but with lower relative benefit from LEAN as reward model; false-positive outcome correctness persists in harder MATH examples; PRM effectiveness limited by SFT/PRM format confusion and imbalance between positive/negative process examples.",
            "scaling_trend": "Clear upward trend from 7B to 20B (e.g., GSM8K 78.1% → 82.6%; MATH 34.6% → 37.7%); RM reranking gains present for both sizes but relative benefits (e.g., LEAN-as-RM) diminish with scale; pretraining token-wise gains plateau after ~80B tokens and may degrade by 200B.",
            "uuid": "e6418.1",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RICO (Reasoning Interleaved with Coding)",
            "name_full": "Reasoning Interleaved with Coding (RICO) - iterative code-assisted reasoning",
            "brief_description": "A code-interpreter prompting/training design where the LLM alternates between symbolic reasoning and program generation/execution in multiple rounds (tool-calling protocol) so the model sees execution results during reasoning to solve complex calculations and multi-step math problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-7B / 20B using RICO",
            "model_family": "seq2seq with tool-call/code-interpreter integration",
            "model_size": "7B and 20B evaluated (same code-interpreter SFT used for both)",
            "training_data_description": "SFT data for code interpreter uses iterative data update and hard-example mining; initial data from ToRA-70B and GPT-4-turbo supplemented by model self-sampling on GSM8K and MATH train sets.",
            "benchmark_name": "GSM8K, MATH (code-assisted evaluations shown in Table 9)",
            "task_type": "Multi-step arithmetic that requires heavy numeric computation, program-of-thought style problems, algebra, and other tasks benefitting from exact computation",
            "problem_format": "Natural-language word problems; model generates code (Python) interleaved with textual reasoning; code is executed and results fed back to model in multiple rounds until answer is produced.",
            "difficulty_level": "Grade-school to high-school numeric problems; complex calculation-heavy tasks",
            "prompting_method": "Tool-augmented prompting: RICO (iterative program generation + execution), code-interpreter protocol, training with executed-code supervision",
            "performance_metric": "Accuracy (greedy)",
            "performance_value": "Code-interpreter results: InternLM2-Math-7B (RICO) — GSM8K 79.4%, MATH 50.9% (Table 9). Larger models further improve.",
            "internal_analysis": "Behavioral evidence that iterative execution and interleaving reasoning with execution yields better exploitation of model's reasoning capabilities than single-shot program-of-thought; training uses iterative self-training and GPT-4 to fill hard examples; no internal neuron/attention mechanistic analyses provided.",
            "failure_modes": "Dependence on correct code generation and execution environment; format/style mismatches between data sources (ToRA vs desired format) require conversion; still sensitive to SFT and prompt formats; does not eliminate reasoning errors that are logical rather than computational.",
            "scaling_trend": "RICO benefits at both 7B and 20B; 20B models outperform 7B on code-assisted tasks; code-interpreter SFT and iterative data mining reduce reliance on external GPT-4 but performance still correlates with model scale and SFT quality.",
            "uuid": "e6418.2",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LEAN formal solving / LEAN as RM",
            "name_full": "Using LEAN (Lean theorem prover) as solver/verifier and reward model (LEAN-RM)",
            "brief_description": "Translate natural-language problems or COT processes to LEAN 3 formal statements and proofs; use LEAN to execute/verify computations and to rerank candidate chains-of-thought based on whether LEAN code compiles/executes to the same answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-7B/20B (using LEAN3 interface)",
            "model_family": "seq2seq with formal-language generation capability",
            "model_size": "7B and 20B evaluated",
            "training_data_description": "SFT includes distilled GPT-4-generated LEAN3 solutions for GSM8K train set, formalization pairs, and MathLib-derived examples; LEAN is relatively sparse in pretraining.",
            "benchmark_name": "GSM8K (solved via LEAN), MiniF2F (formal theorem proving)",
            "task_type": "Formalization of natural-language math problems; formal proof generation and verification; computation via LEAN for arithmetic problems",
            "problem_format": "Model generates comments + LEAN 3 code; code compiled/executed by LEAN to produce numerical answer or proof check",
            "difficulty_level": "GSM8K-level word problems for LEAN solving; MiniF2F includes more advanced formal problems",
            "prompting_method": "Generate LEAN code from natural-language prompt (COT→LEAN); use LEAN execution results as verification/reranking (LEAN-RM)",
            "performance_metric": "Accuracy (whether LEAN compilation/execution yields correct numeric answer or proof completion)",
            "performance_value": "GSM8K solved via LEAN: InternLM2-Math-7B 70.4% (LEAN) vs COT 78.1%; InternLM2-Math-20B 73.9% (LEAN) vs COT 82.6% (Table 8). MiniF2F ICL formal performance: base models achieve ~30.3% on MiniF2F test without fine-tuning (Table 4).",
            "internal_analysis": "LEAN can check/execute arithmetic computations reliably when formalization is correct, but it cannot verify higher-level natural-language logical reasoning unless fully formalized; authors ablate sensitivity to LEAN SFT data size and find MetaMath helps: grammar learned from GSM8K-LEAN and reasoning from MetaMath, so multi-task SFT stabilizes LEAN performance.",
            "failure_modes": "LEAN-specific semantic mismatches: integer-division and natural-number typing (a/b semantics), subtraction with natural-number type constraints (a - b = 0 when a:N &lt; b:N), and LEAN3 vs LEAN4 differences; LEAN checks calculations but not informal logic if formalization is incorrect; LEAN data sparsity causes brittleness without auxiliary SFT.",
            "scaling_trend": "LEAN solving accuracy improves with model size but lags behind COT; LEAN-as-reward-model gives larger relative gains for 7B than 20B (advantage diminishes with scale); LEAN performance sensitive to SFT dataset composition and amount.",
            "uuid": "e6418.3",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reward Modeling (ORM / PRM / LEAN-RM)",
            "name_full": "Outcome Reward Models (ORM), Process Reward Models (PRM) and LEAN-based Reward Modeling",
            "brief_description": "Unified seq2seq reward-model training that supports reranking candidate reasoning paths using outcome checks (ORM), process-based checks (PRM), or LEAN-execution verification (LEAN-RM). PRM tends to outperform ORM empirically in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-7B/20B used both as solver and as reward-model reranker",
            "model_family": "seq2seq trained verifier + policy in unified format",
            "model_size": "7B and 20B",
            "training_data_description": "SFT reward-model data includes ORM labeled pairs, PRM-style process labels generated via model self-sampling and filtering, and LEAN-verified COT→LEAN pairs for LEAN-RM.",
            "benchmark_name": "GSM8K and MATH (tests use reranking on sampled candidates; MATH(500) reported in Table 6)",
            "task_type": "Reranking of multiple candidate solution traces to select the correct arithmetic/logic solution",
            "problem_format": "Multiple sampled chain-of-thought outputs per problem; reranking by verifier score (seq2seq output) using ORM, PRM or LEAN-based verification",
            "difficulty_level": "GSM8K and MATH range (grade-school to high-school)",
            "prompting_method": "Generate K candidate traces (sampling temperature 0.7), then rerank with PRM/ORM/LEAN-RM; majority-voting and oracle baselines reported",
            "performance_metric": "Accuracy after reranking (pass@K-like / majority-vote baselines compared)",
            "performance_value": "On MATH(500): InternLM-MATH-7B PRM K=100 → 47.0%; InternLM-MATH-20B PRM K=100 → 50.0%; ORM and majority-voting yield lower scores (Table 6). LEAN-RM gives substantial gains on GSM8K for 7B but smaller gains for 20B.",
            "internal_analysis": "Behavioral finding: PRM &gt; ORM &gt; majority voting, consistent with prior literature; LEAN-RM provides strong computational verification but cannot check informal logical correctness. Authors note limited PRM gains may stem from SFT/PRM format confusion and imbalance in positive vs negative processes. No mechanistic interpretability (e.g., probe of verifier internals) is reported.",
            "failure_modes": "Reranker limited by quality/diversity of sampled candidates; oracle gap remains large; PRM performance constrained by annotation/mix issues; LEAN-RM cannot detect logical errors not captured in formalization.",
            "scaling_trend": "Reranking helps at both sizes; absolute performance increases with model size but sizable headroom remains between reranking and oracle; LEAN-as-RM advantage reduces with larger base models.",
            "uuid": "e6418.4",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Pretraining & Synthetic Numerical Data / Scratchpad mitigation",
            "name_full": "Pretraining with math corpora and synthetic numerical-operation data, plus scratchpad-style COT rewriting to reduce calculation hallucination",
            "brief_description": "Continuation pretraining on math-specific corpora and a small synthetic numeric dataset improves numeric ability; authors also synthesize scratchpad-like calculation data and apply post-processing to COT chains to avoid 'calculation hallucination' by rewriting chained equations into stepwise calculable steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-Math-Base (7B/20B pre-trained checkpoints)",
            "model_family": "seq2seq continued-pretrained models",
            "model_size": "7B and 20B base models (pretrained to up to 125B continued training tokens; 20B early-stop at 80B)",
            "training_data_description": "125B tokens of continued pretraining across CC-retrieved math (20B tokens), domain-specific math data (~11B tokens), and synthetic numerical data (~0.2B tokens across arithmetic, exponentiation, trig, polynomials), with deduplication and decontamination against MATH test set.",
            "benchmark_name": "ICL evaluations on GSM8K and MATH (pretrain-only ICL shown in Table 2)",
            "task_type": "Arithmetic operations, numerical calculation capability, few-shot/few-query ICL arithmetic",
            "problem_format": "Few-shot ICL prompts; synthetic numeric templates in pretraining",
            "difficulty_level": "Numeric operations up to 10-digit sampling in synthetic data; GSM8K and MATH ranges",
            "prompting_method": "In-context learning (few-shot/majority voting) for pretrain evaluations; scratchpad-like chain-of-thought templates included in SFT",
            "performance_metric": "Majority-voting accuracy for ICL",
            "performance_value": "Pretrain ICL: InternLM2-Math-Base-7B MATH ~21.5% (MAJ@K in Table 2); pretraining beyond 80B tokens yields diminishing returns and sometimes degradation (Table 12 shows ICL performance peaks near 80B tokens).",
            "internal_analysis": "Authors find synthetic numeric data helps baseline numeric operations but SFT format can induce 'calculation hallucination' (models produce equations like '(12+17)^3 = 24389' inline). They mitigate by matching and rewriting such equations into explicit calculatable step sequences in COT; no low-level mechanistic probing performed.",
            "failure_modes": "Calculation hallucination; overfitting to template styles if synthetic templates are not diverse; pretraining too long (beyond ~80B tokens) may degrade ICL/SFT performance; data contamination/de-duplication needed to avoid test leakage.",
            "scaling_trend": "Pretraining improvements saturate around 80B continued tokens; synthetic numeric data by itself yields limited gains without appropriate SFT; careful SFT composition required to translate pretraining gains into downstream math performance.",
            "uuid": "e6418.5",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Observed Failure Modes (summary)",
            "name_full": "Empirical failure modes observed for arithmetic/math reasoning in InternLM-Math experiments",
            "brief_description": "A consolidated summary of the main numeric/arithmetic failure modes reported in the paper: calculation hallucination, false-positive correct outcomes, formalization brittleness, prompt sensitivity, and SFT-format/PRM limitations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across InternLM2-Math 7B & 20B experiments",
            "model_family": "seq2seq (observational across models)",
            "model_size": "7B and 20B (observations apply to both)",
            "training_data_description": "Failure modes arise despite pretraining on math corpora and synthetic numerical data; SFT data composition influences some failure modes.",
            "benchmark_name": "MATH, GSM8K, MiniF2F, Game-of-24, prime-check sets (where failures were annotated)",
            "task_type": "Arithmetic, counting, multi-step reasoning, formalization and verification",
            "problem_format": "Natural language and formal (LEAN) formats; also code-interpreter assisted formats",
            "difficulty_level": "Failure modes more prevalent at higher difficulty levels (MATH level 4–5) and in counting/integer-output tasks",
            "prompting_method": "Issues manifest across COT, ICL, and code-interpreter prompting; prompt sensitivity noted",
            "performance_metric": "N/A (qualitative failure catalogue), process-correctness vs outcome-correctness evaluated on sampled MATH predictions",
            "performance_value": "Paper reports manual inspection across 25 MATH examples where model answered correctly but process was often incorrect; many false positives occur on integer or single-digit answers; no single numeric metric provided.",
            "internal_analysis": "Behavioral analysis: false-positive 'correct answers with incorrect reasoning' increases with problem difficulty; PRM/LEAN used to address process correctness but gaps remain; no interpretability at representation/activation level provided.",
            "failure_modes": "1) Calculation hallucination (equations with immediate/wrong equals chains). 2) False-positive outcomes where answer matches but reasoning is wrong (especially counting/integer results). 3) LEAN semantic brittleness (integer division, nat typing). 4) Sensitivity to prompt wording and SFT format. 5) PRM format confusion and imbalance yielding limited PRM gains. 6) Repetition due to '='-matching rewriting in COT.",
            "scaling_trend": "Some failure modes reduce with model scale (e.g., accuracy improves 7B→20B), but others (false positives, prompt sensitivity, LEAN brittleness) persist and require SFT/data-format solutions rather than pure scaling.",
            "uuid": "e6418.6",
            "source_info": {
                "paper_title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations",
            "rating": 2,
            "sanitized_title": "mathshepherd_verify_and_reinforce_llms_stepbystep_without_human_annotations"
        },
        {
            "paper_title": "Tora: A tool-integrated reasoning agent for mathematical problem solving",
            "rating": 2,
            "sanitized_title": "tora_a_toolintegrated_reasoning_agent_for_mathematical_problem_solving"
        },
        {
            "paper_title": "MiniF2F: a cross-system benchmark for formal olympiad-level mathematics",
            "rating": 2,
            "sanitized_title": "minif2f_a_crosssystem_benchmark_for_formal_olympiadlevel_mathematics"
        }
    ],
    "cost": 0.02047175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning
24 May 2024</p>
<p>Huaiyuan Ying 
Shanghai AI Laboratory</p>
<p>Tsinghua University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Shuo Zhang 
Shanghai AI Laboratory</p>
<p>School of Computer Science
Fudan University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Linyang Li 
School of Computer Science
Fudan University</p>
<p>Zhejian Zhou 
Shanghai AI Laboratory</p>
<p>University of Southern California</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Yunfan Shao 
Shanghai AI Laboratory</p>
<p>School of Computer Science
Fudan University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Zhaoye Fei 
Shanghai AI Laboratory</p>
<p>School of Computer Science
Fudan University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Yichuan Ma 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Jiawei Hong 
Shanghai AI Laboratory</p>
<p>School of Computer Science
Fudan University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Kuikun Liu 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Ziyi Wang 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Yudong Wang 
Zijian Wu 
Shanghai AI Laboratory</p>
<p>Shanghai Jiaotong University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Shuaibin Li 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Fengzhe Zhou 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Hongwei Liu 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Songyang Zhang 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Wenwei Zhang 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Hang Yan 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Xipeng Qiu 
Shanghai AI Laboratory</p>
<p>School of Computer Science
Fudan University</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Jiayu Wang 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Kai Chen 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>Dahua Lin 
Shanghai AI Laboratory</p>
<p>https://github.com/InternLM/InternLM</p>
<p>InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning
24 May 20245B13FE681A5F080A46AED1FB3F4D6FF1arXiv:2402.06332v2[cs.CL]
The math abilities of large language models can represent their abstract reasoning ability.In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from In-ternLM2.We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter.These abilities can be used to develop the next math LLMs or self-iteration.InternLM-Math obtains open-sourced stateof-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F.Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning.We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learning which shows the possibility of using LEAN as a unified platform for solving and proving in math.Our models, codes, and data are released at https://github.com/InternLM/InternLM-Math.Demo: https://huggingface.co/spaces/internlm/internlm2-math-7b * Equal contribution.</p>
<p>Introduction</p>
<p>Large language models (Brown et al., 2020;Lewkowycz et al., 2022b;Taylor et al., 2023;OpenAI, 2023;Anil et al., 2023;InternLM, 2023;Azerbayev et al., 2023b;Google, 2023;Shao et al., 2024) have shown significant abilities in mathematical reasoning tasks from grade school (Cobbe et al., 2021) to high school levels (Hendrycks et al., 2021a) by using chain-ofthought reasoning (Wei et al., 2022) or program-of-thought reasoning (Chen et al., 2023b;Gao et al., 2023).</p>
<p>Building such models requires pre-training on math corpora and supervised fine-tuning on math problems.We introduce InternLM-Math based on InternLM2-Base models 1 .In-ternLM2 shows strong performance in various aspects including math, code, chat experience, instruction following, and creative writing.We retrieve and collect math-related data to continue pre-training on InternLM2-Base and obtain state-of-the-art performance on informal and formal math reasoning benchmarks outperforming Minerva (Lewkowycz et al., 2022b) and Llemma (Azerbayev et al., 2023b).</p>
<p>During supervised fine-tuning, we supervise InternLM-Math not only on solving math problems using chain-of-thought and code interpreters but also many tasks for developing math LLMs which include reward modeling and augment helper.We also introduce using LEAN for translating between natural languages and LEAN statements (Han et al., 2021), solving easy math problems, and proving math statements.InternLM-Math series models achieve open-sourced state-of-the-art performance on multiple benchmarks2 and score more than 90% relative to GPT-4 (OpenAI, 2023).</p>
<p>Our contributions including:</p>
<p>• We open-source our base and SFT LLMs in math reasoning.It achieves opensourced SOTA under the setting of ICL, SFT, RM reranking, and Python-assisted in various benchmarks.• We unify chain-of-thought reasoning, reward modeling, data augmentation, and formal reasoning under a unified seq2seq format.We supervise our model with both problem-solving and verification abilities.• We propose reasoning interleaved with coding (RICO) and achieve state-of-the-art math reasoning with Python's assistance.• We explore using LEAN to solve math word problems and we investigate its performance concerning data size during multi-task learning.</p>
<p>Figure 1: 4-shot MATH performances with 256 times majority voting.Comparison is based on our pre-trained base model, Llemma (Azerbayev et al., 2023b), and Minerva (Lewkowycz et al., 2022b).The figure is modified from Azerbayev et al. (2023b).</p>
<p>Related Work</p>
<p>Math Pre-training Pre-training helps LLMs acquire computational and mathematical knowledge from various sources, such as math corpora (Han et al., 2021;Lewkowycz et al., 2022b;Paster et al., 2023;Wang et al., 2023d), problem sets (Lightman et al., 2023), and synthetic data (Hendrycks et al., 2021b;Liu &amp; Low, 2023;Yang et al., 2023b).ArXiv with abundant math contents is usually used in math pre-training (Lewkowycz et al., 2022a;Taylor et al., 2023;Azerbayev et al., 2023a).Paster et al. (2023) extracts math web pages from common crawl which can be complementary to arXiv.Math problems including GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021a) are used during pretraining (OpenAI, 2023).Synthetic problems generated via rules or math program scripts (Hendrycks et al., 2021b) can also be used for pre-training.InternLM-Math collects pre-train data from math corpora and synthetic data which establish its math ability.</p>
<p>Math Fine-tuning Building a stronger augmented chain-of-thought dataset (Yu et al., 2023b;Yue et al., 2023;Li et al., 2023;Liu &amp; Yao, 2024) for SFT to improve math reasoning performance has received lots of interest.Problem augmentation (Luo et al., 2023;Yu et al., 2023b;Li et al., 2023;Liu &amp; Yao, 2024) and reasoning path augmentation (Zelikman et al., 2022;Huang et al., 2022;Zhu et al., 2023;Yuan et al., 2023) are two common ways which show significant effectiveness.Reinforcement learning (Uesato et al., 2022;Luo et al., 2023;Wang et al., 2023b;Singh et al., 2023) has also been explored to fine-tune language models to lift reasoning performance which requires a reward model (i.e.verifier) that can distinguish correct and incorrect reasoning processes and answers.Compared to previous work, we not only build a stronger chain-of-thought SFT dataset but also integrate multiple abilities all-in-one including verification, proof, code interpreter, and data augment helper.</p>
<p>Math Verifier Reward models (verifiers) are usually used for reranking multiple candidate reasoning paths.Outcome reward models (ORM) (Cobbe et al., 2021) and process reward models (PRM) (Uesato et al., 2022;Lightman et al., 2023) leverage LLMs to verify the correctness of answers and processes.ORM is less efficient than PRM (Lightman et al., 2023;Wang et al., 2023b) while PRM requires many human experts for labeling.To reduce human labor in labeling PRM, Yu et al. (2023a); Wang et al. (2023b) determine the correctness of processes based on multiple completions from the process.Compared to previous work which trained two separate networks for the policy network and the reward model.We combine these two functions into one unified seq2seq format.After SFT, our model can be used for solving problems or verifying reasoning paths.Furthermore, we explore leveraging formal math language LEAN to verify reasoning paths by translating to LEAN codes.</p>
<p>Math Code Interpreter</p>
<p>Code interpreters can complement LLMs with complex calculation capabilities via various Python libraries.Early explorations use program-of-thought (Chen et al., 2023b;Gao et al., 2023) but lack interaction between LLMs and tools, which may not be able to handle scenarios that require multi-step reasoning and calculation.Recent works (Gou et al., 2023;Wang et al., 2023a) try to more seamlessly integrate code interpreter with reasoning by allowing the model to summarize the results based on the one-time code execution outcomes.InternLM-Math explores reasoning interleaved with coding (RICO), i.e., the reasoning and coding processes are interleaved for multiple rounds until the problem is solved, which is more natural and close to the problem-solving process and fully exploits the reasoning capabilities of LLM.</p>
<p>Math Prover Solving a math problem with a correct answer by LLMs still cannot ensure its process's correctness.However, proving a statement using formal languages like Isabelle (Paulson, 2000), LEAN (Moura &amp; Ullrich, 2021), and Coq (The Coq Development Team, 2023) can promise it.Training an LLM to automated theorem proving in formal languages (Han et al., 2021;Polu et al., 2022;Azerbayev et al., 2023a;Yang et al., 2023a;Welleck &amp; Saha, 2023;Zheng et al., 2023) is hard due to sparse data.InternLM-Math achieves state-of-the-art few-shot performances on MiniF2F (Zheng et al., 2021) which shows potential in building a strong math prover.</p>
<p>Pre-training</p>
<p>In this section, we first describe our pre-training data composition.Then, we outline our data post-processing method we perform on the training data.Finally, we dive into the details of our training strategy.</p>
<p>Pretrain data composition</p>
<p>To achieve a competitive performance in mathematical domains, we collected a diverse collection of high-quality data.We do not leverage any LLM-generated data during pretraining.This data is systematically categorized into the following categories:</p>
<p>CC Retrieved Data We employed Query of CC (Fei et al., 2024) to retrive the training corpus for InternLM2-Math-Base.We select math-related corpus from Query of CC corpus as the first part of continue pretraining data.This part includes 20B tokens.</p>
<p>Domain-Specific Data</p>
<p>We selected from open-source dataset (Azerbayev et al., 2023b) and in-house high-quality datasets in the field of mathematics including web pages, codes, arXiv, forums, and books.This part includes 11B tokens.</p>
<p>Synthetic Data</p>
<p>We synthesized numerical operation data to improve the model's numerical operation capabilities.We included five common operations including arithmetic, exponentiation, logarithmic, trigonometric, and polynomial calculations.For numerical operations, we traversed a set of commonly used values and randomly sampled a wider range of values within 10-digit numbers.To prevent the model from overfitting to specific templates, we diversely constructed templates to ensure the model's numerical computing capabilities generalize to some extent.This part includes 0.2B tokens.</p>
<p>Data post-processing</p>
<p>To enhance the quality of the training data, we follow the approach of Query of CC (Fei et al., 2024) and implement a series of data post-processing strategies.Specifically, we trained a scoring model to identify high-quality datasets.Subsequently, We used the Minhash-LSH method for deduplication of the training data.In our practice, we filtered out duplicate data with a similarity exceeding 0.7.</p>
<p>We further conducted exact formulation decontamination for the special domain data on the MATH test set.We extract all formulations within a given paragraph.If the concatenation of formulations hits any in the MATH test set, we simply remove them.</p>
<p>Training strategy</p>
<p>After collecting and post-processing high-quality retrieved and domain-specific data, we set different training epochs.For details of the data ratios, see  context length of 4096.For documents that are too long or too short, we either truncate or concatenate them to achieve the desired context length.We adopt mixed-precision training with bfloat16 and FlashAttention2 (Dao, 2023) to attain optimal memory utilization and training efficiency.The standard AdamW (Loshchilov &amp; Hutter, 2017) optimizer was employed with hyperparameters β 1 = 0.9, β 2 = 0.95, and weight decay = 0.1.We use a standard cosine learning rate scheduler.Specifically, the model's learning rate reaches a maximum of lr max = 3e − 5 after 2000 warm-up steps, and then gradually decreases to a minimum of lr min = 3e − 6 over the course of training.In the continued pre-training process, a total of 125 billion tokens were trained.For the 20B model, we early stop at 80 billion tokens based on in-context learning performance.</p>
<p>Supervised Fine-tuning</p>
<p>Dislike other math-specialized LLMs that focus on solving math problems, our models are targeted to be math solvers and also be ready for self-improving which requires abilities including problem augmentation, reward modeling, self-verifying, formal reasoning, and code interpreters.Our SFT data contains high-quality human-written, rule-generated, and LLM-generated data for the abovementioned abilities, the detailed data composition can be seen in Table 16 and our models generated responses can be seen in Appendix D. We show the query and response format of SFT in Figure 2.</p>
<p>Chain-of-thought</p>
<p>We utilize MetaMath (Yu et al., 2023b) as our fundamental English chain-of-thought data resource which brings consistent reasoning improvement to various LLMs.We leverage in-house Chinese datasets for Chinese chain-of-thought abilities.To improve the math reasoning abilities of our models' weaknesses, we apply reasoning path augmentation (Zelikman et al., 2022) on specific datasets.Inspired by Uesato et al. (2022), we use final answers and our trained PRM to filter augmented reasoning paths.</p>
<p>Although we add synthesis calculation data in pre-training, the models still perform badly on complex calculations.In the spirit of scratchpad (Nye et al., 2021;Liu &amp; Low, 2023), we decompose complex calculations including multiple brackets computation, decimal computation, fraction computation and reduction, remainder problems, and power computation into calculatable steps.We find simply adding scratchpad-like calculation data will not influence the model calculation behavior.The reason comes from the format of SFT data which requires the model immediately to generate the equation answer like (12 + 17) 3 = 24389 inside the chain-of-thought process, and we denote it as calculation hallucination.To alleviate calculation hallucination, we match and rewrite such equations ((12 + 17) 3 = 29 3 = 841 * 29 = (800 + 40 + 1) * 29 = 23200 + 1160 + 29 = 24389) inside the chain-of-thought processes as much as possible.This improves calculation ability during chain-of-thought reasoning.</p>
<p>We also introduce several fundamental math abilities including checking prime, the game of 24, factoring an integer, and calculating triangle areas.Our principle of synthesizing these data is The model generates chain-of-thought based on an algorithm flow chart.The next token prediction must be determined by generated tokens and not dependent on future tokens.Use triangle area calculation as an example, we require the model to first check if three sides can formulate a triangle and if it is a right triangle.The model does not need to assume it forms a right triangle without chain-of-thought reasoning.Table 4 gives an example.We also give an example response that does not follow our principle since predicting right triangle requires the model to know future tokens 3 2 + 4 2 = 5 2 .</p>
<p>Triangle Area</p>
<p>Query: Calculate the area of a triangle with length: 3,4,5.</p>
<p>Response: As 3 + 4 &gt; 5, they can form a triangle.
3 2 = 9, 4 2 = 16, 5 2 = 25, 9 + 16 = 25. So 3 2 + 4 2 = 5 2 , it is a right triangle. The area is 3 * 4/2 = 12/2 = 6.
Bad Response: Since this triangle is a right triangle (3 2 + 4 2 = 5 2 ).The area is 3 * 4/2 = 12/2 = 6.</p>
<p>Reward Modeling</p>
<p>Reward modeling helps the model to better rerank its generation and filter synthesis data for self-improving.Inspired by Math Shepherd (Wang et al., 2023b), we unify ORM and PRM into the same format of seq2seq learning.We also add Chinese ORM data using the model self-sampling chain-of-thought with corresponding final answers.</p>
<p>Formal Math Reasoning Instead of informal natural language problem solving, our data also includes formal reasoning samples based on LEAN 3. Our target is to use LEAN as a solver, verifier, and prover.We distill gpt-4-1106 to solve the GSM8K train set problems using LEAN 3. We select all codes that can generate correct answers.</p>
<p>Augmentation helper</p>
<p>Another important aspect of our data is to help construct synthesized data for self-improving.By rephrasing or augmenting a question, one can easily obtain an enlarged question diversity (Luo et al., 2023;Yu et al., 2023b;Li et al., 2023).</p>
<p>Translating question-answer pairs into a natural language statement is a requirement of using formal math language for proving.</p>
<p>Code Interpreter</p>
<p>Recent attempts (Chen et al., 2023b;Wang et al., 2023a;Gou et al., 2023) have explored enhancing the complex calculation ability of LLMs by tools, where code interpreters have become popular due to their flexibility and functionality backed by various Python libraries.</p>
<p>Early explorations use programs as a kind of thought strategy (Chen et al., 2023b;Gao et al., 2023) but are unsuitable for multi-step reasoning and calculation since LLMs cannot see the code execution results.Recent works (Wang et al., 2023a;Gou et al., 2023) try to more seamlessly integrate code interpreters with reasoning but are incompatible with and require extra modification for general chat services.</p>
<p>We solve the above-mentioned issue by letting LLMs do reasoning interleaved with coding (RICO), where LLMs conduct reasoning in the same format as chat response and adopt a general tool-calling protocol to use the code interpreter.Such a design not only allows the full exploitation of the existing reasoning capability of LLMs when using code interpreters but also allows a direct full integration of tool-calling capability into the chat model.Thus, different capabilities (such as tool calling and chat) of a model can provide service in a universal tool-augmented system, which we believe to be more similar to that of GPT-4 (OpenAI, 2023).</p>
<p>Specifically, as shown in Figure 3, when answering a math problem, we prompt the model to conduct symbolic reasoning and program generation and then observe the code execution results in each round.The model will continue such rounds until it fully answers the problem after summarizing the results, unlike previous methods (Wang et al., 2023a;Gou et al., 2023) that essentially write code once.The reasoning process uses the same format as a general chat response instead of using different marks for both text and code (Wang et al., 2023a).Such a design allows the model to fully exploit its reasoning ability learned in the conventional SFT corpus when using the code interpreter.The program generation can be regarded as a general tool calling, unlike ToRA (Gou et al., 2023), which embeds the code in the text response by markdown syntax.This resolves the ambiguity in the markdown symbol of the code block when deploying the LLMs in a tool-augmented chat system.</p>
<p>The construction of the training data for the math code interpreter adopts an iterative data update and hard example mining strategy to reduce the reliance on GPT-4.At each iteration, we first use the model trained in the previous iteration to generate responses on the train set of GSM8K and MATH.Since the model cannot fully fit the train set, we use GPT-4-turbo to generate responses on the remaining train set once.The correct responses generated by the most recent model and GPT-4-turbo will be used to train a new model for the next iteration.The initial data is generated by ToRA-70B (Gou et al., 2023), which is not ideal due to format differences but can be converted to correct responses to train the initial model.InternLM2-Chat and InternLM2-Math models adopt the same training data for code interpreter capability.</p>
<p>Experiments</p>
<p>Pre-train Performance</p>
<p>To validate the performances of our pretrained base models, we use the standard benchmark for math informal reasoning: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021a) and evaluate them using in-context learning.We adopt the few-shot templates from OpenCompass (Contributors, 2023).We use majority voting (Wang et al., 2023c) accuracy as the metric.The results are listed in Table 2. InternLM2-Math-Base models outperform their initial checkpoints InternLM2-Base on both benchmarks which shows the effectiveness of continue pre-training.InternLM2-Math-Base-7B obtains 21.5 on MATH which outperforms Llemma-7B with 18.0.InternLM2-Math-Base-20B obtains 27.3 on MATH which outperforms Llemma-34B and performs similarly with Minerva-62B with a smaller size.</p>
<p>Table 2: Compare pre-trained models using ICL.The metric is majority voting accuracy.K = 100 for the GSM8K benchmark and K = 256 for the MATH benchmark.We use greedy decoding when K = 1.We sample our models using a temperature of 0.7 when K &gt; 1.</p>
<p>Benchmark GSM8K MATH Model MAJ@1 MAJ@K MAJ@1 MAJ@K Llama2-7B ( One pre-trained base model may have good ICL performance while performing mediocre after SFT due to data overlap among pre-train and SFT.We perform SFT on different models with the same SFT dataset MetaMath (Yu et al., 2023b) to check our models do not suffer such phenomenon.We show results in Table 3.Using MetaMath for SFT, InternLM2-Math-Base-7B still has superiority over Mistral-7B and Llemma-7B on the MATH benchmark.InternLM2-Math-Base-20B outperforms Llemma-34B on both benchmarks.</p>
<p>To show the formal math reasoning ability of our model, we conduct ICL4 on MiniF2F benchmark (Zheng et al., 2021) which includes different level math problems in language LEAN.LEAN can check whether the generated formal proof completes the statement's goals.We compare InternLM2-Math-Base with other pre-trained language models in</p>
<p>SFT Performance</p>
<p>To show the performance of our SFT models, we conduct math reasoning using chain-ofthought, reward modeling, formal reasoning, and code interpreter.We will also test several abilities introduced in our SFT process including game-of-24 and prime checker.</p>
<p>COT Reasoning</p>
<p>We evaluate SFT models on GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021a), Hungary math exam, and MathBench-ZH6 using zero-shot chain-of-thought reasoning.We use the Hungary math exam to test the model generalization ability and use MathBench-ZH to examine Chinese math ability.MathBench-ZH contains 600 Chinese math problems from primary school, middle school, high school, or university level.For each choice problem in MathBench-ZH, we will shuffle the choice order 4 times.A model gives a correct answer 4 times can be considered as correct.We show results in Table 5. InternLM2-Math-7B achieves 78.1, 34.6, 55, and 40 on GSM8K, MATH, the Hungary math exam, and MathBench-ZH respectively which show much stronger in-domain and out-of-domain performance at the same model size.InternLM2-Math-7B also shows better performance compared to using MetaMath for SFT which proves our SFT data can better activate the model's reasoning ability.InternLM2-Math-20B obtains 37.7 and 66 on MATH and Hungary math exam which is only behind GPT-4.It achieves state-of-the-art performance with a much smaller size compared to Qwen-72B and DeepSeek-67B.</p>
<p>Reward Modeling</p>
<p>Reward models can be used for answer reranking to improve model performances (Cobbe et al., 2021;Uesato et al., 2022;Lightman et al., 2023;Wang et al., 2023b).We will test the performance of our reward modeling by ORM and PRM reranking.We use the same SFT model for inference and reward model reranking.We further test using LEAN as a reward model (LRM) by requiring the model to convert COT to LEAN codes and execute LEAN codes.We will mark this COT as correct with LEAN codes and COT obtain the same results.Notice that using LEAN as a reward model for tasks like GSM8K can only check the calculation processes but not the logic processes.We test on GSM8K and MATH (500 test problems), the results are plotted in Figure 4 and shown in Table 19.We find that generally using PRM outperforms ORM, and ORM outperforms majority voting which is consistent with (Lightman et al., 2023;Wang et al., 2023b).We find that using LEAN as RM has a significant advantage in reranking GSM8K with 7B models, but the advantage diminishes with 20B models.There are lots of improvement rooms between RM reranking performances and oracle performances.</p>
<p>We also compare our performance with other RM reranking models in Table 6.Compared to MetaMath-DeepSeek-67B reranking (Wang et al., 2023b), our 20B model uses fewer inference times, a smaller model size, and achieves better performances with an accuracy of 50.0 which shows that our RM is effective.However, compared to GPT-4-MathMix (Lightman et al., 2023), the performance gap is still large.Figure 4: Reranking performances using our reward models on GSM8K and MATH.Oracle shows the upper bound performance which is calculated by Pass@K.</p>
<p>Formal Ability</p>
<p>Targeting a large language model to conduct verifiable math reasoning, multiple abilities are required including translating informal problems or proof statements into formal statements and solving or proofing formal statements using formal languages.</p>
<p>Formal Translation</p>
<p>We first evaluate the ability to translate between natural language and formal statements.We follow Azerbayev et al. (2023a) to translate statements extracted from undergraduate mathematics textbooks.The results are listed in Table 7.While our models outperform ProofGPT and their SFT version (Azerbayev et al., 2023a) in bidirectional translation, we still lag behind CodeX (Chen et al., 2021) and GPT-4 (OpenAI, 2023).Similar to formal reasoning, we do not find significant benefit from scaling parameters.</p>
<p>Table 7: Evaluate translating between LEAN and informal language statement.Results except our models are copied from https://github.com/zhangir-azerbayev/ProofNet.The metric is the BLEU-4 score.</p>
<p>Model Formalization Informalization</p>
<p>ProofGPT-1.3B (Azerbayev et al., 2023a) 8.1 5.1 ProofGPT-6.7B (Azerbayev et al., 2023a) 4.7 6.0 ProofGPT-1.3B back-translated (Azerbayev et al., 2023a) 10.7 -CodeX (Chen et al., 2021) 25.1 13.2 GPT-4 (OpenAI, 2023) 27.1 -
InternLM2-Math-7B 15.0 9.4 InternLM2-Math-20B 15.7 10.2
Using LEAN to solve GSM8K LEAN is mainly used as a proof assistant to prove a statement instead of solving a problem.To unify the solving and proving math problems, we explore using LEAN to solve math problems.We require the model to generate comments before LEAN codes and list an example in Table 6.2.3.We use LEAN 3 to compile and output results as answers.Performances are listed in Table 8.Our 7B model can obtain 70.4 using LEAN to solve GSM8K, while our 20B model can achieve 73.9.We find that our models perform worse than COT.The reasons that using LEAN underperforms COT are: (1) a − b = 0 (when a : N &lt; b : N) (2) division: a/b = [a/b] (when a : N, b : N).These cause inconvenience in some basic calculations.However, we show the possibility of using LEAN as a unified interface for solving and proving math problems.</p>
<p>Solve GSM8K use LEAN 3</p>
<p>Query: A robe takes 2 bolts of blue fiber and half that much white fiber.How many bolts in total does it take?</p>
<p>Response:</p>
<p>--Define the amount of blue fiber needed for one robe def blue_fiber_per_robe : N := 2 --Define the amount of white fiber needed for one robe as half of the blue fiber def white_fiber_per_robe : N := blue_fiber_per_robe / 2 --Calculate the total amount of fiber needed for one robe def total_fiber_per_robe : N := blue_fiber_per_robe + white_fiber_per_robe --Output the result #eval total_fiber_per_robe --This will evaluate to 3 bolts in total</p>
<p>Output: 3</p>
<p>Table 8: The performance on GSM8K using COT and LEAN.</p>
<p>Model COT LEAN</p>
<p>InternLM2-Math-7B 78.1 70.4 InternLM2-Math-20B 82.6 73.9</p>
<p>Code Interpreter</p>
<p>As shown in Table 9. InternLM2-Math-7B can already surpass all the previous open-source models on MATH benchmark except InternLM2-Chat-20B.InternLM2-Math-20B outperforms InternLM2-Chat-20B and obtains the best results among open-source models.Since InternLM2-Math and InternLM2-Chat adopt the same training data for code interpreters, we believe the improvements of InternLM2-Math over InternLM2-Chat models result in the improvement of math reasoning ability.</p>
<p>Other abilities</p>
<p>Game of 24</p>
<p>To test the ability on Game of 24, we use the test set from Yao et al. (2023).</p>
<p>When we sample once for each question, our models achieve an accuracy of 26 and 35 respectively for 7B and 20B model sizes, which greatly outperforms fine-tuned Llamma2-7B and even GPT-4.The performance can match some multi-sampled methods, which demonstrates the capabilities of our generated step-by-step searching process SFT data.</p>
<p>Prime checker For prime number verification, we sampled a test dataset containing 20 numbers for each digit number from 2 to 10 containing 10 prime and 10 composite numbers.</p>
<p>Our model can judge correctly for almost all the numbers, whatever their digits, this is better than GPT-4 which performs worse when digits become bigger.</p>
<p>Model GSM8K MATH</p>
<p>DeepSeek-Coder-Instruct-7B (Guo et al., 2024) 62.8 28.6 MathCODER-CL-7B (Wang et al., 2023a) 67.8 30.2 DeepSeek-Coder-Instruct-1.5-7B (Guo et al., 2024) 72.6 34.1 ToRA-7B (Gou et al., 2023) 72.6 44.6 InternLM2-Chat-7B 77.9 45.1 InternLM2-Math-7B 79.4 50.9</p>
<p>MathCODER-CL-13B (Wang et al., 2023a) 74.1 35.9 MathCODER-CL-34B (Wang et al., 2023a) 81.7 45.2 ToRA-13B (Gou et al., 2023) 75.8 48.1 ToRA-Code-34B (Wang et al., 2023a) 80 TOT (Yao et al., 2023) 45 12(b = 1) OVM (Yu et al., 2023a) 79 20</p>
<p>Discussion</p>
<p>Compare Pretrain Performance with Pretraining Token Amount</p>
<p>We pre-train on InternLM-Base-7B for 200B tokens to evaluate how many epochs should we pretrain.We evaluate our models' performance every 40B tokens using ICL and SFT on MetaMath in Table 12.We find that after 80B tokens, the performance does not improve significantly.When training longer to 200B (approximate 7 epochs) tokens, the performances start to degenerate.</p>
<p>LEAN Performance on GSM8K</p>
<p>We introduce using LEAN to solve GSM8K problems previously.LEAN is sparse during pre-training and fine-tuning compared to natural language, LATEX, and Python.We want to understand how LEAN's ability is regarded to the data size.We ablate the performance of LEAN solving GSM8K using different amounts of SFT data in Table 13.We find if we only use GSM8K-LEAN during SFT, the LEAN performance is worse than using GSM8K-LEAN and MetaMath together.We also find that if we SFT without MetaMath, the LEAN performance is sensitive to data amount.While if we train SFT with MetaMath, the LEAN performance is less sensitive which shows that the model mainly needs to learn grammar from GSM8K-LEAN data and learn reasoning ability from MetaMath.These findings show the reasoning ability of MetaMath helps performance on LEAN with sparse data and suggests a multi-task training strategy of these abilities.</p>
<p>Data Ablation Study in SFT</p>
<p>To study the impact of the composition of our mixture data, we carried out ablation studies for each kind of data in the final mixture.We include the MetaMath dataset for every ablation study to keep basic abilities to solve math questions step by step.The evaluation results are demonstrated in Table 14.Compared with using MetaMath for SFT, our InternLM-Math-7B model and the model with COT data achieve better results on MATH and GSM8K.The other compositions, though containing even much fewer tokens than MetaMath, lead to undermined results on almost all test datasets.This may be attributed to format and style transfer in code, reward model format, and other instructions.However, the SFT data performs better than MetaMath + COT after integration, indicating the latent mutual helpfulness of comprehensive data beyond their original special usage.</p>
<p>False positive in MATH BenchMark</p>
<p>For a math problem that requires an answer, one can always try some special cases to guess the correct answers with incorrect processes.If the question is about counting, generated answers may be sampled wrongly, and obtain a correct answer by chance.This could lead us to overestimate the math reasoning ability of our models and other LLMs.We randomly sample 5 samples for each difficulty level where the InternLM2-Math-20B gives a correct answer in the MATH test set and human-labeled the predicted process of these 25 problems to report this issue.6, 8,9, 12, 18, 24,27, 36, 54, 72, 108, 216.Out of these, the ones that are not divisible by 4 are 2, 6,9,18,27,54,108.I can pair (2, 108), (6,54), (9,27), (18,18).There are four pairs, and each pair corresponds to a different ordered pair (m, n).For example, (2, 108) corresponds to (m, n) = (108, 2).So, there are four ordered pairs (m, n) that satisfy the given conditions.Answer 4.</p>
<p>In this case, the model lists 9 and 27 as an even number, which leads to a false (m, n) pair (9,27).Moreover, the pairs (6,54), (18,18), though satisfying the condition given by model prediction, do not meet the title requirements.The prediction just accidentally lists four pairs as answers.However, if the evaluation takes only the outcome into consideration, this sample will be "correct".Similar cases appear more frequently as the difficulty level increases.Among these false positive cases, most problems have an integer as the answer, especially a single-digit number, and the solution can be carried out without detailed calculations or proof of cases.These make it easy for the model to speculate a correct answer excluding step-by-step reasoning.Simply judging model abilities only by outcome accuracy may be unilateral.Therefore, one of our future works is to use reward models or LEAN to provide a better process checking.</p>
<p>Figure 2 :
2
Figure 2: The left part shows the query and response formats in SFT.The right parts show two possible usages of our SFT model.The upper right is a pipeline of synthesizing new problems using our augment helper, COT, and RM abilities.The lower right is a pipeline of solving informal problems using formal languages with COT and formal reasoning abilities.</p>
<p>Figure 3 :
3
Figure 3: The example of reasoning interleaved with coding (RICO) and conventional chainof-thought.</p>
<p>Table 1 :
1
Pre-train data usage for InternLM-Math.Unique Tokens refers to the number of tokens in the original dataset.Tokens refers to the total number of tokens in the dataset consumed during pre-training.
DomainDatasetUnique Tokens(B) Epochs Tokens(B)CC Retrieved Dataknowledge pile20480open-web-math6424Special Domain Dataalgebraic-stack4416others144Synthetic Datanum0.251Total-31.2-125</p>
<p>Table 1
1. In total, we collected</p>
<p>Table 4
4.
Azerbayev et al. (2023b)ance.InternLM2-Math-7B-Base and InternLM2-Math-20B-Base find 25 and 24 new proofs respectively which do not appear in the official MiniF2F repository 5 .LikeGloeckle et al. (2023);Azerbayev et al. (2023b), we do not find the formal reasoning</p>
<p>Table 3 :
3
Wang et al. (2023b) models by fine-tuning on MetaMath dataset(Yu et al., 2023b).The metric is greedy accuracy.†resultscomefromYuet al. (2023b).<em>resultscome fromWang et al. (2023b).
ModelGSM8K MATHMetaMath-Llama2-7B (Touvron et al., 2023)  †66.519.8MetaMath-Mistral-7B (Jiang et al., 2023)  †77.728.2MetaMath-Llemma-7B (Azerbayev et al., 2023b)  †69.230.0MetaMath-InternLM2-Math-Base-7B76.433.8MetaMath-InternLM2-Math-Base-20B80.736.1MetaMath-Llemma-34B </em>75.834.8performances scale with model parameter sizes. We leave it to the future work of how dataand parameter sizes influence formal reasoning performances.</p>
<p>Table 4 :
4
Azerbayev et al. (2023b)ance.The search budget is the same asAzerbayev et al. (2023b)which is 1 × 32.We use 3-shot and LEAN 4 for base models.
ModelType Search MiniF2F-testReProver (Yang et al., 2023a)SFT-26.5LLMStep (Welleck &amp; Saha, 2023)SFT1 × 3227.9Code-Llama-7B (Rozière et al., 2023)ICL1 × 3220.5Code-Llama-34BICL1 × 3222.1Mistral-7B-v0.1 (Jiang et al., 2023)ICL1 × 3222.1Mixtral-8x7B-v0.1 (Jiang et al., 2024)ICL1 × 3223.4Llemma-7B (Azerbayev et al., 2023b)ICL1 × 3226.2Llemma-34BICL1 × 3225.8Deepseek-coder-7B-v1.5-Base (Guo et al., 2024)ICL1 × 3228.7Deepseek-math-7B-Base (Shao et al., 2024)ICL1 × 3228.3InternLM2-7B-BaseICL1 × 3222.1InternLM2-20B-BaseICL1 × 3225.4InternLM2-Math-7B-BaseICL1 × 3230.3InternLM2-Math-20B-BaseICL1 × 3229.5</p>
<p>Table 5 :
5
Compare SFT models using zero-shot COT reasoning.The metric is greedy accuracy.
ModelGSM8K MATH Hungary MathBench-ZHQwen-7B-Chat (Alibaba, 2023)51.711.61925.0DeepSeek-7B-Chat (Bi et al., 2024)63.015.828.512.7InternLM2-Chat-7B70.723.0-29.2ChatGLM3-6B (Du et al., 2022)53.820.43215.2MetaMath-Mistral-7B (Jiang et al., 2023)77.728.229-MetaMath-Llemma-7B (Azerbayev et al., 2023b)69.230.0--InternLM2-Math-7B78.134.65540.0InternLM2-Chat-20B79.631.9-37.8MetaMath-Llemma-34B (Azerbayev et al., 2023b)75.834.8--InternLM2-Math-20B82.637.76645.3Qwen-72B-Chat (Alibaba, 2023)78.935.25247.8DeepSeek-67B-Chat (Bi et al., 2024)84.132.65833.2ChatGPT80.834.14121.5GPT-4 (original version)92.042.56847.2</p>
<p>Table 6 :
6
Reranking performance on MATH(500) compared to other baselines.
ModelGreedyMethodPerformanceInternLM-MATH-7B34.6PRM K=10047.0InternLM-MATH-20B37.7PRM K=10050.0MetaMath-Llemma-34B34.8PRM K=25647.3MetaMath-DeepSeek-67B36.8PRM K=25648.1GPT-4-MathMix-ORM K=10071.0GPT-4-MathMix-PRM K=10074.5GPT-4-MathMix-ORM K=186072.4GPT-4-MathMix-PRM K=186078.2</p>
<p>Table 9 :
9
Compare SFT models using Python Code Interpreter.The metric is greedy accuracy.* This is tested on GPT-4 web version in 2023 Aug.</p>
<p>Table 10 :
10
The performance of Game-of-24.
.750.8</p>
<p>Table 11 :
11
Prime number verification test by examining 10 prime numbers and 10 composite numbers for digits from 2 to 10, all ending with 1, 3, 7, or 9 (Ending with other numbers are composite numbers.).The results 'a/b' indicate the number of correct identifications, with 'a' representing prime numbers and 'b' representing composite numbers.
Model2345678910InternLM2-Math-7B10/10 10/10 10/8 10/810/610/7 10/10 10/10 10/10InternLM2-Math-20B 10/10 10/10 10/6 10/9 10/10 10/6 10/1010/910/9gpt-4-0125-preview10/10 10/103/82/90/100/100/101/100/10</p>
<p>Table 12 :
12
The pretrain performance on different pretrain token amounts.
BenchmarkGSM8KMATHTokensICL SFT ICL SFT40B30.9 75.3 19.5 30.280B38.7 77.4 21.4 30.7120B39.0 76.4 21.5 33.8160B38.4 75.8 21.4 31.9200B35.1 76.7 21.1 31.3</p>
<p>Table 15 shows the results.One of the false positive cases writes: Response: I know that for any positive integers m and n, gcd(m, n) • lcm[m, n] = mn.So, if I apply this formula to the given problem, I get 2 • 108 = mn.This means that
False Positive Case StudyQuery: How many ordered pairs of positive integers (m, n) satisfy gcd(m, n) = 2and lcm[m, n] = 108?
m and n are factors of 216.I also know that gcd(m, n) = 2, which means that m and n are both even, and neither of them is divisible by 4. So, I need to find the number of ways to choose two even factors of 216 that are not divisible by 4. I can list the even factors of 216 as follows: 2, 4,</p>
<p>Table 13 :
13
Performance on GSM8K using COT and LEAN with different SFT datasets.
SettingCOT LEANInternLM-Math-Base-7B49.2-+ MetaMath SFT76.4-+ Our SFT78.170.4GSM8K-LEAN 1 ⁄1-58.0GSM8K-LEAN 1 ⁄2-53.1GSM8K-LEAN 1 ⁄4-35.1GSM8K-LEAN 1 ⁄8-38.4GSM8K-LEAN 1 ⁄1 + MetaMath75.766.0GSM8K-LEAN 1 ⁄2 + MetaMath75.665.0GSM8K-LEAN 1 ⁄4 + MetaMath77.758.8GSM8K-LEAN 1 ⁄8 + MetaMath77.453.4</p>
<p>Table 14 :
14
The ablation results of our SFT data composition.
BenchmarksMATH GSM8KInternLM-Math-7B34.6078.09Only MetaMath33.7876.35MetaMath + COT34.0277.63MetaMath + CI33.5677.63MetaMath + Formal32.1276.12MetaMath + RM29.8475.13MetaMath + Augmenter32.0875.89up these factors as follows:</p>
<p>Table 15 :
15
The Process Correctness of in MATH BenchMark using our 20B model.
Difficulty level Results correct Process correctLevel 155Level 254Level 354Level 454Level 553</p>
<p>Table 16 :
16
The detailed SFT data composition information.We list the data types and source types here.OSLLM denotes open-sourced LLMs.</p>
<p>Table 17 :
17
Performances of MATH clustered by category.<em> These results come from our reproduction.
ModelProb. Precal. Inter. Pre-Alg. Alg. Geo. Num. OverallMetaMathLlemma-7B </em>24.514.713.745.643.022.818.328.7InternLM-Math-Base-7B28.516.914.553.550.025.524.133.4InternLM-Math-Base-20B32.319.015.756.354.527.625.436.1Our SFT DataInternLM-Math-7B28.917.017.253.551.427.325.634.6InternLM-Math-20B29.121.819.259.155.930.030.237.7</p>
<p>Table 18 :
18
Performances of MATH clustered by difficulty.<em> These results come from our reproduction.
ModelLevel 1 Level 2 Level 3 Level 4 Level 5 OverallMetaMathLlemma-7B </em>66.144.534.520.58.228.7InternLM-Math-Base-7B74.150.437.925.911.333.4InternLM-Math-Base-20B75.655.341.828.312.236.1Our SFT DataInternLM-Math-7B75.153.039.326.112.534.6InternLM-Math-20B75.555.645.431.713.737.7</p>
<p>Table 19 :
19
Using reward model reranking on multiple math benchmarks.Since we use temperate=0.7 when sampling, it is natural that sampling 1 times perform worse than greedy decoding.
ModelMetric1248163264100
This work is concurrent with Deepseek-Math.
https://github.com/InternLM/InternEvo
The setting is following https://github.com/wellecks/llemma formal2formal
https://github.com/openai/miniF2F/blob/main/LEAN/src/test.LEAN
https://github.com/open-compass/MathBench
ConclusionsWe propose InternLM-Math as our first step toward a verifiable mathematical reasoning ability.The excellent InternLM-Math-Base has the potential for versatile math reasoning tasks.Despite the strong performance in informal and formal reasoning of InternLM-Math, our model can be viewed as a starting point for self-improving.InternLM-Math integrates COT and augment helper abilities can be used for synthesizing new problems and new responses.InternLM-Math obtains the abilities of ORM, PRM, and LEAN can be used for verifying the answers and processes of generated responses.We believe such verifiable data augmentation will improve the model's ability at high throughput.LimitationsChain-of-thought reasoning We match and rewrite the formulation in the spirit of scratchpad during COT while it introduces the following problems.The first problem is we cannot easily match all equations and calculations that need to be rewritten (e.g.We solve x 3 − 3x 2 + 3x − 1 = 0, and obtain x = 1).The second problem is multiple '=' induces models to repeat more.The third problem is this sometimes generates naive step-by-step calculations which may annoy end users and could be alleviated via implicit chain-of-thought reasoning(Deng et al., 2023).We will solve such problems in the future work.No self-critique ability Although our model can conduct ORM or PRM.We do not contain any SFT data to let the model apply self-critique since such data can be hard to generate and verify by any means.We will research how to generate self-critique SFT data with verification.Process reward modelingWe find that our model does not have a significant PRM performance which may be due to the confusing format among PRM and SFT and the unbalanced distribution between positive processes and negative processes.Code-switch Due to our SFT data distribution, English data is larger than Chinese data which will cause code-switch during some instructions or problem formats.SFT data is using LEAN 3 We use LEAN 3 as our SFT data since GPT-4 can only generate LEAN 3 codes for GSM8K (even if you require it to apply LEAN 4).Furthermore, we find the data of translating between formal and informal from MathLib is preprocessed in LEAN 3. We will move our SFT data to LEAN 4 in the future version.Data contamination on LEAN-repoWe use LEAN codes introduced from Algebraic-Stack(Azerbayev et al., 2023b), and we do not check the contamination of AlgebraicStack on MiniF2F.AlgebraicStack may or may not contain MiniF2F solutions.However, the comparison between Llemma and our model is fair.Sensitive to PromptThe models are sensitive to format and given instruction prompts.If we give different prompts before the question (e.g.Question:, Q:, Please solve this question step by step), it may obtain different performances.A SFT Data CompositionWe list our SFT Data Composition in Table16.B SFT Experiment ConfigsOur SFT model is initialized from the pre-trained InternLM-Math-Base models, respectively.The peak learning rate is set to lr max = 4e − 5, and the minimal learning rate during warming up is lr max = 6e − 6.These settings are the same for both 7B and 20B models.The SFT training data is tokenized into 622.7 M tokens.The batch size is set to 1 per GPU with a packed dataset whose max sequence length is 32768.We train the model on these data using Adam Optimizer for three epochs with 32 GPUs, and the training procedure takes around 7 hours to finish for 7B models.For the 20B model, we will use 64 GPUs and the training procedure takes about 10 hours.C Detailed performance on MATH benchmarkWe list detailed performance on the MATH benchmark in Table17 and Table 18.We list reranking performance on GSM8K and MATH in Table19.D Case studiesWe list cases generated by InternLM-Math-7B.Prime number Check Query: Check if 5287 is a prime or not.Response: [ √ 5287] = 72 We need to check prime numbers between 3 to 72.Prime numbers:3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67Outcome Reward Model Query: Given a question and an answer, check is it correct?Question: Determine the number of ways to arrange the letters of the word THAT.Answer: The word THAT has 4 letters.We can arrange these 4 letters in 4! = 4 * 3 * 2 * 1 = 24 ways.Therefore, there are 24 ways to arrange the letters of the word THAT.The answer is: 24.Response: This answer is wrong.Process Reward ModelQuery: Given a question and an answer, check is it correct?Question: Camy made a list of every possible distinct five-digit positive integer that can be formed using each of the digits 1, 3, 4, 5 and 9 exactly once in each integer.What is the sum of the integers on Camy's list?Answer: There are 5! = 120 possible five-digit positive integers that can be formed using each of the digits 1, 3, 4, 5, and 9 exactly once.The sum of these integers can be found by considering the sum of each place value.For the ones place, the sum is 1 • 120 = 120.For the tens place, the sum is 3 • 120 = 360.For the hundreds place, the sum is 4 • 120 = 480.For the thousands place, the sum is 5 • 120 = 600.For the ten thousands place, the sum is 9 • 120 = 1080.Adding up all these sums, we get 120 + 360 + 480 + 600 + 1080 = 2880 .The answer is: 2880.Response:Step 1: There are 5! = 120 possible five-digit positive integers that can be formed using each of the digits 1, 3, 4, 5, and 9 exactly once.<PRM +>Step 2: The sum of these integers can be found by considering the sum of each place value.<PRM +> Step 3: For the ones place, the sum is 1 • 120 = 120.<PRM ->Step 4: For the tens place, the sum is 3 • 120 = 360.<PRM ->Step 5: For the hundreds place, the sum is 4 • 120 = 480.<PRM ->Step 6: For the thousands place, the sum is 5 • 120 = 600.<PRM ->Step 7: For the ten thousands place, the sum is 9E Results on InternLM-Math-PlusWe improve InternLM-Math to InternLM-Math-Plus with new pre-training corpora and fine-tuning datasets.We have four sizes of InternLM-Math-Plus including 1.8B, 7B, 20B, and 8x22B.We initialize our models (1.8B, 7B, and 20B) from InternLM-2, and 8x22B from Mixtral-8x22B(Jiang et al., 2024).We evaluate the performance of InternLM2-Math-Plus on formal math reasoning benchmark MiniF2F-test via LEAN 4 (in Table20) and informal math reasoning benchmarks MATH, GSM8K, and MathBench-A(Liu et al., 2024)(in Table22).We further test MATH using Python (in Table21).
. Alibaba. Qwen technical report. 2023</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, 2023a</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2023b</p>
<p>Deepseek llm: Scaling open-source language models with longtermism. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, arXiv:2401.029542024arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T J Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, ArXiv, abs/2005.141652020218971783</p>
<p>Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mcgrew, Dario Amodei, Sam Mccandlish, Ilya Sutskever, Wojciech Zaremba, Andrew N. CarrJan Leike. 2021. 2023a</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2023b</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Opencompass: A universal evaluation platform for foundation models. 2023</p>
<p>FlashAttention-2: Faster attention with better parallelism and work partitioning. Tri Dao, 2023</p>
<p>Implicit chain of thought reasoning via knowledge distillation. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber, ArXiv, abs/2311.014602023</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, Dahua Lin, 10.48550/arXiv.2401.14624arXiv:2401.14624Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. arXiv e-prints, art. January 2024</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 2023</p>
<p>Temperaturescaled large language models for lean proofstep prediction. Fabian Gloeckle, Amaury Baptiste Roziere, Gabriel Hayat, Synnaeve, The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23. 2023</p>
<p>A family of highly capable multimodal models. Google, Gemini, 2023</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2023</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 2024</p>
<p>Proof artifact co-training for theorem proving with language models. Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, Stanislas Polu, arXiv:2102.062032021arXiv preprint</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021barXiv preprint</p>
<p>Unveiling the potential of small language models with scalable training strategies. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, arXiv:2404.063952024arXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 2022</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. Internlm, 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>. Khan, Khan, 2021</p>
<p>Solving quantitative reasoning problems with language models. Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, Xavier Martinet, Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022. 2022a35Hypertree proof search for neural theorem proving</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Slone, NeurIPS. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, 2022b</p>
<p>Query and response augmentation cannot help out-of-domain math reasoning generalization. Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou, arXiv:2310.055062023arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Augmenting math word problems via iterative question composing. Haoxiong Liu, Andrew , Chi-Chih Yao, 2024</p>
<p>Mathbench: Evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, Kai Chen, arXiv:2305.14201Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2024. 2023arXiv preprint</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, 10.48550/arXiv.1711.05101arXiv:1711.05101November 2017arXiv e-prints</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023</p>
<p>The mathlib Community. The lean mathematical library. Math-Eval, Tal-Scq5k, 10.1145/3372885.3373824Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL '20. the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL '20ACM2023. January 2020</p>
<p>The lean 4 theorem prover and programming language. Leonardo De, Moura , Sebastian Ullrich, Automated Deduction -CADE 28. André Platzer, Geoff Sutcliffe, ChamSpringer International Publishing2021</p>
<p>Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. 2023</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, ArXiv, abs/2112.001142021</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba, arXiv:2310.06786Openwebmath: An open dataset of high-quality mathematical web text. 2023arXiv preprint</p>
<p>Isabelle: The next 700 theorem provers. Lawrence C Paulson, 2000</p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. 2022</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. 2023</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Brian Warkentin, Yundi Qian, Ethan Dyer, Behnam Neyshabur, ArXiv, abs/2312.06585Jascha Narain Sohl-Dickstein, and Noah Fiedel. 2023</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. 2023</p>
<p>The Coq Development Team. The Coq reference manual -release 8. 18.02023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 2022</p>
<p>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023a</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, CoRR, abs/2312.089352023b</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023c</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, 10.18653/v1/D17-1088Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Martha Palmer, Rebecca Hwa, Sebastian Riedel, the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSeptember 2017</p>
<p>Zengzhi Wang, Rui Xia, Liu Pengfei, arXiv:2312.17120Generative ai for math: Part i -mathpile: A billion-token-scale pretraining corpus for math. 2023darXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Sean Welleck, Rahul Saha, arXiv:2310.18457Llmstep: Llm proofstep suggestions in lean. 2023arXiv preprint</p>
<p>Leandojo: Theorem proving with retrievalaugmented language models. Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, arXiv:2306.156262023aarXiv preprint</p>
<p>Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang, arXiv:2309.03241Gpt can solve mathematical problems without a calculator. 2023barXiv preprint</p>
<p>Tree of Thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Outcome-supervised verifiers for planning in mathematical reasoning. Fei Yu, Anningzhe Gao, Benyou Wang, 2023a</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, Bootstrap your own mathematical questions for large language models. 2023b</p>
<p>Advancing llm reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, 2023</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.056532023arXiv preprint</p>
<p>STar: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Lyra: Orchestrating dual correction in automated theorem proving. Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, Yu Li, 2023</p>
<p>Kunhao Zheng, Jesse Michael Han, Stanislas Polu, arXiv:2109.00110Minif2f: a cross-system benchmark for formal olympiad-level mathematics. 2021arXiv preprint</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023</p>
<p>Solving math word problems via cooperative reasoning induced language models. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, 245. 7B/GSM8K MAJ 79.0 79.0 81.8 82.9 83.3 83.2 83 83.5 ORM 79.0 80.0 82.3 83.5 83.6 83.8 83.9 84.2Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Table 20: Performance of various models on MiniF2F-test. Oracle 32.6 44.8 54.0 61.2 66.8 73.0 79.6 81.8Models MiniF2F-test</p>
<p>. Yang Reprover, 2023a26</p>
<p>. ( Llmstep, Welleck, Saha, 202327</p>
<p>Polu, GPT-F Expert Iteration. 202236</p>
<p>. Htps (lample, 41.02022</p>
<p>Table 21: Performance of various models on MATH and GSM8K. Models MATH MATH-Python GSM8K. 37.35 InternLM2-Math-Plus-1.8B 38.9 InternLM2-Math-Plus-7B 43.4 InternLM2-Math-Plus-20B 42.6 InternLM2-Math-Plus-Mixtral8x22BInternLM2-Math-7B-Base 30.3 InternLM2-Math-20B-Base 29</p>
<p>. Minicpm-2b ( Hu, 8B 37.0 41.5 58202488 InternLM2-Math-Plus-1.</p>
<p>. Deepseek-Math-7b-Rl ( Shao, 2024517 58.8 88.2 InternLM2-Math-Plus-7B 53.0 59.7 85.8 InternLM2-Math-20B 37.7 54.3 82.6 InternLM2-Math-Plus-20B</p>
<p>. Jiang, Mixtral8x22B-Instruct-v0.12024</p>
<p>Table 22: Performance of various models on MathBench-A. InternLM2-Math-Plus Arithmetic Primary Middle High College Average. Eurux-8x22b-Nca ( Yuan, 58.1 68.5 91.820248B0 --InternLM2-Math-Plus-Mixtral8x22B</p>            </div>
        </div>

    </div>
</body>
</html>
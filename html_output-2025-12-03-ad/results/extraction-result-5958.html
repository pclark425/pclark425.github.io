<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5958 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5958</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5958</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-51b7b3ad7645a69e3c1c80cae69473b8bd472f67</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/51b7b3ad7645a69e3c1c80cae69473b8bd472f67" target="_blank">ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes ResearchAgent, a system that automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents, to enhance the productivity of researchers.</p>
                <p><strong>Paper Abstract:</strong> The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5958.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5958.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-augmented LLM system that generates open-ended research ideas (problem, method, experiment) by combining citation-graph literature retrieval, an entity-centric knowledge store built from tens of thousands of papers, and iterative LLM-based reviewing agents aligned to human-induced criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (primary), with auxiliary experiments using GPT-3.5, Llama3, Mixtral, Qwen1.5</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 (OpenAI release Nov 06, 2023) — a high-capability transformer-based LLM reported to be trained on web data up to Apr 2023; used as the main generation and evaluation model. Auxiliary experiments evaluate lower-capability/other LLMs (GPT-3.5, Llama3, Mixtral, Qwen1.5) to measure sensitivity to model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Distill cross-paper concepts and relationships from large collections of scholarly articles to generate novel, clear, and feasible research ideas (problems, methods, experiments), i.e., to surface generalizable cross-domain relationships and heuristics that suggest new research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multidisciplinary (Computer Science, Medicine, Engineering, Biology, etc.) — applied across many domains represented in Semantic Scholar data.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Combine (1) a citation-graph based literature survey (core paper + selected references), (2) an entity-centric knowledge store built from entity extraction (BLINK) across ~50k papers (titles/abstracts) with co-occurrence counts (sparse m x m matrix), (3) probabilistic top-k entity retrieval (co-occurrence‑based; optionally embedding-based), (4) prompt-based LLM generation for problems/methods/experiments, and (5) iterative refinement via multiple LLM ReviewingAgents prompted with human-preference-induced evaluation criteria; human annotators used to induce/validate evaluation criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Cross-paper conceptual relationships and heuristic principles for proposing research directions (e.g., non-trivial links between entities across domains that suggest new hypotheses or methodological crossovers); in effect, distilled heuristics about which concepts/entities co-occur and how they can generate novel research questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert scoring on five task-specific criteria per sub-idea (Likert 1–5), model-based (GPT-4) evaluations aligned to human-induced criteria, pairwise win-rate comparisons, inter-annotator agreement (Spearman rank correlation for scores, Cohen's kappa for pairwise), and ablation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ResearchAgent (knowledge-augmented + iterative refinement) outperforms ablated variants on human and model-based evaluations. Example numeric results: ResearchAgent mean scores (Problem/Method/Experiment) ≈ 4.52 / 4.28 / 4.18 versus ablations (e.g., -w/o entities: 4.35 / 4.13 / 4.02; -w/o references: 4.26 / 4.08 / 3.97). Iterative refinement improves quality up to ~3 iterations (performance saturates thereafter). Ablations and random-element controls show both references and entities contribute; providing random elements is sometimes better than none. Performance drops substantially with less-capable LLMs (GPT-3.5, Mixtral), indicating reliance on emergent reasoning capabilities of larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop: (a) Ten domain-expert annotators provided example judgments used to induce evaluation criteria; (b) expert human evaluation of generated ideas; (c) humans used to judge alignment/calibration of model evaluations. ReviewingAgents are LLM instances using criteria induced from human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Semantic Scholar Academic Graph (papers published after May 01, 2023) with: 50,091 papers used to build the entity store (titles/abstracts), and a benchmark of 300 high-impact core papers (each with ≈87 references on average) for idea generation/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Entity store limited to titles/abstracts and to a subset of papers (cost constraints) → limited entity coverage (~2.17 entities per abstract average; BLINK yields ~3 entities/paper), BLINK is open-domain (not specialized) causing coverage gaps. LLMs can hallucinate ideas; entity/context augmentation mitigates but does not eliminate this. Iterative ReviewingAgents use a fixed (limited) set of criteria and perspectives (15 ReviewingAgents, 5 criteria each), possibly missing broader domain viewpoints. System less suited to theoretical domains requiring formal proofs. Scaling and keeping the knowledge store current are practical challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Qualitative example: entity-centric augmentation retrieved entities such as 'Drosophila Genetic Reference Panel' and 'CRISPR' allowing the system to propose a novel idea bridging genetic variability and CRISPR applications — illustrating extraction of a non-trivial cross-domain relation from aggregated entity co-occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5958.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5958.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-HypothesisGenerators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recent LLM-based hypothesis generation approaches (cited: Wang et al. 2023b; Qi et al. 2023; Yang et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited body of recent work that leverages LLMs to propose hypotheses or novel scientific directions from literature, typically producing localized relationships (often between two concepts) or sentence-level hypothesis statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>various (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Recent approaches leverage pre-trained LLMs' prior knowledge to propose hypotheses (zero-shot or few-shot prompting); the specific model families are not specified in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate novel hypotheses or directional links between scientific concepts drawn from literature (literature-based discovery augmented by LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Primarily biomedical and multidisciplinary literature-based discovery tasks (as discussed in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompting LLMs to suggest hypotheses from a focused literature context; earlier non-LLM methods used vector similarities/link prediction over concept graphs, while recent works augment or replace those steps by LLM prompting/zero-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Pairwise or sentence-level hypotheses linking concepts (e.g., 'compound X may treat disease Y'); not full multi-component research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typically judged for relevance, novelty, and feasibility; exact evaluation protocols are not detailed in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper reports that LLM-powered methods can propose hypotheses but are often localized (binary links) and may be sub-optimal for capturing complex, multi-variable, open-ended research questions compared with ResearchAgent's broader idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>These prior approaches commonly include human evaluation of generated hypotheses; specifics vary by work and are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Varies by referenced work; literature corpora for domain-specific hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited to localized relations or single-link hypotheses, and may not capture multifaceted, open-ended research problems involving many interacting variables.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>No specific distilled laws quoted in this paper; the paper contrasts these methods' single-link hypotheses with ResearchAgent's open-ended idea outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5958.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5958.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI4Science-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The impact of large language models on scientific discovery: a preliminary study using gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited preliminary study by Microsoft Research demonstrating GPT-4's capabilities across scientific tasks such as understanding DNA sequences, designing biomolecules, predicting molecular behavior, and solving PDE problems — evidence of LLMs' ability to internalize and apply scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The impact of large language models on scientific discovery: a preliminary study using gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 (OpenAI) — high-capability LLM used in the cited study for diverse scientific tasks; used here as supporting evidence for LLM suitability to scientific discovery problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Demonstrate and evaluate GPT-4's capacity to assist or perform components of scientific discovery/experimental validation (e.g., biomolecule design, PDE solving).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Molecular biology, computational physics, and other scientific subdomains explored in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Application of GPT-4 to specific domain tasks (design, prediction, problem-solving); cited as part of background motivating ResearchAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not directly focused on distilling qualitative laws from literature; demonstrates LLMs' domain reasoning and pattern-recognition abilities that can support discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper's citation; referenced as evidence of broad LLM capability rather than specific law-distillation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as showing GPT-4's broad scientific capabilities, supporting the decision to rely on GPT-4 as the main LLM in ResearchAgent experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Not specified here; the citation is used for background motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Not specified in this paper's citation context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cited as preliminary; not discussed in detail in this paper beyond pointing to GPT-4's demonstrated capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery <em>(Rating: 2)</em></li>
                <li>The impact of large language models on scientific discovery: a preliminary study using gpt-4 <em>(Rating: 2)</em></li>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5958",
    "paper_id": "paper-51b7b3ad7645a69e3c1c80cae69473b8bd472f67",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "brief_description": "A knowledge-augmented LLM system that generates open-ended research ideas (problem, method, experiment) by combining citation-graph literature retrieval, an entity-centric knowledge store built from tens of thousands of papers, and iterative LLM-based reviewing agents aligned to human-induced criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4 (primary), with auxiliary experiments using GPT-3.5, Llama3, Mixtral, Qwen1.5",
            "llm_model_description": "GPT-4 (OpenAI release Nov 06, 2023) — a high-capability transformer-based LLM reported to be trained on web data up to Apr 2023; used as the main generation and evaluation model. Auxiliary experiments evaluate lower-capability/other LLMs (GPT-3.5, Llama3, Mixtral, Qwen1.5) to measure sensitivity to model capacity.",
            "task_goal": "Distill cross-paper concepts and relationships from large collections of scholarly articles to generate novel, clear, and feasible research ideas (problems, methods, experiments), i.e., to surface generalizable cross-domain relationships and heuristics that suggest new research directions.",
            "domain": "Multidisciplinary (Computer Science, Medicine, Engineering, Biology, etc.) — applied across many domains represented in Semantic Scholar data.",
            "methodology": "Combine (1) a citation-graph based literature survey (core paper + selected references), (2) an entity-centric knowledge store built from entity extraction (BLINK) across ~50k papers (titles/abstracts) with co-occurrence counts (sparse m x m matrix), (3) probabilistic top-k entity retrieval (co-occurrence‑based; optionally embedding-based), (4) prompt-based LLM generation for problems/methods/experiments, and (5) iterative refinement via multiple LLM ReviewingAgents prompted with human-preference-induced evaluation criteria; human annotators used to induce/validate evaluation criteria.",
            "type_of_qualitative_law": "Cross-paper conceptual relationships and heuristic principles for proposing research directions (e.g., non-trivial links between entities across domains that suggest new hypotheses or methodological crossovers); in effect, distilled heuristics about which concepts/entities co-occur and how they can generate novel research questions.",
            "evaluation_metrics": "Human expert scoring on five task-specific criteria per sub-idea (Likert 1–5), model-based (GPT-4) evaluations aligned to human-induced criteria, pairwise win-rate comparisons, inter-annotator agreement (Spearman rank correlation for scores, Cohen's kappa for pairwise), and ablation comparisons.",
            "results_summary": "ResearchAgent (knowledge-augmented + iterative refinement) outperforms ablated variants on human and model-based evaluations. Example numeric results: ResearchAgent mean scores (Problem/Method/Experiment) ≈ 4.52 / 4.28 / 4.18 versus ablations (e.g., -w/o entities: 4.35 / 4.13 / 4.02; -w/o references: 4.26 / 4.08 / 3.97). Iterative refinement improves quality up to ~3 iterations (performance saturates thereafter). Ablations and random-element controls show both references and entities contribute; providing random elements is sometimes better than none. Performance drops substantially with less-capable LLMs (GPT-3.5, Mixtral), indicating reliance on emergent reasoning capabilities of larger models.",
            "human_involvement": "Human-in-the-loop: (a) Ten domain-expert annotators provided example judgments used to induce evaluation criteria; (b) expert human evaluation of generated ideas; (c) humans used to judge alignment/calibration of model evaluations. ReviewingAgents are LLM instances using criteria induced from human annotations.",
            "dataset_or_corpus": "Semantic Scholar Academic Graph (papers published after May 01, 2023) with: 50,091 papers used to build the entity store (titles/abstracts), and a benchmark of 300 high-impact core papers (each with ≈87 references on average) for idea generation/evaluation.",
            "limitations_or_challenges": "Entity store limited to titles/abstracts and to a subset of papers (cost constraints) → limited entity coverage (~2.17 entities per abstract average; BLINK yields ~3 entities/paper), BLINK is open-domain (not specialized) causing coverage gaps. LLMs can hallucinate ideas; entity/context augmentation mitigates but does not eliminate this. Iterative ReviewingAgents use a fixed (limited) set of criteria and perspectives (15 ReviewingAgents, 5 criteria each), possibly missing broader domain viewpoints. System less suited to theoretical domains requiring formal proofs. Scaling and keeping the knowledge store current are practical challenges.",
            "notable_examples": "Qualitative example: entity-centric augmentation retrieved entities such as 'Drosophila Genetic Reference Panel' and 'CRISPR' allowing the system to propose a novel idea bridging genetic variability and CRISPR applications — illustrating extraction of a non-trivial cross-domain relation from aggregated entity co-occurrences.",
            "uuid": "e5958.0",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-HypothesisGenerators",
            "name_full": "Recent LLM-based hypothesis generation approaches (cited: Wang et al. 2023b; Qi et al. 2023; Yang et al. 2023)",
            "brief_description": "Cited body of recent work that leverages LLMs to propose hypotheses or novel scientific directions from literature, typically producing localized relationships (often between two concepts) or sentence-level hypothesis statements.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_model_name": "various (unspecified in this paper)",
            "llm_model_description": "Recent approaches leverage pre-trained LLMs' prior knowledge to propose hypotheses (zero-shot or few-shot prompting); the specific model families are not specified in this paper's discussion.",
            "task_goal": "Generate novel hypotheses or directional links between scientific concepts drawn from literature (literature-based discovery augmented by LLMs).",
            "domain": "Primarily biomedical and multidisciplinary literature-based discovery tasks (as discussed in related work).",
            "methodology": "Prompting LLMs to suggest hypotheses from a focused literature context; earlier non-LLM methods used vector similarities/link prediction over concept graphs, while recent works augment or replace those steps by LLM prompting/zero-shot generation.",
            "type_of_qualitative_law": "Pairwise or sentence-level hypotheses linking concepts (e.g., 'compound X may treat disease Y'); not full multi-component research ideas.",
            "evaluation_metrics": "Typically judged for relevance, novelty, and feasibility; exact evaluation protocols are not detailed in this paper's summary.",
            "results_summary": "Paper reports that LLM-powered methods can propose hypotheses but are often localized (binary links) and may be sub-optimal for capturing complex, multi-variable, open-ended research questions compared with ResearchAgent's broader idea generation.",
            "human_involvement": "These prior approaches commonly include human evaluation of generated hypotheses; specifics vary by work and are not detailed in this paper.",
            "dataset_or_corpus": "Varies by referenced work; literature corpora for domain-specific hypothesis generation.",
            "limitations_or_challenges": "Limited to localized relations or single-link hypotheses, and may not capture multifaceted, open-ended research problems involving many interacting variables.",
            "notable_examples": "No specific distilled laws quoted in this paper; the paper contrasts these methods' single-link hypotheses with ResearchAgent's open-ended idea outputs.",
            "uuid": "e5958.1",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AI4Science-GPT4",
            "name_full": "The impact of large language models on scientific discovery: a preliminary study using gpt-4",
            "brief_description": "A cited preliminary study by Microsoft Research demonstrating GPT-4's capabilities across scientific tasks such as understanding DNA sequences, designing biomolecules, predicting molecular behavior, and solving PDE problems — evidence of LLMs' ability to internalize and apply scientific knowledge.",
            "citation_title": "The impact of large language models on scientific discovery: a preliminary study using gpt-4",
            "mention_or_use": "mention",
            "llm_model_name": "GPT-4",
            "llm_model_description": "GPT-4 (OpenAI) — high-capability LLM used in the cited study for diverse scientific tasks; used here as supporting evidence for LLM suitability to scientific discovery problems.",
            "task_goal": "Demonstrate and evaluate GPT-4's capacity to assist or perform components of scientific discovery/experimental validation (e.g., biomolecule design, PDE solving).",
            "domain": "Molecular biology, computational physics, and other scientific subdomains explored in the cited study.",
            "methodology": "Application of GPT-4 to specific domain tasks (design, prediction, problem-solving); cited as part of background motivating ResearchAgent.",
            "type_of_qualitative_law": "Not directly focused on distilling qualitative laws from literature; demonstrates LLMs' domain reasoning and pattern-recognition abilities that can support discovery.",
            "evaluation_metrics": "Not detailed in this paper's citation; referenced as evidence of broad LLM capability rather than specific law-distillation benchmarks.",
            "results_summary": "Cited as showing GPT-4's broad scientific capabilities, supporting the decision to rely on GPT-4 as the main LLM in ResearchAgent experiments.",
            "human_involvement": "Not specified here; the citation is used for background motivation.",
            "dataset_or_corpus": "Not specified in this paper's citation context.",
            "limitations_or_challenges": "Cited as preliminary; not discussed in detail in this paper beyond pointing to GPT-4's demonstrated capabilities.",
            "uuid": "e5958.2",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "rating": 2
        },
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using gpt-4",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "rating": 1
        }
    ],
    "cost": 0.016195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</h1>
<p>Jinheon Baek ${ }^{1}$ Sujay Kumar Jauhar ${ }^{2}$ Silviu Cucerzan ${ }^{2}$ Sung Ju Hwang ${ }^{1,3}$<br>KAIST ${ }^{1}$ Microsoft Research ${ }^{2}$ DeepAuto.ai ${ }^{3}$<br>{jinheon.baek, sungju.hwang}@kaist.ac.kr {sjauhar, silviu}@microsoft.com</p>
<h4>Abstract</h4>
<p>The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and modelbased evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Scientific research plays a crucial role in driving innovation, advancing knowledge, solving prob-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (A) The scientific knowledge used for research idea generation consists of a paper, its relationships over an academic graph, and entities within a knowledge store extracted from numerous papers. (B) Given them, the proposed research idea generation process involves problem identification, method development, and experiment design. Those are also iteratively refined by reviews and feedback from reviewing agents, aligned with criteria induced from human judgements.
lems, expanding our understanding of the world, and ultimately improving the lives of people in tangible ways. This process usually consists of two key components: the formulation of new research ideas and the validation of these ideas through wellcrafted experiments, which are typically conducted by human researchers (Hope et al., 2023; Wang et al., 2023a; Huang et al., 2023). However, this is a slow, effort-intensive process, which requires reading and synthesizing overwhelming amounts of knowledge over the vast corpus of rapidly growing scientific literature to formulate research ideas, as well as design and perform experimental validations of those ideas. For example, the number of academic papers published per year is more than 7 million (Fire and Guestrin, 2019). Similarly, the process of testing a new pharmaceutical drug requires deep expertise, and is massively expensive and labor-intensive, often taking several years (Vamathevan et al., 2019).</p>
<p>In the meantime, recent Large Language Models (LLMs) (Touvron et al., 2023; OpenAI, 2023; Anil et al., 2023) have shown impressive capabili-</p>
<p>ties in processing and generating text with remarkable accuracy, even outperforming human experts across diverse specialized domains including math, physics, history, law, medicine, and ethics. They are able to process and analyze large volumes of data at speeds and scales far exceeding human capabilities, have internalized large swaths of human knowledge from being trained on virtually the entire web, and can identify patterns, trends, and correlations that may not be immediately apparent to human researchers (such as the usage of quantum mechanics in medical imaging or applying psychological insights in AI). This renders them ideally poised to become foundational tools to accelerate the two phases of the scientific research process: ideation of novel research opportunities, and scientific validation of those research hypotheses.</p>
<p>A few recent papers in the domain of LLMaugmented scientific discovery have focused on the second phase. Specifically, they attempt (Huang et al., 2023; AI4Science and Quantum, 2023; Bran et al., 2023) to mainly accelerate the experimental validation process, by writing code for machinelearning models, facilitating the exploration of chemical spaces, or advancing the simulation of molecular dynamics. Thus, in this paper, we leverage LLMs in the first phase of scientific research - specifically idea generation, whose key focus is conceptualizing novel research questions, methodologies, and experiments. To the best of our knowledge, our work is the first to leverage and evaluate the capabilities of LLMs to act as mediators in scientific idea generation in an open-ended setting.</p>
<p>Given our goal to build an LLM-powered ResearchAgent, we draw inspiration from how human researchers position themselves to come up with novel research ideas. We draw distinctions between three key components of their workflow: a broad and deep understanding of related scientific literature, an encyclopedic view of concepts and how they relate to one another both within and across domains, and a community of colleagues on which to rely for feedback and constructive criticism.</p>
<p>We model each of these three aspects in our ResearchAgent. Specifically, in order to imbibe related work, the system begins with a core scientific paper and then explores a range of related papers through references and citation relationships. Further, to develop an encyclopedic view of related concepts, we build and then augment ResearchAgent with an entity-centric knowledge store derived from co-occurrences of key concepts in the scien-
tific literature. This repository is aimed at capturing novel underlying relationships within and across domains, thereby increasing the chances of a crosspollination of ideas (Wahle et al., 2023). Finally, to simulate robust feedback mechanisms, we instantiate a number of LLM-powered ReviewingAgents that help the ResearchAgent to iterate on research idea generation with constructive critiques. Crucially, these ReviewingAgents are prompted with evaluation criteria that are induced from real researchers' judgements, thus aligning them with actual scientific preferential standards. An illustration of our system is provided in Figure 1.</p>
<p>We validate the effectiveness of ResearchAgent for research idea generation based on scientific literature across multiple disciplines. Then, on a battery of tests conducted with both human- and modelbased evaluations, we demonstrate that ResearchAgent outperforms strong LLM-powered baselines by large margins, generating more clear, relevant, and significant ideas that are especially novel. Furthermore, analyses show the efficacy of our comprehensive approach to modeling ResearchAgent: the entity-centric knowledge store and the iterative idea refinement steps help the system generate meaningfully better ideas compared with an instantiation that is purely based on prior related work.</p>
<p>These findings highlight the immense potential of AI-mediated research assistants like ResearchAgent to enhance the ideation process in scientific research. In practice, it can support researchers by identifying knowledge gaps, proposing novel problem statements, and suggesting potential methodologies early in the research process. Also, it can assist in designing experiments and streamline the writing and refinement of research papers by generating drafts and offering feedback on how to effectively frame contributions and cite relevant work.</p>
<h2>2 Related Work</h2>
<p>Large Language Models LLMs have shown impressive performances across various tasks (OpenAI, 2023; Anil et al., 2023), including scientific fields such as mathematics, physics, medicine, and computer science (Portenoy et al., 2021; RomeraParedes et al., 2023; Bran et al., 2023; Huang et al., 2023; Liu et al., 2024). For instance, GPT-4 can understand DNA sequences, design biomolecules, predict molecular behavior, and solve PDE problems (AI4Science and Quantum, 2023). However, LLMs have mainly been used for accelerating the</p>
<p>experimental validation of already identified research ideas, but not for identifying new problems.</p>
<p>Hypothesis Generation The principle of hypothesis generation is based on literature-based discovery (Swanson, 1986), which aims to discover relationships between concepts (Henry and McInnes, 2017). For instance, these concepts could be a specific disease and a compound not yet considered as a treatment for it. Early works on automatic hypothesis generation first build a corpus of discrete concepts, and then identify their relationships with machine learning approaches, e.g., using similarities between word (concept) vectors (Tshitoyan et al., 2019) or applying link prediction methods over a graph (where concepts are nodes) (Sybrandt et al., 2020; Nadkarni et al., 2021). Recent approaches are further powered by LLMs (Wang et al., 2023b; Qi et al., 2023; Yang et al., 2023), leveraging their prior knowledge about scientific disciplines. Yet, all these approaches perform idea generation in a localized manner and are designed to identify potential relationships between two variables or generate sentence-level connections, which may be sub-optimal to capture the complexity and multifaceted nature of real-world problems (e.g., urban planning involves numerous interacting variables). Meanwhile, we do not artificially restrict the target research idea to be a predictive single concept or simple binary link, instead allowing the model to generate ideas in a more open-ended fashion.</p>
<p>We note that there has been a recent surge of interest in exploring scientific idea generation: from Li et al. (2024) that focus on evaluating whether LLMs can generate research ideas that are better than human ideas, to Lu et al. (2024) that aim to automatically generate full research papers (including idea development, code writing, and experiment execution), to Li et al. (2024) that enhance the idea generation process by organizing a sequential chain of literature, all of which acknowledge and build upon insights from a prior version of our paper.</p>
<p>Knowledge-Augmented LLMs The approach to augment LLMs with external knowledge makes them more accurate and relevant to target contexts. Much prior work aims at improving the factuality of LLM responses to queries by retrieving the relevant documents and injecting them into the LLM input (Lazaridou et al., 2022; Ram et al., 2023; Shi et al., 2023). In addition, given that entities or facts are atomic units for representing knowledge, recent studies augment LLMs with them (Baek et al., 2023; Wu et al., 2023). In contrast to these efforts, which use knowledge units piecemeal, we instead jointly leverage accumulated knowledge over massive troves of scientific papers. Also, Baek et al. (2024) proposes to use entities for query suggestion, which - while similar - has the entirely different objective of narrowing the focus of LLMs to entities already present in their context. Instead, our approach retrieves and integrates entities outside the given context, enabling LLMs to explore other concepts for research idea generation.</p>
<p>Iterative Refinements with LLMs Similar to humans, LLMs do not always generate optimal outputs on their first attempt. To tackle this, drawing inspiration from humans who can iteratively refine their thoughts based on critiques from themselves and their peers, many recent studies have investigated the potential of LLMs to correct and refine their outputs, demonstrating that they indeed possess those capabilities (Welleck et al., 2023; Madaan et al., 2023; Shridhar et al., 2023; Ganguli et al., 2023; Wang et al., 2023b; Qi et al., 2023; Yang et al., 2023). Based on their findings, we extend this paradigm (and further test their capability) to our novel scenario of research idea generation.</p>
<h2>3 Method</h2>
<p>We present ResearchAgent, a system that automatically proposes research ideas with LLMs.</p>
<h3>3.1 LLM-Powered Research Idea Generation</h3>
<p>We begin by formally introducing the new problem of research idea generation, followed by an explanation of how LLMs are utilized to tackle it.</p>
<p>Research Idea Generation The goal of the research idea generation task is to formulate new and valid research ideas, to enhance the overall efficiency of the first phase of scientific discovery. While we acknowledge that the real process by which humans conduct research is varied and complex to an extent well beyond the scope of this scientific study, we attempt to model simulacra in three systematic steps that would likely be maximally beneficial to a researcher seeking assistance from an AI system. These are namely, identifying novel research ideas, proposing methods to validate these ideas, and designing experiments to measure the success of these methods in relation to the ideas.</p>
<p>To accomplish the aforementioned steps, we utilize the existing literature (such as academic pub-</p>
<p>lications) as a primary source, which provides insights about existing knowledge along with gaps and unanswered questions ${ }^{2}$. Formally, let $\mathcal{L}$ be the literature, and $\boldsymbol{o}$ be the ideas that consist of the problem $\boldsymbol{p}$, method $\boldsymbol{m}$, and experiment design $\boldsymbol{d}$, as follows: $\boldsymbol{o}=[\boldsymbol{p}, \boldsymbol{m}, \boldsymbol{d}]$ where each item consists of a sequence of tokens. Then, the idea generation model $f$ can be represented as follows: $\boldsymbol{o}=f(\mathcal{L})$, which is further decomposed into three submodular steps: $\boldsymbol{p}=f(\mathcal{L})$ for identifying problems, $\boldsymbol{m}=f(\boldsymbol{p}, \mathcal{L})$ for developing methods, and $\boldsymbol{d}=f(\boldsymbol{p}, \boldsymbol{m}, \mathcal{L})$ for designing experiments. We operationalize $f$ with LLMs, leveraging their capability to understand and generate academic text.</p>
<p>Large Language Models Before describing the LLM in the context of our problem setup, let us first provide its general definition, which takes an input sequence of tokens $\boldsymbol{x}$ and generates an output sequence of tokens $\boldsymbol{y}$, as follows: $\boldsymbol{y}=\operatorname{LLM}_{\theta}(\mathcal{T}(\boldsymbol{x}))$. Here, the model parameters $\theta$ are typically fixed after training, due to the high costs of further finetuning. In addition, the prompt template $\mathcal{T}$ serves as a structured format that outlines the context (including the task descriptions and instructions) to direct the model in generating the desired outputs.</p>
<h3>3.2 Knowledge-Augmented LLMs for Research Idea Generation</h3>
<p>We now turn to our primary focus of automatically generating research ideas with LLMs. Recall that we aim to produce a complete idea consisting of the problem, method, and experiment design ( $\boldsymbol{o}=[\boldsymbol{p}, \boldsymbol{m}, \boldsymbol{d}]$ ), while using the existing literature $\mathcal{L}$ as a primary source of information. We operationalize this with LLMs by instantiating the aforementioned research idea generation function $f$ with LLM coupled with the taskspecific template. Formally, $\boldsymbol{p}=\operatorname{LLM}\left(\mathcal{T}<em m="m">{p}(\mathcal{L})\right)$ indicates the problem identification step, followed by $\boldsymbol{m}=\operatorname{LLM}\left(\mathcal{T}</em>]$.}(\boldsymbol{p}, \mathcal{L})\right)$ for method development and $\boldsymbol{d}=\operatorname{LLM}\left(\mathcal{T}_{e}(\boldsymbol{p}, \boldsymbol{m}, \mathcal{L})\right)$ for experiment design, which constitutes the full idea: $\boldsymbol{o}=[\boldsymbol{p}, \boldsymbol{m}, \boldsymbol{d</p>
<p>Following this general formulation, the important question to answer is how the body of scientific literature is leveraged for actually generating research ideas with LLMs. Here, we outline three key desiderata that contribute to the success of human researchers ideating novel research ideas: a broad</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and deep understanding of related work, an encyclopedic perspective on the interconnectedness of concepts within and across scientific domains, and a community of peers who help iteratively improve ideas through constructive critiques. We describe our operationalization of these three desiderata using the prior literature and LLMs in what follows.</p>
<p>Citation Graph-based Literature Survey Due to the constraints on their input lengths and their reasoning abilities, particularly over very long contexts (Liu et al., 2023a), it is not possible to incorporate all the existing publications from the literature $\mathcal{L}$ into the LLM input. Instead, we need to find a meaningful subset relevant to the problem at hand. To achieve this, we mirror the process followed by human researchers, who expand their knowledge of a paper by perusing other papers that either cite or are cited by it. Concretely, for the LLM, we initiate its literature review process by providing a core paper $l_{0}$ from $\mathcal{L}$ and then selectively incorporating subsequent papers $\left{l_{1}, \ldots, l_{n}\right}$ that are directly connected based on a citation graph. This procedure makes the LLM input for idea generation more manageable and coherent. In addition, we operationalize the selection process of the core paper and its relevant citations with two design choices: 1) the core paper is selected based on its citation count (e.g., exceeding 100 over 3 months) typically indicating high impact; 2) its relevant papers (which may be potentially numerous) are further narrow-downed based on their similarities of abstracts with the core paper, ensuring a more focused and relevant set of related work.</p>
<p>However, despite the simplicity and intuitiveness of this idea generation approach, there exists one major limitation. This approach relies exclusively on a set of given papers (the core paper and its citations); however, since scientific knowledge is not confined to specific studies but rather accumulates across a wide range of publications (across various fields), we should ideally harness this extensive, interconnected, and relevant scientific knowledge in our method for research idea generation.</p>
<p>Entity-Centric Knowledge Augmentation In order to model an encyclopedic view of interconnected concepts, we must effectively design a framework to extract, store and effectively leverage the vast amount of knowledge in scientific literature $\mathcal{L}$. In this work, we view entities as the atomic units of knowledge, which allows for ease of representation and accumulation over papers in a unified</p>
<p>manner across different disciplines. For example, we can easily extract the term "database" whenever it appears in any paper, using existing off-the-shelf entity linking methods and then aggregate their linked occurrences into a knowledge store. Then, if the term "database" is prevalent within the realm of medical science but less so in hematology (which is a subdomain of medical science), the constructed knowledge store can capture the affinity between those two domains based on overlapping entities. This representational paradigm can then be used to suggest the term "database" when formulating the ideas about hematology. In other words, this approach enables providing novel and interdisciplinary insights by leveraging the interconnectedness of entities across various fields.</p>
<p>Formally, we design the knowledge store as a two-dimensional matrix $\mathcal{K} \in \mathcal{R}^{m \times m}$ where $m$ is the total number of unique entities identified and $\mathcal{K}$ is implemented in a sparse format. This knowledge store is constructed by extracting entities over all the available scientific articles in literature $\mathcal{L}^{3}$, which not only counts the co-occurrences between entity pairs within individual papers but also quantifies the count for each entity. Our approach is versatile, thus, we can use any entity linker; in this paper we use one developed by Wu et al. (2020). This off-the-shelf system proves capable of extracting key scientific entities (which is demonstrated in Table 16) despite its lack of customized training for the scientific domain. Specifically, this linker tags and canonicalizes entities in a paper $l$ from $\mathcal{L}$, formalized as follows: $\mathcal{E}<em l="l">{l}=\operatorname{EL}(l)$ where $\mathcal{E}</em>$.}$ denotes a multiset of entities (allowing for repetitions) appearing in $l^{4}$. Upon extracting entities $\mathcal{E}$, to store them into the knowledge store $\mathcal{K}$, we consider all possible pairs of $\mathcal{E}$ represented as follows: $\left{e_{i}, e_{j}\right}_{(i, j) \in \mathcal{C}(|\mathcal{E}|, 2)}$ where $e \in \mathcal{E</p>
<p>Given this knowledge store $\mathcal{K}$, our next goal is to enhance the previous vanilla research idea generation process implemented based on a group of interconnected papers, denoted as follows: $\boldsymbol{o}=$ $\operatorname{LLM}\left(\mathcal{T}\left(\left{l_{0}, l_{1}, \ldots, l_{n}\right}\right)\right)$. We do this by augmenting the LLM with the relevant entities from $\mathcal{K}$, which expand the context that LLMs consume with additional knowledge. Formally, let us define entities extracted from the group of interconnected papers, as follows: $\mathcal{E}<em 0="0">{\left{l</em>\right)$. Then, the}, \ldots, l_{n}\right}}=\bigcup_{i=0}^{n} \operatorname{EL}\left(l_{i</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>probabilistic form of retrieving the top- $k$ relevant external entities can be represented as follows:</p>
<p>$$
\operatorname{Ret}\left(\left{l_{0}, \ldots, l_{n}\right} ; \mathcal{K}\right)=\underset{I \subset[m] \backslash I]=\bar{k}}{\arg \max } \prod P\left(e_{i} \mid \mathcal{E}<em 0="0">{\left{l</em>\right)
$$}, \ldots, l_{n}\right}</p>
<p>where $[m]={1, \ldots, m}$ and $e_{i} \notin \mathcal{E}<em 0="0">{\left{l</em>$. Also, for simplicity, by applying Bayes' rule and assuming that entities are independent, the retrieval operation (Equation 1) can be approximated as follows:}, \ldots, l_{n}\right}</p>
<p>$$
\underset{I \subset[m] \backslash I]=\bar{k}}{\arg \max } \prod_{e_{j} \in \mathcal{E}<em 0="0">{\left{l</em>\right)
$$}, \ldots, l_{n}\right}}} P\left(e_{j} \mid e_{i}\right) \times P\left(e_{i</p>
<p>where $P\left(e_{j} \mid e_{i}\right)$ and $P\left(e_{i}\right)$ can be derived from values in the two-dimensional matrix $\mathcal{K}$, suitably normalized. We note that the formulation in Equation 2 is only one instance of operationalizing retrieval; this could be replaced with other retrieval strategies - for example, embedding-based retrieval (discussions and results are provided in Appendix B.3). Hereafter, the instantiation of research proposal generation augmented with relevant entitycentric knowledge is formalized as follows: $\boldsymbol{o}=$ $\operatorname{LLM}\left(\mathcal{T}\left(\left{l_{0}, \ldots, l_{n}\right}, \operatorname{Ret}\left(\left{l_{0}, \ldots, l_{n}\right} ; \mathcal{K}\right)\right)\right)^{5}$. We call this knowledge-augmented LLM-powered idea generation approach ResearchAgent, and provide the templates to instantiate it in Tables 6, 7, and 8.</p>
<p>Iterative Research Idea Refinements We note that attempting to write a full research idea in one go may not be an effective strategy. Humans write drafts that are continually improved based on multiple rounds of reviews and feedback. Therefore, we lastly model a community of peers for iterative idea improvement by introducing a set of LLM-powered reviewing agents (called ReviewingAgents), which provide the ResearchAgent with reviews and feedback according to various criteria for improvement.</p>
<p>Specifically, similar to our approach to instantiate ResearchAgent with an LLM (LLM) and template $(\mathcal{T})$, ReviewingAgents are instantiated similarly but with different templates (See Tables 9, 10, and 11). Then, with ReviewingAgents, each of the generated research ideas (problem, method, and experiment design) is separately evaluated according to its own specific five criteria ${ }^{6}$, which are provided in labels of Figure 2 and detailed in Table 12. Based on the reviews and feedback from ReviewingAgents, the ResearchAgent iteratively updates and refines its generation of research ideas.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Despite the proficiency of LLMs in the evaluation of machine-generated texts (Zheng et al., 2023; Fu et al., 2023), their judgments on research ideas may not be aligned with the judgments of real human researchers. However, there are no ground truth reference judgments available, and collecting them to align LLMs is expensive and often infeasible. Ideally, the judgments made by LLMs should be similar to the ones made by humans, and we aim to ensure this by automatically generating human preference-aligned evaluation criteria (used for automatic evaluations) with a few human annotations. Specifically, to obtain these human-aligned evaluation criteria, we first collect 10 pairs of research ideas and their associated scores for every evaluation criterion on a 5-point Likert scale, annotated by human researchers having at least 3 papers. After that, we prompt the LLM with these human-annotated pairs to induce detailed descriptions for evaluation criteria (Lin et al., 2024) (See Tables 13, 14, and 15). These criteria reflect the underlying human preferences ${ }^{7}$ and are used as evaluation criteria by the ReviewingAgents.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Data</h3>
<p>The main source to generate research ideas is the scientific literature $\mathcal{L}$, which we obtain from the Semantic Scholar Academic Graph API ${ }^{8}$. From this, we select papers appearing after May 01, 2023, because LLMs that we use in our experiments are trained on data from the open web available before this point. This follows the procedure of existing literature-based hypothesis generation work (Qi et al., 2023). Then, we select high-impact papers (that have more than 20 citations) as core papers, mirroring human researchers' tendency to leverage influential work, to ensure the high quality of generated ideas. The resulting data is still very large; thus, we further sample a subset of 300 papers as core papers to obtain a reasonably sized benchmark dataset. The average number of reference papers for each core paper is 87 ; the abstract of each paper has 2.17 entities on average. The distribution of disciplines for all papers is provided in Figure 7.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Baselines and Our Model</h3>
<p>As we target the novel task of research idea generation involving the generation of problems, methods, and experimental designs, there are no baselines for direct comparison. Thus, we mainly compare our full ResearchAgent model against its ablated variants, outlined as follows:</p>
<ol>
<li>Naive ResearchAgent - which uses only a core paper to generate research ideas.</li>
<li>ResearchAgent w/o Entity Retrieval - which uses the core paper and its relevant references without considering entities.</li>
<li>ResearchAgent - which is our full model that uses the relevant references and entities along with the core paper, to augment LLMs.</li>
</ol>
<p>In addition to this set of core baselines, we also compare our approach against existing hypothesis generation work from prior literature in Table 3.</p>
<h3>4.3 Evaluation Setup</h3>
<p>Given our formulation of idea generation (Sec 3.1), there are no ground-truth answers to measure the quality of the generated ideas. Yet, exhaustively listing pairs of core papers and reference research ideas is suboptimal, since there may exist a large number of valid research ideas for each core paper, and this process requires much time, effort and expertise on the part of human researchers. Thus, we use a combination of model-based automatic evaluation and manual human evaluation to validate different models on our experimental benchmark.</p>
<p>Model-based Evaluation Following the recent trends in using LLMs to judge the quality of output texts (especially in the setting of reference-free evaluations) (Zheng et al., 2023; Fu et al., 2023; Liu et al., 2023b), we use GPT-4 to judge the quality of research ideas. Note that each of the problem, method, and experiment design is evaluated with five different criteria (See labels of Figure 2 for criteria and see Table 12 for their detailed descriptions). We ask the LLM-based evaluation model to either rate the generated idea on a 5-point Likert scale for each criterion or perform pairwise comparisons between two ideas from different models. We provide the prompts for evaluations in Appendix A.</p>
<p>Human Evaluation Similar to model-based evaluations, we perform human evaluations that involve assigning a score for each criterion and conducting pairwise comparisons between two ideas. As the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Main results on our research idea generation task with human- (top) and model-based (bottom) evaluations, where we report the score of each idea (problem, method, or experiment design) based on its own five criteria and their average score.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results of pairwise comparisons between ideas from two of any different approaches, where we report the win ratio.</p>
<p>generated ideas are knowledge-intensive, we carefully select annotators who are well-versed in the field and provide them with ideas that are highly relevant to their field of expertise<sup>9</sup>. Specifically, we choose ten expert researchers who have authored at least three papers and ask them to judge only the ideas that are generated based on their own papers.</p>
<h3>4.4 Implementation Details</h3>
<p>We mainly use the GPT-4 (OpenAI, 2023) release from Nov 06, 2023, as the basis for all models, which is, notably, reported to be trained with data up to Apr 2023 (meanwhile, the papers used for idea generation appear after May 2023). To extract entities and build the entity-centric knowledge store, we use the off-the-shelf BLINK entity linker (Wu et al., 2020), with papers from May 01, 2023, to Dec 31, 2023 (available from Semantic Scholar API) along with their references,</p>
<table>
<thead>
<tr>
<th>Categories</th>
<th>Metrics</th>
<th>Problem</th>
<th>Method</th>
<th>Experiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human and Human</td>
<td>Scoring</td>
<td>0.83</td>
<td>0.76</td>
<td>0.67</td>
</tr>
<tr>
<td></td>
<td>Pairwise</td>
<td>0.62</td>
<td>0.62</td>
<td>0.41</td>
</tr>
<tr>
<td>Human and Model</td>
<td>Scoring</td>
<td>0.64</td>
<td>0.58</td>
<td>0.49</td>
</tr>
<tr>
<td></td>
<td>Pairwise</td>
<td>0.71</td>
<td>0.62</td>
<td>0.52</td>
</tr>
</tbody>
</table>
<p>which number 50,091 in total. We provide detailed prompts used to elicit responses for research idea generation in Appendix A.3.</p>
<h3>5 Experimental Results and Analyses</h3>
<p><strong>Main Results</strong> Our main results on scoring with human and model-based evaluations are provided in Figure 2. These demonstrate that our full ResearchAgent outperforms all baselines by large margins on every metric across problems, methods, and experiment designs (constituting the complete research ideas). Particularly, the full ResearchAgent augmented with relevant entities exhibits strong gains on metrics related to creativity (such as Originality for problems and Innovativeness for methods) since entities may offer novel concepts and views that may not be observable in the group of citation-based papers alone. In addition, the results of pairwise comparisons between models with both human and model-based evaluations – shown in Figure 3 – demonstrate that the full ResearchAgent shows the highest win ratio over its baselines.</p>
<p><sup>9</sup>We also experiment with human evaluation using non-domain-experts, but this proves to be suboptimal therefore, we focus on experts for reliable judgments of generated ideas.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results with varying the number of refinement steps. Table 2: Results of ablation study on references and entities.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Problem</th>
<th>Method</th>
<th>Experiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>ResearchAgent</td>
<td>4.52</td>
<td>4.28</td>
<td>4.18</td>
</tr>
<tr>
<td>- w/o Entities</td>
<td>4.35</td>
<td>4.13</td>
<td>4.02</td>
</tr>
<tr>
<td>- w/ Random Entities</td>
<td>4.41</td>
<td>4.19</td>
<td>4.13</td>
</tr>
<tr>
<td>- w/o References</td>
<td>4.26</td>
<td>4.08</td>
<td>3.97</td>
</tr>
<tr>
<td>- w/ Random References</td>
<td>4.35</td>
<td>4.16</td>
<td>4.02</td>
</tr>
<tr>
<td>- w/o Entities &amp; References</td>
<td>4.20</td>
<td>4.03</td>
<td>3.92</td>
</tr>
</tbody>
</table>
<p>Analysis on Inter-Annotator Agreements To validate the quality and reliability of human annotations, we measure the inter-annotator agreements, where 20% of the generated ideas are evaluated by two human judges, and report results in Table 1. Specifically, for the scoring, we first rank scores from each annotator and measure Spearman's correlation coefficient Pirie, 2006 between the ranked scores of two annotators. For the pairwise comparison between two judges, we measure Cohen's kappa coefficient Cohen, 1960. Table 1 shows that the inter-annotator agreement is high, confirming the reliability of our assessments about the quality of generated research ideas. Also, while agreement scores for experimental designs are slightly lower than other aspects, this does not necessarily indicate a shortcoming in the quality of experimental designs produced by ResearchAgent, as demonstrated in Figures 2 and 3. Instead, we view this as the inherent subjectivity and variability in how such designs are perceived and evaluated by different annotators (i.e., the nature of the variability itself makes achieving high agreement challenging).</p>
<p>Analysis on Human-Model Agreements Similar to what we did for the aforementioned inter-annotator agreements, we measure agreements between human-based and model-based evaluations, to ensure the reliability of model-based evaluations. As shown in Table 1, we further confirm that agreements between humans and models are high, indicating that model-based evaluations are a reasonable proxy to judge research idea generation.</p>
<p>Analysis of Refinement Steps To see the effectiveness of iterative refinements of research ideas with ReviewingAgents, in Figure 4, we report the averaged scores on the generated ideas as a function of refinement steps. We first observe initial</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Distributions of model-based evaluation results with and without the human-induced score criteria alignment (middle and right), as well as human evaluation results (left).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Results with bucketing papers based on citations.</p>
<p>Improvements in the quality of generated ideas with increased refinement steps. Yet, the performance becomes saturated after three iterations, which may indicate diminishing returns for subsequent iteration steps, which aligns with the pattern observed in agent-based refinement work Du et al., 2023.</p>
<p>Ablation on Knowledge Sources Recall that the full ResearchAgent is augmented with two different knowledge sources, namely relevant references and entities. To see their individual contribution, we perform an ablation study by either excluding one of the knowledge sources or replacing it with random elements. As shown in Table 2, each knowledge source contributes to performance improvement, and the relevant references are especially helpful. We also note that providing random elements is more helpful than providing no elements at all; we hypothesize that this may be due to the LLM's capability to filter out noise while still gaining incidental value from random inputs.</p>
<p>Analysis on Human Alignment for Evaluation Recall that to align judgments from model-based evaluations with actual human preferences, we generated the evaluation criteria based on human evaluation results and used them as the criteria for model-based evaluations. Figure 5 demonstrates the efficacy of this strategy, presenting the score distribution of human evaluation compared with the distributions of model-based evaluations with and without human alignment. We find that the score distribution of model-based evaluations without alignment is skewed and different from the score distribution of human judgments. Meanwhile, after aligning the model-based evaluations with human-induced score criteria, the calibrated distribution more closely resembles the distribution of humans.</p>
<p>Table 3: Comparisons of ResearchAgent with hypothesis generation methods (Wang et al., 2023b; Yang et al., 2023).</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Clarity</th>
<th>Relevance</th>
<th>Originality</th>
<th>Feasibility</th>
<th>Significance</th>
</tr>
</thead>
<tbody>
<tr>
<td>SciMON</td>
<td>4.04</td>
<td>4.37</td>
<td>4.56</td>
<td>3.98</td>
<td>4.15</td>
</tr>
<tr>
<td>Hypothesis Proposer</td>
<td>3.97</td>
<td>4.14</td>
<td>4.07</td>
<td>4.01</td>
<td>4.11</td>
</tr>
<tr>
<td>ResearchAgent</td>
<td>4.11</td>
<td>4.08</td>
<td>4.77</td>
<td>4.05</td>
<td>4.81</td>
</tr>
</tbody>
</table>
<p>Correlation on Citation Counts We further investigate whether a high-impact paper (when used as a core paper) leads to high-quality research ideas. To measure this, we categorize papers by their citation count (as a proxy for impact), and visualize the average score of each bucket (with model-based evaluations) in Figure 6. We find that ideas from high-impact papers tend to be of higher quality, likely due to their ability to identify research gaps, propose feasible methods, and connect with other works. Additionally, based on the paper distribution (See Figure 7) and for the ease of manual quality check, evaluation criteria for model-based evaluations are induced mainly with computer science papers. To see whether those criteria are applicable to diverse fields, we also compare a correlation between scores of computer science papers and all papers in Figure 6. From this, we observe that the scores increase when the citation increases for both domains, which may support the generalizability of human-preference-induced evaluation criteria.</p>
<p>Comparisons to Hypothesis Generation Recall that existing methods for hypothesis generation focus on predicting links between variables or generating hypotheses based on these links, which differs from our experimental setup of generating openended research ideas (problems, methods, and experiments). Nevertheless, to understand how the quality of the generated research ideas from prior works (Wang et al., 2023b; Yang et al., 2023) differs from our ResearchAgent, we perform comparisons. As shown in Table 3, we observe that ResearchAgent is capable of generating superior research hypotheses, due to the utilization of broad and deep knowledge across domains as well as the iterative review and refinement procedures.</p>
<p>Analysis using Different LLMs To assess how ResearchAgent's performance changes with different LLMs, we conduct an auxiliary analysis with Llama3, Mixtral, Qwen1.5, and GPT-3.5 (Bai et al., 2023; Jiang et al., 2024), as shown in Table 4. These results show a significant performance drop with less capable models. Moreover, the performance differences between the Naive ResearchAgent without knowledge augmentation and the full</p>
<p>Table 4: Results with different, open and proprietary LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Problem</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Experiment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4.0</td>
<td style="text-align: center;">Naive ResearchAgent</td>
<td style="text-align: center;">4.20</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">3.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ResearchAgent (Ours)</td>
<td style="text-align: center;">4.52</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">4.18</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Naive ResearchAgent</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ResearchAgent (Ours)</td>
<td style="text-align: center;">3.58</td>
<td style="text-align: center;">3.58</td>
<td style="text-align: center;">3.60</td>
</tr>
<tr>
<td style="text-align: center;">Llama3 (SB)</td>
<td style="text-align: center;">Naive ResearchAgent</td>
<td style="text-align: center;">3.76</td>
<td style="text-align: center;">3.69</td>
<td style="text-align: center;">3.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ResearchAgent (Ours)</td>
<td style="text-align: center;">4.18</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">3.95</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral (8s7B)</td>
<td style="text-align: center;">Naive ResearchAgent</td>
<td style="text-align: center;">3.31</td>
<td style="text-align: center;">3.27</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ResearchAgent (Ours)</td>
<td style="text-align: center;">3.28</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">3.31</td>
</tr>
<tr>
<td style="text-align: center;">Qwen1.5 (32B)</td>
<td style="text-align: center;">Naive ResearchAgent</td>
<td style="text-align: center;">3.64</td>
<td style="text-align: center;">3.74</td>
<td style="text-align: center;">3.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ResearchAgent (Ours)</td>
<td style="text-align: center;">4.02</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">3.94</td>
</tr>
</tbody>
</table>
<p>ResearchAgent become marginal (for Mixtral and GPT-3.5), which indicates that they might struggle with capturing complex concepts between scientific papers. This can likely be attributed to the emergent abilities of LLMs for complex reasoning (but not in smaller LMs) (Wei et al., 2022), although other subtle issues may also be contributing factors.</p>
<p>Qualitative Analysis We provide qualitative results on generated research ideas in Table 16. One representative example in the last row highlights the advantage of entity-centric knowledge augmentation, where two entities (such as Drosophila Genetic Reference Panel and CRISPR) retrieved from the entity-centric knowledge store enable the generation of a novel research idea: bridging genetic variability and CRISPR applications. This exemplifies how external entity-based knowledge uncovers non-trivial relations between scientific concepts.</p>
<h2>6 Conclusion</h2>
<p>In this work, we introduced ResearchAgent, a system designed to assist researchers by generating research ideas, which encompass problem identification, method development, and experiment design. Inspired by the human process of ideation, our approach conducts broad and deep literature reviews, integrates knowledge across domains to foster idea cross-pollination, and employs a community of reviewing agents to iteratively refine the generated ideas. Our evaluations, both human and modelbased, demonstrated that ResearchAgent produces ideas that are more creative, valid, and clear compared to baselines. While this initial foray shows promising results, multiple challenges remain to operationalize ResearchAgent in real-world research settings. Practical considerations include scaling the knowledge store to encompass diverse research domains, and keeping it current with the latest publications, through which the system can become adaptable even to emerging fields.</p>
<h2>Limitations</h2>
<p>ResearchAgent has some limitations that we hope to address in future work.</p>
<p>First, recall that we built the entity-centric knowledge store to propose beneficial entities during idea generation; however this store is constructed by extracting entities from the titles and abstracts of a limited number of publications (due to the costs of processing them) thereby precluding a large number of other entities and their interconnectedness.</p>
<p>In addition, the number of entities that we obtain from the BLINK entity linker (Wu et al., 2020) amounts to 3 per paper on average, indicating limited coverage (it is an open-domain linker after all), although it does exhibit generally strong understanding of scientific contexts, as demonstrated by the improvement achieved by the inclusion of its predictions (See Figures 2 and 3, and Table 16).</p>
<p>Furthermore, since our ResearchAgent is powered by LLMs, similar to any other approaches based on LLMs, it may hallucinate the generated research ideas. While our proposed ResearchAgent can partially mitigate this problem by augmenting LLMs with additional elements, such as references to the target paper and greater entity-centric knowledge, which help ground the generation process in more accurate and relevant information, validating these generated research ideas with experiments is essential to truly accelerate scientific research.</p>
<p>Moreover, while our iterative refinement process with ReviewingAgents demonstrates promising results, it has inherent limitations in scope. Although we employed diverse perspectives by utilizing 15 ReviewingAgents to evaluate three different ideas (problem, method, and experimental design) with five specific criteria for each, this approach may not fully capture the broad range of potential perspectives and criteria necessary for comprehensive evaluation across all different research domains. As acknowledged in the paper, our selection of criteria was informed by their presumed importance, but conducting an exhaustive exploration of all possible criteria over diverse domains is beyond the scope of this work (given the complexity of categorizing and balancing all relevant factors and perspectives). However, we believe the potential of our modular approach allows for customizing and aligning updated or even new criteria to any specific target domain with novel applications, and we leave further expanding them as future work.</p>
<p>Lastly, our ResearchAgent may be less suited for generating ideas in certain domains, such as theoretical sciences, where mathematical reasoning and proof generation play a central role. However, its flexibility allows for customization through specific instructions, enabling the integration of reasoningbased models and techniques to cater to the needs of theoretical research. For instance, in theoretical mathematics, we can instruct (reasoning-based) LLMs to focus on generating proofs or methods and omit experimental design steps that are less relevant. This as an exciting area for future work, where specialized techniques tailored to each domain could be included to broaden its applicability.</p>
<h2>Ethics Statement</h2>
<p>We are aware that the ResearchAgent may have the potential to be misused for nefarious purposes, such as generating research ideas about new explosives, malicious software, and invasive surveillance tools. Notably, this vulnerability is not unique to our approach but a common challenge faced by existing LLMs that possess significant creative and reasoning capabilities, occasionally generating content that may be deemed undesirable. Consequently, it underscores the necessity to enhance the robustness and safety of LLMs more broadly.</p>
<p>Also, we recognize the risks of unintentional plagiarism associated with using ResearchAgent, where the system might generate ideas that closely mirror existing research due to the regurgitation of training data. While mitigation strategies, such as integrating access to a comprehensive knowledge base to inform users of prior work, can be employed, we understand that building and maintaining such a resource is inherently complex and may not fully eliminate the risk. To further reduce the possibility of plagiarism, recording and tracking all generated ideas could help identify similarities and guide the model to avoid repetition, though this approach would necessitate explicit user consent.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00256259), the grant of the Korea Machine Learning Ledger Orchestration for Drug Discovery Project (KMELLODDY) funded by the Ministry of Health \&amp; Welfare and Ministry of Science and ICT, Republic of Korea (grant number: RS-2024-12345678), the Artificial intelligence industrial convergence</p>
<p>cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) \&amp; Gwangju Metropolitan City, and the Institute for Information \&amp; communications Technology Planning \&amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST)).</p>
<h2>References</h2>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361.</p>
<p>Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. In Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 78-106, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, and Sujay Kumar Jauhar. 2024. Knowledge-augmented large language models for personalized contextual query suggestion. WWW.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.</p>
<p>Andrés M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2023. Chemcrow: Augmenting large-language models with chemistry tools.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37 - 46.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.</p>
<p>Michael Fire and Carlos Guestrin. 2019. Overoptimization of academic publishing metrics: Observing goodhart's law in action. GigaScience, 8.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli TranJohnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023. The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459.</p>
<p>Sam Henry and Bridget T. McInnes. 2017. Literature based discovery: Models, methods, and trends. Journal of biomedical informatics, 74:20-32.</p>
<p>Tom Hope, Doug Downey, Daniel S. Weld, Oren Etzioni, and Eric Horvitz. 2023. A computational inflection for scientific discovery. Commun. ACM, 66(8):62-73.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2023. Benchmarking large language models as AI research agents. arXiv preprint arXiv:2310.03302.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of experts. arXiv preprint arXiv: 2401.04088.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, and Lidong Bing. 2024. Chain of ideas: Revolutionizing research via novel idea development with llm agents.</p>
<p>Ying-Chun Lin, Jennifer Neville, Jack W Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh Tiwary, Brent Hecht, and Jaime Teevan. 2024. Interpretable user satisfaction estimation for conversational systems with large language models. arXiv preprint arXiv:2403.12388.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 2511-2522. Association for Computational Linguistics.</p>
<p>Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, and Yun Huang. 2024. How AI processing delays foster creativity: Exploring research question co-creation with an llm-based agent. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, pages 17:117:25. ACM.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.
R.K. Nadkarni, David Wadden, Iz Beltagy, Noah A. Smith, Hannaneh Hajishirzi, and Tom Hope. 2021.</p>
<p>Scientific language models for biomedical knowledge base completion: An empirical study. ArXiv, abs/2106.09700.</p>
<p>OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.
W. Pirie. 2006. Spearman Rank Correlation Coefficient, volume 8 .</p>
<p>Jason Portenoy, Marissa Radensky, Jevin West, Eric Horvitz, Daniel S. Weld, and Tom Hope. 2021. Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery. arXiv preprint arXiv:2108.05669.</p>
<p>Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. 2023. Large language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965.</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316-1331.</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi, Josh Grochow, Andrea Lodi, Jean-Baptiste Mouret, Talia Ringer, and Tao Yu. 2023. Mathematical discoveries from program search with large language models. Nature, 625:468 - 475.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. Replug: Retrievalaugmented black-box language models. arXiv preprint arXiv:2301.12652.</p>
<p>Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ram Pasunuru, Mrinmaya Sachan, Jason Weston, and Asli Celikyilmaz. 2023. The ART of LLM refinement: Ask, refine, and trust. arXiv preprint arXiv:2311.07961.</p>
<p>Don R. Swanson. 1986. Undiscovered public knowledge. The Library Quarterly, 56:103-118.</p>
<p>Justin Sybrandt, Ilya Tyagin, M. Shtutman, and Ilya Safro. 2020. Agatha: Automatic graph mining and transformer based hypothesis generation approach. Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan</p>
<p>Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alex Dunn, Ziqin Rong, Olga Vitalievna Kononova, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571:95 - 98.</p>
<p>Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo Ferran, George Lee, Bin Li, Anant Madabhushi, Parantu Shah, Michaela Spitzer, and Shanrong Zhao. 2019. Applications of machine learning in drug discovery and development. Nature reviews. Drug discovery, 18(6):463-477.</p>
<p>Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, and Saif M. Mohammad. 2023. We are who we cite: Bridges of influence between natural language processing and other academic fields. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 12896-12913. Association for Computational Linguistics.</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora S. Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. 2023a. Scientific discovery in the age of artificial intelligence. Nat., 620(7972):47-60.</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2023b. Learning to generate novel scientific directions with contextualized literature-based discovery. arXiv preprint arXiv:2305.14259.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin</p>
<p>Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable zeroshot entity linking with dense entity retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6397-6407, Online. Association for Computational Linguistics.</p>
<p>Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, J. Ren, Anhuan Xie, and Wei Song. 2023. Retrieve-rewriteanswer: A kg-to-text enhanced llms framework for knowledge graph question answering. arXiv preprint arXiv:2309.11206.</p>
<p>Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. 2023. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726.
J. Young. 2003. A Technique for Producing Ideas. McGraw Hill LLC.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Visualization of the distribution of disciplines for all core papers, selected for research idea generation.</p>
<h2>A Additional Experimental Details</h2>
<p>In this section, we provide additional details on experiments, including datasets, human evaluation setups, prompts (used for research idea generation and validation), and human-induced criteria.</p>
<h3>A.1 Data Statistics</h3>
<p>We visualize a distribution of core paper categories used for idea generation in Figure 7, where the categories are obtained from Semantic Scholar API<sup>10</sup>. From this, we find that the top 3 categories are computer science, medicine, and engineering.</p>
<h3>A.2 Details on Human Evaluation</h3>
<p>To conduct evaluations with human judges, we recruited 10 researchers from the United States and South Korea, majoring in computer science, medicine, and biology, each with a minimum of 3 published papers. For annotation, they were provided with a 6-page guideline document, which includes the task instruction and annotation examples. After reading this document, the annotators access the Label Studio platform, on which they first read the title and abstract of the target paper, and then review and evaluate the generated research ideas from different models. During the evaluation process, they are allowed to use any external tools, such as web searches. We note that they were compensated at a rate of $22.20 per hour. Also, on average, for one hour, they evaluated 3 sets of research ideas (that are generated from their own papers), with each set comprising three sub-ideas (the problem, method, and experiment design) from three different approaches (i.e., a total of 9 ideas for one hour). We perform three rounds of human evaluations with refinements in between, and, due</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Results on our research idea generation task with model-based evaluation, where we exclude refinement steps.</p>
<p>to the cost associated with human annotations, we are able to fully evaluate a total of 150 ideas.</p>
<h3>A.3 Prompts for Ideas Generation</h3>
<p>We provide the prompts used to elicit the idea generations from our full ResearchAgent, specifically for instantiating problem identification, method development, and experiment design in Table 6, Table 7, and Table 8, respectively.</p>
<h3>A.4 Prompts for Idea Validation</h3>
<p>We provide the prompts used to elicit the idea validation from our ReviewingAgents as well as the model-based evaluations, specifically for instantiating problem validation, method validation, and experiment design validation in Table 9, Table 10, and Table 11, respectively. In addition, we provide the criteria used, which are induced by human judgments in the next subsection (Appendix A.5).</p>
<h3>A.5 Criteria Induced by Human Judgments</h3>
<p>Recall that, to align model-based evaluations with human preferences, we induce the criteria (used for automatic evaluations) with actual human judgments. We note that this is done by prompting GPT-4 with 10 pairs of generated ideas and (randomly selected) human judgments. We provide the resulting criteria for validations of problems, methods, and experiment designs in Table 13, Table 14, and Table 15, respectively.</p>
<h2>B Additional Experimental Results</h2>
<p>We provide additional experimental results, including comparisons without refinements and examples of the generated research ideas.</p>
<h3>B.1 Results without Refinement Steps</h3>
<p>To see whether the proposed ResearchAgent is consistently effective even without ReviewingAgents, we show the model-based evaluation results without any refinement steps in Figure 8. From this, we clearly observe that the full ResearchAgent outperforms its variants, demonstrating its effectiveness.</p>
<p><sup>10</sup>https://www.semanticscholar.org/product/api</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Breakdown results of the research ideas generated from our full ResearchAgent across different domains.</p>
<p>Table 5: Results with different entity retrieval strategies.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Problem</th>
<th>Method</th>
<th>Experiment</th>
</tr>
</thead>
<tbody>
<tr>
<td>ResearchAgent</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- w/ Co-occurrence-based Retrieval</td>
<td>4.52</td>
<td>4.28</td>
<td>4.18</td>
</tr>
<tr>
<td>- w/ Embedding-based Retrieval</td>
<td>4.49</td>
<td>4.34</td>
<td>4.16</td>
</tr>
<tr>
<td>- w/o Entity Retrieval</td>
<td>4.35</td>
<td>4.13</td>
<td>4.02</td>
</tr>
</tbody>
</table>
<h3>B.2 Results on Generated Ideas by Domain</h3>
<p>To see the quality of the generated research ideas across different domains, we breakdown the performance of ResearchAgent according to the categories of core papers in Figure 7, and present the results in Figure 9. From this, we observe that the generated research ideas on the high-resource domains (such as Computer Science, Medicine, and Engineering where there is a greater volume of existing literature as shown in Figure 7) are superior to those generated from the low-resource domain papers (such as Physics, Chemistry, and Mathematics). This disparity might be attributed to the fact that the underlying LLMs used to generate research ideas are likely trained on data predominantly sourced from high-resource domains, which leads to enhancing their ability to comprehend scientific concepts and produce relevant research ideas.</p>
<h3>B.3 Analysis with Different Entity Retrieval</h3>
<p>To see the effectiveness of different entity retrieval strategies, we perform additional experiments, replacing the co-occurrence-based entity retrieval in Equation 2 to the contextual embedding-based retrieval. Notably, this contextual embedding-based retrieval approach uses the entities that have the highest similarity to the entities appearing in the current literature (i.e., core paper and its references) used for idea generation, where the similarities are calculated based on embedding-level similarities between entities over the latent space represented by the entity linker (Wu et al., 2020). Therefore, unlike the previous co-occurrence-based entity retrieval that may retrieve entities that have opposite concepts to the main idea of the current core paper (since we often mention limitations of previous work along with the proposed ideas), this embedding-based approach may provide the ResearchAgent with mostly the entities having similar concepts to the core paper. Nevertheless, as shown in Table 5, the results with the strategy of entity co-occurrence-based retrieval are comparable to the results with the new embedding-based contextual retrieval. These results might confirm that there is not much difference in the quality of entity retrieval among those two strategies, i.e., most entities retrieved from the co-occurrence-based retrieval are contextually relevant for generating research ideas.</p>
<p>Table 6: The prompt used in the full instantiation of ResearchAgent for problem identification.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Types</th>
<th style="text-align: center;">Texts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System Message</td>
<td style="text-align: center;">You are an AI assistant whose primary goal is to identify promising, new, and key scientific <br> problems based on existing scientific literature, in order to aid researchers in discovering novel <br> and significant research opportunities that can advance the field.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">You are going to generate a research problem that should be original, clear, feasible, relevant, and <br> significant to its field. This will be based on the title and abstract of the target paper, those of <br> {len(references)} related papers in the existing literature, and {len(entities)} entities potentially <br> connected to the research area.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Understanding of the target paper, related papers, and entities is essential: <br> - The target paper is the primary research study you aim to enhance or build upon through future <br> research, serving as the central source and focus for identifying and developing the specific <br> research problem. <br> - The related papers are studies that have cited the target paper, indicating their direct relevance <br> and connection to the primary research topic you are focusing on, and providing additional context <br> and insights that are essential for understanding and expanding upon the target paper. <br> - The entities can include topics, keywords, individuals, events, or any subjects with possible <br> direct or indirect connections to the target paper or the related studies, serving as auxiliary sources <br> of inspiration or information that may be instrumental in formulating the research problem.</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">Your approach should be systematic: <br> - Start by thoroughly reading the title and abstract of the target paper to understand its core focus. <br> - Next, proceed to read the titles and abstracts of the related papers to gain a broader perspective <br> and insights relevant to the primary research topic. <br> - Finally, explore the entities to further broaden your perspective, drawing upon a diverse pool of <br> inspiration and information, while keeping in mind that not all may be relevant.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I am going to provide the target paper, related papers, and entities, as follows: <br> Target paper title: {paper['title']} <br> Target paper abstract: {paper['abstract']} <br> Related paper titles: {relatedPaper['titles']} <br> Related paper abstracts: {relatedPaper['abstracts']} <br> Entities: {Entities}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With the provided target paper, related papers, and entities, your objective now is to formulate a <br> research problem that not only builds upon these existing studies but also strives to be original, <br> clear, feasible, relevant, and significant. Before crafting the research problem, revisit the title <br> and abstract of the target paper, to ensure it remains the focal point of your research problem <br> identification process.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Target paper title: {paper['title']} <br> Target paper abstract: {paper['abstract']}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Then, following your review of the above content, please proceed to generate one research <br> problem with the rationale, in the format of <br> Problem: <br> Rationale:</td>
</tr>
</tbody>
</table>
<p>Table 7: The prompt used in the full instantiation of ResearchAgent for method development.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Types</th>
<th style="text-align: center;">Texts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System Message</td>
<td style="text-align: center;">You are an AI assistant whose primary goal is to propose innovative, rigorous, and valid methodologies to solve newly identified scientific problems derived from existing scientific literature, in order to empower researchers to pioneer groundbreaking solutions that catalyze breakthroughs in their fields.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">You are going to propose a scientific method to address a specific research problem. Your method should be clear, innovative, rigorous, valid, and generalizable. This will be based on a deep understanding of the research problem, its rationale, existing studies, and various entities.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Understanding of the research problem, existing studies, and entities is essential: <br> - The research problem has been formulated based on an in-depth review of existing studies and a potential exploration of relevant entities, which should be the cornerstone of your method development. <br> - The existing studies refer to the target paper that has been pivotal in identifying the problem, as well as the related papers that have been additionally referenced in the problem discovery phase, all serving as foundational material for developing the method. <br> - The entities can include topics, keywords, individuals, events, or any subjects with possible direct or indirect connections to the existing studies, serving as auxiliary sources of inspiration or information that may be instrumental in method development.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Your approach should be systematic: <br> - Start by thoroughly reading the research problem and its rationale, to understand your primary focus. <br> - Next, proceed to review the titles and abstracts of existing studies, to gain a broader perspective and insights relevant to the primary research topic. <br> - Finally, explore the entities to further broaden your perspective, drawing upon a diverse pool of inspiration and information, while keeping in mind that not all may be relevant.</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">I am going to provide the research problem, existing studies (target paper \&amp; related papers), and entities, as follows: <br> Research problem: {researchProblem} <br> Rationale: {researchProblemRationale} <br> Target paper title: {paper['title']} <br> Target paper abstract: {paper['abstract']} <br> Related paper titles: {relatedPaper['titles']} <br> Related paper abstracts: {relatedPaper['abstracts']} <br> Entities: {Entities}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With the provided research problem, existing studies, and entities, your objective now is to formulate a method that not only leverages these resources but also strives to be clear, innovative, rigorous, valid, and generalizable. Before crafting the method, revisit the research problem, to ensure it remains the focal point of your method development process.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Research problem: {researchProblem} <br> Rationale: {researchProblemRationale}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Then, following your review of the above content, please proceed to propose your method with its rationale, in the format of <br> Method: <br> Rationale:</td>
</tr>
</tbody>
</table>
<p>Table 8: The prompt used in the full instantiation of ResearchAgent for experiment design.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Types</th>
<th style="text-align: center;">Texts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System Message</td>
<td style="text-align: center;">You are an AI assistant whose primary goal is to design robust, feasible, and impactful experiments based on identified scientific problems and proposed methodologies from existing scientific literature, in order to enable researchers to systematically test hypotheses and validate groundbreaking discoveries that can transform their respective fields.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">You are going to design an experiment, aimed at validating a proposed method to address a specific research problem. Your experiment design should be clear, robust, reproducible, valid, and feasible. This will be based on a deep understanding of the research problem, scientific method, existing studies, and various entities. <br> Understanding of the research problem, scientific method, existing studies, and entities is essential: <br> - The research problem has been formulated based on an in-depth review of existing studies and a potential exploration of relevant entities. <br> - The scientific method has been proposed to tackle the research problem, which has been informed by insights gained from existing studies and relevant entities. <br> - The existing studies refer to the target paper that has been pivotal in identifying the problem and method, as well as the related papers that have been additionally referenced in the discovery phase of the problem and method, all serving as foundational material for designing the experiment. <br> - The entities can include topics, keywords, individuals, events, or any subjects with possible direct or indirect connections to the existing studies, serving as auxiliary sources of inspiration or information that may be instrumental in your experiment design.</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">Your approach should be systematic: <br> - Start by thoroughly reading the research problem and its rationale followed by the proposed method and its rationale, to pinpoint your primary focus. <br> - Next, proceed to review the titles and abstracts of existing studies, to gain a broader perspective and insights relevant to the primary research topic. <br> - Finally, explore the entities to further broaden your perspective, drawing upon a diverse pool of inspiration and information, while keeping in mind that not all may be relevant.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I am going to provide the research problem, scientific method, existing studies (target paper \&amp; related papers), and entities, as follows: <br> Research problem: {researchProblem} <br> Rationale: {researchProblemRationale} <br> Scientific method: {scientificMethod} <br> Rationale: {scientificMethodRationale} <br> Target paper title: ${$ paper['title']} <br> Target paper abstract: ${$ paper['abstract']} <br> Related paper titles: {relatedPaper['titles']} <br> Related paper abstracts: {relatedPaper['abstracts']} <br> Entities: {Entities}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With the provided research problem, scientific method, existing studies, and entities, your objective now is to design an experiment that not only leverages these resources but also strives to be clear, robust, reproducible, valid, and feasible. Before crafting the experiment design, revisit the research problem and proposed method, to ensure they remain at the center of your experiment design process.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Research problem: {researchProblem} <br> Rationale: {researchProblemRationale} <br> Scientific method: {scientificMethod} <br> Rationale: {scientificMethodRationale}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Then, following your review of the above content, please proceed to outline your experiment with its rationale, in the format of <br> Experiment: <br> Rationale:</td>
</tr>
</tbody>
</table>
<p>Table 9: The prompt used in the full instantiation of ReviewingAgent for problem validation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Types</th>
<th style="text-align: center;">Texts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System Message</td>
<td style="text-align: center;">You are an AI assistant whose primary goal is to assess the quality and validity of scientific <br> problems across diverse dimensions, in order to aid researchers in refining their problems based <br> on your evaluations and feedback, thereby enhancing the impact and reach of their work.</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">You are going to evaluate a research problem for its {metric}, focusing on how well it is defined <br> in a clear, precise, and understandable manner. <br> As part of your evaluation, you can refer to the existing studies that may be related to the problem, <br> which will help in understanding the context of the problem for a more comprehensive assessment. <br> - The existing studies refer to the target paper that has been pivotal in identifying the problem, as <br> well as the related papers that have been additionally referenced in the discovery phase of the <br> problem. <br> The existing studies (target paper \&amp; related papers) are as follows: <br> Target paper title: {paper['title']} <br> Target paper abstract: {paper['abstract']} <br> Related paper titles: {relatedPaper['titles']} <br> Related paper abstracts: {relatedPaper['abstracts']}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Now, proceed with your {metric} evaluation approach that should be systematic: <br> - Start by thoroughly reading the research problem and its rationale, keeping in mind the context <br> provided by the existing studies mentioned above. <br> - Next, generate a review and feedback that should be constructive, helpful, and concise, focusing <br> on the ${$ metric $}$ of the problem. <br> - Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a <br> discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless <br> fully justified: <br> {criteria}</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">I am going to provide the research problem with its rationale, as follows: <br> Research problem: {researchProblem <br> Rationale: {researchProblemRationale} <br> After your evaluation of the above content, please provide your review, feedback, and rating, in <br> the format of <br> Review: <br> Feedback: <br> Rating (1-5):</td>
</tr>
</tbody>
</table>
<p>Table 10: The prompt used in the full instantiation of ReviewingAgent for method validation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Types</th>
<th style="text-align: center;">Texts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System Message</td>
<td style="text-align: center;">You are an AI assistant whose primary goal is to assess the quality and soundness of scientific methods across diverse dimensions, in order to aid researchers in refining their methods based on your evaluations and feedback, thereby enhancing the impact and reach of their work.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">You are going to evaluate a scientific method for its {metric} in addressing a research problem, focusing on how well it is described in a clear, precise, and understandable manner that allows for replication and comprehension of the approach.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">As part of your evaluation, you can refer to the research problem, and existing studies, which will help in understanding the context of the proposed method for a more comprehensive assessment. <br> - The research problem has been used as the cornerstone of the method development, formulated based on an in-depth review of existing studies and a potential exploration of relevant entities. <br> - The existing studies refer to the target paper that has been pivotal in identifying the problem and method, as well as the related papers that have been additionally referenced in the discovery phase of the problem and method.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The research problem and existing studies (target paper \&amp; related papers) are as follows: <br> Research problem: {researchProblem} <br> Rationale: {researchProblemRationale} <br> Target paper title: ${$ paper['title']} <br> Target paper abstract: ${$ paper['abstract']} <br> Related paper titles: ${$ relatedPaper['titles']} <br> Related paper abstracts: ${$ relatedPaper['abstracts']}</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">Now, proceed with your {metric} evaluation approach that should be systematic: <br> - Start by thoroughly reading the proposed method and its rationale, keeping in mind the context provided by the research problem, and existing studies mentioned above. <br> - Next, generate a review and feedback that should be constructive, helpful, and concise, focusing on the ${$ metric $}$ of the method. <br> - Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless fully justified: <br> {criteria}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I am going to provide the proposed method with its rationale, as follows: <br> Scientific method: {scientificMethod} <br> Rationale: {scientificMethodRationale}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">After your evaluation of the above content, please provide your review, feedback, and rating, in the format of <br> Review: <br> Feedback: <br> Rating (1-5):</td>
</tr>
</tbody>
</table>
<p>Table 11: The prompt used in the full instantiation of ReviewingAgent for experiment design validation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Types</th>
<th style="text-align: center;">Texts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">System Message</td>
<td style="text-align: center;">You are an AI assistant whose primary goal is to meticulously evaluate the experimental designs of scientific papers across diverse dimensions, in order to aid researchers in refining their experimental approaches based on your evaluations and feedback, thereby amplifying the quality and impact of their scientific contributions.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">You are going to evaluate an experiment design for its {metric} in validating a scientific method to address a research problem, focusing on how well it is described in a clear, precise, and understandable manner, enabling others to grasp the setup, procedure, and expected outcomes.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">As part of your evaluation, you can refer to the research problem, scientific method, and existing studies, which will help in understanding the context of the designed experiment for a more comprehensive assessment. <br> - The research problem has been formulated based on an in-depth review of existing studies and a potential exploration of relevant entities. <br> - The scientific method has been proposed to tackle the research problem, which has been informed by insights gained from existing studies and relevant entities. <br> - The existing studies refer to the target paper that has been pivotal in identifying the problem, method, and experiment, as well as the related papers that have been additionally referenced in their discovery phases.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The research problem, scientific method, and existing studies (target paper \&amp; related papers) are as follows: <br> Research problem: {researchProblem} <br> Rationale: {researchProblemRationale} <br> Scientific method: {scientificMethod} <br> Rationale: {scientificMethodRationale} <br> Target paper title: ${$ paper['title']} <br> Target paper abstract: ${$ paper['abstract']} <br> Related paper titles: {relatedPaper['titles']} <br> Related paper abstracts: {relatedPaper['abstracts']}</td>
</tr>
<tr>
<td style="text-align: center;">User Message</td>
<td style="text-align: center;">Now, proceed with your {metric} evaluation approach that should be systematic: <br> - Start by thoroughly reading the experiment design and its rationale, keeping in mind the context provided by the research problem, scientific method, and existing studies mentioned above. <br> - Next, generate a review and feedback that should be constructive, helpful, and concise, focusing on the ${$ metric $}$ of the experiment. <br> - Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless fully justified: ${$ criteria $}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I am going to provide the designed experiment with its rationale, as follows: <br> Experiment design: {experimentDesign} <br> Rationale: {experimentDesignRationale}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">After your evaluation of the above content, please provide your review, feedback, and rating, in the format of <br> Review: <br> Feedback: <br> Rating (1-5):</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ We additionally ask five human annotators, who evaluate research ideas, to judge the quality of the induced criteria; two of them agree strongly, while the other three agree moderately.
${ }^{8}$ https://www.semanticscholar.org/product/api&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ There may be additional knowledge sources (beyond the existing literature and entities) for research idea generation, and we leave exploring them as future work.
${ }^{6}$ We select the top five criteria which we consider as the most important, and leave exploring others as future work.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-258823336</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11598v1.pdf" target="_blank">Introspective Tips: Large Language Model for In-Context Decision Making</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips"to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Introspective Tips (tips memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Introspective Tips: Condensed trajectory summaries used as memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based memory paradigm where an LLM (GPT-4 in this paper) generates concise, generalizable 'tips' from past trajectories (self-play, expert demos, or multi-game trials) and uses those tips as prompt-augmentation to improve decision-making in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 LLM agent with Introspective Tips</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 is used directly as a policy model: the system prompts GPT-4 with environment descriptions, action lists, and introspective tips; GPT-4 generates text actions as the agent's policy without parameter fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tips summary memory (condensed trajectory summaries served as prompt memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory consists of short, high-level 'tips' produced by prompting the same or another LLM to summarize important, generalizable lessons from failed or successful trajectories; these tips are injected into subsequent prompts to guide action selection (reducing reliance on full-length trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TextWorld Cooking (text-based game decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Partially-observable sequential decision task in the TextWorld cooking domain: agents must gather, prepare (slice/dice/chop), and cook ingredients using the correct appliances and actions to satisfy cookbook recipes; tasks are grouped into difficulty levels 0–4 with sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Few-shot: with tips-memory the agent reaches cumulative success rate >0.9 and normalized points >0.95 after multiple trials in the few-shot setting. Zero-shot: general tips aggregated from 10 training games (48 trajectories) enable performance comparable to SOTA for difficulty levels 0–3 and outperform SOTA on levels 3–4 (SOTA baselines include TDQN, GATA, ITL trained with ~100k episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Concise tips used as memory substantially outperform naively replaying raw past trajectories (pure replay) under the same LLM prompting framework; expert-derived tips accelerate learning further (reach zero error within ≤6 trajectories). Aggregated general tips enable strong zero-shot transfer, matching or exceeding deep-RL SOTA that used far more interaction data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The LLM sometimes fails to produce suitably general tips for easier-level edge cases, causing underperformance on some low-difficulty games; LLM non-determinism can cause the agent to ignore tips and make erroneous actions; tips generation quality depends on the correctness and coverage of source trajectories and on prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Condensing long trajectories into short, generalizable tips mitigates input-length limits and sparse-reward credit-assignment issues; tips are more transferable across agents/environments than detailed reflections; leveraging expert demonstrations to produce tips accelerates learning; aggregating tips across games acts like a form of meta-learning/Meta-RL enabling zero-shot generalization with far fewer trajectories than standard RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pure replay (trajectory memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Replay Memory (raw past trajectories used as prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline memory strategy where the LLM is prompted with raw past trajectories (up to a limited number due to input length limits) to try to learn from prior interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 LLM agent with trajectory replay</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same GPT-4 agent, but memory is implemented by inserting full previous trajectory logs into the prompt (replay) rather than summarized tips.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>raw trajectory replay (episodic prompt memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>The agent is given up to a small number (experimentally limited to 3) of prior full trajectories as prompt context so the model can autoregressively condition on past action/state sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TextWorld Cooking (text-based game decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See Introspective Tips entry; replay provides prior full trajectories (failed or successful) directly as context for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Pure replay was less effective than tips-summary memory; input-length constraints (prompt size) limited the number of trajectories the agent could ingest (max 3), reducing the agent's ability to aggregate lessons across many trials compared to tips summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Severely limited by prompt input length (can only include few trajectories), making it difficult to learn from many past trials; raw trajectories are longer and harder for the LLM to distill actionable, generalizable guidance compared to concise tips.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Raw trajectory replay as prompt memory is feasible but inefficient; concise summaries (tips) are a more compact and generalizable memory representation for LLM agents in long-horizon, sparse-reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4819.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4819.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert-derived tips</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tips generated from expert demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where the LLM ingests expert demonstration trajectories (walkthroughs) and produces tips from them which are then used as prompts for the agent, accelerating learning compared to self-generated tips from failed trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 LLM agent with expert-derived tips</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 is prompted to compare its behavior with expert walkthroughs and generate concise tips summarizing successful strategies from the expert trajectories; those tips are used to guide subsequent plays.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>expert demonstration-derived tips (demonstration memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Expert walkthroughs (provided in the dataset) are summarized by the LLM into tips that capture successful action sequences and domain knowledge; tips are then added to prompts for future episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TextWorld Cooking (text-based game decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent observes expert demonstrations/walkthroughs for cooking tasks and extracts tips to improve its policy in few-shot trials.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>With expert demonstrations used to generate tips, the LLM agent learns faster and reportedly achieves zero error across all difficulty levels after at most 6 trajectories in the few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Expert-derived tips produce faster convergence than self-generated tips or pure replay; expert demos allow the agent to avoid lengthy trial-and-error and reach perfect performance within a small number of trials.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dependence on the availability and coverage of expert walkthroughs; tips derived from experts must be correctly parsed and applicable (some initial autogenerated tips could be invalid and require correction); stochastic outputs of the LLM still cause occasional tip-ignoring.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Using demonstrations to create concise tips is an efficient way to transfer expert knowledge into LLM agents without fine-tuning, enabling rapid few-shot mastery of complex sequential tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4819.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4819.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregated general tips (multi-game memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregated General Tips from Multiple Games (meta-memory across environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-memory approach where final successful tips from many games are collected and fed into another LLM to generate higher-level general tips that transfer zero-shot to unseen games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 LLM agent with aggregated multi-game tips</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 agents first produce game-specific final tips from multiple training games; another LLM processes these into general tips that are then used as prompts for agents in unseen test games to enable zero-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>aggregated summary memory / meta-memory (cross-game tip aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Final tips (those associated with successful trials) from several games are collected (in experiments, 10 games → 48 trajectories) and summarized by a second LLM into a set of generalized tips; those general tips are used as static prompts when evaluating on unseen games.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot transfer on TextWorld Cooking (unseen games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether general tips synthesized from multiple training games enable an LLM agent to perform well in unseen TextWorld cooking games without in-task trials.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (Cooking domain)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using general tips generated from 10 training games (48 trajectories), the LLM achieved performance comparable to SOTA deep-RL methods for difficulty levels 0–3 and outperformed them on levels 3–4 in zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Aggregated general tips enable strong zero-shot performance with orders-of-magnitude fewer trajectories than deep-RL SOTA training (10 games / 48 trajectories vs. 100 games / 100k episodes), suggesting that distilled cross-game tips can serve as an effective memory modality for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fails in some low-difficulty cases when tips are overly general or do not capture specific edge-case conditions (e.g., when an ingredient is already in the required state); LLM stochasticity can still cause deviation from tips.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Meta-aggregation of final tips across related environments acts like a lightweight Meta-RL memory, enabling sample-efficient zero-shot generalization; concise, corrected tips aggregated across environments are more effective for transfer than raw trajectories or per-episode reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introspective Tips: Large Language Model for In-Context Decision Making', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>In-context reinforcement learning with algorithm distillation <em>(Rating: 1)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 1)</em></li>
                <li>Learning to follow instructions in text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4819",
    "paper_id": "paper-258823336",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Introspective Tips (tips memory)",
            "name_full": "Introspective Tips: Condensed trajectory summaries used as memory",
            "brief_description": "A prompt-based memory paradigm where an LLM (GPT-4 in this paper) generates concise, generalizable 'tips' from past trajectories (self-play, expert demos, or multi-game trials) and uses those tips as prompt-augmentation to improve decision-making in text-based games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 LLM agent with Introspective Tips",
            "agent_description": "GPT-4 is used directly as a policy model: the system prompts GPT-4 with environment descriptions, action lists, and introspective tips; GPT-4 generates text actions as the agent's policy without parameter fine-tuning.",
            "memory_type": "tips summary memory (condensed trajectory summaries served as prompt memory)",
            "memory_description": "Memory consists of short, high-level 'tips' produced by prompting the same or another LLM to summarize important, generalizable lessons from failed or successful trajectories; these tips are injected into subsequent prompts to guide action selection (reducing reliance on full-length trajectories).",
            "task_name": "TextWorld Cooking (text-based game decision-making)",
            "task_description": "Partially-observable sequential decision task in the TextWorld cooking domain: agents must gather, prepare (slice/dice/chop), and cook ingredients using the correct appliances and actions to satisfy cookbook recipes; tasks are grouped into difficulty levels 0–4 with sparse rewards.",
            "benchmark_name": "TextWorld (Cooking domain)",
            "performance_with_memory": "Few-shot: with tips-memory the agent reaches cumulative success rate &gt;0.9 and normalized points &gt;0.95 after multiple trials in the few-shot setting. Zero-shot: general tips aggregated from 10 training games (48 trajectories) enable performance comparable to SOTA for difficulty levels 0–3 and outperform SOTA on levels 3–4 (SOTA baselines include TDQN, GATA, ITL trained with ~100k episodes).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Concise tips used as memory substantially outperform naively replaying raw past trajectories (pure replay) under the same LLM prompting framework; expert-derived tips accelerate learning further (reach zero error within ≤6 trajectories). Aggregated general tips enable strong zero-shot transfer, matching or exceeding deep-RL SOTA that used far more interaction data.",
            "limitations_or_challenges": "The LLM sometimes fails to produce suitably general tips for easier-level edge cases, causing underperformance on some low-difficulty games; LLM non-determinism can cause the agent to ignore tips and make erroneous actions; tips generation quality depends on the correctness and coverage of source trajectories and on prompt engineering.",
            "key_insights": "Condensing long trajectories into short, generalizable tips mitigates input-length limits and sparse-reward credit-assignment issues; tips are more transferable across agents/environments than detailed reflections; leveraging expert demonstrations to produce tips accelerates learning; aggregating tips across games acts like a form of meta-learning/Meta-RL enabling zero-shot generalization with far fewer trajectories than standard RL training.",
            "uuid": "e4819.0",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Pure replay (trajectory memory)",
            "name_full": "Trajectory Replay Memory (raw past trajectories used as prompts)",
            "brief_description": "A baseline memory strategy where the LLM is prompted with raw past trajectories (up to a limited number due to input length limits) to try to learn from prior interactions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 LLM agent with trajectory replay",
            "agent_description": "Same GPT-4 agent, but memory is implemented by inserting full previous trajectory logs into the prompt (replay) rather than summarized tips.",
            "memory_type": "raw trajectory replay (episodic prompt memory)",
            "memory_description": "The agent is given up to a small number (experimentally limited to 3) of prior full trajectories as prompt context so the model can autoregressively condition on past action/state sequences.",
            "task_name": "TextWorld Cooking (text-based game decision-making)",
            "task_description": "See Introspective Tips entry; replay provides prior full trajectories (failed or successful) directly as context for decision-making.",
            "benchmark_name": "TextWorld (Cooking domain)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Pure replay was less effective than tips-summary memory; input-length constraints (prompt size) limited the number of trajectories the agent could ingest (max 3), reducing the agent's ability to aggregate lessons across many trials compared to tips summaries.",
            "limitations_or_challenges": "Severely limited by prompt input length (can only include few trajectories), making it difficult to learn from many past trials; raw trajectories are longer and harder for the LLM to distill actionable, generalizable guidance compared to concise tips.",
            "key_insights": "Raw trajectory replay as prompt memory is feasible but inefficient; concise summaries (tips) are a more compact and generalizable memory representation for LLM agents in long-horizon, sparse-reward tasks.",
            "uuid": "e4819.1",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Expert-derived tips",
            "name_full": "Tips generated from expert demonstrations",
            "brief_description": "A variant where the LLM ingests expert demonstration trajectories (walkthroughs) and produces tips from them which are then used as prompts for the agent, accelerating learning compared to self-generated tips from failed trials.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 LLM agent with expert-derived tips",
            "agent_description": "GPT-4 is prompted to compare its behavior with expert walkthroughs and generate concise tips summarizing successful strategies from the expert trajectories; those tips are used to guide subsequent plays.",
            "memory_type": "expert demonstration-derived tips (demonstration memory)",
            "memory_description": "Expert walkthroughs (provided in the dataset) are summarized by the LLM into tips that capture successful action sequences and domain knowledge; tips are then added to prompts for future episodes.",
            "task_name": "TextWorld Cooking (text-based game decision-making)",
            "task_description": "Agent observes expert demonstrations/walkthroughs for cooking tasks and extracts tips to improve its policy in few-shot trials.",
            "benchmark_name": "TextWorld (Cooking domain)",
            "performance_with_memory": "With expert demonstrations used to generate tips, the LLM agent learns faster and reportedly achieves zero error across all difficulty levels after at most 6 trajectories in the few-shot experiments.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Expert-derived tips produce faster convergence than self-generated tips or pure replay; expert demos allow the agent to avoid lengthy trial-and-error and reach perfect performance within a small number of trials.",
            "limitations_or_challenges": "Dependence on the availability and coverage of expert walkthroughs; tips derived from experts must be correctly parsed and applicable (some initial autogenerated tips could be invalid and require correction); stochastic outputs of the LLM still cause occasional tip-ignoring.",
            "key_insights": "Using demonstrations to create concise tips is an efficient way to transfer expert knowledge into LLM agents without fine-tuning, enabling rapid few-shot mastery of complex sequential tasks.",
            "uuid": "e4819.2",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Aggregated general tips (multi-game memory)",
            "name_full": "Aggregated General Tips from Multiple Games (meta-memory across environments)",
            "brief_description": "A meta-memory approach where final successful tips from many games are collected and fed into another LLM to generate higher-level general tips that transfer zero-shot to unseen games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 LLM agent with aggregated multi-game tips",
            "agent_description": "GPT-4 agents first produce game-specific final tips from multiple training games; another LLM processes these into general tips that are then used as prompts for agents in unseen test games to enable zero-shot generalization.",
            "memory_type": "aggregated summary memory / meta-memory (cross-game tip aggregation)",
            "memory_description": "Final tips (those associated with successful trials) from several games are collected (in experiments, 10 games → 48 trajectories) and summarized by a second LLM into a set of generalized tips; those general tips are used as static prompts when evaluating on unseen games.",
            "task_name": "Zero-shot transfer on TextWorld Cooking (unseen games)",
            "task_description": "Evaluate whether general tips synthesized from multiple training games enable an LLM agent to perform well in unseen TextWorld cooking games without in-task trials.",
            "benchmark_name": "TextWorld (Cooking domain)",
            "performance_with_memory": "Using general tips generated from 10 training games (48 trajectories), the LLM achieved performance comparable to SOTA deep-RL methods for difficulty levels 0–3 and outperformed them on levels 3–4 in zero-shot evaluation.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Aggregated general tips enable strong zero-shot performance with orders-of-magnitude fewer trajectories than deep-RL SOTA training (10 games / 48 trajectories vs. 100 games / 100k episodes), suggesting that distilled cross-game tips can serve as an effective memory modality for transfer.",
            "limitations_or_challenges": "Fails in some low-difficulty cases when tips are overly general or do not capture specific edge-case conditions (e.g., when an ingredient is already in the required state); LLM stochasticity can still cause deviation from tips.",
            "key_insights": "Meta-aggregation of final tips across related environments acts like a lightweight Meta-RL memory, enabling sample-efficient zero-shot generalization; concise, corrected tips aggregated across environments are more effective for transfer than raw trajectories or per-episode reflections.",
            "uuid": "e4819.3",
            "source_info": {
                "paper_title": "Introspective Tips: Large Language Model for In-Context Decision Making",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "In-context reinforcement learning with algorithm distillation",
            "rating": 1
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 1
        },
        {
            "paper_title": "Learning to follow instructions in text-based games",
            "rating": 2
        }
    ],
    "cost": 0.01211375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Introspective Tips: Large Language Model for In-Context Decision Making
19 May 2023</p>
<p>Liting Chen 
Lu Wang 
Hang Dong 
Yali Du 
Kings College London</p>
<p>Jie Yan 
Fangkai Yang 
Shuang Li 
The Chinese University of Hong Kong (Shenzhen)</p>
<p>Pu Zhao 
Si Qin 
Saravan Rajmohan 
Qingwei Lin 
Dongmei Zhang 
Introspective Tips: Large Language Model for In-Context Decision Making
19 May 20235911203F2C4BB200C05B5BA232D524C6arXiv:2305.11598v1[cs.AI]
The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks.In this study, we employ "Introspective Tips" to facilitate LLMs in self-optimizing their decision-making.By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips.Our method enhances the agent's performance in both few-shot and zeroshot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games.Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations.Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making.Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), including OpenAI's GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), Google's PaLM (Chowdhery et al., 2022) and other benchmarks (Meta, 2023;Taori et al., 2023) have consistently achieved remarkable performance across various NLP tasks.The integration of LLMs into decision-making tasks (Huang et al., 2022a;Ahn et al., 2022;Kwon et al., 2023;Brooks et al., 2022;Yao et al., 2022) has garnered significant attention, as it presents an opportunity to develop decision-making agents that can emulate human-like cognitive processes (Shevlin et al., 2019).In decision-making tasks, such as in the domain of Reinforcement Learning (RL), limited interactions hinder optimal policy learning (Yarats et al., 2021).Owing to the power of LLMs, which have undergone extensive pre-training on vast amounts of data, agents can leverage the generalization capabilities of LLMs to enhance their performance across different tasks.In addition, common sense knowledge is a valuable asset in safety decision-making tasks (Brunke et al., 2022), and LLMs possess a wealth of this knowledge due to their extensive training and tuned with human feedback (Ouyang et al., 2022).By leveraging the inherent common sense knowledge within LLMs, decision-making agents can make more informed decisions, effectively tackling challenges such as sparse rewards and enhancing the learning process.</p>
<p>However, LLMs sometimes demonstrate errors or hallucinations (Ji et al., 2023;Peng et al., 2023), especially in domain-specific scenarios.Recent works focus on designing selfcorrection mechanisms to enhance the LLM's decision-making performance.For example, Chain-of-thought (CoT) (Wei et al., 2022) concentrates on static reasoning for one-step actions without self-correction.ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2023) focus on dynamic reasoning based on historical interactions to enable better decision-making performance.However, these works primarily focus on generating instructions or reflections specific to individual agents, hindering the adoption of the generalization and universality abilities of LLMs.Furthermore, the absence of correction for the generated reflections can lead to errors during the decision-making process.</p>
<p>Humans possess high generalization and self-evolution capabilities due to their natural ability to summarize tips through introspection (Cox, 1996;Van Merrienboer and Sweller, 2005).Introspection enables humans to reflect on their past experiences (successes or failures), distill key insights, and extract valuable lessons that can be applied to new situations.Inspired by the remarkable introspection capabilities of humans, we introduce "Introspective Tips" (or "Tips" for short) as a novel prompt-based paradigm, empowering an LLM decision-making agent with self-optimizing capabilities via learning tips from trajectories of itself, experts, and other environments.Figure 1 provides an example of introspective tips in a text-based game.</p>
<p>Figure 1: In both Game 1 and Game 2, the agent is tasked with checking the cookbook and cooking a meal.The cookbooks for each game are different, but the initial state remains the same.After playing in Game 1, the agent is asked to participate in Game 2, using tips or reflections generated from the Game 1 trajectory.The agent's reflection focuses on the errors made in Game 1 and is specific to that game.Tips, on the other hand, offer more general and high-level information about the game.Utilizing tips from Game 1, the agent learns that the stove is for frying ingredients and uses the oven to roast the apple.The agent subsequently updates its knowledge about the game by incorporating a third tip.However, when using reflections, the agent receives an irrelevant message about potatoes, which leads to its failure to check the cookbook and complete the task in Game 2. In this case, the general tips prove to be more helpful in guiding the agent's behavior and improving its performance, whereas the specific reflections from Game 1 do not provide useful information for Game 2.</p>
<p>Distinct from reflection (Shinn et al., 2023), which is a self-analysis process that delves into an agent's actions and experiences in detail, "tips" are compact pieces of information offering high-level guidance or suggestions.The goal of tips is to provide succinct and practical advice for enhancing decision-making performance without requiring an in-depth examination of past trajectories.In contrast, reflections can only be derived from the agent itself and primarily concentrate on learning from failures.Tips, on the other hand, can be acquired from the trajectories of other agents and human experts, focusing on learning from both successes and failures.For example, in the game setting, the agent can generalize tips to correctly use oven based on its successful trajectory, but reflection may focus more on failures.</p>
<p>Additionally, in contrast to using original trajectories as prompts, introspective tips serve as condensed and comprehensive insights extracted from lengthy trajectories.This allows LLMs to overcome the challenges in discerning the intricate relationship between dependent actions and sparse rewards across multiple trajectories (Adhikari et al., 2020).Introspective tips provide distinct advantages in terms of their applicability and generalization across different agents and contexts, as opposed to previous prompting methods such as reflections.Unlike reflections that target specific agents, tips can be shared among different agents, allowing for higher generalization performance.In the game setting of Figure 1, the tips generalized by the agent can be applied to various game settings with different recipes.In contrast, reflections are more specific to a single past trajectory.</p>
<p>Furthermore, designing suitable prompts for LLMs to generate introspective tips is a crucial challenge; manually crafting prompts can be burdensome.Therefore we propose a framework that dynamically adjusts the prompt based on insights derived from past trajectories or expert demonstrations through introspection.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• Introspective Tips for Self-Optimizing Decision-Making: We introduce Introspective Tips as a novel prompt-based paradigm, empowering LLM decision-making agents with self-optimizing capabilities for both few-shot and zero-shot scenarios.Unlike previous RL agents, Introspective Tips offers an end-to-end solution without the need for training or fine-tuning.It generates meaningful and easy-to-understand tips, facilitating human understanding and intervention in the decision-making process.</p>
<p>• Multiple Strategies in different scenarios for Learning Introspective Tips: We present distinct strategies applicable in different scenarios to prompt LLMs to learn Introspective Tips from their own trajectories, expert demonstrations, and multi-environment trajectories, which also shows the generalization and adaptability of Introspective Tips.</p>
<p>• A Dynamic Prompt Adjustment Framework: To simplify the prompting engineering process, we introduce a framework that dynamically adjusts the prompt by leveraging insights from past trajectories or expert demonstrations through introspection.This framework streamlines the improvement process, enhances the model's adaptability, and enables more efficient decision-making.</p>
<p>• Comprehensive Evaluation and Comparison: Our extensive experiments encompass over 100 games in TextWorld (Adhikari et al., 2020;Côté et al., 2019), testing few-shot and zero-shot learning scenarios.We evaluate the performance of our decisionmaking agent against state-of-the-art methods in the field (Tuli et al., 2022), highlighting the effectiveness and superiority of Introspective Tips.With tips generated from 48 trajectories, LLM agent performs better than previous deep learning method training for 100, 000 episodes in the unseen game with the highest difficulty level.</p>
<p>Related work</p>
<p>Language model for decision making LLMs (OpenAI, 2023;Chowdhery et al., 2022) have exhibited impressive proficiencies, facilitating their use in tasks beyond mere language generation and increasingly serving as policy models for decision-making in interactive settings (Yang et al., 2023).Wei et al. (2022) demonstrates that incorporating a series of intermediate reasoning steps can enhance decision-making abilities.Yao et al. (2022) introduces ReAct, a method for interleaved reasoning and action generation that fosters improved synergy between language comprehension and interactive decision-making tasks.Shinn et al. (2023) presents Reflexion, a technique that equips an LLM-based agent with a self-reflective LLM and a straightforward heuristic for detecting hallucination and inefficient action execution to examine the hypothesis.Madaan et al. (2023) adopts a similar strategy, enabling an LLM to offer feedback on its previously generated text and refine it to meet specific requirements.When regarded as dialogue agents, LLMs can also be trained to learn from human feedback and optimize their output (Ouyang et al., 2022;Bai et al., 2022).With further training, Li et al. (2022) constructs a general framework for decision-making tasks using pre-trained LMs, even in scenarios where language is neither provided as input nor output.Other studies (Singh et al., 2022;Huang et al., 2022a,b;Liang et al., 2022;Vemprala et al., 2023) have explored innovative strategies involving prompt engineering and the utilization of high-level function libraries to enhance the capabilities of LLMs.</p>
<p>Recent attempts explored different aspects of LLMs for decision-making.Huang et al. (2022a) and Ahn et al. (2022) use LLMs to generate plans or sub-goals that guide low-level Reinforcement Learning (RL) agents in taking actions.Kwon et al. (2023) utilize LLMs as proxy reward functions by prompting them with desired behaviors.Yao et al. (2022) focus on enabling LLM agents to select actions in text-based environments.In addition, a recent approach considers LLMs as world models (Brooks et al., 2022), where the agent learns the policy by interacting with the LLM-based world model.In this paper, we focus on directly grounding LLM in decision-making to take action because the other three methods required learning an extra decision-making agent, which required more samples.</p>
<p>In-context reinforcement learning In-context learning pertains to the ability of sequence prediction models to adapt to novel downstream tasks solely through the use of prompts without retraining or fine-tuning (Lu et al., 2021;Brown et al., 2020;Min et al., 2022).When applied to reinforcement learning, in-context learning models can generalize to diverse downstream tasks when provided with contexts such as demonstrations and task information.Laskin et al. (2022) recasts RL as an across-episode sequential prediction problem, and trains a causal transformer to autoregressively predict actions based on preceding learning histories as context.Brooks et al. (2022) employs the LLM as a world model for planning future trajectories and executing decisions in-context.Team et al. (2023) develops a versatile in-context learning algorithm capable of adapting to new and open-ended 3D challenging problems as rapidly as humans, by training an RL agent at a large scale.Lu et al. (2023) proposes to meta-learn across random linear projections of the observation and action spaces of randomly sampled DMControl tasks (Tassa et al., 2018).Trained on an extensive dataset, Gato (Reed et al., 2022) can generalize to new tasks by conditioning on demonstrations of the desired behavior.</p>
<p>Text-based game Text-based games are typically turn-based experiences played via a command line terminal.During each turn, the game state is conveyed through multiple lines of text, which enables players to input text commands that modify the state according to their preferences (Liu et al., 2022;Hendrycks et al., 2021;Osborne et al., 2022).Text-based games can be formally characterized as partially observable Markov decision processes (POMDPs) (Côté et al., 2019), considering that the agent only observes partial information about the environment at each turn.Intrinsic obstacles such as longterm dependencies, partial observation of current states, and sparse rewards, and complex action combinations render these games particularly challenging.Various deep learning approaches have been employed to address text-based games (Xu et al., 2022;Yin and May, 2019;Ammanabrolu and Hausknecht, 2020;Kimura et al., 2021).Focusing on tasks in the TextWorld domain (Côté et al., 2019), Adhikari et al. (2020) explores learning graphstructured state representations via a data-driven approach, introducing the Graph Aided Transformer Agent (GATA) that learns to construct and update graph-structured beliefs while optimizing rewards.Building upon their work, Tuli et al. (2022) equips GATA with an internal structured representation of natural language instructions using Linear Temporal Logic (LTL) to enhance the instruction-following capabilities of text-based game agents.</p>
<p>Method</p>
<p>In this section, we elaborate on the method employed to leverage the capabilities of LLMs in sequential decision-making tasks, particularly text-based games, by addressing LLMs inherent limitations and capitalizing on their strengths.LLMs' proficiency in understanding and generating human-like text renders them promising candidates for tasks involving natural language processing, such as text-based games.However, they lack specific domain knowledge when asked to make decisions in certain tasks thus may not reach the greatest potential when used directly.</p>
<p>We conjecture that the function space of generative LLMs P (θ) is sufficiently expansive to encompass the function of an expert policy in text-based games.The action space of policy π is constrained by text output.By selecting an appropriate prompt pr containing enough domain knowledge, we can derive a model P (θ|pr) that can function as a π in decision making, bridging the gap between LLMs and reinforcement learning.Through incorporating strategies including learning from past experiences, expert demonstrations, and multiple games, we aim to develop a versatile and robust framework that excels in a wide range of gaming scenarios.This approach ultimately opens up the journey of discovering the potential of LLMs in the domain of complex sequential decision-making tasks.</p>
<p>Challenges and Solutions for LLMs in Decision-Making Problems</p>
<p>There are several major challenges when applying Language Learning Models (LLMs) to solve decision-making problems, including self-optimization, input length limitations, and prompt dependence.In this section, we discuss these challenges and potential solutions.</p>
<p>Self-Optimizing decision-making LLMs can exhibit errors or hallucinations, particularly in domain-specific scenarios.Developing self-optimizing mechanisms to enable LLMs to correct their mistakes is essential for improving their decision-making performance.Moreover, allowing LLMs to self-optimize across various scenarios poses a significant challenge.Current approaches, such as self-reflection, attempt to rectify errors by concentrating on the model's own mistakes.However, this method has limitations, including the risk of LLMs becoming misled by their own errors during reflection and lacking a comprehensive understanding of the task or environment.To address these challenges, we employ "Introspective Tips" to facilitate LLMs in self-optimizing their decision-making.We empower LLMs to learn from their own trajectories, expert demonstrations, and trajectories in different environments.By extracting concise and actionable advice from these trajectories, the agent gains a better understanding of the environment.Furthermore, by refining its own tips, the problem of hallucination is alleviated.</p>
<p>Limited input length Language models like GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022) often encounter input length limitations.In RL, agents typically face lengthy and complex trajectories, as well as sparse rewards that offer feedback only after numerous steps or interactions.Due to the restricted input capacity and extended trajectories, LLMs struggle to discern the relationship between actions and rewards, ultimately impeding the acquisition of the optimal policy.To address these issues and optimize the available input space, we use tips that effectively condense the training dataset or critical information generally found in classical RL settings.By incorporating these summaries, LLMs can better understand the relationships within the data and identify essential patterns that might be otherwise obscured by the sheer length or complexity of the trajectories.Moreover, providing concise, relevant information enables LLMs to focus on the crucial aspects of the problem at hand, potentially leading to more accurate results.This strategy can be particularly beneficial in scenarios where the agent must learn from limited data or adapt to changing environments, as the distilled information can help guide the learning process more efficiently.</p>
<p>Prompt dependence: The LLM's performance on a specific problem heavily relies on the quality and order of prompts.In order to solve the sequential decision problem in text-based games, that is to produce more relevant and valuable responses in each round, it is crucial to craft prompts in decision making.However, creating prompts using human interaction can be time-consuming and tedious.To address this issue, our method introduces a framework that dynamically adjusts the prompt based on past trajectories through introspection, streamlining the process and improving the model's adaptability.While designing a specific tip for one game can be laborious, we also explore the possibility of generating a universal prompt that can transform the LLM into an efficient RL agent across various games and contexts.This general prompt would further facilitate the LLM's ability to adapt and perform effectively in a wide range of decision-making situations.</p>
<p>Basic setting</p>
<p>In our basic setting (Fig 2a ), we directly utilize an LLM as an agent participating in a text-based game.A sample interaction proceeds as follows: Agent: inventory( )The system (user) clearly defines the role and action space for the LLM agent and provides an initial text-based description of the environment, setting the scene for the agent.The LLM-agent interprets the provided information and decides on an action based on its understanding of the game's context.The agent submits its chosen action as text, which the system interprets, processes, updates the environment, and provides new feedback, including information on invalid actions or a description of the updated state, to the agent.The LLMagent continues to interact with the game, choosing actions and receiving feedback, until the game reaches its conclusion.The game concludes as a success if the agent completes all required steps, or as a failure if the agent takes erroneous actions or reaches the maximum number of turns.Throughout the entire process, the agent's goal is to navigate the game world and make decisions based on textual input and output.</p>
<p>Strategies for three scenarios</p>
<p>We have developed strategies for three scenarios to enhance the agent's learning addressing the two challenges:</p>
<p>Learning from its own history: The agent employs a history replay method to generate tips for self-improvement.The agent is prompted as the basic setting to finish a basic trial.If the agent fails, the agent creates several tips to address its past mistakes.In subsequent attempts, these tips serve as prompts.If the agent fails even if provided with tips, it is prompted to generate more effective tips for itself.The agent is also provided with all its previous actions that led to failure in the game.By introspectively analyzing its past actions and their outcomes, the agent generates valuable insights to refine its policy.This self-enhancement process enables the agent to overcome obstacles and make better choices in future gameplay.Moreover, since tips and incorrect actions are shorter than an entire trajectory, the agent can learn from a more extensive history than relying solely on past experiences as memory (Fig 2b).</p>
<p>Learning from expert demonstrations: The agent evaluates its performance by comparing it to expert demonstrations and makes adjustments as necessary.In this setting, the agent is instructed to observe both the actions leading the expert agent to achieve the goal and the actions causing it to fail the game.By examining successful strategies and tactics employed by experts, the agent can pinpoint areas for improvement and generat tips that can help modify its actions accordingly.This learning approach is faster compared to a purely trial-and-error method.For instance, the agent can learn the correct usage of an appliance by contrasting its failed attempts with the successful actions demonstrated by an expert in a text-based game, rather than experimenting with all possible actions.</p>
<p>The agent generates tips based on these observations, and these tips serve as new prompts when playing subsequent rounds.If the agent fails even when provided with tips, it is prompted to reflect on the given advice.This method enables knowledge transfer from expert demonstrations, resulting in a more efficient and informed decision-making process.Feedback from the environment acts as guidance for the LLM model, assisting it in determining the accuracy and effectiveness of its generated tips (Fig 2c).</p>
<p>Learning from multiple games: In this scenario, agents are allowed to participate in several games.All final tips (tips that lead to a successful trial) generated across these games are sent to another LLM, which is then tasked with generating general tips to help the agent become an expert across various game settings.In this context, the LLM leverages its knowledge of summarization to produce comprehensive and valuable tips that enable the agent to improve its performance in a wide range of games.These games share some similarities but with some information different.For example, in the cooking game, these games share a similar theme (cooking in a modern house), similar text commands, and similar entities (i.e., interactable objects within the games), but with different cookbooks and maps involved in the game.The LLM generalizes knowledge across diverse games, functioning similarly to a Meta-RL agent.The general tips are served as prompts in the unseen test game.This ability allows the LLM to perform effectively in a broad range of text-based games, positioning it as a powerful tool for reinforcement learning in complex environments (see Fig 2d).</p>
<p>By incorporating these strategies in the three scenarios, our method presents a robust framework for exploiting the potential of LLMs in reinforcement learning for text-based games.Learning from past experiences, expert demonstrations, and multiple games allows the agent to adapt and excel in various gaming scenarios, ultimately demonstrating the versatility and effectiveness of LLMs in the realm of reinforcement learning.</p>
<p>Experimental Results</p>
<p>We conduct experiments on text-based games to assess the performance of LLMs as agents and address three key questions: Q1: Can LLMs learn from their historical trajectories and improve their performance by reflecting on different tips?Q2: Can LLM learn from expert Figure 3: Few shot performance over different difficulty levels of games demonstrations and can expert demonstration boost the learning of different strategies?Q3: Can LLMs generalize some tips to play in various game settings?For Q1 and Q2, we analyze the few-shot decision-making capabilities of LLMs, while for Q3, we concentrate on their zero-shot decision-making abilities.</p>
<p>Experiment setup</p>
<p>Table 1: Statistics of The Game</p>
<p>Level #Ingredients #Locations Points (Cook, Cut, Open )
0 1 1 3 (×, ×, ) 1 1 1 4 (×, , ) 2 1 1 5 ( , , ) 3 1 9 3 (×, ×, ) 4 3 6 11 ( , , )
We concentrate on the TextWorld Cooking domain, which gained prominence through GATA (Adhikari et al., 2020) and Microsoft's TextWorld Problems: A Language and Reinforcement Learning Challenge (Trischler et al., 2019).Following Adhikari et al. (2020) and Tuli et al. (2022), we divide the games into five subsets with different difficulty levels.For easier games (with smaller difficulty levels), the recipe requires fewer ingredients, and the agent does not need to navigate through the world.For harder games, the agent is asked to navigate through different rooms to acquire various ingredients and cook the meal.The score is related to the relevant steps required in the cookbook of each game.If the agent completes a required step, it will earn one point.For the hardest game, the agent is required to finish 11 steps to complete this task.These steps involve opening certain containers to obtain the ingredients, cutting the ingredients as required (dice, slice, chop), and cooking the ingredient using the correct heat source (oven or stove).Statistics of the games are presented in Table 1.In the dataset, expert demonstrations are provided in the form of walkthroughs and do not require human intervention for generation.Following previous work, we measure the performance of the algorithms using two metrics: normalized game points and game success rate.We test the game in 20 different games for each difficulty level.By averaging the points over the 20 games, and then dividing by the maximum score an agent can earn, we obtain the normalized game points.The game success rate is calculated as the percentage of games in which the agent successfully completes all the required steps.We use GPT-4 as our base LLM.</p>
<p>Few-shot performance</p>
<p>We first test the few-shot performance of our method.We make a comparison between purely replay, tips summary using past history, and tips summary compared to expert trajectory.The latter two cases corresponds to our first and second scenario.For purely replay, we directly ingest the past trajectory as LLM's prompts to see if it can learn.Given that the input length is limited, we set the maximum number of trajectories as 3 in the experiments.For tips summary memory, the LLM agent is provided with the past failed actions and the tips generated from the last trial.We observe that with tips memory, the agent can collect more information using different trials and cumulatively achieve a high success rate (larger than 0.9) and high points earned (larger than 0.95) in the end, as shown in Figure 3.With expert demonstration, the agent can learn faster and achieve zero error in all difficulty levels after at most 6 trajectories.This comparison shows that concise tip summary assists the LLM in gathering information and making better decisions.With expert demonstrations, the LLM can learn more efficiently compared to solely relying on trial and error.We further show an example about how tip summary can help the agent make better decision.In this log file (showing only relevant information), the agent is tasked with roasting a purple potato using a predefined ActionList in a text-based game.In the first attempt, the agent mistakenly uses the stove to cook the purple potato, resulting in frying it and losing the game.The agent then generates a tip to use the "roast" action.However, this is an invalid action, and it fails again.Next, the agent generates a second tip to use the "cook" command with the stove, suggesting a different heat setting or method to avoid frying.Unfortunately, in this text-based game, there are no commands to change the heat setting, leading to another failure.After the third attempt, the agent realizes it should try using an oven to roast the potato, and finally succeeds in the fourth trial.This demonstrates that improved tips can lead to successful learning.</p>
<p>Zero-shot Performance</p>
<p>First trial: &gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try roast the potato next time instead of cook purple potato with stove after dicing the purple potato;</p>
<p>Second trial: &gt;agent: roast the potato system: Invalid action.&gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try cook purple potato with stove next time after you have dicing the purple potato, but make sure to use a different heat setting or method to avoid frying the purple potato; Third trial: &gt;agent: cook purple potato with stove system: You fried the purple potato.Tips to win the game next time: 1.You should try cook purple potato with oven next time instead of cook purple potato with stove, as the recipe suggests roasting the purple potato rather than frying it;</p>
<p>Forth trial: &gt;agent: cook purple potato with oven system: You roasted the purple potato.Your score has just gone up by one point.</p>
<p>In this experiment, we focus on training LLM agents using tips generated from the training set.We collect final tips, which are those leading to successful trials, from difficulty level 4 (the most challenging level) games in the training set.These tips are then sent to another LLM, which generates tips aimed at helping the agent become proficient across various game settings.The final tips can be found in the appendix.We used 10 games, which consist of 48 trajectories in total (averaging 4.8 trajectories per game) to generate the final tip.We then test these tips on an unseen test set across different difficulty levels.To evaluate our LLM model with general tips as prompts, we compare it to state-of-the-art (SOTA) text-based game agents utilizing deep learning techniques, including TDQN (Adhikari et al., 2020), GATA (Adhikari et al., 2020), and ITL (Tuli et al., 2022).These models are trained on 100 different games for each difficulty level, with 100,000 episodes per level.Results for levels 0 to 3 are provided by the authors of (Tuli et al., 2022), while level 4 results are obtained from Adhikari et al. (2020).</p>
<p>As shown in Figure 4, we prompt the LLM with general tips and test it on levels 1 to 4. Given level 0's simplicity, we do not provide tips for this level.Our findings reveal that the LLM with tips, using only 10 games and 48 trajectories, achieves performance comparable to SOTA methods for difficulty levels 0-3.For levels 3-4, LLM outperforms other methods, due to the reasoning ability of the LLM that allows us to bypass the need to learn navigation and the correct tips generated in the same difficulty level, an aspect that hinders SOTA methods (Adhikari et al., 2020).</p>
<p>Limitation Despite these achievements, the LLM agent underperforms in some lower difficulty level games.This underperformance can be attributed to two factors.First, the LLM fails to generate a general tip that addresses specific situations encountered in easier levels.For instance, in difficulty level 1, when an ingredient is already in its desired state according to the recipe (e.g., roasted or fried), the corresponding actions (roasting or frying) become unnecessary.By incorporating human-generated tips (see the appendix for the full list of tips), the LLM agent can achieve significantly higher points and success rates, as demonstrated in Table 2. Second, the LLM's probabilistic nature leads to non-deterministic outputs, which occasionally cause the agent to disregard the tips and execute erroneous actions.Despite being provided with tips, the LLM agents will still make some mistakes due to their inherent probabilistic behavior.Nevertheless, even with this randomness, the LLM agent with general tips can outperform state-of-the-art (SOTA) agents specifically trained to excel in this task.</p>
<p>Conclusion</p>
<p>We introduce the novel concept of "Introspective Tips" as a powerful mechanism to improve the decision-making capabilities of LLM agents.Drawing inspiration from human introspection, this approach enables agents to extract and learn from generalized, high-level information that can be applied across various tasks and contexts.To effectively implement Introspective Tips, we propose a framework that dynamically adjusts prompts based on insights derived from past trajectories or expert demonstrations through introspection.This approach alleviates the burden of manual prompt crafting while empowering LLM agents with self-optimizing capabilities.By leveraging the rich common sense knowledge and generalization abilities of LLMs, our Introspective Tips paradigm outperforms SOTA methods in text-based game.Future work could focus on refining the framework for prompt generation, exploring more sophisticated methods for extracting tips from trajectories, and evaluating the effectiveness of introspective tips in a broader range of tasks and real-world applications.</p>
<p>• 3. Pay attention to the cooking methods mentioned in the recipe, such as roasting or frying, and use the appropriate appliances like the oven and stove.To operate an appliance, use actions from the ActionList, such as "cook(food, heat source)".</p>
<p>• 4. Use the correct actions from the ActionList to prepare the ingredients, such as "slice(food, sharp object)", "dice(food, sharp object)", or "chop(food, sharp object)".</p>
<p>• 5. Keep track of your inventory and the state of each item to avoid making mistakes in the preparation process.Use the "inventory()" action to check your inventory.</p>
<p>• 6. Follow the steps in the recipe in the correct order, ensuring each ingredient is prepared as instructed.</p>
<p>• 7. Once all ingredients are prepared according to the recipe, use the "prepare meal()" action to combine them into a meal.</p>
<p>• 8.If you encounter any difficulties or errors, refer back to the ActionList and the environment description to make sure you're using the correct actions and items.• 2 Before cooking an ingredient, always use inventory() first to check current state of the ingredient.If an ingredient is already in the desired state(roasted or fried) as per the recipe, there is no need to perform the actions (roast or fry) mentioned in the recipe.</p>
<p>• 3 Try take(food) before you prepare(meal).After you take(food), use inventory() to check the status of the food.</p>
<p>• 4 Make sure the food is in your inventory before you prepare(meal).Try eat(meal) when the meal is prepared.</p>
<p>• 5 Dice, chop, and slice are different.</p>
<p>• 6 Fry and Roast are different.Use correct heat source to cook the ingredient.Oven is for roasting something, not frying something.</p>
<p>• 7 Use cook(food, oven) to roast something, instead of putting the food into the oven and wait.</p>
<p>• 8 Be careful when you cook(food, stove) or cook(food, oven), make sure the food is not fried or roasted before.Otherwise you may burn the food and lose the game.slice the red potato roast the red potato dice the white onion fry the white onion slice the yellow bell pepper fry the yellow bell pepper prepare meal &gt;agent : take(red potato) Env: You take the red potato from the counter.Your score has just gone up by one point.&gt;agent : take(cutting tool) Env: You take the knife from the counter.&gt;agent : slice(red potato, cutting tool) Env: You slice the red potato.Your score has just gone up by one point.&gt;agent : cook(red potato, oven) Env: You roasted the red potato.Your score has just gone up by one point.&gt;agent: go(south) Env:</p>
<p>-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(west) Env:</p>
<p>-= Bathroom =-You find yourself in a bathroom.An ordinary kind of place.You make out a toilet.But oh no! there's nothing on this piece of garbage.</p>
<p>You need an exit without a door?You should try going east.&gt;agent : go(east) Env: -= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(east) Env:</p>
<p>-= Bedroom =-You are in a bedroom.An ordinary kind of place.You smell an intriguing smell, and follow it to a bed.Unfortunately, there isn't a thing on it.You move on, clearly infuriated by TextWorld.You don't like doors?Why not try going west, that entranceway is not blocked by one.&gt;agent : go(west) Env:</p>
<p>-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.</p>
<p>There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(north) Env:</p>
<p>-= kitchen =-You find yourself in a kitchen.A standard kind of place.You can see a closed fridge, which looks conventional, nearby.You can see a closed oven.What's that over there?It looks like it's a table.The table is massive.On the table you can make out a cookbook.You rest your hand against a wall, but you miss the wall and fall onto a counter!On the counter you make out a red apple, a raw purple potato, a red hot pepper, a raw yellow potato and a banana.You see a stove.The stove is conventional.However, the stove, like an empty stove, has nothing on it.There is a closed plain door leading north.You don't like doors?Why not try going south, that entranceway is not blocked by one.There is an exit to the west.&gt;agent : open(fridge) Env: You open the fridge, revealing a yellow bell pepper, a white onion, a carrot, a red onion, an orange bell pepper, a block of cheese and a raw pork chop.&gt;agent : take(white onion) Env: You take the white onion from the fridge.Your score has just gone up by one point.&gt;agent : take(yellow bell pepper) Env: You take the yellow bell pepper from the fridge.Your score has just gone up by one point.</p>
<p>Figure 2 :
2
Figure 2: LLM as RL agent</p>
<p>Figure 4 :
4
Figure 4: Performance of LLM as a text-based game agent compared to SOTA methods.Given that experimental results of difficulty level 4 is not included in Tuli et al. (2022), we obtain data from Adhikari et al. (2020) and some data is missing.</p>
<p>B. 2
2
Human-generated tips • 1 If you get stuck, use the actions goal(), examine(cookbook), inventory() and look() to figure out what you have and what you need to do.</p>
<p>Table 2 :
2
Performance of the LLM agent with human-generated tips across different difficulty levels.
Level 0Level 1Level 2Level 3Level 4Points Suc. Rate Points Suc. Rate Points Suc. Rate Points Suc. Rate Points Suc. Rate110.880.800.920.900.960.950.960.95</p>
<p>You can see a closed fridge, which looks conventional, nearby.You can see a closed oven.What's that over there?It looks like it's a table.The table is massive.On the table you can make out a cookbook.You rest your hand against a wall, but you miss the wall and fall onto a counter!On the counter you make out a raw red potato, a red apple, a raw purple potato, a red hot pepper, a raw yellow potato, a banana and a knife.You see a stove.The stove is conventional.However, the stove, like an empty stove, has nothing on it.There is a closed plain door leading north.You don't like doors?Why not try going south, that entranceway is not blocked by one.There is an exit to the west.&gt;agent: examine(cookbook) Env: You open the copy of "Cooking: A Modern Approach (3rd Ed.)" and start reading: Recipe #1 ---Gather all following ingredients and follow the directions to prepare this tasty meal.
Ingredients:red potatowhite onionyellow bell pepperDirections:
• take(item) # take an item that is on the floor • put(item, supporter) # place an item on a supporter• take from(item, container) # take an item from a container or a supporter• insert(item, container) # place an item into a container• lock(item, key) # lock a door or a container with a key• unlock(item, key) # unlock a door or a container with a key• cook(food, heat source) # cook cookable food with something providing heat • slice(food, sharp object) # slice cuttable food with something sharp• chop(food, sharp object) # chop cuttable food with something sharp• dice(food, sharp object) # dice cuttable food with something sharp• prepare meal() # combine ingredients from inventory into a mealAppendix B. Gnenral TipsB.1 Tips generated using 10 games • 1. Always double-check the recipe before performing any actions, and follow the instructions closely.• 2. Make sure to gather all required ingredients and tools before starting to cook.Use the "examine(item)" and "take(item)" actions to interact with items in the environment.Appendix C. PromptsPrompts for generating the tips: Generating tips for the first trial Now you failed the game.Come up with tips for yourself to win the game next time.Pay attention to steps that make you lose the game.Pay attention to those actions and come up with concise actions in the ActionList that you should try next time.Starting your response with "Tips to win the game next time:"Generating tips for other trial Now you failed the game.Come up with tips for yourself to win the game next time.Pay attention to steps that make you lose the game.Make a note on those actions and come up with concise actions in the ActionList that you should try next time.Correct and improve the previous tips that I gave you if you think some of them are wrong.Generating general tips I want you to provide me all possible specific reasons that an agent may fail in a textbased game and summarize all tips for the agent to improve.I will present you several reasons and tips that summarized by others before.In every file, the action that make an agent failed before and some tips to correct this error are provided.Noticed that the tips provided may not be correct.For example, some actions suggested by the tips are not in the ActionList.Correct this kind of error in the tips that you generate.This tips will be used by another rookie agent.So make sure to provide all necessary and userful information for the rookie agent.You should also include action in the ActionList that the agent should use to operate appliance.Appendix D. Sample TrajectoryEnv: You are hungry!Let's cook a delicious meal.Check the cookbook in the kitchen for the recipe.Once done, enjoy your meal! -= Bedroom =-You are in a bedroom.An ordinary kind of place.You smell an intriguing smell, and follow it to a bed.Unfortunately, there isn't a thing on it.You move on, clearly infuriated by TextWorld.You don't like doors?Why not try going west, that entranceway is not blocked by one.&gt;agent: go(west) Env:-= Corridor =-You are in a corridor.An usual kind of place.I guess you better just go and list everything you see here.There is an exit to the east.There is an exit to the north.There is an exit to the west.Don't worry, there is no door.&gt;agent : go(north) Env:-= Kitchen =-You find yourself in a kitchen.A standard kind of place.
Learning dynamic belief graphs to generalize on text-based games. A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W Hamilton, Advances in Neural Information Processing Systems. 202033</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>E Brooks, L Walls, R L Lewis, S Singh, arXiv:2210.03821-context policy iteration. 2022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Safe learning in robotics: From learning-based control to safe reinforcement learning. L Brunke, M Greeff, A W Hall, Z Yuan, S Zhou, J Panerati, A P Schoellig, Robotics, and Autonomous Systems. 52022Annual Review of Control</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Revised Selected Papers. Stockholm, SwedenSpringer2018. July 13. 2018. 20197</p>
<p>Introspective multistrategy learning: Constructing a learning strategy under reasoning failure. M T Cox, 1996Georgia Institute of Technology</p>
<p>),<em> advances in neural information processing systems (datasets and benchmarks track),</em> 2021. D Hendrycks, M Mazeika, A Zou, S Patel, C Zhu, J Navarro, D Song, B Li, J Steinhardt, Advances in neural information processing systems. 2021what would jiminy cricket do? towards agents that behave morally</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022barXiv preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. D Kimura, M Ono, S Chaudhury, R Kohita, A Wachi, D J Agravante, M Tatsubori, A Munawar, A Gray, arXiv:2110.109632021arXiv preprint</p>
<p>M Kwon, S M Xie, K Bullard, D Sadigh, arXiv:2303.00001Reward design with language models. 2023arXiv preprint</p>
<p>M Laskin, L Wang, J Oh, E Parisotto, S Spencer, R Steigerwald, D Strouse, S Hansen, A Filos, E Brooks, arXiv:2210.14215-context reinforcement learning with algorithm distillation. 2022arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, C Paxton, Y Du, C Wang, L Fan, T Chen, D.-A Huang, E Akyürek, A Anandkumar, A Jacob, M Igor, T Antonio, Z Yuke, Advances in Neural Information Processing Systems. 202235</p>
<p>J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. 2022arXiv preprint</p>
<p>Learning object-oriented dynamics for planning from text. G Liu, A Adhikari, A -M. Farahmand, P Poupart, International Conference on Learning Representations. 2022</p>
<p>Structured state space models for in-context reinforcement learning. C Lu, Y Schroecker, A Gu, E Parisotto, J Foerster, S Singh, F Behbahani, arXiv:2303.039822023arXiv preprint</p>
<p>K Lu, A Grover, P Abbeel, I Mordatch, arXiv:2103.05247Pretrained transformers as universal computation engines. 20211arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Introducing llama: A foundational, 65-billion-parameter large language model. A Meta, Meta AI. 2023</p>
<p>S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>A survey of text games for reinforcement learning informed by natural language. P Osborne, H Nõmm, A Freitas, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, M Galley, P He, H Cheng, Y Xie, Y Hu, Q Huang, L Liden, Z Yu, W Chen, arXiv:2302.128132023arXiv preprint</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022A generalist agent. arXiv preprint</p>
<p>The limits of machine intelligence: Despite progress in machine intelligence, artificial general intelligence is still a major challenge. H Shevlin, K Vold, M Crosby, M Halina, EMBO reports. 2010e491772019</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, arXiv:2209.11302Progprompt: Generating situated robot task plans using large language models. 2022arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. 2018arXiv preprint</p>
<p>Human-timescale adaptation in an open-ended task space. A A Team, J Bauer, K Baumli, S Baveja, F Behbahani, A Bhoopchand, N Bradley-Schmieg, M Chang, N Clay, A Collister, arXiv:2301.076082023arXiv preprint</p>
<p>First textworld problems, the competition: Using text-based games to advance capabilities of ai agents. A Trischler, M.-A Côté, P Lima, 2019Microsoft Research Blog</p>
<p>Learning to follow instructions in text-based games. M Tuli, A Li, P Vaezipoor, T Klassen, S Sanner, S Mcilraith, Advances in Neural Information Processing Systems. 202235</p>
<p>Cognitive load theory and complex learning: Recent developments and future directions. J J Van Merrienboer, J Sweller, Educational psychology review. 2005</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, 2023</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Perceiving the world: Questionguided reinforcement learning for text-based games. M Xu, L Fang, Y Chen, J T Du, C Zhou, Zhang, arXiv:2204.095972022arXiv preprint</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, arXiv:2303.041292023arXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. D Yarats, A Zhang, I Kostrikov, B Amos, J Pineau, R Fergus, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<blockquote>
<p>agent: dice(white onion, cutting tool) Env: You dice the white onion. Your score has just gone up by one point. &gt;agent: cook(white onion, stove) Env: You fried the white onion. Your score has just gone up by one point. &gt;agent: slice(yellow bell pepper, cutting tool) Env: You slice the yellow bell pepper. Your score has just gone up by one point. &gt;agent: cook(yellow bell pepper, stove) Env: You fried the yellow bell pepper. Your score has just gone up by one point. X Yin, J May, 2019 IEEE Conference on Games (CoG). IEEE2019Env: Adding the meal to your inventory. Your score has just gone up by one point. &gt;agent: eat(meal) Env: You eat the meal. Not bad. Your score has just gone up by one point. *** The End *** You scored 11 out of a possible 11, in 23 turn(s</p>
</blockquote>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8056 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8056</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8056</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-38d64919ba526868a850a0e5f6239d4c474b7e7e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38d64919ba526868a850a0e5f6239d4c474b7e7e" target="_blank">Large Language Models are not Fair Evaluators</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a calibration framework with three simple yet effective strategies that successfully mitigates evaluation bias, resulting in closer alignment with human judgments.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the"win/tie/lose"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \url{https://github.com/i-Eval/FairEval} to facilitate future research.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8056.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8056.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used as an automatic evaluator to compare pairwise assistant responses; exhibits positional bias (preference for first-position response) and can produce self-conflicting judgments when candidate order is swapped.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are not Fair Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise comparison of conversational assistant responses (helpfulness, relevance, accuracy, level of detail)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Vicuna Benchmark (80 questions across 9 categories)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI API 'gpt-4' used as evaluator; deterministic when temperature=0 for non-sampling methods; sampling with temperature=1 for MEC experiments (k sampled evidences).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three authors (AI researchers) independently annotated win/tie/lose per example; final label = majority vote; each annotation ~3 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy; kappa (kappa correlation coefficient)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.527</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>strong positional bias favoring first-displayed response; high conflict rate when swapping candidate order (self-conflicting outputs); instability on examples with small score gaps; lower agreement with humans in vanilla prompting; output conclusions sometimes unsupported by subsequent explanations (conclusion-first unfaithful to evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Quantitative results show GPT-4's vanilla evaluator aligns moderately with human majority (VANILLA accuracy 52.7%, kappa 0.24) but exhibits a large Conflict Rate (e.g., 46.3% for Vicuna-13B vs ChatGPT case). GPT-4 tends to prefer the first-position response. Conflict rate is negatively correlated with the score gap: swaps produce more conflicts when candidate scores are close (score gap <=1). Applying calibration (EC/MEC/BPC) improves alignment progressively (MEC k=3 -> accuracy 58.7% kappa 0.30; MEC k=3 + BPC -> 62.5% kappa 0.37), and adding human-in-the-loop (HITLC, top-20% by BPDE) yields alignment comparable to humans (73.8% accuracy, kappa 0.56). The paper highlights failure modes (positional bias, self-conflict) and shows calibration strategies reduce these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Lower monetary cost and faster throughput than full human annotation (example: GPT-4 VANILLA cost reported $2.00 vs human annotator cost $30 per annotator for the 80-example benchmark); scalable and reproducible evaluation pipeline; provides explanations which can be leveraged in MEC.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise evaluation template (Table 1) used first (VANILLA): prompt asks to rate helpfulness/relevance/accuracy/detail and provide explanation; Evidence Calibration (EC, Table 3) forces the model to generate evaluation evidence before scoring; Multiple Evidence Calibration (MEC) samples k evidences (k=3 default, also tested k=6) with sampling temperature t (0 for deterministic methods, t=1 for MEC sampling); Balanced Position Calibration (BPC) evaluates both orderings and averages 2k scores; Human-in-the-Loop Calibration (HITLC) computes Balanced Position Diversity Entropy (BPDE) across 2k ERs to select top-β examples (β=20% in experiments) for human annotation; dataset size N=80; metrics: accuracy and kappa against majority human vote; conflict rate defined as fraction of examples where ER_r12 != ER_r21.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8056.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8056.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT used as an automatic evaluator for pairwise assistant comparisons; shows a strong positional bias (preference for second-position response), very high conflict rates in vanilla prompting, and lower alignment with human judgments compared to GPT-4 absent calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models are not Fair Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise comparison of conversational assistant responses (helpfulness, relevance, accuracy, level of detail)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Vicuna Benchmark (80 questions across 9 categories)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI API 'gpt-3.5-turbo-0301' used as evaluator; deterministic with temperature=0 for non-sampling methods; sampling used for MEC experiments (temperature varied in analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three authors (AI researchers) independently annotated win/tie/lose per example; final label = majority vote; each annotation ~3 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy; kappa (kappa correlation coefficient)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.444</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>very strong positional bias favoring second-displayed response; extremely high conflict rates when swapping order (e.g., up to 82.5% in some pairings); low baseline agreement with human majority; sensitivity to prompt template (scoring vs comparing); instability when score gaps are small; lower performance on complex tasks relative to GPT-4 without calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT's VANILLA alignment with human majority is poor (VANILLA accuracy 44.4%, kappa 0.06) and conflict rates are very high (e.g., 82.5% conflict in Vicuna-13B vs ChatGPT case). Calibration improves results substantially: EC (k=1) raises accuracy to 52.6% kappa 0.23; MEC (k=3) to 53.2% kappa 0.24; MEC k=3 + BPC k=3 to 58.7% kappa 0.31. Adding HITLC with β=20% human intervention achieves ~71.3% accuracy and kappa 0.52, approaching human-average alignment. The paper reports that ChatGPT prefers the second response in the vanilla template and is more sensitive to order than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Low monetary cost per evaluation (reported $0.10 for VANILLA ChatGPT runs on the 80-example set) and high speed; calibration methods (MEC, BPC) can substantially improve alignment with humans while retaining lower cost than full human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same evaluation templates as GPT-4: VANILLA scoring template (conclusion then explanation), EC (generate evidence first), MEC sampling k evidences (k tested 1,3,5,7), temperature tuning experiments (found t around 0.6–1.0 best for MEC), BPC swaps positions and averages 2k scores, HITLC selects top-β examples by BPDE (β=20% used) for human annotation; comparisons performed over 100 randomized runs when not using BPC to stabilize results; metrics reported vs majority human vote: accuracy and kappa; conflict rate reported for vanilla templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Instruction Tuning with GPT-4 <em>(Rating: 1)</em></li>
                <li>Alpacafarm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
                <li>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8056",
    "paper_id": "paper-38d64919ba526868a850a0e5f6239d4c474b7e7e",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GPT-4 (judge)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "GPT-4 used as an automatic evaluator to compare pairwise assistant responses; exhibits positional bias (preference for first-position response) and can produce self-conflicting judgments when candidate order is swapped.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are not Fair Evaluators",
            "evaluation_task": "Pairwise comparison of conversational assistant responses (helpfulness, relevance, accuracy, level of detail)",
            "dataset_name": "Vicuna Benchmark (80 questions across 9 categories)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "OpenAI API 'gpt-4' used as evaluator; deterministic when temperature=0 for non-sampling methods; sampling with temperature=1 for MEC experiments (k sampled evidences).",
            "human_evaluator_type": "Three authors (AI researchers) independently annotated win/tie/lose per example; final label = majority vote; each annotation ~3 minutes.",
            "agreement_metric": "accuracy; kappa (kappa correlation coefficient)",
            "agreement_score": 0.527,
            "reported_loss_aspects": "strong positional bias favoring first-displayed response; high conflict rate when swapping candidate order (self-conflicting outputs); instability on examples with small score gaps; lower agreement with humans in vanilla prompting; output conclusions sometimes unsupported by subsequent explanations (conclusion-first unfaithful to evidence)",
            "qualitative_findings": "Quantitative results show GPT-4's vanilla evaluator aligns moderately with human majority (VANILLA accuracy 52.7%, kappa 0.24) but exhibits a large Conflict Rate (e.g., 46.3% for Vicuna-13B vs ChatGPT case). GPT-4 tends to prefer the first-position response. Conflict rate is negatively correlated with the score gap: swaps produce more conflicts when candidate scores are close (score gap &lt;=1). Applying calibration (EC/MEC/BPC) improves alignment progressively (MEC k=3 -&gt; accuracy 58.7% kappa 0.30; MEC k=3 + BPC -&gt; 62.5% kappa 0.37), and adding human-in-the-loop (HITLC, top-20% by BPDE) yields alignment comparable to humans (73.8% accuracy, kappa 0.56). The paper highlights failure modes (positional bias, self-conflict) and shows calibration strategies reduce these issues.",
            "advantages_of_llm_judge": "Lower monetary cost and faster throughput than full human annotation (example: GPT-4 VANILLA cost reported $2.00 vs human annotator cost $30 per annotator for the 80-example benchmark); scalable and reproducible evaluation pipeline; provides explanations which can be leveraged in MEC.",
            "experimental_setting": "Pairwise evaluation template (Table 1) used first (VANILLA): prompt asks to rate helpfulness/relevance/accuracy/detail and provide explanation; Evidence Calibration (EC, Table 3) forces the model to generate evaluation evidence before scoring; Multiple Evidence Calibration (MEC) samples k evidences (k=3 default, also tested k=6) with sampling temperature t (0 for deterministic methods, t=1 for MEC sampling); Balanced Position Calibration (BPC) evaluates both orderings and averages 2k scores; Human-in-the-Loop Calibration (HITLC) computes Balanced Position Diversity Entropy (BPDE) across 2k ERs to select top-β examples (β=20% in experiments) for human annotation; dataset size N=80; metrics: accuracy and kappa against majority human vote; conflict rate defined as fraction of examples where ER_r12 != ER_r21.",
            "uuid": "e8056.0",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (judge)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301)",
            "brief_description": "ChatGPT used as an automatic evaluator for pairwise assistant comparisons; shows a strong positional bias (preference for second-position response), very high conflict rates in vanilla prompting, and lower alignment with human judgments compared to GPT-4 absent calibration.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Large Language Models are not Fair Evaluators",
            "evaluation_task": "Pairwise comparison of conversational assistant responses (helpfulness, relevance, accuracy, level of detail)",
            "dataset_name": "Vicuna Benchmark (80 questions across 9 categories)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI API 'gpt-3.5-turbo-0301' used as evaluator; deterministic with temperature=0 for non-sampling methods; sampling used for MEC experiments (temperature varied in analysis).",
            "human_evaluator_type": "Three authors (AI researchers) independently annotated win/tie/lose per example; final label = majority vote; each annotation ~3 minutes.",
            "agreement_metric": "accuracy; kappa (kappa correlation coefficient)",
            "agreement_score": 0.444,
            "reported_loss_aspects": "very strong positional bias favoring second-displayed response; extremely high conflict rates when swapping order (e.g., up to 82.5% in some pairings); low baseline agreement with human majority; sensitivity to prompt template (scoring vs comparing); instability when score gaps are small; lower performance on complex tasks relative to GPT-4 without calibration.",
            "qualitative_findings": "ChatGPT's VANILLA alignment with human majority is poor (VANILLA accuracy 44.4%, kappa 0.06) and conflict rates are very high (e.g., 82.5% conflict in Vicuna-13B vs ChatGPT case). Calibration improves results substantially: EC (k=1) raises accuracy to 52.6% kappa 0.23; MEC (k=3) to 53.2% kappa 0.24; MEC k=3 + BPC k=3 to 58.7% kappa 0.31. Adding HITLC with β=20% human intervention achieves ~71.3% accuracy and kappa 0.52, approaching human-average alignment. The paper reports that ChatGPT prefers the second response in the vanilla template and is more sensitive to order than GPT-4.",
            "advantages_of_llm_judge": "Low monetary cost per evaluation (reported $0.10 for VANILLA ChatGPT runs on the 80-example set) and high speed; calibration methods (MEC, BPC) can substantially improve alignment with humans while retaining lower cost than full human annotation.",
            "experimental_setting": "Same evaluation templates as GPT-4: VANILLA scoring template (conclusion then explanation), EC (generate evidence first), MEC sampling k evidences (k tested 1,3,5,7), temperature tuning experiments (found t around 0.6–1.0 best for MEC), BPC swaps positions and averages 2k scores, HITLC selects top-β examples by BPDE (β=20% used) for human annotation; comparisons performed over 100 randomized runs when not using BPC to stabilize results; metrics reported vs majority human vote: accuracy and kappa; conflict rate reported for vanilla templates.",
            "uuid": "e8056.1",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "rating": 2
        },
        {
            "paper_title": "Instruction Tuning with GPT-4",
            "rating": 1
        },
        {
            "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "rating": 1
        },
        {
            "paper_title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "rating": 1
        }
    ],
    "cost": 0.0101365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are not Fair Evaluators</h1>
<p>Peiyi Wang ${ }^{1}$ Lei Li ${ }^{1}$ Liang Chen ${ }^{1}$ Zefan Cai ${ }^{1}$ Dawei Zhu ${ }^{1}$<br>Binghuai Lin ${ }^{3}$ Yunbo Cao ${ }^{3}$ Qi Liu ${ }^{2}$ Tianyu Liu ${ }^{3}$ Zhifang Sui ${ }^{1}$<br>${ }^{1}$ National Key Laboratory for Multimedia Information Processing, Peking University<br>${ }^{2}$ The University of Hong Kong ${ }^{3}$ Tencent Cloud AI<br>{wangpeiyi9979, nlp.lilei, zefncai}@gmail.com<br>leo.liang.chen@outlook.com; {dwzhu, szf}@pku.edu.cn<br>liuqi@cs.hku.hk; {binghuailin, yunbocao, rogertyliu}@tencent.com</p>
<h4>Abstract</h4>
<p>In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at https://github.com/i-Eval/ FairEval to facilitate future research.</p>
<h2>1 Introduction</h2>
<p>The rapid advancement of Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022) has underscored the importance of evaluating their alignment with human intent in generated responses, making it an active field of research. Traditional n-gram metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as well as more sophisticated model-based evaluations such as BERTScore (Zhang et al., 2020) and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Simply changing the order of candidate responses leads to overturned comparison results, even though we add the command "ensuring that the order in which the responses were presented does not affect your judgment" into the prompt.</p>
<p>BARTScore (Yuan, Neubig, and Liu, 2021), are insufficient for thoroughly assessing this alignment (He et al., 2023). While human evaluation provides the most accurate measure of model performance and valuable insights, it can often be costly and time-consuming. As a result, there is a growing demand for automated assessment methods that can consistently align with human judgments while being more efficient and cost-effective.</p>
<p>ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) have recently demonstrated remarkable performance across various tasks, leading to their widespread use as both the annotators (Peng et al., 2023; Xu et al., 2023) and evaluators (Zheng et al., 2023; Peng et al., 2023; Sun et al., 2023; Zhou et al., 2023; Gao et al., 2023; Wang et al., 2023b; Dubois et al., 2023; Wang et al., 2023a). For example, The evaluation pipeline of Vicuna (Zheng et al., 2023) has gained significant interest and wide usage due to its simplicity and interpretability. It prompts GPT-4 to score and compare candidate responses and provide explanations, making it a valuable tool for evaluation. However, it is unclear how reliable LLMs are as evaluators, as they are known to be sensitive to textual instructions and inputs (Dong et al., 2022; Turpin et al., 2023;</p>
<p>Bowman, 2023). This raises questions about the resilience of this paradigm against perturbations, such as the ordering of candidates during scoring, potentially becoming the Achilles' Heel that can be easily hacked for unreliable evaluations.</p>
<p>In this paper, we take a sober look at the LLMs-as-evaluator paradigm and uncover a significant positional bias. Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in ChatGPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.</p>
<p>To address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-in-the-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC.</p>
<p>To assess the efficacy of our methods, we manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark (Zheng et al., 2023), encompassing 80 questions spanning 9 distinct question categories. Our MEC and BPC enhance the evaluation alignment of GPT-4 and ChatGPT by 9.8% and 14.3% accuracy, respectively. Moreover, based on MEC and BPC, our HITLC can further effectively integrate human assistance into the evaluation process. Specifically, with only a 20% human annotation cost, GPT-4 and ChatGPT can achieve comparable or even better annotation alignment with the average human performance, reducing the annotation cost by up to 39%.</p>
<p>In summary, our key contributions are: 1) We reveal that LLMs exhibit severe positional bias, com-</p>
<p>[Question]
[Q]
[The Start of Assistant 1's response]
[R1]
[The End of Assistant 1's response]
[The Start of Assistant 2's response]
[R2]
[The End of Assistant 2's response]
[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.
Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively.
The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.</p>
<p>Table 1: The evaluation template with three slots (⟨Q⟩, [R1] and [R2]) from Zheng et al. (2023). Even though the template emphasizes not letting the order affect the results (red text), large language models still have a large positional bias.</p>
<p>promising their fairness as evaluators; 2) We develop a calibration framework with three simple yet effective strategies to calibrate the positional bias of LLMs; 3) We manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark and demonstrate the effectiveness of our proposed approach through experimental results, which show closer alignment with human judgments.</p>
<h2>2 Positional Bias of the LLM Evaluator</h2>
<h3>2.1 LLMs as Evaluators</h3>
<p>Recently, researchers have been utilizing LLMs such as GPT-4 as evaluators to compare the performance of two AI assistants. As shown in Table 1, an evaluation template with three placeholders $T(Q, R 1, R 2)$, is used to query the LLM for evaluation. For each testing question $q$, given two responses $r 1$ and $r 2$ from Assistant 1 and Assistant 2, respectively, the researchers populate these responses into the corresponding slots of the evaluation template to form a prompt: $T(Q=q, R 1=$ $r 1, R 2=r 2)$. The prompt is then used to query the LLM in order to obtain the comparison result. In this paper, we found that LLM suffers from severe</p>
<table>
<thead>
<tr>
<th>Evaluators</th>
<th>Vicuna-13B v.s. Other Models</th>
<th>Vicuna-13B Win Rate</th>
<th></th>
<th>Conflict Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>as Assistant1</td>
<td>as Assistant2</td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>Vicuna-13B v.s. ChatGPT</td>
<td>51.3%</td>
<td>23.8%</td>
<td>37 / 80 (46.3%)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>Vicuna-13B v.s. Alpaca-13B</td>
<td>92.5%</td>
<td>92.5%</td>
<td>4 / 80 (5.0%)</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>Vicuna-13B v.s. ChatGPT</td>
<td>2.5%</td>
<td>82.5%</td>
<td>66 / 80 (82.5%)</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>Vicuna-13B v.s. Alpaca-13B</td>
<td>37.5%</td>
<td>90%</td>
<td>42 / 80 (52.5%)</td>
</tr>
</tbody>
</table>
<p>Table 2: The Win Rate of Vicuna-13B significantly fluctuates when positioned as Assistant 1 and Assistant 2, with GPT-4 and ChatGPT serving as evaluators. Conflict Rate refers to the proportion of conflicting results given by the same evaluator when simply changing the position of two models.
positional bias, i.e., by swapping the slots of the two responses and querying LLM twice, the evaluator will most likely produce conflicting evaluation results, and the evaluator prefers the response at a certain position.</p>
<h3>2.2 Revealing the Positional Bias</h3>
<p>In this section, we adopt GPT-4 and ChatGPT as evaluators to analyze the characteristics of positional bias in LLM evaluators. We find that:</p>
<p>LLMs are sensitive to the position of responses. As shown in Table 2, in the evaluation of "Vicuna13B v.s. ChatGPT" and "Vicuna-13B v.s. Alpaca13B", when the order was changed, LLMs provide different evaluation results, e.g., the win rate of Vicuna-13B extremely differs when Vicuna-13B is evaluated as Assistant 1 and Assistant 2.</p>
<p>To empirically evaluate the sensitivity, we introduced a metric Conflict Rate to measure the sensitivity of the model to response positions quantitatively. Formally, given $N$ examples $\left{\left(q_{i}, r 1_{i}, r 2_{i}\right)\right}<em i="i">{i=1}^{N}$, for each example $\left(q</em>}, r 1_{i}, r 2_{i}\right)$, we query the LLM with two prompts $T\left(q_{i}, r 1_{i}, r 2_{i}\right)$ and $T\left(q_{i}, r 2_{i}, r 1_{i}\right)$, and obtain corresponding two evaluation results $\mathbf{E R<em i="i">{i}^{r 12}$ and $\mathbf{E R}</em>$. Then we calculate the Conflict Rate of the LLM evaluator as follows:}^{r 21</p>
<p>$$
\text { Conflict Rate }=\frac{\sum_{i=1}^{N} \mathbb{I}\left(\mathbf{E R}<em i="i">{i}^{r 12} \neq \mathbf{E R}</em>
$$}^{r 21}\right)}{N</p>
<p>where $\mathbb{I}($.$) is the indicator function. We found$ that GPT-4 exhibited conflict rates of 46.3\% and 5.0\%, respectively. In contrast, ChatGPT displayed considerably higher conflict rates, with figures of $82.5 \%$ and $52.5 \%$, respectively. These findings indicate that LLMs can be self-conflicting due to the sensitivity of the response order in the template, with stronger models being less influenced by the placement of responses.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The conflict rate is negatively correlated with the score gap between the two responses. When swapping the order of two responses, the smaller the score gap between them, the more likely GPT-4 is to produce conflicting results.</p>
<p>LLMs suffer from Positional Bias, i.e., they prefer the response in the specific position. Based on the same evaluation template $T$ in Table 1, GPT4 tends to favor the response in the first position, while ChatGPT shows a preference for the response in the second position. For example, as illustrated in Table 2, in the comparison "Vicuna-13B v.s. ChatGPT", GPT-4 yields Win Rates of $51.3 \%$ and $23.8 \%$ for Vicuna-13B when it is positioned as Assistant 1 and Assistant 2, respectively. Conversely, ChatGPT indicates Win Rates of only $2.5 \%$ and up to $82.5 \%$ for Vicuna-13B when it is positioned as Assistant 1 and Assistant 2, respectively.</p>
<p>The degree of positional bias varies based on the difference in response quality. We notice that the conflict rate of "Vicuna-13B v.s. Alpaca13B" is much lower than that of "Vicuna-13B v.s. ChatGPT", suggesting that positional bias may not have the same impact on the assessment of different responses. One potential reason is that there is a significant difference in the quality of responses between Alpaca models and Vicuna models, and positional bias is not strong enough to change the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Demonstration of our calibration framework with three calibration methods. $S_{r}$ and $S_{r}^{\prime}$ denotes scores of the response $r$ in the first and second positions, respectively. BPDE is short for Balanced Position Diversity Entropy score, which is calculated based on the evaluation results (ER) of MEC and BPC.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="k">[Q]</span>
<span class="k">[The Start of Assistant 1&#39;s response]</span>
<span class="k">[R1]</span>
<span class="k">[The End of Assistant 1&#39;s response]</span>
<span class="k">[The Start of Assistant 2&#39;s response]</span>
<span class="k">[R2]</span>
<span class="k">[The End of Assistant 2&#39;s response]</span>
<span class="k">[System]</span>
<span class="na">We would like to request your feedback on the per-</span>
<span class="na">formance of two AI assistants in response to the user</span>
<span class="na">question displayed above.</span>
<span class="na">Please rate the helpfulness, relevance, accuracy, and</span>
<span class="na">level of detail of their responses. Each assistant re-</span>
<span class="na">ceives an overall score on a scale of 1 to 10, where a</span>
<span class="na">higher score indicates better overall performance.</span>
<span class="na">Please first provide a comprehensive explanation of</span>
<span class="na">your evaluation, avoiding any potential bias and en-</span>
<span class="na">suring that the order in which the responses were</span>
<span class="na">presented does not affect your judgment. Then, out-</span>
<span class="na">put two lines indicating the scores for Assistant 1 and</span>
<span class="na">2, respectively.</span>
<span class="na">Output with the following format</span><span class="o">:</span>
<span class="na">Evaluation evidence</span><span class="o">:</span><span class="w"> </span><span class="s">&lt;evaluation explanation here&gt;</span>
<span class="na">The score of Assistant 1</span><span class="o">:</span><span class="w"> </span><span class="s">&lt;score&gt;</span>
<span class="na">The score of Assistant 2</span><span class="o">:</span><span class="w"> </span><span class="s">&lt;score&gt;</span>
</code></pre></div>

<p>Table 3: The evidence calibration evaluation template that prompts LLMs to generate the evaluation evidence first (red text), and then evaluate two responses.</p>
<p>judgment in such a situation. To further investigate this issue, we grouped all the examples based on the score difference between the two responses. As shown in Figure 2, we found that when the score difference between the two responses is small (e.g., score gap $\leq 1$ ), the evaluation results of GPT-4 are significantly affected by the position of the responses. On the other hand, when the score difference between the two responses is large (e.g., score gap $\geq 3$ ), GPT-4's evaluation results are relatively stable.</p>
<h2>3 Calibrating the Positional Bias</h2>
<p>We have identified that positional bias can significantly impact the evaluation results of LLMs, making them unfair evaluators. In this section, we propose a calibration framework with three simple yet effective strategies to alleviate this bias to achieve a more reliable and fair evaluation result.</p>
<h3>3.1 Multiple Evidence Calibration</h3>
<p>Previous studies (Zheng et al., 2023; Wang et al., 2023b) utilize the evaluation template that draws the conclusion first and then makes an explanation, e.g., the template used in Table 1. However, due to the nature of the auto-regressive model, the conclusions generated by the model are not supported by the explanation generated afterward. To this end, as shown in Table 3, we design an evidence calibration (EC) evaluation template $T_{E C}(Q, R 1, R 2)$ that requires the model to generate the explanation (evaluation evidence) first and then give the score. In this way, the score can be calibrated with the evaluation evidence. To further improve the reliability of the evaluation, rather than generating only a single EC score for each response, we perform a multiple evidence calibration (MEC, Figure 3(a)) that samples $k$ EC scores $\left{S_{r 1}^{1}, \ldots, S_{r 1}^{k}\right}$ and $\left{S_{r 2}^{1}, \ldots, S_{r 2}^{k}\right}$ for responses $r 1$ and $r 2$, where $S_{r}$ and $S_{r}^{\prime}$ denotes scores of the response $r$ at the first and second positions, respectively.</p>
<p>3.2 Balanced Position Calibration</p>
<p>We further employ a balanced position calibration (BPC) strategy to alleviate the previously identified positional bias of LLMs. As shown in Figure 3(b), for each example $(q, r 1, r 2)$, BPC additionally creates a query prompt $T_{E C}(q, r 2, r 1)$ by swapping the position of two responses in the original query prompt $T_{E C}(q, r 1, r 2)$. Combined with MEC, we can achieve $2 k$ scores $\left{S_{r 1}^{1}, \ldots, S_{r 1}^{k}, \ldots, S_{r 1}^{r 1}, \ldots, S_{r 1}^{r k}\right}$ and $\left{S_{r 2}^{1}, \ldots, S_{r 2}^{r k}, \ldots, S_{r 2}^{1}, \ldots, S_{r 2}^{k}\right}$ for $r 1$ and $r 2$, respectively. The final calibrated scores of two responses ( $C S_{r 1}$ and $C S_{r 2}$ ) are the average of the $2 k$ scores:</p>
<p>$$
C S_{R}=\sum_{i=1}^{k} \frac{S_{R}^{i}+S_{R}^{i}}{2 k}, R=r 1, r 2
$$</p>
<p>and we regard the response with the higher average score as the better response.</p>
<h3>3.3 Human-in-the-Loop Calibration</h3>
<p>In addition to the automatic calibration strategies, another interesting question we want to explore is whether Human-In-The-Loop Calibration (HITLC) which performs the cooperation of humans and LLMs as evaluators, could stabilize the evaluation result. The key point of human-in-the-loop calibration is when humans should be involved in the evaluation and calibrate the evaluation result on which LLM evaluators do not perform well.</p>
<p>To target the "when" problem, inspired by Cai, Chang, and Han (2023), we introduce a Balanced Position Diversity Entropy (BPDE) score to find examples requiring auxiliary human calibration based on the evaluation results of MEC and BPC. Specifically, as shown in Figure 3(c), we first compute $2 k$ evaluation results $\left{\mathbf{E R}<em i="1">{i}\right}</em>$ based on the $2 k$ pairs of scores.}^{2 k</p>
<p>$$
\underset{1 \leq i \leq k}{\mathbf{E R}<em 1="1" r="r">{i}}=\left{\begin{array}{l}
\text { win, } S</em> \
\text { tie, } S_{r 1}^{i}=S_{r 2}^{i} \
\text { lose }, S_{r 1}^{i}<S_{r 2}^{i}
\end{array}, \begin{array}{l}
\mathbf{E R}_{1}^{\prime} \\
1 \leq i \leq k
\end{array}= \begin{cases}\text { win, } S_{r 1}^{i_{1}}>S_{r 2}^{i} \
\text { tie, } S_{r 1}^{i}=S_{r 2}^{i} \
\text { lose, } S_{r 1}^{i}&lt;S_{r 2}^{i}\end{cases}
\end{array}\right.
$$}^{i}&gt;S_{r 2}^{i</p>
<p>and BPDE is defined as the entropy of the evaluation results:</p>
<p>$$
\begin{gathered}
\operatorname{BPDE}=\sum_{\mathbf{e r} \in{\text { win,tie,lose }}}-\mathbf{p}<em _mathbf_e="\mathbf{e" r="r">{\mathbf{e r}} \log \mathbf{p}</em> \
\mathbf{p}}<em i="1">{\mathbf{e r}}=\frac{\sum</em>}^{k} \mathbb{I}\left(\mathbf{E R<em i="i">{i}=\mathbf{e r}\right)+\mathbb{I}\left(\mathbf{E R}</em>
\end{gathered}
$$}^{\prime}=\mathbf{e r}\right)}{2 k</p>
<p>A higher BPDE score indicates that it is more likely the evaluation requires manual correction. A threshold is needed for BPDE as the hyper-parameter to select the top- $\beta$ most likely biased evaluations. After selection based on the BPDE score, the annotators will evaluate the selected examples and integrate the human annotations based on the majority opinion as described in Section 4.1.</p>
<h2>4 Experiments</h2>
<h3>4.1 Human Annotation</h3>
<p>To assess the effectiveness of our proposed strategies, three of the authors manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B independently in all 80 Vicuna Benchmark questions. All of the annotators are researchers familiar with Artificial Intelligence and are well-equipped to assess the quality of the responses. Following the same template as the original Vicuna, the annotators are instructed to assess the responses provided by Vicuna-13B and ChatGPT from four different perspectives: helpfulness, relevance, accuracy, and level of detail. The responses of Vicuna and ChatGPT are presented to the annotators in random order. The evaluation process for each example took an average of three minutes. The final result is based on the majority opinion among three annotators.</p>
<h3>4.2 Experimental Setup and Metric</h3>
<p>We use the OpenAI API to conduct our experiments ("gpt-3.5-turbo-0301" for ChatGPT, and "gpt-4" for GPT-4). For the methods that do not need to sample multiple generation results, we set the generated temperature to 0 for deterministic generation results. For the multiple evidence strategy, we set the temperature to 1 and sample three generation results $(k=3)$. We use the accuracy and kappa correlation coefficient (McHugh, 2012) with the final majority of human annotation results to measure the performance of different evaluators and evaluation methods. When calculating the results for methods that do not utilize BPC, we randomize the order of the two responses from the assistants and calculate the average results of 100 runs to ensure stable results.</p>
<h3>4.3 Main Results</h3>
<p>Table 4 illustrates the performance of different methods on our manually annotated 80 annotated examples. As is shown: 1) There is a good correla-</p>
<table>
<thead>
<tr>
<th>Evaluators</th>
<th>Methods</th>
<th>Accuracy</th>
<th>Kappa</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human 1</td>
<td>-</td>
<td>68.8%</td>
<td>0.50</td>
<td>$30.0</td>
</tr>
<tr>
<td>Human 2</td>
<td>-</td>
<td>76.3%</td>
<td>0.62</td>
<td>$30.0</td>
</tr>
<tr>
<td>Human 3</td>
<td>-</td>
<td>70.0%</td>
<td>0.50</td>
<td>$30.0</td>
</tr>
<tr>
<td>Human Average</td>
<td>-</td>
<td>71.7%</td>
<td>0.54</td>
<td>$30.0</td>
</tr>
<tr>
<td>GPT-4</td>
<td>VANILLA</td>
<td>52.7%</td>
<td>0.24</td>
<td>$2.00</td>
</tr>
<tr>
<td>GPT-4</td>
<td>EC $(k=1)$</td>
<td>56.5%</td>
<td>0.29</td>
<td>$2.00</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=3)$</td>
<td>58.7%</td>
<td>0.30</td>
<td>$3.19</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=6)$</td>
<td>60.9%</td>
<td>0.33</td>
<td>$6.38</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)$</td>
<td>62.5%</td>
<td>0.37</td>
<td>$6.38</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)+\operatorname{HITLC}(\beta=20\%)$</td>
<td>73.8%</td>
<td>0.56</td>
<td>$23.1</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>VANILLA</td>
<td>44.4%</td>
<td>0.06</td>
<td>$0.10</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>EC $(k=1)$</td>
<td>52.6%</td>
<td>0.23</td>
<td>$0.10</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=3)$</td>
<td>53.2%</td>
<td>0.24</td>
<td>$0.17</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=6)$</td>
<td>55.6%</td>
<td>0.27</td>
<td>$0.34</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)$</td>
<td>58.7%</td>
<td>0.31</td>
<td>$0.34</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)+\operatorname{HITLC}(\beta=20\%)$</td>
<td>71.3%</td>
<td>0.52</td>
<td>$18.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy and kappa correlation coefficient of different methods and annotators with the final voting human annotations. The VANILLA evaluation method was commonly used in previous works, which provided the conclusion first and then followed with the explanation. (M)EC, BPC, and HITLC denote our proposed (multiple) evidence calibration, balanced position calibration, and human-in-the-loop calibration respectively. $\beta \%$ means selecting the top- $\beta$ most likely biased examples for human annotation.</p>
<p>tion coefficient between the annotations provided by each human annotator and the final voting results. In detail, the average accuracy and the kappa correlation coefficient of human annotations are 71.7% and 0.54, respectively; 2) Overall, GPT-4 achieves higher alignment with human judgments compared with ChatGPT, showing its powerful alignment ability with humans; 3) Compared to the commonly used VANILLA evaluation method, our proposed automatic calibration strategies (i.e., EC, MEC and BPC) significantly enhance the alignment between GPT-4 and ChatGPT with human judgments; For instance, by employing the MEC and BPC calibration strategies, ChatGPT demonstrates a notable improvement in both accuracy and the kappa correlation coefficient. Specifically, the accuracy is improved by 14.3%, and the kappa correlation coefficient is increased from 0.06 to 0.31; 4) "MEC $(k=3)+\operatorname{BPC}(k=3)$" outperforms "MEC $(k=6)$", demonstrating that LLMs are affected by positional bias, and BPC effectively ensures that LLMs serve as fair evaluators; 5) Our proposed HITLC can effectively enhance the alignment between GPT-4 and ChatGPT with human judgments, requiring only a small amount of human labor. For example, by incorporating just 20% $(\beta=20\%)$ human assistance, ChatGPT attains comparable Human Average accuracy, while reducing the annotation cost from $30 to $18.3, a 39% reduction.</p>
<p>In conclusion, our proposed calibration methods are simple yet very effective in improving the evaluation performance with LLM as evaluators, while maintaining low costs.</p>
<h2>5 Analysis</h2>
<h3>5.1 Ablation on Evidence Number $k$ and Temperature $t$</h3>
<p>In the MEC and BPC strategy, we sample $k$ evaluation results for each query prompt and ensemble them to enhance the evaluation process. We conduct an analysis to examine the influence of the number of evidence $k$, on the model’s evaluation performance. As illustrated in Figure 4(a), we compared the performance of ChatGPT with different values of $k$, namely 1, 3, 5, and 7. The</p>
<p>The minimum hourly wage in the United States is near $7.5, which can be found at https://www.worker.gov/. On average, annotating an example takes 3 minutes, and the Vicuna evaluation benchmark comprises 80 examples in total. Consequently, the cost per annotator amounts to $30.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Variation of accuracy and kappa coefficient with a different number of evidence <em>k</em> and sampling temperature <em>t</em> when ChatGPT is used as the evaluator.</p>
<p>model's performance increases and then tends to be constant or decreases slightly as <em>k</em> becomes larger. Despite the slight decrease, the enhancement of the model effect by the MCE strategy is still significant, illustrating the stability of the MEC strategy. Consequently, we found that a <em>k</em> value of 3 yields an optimal performance. With this value, the model achieves a notable level of performance while keeping the API cost relatively low.</p>
<p>We further investigate the impact of sampling temperature <em>t</em> on evaluation performance. Figure 4(b) illustrates that both low temperature (i.e., 0.2) and high temperature (i.e., 1.4) result in suboptimal evaluation alignment. We believe that low temperature eliminates the randomness of sampling, weakening the effect of MEC, while high temperature compromises the quality of generation results, leading to poor performance. Hence, it is crucial to select an appropriate temperature (e.g., 0.6 or 1.0 in our experiments) for the LLM evaluators.</p>
<h3>5.2 Effectiveness of the BPDE</h3>
<p>Our HITLC strategy utilizes BPDE score to select examples for human annotations. In order to analyze the efficiency of BPDE score, we compare BPDE with two typical baselines, <em>Random</em> and <em>Vanilla Diversity Entropy</em>, where Random denotes randomly select examples for human annotations, and Vanilla Diversity Entropy is calculated by using only the evaluation results of one position without swapping the position of two responses. To ensure fairness, the total number of evaluation results is 6 for both BPDE and Vanilla Diversity Entropy. As shown in Figure 5: <strong>1)</strong> Two Diversity Entropy methods outperform Random, showing the effectiveness of selecting examples based on the diversity entropy; <strong>2)</strong> BPDE outperforms Vanilla DE, which shows LLMs are sensitive to position exchange, and the results of BPC can significantly improve</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The accuracy of various methods changes with different human assistant thresholds (Top-β) when ChatGPT is used as the evaluator.</p>
<table>
<thead>
<tr>
<th>Templates</th>
<th>Methods</th>
<th>Acc.</th>
<th>Kap.</th>
<th>C.R</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scoring</td>
<td>Vanilla</td>
<td>44.4%</td>
<td>0.06</td>
<td>82.5%</td>
</tr>
<tr>
<td>Scoring</td>
<td>MEC</td>
<td>53.2%</td>
<td>0.24</td>
<td>35.0%</td>
</tr>
<tr>
<td>Scoring</td>
<td>MEC + BPC</td>
<td>58.7%</td>
<td>0.31</td>
<td>N/A</td>
</tr>
<tr>
<td>Comparing</td>
<td>Vanilla</td>
<td>50.2%</td>
<td>0.18</td>
<td>50.0%</td>
</tr>
<tr>
<td>Comparing</td>
<td>MEC</td>
<td>54.8%</td>
<td>0.27</td>
<td>42.5%</td>
</tr>
<tr>
<td>Comparing</td>
<td>MEC + BPC</td>
<td>60.3%</td>
<td>0.35</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>Table 5: Effectiveness of our proposed two automatic calibrated methods on two different evaluation templates with ChatGPT as the evaluator. Acc., Kap. and C.R are short for Accuracy, Kappa correlation coefficient, and Conflict Rate, respectively. N/A means the Conflict Rate is not valid for BPC methods.</p>
<p>the performance of HITLC compared to relying solely on the results of MEC.</p>
<h3>5.3 Generalization on the Pairwise Comparison Evaluation Template</h3>
<p>To provide a more comprehensive validation of our proposed calibration methods, in addition to the previous Scoring evaluation template that rates each response, we extend our analysis to incorporate the Comparing evaluation template. This template facilitates a direct comparison between two responses, eschewing explicit scores in its assessment. Specifically, we prompt LLMs to produce results labeled as "Assistant 1", "Assistant 2", or "Same", indicating whether the response from Assistant 1 is better, worse, or equal to that of Assistant 2. As is shown in Table 5: <strong>1)</strong> Our proposed methods are applicable to both of these templates, leading to enhanced accuracy and a heightened correlation coefficient for ChatGPT; <strong>2)</strong> The significant</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Fine-grained analysis of evaluation quality. Our MEC and BPC improve the evaluation performance of ChatGPT and GPT-4 in nearly all categories. Especially on the complex task categories such as common sense, coding, and math for ChatGPT.</p>
<p>performance gap (nearly 6% accuracy) between the VANILLA method of two templates, coupled with the high conflict rate, highlights the sensitivity and unreliability of LLMs. However, our methods effectively narrow this performance gap and reduce conflict, showcasing how calibration enhances LLM robustness.</p>
<h3>5.4 Fine-Grained Analysis of Evaluation Quality</h3>
<p>In order to further analyze the evaluation capabilities of the model, we perform a fine-grained analysis of the questions by dividing them into 9 categories following <em>Zheng et al. (2023)</em>. We calculate the performance of different evaluators within these categories. As shown in Figure 6, we find that: 1) In certain complex tasks such as common-sense, coding and math, GPT-4 performs significantly better than ChatGPT, highlighting the strength of GPT-4 as a more fair evaluator in these scenarios; 2) Our proposed MEC+BPC strategy demonstrates noticeable improvement in evaluating ChatGPT's performance on complex tasks, allowing us to obtain satisfactory evaluation results with a low API cost.</p>
<h2>6 Related Work</h2>
<h3>6.1 Evaluation of Large Language Models</h3>
<p>LLMs have demonstrated powerful general generation capabilities, becoming universal assistants (OpenAI, 2022, 2023; Song et al., 2023b). With the rapid advancement of LLMs, it becomes crucial to evaluate their ability to follow human instructions. Traditional evaluation methods assess the ability by calculating a metric, like BLEU, ROUGE, BERTScore, or BARTScore, to compare the generated response with a reference response. However, these metrics do not adequately measure the alignment of the generated response with human intent (He et al., 2023). While human evaluation is treated as the most accurate measurement of model performance, it is costly and time-consuming to operate at scales. Considering the potent capabilities of LLMs, researchers have started utilizing LLMs to evaluate the proficiency of generative models in adhering to human instructions (Zheng et al., 2023; Lu et al., 2023; Li et al., 2023). In these works, Vicuna's evaluation paradigm (Zheng et al., 2023) is widely adopted, where it provides a question and two responses from two models, and uses GPT-4 to determine which response has better quality.</p>
<h3>6.2 Bias of Deep Neural Networks</h3>
<p>Deep Neural Networks have been proven to easily learn biases from the data, which significantly impacts their reliability. Specifically, bias has also been investigated in natural language inference (Gururangan et al., 2018; McCoy, Pavlick, and Linzen, 2019; Belinkov et al., 2019; Liu et al., 2020a,b), question answering (Min et al., 2019), ROC story cloze (Cai, Tu, and Gimpel, 2017; Schwartz et al., 2017), lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), information extraction (Wang et al., 2021, 2022; Song et al., 2023a; Xia et al., 2023) and so on. LLMs are pre-trained using a vast amount of data from the internet, making it highly likely for them to learn biases present in those materials. Although the LLMs are already widely adopted as a proxy of human evaluators, the reliability of this paradigm</p>
<p>is not well explored. In this paper, we critically examine the LLMs-as-evaluator paradigm and uncover a significant positional bias. Furthermore, we propose three simple yet effective methods to calibrate the positional bias to achieve reliable and fair evaluation results.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we reveal a systematic positional bias in evaluation with advanced ChatGPT/GPT-4 models: by manipulating the order of candidate responses during evaluation, the quality ranking results can be significantly influenced. To this end, we introduce three effective strategies, namely Multiple Evidence Calibration (MEC), Balanced Position Calibration (BPC), and Human-in-the-Loop Calibration (HITLC). MEC requires the LLM evaluator to first provide multiple evaluation evidence to support their subsequent ratings and BPC aggregates the results from various orders to determine the final score. Based on the results of MEC and BPC, HITLC further calculates a balanced position diversity entropy to select examples for human annotations. These strategies successfully reduce the evaluation bias and improve alignment with human judgments. We provide our code and human annotations to support future studies and enhance the evaluation of generative models.</p>
<h2>References</h2>
<p>Belinkov, Y.; Poliak, A.; Shieber, S.; Van Durme, B.; and Rush, A. 2019. Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Bowman, S. R. 2023. Eight things to know about large language models. arXiv preprint arXiv:2304.00612.</p>
<p>Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Cai, Z.; Chang, B.; and Han, W. 2023. Human-in-the-Loop through Chain-of-Thought. arXiv preprint arXiv:2306.07932.</p>
<p>Cai, Z.; Tu, L.; and Gimpel, K. 2017. Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N. M.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B. C.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; García, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Díaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K. S.; Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. PaLM: Scaling Language Modeling with Pathways. ArXiv, abs/2204.02311.</p>
<p>Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; and Sui, Z. 2022. A Survey for Incontext Learning. arXiv preprint arXiv:2301.00234.</p>
<p>Dubois, Y.; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.</p>
<p>Gao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.; Zhang, W.; Lu, P.; He, C.; Yue, X.; Li, H.; and Qiao, Y. J. 2023. LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. ArXiv, abs/2304.15010.</p>
<p>Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; and Parikh, D. 2017. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; Bowman, S.; and Smith, N. A. 2018. Annotation Artifacts in Natural Language Inference Data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>He, T.; Zhang, J.; Wang, T.; Kumar, S.; Cho, K.; Glass, J.; and Tsvetkov, Y. 2023. On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 12067-12097. Toronto, Canada: Association for Computational Linguistics.</p>
<p>Levy, O.; Remus, S.; Biemann, C.; and Dagan, I. 2015. Do Supervised Distributional Methods Really Learn Lexical Inference Relations? In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Li, L.; Yin, Y.; Li, S.; Chen, L.; Wang, P.; Ren, S.; Li, M.; Yang, Y.; Xu, J.; Sun, X.; et al. 2023. M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. arXiv preprint arXiv:2306.04387.</p>
<p>Lin, C.-Y. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, 74-81. Barcelona, Spain: Association for Computational Linguistics.</p>
<p>Liu, T.; Xin, Z.; Chang, B.; and Sui, Z. 2020a. HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference. In Proceedings of the 12th Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association. ISBN 979-10-95546-34-4.</p>
<p>Liu, T.; Xin, Z.; Ding, X.; Chang, B.; and Sui, Z. 2020b. An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference. In Proceedings of the 24th Conference on Computational Natural Language Learning. Online: Association for Computational Linguistics.</p>
<p>Lu, Q.; Qiu, B.; Ding, L.; Xie, L.; and Tao, D. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. arXiv preprint arXiv:2303.13809.</p>
<p>McCoy, T.; Pavlick, E.; and Linzen, T. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>McHugh, M. L. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3): 276-282.</p>
<p>Min, S.; Wallace, E.; Singh, S.; Gardner, M.; Hajishirzi, H.; and Zettlemoyer, L. 2019. Compositional Questions Do Not Necessitate Multi-hop Reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>OpenAI. 2022. Introducing ChatGPT.
OpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774.</p>
<p>Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311-318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics.</p>
<p>Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. Instruction Tuning with GPT-4. ArXiv, abs/2304.03277.</p>
<p>Schwartz, R.; Sap, M.; Konstas, I.; Zilles, L.; Choi, Y.; and Smith, N. A. 2017. The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL).</p>
<p>Song, Y.; Wang, P.; Zhu, D.; Liu, T.; Sui, Z.; and Li, S. 2023a. RepCL: Exploring Effective Representation for Continual Text Classification. arXiv preprint arXiv:2305.07289.</p>
<p>Song, Y.; Xiong, W.; Zhu, D.; Li, C.; Wang, K.; Tian, Y.; and Li, S. 2023b. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs. arXiv preprint arXiv:2306.06624.</p>
<p>Sun, Z.; Shen, Y.; Zhou, Q.; Zhang, H.; Chen, Z.; Cox, D. D.; Yang, Y.; and Gan, C. 2023. PrincipleDriven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. ArXiv, abs/2305.03047.</p>
<p>Turpin, M.; Michael, J.; Perez, E.; and Bowman, S. R. 2023. Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-ofThought Prompting. CoRR, abs/2305.04388.</p>
<p>Wang, P.; Song, Y.; Liu, T.; Lin, B.; Cao, Y.; Li, S.; and Sui, Z. 2022. Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 6264-6278. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.</p>
<p>Wang, P.; Xun, R.; Liu, T.; Dai, D.; Chang, B.; and Sui, Z. 2021. Behind the Scenes: An Exploration of Trigger Biases Problem in Few-Shot Event Classification. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, 1969-1978.</p>
<p>Wang, Y.; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu, K. R.; Wadden, D.; MacMillan, K.; Smith, N. A.; Beltagy, I.; et al. 2023a. How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. arXiv preprint arXiv:2306.04751.</p>
<p>Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; et al. 2023b. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. arXiv preprint arXiv:2306.05087.</p>
<p>Xia, H.; Wang, P.; Liu, T.; Lin, B.; Cao, Y.; and Sui, Z. 2023. Enhancing Continual Relation Extraction via Classifier Decomposition. In Findings of the Association for Computational Linguistics: ACL 2023, 10053-10062. Toronto, Canada: Association for Computational Linguistics.</p>
<p>Xu, C.; Guo, D.; Duan, N.; and McAuley, J. 2023. Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. ArXiv, abs/2304.01196.</p>
<p>Yuan, W.; Neubig, G.; and Liu, P. 2021. BARTScore: Evaluating Generated Text as Text Generation. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 27263-27277.</p>
<p>Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2020. BERTScore: Evaluating Text Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685.</p>
<p>Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.; Efrat, A.; Yu, P.; Yu, L.; Zhang, S.; Ghosh, G.; Lewis, M.; Zettlemoyer, L.; and Levy, O. 2023. LIMA: Less Is More for Alignment.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9264 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9264</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9264</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-271891859</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.08780v7.pdf" target="_blank">Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions</a></p>
                <p><strong>Paper Abstract:</strong> With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks. However, the function of descriptive instructions during ICL remains under-explored. In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL performance. But to our surprise, LLMs might not care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since it could lead to improvement even with random descriptive nouns. We further apply this new ensemble framework on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9264.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9264.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT_XGLM_Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine Translation experiments with XGLM using Ensemble prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of prompt-format interventions (Ensemble prompt variants) on XGLM-7.5B for multilingual machine translation (FLORES-101 directions), showing that ensemble-style prompts improve COMET scores relative to a vanilla prompt and that improvements persist even when example-level descriptions are semantically incorrect or random.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7.5B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (FLORES-101 directions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate short-sentence test inputs among English, German, French and Russian (FLORES-101 devtest, averaged over six translation directions). Evaluation with COMET (wmt20-comet-da).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>4-shot in-context learning. Baseline 'Vanilla' prompt: task instruction + 4 in-context examples (no example-level descriptive lines). 'Ensemble' prompt: add two example-level descriptive lines labeling halves of the examples (Noun-A / Noun-B), with variants: Ensemble (Word+Syntax), Ensemble (Syntax+Word), Ensemble (Word+Semantics), Different Ensemble (uses 'different' qualifier), Ensemble (Random+Random) where Noun-A/B are random English nouns. Example selection methods: Random, BM25 (word-level), Polynomial (syntax-level), BM25+Polynomial (concatenation), Polynomial+BM25.  COMET used as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Vanilla prompt and ablated variants (Single (one descriptive line), Single (Example) with generic 'Examples:' line, Vanilla(Translate) where 'Translate the following sentence:' is added before test input).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Vanilla baseline COMET (averaged over six directions) reported in paper: Random: XGLM 54.07; BM25: 55.00; Polynomial: 55.52; BM25+Polynomial: 56.17; Polynomial+BM25: 56.18. Ensemble prompt variants produced consistent improvements over these Vanilla baselines (improvement magnitudes shown in paper figures; exact per-variant COMET numbers not tabulated in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports that Ensemble variants (including Ensemble (Random+Random)) improved over Vanilla in most cases; in some cases 'incorrect' or reversed descriptive labels improved as much or more than semantically 'correct' labels (e.g., Ensemble (Syntax+Word) sometimes improved more than Ensemble (Word+Syntax)). Exact COMET increases versus Vanilla are presented graphically in figures but not listed as single-number deltas in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that the performance gain primarily stems from the prompt format (organization and presence of example-level descriptive lines) rather than the semantic content of those descriptions; attention analyses show LLMs pay limited or decreasing attention to descriptive nouns in deeper layers, supporting the claim that LLMs 'do not care' about the actual meaning of those nouns. They conjecture models have seen many patterns like the Ensemble format during pretraining, making the format a useful cue.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>4-shot setting for MT (four in-context examples). Example databases: Europarl (German, French) and ParaCrawl (Russian). Evaluation metric: COMET (wmt20-comet-da). Models: XGLM-7.5B. Results averaged over six translation directions. Various Ensemble prompt variants tested including random nouns; selection methods include BM25 and Polynomial; in combining BM25+Polynomial the first two examples are from one method and remaining two from the other.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9264.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9264.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT_Alpaca_Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine Translation experiments with Alpaca using Ensemble prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same MT experiment family as above but using Alpaca (7B), finding Ensemble formats generally improve COMET over Vanilla baselines and that random/incorrect example-level descriptions still yield gains; Alpaca shows some instruction-tuned idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (FLORES-101 directions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate short-sentence test inputs among English, German, French and Russian; measured with COMET.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>4-shot ICL. Vanilla vs Ensemble variants with example-level descriptive lines (Word+Syntax, Syntax+Word, Word+Semantics, Different, Random+Random).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Vanilla prompt and ablated variants (Single, Single(Example), Vanilla(Translate)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Vanilla baseline COMET (averaged): Random 55.42; BM25 56.27; Polynomial 56.13; BM25+Polynomial 56.18; Polynomial+BM25 55.49. Ensemble prompts generally improved over these baselines across selections; Ensemble (Random+Random) often produced gains and in one setting (Polynomial+BM25 examples) delivered the largest gain among prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports Ensemble improvements across most selection methods; however Alpaca exhibited some anomalous patterns (e.g., in certain Polynomial/BM25 configurations Ensemble(Random+Random) did not outperform other prompts). Exact numeric deltas are shown graphically but not fully enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest Alpaca's instruction-tuned nature may make it more robust to different prompt phrasings, leading to smaller differences between certain prompt variants; overall effect attributed to format cues rather than semantics of example-level descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>4-shot, COMET metric, Alpaca-7B model. Attention analysis (in appendix) similarly shows limited reliance on descriptive nouns in deeper layers. Ablation study shows removing descriptive lines or random nouns hurts gains in most cases, indicating the Ensemble organization (two labeled groups) is an important part of the format effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9264.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9264.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT_Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study on prompt organization for MT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation experiments dissecting which elements of the Ensemble format drive gains: comparing full Ensemble to Single (one descriptive line), Single (Example) generic line, and Vanilla(Translate); finds Ensemble outperforms ablated formats in most MT cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XGLM / Alpaca (both evaluated in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XGLM 7.5B / Alpaca 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (FLORES-101 directions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation with 4-shot ICL; ablations test prompt organization differences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Variants: Ensemble (two example-level descriptions labeling two halves), Single (only one descriptive line above all examples, either random noun or generic 'Examples:'), Vanilla(Translate) (no example-level descriptions but 'Translate the following sentence:' before test input), Vanilla (no added translate instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Full Ensemble vs Single (Random), Single (Example), Vanilla(Translate), Vanilla.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across averaged six-direction MT results, the original Ensemble format produced greater COMET improvements over Vanilla than the ablated formats in most cases; when one or both example-level descriptions or random nouns were removed the performance gain was reduced in most configurations. Specific numeric deltas are shown in paper figures (not fully tabulated in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The organization (presence of two labeled groups of examples) appears to be a key structural cue that LLMs exploit; removing one or both labels reduces benefit, consistent with the hypothesis that format/layout rather than label semantics drives the effect.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation runs averaged over six translation directions; models XGLM and Alpaca; four in-context examples; attention analyses complement ablation findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9264.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9264.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention_Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-weight analysis relating prompt-format to model focus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of averaged attention weights between prompt components (Example-A, Example-B, Target token, Noun-A, Noun-B) across layers and heads showing that attention to meaningful descriptive nouns is not consistently higher than to random nouns and that attention to nouns diminishes in deeper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XGLM (with supplementary Alpaca analysis in appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XGLM 7.5B; Alpaca 7B (appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (analysis on MT prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Layerwise attention weight measurements between prompt components to probe which parts of prompt the model attends to when generating the translation token.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compare attention patterns for Ensemble (Word+Syntax) (EWS) vs Ensemble (Random+Random) (ERR) prompts; measure attention from target position to nouns and to examples, averaged over heads and layers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>EWS (meaningful nouns) vs ERR (random nouns).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not a performance metric entry; analysis shows EWS attention to nouns is higher in shallow layers for some target-to-noun paths but in deeper layers ERR often exceeds EWS; overall EWS does not consistently outperform ERR in attention magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The attention patterns support the paper's claim that LLMs do not meaningfully use the semantic content of descriptive nouns for generation: meaningful nouns may help early-context understanding (shallow layers) but are not relied upon in deeper layers where generation decisions are made. This aligns with the hypothesis that the ensemble format (structural cue) is the main driver of improved ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Attention weights averaged across all attention heads and layers (32 layers reported for XGLM analysis), aggregated over six language directions; target position defined as final token in input per Wang et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9264.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9264.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning_ERR_SmallModels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERR (Ensemble Random+Random) evaluation on reasoning and hallucination tasks with small models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the Ensemble prompt with random nouns (ERR) to nine benchmarks across commonsense QA, logical reasoning, arithmetic reasoning, and hallucination detection, using Alpaca, Llama3, and Mistral (~7B models); ERR improves or matches Vanilla in the vast majority of experiments, sometimes surpassing chain-of-thought effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca, Llama3 (Llama-3.1-8B), Mistral (Mistral-7B-Instruct-v0.2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Alpaca 7B; Llama3 8B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Nine reasoning/hallucination datasets (CSQA, StrategyQA, Date Understanding, Sports Understanding, Logical Fallacy, Three Objects, GSM8K, AQuA, Known Unknowns)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice and free-response reasoning tasks covering commonsense QA, logical deduction, arithmetic word problems, and hallucination detection; metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>4-shot ICL with randomly selected examples (uniform set of 4-shot examples used for all test inputs). Compare Vanilla prompt vs ERR (Ensemble with Random+Random nouns). Experiments performed with and without chain-of-thought (CoT) prompting (ERR(w/CoT), ERR(w/oCoT), Vanilla(w/CoT), Vanilla(w/oCoT)).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Vanilla prompt (no example-level descriptions) ± CoT; ERR (Random+Random) ± CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ERR(w/CoT) either significantly outperforms or matches Vanilla(w/CoT) in 25 out of 27 experiments across the three small models; exceptions: Alpaca on Sports dataset and Mistral on AQuA where ERR(w/CoT) was lower. ERR(w/oCoT) often outperformed or matched Vanilla(w/oCoT) and sometimes even exceeded Vanilla(w/CoT), indicating strong format-driven gains. Exact accuracy numbers are provided in appendix tables (not fully enumerated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Summary: 25/27 experiments show ERR(w/CoT) >= Vanilla(w/CoT); some ERR(w/oCoT) > Vanilla(w/CoT). Detailed per-dataset per-model accuracies are in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue ERR's benefit is due to prompt format organization rather than semantic content of example-level descriptions; the format provides structural cues that models exploit. They conjecture pretraining exposure to similar patterns could be the mechanism but lack access to pretraining data to validate.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>9 datasets with varying test input counts (e.g., CSQA 1221, StrategyQA 1012, GSM8K 1319, AQuA 254). Uniform random 4-shot examples, accuracy metric. Models: Alpaca-7B, Llama-3.1-8B, Mistral-7B-Instruct-v0.2. Both CoT and no-CoT settings tested. Results averaged and reported; full numeric tables in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9264.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9264.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning_ERR_GPT35</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERR (Ensemble Random+Random) evaluation on reasoning tasks with GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Testing whether ERR benefits scale to a larger instruction-tuned model (GPT-3.5); results show ERR performs similarly to Vanilla across most reasoning/hallucination datasets, with minor improvements on a few (AQuA, Known Unknowns).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same nine reasoning/hallucination datasets as small-model experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense, logical, arithmetic reasoning and hallucination detection; metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>4-shot ICL with randomly selected examples; compare Vanilla vs ERR(Random+Random); tests both with and without CoT where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Vanilla prompt ± CoT vs ERR(Random+Random) ± CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ERR performed similarly to Vanilla across every dataset on GPT-3.5; in a few cases ERR(w/CoT) slightly outperformed Vanilla(w/CoT) (notably AQuA and Known Unknowns). No systematic large gains observed for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors interpret GPT-3.5's insensitivity to random descriptive nouns as consistent with the overall finding that semantic content of the descriptions is not crucial; for stronger models, the ERR format does not harm performance and can sometimes slightly help. They hypothesize that large models may already generalize well regardless of additional format cues.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used gpt-3.5-turbo-0125 via API as the large-model testbed. Same uniform random 4-shot examples and datasets as small-model experiments. Accuracy is reported; figures in paper summarize results (full numbers in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Larger language models do in-context learning differently <em>(Rating: 1)</em></li>
                <li>Learning to retrieve prompts for in-context learning <em>(Rating: 1)</em></li>
                <li>SCOI: Syntax-augmented coverage-based in-context example selection for machine translation <em>(Rating: 2)</em></li>
                <li>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9264",
    "paper_id": "paper-271891859",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "MT_XGLM_Ensemble",
            "name_full": "Machine Translation experiments with XGLM using Ensemble prompt formats",
            "brief_description": "Evaluation of prompt-format interventions (Ensemble prompt variants) on XGLM-7.5B for multilingual machine translation (FLORES-101 directions), showing that ensemble-style prompts improve COMET scores relative to a vanilla prompt and that improvements persist even when example-level descriptions are semantically incorrect or random.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "XGLM",
            "model_size": "7.5B",
            "task_name": "Machine Translation (FLORES-101 directions)",
            "task_description": "Translate short-sentence test inputs among English, German, French and Russian (FLORES-101 devtest, averaged over six translation directions). Evaluation with COMET (wmt20-comet-da).",
            "presentation_format": "4-shot in-context learning. Baseline 'Vanilla' prompt: task instruction + 4 in-context examples (no example-level descriptive lines). 'Ensemble' prompt: add two example-level descriptive lines labeling halves of the examples (Noun-A / Noun-B), with variants: Ensemble (Word+Syntax), Ensemble (Syntax+Word), Ensemble (Word+Semantics), Different Ensemble (uses 'different' qualifier), Ensemble (Random+Random) where Noun-A/B are random English nouns. Example selection methods: Random, BM25 (word-level), Polynomial (syntax-level), BM25+Polynomial (concatenation), Polynomial+BM25.  COMET used as metric.",
            "comparison_format": "Compared to Vanilla prompt and ablated variants (Single (one descriptive line), Single (Example) with generic 'Examples:' line, Vanilla(Translate) where 'Translate the following sentence:' is added before test input).",
            "performance": "Vanilla baseline COMET (averaged over six directions) reported in paper: Random: XGLM 54.07; BM25: 55.00; Polynomial: 55.52; BM25+Polynomial: 56.17; Polynomial+BM25: 56.18. Ensemble prompt variants produced consistent improvements over these Vanilla baselines (improvement magnitudes shown in paper figures; exact per-variant COMET numbers not tabulated in the main text).",
            "performance_comparison": "Paper reports that Ensemble variants (including Ensemble (Random+Random)) improved over Vanilla in most cases; in some cases 'incorrect' or reversed descriptive labels improved as much or more than semantically 'correct' labels (e.g., Ensemble (Syntax+Word) sometimes improved more than Ensemble (Word+Syntax)). Exact COMET increases versus Vanilla are presented graphically in figures but not listed as single-number deltas in main text.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors hypothesize that the performance gain primarily stems from the prompt format (organization and presence of example-level descriptive lines) rather than the semantic content of those descriptions; attention analyses show LLMs pay limited or decreasing attention to descriptive nouns in deeper layers, supporting the claim that LLMs 'do not care' about the actual meaning of those nouns. They conjecture models have seen many patterns like the Ensemble format during pretraining, making the format a useful cue.",
            "null_or_negative_result": false,
            "experimental_details": "4-shot setting for MT (four in-context examples). Example databases: Europarl (German, French) and ParaCrawl (Russian). Evaluation metric: COMET (wmt20-comet-da). Models: XGLM-7.5B. Results averaged over six translation directions. Various Ensemble prompt variants tested including random nouns; selection methods include BM25 and Polynomial; in combining BM25+Polynomial the first two examples are from one method and remaining two from the other.",
            "uuid": "e9264.0",
            "source_info": {
                "paper_title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "MT_Alpaca_Ensemble",
            "name_full": "Machine Translation experiments with Alpaca using Ensemble prompt formats",
            "brief_description": "Same MT experiment family as above but using Alpaca (7B), finding Ensemble formats generally improve COMET over Vanilla baselines and that random/incorrect example-level descriptions still yield gains; Alpaca shows some instruction-tuned idiosyncrasies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca",
            "model_size": "7B",
            "task_name": "Machine Translation (FLORES-101 directions)",
            "task_description": "Translate short-sentence test inputs among English, German, French and Russian; measured with COMET.",
            "presentation_format": "4-shot ICL. Vanilla vs Ensemble variants with example-level descriptive lines (Word+Syntax, Syntax+Word, Word+Semantics, Different, Random+Random).",
            "comparison_format": "Compared to Vanilla prompt and ablated variants (Single, Single(Example), Vanilla(Translate)).",
            "performance": "Vanilla baseline COMET (averaged): Random 55.42; BM25 56.27; Polynomial 56.13; BM25+Polynomial 56.18; Polynomial+BM25 55.49. Ensemble prompts generally improved over these baselines across selections; Ensemble (Random+Random) often produced gains and in one setting (Polynomial+BM25 examples) delivered the largest gain among prompt variants.",
            "performance_comparison": "Paper reports Ensemble improvements across most selection methods; however Alpaca exhibited some anomalous patterns (e.g., in certain Polynomial/BM25 configurations Ensemble(Random+Random) did not outperform other prompts). Exact numeric deltas are shown graphically but not fully enumerated in main text.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors suggest Alpaca's instruction-tuned nature may make it more robust to different prompt phrasings, leading to smaller differences between certain prompt variants; overall effect attributed to format cues rather than semantics of example-level descriptions.",
            "null_or_negative_result": false,
            "experimental_details": "4-shot, COMET metric, Alpaca-7B model. Attention analysis (in appendix) similarly shows limited reliance on descriptive nouns in deeper layers. Ablation study shows removing descriptive lines or random nouns hurts gains in most cases, indicating the Ensemble organization (two labeled groups) is an important part of the format effect.",
            "uuid": "e9264.1",
            "source_info": {
                "paper_title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "MT_Ablation",
            "name_full": "Ablation study on prompt organization for MT",
            "brief_description": "Ablation experiments dissecting which elements of the Ensemble format drive gains: comparing full Ensemble to Single (one descriptive line), Single (Example) generic line, and Vanilla(Translate); finds Ensemble outperforms ablated formats in most MT cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "XGLM / Alpaca (both evaluated in ablation)",
            "model_size": "XGLM 7.5B / Alpaca 7B",
            "task_name": "Machine Translation (FLORES-101 directions)",
            "task_description": "Translation with 4-shot ICL; ablations test prompt organization differences.",
            "presentation_format": "Variants: Ensemble (two example-level descriptions labeling two halves), Single (only one descriptive line above all examples, either random noun or generic 'Examples:'), Vanilla(Translate) (no example-level descriptions but 'Translate the following sentence:' before test input), Vanilla (no added translate instruction).",
            "comparison_format": "Full Ensemble vs Single (Random), Single (Example), Vanilla(Translate), Vanilla.",
            "performance": "Across averaged six-direction MT results, the original Ensemble format produced greater COMET improvements over Vanilla than the ablated formats in most cases; when one or both example-level descriptions or random nouns were removed the performance gain was reduced in most configurations. Specific numeric deltas are shown in paper figures (not fully tabulated in the main text).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The organization (presence of two labeled groups of examples) appears to be a key structural cue that LLMs exploit; removing one or both labels reduces benefit, consistent with the hypothesis that format/layout rather than label semantics drives the effect.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation runs averaged over six translation directions; models XGLM and Alpaca; four in-context examples; attention analyses complement ablation findings.",
            "uuid": "e9264.2",
            "source_info": {
                "paper_title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Attention_Analysis",
            "name_full": "Attention-weight analysis relating prompt-format to model focus",
            "brief_description": "Analysis of averaged attention weights between prompt components (Example-A, Example-B, Target token, Noun-A, Noun-B) across layers and heads showing that attention to meaningful descriptive nouns is not consistently higher than to random nouns and that attention to nouns diminishes in deeper layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "XGLM (with supplementary Alpaca analysis in appendix)",
            "model_size": "XGLM 7.5B; Alpaca 7B (appendix)",
            "task_name": "Machine Translation (analysis on MT prompts)",
            "task_description": "Layerwise attention weight measurements between prompt components to probe which parts of prompt the model attends to when generating the translation token.",
            "presentation_format": "Compare attention patterns for Ensemble (Word+Syntax) (EWS) vs Ensemble (Random+Random) (ERR) prompts; measure attention from target position to nouns and to examples, averaged over heads and layers.",
            "comparison_format": "EWS (meaningful nouns) vs ERR (random nouns).",
            "performance": "Not a performance metric entry; analysis shows EWS attention to nouns is higher in shallow layers for some target-to-noun paths but in deeper layers ERR often exceeds EWS; overall EWS does not consistently outperform ERR in attention magnitude.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The attention patterns support the paper's claim that LLMs do not meaningfully use the semantic content of descriptive nouns for generation: meaningful nouns may help early-context understanding (shallow layers) but are not relied upon in deeper layers where generation decisions are made. This aligns with the hypothesis that the ensemble format (structural cue) is the main driver of improved ICL.",
            "null_or_negative_result": false,
            "experimental_details": "Attention weights averaged across all attention heads and layers (32 layers reported for XGLM analysis), aggregated over six language directions; target position defined as final token in input per Wang et al. (2023).",
            "uuid": "e9264.3",
            "source_info": {
                "paper_title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Reasoning_ERR_SmallModels",
            "name_full": "ERR (Ensemble Random+Random) evaluation on reasoning and hallucination tasks with small models",
            "brief_description": "Application of the Ensemble prompt with random nouns (ERR) to nine benchmarks across commonsense QA, logical reasoning, arithmetic reasoning, and hallucination detection, using Alpaca, Llama3, and Mistral (~7B models); ERR improves or matches Vanilla in the vast majority of experiments, sometimes surpassing chain-of-thought effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca, Llama3 (Llama-3.1-8B), Mistral (Mistral-7B-Instruct-v0.2)",
            "model_size": "Alpaca 7B; Llama3 8B; Mistral 7B",
            "task_name": "Nine reasoning/hallucination datasets (CSQA, StrategyQA, Date Understanding, Sports Understanding, Logical Fallacy, Three Objects, GSM8K, AQuA, Known Unknowns)",
            "task_description": "Multiple-choice and free-response reasoning tasks covering commonsense QA, logical deduction, arithmetic word problems, and hallucination detection; metric: accuracy.",
            "presentation_format": "4-shot ICL with randomly selected examples (uniform set of 4-shot examples used for all test inputs). Compare Vanilla prompt vs ERR (Ensemble with Random+Random nouns). Experiments performed with and without chain-of-thought (CoT) prompting (ERR(w/CoT), ERR(w/oCoT), Vanilla(w/CoT), Vanilla(w/oCoT)).",
            "comparison_format": "Vanilla prompt (no example-level descriptions) ± CoT; ERR (Random+Random) ± CoT.",
            "performance": "ERR(w/CoT) either significantly outperforms or matches Vanilla(w/CoT) in 25 out of 27 experiments across the three small models; exceptions: Alpaca on Sports dataset and Mistral on AQuA where ERR(w/CoT) was lower. ERR(w/oCoT) often outperformed or matched Vanilla(w/oCoT) and sometimes even exceeded Vanilla(w/CoT), indicating strong format-driven gains. Exact accuracy numbers are provided in appendix tables (not fully enumerated in main text).",
            "performance_comparison": "Summary: 25/27 experiments show ERR(w/CoT) &gt;= Vanilla(w/CoT); some ERR(w/oCoT) &gt; Vanilla(w/CoT). Detailed per-dataset per-model accuracies are in appendices.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors argue ERR's benefit is due to prompt format organization rather than semantic content of example-level descriptions; the format provides structural cues that models exploit. They conjecture pretraining exposure to similar patterns could be the mechanism but lack access to pretraining data to validate.",
            "null_or_negative_result": true,
            "experimental_details": "9 datasets with varying test input counts (e.g., CSQA 1221, StrategyQA 1012, GSM8K 1319, AQuA 254). Uniform random 4-shot examples, accuracy metric. Models: Alpaca-7B, Llama-3.1-8B, Mistral-7B-Instruct-v0.2. Both CoT and no-CoT settings tested. Results averaged and reported; full numeric tables in appendix.",
            "uuid": "e9264.4",
            "source_info": {
                "paper_title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Reasoning_ERR_GPT35",
            "name_full": "ERR (Ensemble Random+Random) evaluation on reasoning tasks with GPT-3.5",
            "brief_description": "Testing whether ERR benefits scale to a larger instruction-tuned model (GPT-3.5); results show ERR performs similarly to Vanilla across most reasoning/hallucination datasets, with minor improvements on a few (AQuA, Known Unknowns).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0125)",
            "model_size": null,
            "task_name": "Same nine reasoning/hallucination datasets as small-model experiments",
            "task_description": "Commonsense, logical, arithmetic reasoning and hallucination detection; metric: accuracy.",
            "presentation_format": "4-shot ICL with randomly selected examples; compare Vanilla vs ERR(Random+Random); tests both with and without CoT where applicable.",
            "comparison_format": "Vanilla prompt ± CoT vs ERR(Random+Random) ± CoT.",
            "performance": "ERR performed similarly to Vanilla across every dataset on GPT-3.5; in a few cases ERR(w/CoT) slightly outperformed Vanilla(w/CoT) (notably AQuA and Known Unknowns). No systematic large gains observed for GPT-3.5.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors interpret GPT-3.5's insensitivity to random descriptive nouns as consistent with the overall finding that semantic content of the descriptions is not crucial; for stronger models, the ERR format does not harm performance and can sometimes slightly help. They hypothesize that large models may already generalize well regardless of additional format cues.",
            "null_or_negative_result": true,
            "experimental_details": "Used gpt-3.5-turbo-0125 via API as the large-model testbed. Same uniform random 4-shot examples and datasets as small-model experiments. Accuracy is reported; figures in paper summarize results (full numbers in appendix).",
            "uuid": "e9264.5",
            "source_info": {
                "paper_title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 2,
            "sanitized_title": "rethinking_the_role_of_demonstrations_what_makes_incontext_learning_work"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Larger language models do in-context learning differently",
            "rating": 1,
            "sanitized_title": "larger_language_models_do_incontext_learning_differently"
        },
        {
            "paper_title": "Learning to retrieve prompts for in-context learning",
            "rating": 1,
            "sanitized_title": "learning_to_retrieve_prompts_for_incontext_learning"
        },
        {
            "paper_title": "SCOI: Syntax-augmented coverage-based in-context example selection for machine translation",
            "rating": 2,
            "sanitized_title": "scoi_syntaxaugmented_coveragebased_incontext_example_selection_for_machine_translation"
        },
        {
            "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
            "rating": 1,
            "sanitized_title": "did_aristotle_use_a_laptop_a_question_answering_benchmark_with_implicit_reasoning_strategies"
        }
    ],
    "cost": 0.0135465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions
6 Sep 2025</p>
<p>Chenming Tang tangchenming@stu.pku.edu.cn 
School of Computer Science
National Key Laboratory for Multimedia Information Processing
Peking University MOE Key Laboratory of Computational Linguistics
Peking University
Peking University</p>
<p>Zhixiang Wang 
School of Computer Science
National Key Laboratory for Multimedia Information Processing
Peking University MOE Key Laboratory of Computational Linguistics
Peking University
Peking University</p>
<p>Hao Sun 
School of Computer Science
National Key Laboratory for Multimedia Information Processing
Peking University MOE Key Laboratory of Computational Linguistics
Peking University
Peking University</p>
<p>Yunfang Wu 
School of Computer Science
National Key Laboratory for Multimedia Information Processing
Peking University MOE Key Laboratory of Computational Linguistics
Peking University
Peking University</p>
<p>Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions
6 Sep 20258E24F961801F8D94BB8AE1B2F767A667arXiv:2408.08780v7[cs.CL]German: Wer ist Martha? English: Who is Martha? German: Was ist NLP? English: What is NLP? Translate the following sentence: German: Was sind diese vier? English: German: Diese vier sind wichtigEnglish: These four are importantGerman: Diese fünf sind interessant
With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks.However, the function of descriptive instructions during ICL remains under-explored.In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL performance.But to our surprise, LLMs might not care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since it could lead to improvement even with random descriptive nouns.We further apply this new ensemble framework on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions.</p>
<p>Introduction</p>
<p>In-context learning (ICL) boosts the performance of large language models (LLMs) across numerous natural language processing (NLP) tasks, where LLMs are presented with in-context examples containing input and ground truth output (Brown et al., 2020;Dong et al., 2023).Many works have verified the vital role of in-context examples in ICL (Wang et al., 2023;Wei et al., 2023).However, Min et al. (2022) find that ground truth labels might not be the key to ICL performance on classification tasks.</p>
<p>The selection of in-context examples has been proven significant to the performance of ICL (Rubin et al., 2022) and there have been various works on in-context example selection (Agrawal et al  2023; Li et al., 2023;Ye et al., 2023).Besides diverse approaches of selecting examples, no existing work has tried to explicitly tell LLMs in what way those specific examples are selected.We hypothesize that if LLMs are prompted with instructions describing the properties of selected in-context examples, they might learn better from these examples, since instruction following is one of LLMs' most important qualities nowadays (Ouyang et al., 2022;Peng et al., 2023;Zhang et al., 2024).Tang et al. (2024) prompt LLMs with examples selected based on both word-level and syntax-level criteria for machine translation (MT) for better ICL performance.This inspires us to tell LLMs where different in-context examples come from when they are selected by multiple methods.</p>
<p>In our experiments on MT, we first select incontext examples based on lexical and syntactic similarity for each test input separately.Then we combine both to construct the complete set of examples, with half word-level examples and half syntaxlevel examples.Further, we devise a novel ensemble prompt framework (as shown in the left part "Prompt Template" of Figure 1), adding examplelevel instructions to describe that the following examples are with similar words or similar syntax.</p>
<p>Experimental results on MT demonstrate that adding such ensemble prompt framework does improve LLMs' performance over conventional prompts.Meanwhile, we find that when the example-level descriptions do not correspond to the source of in-context examples or are completely nonsense, LLMs still benefit from the prompt.These surprising results indicate that in fact LLMs might not care what the descriptions say and are more sensitive to the prompt format.In other words, a proper format can be much more effective than well-designed descriptions in ICL.</p>
<p>To further verify the superiority of the ensemble framework, we present empirical evaluations on commonsense, math, logical reasoning and hallucination benchmarks (including nine datasets in total) across three small-scale LLMs (Alpaca, Llama3 and Mistral) and one large-scale LLM (GPT-3.5).The novel prompt framework is able to achieve promising results even with the descriptive nouns in the prompt being random nouns, further suggesting that a proper prompt format would be much more effective and efficient compared with laborious design of detailed and specific descriptions.</p>
<p>There are a few studies very related to our work.Min et al. (2022) find that the labels of in-context examples do not need to be correct for classification tasks.Wei et al. (2023) find that larger language models learn from in-context examples even when the labels are flipped or unrelated.Srivastava et al. (2024) demonstrate that optimizing examples is less effective in some tasks given a high-quality task instruction.Our work is different from the above in that we focus on the meaning of descriptions rather than labels or examples in ICL and our finding is that the format of prompts is more important than carefully designed descriptions.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• For the first time, we specifically analyze the effect of prompt descriptions on ICL performance and find that LLMs might not care what users actually say in descriptions, while they are more sensitive to the prompt format.</p>
<p>• We present a simple yet effective prompt framework that is proven feasible on MT through comprehensive experiments across six translation directions.Promising experimental results on three LLMs further verify the superiority of the novel framework on a range of commonsense, math, logical reasoning and hallucination tasks.</p>
<p>Our code is available at https://github.com/JamyDon/Format-Beats-Descriptions.</p>
<p>Prompting LLMs for MT</p>
<p>Primarily, we focus on MT, a typical generation task.Recently, various approaches of selecting in-context examples have been proposed for MT (Agrawal et al., 2023;Kumar et al., 2023;Tang et al., 2024).However, no existing work has tried to make LLMs aware of in what way those specific in-context examples are selected.</p>
<p>We assume that LLMs would perform better when they are told the reasons for selecting those examples.Tang et al. (2024) select examples based on a combination of word-level and syntax-level criteria, which inspires us to present an ensemble prompt framework to make LLMs clearly know the reasons behind example selection.In addition, to have a comprehensive idea of whether LLMs really know what is said in the descriptions, we design some prompt variants that are less meaningful or completely nonsense.</p>
<p>In-context Example Selection for MT</p>
<p>For word-level examples, we simply select them using BM25 (Bassani, 2023).For syntax-level examples, we use the top-k polynomial algorithm proposed by Tang et al. (2024) to convert dependency trees into polynomials and compute syntactic similarity based on the Manhattan distances (Craw, 2017) between polynomial terms.For brevity, we denote the syntax-level algorithm by "Polynomial".</p>
<p>To combine word-level and syntax-level examples, we simply concatenate them.For example, the first and the remaining half of examples are selected by BM25 and Polynomial respectively.</p>
<p>A New Ensemble Prompt Framework</p>
<p>To maintain consistency, all our MT experiments use four in-context examples.</p>
<p>First of all, we use the most regular prompt without any example-level descriptions as baseline (referred to as Vanilla), which is shown in Figure 2.  In the template, "Task-level Instruction" instructs the model to do the current task (MT here)."Example A i " and "Example B i " denote the i-th example from selection approach A (e.g., BM25) and B (e.g., Polynomial) respectively, all containing both source language inputs and target language translations."Test Input" refers to the source language input of the test sample, which requires the LLM to translate it into the target language.Then, we add example-level descriptions for examples from different selection approaches and explicitly instruct the LLM to translate the test input.This prompt framework is referred to as Ensemble and is shown in Figure 1 as presented in Section 1. "Noun A" and "Noun B" describe the examples from selection A and B respectively.For example, the two nouns can be "words" and "syntax" to properly describe examples selected by BM25 and Polynomial respectively.In this way, we can conveniently control the example-level descriptions to tell the LLM why those examples are used.</p>
<p>Experimental Setup</p>
<p>Datasets</p>
<p>We perform evaluation on the devtest set of FLORES-101 (Goyal et al., 2022), which contains 1012 sentences with translations in 101 languages.We experiment between English and three common languages: German, French and Russian.We use Europarl (Koehn, 2005) for German and French and ParaCrawl (Bañón et al., 2020) for Russian as example database, from which we select in-context examples.Detailed statistics are in Table 1.</p>
<p>Evaluation Metrics</p>
<p>We report COMET (Rei et al., 2020) scores from wmt20-comet-da1 , which is considered a superior metric for MT today (Kocmi et al., 2021).</p>
<p>Language Models</p>
<p>We experiment with two LLMs commonly used in MT: XGLM 7.5B (Lin et al., 2022) and Alpaca (Taori et al., 2023) 1.</p>
<p>Random:</p>
<p>The four examples are randomly sampled from the example database.We report the average result of three different random seeds.</p>
<p>BM25: We retrieve the top-4 matching examples for each test input using BM25 (Bassani, 2023).</p>
<p>Polynomial: It is rather time-consuming to retrieve examples from databases containing millions of data using the Polynomial algorithm.Following Tang et al. (2024)</p>
<p>Prompts</p>
<p>We design various prompts to explore whether LLMs can benefit from explicit descriptions of examples and whether they really understand the meaning of descriptions.</p>
<p>Vanilla: The normal prompt without any example-level descriptions as shown in Figure 2.</p>
<p>Ensemble (Word + Syntax): Shown in Figure 3a, Noun A and Noun B are "words" and "syntax" respectively, which semantically corresponds to BM25 + Polynomial examples but is converse to Polynomial + BM25.</p>
<p>Ensemble (Syntax + Word): Shown in Figure 3b, Noun A and Noun B are "syntax" and "words" respectively, which semantically matches Polynomial + BM25 examples but mismatches BM25 + Polynomial.</p>
<p>Different Ensemble (Word + Syntax): Shown in Figure 3c, Noun A and Noun B are still "words" and "syntax" respectively but the qualifier "simi-lar" is replaced with "different".This aims to find out whether LLMs pay attention to the meaning of "different/similar" and care the semantics of descriptions.</p>
<p>Ensemble (Word + Semantics): Shown in Figure 3d, Noun A and Noun B are "words" and "semantics" respectively, which does not semantically match any of our example selection methods.</p>
<p>Ensemble (Random + Random): Shown in Figure 3e, for each input, Noun A and Noun B are different random English nouns sampled using Wonderwords 2 , aiming to explore LLMs' understanding of descriptions.</p>
<p>Main Results</p>
<p>To give a quick view of LLMs' MT performance, Table 2 shows the COMET scores of Vanilla baselines averaged over six translation directions.As can be seen from the results, those "correct" prompts, exactly corresponding to the selection of in-context examples (e.g., Ensemble (Word + Syntax) with BM25 + Polynomial examples and Ensemble (Syntax + Word) with Polynomial + BM25 examples), do bring some help as expected.However, when the prompt does not correspond to the selection of examples (i.e., is "incorrect"), the performance improves as well and sometimes even more than those "correct" cases.For example, on XGLM with BM25 + Polynomial examples, Ensemble (Syntax + Word) improves more than Ensemble (Word + Syntax), even though the former is completely reversed.On Alpaca with BM25 + English: Who is Martha?</p>
<p>German: Was ist NLP? English: What is NLP?Translate the following sentence: German: Was sind diese vier?English:</p>
<p>Task-level Instruction</p>
<p>Examples with similar Noun :
Example 𝐴 ! Example 𝐴 "
Examples with similar Noun :
Example 𝐵 ! Example 𝐵 "
Translate the following sentence:</p>
<p>Test Input</p>
<p>Prompt Template Prompt Example of Alpaca</p>
<p>(e) Ensemble (Random + Random).To sum up, the experimental results on MT suggest that a proper prompt format leads to better ICL performance of LLMs while a careful design of descriptions might be less effective.</p>
<p>Ablation Study</p>
<p>To better understand how the Ensemble format brings improvement, we perform ablation experiments over the organization of the prompt: Figure 5: Ablation studies over the organization of the prompt, showing the performance gain of different prompts over Vanilla, averaged over all six translation directions."Rand.","Poly.","Ens.","Sgl.","V.", "Trans."refer to "Random", "Polynomial", "Ensemble", "Single", "Vanilla", "Translate", respectively.</p>
<p>Ablation experiments suggest that in MT, our proposed Ensemble is a relatively superior prompt format, performing better than other variants.</p>
<p>Analysis via Attention Weights</p>
<p>To have a better idea of the internal mechanism of LLMs when prompted with different prompts, we compute the attention weights between different prompt components.We focus on three components: in-context examples (from A or B, denoted by "Example-A" and "Example-B"), the target position (denoted by "Target") where the model starts to generate predictions (following Wang et al. (2023), we use the final token in the input) and the two descriptive nouns ("Noun-A" and "Noun-B").We obtain the attention weights averaged over all attention heads from the attention matrix across all the layers.All the results are averaged over all six language directions.</p>
<p>Results comparing Ensemble (Word + Syntax) (EWS) and Ensemble (Random + Random) (ERR) on XGLM with BM25 + Polynomial examples are presented in Figure 6 (for results on Alpaca, refer to Appendix C).If the model really cares what the descriptions say, its attention to meaningful descriptive nouns (in EWS) should be much greater than those meaningless (in ERR).However, in most cases, EWS performs no higher than ERR, indicating that the model does not really care what the descriptive nouns actually are."Target to Noun-A" is a special case, where EWS is high in shallow layers.But in deeper layers, EWS falls behind and ERR takes the lead.This shows that the model might pay more attention to the meaningful noun when understanding the context in shallow layers but gradually forgets it when it comes to generation in deeper layers.In a word, the attention weights further confirm our claim that LLMs do not really care what the descriptive nouns are in most cases.</p>
<p>Discussion</p>
<p>Above results show that LLMs benefit from our Ensemble prompts in most cases.However, the benefit comes from a proper format rather than the meaningful descriptions (e.g., "similar words" and "similar syntax").This demonstrates that LLMs might not care what users say in the descriptions but is more sensitive to the format of prompts.In other words, designing a proper prompt format would be more efficient than paying a lot of effort into looking for a perfect description.</p>
<p>In the next section, we apply Ensemble format to more tasks to further verify its generalizability.</p>
<p>Generalizing the New Ensemble Prompt Framework to More Tasks</p>
<p>To further verify our conclusion obtained from MT that our proposed Ensemble framework improves ICL even when the example-level descriptions are incorrect or meaningless, we perform the comparison between Vanilla and Ensemble (Random + Random), which we would refer to as ERR, on more types of tasks across different language models.</p>
<p>Experimental Setup</p>
<p>Datasets</p>
<p>We use a total of nine benchmarks, covering four task types: commonsense QA, logical reasoning, arithmetic reasoning, and hallucination detection.</p>
<p>For commonsense QA, we adopt four datasets.The widely-used CSQA (Talmor et al., 2019) features commonsense questions about the world involving complex semantics requiring prior knowledge.StrategyQA (Geva et al., 2021) challenges models to infer implicit reasoning steps using a strategy to answer questions.We also choose two specialized evaluation sets from BIG-bench (Srivastava et al., 2023): Date Understanding, which asks models to infer the date from a context, and Sports Understanding, which involves assessing the plausibility of sentences related to sports.For logical reasoning task, we choose Logical Fallacy and Three Objects (a subset of Logical Deduction) from Big-bench (Srivastava et al., 2023).Logical Fallacy aims to test the model's ability to identify whether there are fallacies in a given logical reasoning, and Three Objects requires the model to infer the order of a sequence of objects from a set of minimal conditions.</p>
<p>To explore the performance of ERR on math word problems, we adopt the following two datasets: GSM8K (Cobbe et al., 2021), which consists of high quality free-response grade school math problems, and AQuA (Ling et al., 2017), containing the algebraic word problems in the form of multiple-choice questions.</p>
<p>In addition, to explore whether ERR could alleviate LLMs' hallucination, we choose Known Unknowns from Big-bench (Srivastava et al., 2023).</p>
<p>The number of test inputs for each dataset is listed in Table 3</p>
<p>Evaluation Metric</p>
<p>These nine datasets are either in the form of multiple-choice questions or free-response questions with standard answers, so we use accuracy as the metric for all of them.</p>
<p>Language Models</p>
<p>We experiment with both instruction-tuned and non-instruction-tuned models to see whether our findings could extend to different kinds of mod-els.We evaluate three frequently used open source LLMs with around 7B parameters, including Alpaca (Taori et al., 2023), Llama3 (Llama Team, 2024), and Mistral (Jiang et al., 2023), among which Llama3 is a base model before instruction tuning.To assess the effect of ERR on more powerful models, we also evaluate GPT-3.5 (Ouyang et al., 2022) 3 .We use Llama-3.1-8B,Mistral-7B-Instruct-v0.2 and gpt-3.5-turbo-0125 4 for Llama3, Mistral and GPT-3.5 respectively.</p>
<p>Example Selection</p>
<p>Note that randomly selected examples combined with ERR have already brought non-trivial improvements to MT.Therefore, for each dataset discussed in this section, we randomly select a uniform set of examples (4-shot) for all test inputs without applying any carefully designed selection method, in order to focus on and verify the simple yet effective and universal nature of ERR.</p>
<p>Prompts</p>
<p>We compare ERR with Vanilla across different datasets and LLMs.Given that these tasks usually involve reasoning, on which chain-of-thought (CoT) is commonly utilized (Wei et al., 2022)  3 We choose this model because it is a commonly used costeffective API-based LLM and a de facto black box baseline.</p>
<p>4 https://openai.com/api/</p>
<p>Results of Small-scale Models</p>
<p>Results across all nine datasets and three smallscale models (Alpaca, Llama3 and Mistral) are illustrated in Figure 7. Detailed results are presented in Appendix B.</p>
<p>The results demonstrate that ERR (w/ CoT), achieved by integrating CoT with our proposed prompt framework, either significantly outperforms or matches Vanilla (w/ CoT) in 25 out of 27 experiments (covering nine datasets and three models).The exceptions are Alpaca on the Sports dataset and Mistral on the AQuA dataset, where ERR (w/ CoT) shows somewhat lower performance compared to Vanilla (w/ CoT).When CoT is not incorporated, ERR generally performs much better than or on par with Vanilla, except for the Sports dataset with Llama3, where ERR performs a little poorer.</p>
<p>Surprisingly, ERR (w/o CoT) sometimes even surpasses Vanilla (w/ CoT), suggesting that the ERR framework alone can offer more improvements than CoT.This highlights the value of ERR and reaffirms that the format plays a crucial role in enhancing LLMs' problem-solving capabilities.In terms of models, the performance of ERR on Alpaca is far less impressive than on Llama3 and Mistral, which may be because Alpaca has strong instruction-following capabilities and is more robust to different prompts.</p>
<p>In summary, without using any carefully designed selection methods, directly filling the randomly selected examples into the ERR framework brings significant improvement to various reasoning tasks and even alleviates the hallucination of models in most cases, no matter how meaningless and incorrect the example-level descriptions are.Moreover, ERR can work perfectly with CoT.Therefore, at least for relatively small models, this simple yet effective trick is worth introducing into prompt engineering for various tasks.</p>
<p>We also experiment with Llama2 (Llama Team, 2023) and the results are in Appendix F. The overall trend is consistent with Llama3.</p>
<p>Results of GPT-3.5</p>
<p>As shown in Figure 8, ERR performs similarly to Vanilla across every dataset using GPT-3.5.Although the ERR format does not bring significant improvement to these tasks with GPT-3.5 and Alpaca (as shown in Figure 7), the fact remains that the incorrect or meaningless examplelevel descriptions caused by random nouns do not have much negative impact on GPT-3.5, a sufficiently powerful model, or Alpaca, which has strong instruction-following capabilities.In some cases, it even slightly improves performance (e.g., ERR (w/ CoT) outperforms Vanilla (w/ CoT) on AQuA and Known Unknowns).In other words, LLMs might not care what users actually say to describe the provided examples while they are more sensitive to the format of prompts, which is in line with our findings obtained from MT.</p>
<p>Discussion</p>
<p>Based on the experiments conducted on both smallscale and large-scale models, we can conclude that ERR is a simple yet practical and universal prompt framework.It can enhance problem-solving capabilities of small models and be applied to large models without the risk of performance degradation due to the meaningless noise within it.In other words, there might be less need to meticulously select examples or design detailed descriptions.Instead, you can uniformly and efficiently apply ERR to various tasks with different models.</p>
<p>As analyzed in Section 2.6, the ERR framework can work because LLMs pay less attention to the descriptive nouns while being more sensitive to the overall prompt format.We conjecture that the underlying reason could be that LLMs have been presented with many patterns similar to ERR during pre-training and thus perform better when presented with ERR prompts (Chen et al., 2024).However, due to lack of access to the pre-training process of LLMs (either open-source or close-source), we cannot further validate our conjecture more solidly and our understanding of the deeper mechanism remains limited to superficial analysis, which is one of the limitations of this work.</p>
<p>Related Work</p>
<p>In-context Example Selection Rubin et al. (2022) suggest that LLMs' ICL performance strongly depends on the selection of in-context examples.In consequence, many works have been trying to explore ways of selecting better in-context examples in recent years.Li et al. (2023) train a unified in-context example retriever across a wide range of tasks.Ye et al. (2023) select examples based on both relevance and diversity, with the help of determinantal point processes.Agrawal et al. (2023) ensure n-gram coverage to select better examples for MT.Kumar et al. (2023) train an in-context example scorer for MT based on several features.Tang et al. (2024) combine both word-level and syntax-level coverage when selecting examples for MT.</p>
<p>Mechanism of In-context Learning With the popularity of ICL, there have been numerous studies on analyzing the mechanism of ICL.One stream of these studies focuses on explaining the essence of ICL, relating ICL to gradient descent (Von Oswald et al., 2023), implicit Bayesian inference (Xie et al., 2022), induction heads completing token sequences based on similar context (Olsson et al., 2022), generation maintaining coherency (Sia and Duh, 2023), creation of task vectors based on incontext examples (Hendel et al., 2023), etc.The other stream focuses on the role of in-context examples, especially labels of these examples.Min et al. (2022) find that ground truth labels are not necessary and LLMs perform fairly well even with random labels.Wang et al. (2023) find that label words play the role of anchors that aggregating information of the whole examples and serve as a reference for LLMs' final predictions.Wei et al. (2023) find that larger language models can override semantic priors and learn from in-context examples with flipped labels or semantically-unrelated labels.</p>
<p>Conclusion</p>
<p>In this work, we analyze the effect of descriptive instructions in prompts during ICL and propose an Ensemble prompt framework describing the properties of in-context examples selected by different methods.Experimental results on MT indicate that while LLMs are sensitive to prompt formats, they might not care the actual meaning of the descriptions and the framework improves LLMs' performance even with meaningless descriptions compared with the conventional prompt.We further apply the Ensemble framework to four other NLP tasks and find that it achieves promising results, especially on small-scale models.These results suggest that rather than working hard on well-designed descriptions, making use of a proper prompt format would be more effective and efficient.</p>
<p>Limitations</p>
<p>First, since there are so many open-source LLMs in the world nowadays, it is impossible to experiment with all existing models and thus our work only employ several commonly-used LLMs.Second, since we do not have access to the pre-training or post-training process of LLMs (either open-source or close-source), our analysis of the mechanism of ICL could be somewhat superficial.The behavior of LLMs can be highly subject to their training data, which we have no access to.Lastly, although we reveal that ERR is a superior prompt format for several models, it could still be a local optimum and how to effectively search for a best prompt format for different models and tasks is still underexplored, which we leave for future work.close to EWS.For target-to-noun attention weights, EWS is higher in shallow layers but falls behind ERR in deeper layers, especially in the last layer.This demonstrates that Alpaca might pay more attention to the meaningful words ("word" and "syntax") when understanding the context in shallow layers but gradually forgets them when it comes to generation in the deeper layers.In short, EWS performs no higher than ERR in most cases.</p>
<p>D Dataset Details for Reasoning Tasks</p>
<p>We list the details of splitting training set (example database) and test set for our conducted reasoning tasks, covering four types and nine datasets.We set random seed for all possible shuffling and sampling operations to 42.Note that we experiment with 4shot for all datasets.</p>
<p>D.1 Datasets Fetched from Exclusive Source</p>
<p>D.2 Datasets Fetched from The Big-bench</p>
<p>For StrategyQA (Geva et al., 2021), Date, Sports, Logical Fallacy, Three Objects, and Known Unknowns, we fetched them from the Big-bench (Srivastava et al., 2023).Each of them has a task.json.We randomly shuffle the task. . 5The reasoning is generated by ChatGPT6 .Note the ChatGPT is not the same as GPT-3.5 we used for experiments.</p>
<p>F Results of Llama2</p>
<p>Results of Llama2-7B-chat-hf (Llama Team, 2023) on the nine datasets are presented in Figure 23.</p>
<p>While ERR outperforms Vanilla with Llama2 across most datasets, its performance on Logical Fallacy and Sports is notably poor.Llama2 almost always responds with confused emojis for Logical Fallacy questions and outputs questions like "plausible or implausible?"for Sports, leading to predominantly incorrect answers.Further investigation into these issues is left for future work.</p>
<p>G Computational Details</p>
<p>G.1 Hardware</p>
<p>Inference of LLMs runs on an NVIDIA A40 GPU (with memory of 48 GB).Other experiments run    on Intel ® Xeon ® Gold 6348 CPU (with memory of 256 GB).</p>
<p>G.2 Software</p>
<p>Our OS: Ubuntu 20.04.6 LTS.Our code: Python only.Libraries and packages are specified in the source code.The licenses of the scientific artifacts we use are shown in Table 7.</p>
<p>H Licenses</p>
<p>Figure 1 :
1
Figure 1: Template and Alpaca's example of Ensemble.</p>
<p>Figure 2 :
2
Figure 2: Template and Alpaca's example of Vanilla.</p>
<p>, we instead re-rank the top-100 examples retrieved by BM25 using Polynomial and the top-4 are used as final in-context examples.BM25 + Polynomial: To combine examples with both lexical and syntactic similarity, we simply concatenate examples from BM25 and Polynomial.Specifically, the first two examples are from BM25 and the remaining two are from Polynomial.Polynomial + BM25: The first two examples are from Polynomial and the remaining two are from BM25.</p>
<p>Figure 3 :
3
Figure 3: Templates and Alpaca's examples of Ensemble prompts.</p>
<p>Figure 4 :
4
Figure 4: Main results on XGLM and Alpaca, showing the performance gain of different prompts over the Vanilla prompt, averaged over all six translation directions.Each cluster presents the results of a selection of in-context examples and each bar in it presents the result of a prompt."Ens.","W.", "Syn.","Sem.","Diff.","Rand.","Poly."refer to "Ensemble", "Word", "Syntax", "Semantics", "Different", "Random", "Polynomial", respectively.Polynomial examples, Ensemble (Word + Semantics) improves more than Ensemble (Word + Syntax), albeit the examples with similar syntax do not necessarily bear similar semantics.More interestingly, Different Ensemble (Word + Syntax), telling the LLM that the in-context examples are with different properties, is able to beat "correct" prompts sometimes (e.g., on XGLM with BM25 + Polynomial examples and Alpaca with Polynomial + BM25 examples).Surprisingly, no matter how in-context examples are selected and whether the prompts are "correct", Ensemble prompts bring improvement in most cases.Even Ensemble (Random + Random), in which example-level descriptions are with random nouns and could be completely nonsense (like "examples with similar nobody"), brings improvement in most cases, especially obtaining the most gain on Alpaca with Polynomial + BM25 examples compared with other prompts, correct or incorrect.These results indicate that LLMs might not really take the example-level descriptions into consideration during ICL.In other words, they might not necessarily care what users say in the descriptions.Compared with proper descriptions, it seems the format of prompts matters more.For example, on Alpaca with Random examples, no matter what the example-level descriptions say, all Ensemble prompts bring nearly equal improvement over Vanilla.This indicates that Ensemble is a superior format compared with Vanilla in this case.To sum up, the experimental results on MT suggest that a proper prompt format leads to better ICL performance of LLMs while a careful design of descriptions might be less effective.</p>
<p>Ensemble (Random + Random): The Ensemble prompt with random nouns in the example-level descriptions as described in Section 2.3.Single (Random): Organized based on Figure 1, but the second description is removed.There is only one example-level description above the four examples, where Noun A is a random noun.Single (Example): Organized based on Figure 1, but the second description is removed.There is only one example-level description above the four examples, being "Examples:" only, without any further descriptions.This prompt only informs the LLM that the following four instances are examples and does not describe their properties.Vanilla (Translate): Organized based on Figure 1, but both the two descriptions are removed.The only difference with Vanilla is the translation instruction "Translate the following sentence:" before the test input.This prompt only informs the LLM to translate the test input and tells nothing about the in-context examples.Detailed templates and examples of the above prompts are presented in Appendix A. Results are presented in Figure 5, showing that removing one or two example-level descriptions or removing the random noun describing the property of in-context examples hurt the performance gain in most cases.On XGLM, only the original Ensemble format performs better than Vanilla.Alpaca exhibits an abnormal trend when prompted with Polynomial and BM25 + Polynomial examples, where Ensemble (Random + Random) cannot outperform other prompts.This may be due to that Alpaca is instruction-tuned and the Single or Vanilla (Translate) prompts are also friendly to it in some cases because of the post-training stage.But overall, Single (Random), Single (Example) and Vanilla (Translate) still bring less improvement than Ensemble (Random + Random) in more than half of the cases.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Attention weights (×1e-4) on XGLM of all 32 layers with BM25 + Polynomial examples.EWS and ERR denotes Ensemble (Word + Syntax) and Ensemble (Random + Random) respectively.</p>
<p>Figure 8 :
8
Figure 8: Results of the four types of tasks on GPT-3.5.</p>
<p>Figure 13 :
13
Figure 13: Attention weights (×1e-4) on Alpaca of all 32 layers with BM25 + Polynomial examples.EWS and ERR denotes Ensemble (Word + Syntax) and Ensemble (Random + Random) respectively.</p>
<p>show the examples of ERR (w/ CoT) prompt for respective datasets.Some tasks contain Answer Choices.In order to save space, the blank lines between the options are replaced with spaces in those figures.Each Figure has grey text for reasoning, cyan text for the parts of ERR that are unique to Vanilla, and italic words in the cyan text representing random nouns.Therefore, deleting the grey text gives ERR (w/o CoT), keeping the grey text but deleting the cyan text gives Vanilla (w/ CoT), and deleting both the cyan and grey text gives Vanilla (w/o CoT).5The reasoning is generated by ChatGPT 6 .Note the ChatGPT is not the same as GPT-3.5 we used for experiments.</p>
<p>Figure 14 :
14
Figure 14: Prompt for CSQA.</p>
<p>Figure 15 :
15
Figure 15: Prompt for StrategyQA.</p>
<p>Figure 16 :
16
Figure 16: Prompt for Date.</p>
<p>Figure 17 :
17
Figure 17: Prompt for Sports.</p>
<p>Figure 18 :
18
Figure 18: Prompt for Logical Fallacy.</p>
<p>Figure 19 :
19
Figure 19: Prompt for Three Objects.</p>
<p>Figure 20 :
20
Figure 20: Prompt for Known Unknowns.</p>
<p>Figure 21 :
21
Figure 21: Prompt for GSM8K.For this dataset, we let LLMs first generate reasoning and then answer under the "w/ CoT" setting.</p>
<p>Figure 22 :
22
Figure 22: Prompt for AQuA.</p>
<p>Figure 23 :
23
Figure 23: Results of Llama2 on the nine datasets.</p>
<p>., Examples with similar Noun :
Prompt TemplatePrompt Example of AlpacaTask-level InstructionExamples with similar Noun 𝐴:Example 𝐴 !Example 𝐴 "Example 𝐵 !Example 𝐵 "Translate the following sentence:Test Input</p>
<p>Prompt Template Prompt Example of Alpaca
Task-level InstructionInstruction: Translate German into English. German: Diese vier sind wichtig.Example 𝐴 !English: These four are important. German: Diese fünf sind interessant.Example 𝐴 "English: These five are interesting.Example 𝐵 !German: Wer ist Martha? English: Who is Martha?Example 𝐵 "German: Was ist NLP? English: What is NLP?Test InputGerman: Was sind diese vier?English:</p>
<p>Table 1 :
1
Data statistics.
Language ISO CodeDataset#Pairs (M)GermanDEEuroparl1.8FrenchFREuroparl1.9RussianRUParaCrawl5.4</p>
<p>For convenient comparison, we present the performance gain of different Ensemble prompts over Vanilla with different selections of in-context examples and the results are averaged over six translation directions.For detailed results of different translation directions, please refer to Appendix B.
Example SelectionXGLM AlpacaRandom54.0755.42BM2555.0056.27Polynomial55.5256.13BM25 + Polynomial56.1756.18Polynomial + BM2556.1855.49
Table 2: Results of Vanilla baselines of XGLM and Alpaca with different example selection methods, averaged over six translation directions.Main results are shown in Figure4.</p>
<p>. Details of splitting training set (example database) and test set are in Appendix D.
DatasetTest InputsCSQA1221StrategyQA1012Date365Sports996Logical Fallacy1012Three Objects296Known Unknowns42GSM8K1319AQuA254</p>
<p>Table 3 :
3
Number of test inputs for each dataset.</p>
<p>Table 4 :
4
Full MT results of XGLM.</p>
<p>(Talmor et al., 2019) 2019): https://www.tau-nlp.org/commonsenseqa.We follow the official split and select the training set as our example database and the dev set as our test set.Because the training set itself is randomly divided from the whole dataset, we directly select examples from it in order.
• GSM8K(Cobbeetal.,2021):https://github.com/openai/grade-school-math.We select thetest.jsonl as our test set and thetrain.jsonl as our example database andrandomly sample four examples from it.• AQuA (Ling et al., 2017): https://github.com/google-deepmind/AQuA. We selectthe test.json as our test set. Since the orig-inal training set is relatively large, for simplic-ity, we directly copy the four examples listedin the supplementary materials of Wei et al.(2022) and we ensure that these four examplesdo not appear in the test set.</p>
<p>Table 7 :
7
Licenses of scientific artifacts we use.
ArtifactLicenseXGLMMITAlpacaApache-2.0LlamaLlama Community License AgreementMistralApache-2.0COMETApache-2.0FLORES-101 CC-BY-SA-4.0EuroparlUnknownParaCrawlCC0CSQACC-BY-SA-4.0StrategyQAMITBIG-benchApache-2.0GSM8KMITAQuAApache-2.0
https://huggingface.co/Unbabel/wmt20-comet-da
When changing "w/ CoT" to "w/o CoT", you may also need to replace "So the answer is ..." with "The answer is ..." for syntactical reasons.
https://chatgpt.com/
AcknowledgmentsThis work is supported by the National Natural Science Foundation of China (62076008).A Prompts for MT Ablation StudyB Full Experimental ResultsFull results of MT are presented in Table4 and 5. Full results of other tasks in Section 3.2 are presented in Table6.C Attention Weights on AlpacaFigure13presents the attention weights on Alpaca.For example-to-noun attention weights, ERR is
Incontext examples selection for machine translation. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>ParaCrawl: Web-scale acquisition of parallel corpora. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L Forcada, Faheem Amir Kamran, Philipp Kirefu, Sergio Ortiz Koehn, Leopoldo Pla Rojas, Gema Sempere, Elsa Ramírez-Sánchez, Marek Sarrías, Strelec, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBrian Thompson, William Waites, Dion Wiggins2020</p>
<p>retriv: A Python Search Engine for the Common Man. Elias Bassani, 2023</p>
<p>Kathleen McKeown, and He He. 2024. Parallel structures in pretraining data yield in-context learning. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei ; Chen, Zhou Zhao, Yu, 10.18653/v1/2024.acl-long.465Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics202033Advances in Neural Information Processing Systems</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.141682021Preprint</p>
<p>Susan Craw, 10.1007/978-1-4899-7687-1_511Manhattan distance. Encyclopedia of Machine Learning and Data Mining. 2017</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, arXiv:2301.002342023Preprint</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc ' , Aurelio Ranzato, Francisco Guzmán, Angela Fan, 2022Transactions of the Association for Computational Linguistics</p>
<p>In-context learning creates task vectors. Roee Hendel, Mor Geva, Amir Globerson, 10.18653/v1/2023.findings-emnlp.624Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Proceedings of the Sixth Conference on Machine Translation. the Sixth Conference on Machine Translation</p>
<p>Europarl: A parallel corpus for statistical machine translation. Philipp Koehn, Proceedings of Machine Translation Summit X: Papers. Machine Translation Summit X: Papers2005</p>
<p>CTQScorer: Combining multiple features for in-context example selection for machine translation. Aswanth Kumar, Ratish Puduppully, Raj Dabre, Anoop Kunchukuttan, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Unified demonstration retriever for incontext learning. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu, 10.18653/v1/2023.acl-long.256Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O' Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingMona Diab</p>
<p>Program induction by rationale generation : Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017Preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.092882023Preprint</p>
<p>The llama 3 herd of models. arXiv:2407.217832024Preprint</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, arXiv:2209.11895Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah2022Preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 202235</p>
<p>Unbabel's participation in the WMT20 metrics shared task. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao, arXiv:2304.03277Proceedings of the Fifth Conference on Machine Translation. Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie, the Fifth Conference on Machine Translation2023. 2020PreprintInstruction tuning with gpt-4</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>In-context learning as maintaining coherency: A study of on-thefly machine translation using large language models. Suzanna Sia, Kevin Duh, Proceedings of Machine Translation Summit XIX. Research Track, Machine Translation Summit XIXMacau SAR, China20231</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Transactions on Machine Learning Research. 2023</p>
<p>NICE: To optimize incontext examples or not?. Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma, 10.18653/v1/2024.acl-long.300Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>SCOI: Syntax-augmented coverage-based incontext example selection for machine translation. Chenming Tang, Zhixiang Wang, Yunfang Wu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Transformers learn in-context by gradient descent. Johannes Von, Oswald , Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov, International Conference on Machine Learning. PMLR2023</p>
<p>Label words are anchors: An information flow perspective for understanding in-context learning. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun, 10.18653/v1/2023.emnlp-main.609Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, ; Jerry Zhou, Jason Wei, Yi Wei, Dustin Tay, Albert Tran, Yifeng Webson, Xinyun Lu, Hanxiao Chen, Da Liu, Denny Huang, Tengyu Zhou, Ma, arXiv:2303.03846Advances in Neural Information Processing Systems. Curran Associates, Inc2022. 202335Larger language models do in-context learning dif-Preprint</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2022</p>
<p>Compositional exemplars for in-context learning. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202</p>
<p>Instruction tuning for large language models: A survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, arXiv:2308.107923.01 50.99 21.70 50.20 48.72 30.74 22.29 21.65 45.24 32.732024PreprintLLaMA-2-7B Vanilla w/ CoT</p>
<p>LLaMA-3.1-8B Vanilla w/ CoT 9. 04 51.28 24.41 55.32 52.08 35.47 49.05 24.80 69.05 41.17</p>
<p>Alpaca-7B Vanilla w/ CoT. 26.03 60.38 50.53 80.12 52.27 35.81</p>
<p>StrategyQA (SgyQA). CSQA, Sports Understanding (Sports), Logical Fallacy (LF), Three Objects (TO), GSM8K, AQuA and Known Unknowns. KU6Full results of Date Understanding (Date)</p>            </div>
        </div>

    </div>
</body>
</html>
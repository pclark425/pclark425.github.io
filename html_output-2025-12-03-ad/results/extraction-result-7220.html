<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7220 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7220</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7220</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-270370866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05343v2.pdf" target="_blank">M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark</a></p>
                <p><strong>Paper Abstract:</strong> As recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence. However, existing benchmarks mainly focus on evaluating solely on task performance, such as the accuracy of identifying the attribute of an object. Combining well-developed cognitive science to understand the intelligence of MLLMs beyond superficial achievements remains largely unexplored. To this end, we introduce the first cognitive-driven multi-lingual and multi-modal benchmark to evaluate the general intelligence ability of MLLMs, dubbed M3GIA. Specifically, we identify five key cognitive factors based on the well-recognized Cattell-Horn-Carrol (CHC) model of intelligence and propose a novel evaluation metric. In addition, since most MLLMs are trained to perform in different languages, a natural question arises: is language a key factor influencing the cognitive ability of MLLMs? As such, we go beyond English to encompass other languages based on their popularity, including Chinese, French, Spanish, Portuguese and Korean, to construct our M3GIA. We make sure all the data relevant to the cultural backgrounds are collected from their native context to avoid English-centric bias. We collected a significant corpus of data from human participants, revealing that the most advanced MLLM reaches the lower boundary of human intelligence in English. Yet, there remains a pronounced disparity in the other five languages assessed. We also reveals an interesting winner takes all phenomenon that are aligned with the discovery in cognitive studies. Our benchmark will be open-sourced, with the aspiration of facilitating the enhancement of cognitive capabilities in MLLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7220.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7220.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (overall M3GIA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o evaluated on the M3GIA benchmark (overall accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source multimodal large language model evaluated on the full M3GIA cognitive benchmark; reported overall accuracy compared to human baseline collected by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal large language model from OpenAI; used in this paper's evaluation suite as an API/closed model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>M3GIA overall accuracy (composite across five cognitive factors)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>general intelligence / multiple cognitive domains (reasoning, knowledge, visual-spatial, reading/writing, quantitative)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Composite multiple-choice benchmark (M3GIA) of 1,800 items spanning five CHC-derived cognitive factors (Gc, Grw, Gq, Gf, Gv) and 18 narrow task types; evaluated in a zero-shot setting on language-specific subquestionnaires.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 76.9%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 59.8%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (with prompt engineering on validation set)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human baseline collected by the authors: 480 participants via electronic questionnaires (80 per language), native speakers of six languages</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors report that GPT-4o's overall accuracy (59.8%) is substantially below the human average (76.9%). Models were evaluated zero-shot; prompt engineering was applied on validation. No p-values or formal hypothesis tests comparing model vs human overall accuracy are reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7220.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7220.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro (overall & Gv)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5-Pro evaluated on M3GIA (overall accuracy and Visual-Spatial ability Gv)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source multimodal model (Gemini family) evaluated on M3GIA; specific values reported for overall accuracy and for the Visual-Spatial (Gv) factor compared to human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal large language model (Gemini family) accessed via API; evaluated by authors on M3GIA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>M3GIA overall accuracy and Visual-Spatial (Gv) subtest(s)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>general intelligence / visual-spatial processing</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Overall: composite M3GIA accuracy; Gv: visual-spatial cluster including visualization (block rotation, spatial relations), picture recognition, and real-world spatial problems requiring visual mental imagery and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>overall accuracy 76.9%; Visual-Spatial (Gv) accuracy 81.1%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>overall accuracy 62.4%; Visual-Spatial (Gv) accuracy 53.8%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (with prompt engineering on validation set)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human baseline collected by the authors: 480 participants via electronic questionnaires (80 per language), native speakers of six languages</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors highlight a large gap in visual-spatial abilities: Gemini-1.5-Pro Gv (53.8%) vs human (81.1%). Overall accuracy for Gemini-1.5-Pro (62.4%) is reported near but below the authors' 'passing line' of 60 and below the human average. No formal significance tests reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7220.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7220.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (Deductive reasoning RG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o performance on Deductive Reasoning (RG) subtests of M3GIA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on the deductive-reasoning (RG / syllogism and real-world deductive tasks) portion of the M3GIA benchmark; compared to human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal large language model from OpenAI; evaluated on M3GIA reasoning subtests.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Deductive Reasoning (RG) subtests (syllogism, real-world deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Classic syllogism problems and real-world deductive reasoning tasks that require stepwise logical inference from given premises; part of the Fluid Reasoning (Gf) factor in M3GIA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 60.0% (reported average for RG in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 59.2%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (with prompt engineering on validation set)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human baseline collected by the authors: 480 participants via electronic questionnaires (80 per language), native speakers of six languages</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors note GPT-4o approaches the average human level on deductive reasoning (59.2% vs 60.0%) and suggest this may be due to use of synthetic reasoning data in training; however, no formal statistical tests comparing model and human RG scores are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Cogbench: a large language model walks into a psychology lab. <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 2)</em></li>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7220",
    "paper_id": "paper-270370866",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4o (overall M3GIA)",
            "name_full": "GPT-4o evaluated on the M3GIA benchmark (overall accuracy)",
            "brief_description": "Closed-source multimodal large language model evaluated on the full M3GIA cognitive benchmark; reported overall accuracy compared to human baseline collected by the authors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal large language model from OpenAI; used in this paper's evaluation suite as an API/closed model.",
            "model_size": null,
            "test_name": "M3GIA overall accuracy (composite across five cognitive factors)",
            "test_category": "general intelligence / multiple cognitive domains (reasoning, knowledge, visual-spatial, reading/writing, quantitative)",
            "test_description": "Composite multiple-choice benchmark (M3GIA) of 1,800 items spanning five CHC-derived cognitive factors (Gc, Grw, Gq, Gf, Gv) and 18 narrow task types; evaluated in a zero-shot setting on language-specific subquestionnaires.",
            "evaluation_metric": "accuracy (percentage)",
            "human_performance": "accuracy 76.9%",
            "llm_performance": "accuracy 59.8%",
            "prompting_method": "zero-shot (with prompt engineering on validation set)",
            "fine_tuned": false,
            "human_data_source": "Human baseline collected by the authors: 480 participants via electronic questionnaires (80 per language), native speakers of six languages",
            "statistical_significance": null,
            "notes": "Authors report that GPT-4o's overall accuracy (59.8%) is substantially below the human average (76.9%). Models were evaluated zero-shot; prompt engineering was applied on validation. No p-values or formal hypothesis tests comparing model vs human overall accuracy are reported in the main text.",
            "uuid": "e7220.0",
            "source_info": {
                "paper_title": "M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro (overall & Gv)",
            "name_full": "Gemini-1.5-Pro evaluated on M3GIA (overall accuracy and Visual-Spatial ability Gv)",
            "brief_description": "Closed-source multimodal model (Gemini family) evaluated on M3GIA; specific values reported for overall accuracy and for the Visual-Spatial (Gv) factor compared to human baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Pro",
            "model_description": "Closed-source multimodal large language model (Gemini family) accessed via API; evaluated by authors on M3GIA.",
            "model_size": null,
            "test_name": "M3GIA overall accuracy and Visual-Spatial (Gv) subtest(s)",
            "test_category": "general intelligence / visual-spatial processing",
            "test_description": "Overall: composite M3GIA accuracy; Gv: visual-spatial cluster including visualization (block rotation, spatial relations), picture recognition, and real-world spatial problems requiring visual mental imagery and spatial reasoning.",
            "evaluation_metric": "accuracy (percentage)",
            "human_performance": "overall accuracy 76.9%; Visual-Spatial (Gv) accuracy 81.1%",
            "llm_performance": "overall accuracy 62.4%; Visual-Spatial (Gv) accuracy 53.8%",
            "prompting_method": "zero-shot (with prompt engineering on validation set)",
            "fine_tuned": false,
            "human_data_source": "Human baseline collected by the authors: 480 participants via electronic questionnaires (80 per language), native speakers of six languages",
            "statistical_significance": null,
            "notes": "Authors highlight a large gap in visual-spatial abilities: Gemini-1.5-Pro Gv (53.8%) vs human (81.1%). Overall accuracy for Gemini-1.5-Pro (62.4%) is reported near but below the authors' 'passing line' of 60 and below the human average. No formal significance tests reported.",
            "uuid": "e7220.1",
            "source_info": {
                "paper_title": "M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4o (Deductive reasoning RG)",
            "name_full": "GPT-4o performance on Deductive Reasoning (RG) subtests of M3GIA",
            "brief_description": "GPT-4o evaluated on the deductive-reasoning (RG / syllogism and real-world deductive tasks) portion of the M3GIA benchmark; compared to human baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal large language model from OpenAI; evaluated on M3GIA reasoning subtests.",
            "model_size": null,
            "test_name": "Deductive Reasoning (RG) subtests (syllogism, real-world deductive reasoning)",
            "test_category": "reasoning / deductive reasoning",
            "test_description": "Classic syllogism problems and real-world deductive reasoning tasks that require stepwise logical inference from given premises; part of the Fluid Reasoning (Gf) factor in M3GIA.",
            "evaluation_metric": "accuracy (percentage)",
            "human_performance": "accuracy 60.0% (reported average for RG in paper)",
            "llm_performance": "accuracy 59.2%",
            "prompting_method": "zero-shot (with prompt engineering on validation set)",
            "fine_tuned": false,
            "human_data_source": "Human baseline collected by the authors: 480 participants via electronic questionnaires (80 per language), native speakers of six languages",
            "statistical_significance": null,
            "notes": "Authors note GPT-4o approaches the average human level on deductive reasoning (59.2% vs 60.0%) and suggest this may be due to use of synthetic reasoning data in training; however, no formal statistical tests comparing model and human RG scores are reported.",
            "uuid": "e7220.2",
            "source_info": {
                "paper_title": "M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Cogbench: a large language model walks into a psychology lab.",
            "rating": 2,
            "sanitized_title": "cogbench_a_large_language_model_walks_into_a_psychology_lab"
        },
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 1,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        }
    ],
    "cost": 0.013728,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark
14 Jun 2024</p>
<p>Wei Song songwei@westlake.edu.cn 
AutoLab
Westlake University</p>
<p>Alibaba Group
AI Business</p>
<p>Zhejiang University</p>
<p>Yadong Li 
Alibaba Group
AI Business</p>
<p>Jianhua Xu 
Alibaba Group
AI Business</p>
<p>Guowei Wu 
Institute of Psychology
Key Laboratory of Behavioral Science
CAS</p>
<p>† Lingfeng 
Alibaba Group
AI Business</p>
<p>Kexin Yi 
Alibaba Group
AI Business</p>
<p>Weihua Luo 
Alibaba Group
AI Business</p>
<p>Houyi Li 
Yi Du 
Alibaba Group
AI Business</p>
<p>Institute of Psychology
Key Laboratory of Behavioral Science
CAS</p>
<p>Fangda Guo 
Institute of Computing Technology
Key Laboratory of AI Safety
CAS</p>
<p>Kaicheng Yu 
AutoLab
Westlake University</p>
<p>M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark
14 Jun 2024EFCB37EF6DF959BF557B20907D6AF546arXiv:2406.05343v2[cs.AI]
As recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence.However, existing benchmarks mainly focus on evaluating solely on task performance, such as the accuracy of identifying the attribute of an object.Combining well-developed cognitive science to understand the intelligence of MLLMs beyond superficial achievements remains largely unexplored.To this end, we introduce the first cognitive-driven multi-lingual and multi-modal benchmark to evaluate the general intelligence ability of MLLMs, dubbed M3GIA.Specifically, we identify five key cognitive factors based on the well-recognized Cattell-Horn-Carrol (CHC) model of intelligence and propose a novel evaluation metric.In addition, since most MLLMs are trained to perform in different languages, a natural question arises: is language a key factor influencing the cognitive ability of MLLMs?As such, we go beyond English to encompass other languages based on their popularity, including Chinese, French, Spanish, Portuguese and Korean, to construct our M3GIA.We make sure all the data relevant to the cultural backgrounds are collected from their native context to avoid English-centric bias.We collected a significant corpus of data from human participants, revealing that the most advanced MLLM reaches the lower boundary of human intelligence in English.Yet, there remains a pronounced disparity in the other five languages assessed.We also reveals an interesting winner takes all phenomenon that are aligned with the discovery in cognitive studies.Our benchmark will be open-sourced, with the aspiration of facilitating the enhancement of cognitive capabilities in MLLMs.</p>
<p>Introduction</p>
<p>In 1956, researchers across different domains, including mathematics, cognitive psychology and computer science, pointed out an interesting direction, dubbed artificial intelligence (AI).The formal definition is "The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."[33].Through extensive efforts in pursuing artificial intelligence, the field has converged to a paradigm of data-driven machine learning models, which are still deeply intertwined with cognitive science as they often mirror basic cognitive mechanisms, e.g.convolutional neural In contrast to traditional benchmarks that focus on evaluating specific task performances, we draw inspiration from cognitive science to categorize five cognitive factors, try to provide a feasible evaluation of general intelligence ability (GIA).(Right) Specifically, we adopt the factors from the CHC theory to disentangle fundamental cognitive abilities with existing evaluation tasks.In addition, to further understand how language impacts such ability, we collect or design questions in six languages with large population.networks [23] and the attention mechanism [50].Recent advances, such as GPT-4o [36], demonstrate that these MLLMs can outperform human on various complex tasks [1,53] and shed light to emergent ability with the increasing scale of data and model size [55].In light of these developments, our aim is to evaluate these state-of-the-art models through the lens of cognitive science, as it directly aligns with the primary motivation of AI research.</p>
<p>To explore the mental intelligence emerging from these large models, efforts have been directed toward analyzing these models from a psychological perspective.Some pioneering works report that LLMs have demonstrated human-like cognition [6,22].For instance, Theory of mind (ToM) has been applied to assess large models, revealing that GPT-4 exhibits ToM capabilities similar to human inference patterns [7,22,20].Meanwhile, Multimodal Large Language Models (MLLMs), which use powerful LLMs as brain to process and integrate multimodal information, have exhibited impressive emergent abilities, such as generating website code from images [65], understanding the meaning of a meme [57], and math reasoning [16].Thanks to their ability to process information from a broader spectrum of sources, they exhibit a more holistic cognitive process, resembling human cognition more closely than models confined to purely linguistic input.</p>
<p>Existing multi-modality benchmarks, such as MMBench [28], MME [19], and MM-Vet [61], have made the attempt to compartmentalize model capabilities across multiple tasks.For instance, MM-Bench covers 20 different abilities, encompassing function reasoning, physical property reasoning, object localization and social reasoning.However, they often fail to provide a persuasive explanation for their selection of dimensions, as they tend to be mired in subjectivity and lack a solid theoretical underpinning.Moreover, as depicted in Figure 1 (left), their ability dimensions are still rather task-oriented, neglecting a systematic evaluation of the models' underlying cognitive abilities that govern task performance through the lens of cognitive science.This oversight raises concerns that benchmarks might devolve into mere training targets rather than instruments for true insight, failing to provide a holistic measure of the models' capabilities [41].In short, the ability to solve specific tasks is insufficient to reflect the true level of intelligence, as supported by a psychological study [37], and formally evaluating the cognitive factors of MLLMs remains largely unexplored.</p>
<p>In this paper, we close the gap by introducing the first benchmark that comprehensively evaluate the cognitive abilities of MLLMs under the theoretical umbrella of the well-recognized Cattell-Horn-Carroll (CHC) Model of Intelligence [42], dubbed M3GIA.As in Figure 1(right), based on the CHC Model, we categorizes the cognitive capacities of current MLLMs into five dimensions: Fluid reasoning (Gf), Comprehension-Knowledge (Gc), Visual processing (Gv), Reading and Writing (Grw), Quantitative knowledge (Gq), and collect corresponding questions as a measurement.In addition, as using multi-lingual data to scale up the capability of MLLMs becomes a de-facto standard, we are curious whether languages make any impact on their cognitive abilities.As such, we extend our benchmark to include five more languages, including Chinese, Spanish, French, Portuguese and Korean roughly based on their population, to disentangle the language factor with cognitive ability.</p>
<p>To evenly assess the five cognitive dimensions, we refer to human intelligence tests, such as Raven's Progressive Matrices Test [38] and the Woodcock-Johnson IV Tests of Cognitive Abilities (WJ IV) [44], and establish broad question types that correspond to the these cognitive dimensions, which are further subdivided into 18 narrow question types (see later Sec.3).All in all, our M3GIA contains 1,800 questions, where over half are carefully designed from scratch following the standard.The test for each language maintain consistency in terms of the number of questions, structure, and distribution of question types.In addition, to highlight the multilingual nature of our benchmark, we collect data relevant to cultural backgrounds from native language sources rather than simply translating them from English, thereby avoiding the English-centric bias.</p>
<p>We evaluate 24 MLLMs, including the state-of-the-art close and open-sourced ones.In general, The latest advancements in MLLMs have achieved performance levels that fall within the lower boundary of human intelligence in English.Yet, there remains a pronounced disparity in the other five languages assessed.We also notice that MLLMs' proficiency in one cognitive domain often translates into superior performance across other domains as well.This phenomenon interestingly aligns with the pattern observed in human intelligence which empirically suggests the existence of General Intelligence Ability (GIA) in MLLMs.</p>
<p>Related works</p>
<p>Evaluation Benchmark for MLLMs.As multimodal large language models (MLLMs) exhibit remarkable generalization capabilities across a broad spectrum of downstream tasks, relying exclusively on their performance within single vision-language tasks -such as visual recognition [21], image description [11,2,60], scene text understanding [47,46], and external knowledge [32] is insufficient to fully uncover the comprehensive performance of MLLMs.People then turn to a new paradigm to construct all-round benchmarks to assess a broader spectrum of challenging multimodal tasks [58,56,25,28,19,61].Another trend in MLLM assessment is the use of human exam questions [31,30,64,62,63].For instance, AGIEval [64] sources questions from standardized exams such as college entrance exams and lawyer qualification tests.While these benchmarks makes progresses in evaluating the human-centric ability of MLLMs, it may not be suitable to evaluate the intelligence of MLLMs because research in psychological field points out that the superficial performance on tasks alone cannot be a solid indicator for human's intelligence.[37] General Intelligence Ability and the CHC Theory.Arising from the empirical fact that an individual's proficiency in one area frequently correlates with high performance in other areas, Charles Spearman first introduced General Intelligence Ability (GIA) in 1904 [48].This construct refers to the idea that a single underlying factor, known as the g-factor, can account for the positive correlations among cognitive abilities and reflect the general intelligence that fundamentally underlies an individual's intelligence.To concretely understand GIA, numerous attempts has been made to model the structure of human cognition.John Carroll's Three-Stratum Model [9] elaborated on this with a hierarchical structure of intelligence, including a general "g" factor and specific cognitive abilities.Howard Gardner's Multiple Intelligence Theory [18] proposed diverse forms of intelligence, while Sternberg's Triarchic Theory [49] focused on practical, creative, and analytical aspects.These theories collectively contributed to the development of the Cattell-Horn-Carroll (CHC) model of intelligence, which is the most comprehensive and empirically validated structural model of human cognition [35] to date, integrating various aspects of cognition into a unified framework.</p>
<p>Recent study [14], has primarily focused on evaluating the performance of large language models on sophisticated psychological tasks, neglecting the assessment of models' intelligence from the foundational standpoint of cognitive models.Our M3GIA constitutes the first attempt to bring the latest cognitive science modeling of intelligence into MLLMs evaluation to address this gap.</p>
<p>M3GIA</p>
<p>Concretely, we introduce the first cognition inspired multi-linguistic and multi-modal benchmark to evaluate the general intelligence accuracy of large models.In short, our M3GIA distinguishes itself from existing benchmarks as follow:</p>
<p>Gc</p>
<p>Gq</p>
<p>Grw Gf Gv</p>
<p>I RG RQ</p>
<p>Acquired Knowledge Reasoning Sensory</p>
<p>Intelligence of MLLMs</p>
<p>Grw</p>
<p>Reading-text Reading-VL</p>
<p>Gc</p>
<p>General Information Oral Vocabulary</p>
<p>Gq</p>
<p>Math Facts • Cognition Inspired: In contrast to existing benchmarks that focuses on task-level evaluation, we study the intelligence of large models from a cognition perspective.The benchmark dissects the cognitive abilities of contemporary MLLMs into five foundational factors, as per the Cattell-Horn-Carroll theory.This cognitive theory underpins the structure of our evaluation, informing the specific types of questions devised to test each cognitive skill.• Multilingual Coverage: To comprehensively measure the cognitive abilities of multimodal large models across multiple languages, M3GIA is constructed to span six languages: English, French, Chinese, Spanish, Portuguese, and Korean.In order to mitigate English-centric bias, all data relevant to cultural backgrounds have been sourced from native language resources, except for questions that transcend cultural considerations-such as the Raven test and number series problems.</p>
<p>The subsequent content of this section is organized as follows: In sec.3.1, we introduce the five-factor cognitive model of M3GIA and discuss the design philosophy behind it.In sec.3.2, we describe how we designed and collected the questions for M3GIA and provide some statistical data on M3GIA.</p>
<p>The Five-factor Cognitive Model of M3GIA</p>
<p>To formally study the large models intelligence level, we start from the state-of-the-art cognitive model, Cattell-Horn-Carroll (CHC) [42], which is by far the most empirically validated structure model of human cognition [35].The CHC theory articulates a hierarchical framework of human cognitive abilities divided into three strata: general intelligence "g" (stratum III), broad cognitive abilities (stratum II), and narrow cognitive abilities (stratum I).The theory has now expanded to include 16 broad abilities and over 80 narrow abilities.These broad but domain-specific abilities are nevertheless positively associated with one another.This positive manifold is accounted for in the CHC model by a general factor of intelligence ("g") at stratum III.While there is ongoing discourse regarding the exact delineation of the narrow abilities, 9 out of the 16 broad cognitive abilities have achieved substantial consensus and are well-supported by empirical evidence and practical application.[8] These include Fluid Reasoning (Gf), Comprehension-Knowledge (Gc), Visual Processing (Gv), Auditory Processing (Ga), Short-term Memory (Gsm), Long-term Retrieval (Glr), Processing Speed (Gs), Quantitative Knowledge (Gq), and Reading and Writing Abilities (Grw).</p>
<p>As shown in Fig. 2, the structure of our M3GIA is underpinned by the five-factor hierarchical cognitive model, which is derived from the CHC model of cognitive abilities.Although Large Language Models exhibit cognitive processes similar to humans, they also differ in internal mechanisms, particularly with regard to processing speed (Gs) and memory (Gwm, Glr), which is greatly related to external technologies beyond the model itself, such as external databases and retrieval-augmented generation (RAG) [24].Additionally, given that the majority of current MLLMs, with the exception of a select</p>
<p>General Info. |Gc</p>
<p>Question: Where can you find the building featured on this note?</p>
<p>Options:
[A] Washington DC [B] London [C] Philadelphia [D] Atlanta</p>
<p>Readings |Grw</p>
<p>Question: Which image best describes the structure of this passage?Article： P1: Some people learn a second language easily.Others have trouble learning a new language.How can you learn a new language, such as English?There are several ways to help you learn English more easily.P2: Firstly, feel positive about learning English.If you believe that you can learn, you will learn.Be patient.</p>
<p>You don't have to understand everything all at once.Often you will make mistakes when you are learning something new.We can learn from our mistakes.P3: Try to practice using your English as possible as you can.For example, you can write a diary every day.Soon, you will get used to writing your ideas in English.After several weeks, your writing skills will improve.Besides, try to speak English every day.You can practice speaking with your classmates after class.You might make mistakes, but don't worry.Slowly, you will become comfortable communicating in English.P4: It's a great idea to keep a record of your language learning experience.You can write your learning experience in your diary.After each class, think about it.Do you answer questions correctly in class?Do you understand your teacher?Perhaps the lesson is a bit difficult, but you can try to understand it.Write these reflections in your diary to practice using your English and write down your little progress.Finally, you will find yourself enjoy learning English.P5: All in all, be positive, confident and patient --believe that you can make it sooner or later.Make good use of all the time we can get.</p>
<p>Options:
[A] [B] [C] [D]</p>
<p>Oral Vocabulary |Gc</p>
<p>Question: Please choose the word which best expresses the meaning of the given word.</p>
<p>Brief:
[A] Limited [B] Small [C] Short [D] Little ACORDADO (Portuguese): [A] Iluminado [B] Percebido [C] Abalado [D] Despertado</p>
<p>Algebra |Gq RQ</p>
<p>Question: Options:</p>
<p>In LaTeX format</p>
<p>Translation:
[A] 16 [B] 16\sqrt{2} [C] 32 [D] 32\sqrt{2} [E] 64</p>
<p>Concept Formation |I</p>
<p>Options:
[A] A [B] B [C] C [D] D A B C D E
Question: Observe the pattern of the following figures and identify the one that does not belong to the same category as the others.</p>
<p>Logo Problem |Gc Gv</p>
<p>Options:</p>
<p>Raven's Matrices |I</p>
<p>Options:
[A] [B] [C] [D]
Question:</p>
<p>Please select the correct tile from the four options to complete the general pattern in the 3x3 matrix.</p>
<p>Visualization |Gv</p>
<p>Sub-test 1: ... Please choose the 3D block that shows the target 3D block rotated in space.</p>
<p>Sub-test 2: …</p>
<p>Which set of patterns can be combined to make the target figure?</p>
<p>Comic Problem |Grw Gv</p>
<p>Options (in French):</p>
<p>[A] Parce que la jeune fille n'était pas d'accord avec le contenu de la note.</p>
<p>[B] Parce que la fille a cassé son cookie.</p>
<p>[C] Parce que la fille a pris la note au pied de la lettre et a épousé le cookie.</p>
<p>[D] Parce que la fille est tombée amoureuse d'autres hommes.</p>
<p>Question:</p>
<p>Pourquoi le garçon était-il triste à la fin ?</p>
<p>Translation: Why is the boy sad at the end?few closed-source models, are not yet expanded to embrace the auditory modality, we have not included the Ga (Auditory Processing) factor in this version of M3GIA, reserving it as one of the directions for future expansion.Consequently, based on the consultations with psychology experts, we have chosen to assess the cognitive abilities of current MLLMs in this iteration of M3GIA by focusing on five key CHC factors: Gc, Grw, Gq, Gf, and Gv, from among the nine most frequently identified CHC factors.</p>
<p>Interestingly, the five factors we select align closely with those of the renowned Stanford-Binet Test, Fifth Edition (SB5) [40], which was also constructed upon five cognitive factors derived from the CHC theory.Specifically, the five cognitive factors identified in the SB5 are: Fluid Reasoning (FR), Knowledge (KN), Quantitative Reasoning (QR), Visual-Spatial Processing (VS), and Working Memory (WM).Except for Working Memory (WM), which we have substituted with Grw, these factors align directly with our selected factors, corresponding to Gf, Gc, Gq, and Gv, respectively.This alignment is noteworthy, as the selection of these factors for the SB5 was based on extensive research on school achievement and expert ratings of the importance of these factors in the assessment of reasoning, especially in giftedness assessment [39].</p>
<p>Question Design and Collection</p>
<p>Our M3GIA contains a total of 1,800 multiple choice problems, of which 1,200 are Visual Question Answering (VQA) questions.The ground truths for 630 questions are human-annotated, while the remainder of the answers for 1,170 questions were gathered from the Internet.</p>
<p>As shown in Fig. 1, we have devised five broad question clusters: reasoning, visual-spatial, common sense, mathematics and comprehension, separately corresponding to the assessment of the five CHC cognitive factors -Gf, Gv, Gc, Gq, and Grw.See Appendix for more details.To prevent the assessment of any particular ability from being constrained to a fixed and singular perspective, we have stratified each of the five clusters into 2-4 narrow question types that reflect different perspectives on a broad CHC construct.This subdivision results in a total of 18 distinct question types, each designed to tap into different facets of the ability being measured.Consequently, any generalizations that are made from a cluster are based on two or more samples of ability, which reduces the possibility of making over-generalizations from a narrow sampling of ability.</p>
<p>Moreover, as illustrated in the right part of Fig. 2, the five cognitive factors are not isolated but rather overlap with each other.For example, Fluid reasoning (Gf) not only has a process facet (inductive vs. deductive reasoning) but also has a content facet (verbal, spatial, and quantitative), each of which overlaps with other broad abilities.[43].In order to conduct a comprehensive measurement of this overlapping nature, our narrow question types include not only tests that measure each cognitive factor individually but also cover the parts where these factors overlap.The corresponding relationships between the question types, the cognitive factors and their intersections are also shown in Fig. 2.</p>
<p>What is more, to ensure that our assessment remains anchored in reality, we incorporate real-world problems into the evaluations of cognitive abilities.Specifically, each broad question type includes not only abstract cognitive test questions but also typical real-world problems that require the use of one or more cognitive abilities.This approach enables us to conduct a more accurate and practical assessment of how well these abilities are applied outside controlled, test-like environments.To ensure a balanced and comprehensive evaluation for each ability, we have tried our best to maintain an even distribution among problems associated with different abilities during data collection.</p>
<p>Examples of the narrow question types can be seen in Fig. 3, while more detailed descriptions are included in the Appendix.</p>
<p>Metrics</p>
<p>We use two type of metrics in our evaluation benchmark.For each narrow question type, we follow the existing benchmarks [28,19] to use accuracy.However, to holistically compare the cognitive ability, we design a novel metric general intelligence accuracy (GIA) based on findings in cognitive field.To compute the GIA scores of the models and validate the consistency of the cognitive structure between MLLMs and human intelligence, we adopted a standard psychometric approach.This involved utilizing a confirmatory factor analysis (CFA) model, developed from our collected human evaluation data.For more details about the CFA process, see Appendix for more details.</p>
<p>Evaluation Results</p>
<p>In this section, we evaluate a total of 24 MLLMs and 480 human participants using our M3GIA.The MLLMs comprise both closed-source models, such as GPT-4o [36], and open-source models [27,26,59,4,52,29,12], including LLaVA [27] and Mini-Gemini [26].Our evaluation for the MLLMs is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark.For all models, we conduct prompt engineering on the validation set and use the most effective prompt for the zero-shot setup in the experiments.All experiments are conducted with NVIDIA A800 GPUs [27,26].Human Performance Baseline.To establish a reference for human cognitive levels against MLLMs, we collected 480 valid sets of test data from human subjects using electronic questionnaires.These 480 participants were from native countries of the six selected languages, with 80 individuals per language.The 1,800 questions of M3GIA are then divided into six complete sub-questionnaires by language, with each individual only responsible for completing the sub-questionnaire corresponding to their native language.See supplementary for more details.</p>
<p>Accuracy Score on Five Cognitive Factors</p>
<p>We report the accuracy of each type of question for the 24 models alongside the average human performance for each cognitive ability in Table .1.We categorize the models into groups by their types, where open-source (OSS) MLLMs are grouped according to the size of their LLMs.It's observed that even the most advanced MLLMs only marginally meet the passing line (60) for overall accuracy, e.g., Gemini-1.5-Pro(62.4) / GPT-4o (59.8) vs human (76.9).Notably, these models excel in domains related to verbal skills and knowledge, such as Gc and Grw.This success can likely be attributed to the powerful language capabilities inherent in large language models, bolstered by their extensive training datasets.</p>
<p>However, a significant performance gap remains between MLLMs and humans in areas like Visual-Spatial Abilities (Gv) and Fluid Reasoning (Gf).This is particularly evident in the Visual-Spatial Abilities domain, where all models lag considerably behind human capabilities, e.g., Gemini-1.5-Pro(53.8) vs human (81.1).This underscores a substantial opportunity for advancements in the visual aspects of MLLMs.See supplementary for case studies.Furthermore, our findings also highlight a pronounced deficiency in the Fluid Reasoning (Gf) capability among all MLLMs, particularly in tasks involving Induction (I) and Quantitative Reasoning (RQ).However, it is surprising to note that in the domain of Deductive Reasoning (RG), the most advanced MLLMs, such as GPT-4o, are approaching the average human level with scores of 59.2 compared to 60.0 for human participants.This might be attributed to the strategy they use synthetic reasoning data to enhance such ability [13].</p>
<p>Overall, MLLMs perform well in crystallized intelligence (Gc), possibly owing to their extensive training data, while the most advanced MLLMs still have a large gap with humans in fluid intelligence.This proves that our benchmark M3GIA can measure the difference between crystallized intelligence and fluid intelligence of MLLMs from a cognitive perspective, which is the key difference between M3GIA and other benchmarks.Winner Takes All.More importantly, our finding reveals an intriguing Winner Takes All phenomenon that merits further attention beyond the initial observations.Specifically, we noted a consistent trend within each group of models where proficiency in one cognitive domain often translates into superior performance across other domains as well in Table 1.In particular, despite the diversity in score distribution among different abilities, there is a noteworthy pattern: the models achieving the top and second-best scores across various cognitive abilities are predominantly the same two models within each group.</p>
<p>This shows an interesting consistency to the pattern observed in human intelligence which empirically suggests the existence of General Intelligence Ability (see Sec. 2).Therefore, it offers compelling evidence that general intelligence ability, also identified as the general factor of intelligence ("g") at the stratum III of the CHC model, has also emerged in large models.Furthermore, it suggests that as MLLMs evolve towards more comprehensive cognitive processes, they too demonstrate a foundational GIA factor that simultaneously governs a variety of cognitive abilities.</p>
<p>Multilingual GIA Scores</p>
<p>By collecting a large amount of testing data from human subjects, we adoptted CFA (Confirmatory Factor Analysis) model to calculate the GIA scores which can reflect comprehensive intelligence factors.Since the questions for each language are not exactly the same, we need to establish a separate CFA model for each language.To our surprise, the model built with human data showed high explanatory validity for the test results of MLLMs (cor &gt; 0.93).This indicates, to some extent, that the cognitive structure of MLLMs indeed shows similarities to humans.We report the GIA scores of each language for some MLLMs of different sizes in Table. 2 and Fig. 5. It's observed that the current state-of-the-art MLLMs have reached the minimum level within the human subjects' confidence interval in English.However, these MLLMs still exhibit a significant performance gap compared to humans in other languages.</p>
<p>To further investigate the influence of LLM size to the GIA score, we conducted an ablation study with the Qwen series.In order to strictly control variables like different training data and ViT components.we trained the models by ourselves using the same training data for pretraining and fine-tuning and we also use the same ViT component (CLIP-ViT-L-14) in the series.Overall, the GIA scores of the models increase with the rise in LLM parameters.However, we observed a somewhat counterintuitive phenomenon.There is often no improvement in cognitive abilities from 7B to 13B, and seems to be a emerging point of General Intelligence Ability between 13B and 34B.</p>
<p>Conclusion</p>
<p>This paper has presented M3GIA, the first evaluation benchmark that comprehensively evaluate the cognitive abilities of MLLMs under the theoretical umbrella of the well-recognized Cattell-Horn-Carroll (CHC) Model of Intelligence.Based on the CHC theory, we identified five key cognitive factors for current MLLMs: Fluid reasoning (Gf), Comprehension-Knowledge (Gc), Visual processing (Gv), Reading and Writing (Grw), Quantitative knowledge (Gq), and designed five broad types of questions to measure them.In order to meet the pressing need for multilingual assessment, our evaluation data spans across six languages and are collected from native language sources, including English, Chinese, French, Spanish, Portuguese and Korean.We conducted a series of experiments to comparative analyze the cognitive abilities of various MLLMs against human performance, and discussed how factors like the size of LLM component impact cognitive abilities.</p>
<p>Limitations and Discussions</p>
<p>This version of M3GIA does not include all the broad cognitive factors of the CHC model, such as auditory processing (Ga), olfactory processing (Go), etc.As advancements in MLLMs incorporate a broader range of modalities, more factors from the CHC framework can be integrated, ensuring that M3GIA remains at the forefront of evaluating future generations of multimodal models.</p>
<p>A Dataset Documentation</p>
<p>A.1 Motivation M3GIA is a multimodal and multilingual benchmark designed to evaluate the cognitive abilities and general intelligence of MLLMs under the theoretical underpinning of human cognition.Instead of leveraging well-developed cognitive science to understand the intelligence of MLLMs beyond superficial achievements, existing benchmarks still mainly focus on evaluating solely on task performance.As described in the paper, these approaches have several limitations.We aim to bridge this gap through M3GIA, providing helpful insights into the development of artificial intelligence models with true intelligence.The creation of the dataset is funded by AI Business, Alibaba Group.</p>
<p>A.2 Composition</p>
<p>• M3GIA contains a total of 1,800 multiple-choice problems, of which 1,200 are Visual Question Answering (VQA), while the remaining 600 are textual questions.We ensure that all VQA tasks necessitate reliance on images for resolution and cannot be resolved with text alone (see Sec. D.2).</p>
<p>The ground truths for 630 questions are human-annotated, while the remainder of the answers for 1,170 questions were gathered from the Internet.M3GIA includes question sets in six languages, comprising Chinese, English, Spanish, Korean, Portuguese, and French, each with 300 questions.• Each question is labeled with one or several CHC factors, with involved factors marked as '1' and non-involved factors marked as '0'.Each question is also annotated with the question cluster and the narrow question type to which it belongs, to facilitate the calculation of accuracy rates.• M3GIA is self-contained.We bear all responsibility in case of violation of rights.</p>
<p>• The dataset does not contain any information that might be offensive, insulting, or threatening.</p>
<p>A.3 Usage and Distribution</p>
<p>• The evaluation dataset is released at https://huggingface.co/datasets/Songweii/M3GIA.• M3GIA is released under the Apache 2.0 license.• The data is saved in Parquet format, where an example is shown in the README.mdfile.An example code snippet is also provided showing how to read and process the data.</p>
<p>A.4 Maintenance</p>
<p>• M3GIA will be managed and maintained by our research group.For any questions, please contact Wei Song (songweii@zju.edu.cn) and Prof. Kaicheng Yu (kyu@westlake.edu.cn),who are responsible for maintenance.• If we further expand our dataset or find any errors, we will update the dataset and results in the leaderboard accordingly.It will be updated on our website.</p>
<p>B Definitions of the CHC factors</p>
<p>According to the Cattell-Horn-Carroll (CHC) Model of Intelligence [42,43], the definitions of the five cognitive factors are as follows:</p>
<p>Comprehension-Knowledge (Gc), also known as Crystallized Intelligence, is the knowledge of culture that is incorporated by individuals through a process of "acculturation" [34].Gc is typically described as the breadth and depth of acquired knowledge of the language, information and concepts of a culture, and the application of the knowledge.Gc is primarily a store of verbal or language-based declarative (knowing what) and procedural (knowing how) knowledge acquired during general life experiences.In short, Gc reflects the ability to apply and reason using previously learned experiences and common knowledge.[42] Fluid Reasoning (Gf) is the broad ability involved in reasoning, forming concepts, and solving problems using unfamiliar information or in novel situations.It includes inductive, deductive, and quantitative reasoning and is typically evident in mental operations, such as inferential reasoning, forming concepts, classification of unfamiliar stimuli and recognizing patterns.[34,42] Furthermore, there are three factors that are generally considered the hallmark indicators of Gf:</p>
<p>• Induction (I).The ability to observe a phenomenon and discover the underlying principles or rules that determine its behavior.• Deductive Reasoning (RG).This ability, also known as general sequential reasoning, refers to the capacity to reason logically using known premises and principles step by step.• Quantitative Reasoning (RQ).The ability to reason, either with induction or deduction, with numbers, mathematical relations, and operators.</p>
<p>Visual-spatial Processing (Gv) is the ability to perceive, analyze, synthesize, and think with visual patterns, or more succinctly, "the ability to make use of simulated mental imagery to solve problems".</p>
<p>Once the eyes have transmitted visual information, the visual system of the brain automatically performs a large number of low-level computations (e.g., edge detection, light/dark perception, color-differentiation, motion-detection, and so forth).The results of these low-level computations are used by various higher-order processors to infer more complex aspects of the visual image.[42].</p>
<p>Gv abilities are typically measured by tasks (figural or geometric stimuli) that require the perception and transformation of visual shapes, forms, or images and/or tasks that require maintaining spatial orientation with regard to objects that may change or move through space.[34] Reading and Writing (Grw) is the depth and breadth of knowledge and skills related to written language.It is worth noting that, although reading and writing are clearly distinct activities, the underlying sources of individual differences in reading and writing skills do not differentiate between the two activities cleanly [42].It appears that the ability that is common across all reading skills also unites all writing skills.</p>
<p>Quantitative Knowledge (Gq) is the depth and breadth of knowledge related to mathematics.Specifically, it is the ability to comprehend quantitative concepts and relationships and to manipulate numerical symbols.It consists of acquired knowledge about mathematics such as knowledge of mathematical symbols (e.g., , π, , ∞, ̸ =, ≤, +, −, ×, ÷, and many others), operations (e.g., addition/subtraction, multiplication/division, exponentiation/nth rooting, factorials, negation, and many others), computational procedures (e.g., long division, reducing fractions, quadratic formula, and many others).Gq abilities are typically measured by tests include measures of math calculation, applied problems (or math problem solving), and general math knowledge (e.g., Arithmetic on the Wechsler Scales, Quantitative Reasoning on the SB5).</p>
<p>C Introduction to the Evaluation Questions</p>
<p>In this section, we will outline the five question clusters and the 18 narrow question types they encompass.</p>
<p>The Common Sense Cluster.The common sense cluster is designed to measures the Gc factor of an MLLM and includes 3 narrow question types: general information, oral vocabulary and logo problem.In general information, the model is presented with an image and is asked, "Where would you find [the object] in the picture?" or "What would you do with [the object] in the picture?"The initial items in each subtest draw from familiar everyday objects, and the items become increasingly difficult as the objects become more obscure or less familiar.Oral vocabulary consists of two subtests: Synonyms and Antonyms.In the Synonyms subtest, the model is provided with a word and is asked to choose its synonym.In the Antonyms subtest, the model is provided with a word and is asked to choose its antonym.In CHC theory, this test primarily measures a narrow aspect of Comprehension-Knowledge (Gc) referred to as lexical knowledge (VL; vocabulary knowledge), or knowledge of words and word meanings.[45] The logo problem is the real-world problem of the cluster, where a model is provided with a logo and is required to identify an abstract element within it.To achieve this, it must have a very deep impression on the element, such as a confusing artistic characters or symbolic expression of cultural elements, which requires a high level of Gc and a certain level of Gv.</p>
<p>The Visual-spatial Cluster.This cluster is designed to evaluate the Gv factor and includes 3 narrow question types: visualization, picture recognition and real-world spatial.Visualization consists of two subtests: Block Rotation and Spatial Relations.In the former, the model is asked to identify the rotated 3D block that match the original 3D block.In the latter, the model is required to identify three or four pieces that form a complete target shape.In picture recognition, a model is asked to identify a subset of specified pictures within a field of distracting pictures.The stimuli and distracters Table 3: The number and cognitive factors of each question type.Our M3GIA is organized into five clusters, each cluster is further defined to combine two or more narrow question types that are aspects of a broad CHC construct (real-world problems in bold).In total, it contains a total of 1,800 meticulously designed multilingual questions, with the number of questions and the distribution of question types being completely consistent across different languages.Questions potentially related to the cultural backgrounds are marked in green, while purely intellectual questions, unrelated to cultural background, are marked in yellow.The former's data are sourced from native language context, while the latter uses questions translated into the six languages.for each item include varieties of the same type of object (e.g., several different leaves) to eliminate verbal mediation as a memory strategy [44].Real-world spatial problem necessitates that the model accurately determines the relative 3D positioning of objects within an image depicting real-world scenarios.This requires the model to recognize and interpret all existing relationships in the physical world, including comprehensive 3D spatial relationships and the dynamic interconnections between the objects portrayed.</p>
<p>The Comprehension Cluster.This cluster is designed to evaluate the Grw factor and includes 3 narrow question types: readings-text, readings-VL and the comic problem.In readings-text, the model is provided with long articles (about 4-6 paragraphs) and will be required to answer questions related to the main ideas of the articles or the relationships between paragraphs.The articles are collected from reading comprehension exercises found in middle and high school levels across the six countries.To highlight the multimodal nature of our benchmark, we designed readings-VL, where responses must be selected from image-based options besides the conventional text-based queries.</p>
<p>In the comic problem, the model will be provided with a comic consisting of four or more panels that make up a complete plot.To answer the questions, the model needs to understand the entire story's connotation based on the textual dialogues between characters and the plot development.</p>
<p>This approach evaluates the model's ability to integrate visual narrative comprehension with textual comprehension, challenging it to understand scenarios represented both visually and textually.</p>
<p>The Mathematics Cluster.This cluster is designed to evaluate the Gq factor and includes 4 narrow question types.Math facts is tailored to measure Gq alone and consists of two subtests: symbolic</p>
<p>Average=50</p>
<p>Figure 6: Data Balancing.we keep the number of questions for each cluster as balanced as possible when collecting questions.Given the unique characteristics of Gf, we have divided the Reasoning Cluster into Reasoning (I) and Reasoning (RG) for statistical analysis.Across each language, the number of questions within each cluster varies from 40 to 60, with an average of 50.</p>
<p>knowledge and geometric knowledge.The former focuses on the model's acquired knowledge about mathematical symbols and operations.It covers knowledge from elementary to university level, including arithmetic, vector operations, calculus, etc.The latter emphasizes the model's capability to solve problems using geometric knowledge.In algebra and geometry, we source the questions from authentic middle school and high school exam papers across the six countries.Unlike math facts problem which can be directly answered once the knowledge is acquired, these problems require a further reasoning process.Thus, they not only call upon Gq but also require RQ.To evaluate the model's ability to solve mathematical problems in real-life scenarios, we have specially designed application problems.For example, the model might be provided with a restaurant bill and asked to calculate the total amount to be paid.Since it rely heavily on common knowledge, Gc is also annotated in this type of problems.</p>
<p>The Reasoning Cluster.This cluster is designed to assess the Gf factor and includes five narrow question types.Specifically, number series, concept formation, and Raven's Matrices are targeted at evaluating the I (inductive) factor, while the syllogism problem and real-world reasoning target the RG (deductive reasoning) factor.In number series, the model is presented a series of numbers with one or more numbers missing.The model must determine the numerical pattern and provide the missing number in the series.Concept formation measures the ability to categorize and compare [3], a basis for abstracting concepts [51].It requires the model to examine a series of shapes or pictures and then formulate a rule that applies to the item and then figure out the item that do not coincide with the rule.The syllogism problem is a classic form of deductive reasoning, where the model is presented with two statements followed by two conclusions.The model have to take the statements to be true even if they appear to contradict commonly known facts.Then it is asked to decide which of the given conclusions logically follows from the two given statements, disregarding commonly known facts.Real-world reasoning refers to logical reasoning questions rooted in real-world scenarios, where Gc is also important.</p>
<p>D Data Curation Process D.1 Data Collection and Statistics</p>
<p>Data Balancing.To ensure equal consideration for each CHC factor during the assessment, we have maintained a balanced number of questions for each cluster that measures the various CHC factors, as shown in Fig. 6.Specifically, the number of questions in each cluster fluctuates around 50, with a maximum capped at 60 and a minimum threshold of 40.</p>
<p>Questions Crafted from Scratch.Due to the fact that many human intelligence tests are not open to the public, and considering the novelty of some of our question types (such as logo problem, comic problem, etc.), we could not source pre-existing QA pairs from available datasets for many questions.Consequently, we have crafted numerous questions from scratch.For these questions, ensuring the correctness of the answers and the clarity of the descriptions is particularly important.See later Sec.D.2 for more detailed information.</p>
<p>English-centric Bias.Apart from questions that are completely independent of cultural background, such as Number Series and Raven's Matrices, all data are sourced from native websites corresponding</p>
<p>Data Quality Control</p>
<p>Annotation with the CHC Factors</p>
<p>Authenticity of Language Expression</p>
<p>Based on the consultations with psychology experts.</p>
<p>Eliminated blurry images.</p>
<p>(</p>
<p>Replace with clean images.</p>
<p>Volunteer Feedback and Peer Review to the language.These data encompass not only text explicitly linked to cultural backgrounds but also images, since images can also convey information about the cultural contexts implicitly, such as the attire of people in the image background, architectural styles specific to a region, etc.</p>
<p>Multimodal Nature.As a multimodal benchmark, safeguarding the dataset's multimodal attributes is crucial.In particular, questions related to images should require the visual information for resolution and not be solvable through text alone.This principle was rigorously adhered to during the data collection phase, and we also placed emphasis on it during the checking process (see later Sec.D.2).We further validated the importance of image information in our benchmark through an experiment that involved removing images from the evaluation dataset, as shown in Fig. 4.</p>
<p>Table 4: Comparison of GPT-4v's accuracy rates across five clusters before and after the exclusion of images from the evaluation dataset.Removing images from the dataset resulted in a notable decline in evaluation performance, underscoring the significance of visual information in the assessment and emphasizing the multimodal nature of our M3GIA benchmark.</p>
<p>D.2 Data Quality Control</p>
<p>To further control the quality of our data, we perform the data cleaning process from three perspectives, as illustrated in Fig. 7.</p>
<p>• Image Quality.We traverse the dataset and locate all blurry images with resolutions lower than 100×100 px.For questions featuring these images, we either replace them with similar questions that use high-resolution images or substitute the images with clear alternatives that convey the same meaning.</p>
<p>• Accuracy Check.For the questions we designed from scratch, we have paid special attention to ensuring their correctness.(i) To guarantee the authenticity of the language expression in our questions, we engaged native speakers to both formulate and review the descriptions of the question stems.Specifically, after establishing the intended meaning and creating a draft version, these native speakers undertake a thorough review, culminating in the finalized version of the question descriptions.</p>
<p>(ii) We employed volunteer feedback and peer review as methods to assess the clarity of our question descriptions and to detect any potential issues with the answers.Clarity of Descriptions: We recruited 10 volunteers for each language who were not involved in question creation to take our tests and provide feedback on any errors or unclear descriptions they encountered in the questions.After thorough discussion of their feedback, we ultimately incorporated revisions into 28 questions.</p>
<p>Correctness of Answers: After the volunteers submit their answers to the electronic questionnaire, the correct answers will be automatically disclosed.They will then be prompted to revisit any questions they answered incorrectly and are encouraged to challenge these, offering feedback on any they assert to be correct or view as contentious.This feedback was taken seriously, and we ultimately made corrections to six instances where we recognized that the answers were indeed controversial or misleading.Besides, we also employed peer review within our group to ensure the correctness of answers.Specifically, after formulating their questions, team members will swap them with each other for a round of testing.Following this exercise, if a tester has a justifiable reason for an incorrect response, they will engage in a direct discussion with the question's author.This method led to the identification of around ten answers that were deemed contentious.• Annotation of the CHC Factors.To ensure the rationality of the questions designed for each CHC factor and the validity of the CHC factors annotated for each question, psychologists were deeply involved and cooperated in the question design and annotation phases.</p>
<p>E The GIA Metrics E.1 Human Data Collection</p>
<p>We collected human data in each language from 80 participants using paid electronic questionnaires.Participation in the test is compensated and entirely voluntary.To protect user privacy, the test is also conducted anonymously.Each participant was mandated to answer all questions to be eligible for payment.To motivate participants to provide thoughtful responses, the compensation is structured incrementally, increasing with the number of questions answered accurately.Additionally, to mitigate the risk of participants choosing answers at random just for the monetary incentive, we randomly inserted several "check question" within the questionnaire.For instance, a check question might instruct participants to "Please select option B." If a participant answer more than two such questions incorrectly, their submission would be considered invalid.</p>
<p>E.2 Calculation of the GIA Score</p>
<p>In this study, we employed a cognitive factor analysis (CFA) approach to model the General Intelligence Ability (GIA) of human subjects based on the CHC theory of cognitive abilities [17].The CHC theory posits a hierarchical structure of cognitive abilities, encompassing broad factors such as Gc, Gf, Gv, Gq, and Grw, which are further broken down into narrower tasks.Our MarcoBench, a comprehensive set of 1,800 multiple choice problems corresponding to the assessment of the five CHC cognitive factors, was meticulously subdivided into 18 distinct question types, each designed to measure different facets of the cognitive abilities being assessed.</p>
<p>Data collection involved 80 human subjects across four different languages: Chinese, English, Portuguese, and Korean.A total of 60 subjects were utilized for model building, while the remaining 20 subjects were reserved for model validation.Subjects were administered the MarcoBench, and their performance on the tasks was meticulously recorded.The data comprised accuracy scores on 18 cognitive tasks, representing the 18 distinct question types.The accuracy data was firstly normalized to generate z-scores.And then, the EFAtools package was employed to scale the data and calculate the correlations between the variables.A series of statistical tests, including Bartlett's test and the Kaiser-Meyer-Olkin (KMO) measure, were conducted to assess the suitability of the data for factor analysis.An overall KMO value larger than 0.6 was deemed acceptable for factor analysis [54].</p>
<p>The CFA model was constructed in accordance with the CHC theory, with the broad and narrow factors defined as per the theoretical framework.We used the lavaan package (https://www.lavaan.ugent.be/) to fit the CFA model to the pre-processed data.The CFA model structure included:</p>
<p>• Gc: Measured through general information, oral vocabulary, and logo problem tasks.</p>
<p>• Gv: Included visualization, picture recognition, and real-world spatial tasks.</p>
<p>• Grw: Assessed through readings-text, readings-visual-language (VL), and comic problem.</p>
<p>• Gq: Comprised math facts, algebra, geometry, and application problems.</p>
<p>• Gf: Evaluated through number series, concept formation, Raven's Matrices, syllogism problem, and real-world reasoning tasks.</p>
<p>Additionally, a General Intelligence Ability (GIA) factor was included, integrating all five broad factors.Model estimation was performed using Maximum Likelihood with Restricted Maximum Likelihood (MLR) estimation, which has been demonstrated to be more robust in the presence of multicollinearity.</p>
<p>The model's fit was evaluated using a range of indices, including the chi-square statistic, degrees of freedom, p-value, Comparative Fit Index (CFI), Root Mean Square Error of Approximation (RMSEA), Standardized Root Mean Square Residual (SRMR), and Akaike Information Criterion (AIC).The primary focus was on the CFI and SRMR, as they are considered more reliable indicators of model fit.A CFI larger than 0.8 or 0.9 was considered acceptable, while an SRMR equal to or lower than 0.08 was deemed acceptable [5,15].</p>
<p>Upon establishing a satisfactory model fit, we employed it to calculate latent scores for the GIA on a separate set of test data.Subsequently, we calculated the Pearson correlation coefficient between the GIA latent score and the overall accuracy of the subjects on the test data to validate the model's effectiveness.The results of this analysis provided robust evidence for the validity of the CFA model in capturing the GIA of human subjects, as indicated by the significant positive correlation between the GIA latent score and overall accuracy.This validation process underscores the model's theoretical grounding in the CHC theory and its empirical support from the data.Subsequently, we applied the CFA model to estimate the GIA for several MLLMs, including gpt-4o [36], gpt-4v [1], llava1.6-34b[27], llava1.6-13b,llava1.6-7b,mini-gemini-34b [26], mini-gemini-7*8b, mini-gemini-13b, and mini-gemini-8b, enabling a comparative analysis of their cognitive abilities against human performance.</p>
<p>F Evaluation Strategy</p>
<p>Option Extraction For choice extraction, we adopted a two-stage strategy.In the first stage, we employed a keyword-based rule method to parse the model output in order to obtain options.This approach proved very effective, with the majority of existing multimodal large models successfully identifying correct answers at this stage.Yet, to enhance the robustness of our evaluation, we adopted a second stage of precautionary measures in case the parsing in the first stage fails.This involves deploying GPT-4-turbo for the concise summary of answer choices from the original model responses.</p>
<p>If the second stage still fails, we will randomly generate an option for the model as the answer to the question.It is noteworthy, though, that throughout the actual testing process thus far, we have not encountered scenarios necessitating the use of random option generation.</p>
<p>The rationale behind not directly resorting to large language models for option extraction in the first stage stems from the superior stability and reliability of the rule-based method.Despite leveraging large language models for option extraction has been a common practice model evaluations, it still carries a certain error rate.On the contrary, the rule-based method, while not infallible in parsing answers across all scenarios, nearly guarantees correctness in the instances where parsing is successful.Consequently, we advocate for an initial screening using the rule-based method, followed by the employment of large language models for extraction, as a strategy that enhances overall robustness.</p>
<p>Scoring In addition to the calculation of the GIA score mentioned above, our benchmark can also be broken down to calculate accuracy across various cognitive dimensions.Specifically, each question is annotated with the CHC factors it involves; factors that are involved are marked with a 1, and those that are not involved are marked with a 0. When a question involves a certain factor, the correctness of that question will contribute to the accuracy statistics for that particular CHC factor; otherwise, it will not be included in the statistics.Taking the calculation of the accuracy score of the Gc factor as an example:
Acc_Gc = n i=1 Gc i • T i n i=1 Gc i (1)
where n is the total number of questions, Gc i indicates whether the i th question involves the Gc factor, marked as 1 if it does, and 0 otherwise.T i indicates whether the i th question was answered correctly, with 1 representing a correct answer and 0 representing an incorrect answer.To mitigate the effects of randomness on the evaluation results, including both the scores of the various CHC factors and the overall GIA score, we adopt a strategy of iterating five times and taking the average.</p>
<p>G GIA Scores on More Languages</p>
<p>G.1 Ablation Study on LLM Size To further investigate the influence of LLM size to the GIA score, we conducted an ablation study with the Qwen series from 1.8B to 72B.In this experiment, we applied the LLaVA architecture and used the same ViT component (CLIP-ViT-L-14).In order to strictly control variables, we trained the models by ourselves using the same training data and the same set of hyperparameters for pretraining and fine-tuning.The data for pretraining is completely from LLaVA-1.5, and the data for fine-tuning is composed of LLaVA1.5 [27] dataset, ShareGPT4v [10] dataset and our private visual-text instruct data.We show the training data and hyperparameters for both first-stage vision-language alignment pretraining and the second-stage visual instruction tuning in Table .5. We use greedy decoding for evaluation to ensure reproducibility.The GIA scores on six languages are shown in Fig. 8.</p>
<p>Across the six languages analyzed, we consistently observe a significant increase in GIA scores with the expansion of LLM parameters.However, it is notably surprising that scaling up the size of LLMs from 7B to 14B parameters often yields no observable performance enhancement (and there might even be a slight decline).This phenomenon suggests the existence of a threshold -indicative of an emerging point of general intelligence for MLLMs somewhere between 13B and 32B parameters.In other words, it indicates a potential threshold for attaining a superior level of general intelligence, likely situated in the parameter range of 13B to 32B.</p>
<p>H Case Study H.1 The Common Sense Cluster</p>
<p>Current advanced MLLMs excel in common sense cluster, especially in general information and oral vocabulary questions, which can likely be bolstered by their extensive training datasets.However, there are still some deficiencies in logo problem related to cultural background for some MLLMs, e.g.GPT-4v.Logo problems usually contain confusing artistic characters or symbolic expression of cultural elements, which requires a high level of Gc and a certain level of Gv.As shown in Fig. 9, GPT-4v can recognize the locomotive in the logo of chinese question 46, but it fails to recognize the Chinese character ("hang" in pinyin) in chinese question 41, while GPT-4o can perfectly recognize characters containing Chinese cultural elements.Generally, the GIA scores increase with the rise of LLM parameters.However, a threshold is observed when scaling up the LLMs' size from 7B to 14B.</p>
<p>H.2 The Visual-spatial Cluster</p>
<p>In the Visual-spatial Cluster, current advanced MLLMs performe very well on the Picture Recognition questions, followed by the Real-world Spatial questions, and performed the worst on the Visualization transformation questions.The high accuracy on the Picture Recognition questions shows that the advanced MLLMs already has a good object recognition ability.Compared with object recognition ability, their ability to recognize three-dimensional spatial relationships is much worse, which can be divided into translation transformation and rotation transformation.The performance on the Real-world Spatial questions proves that the MLLMs can recognize the translation transformation relationship of objects in three-dimensional space with a certain probability, including up, down, left, right, front, and back.At the same time, the MLLMs suffer from the rotation transformation ability and spatial imagination ability in three-dimensional space, resulting the low accuracy on the Visualization transformation questions.As shown in Fig. 11, after multiple inferences, GPT-4o can always recognize the same cup in english question 81 and the spatial relationship between the two remote controls with a high probability in english question 99, but it is difficult to recognize the same blocks after rotation in english question 80 in Fig. 10.</p>
<p>H.3 The Comprehension Cluster</p>
<p>Similar to the common sense cluster, current advanced MLLMs perform very well in comprehension cluster, including readings-text, readings-VL and the comic problem, which can be attributed to the powerful language capabilities of LLM.Surprisingly, GPT-4o understands the scenarios represented both visually and textually in comics quite well, which proves it can integrate visual narrative comprehension with textual comprehension.As shown in Fig. 13, in english question 146 and french question 144, GPT-4o can understand the entire story's connotation based on the textual dialogues between characters and the plot development, especially can recognize the facial expressions and quantitative contrast of population in english question 146.At the same time, GPT-4o still has some shortcomings in understanding the relationship between text paragraphs.As shown in Fig. 12, in english question 7, GPT-4o fails to capture the "general-specific-general" structure of the article.</p>
<p>H.4 The Mathematics Cluster</p>
<p>This Mathematics cluster is designed to evaluate the Gq factor.Although current advanced MLLMs did not perform well on math problems overall, we found two interesting phenomena.One is that the model performs better on algebra problems than geometry problems, such as the english question 182 in Fig. 14.This may be attributed to the training data of LLM contains enough math knowledge text, but the visual module of MLLMs still has defects in abstract geometric figures and their relationships.</p>
<p>The other is that the model performed better on math facts problems and problems that can be solved in one step by directly applying mathematical knowledge including symbolic knowledge and geometric knowledge than on problems that require multi-step reasoning.For example in Fig. 14, GPT-4o can apply the Central Angle Theorem to solve the english question 182, but fails to solve the english question 175 which needs multi-step reasoning and calculation.In addition, GPT-4o has reached a level of practical application in simple mathematical applied problem, such as the problem of choosing the shortest flight time in english question 188 as shown in Fig. 15.</p>
<p>H.5 The Reasoning Cluster</p>
<p>The reasoning cluster is designed to evaluate the I (inductive) factor and RG (deductive reasoning) factor.Similar to the performance gap between geometry and algebra in mathematics cluster, there is also a performance gap between deductive and inductive reasoning.Although GPT-4o are approaching the average human level for deductive reasoning, it only marginally meet the passing line (60) on syllogism problem and real-world reasoning problem.For example in Fig. 17, GPT-4o fails on the english question 286 which is a classic form of deductive reasoning and ask GPT-4o to decide which of the given conclusions logically follows from the two given statements.For inductive reasoning, GPT-4o performs quiet well in number series and concept formation problems, such as the english question 214 and 237 in Fig. 16, but performs very poorly on the Raven's Matrices problems.Take the english question 254 in Fig. 18 as an example, GPT-4o mistakenly recognized the graphic in the third row and first column as a vertical line with a black square at the bottom, when it should actually be a black square at the top, resulting in the incorrect selection.GPT-4o can perform effective reasoning, but there is a certain probability that it will make small mistakes when recognizing graphics, which shows that its visual module needs to be further improved.In addition, we also show the results of GPT-4v, which misidentifies counterclockwise rotation as clockwise rotation and incorrectly identifies option E as the arrow pointing straight down.This proves GPT-4v is much worse than GPT-4o in both reasoning and visual recognition.</p>
<p>I Limitations</p>
<p>• We have observed a phenomenon in MLLMs similar to human cognition known as "winner takes all", which corroborates the emergence of GIA within cutting-edge MLLMs.However, we have not yet been able to provide a more definitive and persuasive explanation for the underlying causes.Unraveling this will be one of the directions we dedicate ourselves to in the future.</p>
<p>• We have gathered human data to construct the GIA model and to compare the cognitive abilities of current MLLMs with those of humans.Yet, the human data we have amassed thus far is limited, which might impinge on the accuracy of the GIA model and the objectivity of our findings.Hence, we aim to continue maintaining and our dataset, as well as collecting more human participant data in the future to encompass a more comprehensive and varied set of human samples.</p>
<p>Figure 1 :
1
Figure 1: Overview of multi-lingual multi-modal general intelligence ability benchmark.(Left)In contrast to traditional benchmarks that focus on evaluating specific task performances, we draw inspiration from cognitive science to categorize five cognitive factors, try to provide a feasible evaluation of general intelligence ability (GIA).(Right) Specifically, we adopt the factors from the CHC theory to disentangle fundamental cognitive abilities with existing evaluation tasks.In addition, to further understand how language impacts such ability, we collect or design questions in six languages with large population.</p>
<p>Figure 2 :
2
Figure 2: Structure of our CHC inspired model of cognitive abilities.(Left) We identified five key cognitive factors for current MLLMs: Comprehension-Knowledge (Gc), Quantitative knowledge (Gq), Reading and Writing (Grw), Fluid reasoning (Gf), and Visual-spatial processing (Gv).In the hierarchical structure, Gf is further subdivided into three narrow factors: I (Induction), RG (Deductive Reasoning), and RQ (Quantitative Reasoning).(Right) A conceptual map of the five cognitive factors and their overlaps with each other.</p>
<p>The picture shows the logo of a Chinese group.What is the most similar thing to it?Picture Recog.|Gv Question: Please find two objects that exactly match the target object.AB is the diameter of the circle with centre at O. P is a point on the circle such that PB = 2PA.If AB = d units, then what is the length of PA? Options (in LaTeX format ): [A] \sqrt{2} d units [B] \frac{d}{\sqrt{5}} units [C] \sqrt{5} d units [D] 2\sqrt{2} d units</p>
<p>What position is the black remote control located in relation to the white remote control?Eric plans to fly from New York to Las Vegas to attend a conference.Which flight takes the shortest time?Options: [A] Flight 1 [B] Flight 2 [C] Flight 3 [D] They are exactly the same.Only conclusion I follows.[B] Only conclusion II follows.[C] Neither I nor II follows [D] Neither I nor II follows [E] Both I and II follow.Statements: (1) Some swords are sharp.(2) All swords are rusty.Conclusions: I. Some rusty things are sharp.II.Some rusty things are not sharp.</p>
<p>Figure 3 :
3
Figure 3: Questions overview of M3GIA.To assess the five CHC cognitive factors-Gf, Gc, Gq, Grw, Gv correspondingly-we devised five broad question clusters: common sense (orange), visualspatial (blue), comprehension (yellow), mathematics (red), and reasoning (green).To prevent the assessment of any particular ability from being constrained to a fixed and singular perspective, we have stratified each of the five clusters into 2-4 specialized narrow question types, each narrow question type reflects a different perspective on the broad CHC ability.This subdivision results in a total of 18 subtasks.All the QAs are in the format of multiple choice problems whose answers are marked [A][B][C][D].</p>
<p>Figure 4 :
4
Figure 4: (Left) The correlation matrix of MLLMs' accuracy scores across various cognitive factors.It indicates a notable correlation in their performance across these dimensions.(Right) We find that the GIA scores of MLLMs, calculated using the CFA model constructed from human data, demonstrate high explanatory power for their overall test performance (R 2 ≥ 0.93).</p>
<p>Figure 5 :
5
Figure5: The GIA scores of some representative MLLMs across the six languages.(Left) We designate the average human score as 100 in each language and normalize the scores of different models accordingly, making the GIA scores comparable across languages.(Right) Taking English as an example, we visualized the performance of various models and compared it with the human level.</p>
<p>Figure 7 :
7
Figure 7: How we ensure the data quality of M3GIA.</p>
<p>Figure 8 :
8
Figure8: The GIA scores across the six languages, with Qwen LLM series from 1.8B to 72B.Generally, the GIA scores increase with the rise of LLM parameters.However, a threshold is observed when scaling up the LLMs' size from 7B to 14B.</p>
<p>In the Block Rotation subtest, you will see five blocks of different 3D shapes.The block on the top shows the target block.One of the four blocks labeled A through D also shows the target block, but rotated in space.The remaining blocks show a different block, rotated to various positions in space.Your task is to choose the block (from the blocks labeled A through D) that shows the target block rotated in space.Please select the correct option.task is to identify which block (from A to D) is a rotated view of the target block shown at the top.Here's the detailed analysis: Target Block Description: Top block: Pink Middle layer: Left block: Blue Right block: Yellow Bottom layer: Left block: Blue Right block: Green Options Analysis: Option 1 (A) shows a horizontal layout, not matching the vertical arrangement of the target block.Option 2 (B) has a vertical layout but the arrangement of colors and blocks does not match the target.Option 3 (C) correctly shows a vertical arrangement where the: The top block is pink.The middle layer on the left is blue and the right is green.The bottom layer on the left is pink, the right is blue.Option 4 (D) also shows a vertical arrangement but the relative positions of colors do not match exactly.Conclusion: After examining the structure and rotational perspectives, Option 3 (C) is the correct choice that represents the target block rotated in space.Thus, the correct answer is: Choices: (A) 1 (B) 2 (C) 3 (D) 4 Answer: (C) 3 Option (C) correctly maintains the spatial arrangement of the target block when rotated.The top is pink, middle includes blue and green, and the bottom has pink</p>
<p>Figure 10 :
10
Figure 10: Case Study of Visualization problem in Visual-Spatial Cluster.</p>
<p>Math Facts Sub-test 1 |Gq Sub-test 2 (Geo):
If angle PMQ is 40degrees, what isthe measure ofangle PRQ?</p>
<p>|Gq Gv Number Series |RQ I Question:</p>
<p>Look at this series: 22, 21, 23, 22, 24, 23, ... What number should come next?
Options:[A] 22[B] 24[C] 25[D] 26Hint:22-1=21, 21+2=23, 23-1=22, 22+2=24, 24-1=23, 23+2=? ...</p>
<p>Table 1 :
1
The accuracy results on 24 MLLMs regarding each cognitive ability.The best in bold and the second-best underlined.All the numbers are presented in decimal and the full score is 100.
Types (LLM Size)ModelsViT SizeIRGGf RQOverallGcGqGrwGvOverall AccHumanAverage Performance-86.860.071.269.779.165.478.181.176.9GPT-4o-58.059.233.950.172.342.879.646.359.8GPT-4v-56.756.340.951.974.846.477.552.459.2APIGemini-1.5-Pro Gemini-Pro--54.3 39.056.4 30.841.8 22.754.3 32.475.8 56.560.8 31.777.1 67.153.8 43.162.4 46.5Cluade3-Sonnet-39.732.927.334.058.334.261.343.947.0Cluade3-Haiku-35.335.830.333.155.833.357.936.443.1Mini-Gemini-34b0.3B37.737.530.634.861.034.262.945.748.2OSS (Large)Mini-Gemini-8*7b LLaVA-v1.6-34b Yi-VL-34b0.3B 0.3B 0.6B28.7 20.7 25.030.0 40.0 32.926.7 28.5 35.830.3 30.8 29.558.1 53.8 48.135.0 36.4 29.261.3 61.7 54.641.9 40.4 35.744.8 42.8 38.2InternVL-chat-v1.2-plus6B45.042.532.442.564.641.466.747.551.9OSS (Medium)Mini-Gemini-13b LLaVA-v1.5-13b LLaVA-v1.6-vicuna-13b0.3B 0.3B 0.3B22.3 17.7 23.329.2 26.3 19.623.3 15.2 24.524.3 19.9 23.141.5 42.1 36.726.1 20.3 26.944.2 40.0 47.528.3 28.8 28.532.9 30.4 33.2Fuyu-8b-21.722.127.323.327.324.427.124.925.1Mini-Gemini-8b0.3B37.329.631.830.451.530.656.336.141.4LLaVA-v1.5-7b0.3B18.025.015.819.741.519.735.025.728.4LLaVA-v1.6-vicuna-7b0.3B21.322.918.220.536.519.432.926.931.5OSS (Small)LLaVA-v1.6-mistral-7b Deepseek-VL-7b Yi-VL-6b0.3B 0.38B 0.6B24.3 32.3 25.225.8 29.2 35.524.5 22.1 26.224.9 28.3 28.838.5 50.4 35.624.2 24.4 29.036.7 54.2 54.532.1 32.4 30.828.9 37.5 34.4Qwen-VL1.9B18.723.825.222.541.027.542.530.132.1CogVLM2-LLaMA3-Chinese10B29.721.729.726.554.827.237.940.338.7</p>
<p>Table 2 :
2
The General Intelligence Ability of different models accross the six languages.The left side displays the actual GIA scores, while the right side shows the normalized results after setting the average human GIA scores for each language to 100.0.
ModelsEnGeneral Intelligence Ability (GIA) Ch Fr Sp PtKoEnChNormalized GIA Scores Fr SpPtKoHuman16.0116.6919.5216.2216.0018.05100.0100.0100.0100.0100.0100.0GPT-4o13.8511.4612.3713.1212.8013.0186.568.763.380.980.072.1GPT-4v12.6110.9513.8314.0412.1212.2578.865.670.886.575.867.9LLaVA-1.6-34b11.479.2511.357.9610.679.0471.655.458.149.166.750.1LLaVA-1.6-13b6.966.898.717.756.947.7543.541.344.647.843.442.9LLaVA-1.6-7b6.755.997.676.746.015.9342.135.939.341.537.632.9Mini-Gemini-34b 11.009.9612.7511.529.4510.6968.759.765.371.059.159.2Mini-Gemini-13b 8.687.768.657.737.107.9854.246.544.347.744.444.2Mini-Gemini-8b9.328.1111.259.768.997.0858.248.657.660.256.239.3Qwen-72b  †11.6810.7510.2010.509.719.7672.964.452.264.760.754.1Qwen-32b  †10.589.799.6210.119.259.1866.158.749.362.357.850.9Qwen-14b  †8.468.769.158.498.798.3252.852.546.952.454.946.1Qwen-7b  †8.568.938.988.428.778.4153.453.546.051.954.846.6Qwen-1.8b  †7.346.568.017.376.496.4845.839.341.045.540.635.9</p>
<p>Table 5 :
5
The training data and hyperparameters of MLLM with Qwen series.
Data and HyperparametersPretrainFinetunedata size558K1550Kbatch size256128lr1e-32e-5lr schedulecosine decaycosine decaylr warmup ratio0.030.03weight decay00epoch11optimizerAdamWAdamW
GPT-4V: (A)IJKComprehension ClusterReadings-VL | Grw | En.127Question:Passage: P1: Some people learn a second language easily.Others have trouble learning a new language.How can you learn a new language, such as English?There are several ways to help you learn English more easily.P2: Firstly, feel positive about learning English.If you believe that you can learn, you will learn.Be patient.You don't have to understand everything all at once.Often you will make mistakes when you are learning something new.We can learn from our mistakes.P3: Try to practice using your English as possible as you can.For example, you can write a diary every day.Soon, you will get used to writing your ideas in English.After several weeks, your writing skills will improve.Besides, try to speak English every day.You can practice speaking with your classmates after class.You might make mistakes, but don't worry.Slowly, you will become comfortable communicating in English.P4: It's a great idea to keep a record of your language learning experience.You can write your learning experience in your diary.After each class, think about it.Do you answer questions correctly in class?Do you understand your teacher?Perhaps the lesson is a bit difficult, but you can try to understand it.Write these reflections in your diary to practice using your English and write down your little progress.Finally, you will find yourself enjoy learning English.P5: All in all, be positive, confident and patient --believe that you can make it sooner or later.Make good use of all the time we can get.Question: Which image best describes the structure of this passage?Choices:The passage provided describes a clear, logical progression of steps for learning English, presented in a sequential manner.Let's break down the paragraphs: P1: Introduces the topic of learning a new language and sets up the context.P2: Discusses the importance of feeling positive about learning English.P3: Emphasizes practicing English in various ways to improve gradually.P4: Suggests keeping a record of the language learning experience.P5: Concludes with a summary, encouraging positivity, confidence, and patience.The structure of the passage is linear, with each paragraph building upon the previous ones in a step-by-step manner.Considering the images:Image A shows a more interconnected structure that isn't purely linear.Image B shows a similar structure but still has some intersections.Image C shows a purely parallel structure with equal importance given to P2 through P5, all stemming directly from P1, which is not quite accurate as each subsequent paragraph builds on the idea of the previous one.Image D shows a strictly sequential structure where each paragraph follows logically from the previous one.Given the sequential nature of the passage: The best choice is (D) D, as it accurately represents the step-by-step structure of the passage.Explanation: In the comic, the boy asks the girl which part of China she comes from and she responds that she comes from a small city in the south.When the boy learns that the population is around five million, he is visibly shocked.This indicates that the boy finds five million to be a large number for a "small city," highlighting the difference in perception regarding city size.Therefore, option (C) is the most fitting description.Option A: This suggests that the girl disagreed with the content of the note.However, the comic doesn't depict any disagreement; rather, it shows confusion and literal interpretation.Option B: This suggests that the girl broke the cookie.This is not depicted in the comic; instead, the focus is on the literal interpretation of the note in the cookie.Option C: This suggests that the girl interpreted the note literally and decided to marry the cookie.This is in line with the comic, where the girl says "Oui" to the note's proposal and the boy is upset, lamenting that she married the cookie.Option D: This suggests that the girl fell in love with other men.There's no such detail in the comic.The best answer based on the interpretation of the comic: Answer: C. Parce que la fille a pris la note au pied de la lettre et a épousé le cookie.This answer fits the context where the girl misunderstood the proposal in the fortune cookie and decided to take it literally, leading to the boy's sadness.The value of ( x ) is 60°.However, there is an inconsistency here as ( 60°) is not an option among the given choices, and we see that 35°and ( x ) should add up to make 90°due to the perpendicular formed at the intersection.Thus, looking at the diagram closely again, consider whether there might be an error with the provided options or diagram interpretation.Given how we used the provided values to solve through standard geometric relationships, it's plausible to question the integrity or context of the problem statement.Note that: [ x = 35°+ y ] where it seems (y = 55°) should make angles:    From the first statement "All good athletes win," we know that if someone is a good athlete, they will win.From the second statement "All good athletes eat well," we know that if someone is a good athlete, they will eat well.
. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Nocaps: Novel object captioning at scale. H Agrawal, K Desai, Y Wang, X Chen, R Jain, M Johnson, D Batra, D Parikh, S Lee, P Anderson, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Neuropsychology: From theory to practice. D Andrewes, 2015Psychology Press</p>
<p>J Bai, S Bai, S Yang, S Wang, S Tan, P Wang, J Lin, C Zhou, J Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>Applications of structural equation modeling in marketing and consumer research: A review. H Baumgartner, C Homburg, International journal of Research in Marketing. 1321996</p>
<p>Using cognitive psychology to understand gpt-3. M Binz, E Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Beyond individual intelligence tests: application of cattell-horn-carroll theory. J M Caemmerer, T Z Keith, M R Reynolds, Intelligence. 791014332020</p>
<p>Human Cognitive Abilities: A of Factor-Analytic Studies. J B Carroll, 1993Cambridge University Press</p>
<p>L Chen, J Li, X Dong, P Zhang, C He, J Wang, F Zhao, D Lin, arXiv:2311.12793Sharegpt4v: Improving large multi-modal models with better captions. 2023arXiv preprint</p>
<p>X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Dollár, C L Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015arXiv preprint</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, B Li, P Luo, T Lu, Y Qiao, J Dai, arXiv:2312.142382023arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, Journal of Machine Learning Research. 25702024</p>
<p>J Coda-Forno, M Binz, J X Wang, E Schulz, arXiv:2402.18225Cogbench: a large language model walks into a psychology lab. 2024arXiv preprint</p>
<p>A confirmatory factor analysis of the end-user computing satisfaction instrument. MIS quarterly. W J Doll, W Xia, G Torkzadeh, 1994</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>A distributed brain network predicts general intelligence from resting-state human neuroimaging data. J Dubois, P Galdi, L K Paul, R Adolphs, Philosophical Transactions of the Royal Society B: Biological Sciences. 373201702841756. 2018</p>
<p>Massive iq gains in 14 nations: What iq tests really measure. J R Flynn, Psychological bulletin. 10121711987</p>
<p>C Fu, P Chen, Y Shen, Y Qin, M Zhang, X Lin, J Yang, X Zheng, K Li, X Sun, Y Wu, R Ji, Mme, A comprehensive evaluation benchmark for multimodal large language models. 2024</p>
<p>Understanding social reasoning in language models with language models. K Gandhi, J.-P Fränken, T Gerstenberg, N Goodman, Advances in Neural Information Processing Systems. 362024</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>M Kosinski, arXiv:2302.02083Theory of mind may have spontaneously emerged in large language models. 20234169arXiv preprint</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 252012</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>B Li, R Wang, G Wang, Y Ge, Y Ge, Y Shan, arXiv:2307.16125Seed-bench: Benchmarking multimodal llms with generative comprehension. 2023arXiv preprint</p>
<p>Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu, J Jia, arXiv:2403.18814Mini-gemini: Mining the potential of multi-modality vision language models. 2024arXiv preprint</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, arXiv:2310.037442023arXiv preprint</p>
<p>Y Liu, H Duan, Y Zhang, B Li, S Zhang, W Zhao, Y Yuan, J Wang, C He, Z Liu, arXiv:2307.06281Is your multi-modal model an all-around player?. 2023arXiv preprint</p>
<p>Deepseek-vl: towards real-world vision-language understanding. H Lu, W Liu, B Zhang, B Wang, K Dong, B Liu, J Sun, T Ren, Z Li, Y Sun, arXiv:2403.055252024arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, Mathvista, arXiv:2310.02255Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. K Marino, M Rastegari, A Farhadi, R Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognition2019</p>
<p>A proposal for the dartmouth summer research project on artificial intelligence. J Mccarthy, M L Minsky, N Rochester, C E Shannon, AI magazine. 274august 31. 1955. 2006</p>
<p>Chc theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research. K S Mcgrew, 2009</p>
<p>Internal and external factorial extensions to the cattell-horncarroll (chc) theory of cognitive abilities: A review of factor analytic research since carroll's seminal 1993 treatise. K S Mcgrew, J J Evans, 2004Institute for Applied Psychometrics</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>From brain maps to cognitive ontologies: informatics and the search for mental structure. R A Poldrack, T Yarkoni, Annual review of psychology. 672016</p>
<p>Raven progressive matrices. J Raven, Handbook of nonverbal assessment. Springer2003</p>
<p>Essentials of Stanford-Binet intelligence scales (SB5) assessment. G H Roid, R A Barram, 2004John Wiley &amp; Sons39</p>
<p>The stanford-binet intelligence scales. G H Roid, M Pomplun, 2012The Guilford Press654New York, NY, USA</p>
<p>Are emergent abilities of large language models a mirage?. R Schaeffer, B Miranda, S Koyejo, Advances in Neural Information Processing Systems. 362024</p>
<p>The cattell-horn-carroll model of intelligence. W J Schneider, K S Mcgrew, Contemporary intellectual assessment: Theories, tests, and issues. 2012</p>
<p>The cattell-horn-carroll theory of cognitive abilities. W J Schneider, K S Mcgrew, Contemporary intellectual assessment: Theories, tests, and issues. 2018</p>
<p>The woodcock-johnson iv. F A Schrank, B J Wendling, Contemporary intellectual assessment: Theories, tests, and issues. 2018383</p>
<p>F A Schrank, S L Decker, J M Garruto, Essentials of WJ IV cognitive abilities assessment. John Wiley &amp; Sons2016</p>
<p>Textcaps: a dataset for image captioning with reading comprehension. O Sidorov, R Hu, M Rohrbach, A Singh, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part II 16</p>
<p>Towards vqa models that can read. A Singh, V Natarajan, M Shah, Y Jiang, X Chen, D Batra, D Parikh, M Rohrbach, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>general intelligence" objectively determined and measured. C Spearman, The American Journal of Psychology. 1961</p>
<p>Beyond IQ: A triarchic theory of human intelligence. R J Sternberg, CUP Archive. 1985</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Concept formation and frontal lobe function: The search for a clinical frontal lobe test. P L Wang, The frontal lobes revisited. Psychology Press2019</p>
<p>W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, arXiv:2311.03079Visual expert for pretrained language models. 2023arXiv preprint</p>
<p>Emotional intelligence of large language models. X Wang, X Li, Z Yin, Y Wu, J Liu, Journal of Pacific Rim Psychology. 17183449092312139582023</p>
<p>Exploratory factor analysis: A guide to best practice. M W Watkins, Journal of black psychology. 4432018</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>P Xu, W Shao, K Zhang, P Gao, S Liu, M Lei, F Meng, S Huang, Y Qiao, P Luo, arXiv:2306.09265Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. 2023arXiv preprint</p>
<p>Z Yang, L Li, J Wang, K Lin, E Azarnasab, F Ahmed, Z Liu, C Liu, M Zeng, L Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023arXiv preprint</p>
<p>Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. Z Yin, J Wang, J Cao, Z Shi, D Liu, M Li, X Huang, Z Wang, L Sheng, L Bai, Advances in Neural Information Processing Systems. 202436</p>
<p>Open foundation models by 01. A Young, B Chen, C Li, C Huang, G Zhang, G Zhang, H Li, J Zhu, J Chen, J Chang, arXiv:2403.046522024arXiv preprint</p>
<p>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. P Young, A Lai, M Hodosh, J Hockenmaier, 2014Transactions of the Association for Computational Linguistics2</p>
<p>W Yu, Z Yang, L Li, J Wang, K Lin, Z Liu, X Wang, L Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023arXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. X Yue, Y Ni, K Zhang, T Zheng, R Liu, G Zhang, S Stevens, D Jiang, W Ren, Y Sun, arXiv:2311.165022023arXiv preprint</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. W Zhang, M Aljunied, C Gao, Y K Chia, Bing , L , Advances in Neural Information Processing Systems. 202436</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
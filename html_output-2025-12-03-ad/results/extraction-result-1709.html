<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1709 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1709</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1709</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-265466250</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.16714v2.pdf" target="_blank">Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld</a></p>
                <p><strong>Paper Abstract:</strong> While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1709.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1709.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied Multi-Modal Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language agent finetuned to act in a 3D embodied visual simulator by distilling an LLM expert operating in a parallel text world via an online cross-modality imitation algorithm (DAgger-DPO) with retrospection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>EMMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Modular VLM composed of a frozen ViT encoder, a frozen Q-Former that produces 32 visual tokens, a learnable linear projection layer mapping visual tokens to text embedding space, and a frozen LLM decoder; only the linear projection (and downstream policy head) is updated. Trained online with a novel DAgger-DPO objective using an LLM actor+critic as expert and a long-term retrospection memory.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language pretraining (static image-text pairs) for VLM components; language-model pretraining for the LLM expert (text world)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>EMMA reuses pretrained components from InstructBLIP (ViT, Q-Former, and an LLM decoder) which were pretrained on static image–text corpora (details in InstructBLIP paper). EMMA is then initialized with behavior cloning on a demonstration dataset created for ALFWorld: the paper's generated dataset contains ~15,247 expert episodes (≈178,585 image-text pairs) produced by a rule-based planner and LLM-generated task instructions (the paper also refers to ~170K demonstrations used for BC initialization). The LLM expert used for distillation is OpenAI's text-davinci-003 (API).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFWorld (visual ALFWorld / AI2-THOR visual environment)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Long-horizon household manipulation and navigation tasks in the ALFWorld benchmark rendered by AI2-THOR; tasks include Pick & Place, Clean & Place, Heat & Place, Cool & Place, Look in Light, and Pick Two Objects & Place, often requiring interacting with >10 objects and >30 steps, using a discrete high-level textual action interface executed by the simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level textual commands in the text world / PDDL-like format (e.g., 'go to cabinet 1', 'take apple 1', 'put apple 1 in/on fridge 1').</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level discrete textual actions executed by the AI2-THOR / ALFWorld simulator (not low-level continuous motor control). The agent outputs textual action tokens that the environment executes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Cross-modality interactive imitation: each pixel observation is translated into an equivalent textual state (PDDL/metadata via the TextWorld engine); the LLM actor (text-davinci-003) generates expert textual actions x*_a for that textual state; EMMA (VLM) is trained to prefer the expert action over its own via DAgger dataset aggregation and a Direct Preference Optimization (DPO) loss, with a retrospection critic producing feedback stored in a long-term memory to improve future actor outputs. The VLM's visual tokens are linearly projected to the LLM token embedding space so the LLM decoder can autoregressively generate the same high-level textual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB pixel observations from AI2-THOR processed by a ViT encoder; metadata extraction (Observed Objects, Observed Relations, Inventory, Locations) is used to produce the textual state for the LLM expert.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>EMMA achieves state-of-the-art success rates on visual ALFWorld tasks (per Table 1): Pick: 0.82 (avg steps 19.5), Clean: 0.71 (19.3), Heat: 0.94 (17.5), Cool: 0.85 (19.6), Look: 0.83 (19.9), PickTwo: 0.88 (19.6). The paper reports EMMA improves over SOTA VLM-based agents by roughly 20%–70% relative in success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>VLM baselines finetuned by behavior cloning (e.g., InstructBLIP finetuned on the demonstration dataset) achieve very low success rates (many task-type entries near 0.00–0.27 depending on task); GPT-4V zero-shot also fails on embodied ALFWorld (near 0% in zero-shot setting). Exact per-task baseline numbers are reported in Table 1 of the paper (InstructBLIP and other VLMs show much lower success rates versus EMMA).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>(1) Semantic/planning priors from the LLM expert distilled into the VLM; (2) use of a parallel TextWorld (PDDL metadata) to bridge modalities so LLM can act on equivalent states; (3) interactive dataset aggregation (DAgger) to correct distribution shift; (4) DPO loss to prefer expert actions; (5) retrospection/critic memory to improve expert signals; (6) BC initialization on a large demonstration corpus for stable start.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Remaining challenges include perception gap between static pretraining and environment dynamics, VLM hallucination, noisy visual representations, and potential suboptimality in LLM-provided actions when environmental feedback is sparse (the paper mitigates many of these but notes them as limiting factors).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Distilling an LLM expert operating in a parallel text world into a frozen-component VLM via online cross-modality imitation (DAgger-DPO) yields large positive transfer: EMMA substantially outperforms VLMs finetuned by BC and is robust to visual noise; the approach leverages LLM planning and retrospection while aligning the VLM to visual dynamics by converting pixel states to PDDL textual states and training the VLM to emit the same high-level textual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1709.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1709.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP (baseline use)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (pretrained vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained vision-language model (ViT + Q-Former + LLM) used as the base VLM in this work; the paper uses frozen InstructBLIP components as EMMA's backbones and also evaluates InstructBLIP finetuned by behavior cloning as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>InstructBLIP (pretrained VLM components)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Vision-language architecture combining a ViT visual encoder, a Q-Former to extract cross-modal visual tokens, and an LLM decoder; in this work components are reused (frozen) and only a linear projection layer is updated for EMMA. Also used standalone as a baseline VLM finetuned by behavior cloning on ALFWorld demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Static image–text pairs (vision-language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper reuses InstructBLIP pretrained components (pretraining corpus details are according to the InstructBLIP original work, not enumerated here). Behavior-cloning finetuning used the paper's generated demonstration dataset (~15,247 episodes / ≈178,585 image-text pairs, or ~170K examples referenced) for ALFWorld adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFWorld (visual ALFWorld / AI2-THOR visual environment)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same ALFWorld household tasks as above; used as the visual-embodied benchmark for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level textual commands in ALFWorld / PDDL grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level discrete textual actions executed in AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct: the VLM maps visual input to textual action outputs via its LLM decoder after finetuning (behavior cloning) on demonstration pairs (visual observation → textual action). No cross-modality LLM teacher was used in the BC-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB pixel observations processed through a ViT-based visual encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>InstructBLIP finetuned by BC on the demonstration set achieves low success rates across ALFWorld tasks (many task-specific success rates reported near 0.00–0.27 in Table 1), substantially worse than EMMA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Vision-only architectures (ResNet-18, MCNN-FPN) and other VLM baselines also achieve very low success rates (e.g., ~0.04–0.17 depending on task); BC-only finetuning on demonstrations does not yield strong embodied-task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained visual-language features and object priors help basic object recognition and language alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretraining was static image-text alignment and not aligned with environment dynamics, leading to poor temporal/dynamics reasoning; distribution shift from training demonstrations to online interaction; sparse rewards and noisy visual observations cause failures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained VLMs (InstructBLIP) fine-tuned by offline BC on demonstrations have limited ability to solve long-horizon embodied tasks in ALFWorld; additional online cross-modality imitation and LLM distillation (as in EMMA) are required to align VLMs with dynamics and obtain strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1709.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1709.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art multimodal vision-language model mentioned as an example of a privileged VLM which fails to complete embodied ALFWorld tasks in a zero-shot visual-only setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multimodal large model with strong image–text capabilities; evaluated zero-shot on ALFWorld in the paper and observed to rely on language priors rather than visual-dynamics alignment, leading to poor task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal pretraining on image–text and language corpora (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; referenced as an off-the-shelf privileged VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFWorld (visual ALFWorld / AI2-THOR visual environment) — zero-shot evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Zero-shot application of GPT-4V to ALFWorld visual tasks where the model receives the task instruction and current-step observation and must predict the next textual action.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level textual commands (same ALFWorld action grammar).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-level discrete textual actions executed by the embedded simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Zero-shot: use VLM directly to map visual input to textual action output without task-specific finetuning or cross-modality imitation; no bridging via PDDL or LLM expert.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Multimodal vision + language input (images and instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Zero-shot GPT-4V fails to accomplish tasks in ALFWorld (the paper reports it tends to rely on language priors rather than environment dynamics; numeric zero-shot success near 0% was described qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong object and language priors from multimodal pretraining can support recognition and common-sense associations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lack of alignment between static/multimodal pretraining and environment dynamics; reliance on per-step language priors rather than sequential dynamic reasoning; no online interaction or expert guidance to address distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even powerful off-the-shelf multimodal LLMs/VLMs (GPT-4V) can fail to solve embodied tasks zero-shot because their pretraining focuses on static image–text alignment rather than sequential embodied dynamics—motivating the need for online adaptation or cross-modality distillation from a text expert as used by EMMA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1709.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1709.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003 (LLM expert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (OpenAI LLM used as retrospective expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An API-accessible large language model (GPT-3.5 family) used by the authors as the LLM actor and critic in a parallel TextWorld to produce expert actions and retrospection feedback for training EMMA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>text-davinci-003 (LLM actor + critic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An LLM used in two roles: (1) actor—given textual PDDL-like state and instruction, it outputs candidate high-level textual actions; (2) critic—analyzes past VLM trajectories to produce retrospective feedback which is stored in a memory and used to improve later actor outputs. Used as an offline/online teacher via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale internet text pretraining (general language model pretraining); exact corpora not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper beyond being an off-the-shelf OpenAI LLM (text-davinci-003) accessed via API and prompted with textual state and tasks derived from simulator metadata (PDDL).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Indirect: used to supervise EMMA on ALFWorld (text world expert supervising visual agent).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Operates in the parallel TextWorld equivalent of the ALFWorld visual environments (TextWorld PDDL states), producing high-level textual action sequences and retrospective feedback; these outputs are used to train a VLM that acts in the visual AI2-THOR environment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level textual PDDL-like actions (same action grammar used by ALFWorld/TextWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>The paper converts visual pixel observations to textual PDDL metadata (Observed Objects, Relations, Inventory, Locations) to create the LLM's input state; the LLM's textual actions are used directly as the expert target actions for EMMA. Training uses DAgger with a DPO loss to prefer the LLM action over the student's action.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>The LLM only receives textual states constructed from simulator metadata (no raw pixels); the VLM receives raw RGB pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using text-davinci-003 as a retrospective expert enables EMMA to achieve SOTA visual ALFWorld performance (EMMA results listed above). The LLM actor+critic + retrospection pipeline progressively improves expert action quality over trials (Fig. 4) and reduces the gap between expert and student.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>LLM's planning and reflection capabilities, ability to ingest symbolic PDDL-like states, and the retrospective memory loop which improves expert outputs over trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>LLM actions can still be suboptimal due to sparse environmental feedback or poor in-context instructions; the LLM requires the textual PDDL bridge (metadata) and thus is not directly operating on pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An LLM pretrained on language (text-davinci-003) can serve as a high-quality, scalable expert in a parallel text world; converting pixel states to PDDL-like textual states allows distillation of LLM planning/reflection into a VLM, producing large gains on visual embodied tasks when combined with DAgger-DPO and retrospection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>InstructBLIP: Towards general-purpose vision-language models with instruction tuning <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>EUREKA: Human-level reward design via coding large language models <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 1)</em></li>
                <li>VIMA: Robot manipulation with multimodal prompts <em>(Rating: 1)</em></li>
                <li>RT-1: Robotics Transformer for real-world control at scale <em>(Rating: 1)</em></li>
                <li>Palm-E: An embodied multimodal language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1709",
    "paper_id": "paper-265466250",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "EMMA",
            "name_full": "Embodied Multi-Modal Agent",
            "brief_description": "A vision-language agent finetuned to act in a 3D embodied visual simulator by distilling an LLM expert operating in a parallel text world via an online cross-modality imitation algorithm (DAgger-DPO) with retrospection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "EMMA",
            "model_agent_description": "Modular VLM composed of a frozen ViT encoder, a frozen Q-Former that produces 32 visual tokens, a learnable linear projection layer mapping visual tokens to text embedding space, and a frozen LLM decoder; only the linear projection (and downstream policy head) is updated. Trained online with a novel DAgger-DPO objective using an LLM actor+critic as expert and a long-term retrospection memory.",
            "pretraining_data_type": "Vision-language pretraining (static image-text pairs) for VLM components; language-model pretraining for the LLM expert (text world)",
            "pretraining_data_details": "EMMA reuses pretrained components from InstructBLIP (ViT, Q-Former, and an LLM decoder) which were pretrained on static image–text corpora (details in InstructBLIP paper). EMMA is then initialized with behavior cloning on a demonstration dataset created for ALFWorld: the paper's generated dataset contains ~15,247 expert episodes (≈178,585 image-text pairs) produced by a rule-based planner and LLM-generated task instructions (the paper also refers to ~170K demonstrations used for BC initialization). The LLM expert used for distillation is OpenAI's text-davinci-003 (API).",
            "embodied_task_name": "ALFWorld (visual ALFWorld / AI2-THOR visual environment)",
            "embodied_task_description": "Long-horizon household manipulation and navigation tasks in the ALFWorld benchmark rendered by AI2-THOR; tasks include Pick & Place, Clean & Place, Heat & Place, Cool & Place, Look in Light, and Pick Two Objects & Place, often requiring interacting with &gt;10 objects and &gt;30 steps, using a discrete high-level textual action interface executed by the simulator.",
            "action_space_text": "High-level textual commands in the text world / PDDL-like format (e.g., 'go to cabinet 1', 'take apple 1', 'put apple 1 in/on fridge 1').",
            "action_space_embodied": "High-level discrete textual actions executed by the AI2-THOR / ALFWorld simulator (not low-level continuous motor control). The agent outputs textual action tokens that the environment executes.",
            "action_mapping_method": "Cross-modality interactive imitation: each pixel observation is translated into an equivalent textual state (PDDL/metadata via the TextWorld engine); the LLM actor (text-davinci-003) generates expert textual actions x*_a for that textual state; EMMA (VLM) is trained to prefer the expert action over its own via DAgger dataset aggregation and a Direct Preference Optimization (DPO) loss, with a retrospection critic producing feedback stored in a long-term memory to improve future actor outputs. The VLM's visual tokens are linearly projected to the LLM token embedding space so the LLM decoder can autoregressively generate the same high-level textual actions.",
            "perception_requirements": "RGB pixel observations from AI2-THOR processed by a ViT encoder; metadata extraction (Observed Objects, Observed Relations, Inventory, Locations) is used to produce the textual state for the LLM expert.",
            "transfer_successful": true,
            "performance_with_pretraining": "EMMA achieves state-of-the-art success rates on visual ALFWorld tasks (per Table 1): Pick: 0.82 (avg steps 19.5), Clean: 0.71 (19.3), Heat: 0.94 (17.5), Cool: 0.85 (19.6), Look: 0.83 (19.9), PickTwo: 0.88 (19.6). The paper reports EMMA improves over SOTA VLM-based agents by roughly 20%–70% relative in success rate.",
            "performance_without_pretraining": "VLM baselines finetuned by behavior cloning (e.g., InstructBLIP finetuned on the demonstration dataset) achieve very low success rates (many task-type entries near 0.00–0.27 depending on task); GPT-4V zero-shot also fails on embodied ALFWorld (near 0% in zero-shot setting). Exact per-task baseline numbers are reported in Table 1 of the paper (InstructBLIP and other VLMs show much lower success rates versus EMMA).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "(1) Semantic/planning priors from the LLM expert distilled into the VLM; (2) use of a parallel TextWorld (PDDL metadata) to bridge modalities so LLM can act on equivalent states; (3) interactive dataset aggregation (DAgger) to correct distribution shift; (4) DPO loss to prefer expert actions; (5) retrospection/critic memory to improve expert signals; (6) BC initialization on a large demonstration corpus for stable start.",
            "transfer_failure_factors": "Remaining challenges include perception gap between static pretraining and environment dynamics, VLM hallucination, noisy visual representations, and potential suboptimality in LLM-provided actions when environmental feedback is sparse (the paper mitigates many of these but notes them as limiting factors).",
            "key_findings": "Distilling an LLM expert operating in a parallel text world into a frozen-component VLM via online cross-modality imitation (DAgger-DPO) yields large positive transfer: EMMA substantially outperforms VLMs finetuned by BC and is robust to visual noise; the approach leverages LLM planning and retrospection while aligning the VLM to visual dynamics by converting pixel states to PDDL textual states and training the VLM to emit the same high-level textual actions.",
            "uuid": "e1709.0",
            "source_info": {
                "paper_title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "InstructBLIP (baseline use)",
            "name_full": "InstructBLIP (pretrained vision-language model)",
            "brief_description": "A pretrained vision-language model (ViT + Q-Former + LLM) used as the base VLM in this work; the paper uses frozen InstructBLIP components as EMMA's backbones and also evaluates InstructBLIP finetuned by behavior cloning as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "InstructBLIP (pretrained VLM components)",
            "model_agent_description": "Vision-language architecture combining a ViT visual encoder, a Q-Former to extract cross-modal visual tokens, and an LLM decoder; in this work components are reused (frozen) and only a linear projection layer is updated for EMMA. Also used standalone as a baseline VLM finetuned by behavior cloning on ALFWorld demonstrations.",
            "pretraining_data_type": "Static image–text pairs (vision-language pretraining)",
            "pretraining_data_details": "The paper reuses InstructBLIP pretrained components (pretraining corpus details are according to the InstructBLIP original work, not enumerated here). Behavior-cloning finetuning used the paper's generated demonstration dataset (~15,247 episodes / ≈178,585 image-text pairs, or ~170K examples referenced) for ALFWorld adaptation.",
            "embodied_task_name": "ALFWorld (visual ALFWorld / AI2-THOR visual environment)",
            "embodied_task_description": "Same ALFWorld household tasks as above; used as the visual-embodied benchmark for evaluation.",
            "action_space_text": "High-level textual commands in ALFWorld / PDDL grammar.",
            "action_space_embodied": "High-level discrete textual actions executed in AI2-THOR.",
            "action_mapping_method": "Direct: the VLM maps visual input to textual action outputs via its LLM decoder after finetuning (behavior cloning) on demonstration pairs (visual observation → textual action). No cross-modality LLM teacher was used in the BC-only baseline.",
            "perception_requirements": "RGB pixel observations processed through a ViT-based visual encoder.",
            "transfer_successful": false,
            "performance_with_pretraining": "InstructBLIP finetuned by BC on the demonstration set achieves low success rates across ALFWorld tasks (many task-specific success rates reported near 0.00–0.27 in Table 1), substantially worse than EMMA.",
            "performance_without_pretraining": "Vision-only architectures (ResNet-18, MCNN-FPN) and other VLM baselines also achieve very low success rates (e.g., ~0.04–0.17 depending on task); BC-only finetuning on demonstrations does not yield strong embodied-task performance.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretrained visual-language features and object priors help basic object recognition and language alignment.",
            "transfer_failure_factors": "Pretraining was static image-text alignment and not aligned with environment dynamics, leading to poor temporal/dynamics reasoning; distribution shift from training demonstrations to online interaction; sparse rewards and noisy visual observations cause failures.",
            "key_findings": "Pretrained VLMs (InstructBLIP) fine-tuned by offline BC on demonstrations have limited ability to solve long-horizon embodied tasks in ALFWorld; additional online cross-modality imitation and LLM distillation (as in EMMA) are required to align VLMs with dynamics and obtain strong performance.",
            "uuid": "e1709.1",
            "source_info": {
                "paper_title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V(ision)",
            "brief_description": "A state-of-the-art multimodal vision-language model mentioned as an example of a privileged VLM which fails to complete embodied ALFWorld tasks in a zero-shot visual-only setting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "GPT-4V(ision)",
            "model_agent_description": "A multimodal large model with strong image–text capabilities; evaluated zero-shot on ALFWorld in the paper and observed to rely on language priors rather than visual-dynamics alignment, leading to poor task completion.",
            "pretraining_data_type": "Multimodal pretraining on image–text and language corpora (not specified in this paper).",
            "pretraining_data_details": "Not specified in this paper; referenced as an off-the-shelf privileged VLM.",
            "embodied_task_name": "ALFWorld (visual ALFWorld / AI2-THOR visual environment) — zero-shot evaluation",
            "embodied_task_description": "Zero-shot application of GPT-4V to ALFWorld visual tasks where the model receives the task instruction and current-step observation and must predict the next textual action.",
            "action_space_text": "High-level textual commands (same ALFWorld action grammar).",
            "action_space_embodied": "High-level discrete textual actions executed by the embedded simulator.",
            "action_mapping_method": "Zero-shot: use VLM directly to map visual input to textual action output without task-specific finetuning or cross-modality imitation; no bridging via PDDL or LLM expert.",
            "perception_requirements": "Multimodal vision + language input (images and instructions).",
            "transfer_successful": false,
            "performance_with_pretraining": "Zero-shot GPT-4V fails to accomplish tasks in ALFWorld (the paper reports it tends to rely on language priors rather than environment dynamics; numeric zero-shot success near 0% was described qualitatively).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong object and language priors from multimodal pretraining can support recognition and common-sense associations.",
            "transfer_failure_factors": "Lack of alignment between static/multimodal pretraining and environment dynamics; reliance on per-step language priors rather than sequential dynamic reasoning; no online interaction or expert guidance to address distribution shift.",
            "key_findings": "Even powerful off-the-shelf multimodal LLMs/VLMs (GPT-4V) can fail to solve embodied tasks zero-shot because their pretraining focuses on static image–text alignment rather than sequential embodied dynamics—motivating the need for online adaptation or cross-modality distillation from a text expert as used by EMMA.",
            "uuid": "e1709.2",
            "source_info": {
                "paper_title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "text-davinci-003 (LLM expert)",
            "name_full": "text-davinci-003 (OpenAI LLM used as retrospective expert)",
            "brief_description": "An API-accessible large language model (GPT-3.5 family) used by the authors as the LLM actor and critic in a parallel TextWorld to produce expert actions and retrospection feedback for training EMMA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "text-davinci-003 (LLM actor + critic)",
            "model_agent_description": "An LLM used in two roles: (1) actor—given textual PDDL-like state and instruction, it outputs candidate high-level textual actions; (2) critic—analyzes past VLM trajectories to produce retrospective feedback which is stored in a memory and used to improve later actor outputs. Used as an offline/online teacher via prompting.",
            "pretraining_data_type": "Large-scale internet text pretraining (general language model pretraining); exact corpora not specified in the paper.",
            "pretraining_data_details": "Not specified in this paper beyond being an off-the-shelf OpenAI LLM (text-davinci-003) accessed via API and prompted with textual state and tasks derived from simulator metadata (PDDL).",
            "embodied_task_name": "Indirect: used to supervise EMMA on ALFWorld (text world expert supervising visual agent).",
            "embodied_task_description": "Operates in the parallel TextWorld equivalent of the ALFWorld visual environments (TextWorld PDDL states), producing high-level textual action sequences and retrospective feedback; these outputs are used to train a VLM that acts in the visual AI2-THOR environment.",
            "action_space_text": "High-level textual PDDL-like actions (same action grammar used by ALFWorld/TextWorld).",
            "action_space_embodied": null,
            "action_mapping_method": "The paper converts visual pixel observations to textual PDDL metadata (Observed Objects, Relations, Inventory, Locations) to create the LLM's input state; the LLM's textual actions are used directly as the expert target actions for EMMA. Training uses DAgger with a DPO loss to prefer the LLM action over the student's action.",
            "perception_requirements": "The LLM only receives textual states constructed from simulator metadata (no raw pixels); the VLM receives raw RGB pixels.",
            "transfer_successful": true,
            "performance_with_pretraining": "Using text-davinci-003 as a retrospective expert enables EMMA to achieve SOTA visual ALFWorld performance (EMMA results listed above). The LLM actor+critic + retrospection pipeline progressively improves expert action quality over trials (Fig. 4) and reduces the gap between expert and student.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "LLM's planning and reflection capabilities, ability to ingest symbolic PDDL-like states, and the retrospective memory loop which improves expert outputs over trials.",
            "transfer_failure_factors": "LLM actions can still be suboptimal due to sparse environmental feedback or poor in-context instructions; the LLM requires the textual PDDL bridge (metadata) and thus is not directly operating on pixels.",
            "key_findings": "An LLM pretrained on language (text-davinci-003) can serve as a high-quality, scalable expert in a parallel text world; converting pixel states to PDDL-like textual states allows distillation of LLM planning/reflection into a VLM, producing large gains on visual embodied tasks when combined with DAgger-DPO and retrospection.",
            "uuid": "e1709.3",
            "source_info": {
                "paper_title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
            "rating": 2,
            "sanitized_title": "instructblip_towards_generalpurpose_visionlanguage_models_with_instruction_tuning"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "EUREKA: Human-level reward design via coding large language models",
            "rating": 2,
            "sanitized_title": "eureka_humanlevel_reward_design_via_coding_large_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "VIMA: Robot manipulation with multimodal prompts",
            "rating": 1,
            "sanitized_title": "vima_robot_manipulation_with_multimodal_prompts"
        },
        {
            "paper_title": "RT-1: Robotics Transformer for real-world control at scale",
            "rating": 1,
            "sanitized_title": "rt1_robotics_transformer_for_realworld_control_at_scale"
        },
        {
            "paper_title": "Palm-E: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        }
    ],
    "cost": 0.021545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld
29 Mar 2024</p>
<p>Yijun Yang 
Southern University of Science and Technology</p>
<p>JD Explore Academy</p>
<p>University of Technology Sydney https://github
com/stevenyangyj/Emma-Alfworld</p>
<p>Tianyi Zhou 
University of Maryland
College Park</p>
<p>Kanxue Li 
Yunnan University</p>
<p>Dapeng Tao 
Yunnan University</p>
<p>Lusong Li 
JD Explore Academy</p>
<p>Li Shen 
JD Explore Academy</p>
<p>Xiaodong He 
Jing Jiang 
JD Explore Academy</p>
<p>University of Technology Sydney https://github
com/stevenyangyj/Emma-Alfworld</p>
<p>Yuhui Shi 
Southern University of Science and Technology</p>
<p>Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld
29 Mar 2024D9AA671308005FFBEE96DE6B5AC8704CarXiv:2311.16714v2[cs.CV]
a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics.Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert.Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.</p>
<p>Abstract</p>
<p>While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals.Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics.On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient.In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world.Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in</p>
<p>Introduction</p>
<p>Embodied multi-modal agents have been acknowledged as a pivotal stride towards achieving Artificial General Intelligence (AGI), as they encompass the potential for a broad scope of intelligent activities [27,63].The rising of founda-tion models brings a glimmer of hope for constructing such agents [11,32,34,35,38,71,76], and notable efforts from the community have tried to harness them in many decisionmaking scenarios, e.g., autonomous driving [15,66], daily household robots [6,13,77], and complex manipulation tasks [25,29,42,57].</p>
<p>LLMs can perform as reflex agents interacting with a text world via verbalized descriptions of the world states and textual actions.In addition, they exhibit great potential in planning, reflection, and reward shaping.This is mainly due to their prior knowledge and semantic abstraction of the world.However, LLM-based agents cannot be directly applied in a visual world.Although current vision-language models (VLMs) try to align LLMs with the visual modality, their pretraining only focuses on static alignment between image-text pairs so the resulting agents have not been aligned well with the dynamics of the visual world.As shown in Fig. 1 (a), even the privileged VLM, e.g., GPT-4V [43], fails to accomplish tasks in an embodied ALFWorld environment [56].In such a zero-shot setting, GPT-4V tends to mainly rely on its language prior of these objects detected in the current step, rather than the alignment between the visual input and the environment dynamics conditioned on the task instruction.</p>
<p>In this paper, we study how to train a VLM towards an embodied agent in a visual world by aligning it with the visual world dynamics and distilling the skills of an LLM agent in a parallel text world.Our overarching goal is to build such an Embodied Multi-Modal Agent (EMMA) that can take a textual task instruction (e.g., from human users) and pixel observations of the state per step to produce a sequence of actions leading to the efficient completion of the task.This is a challenging problem due to (1) the sparsity of task reward [2][3][4]14], (2) noisy visual representations, (3) the hallucination of VLMs [60], and (4) the misalignment of VLM's static representations to the visual world dynamics.While offline distillation and imitation from a powerful LLM agent in a parallel text world can potentially overcome the former two challenges [75], effectively mitigating the remaining challenges necessitates online finetuning of the VLM agent within an interactive and visual world [18,31,70].</p>
<p>To this end, we finetune a VLM agent by imitation learning from an LLM expert (e.g., built on ChatGPT) launched in a parallel text world on the same tasks.Specifically, in each step of EMMA interacting with the visual world, we convert its visual observation into an equivalent textual description sent to the LLM agent, which produces an action for EMMA to imitate.Such cross-modality interactive imitation learning is based on DAgger [51], which overcomes the cumulative errors and distribution shifts caused by behavior cloning (BC).As depicted in Fig. 1 (b), an InstructBLIP [11] agent finetuned by BC on 170K expert demonstrations produced by a rule-based expert in the  visual world still fails to take correct actions based on visual observations.We further improve the DAgger's objective to be the direct preference optimization (DPO) [48], which maximizes the preference of LLM-expert's action (positive) over VLM-student's action (negative) in each interaction step.To collect better teaching signals retrospective to the VLM student's actions, the LLM expert is composed of an LLM actor prompted to output expert actions, and an LLM critic prompted for reflection feedback on the VLM agent's historical trajectories.We maintain a long-term memory storing the feedback, which is then used to induce the LLM actor to improve actions for imitation in future episodes.</p>
<p>Bird's-Eye View</p>
<p>We evaluate EMMA and compare it with vision-only agents, LLM agents, and VLM agents deployed to the ALF-World benchmark [56], which includes numerous tasks in both visual and textual environments.Extensive evaluations highlight that EMMA substantially outperforms state-ofthe-art (SOTA) VLM agents in visual-only environments by 20%-70% in terms of success rate.In addition, EMMA is the only VLM agent that can generalize to open-vocabulary and free-form test tasks, shedding novel insights on using LLM feedback to train more versatile and generalizable embodied agents in multi-modality environments.</p>
<p>Embodied Multi-Modal Agent</p>
<p>Fig. 3 illustrates the main idea of our "Embodied Multi-Modal Agent (EMMA)", whose detailed training procedures are given in Alg. 1.The agent is built upon a modularized VLM, which can follow instructions and interact with the environment through pixel observations and textual actions.To overcome these challenges associated with training EMMA, such as sparse reward or distribution shift, we explore the construction of an LLM expert from a parallel text world (Sec.2.2) for providing EMMA with step-by-step guidance.Lastly, Sec.2.3 further discusses how to harness the LLM expert to train EMMA via cross-modality imitation learning.</p>
<p>EMMA in the Visual World</p>
<p>In visual environments, EMMA π θ is designed to process a task instruction x task (e.g., provided by human users) and the pixel observation s t v at each interaction step t.It is expected to generate a sequence of high-level textual actions
{x t a ∼ π θ (•|x task , s t v )} T t=0
1 towards efficient completion of the task.To achieve this, we draw inspiration from recent advances of large pretrained VLMs [11,32,34,38,76], and modularize EMMA's architecture into four components: (1) a ViT to encode s v into visual embeddings, (2) a querying transformer (Q-Former) tailored to extract the most relevant visual features via the cross-attention between the visual embeddings and query tokens, (3) a linear projection layer to align visual features to text embeddings, (4) an LLM decoder taking the concatenation of the instruction tokens and the output of the linear projection layer to autoregressively generate the action x a .In order to reduce computational overhead and prevent catastrophic forgetting [37], we adopt the pretrained ViT, Q-Former, and LLM from InstructBLIP [11] and keep them frozen at the finetuning stage.We only update the linear projection layer, as illustrated in Fig. 3.Such a modularized architecture enables EMMA to integrate any existing pretrained vision models and LLMs in a flexible and computationally-efficient way.</p>
<p>However, deploying EMMA into a complex visual world remains challenging.One of the main obstacles is that the direct use of any pretrained VLM is suboptimal because existing pretraining only focuses on static alignment between 1 For brevity, we omit x task in the rest of this paper.</p>
<p>image-text pairs [11,32,34], so that the resulting agent may struggle to reason about the dynamics of the world.As discussed in Sec. 1, even the SOTA VLM, i.e., GPT-4V, fails to accomplish tasks in an embodied ALFWorld environment [56].In such a zero-shot setting, GPT-4V tends to rely on the linguistic prior of the currently detected objects, rather than the given task instruction and the underlying dynamics of the environment, to guide the interactions.Moreover, finetuning a pretrained VLM on a pre-collected demonstration dataset is also suboptimal due to the diversity of environments and tasks [77], the lack of large-scale expert annotations [44] as well as the challenges posed by the distribution shift issue [31].A seemingly natural solution to the above challenges is reinforcement learning from environmental feedback (RLEF) [68], in which reward signals rely on decomposing a task into a sequence of reasonable subgoals and checking their completion.However, in real-world scenarios, most sub-goals cannot be defined or described precisely, so the reward is sparse; hence, we do not expect RLEF to be effective.</p>
<p>To this end, we propose to leverage interactive imitation learning (IL) to align EMMA with the dynamics of any environment, which however results in two critical algorithmic challenges: (1) How to obtain a high-quality, accessible, and scalable expert that EMMA can query during IL (Sec.2.2)? (2) Designing an effective strategy to train EMMA using this expert in complex, diverse, and potentially open-ended environments (Sec.2.3).</p>
<p>LLM Expert from a Parallel TextWorld</p>
<p>Thanks to a series of prompting techniques in the realm of in-context learning, such as chain-of-thought [62], tree-ofthought [72], ReAct [73], and Reflexion [54], pretrained
Get τ i v = [x task , s 0 v , x 0 a , . . . , s T v , x T a ] via E v with π θ 6: Get τ i l = [x task , s 0 l , x 0 a , . . . , s T l , x T a ] via E l with τ i v 7:
Generate retrospective feedback P i = M c (τ i l )</p>
<p>8:</p>
<p>Update P with P i (i.e., P ← P ∪ P i )</p>
<p>9:</p>
<p>for t = 0 to T do ▷ Dataset Aggregation 10:</p>
<p>x * a = M a (P, x task , . . ., x t−1 a , s t l )</p>
<p>11:
D ← D ∪ {(x task , s t v , x t a , x * a )} 12:
for j = 0 to I e − 1 do ▷ Gradient Descent on θ 13:</p>
<p>Sample a mini-batch τ from D 14:</p>
<p>Update θ by minimizing Eq. ( 2) with π ref and τ 15:
i ← i + 1 16: output: π θ *
LLMs have demonstrated impressive zero-shot performance across many decision-making scenarios [6,13,15,25,29,42,57,66,77].Despite the great potential in serving as highquality and scalable experts, they are only able to interact with the environment via textual descriptions of the states, rather than using raw pixel observations like EMMA.To bridge this gap, we convert each pixel observation s v into a textual equivalent by extracting its metadata from the simulator [30], which is composed of attributes such as Observed Objects, Observed Relations, Inventory, and Locations.We then employ the Planning Domain Definition Language (PDDL) [1] to describe this metadata and create an equivalent textual description/state s l using the TextWorld engine [10].Additional details are available in Appendix 7 and an example of this process is illustrated in Fig. 2.This methodology enables the utilization of any pretrained LLM agent that generates a sequence of actions facilitating the training of EMMA through cross-modality IL between the two agents.</p>
<p>Training EMMA via Cross-Modality Imitation</p>
<p>Given an LLM expert from the parallel TextWorld, we aim to train a VLM agent π θ in the visual world to imitate its behaviors closely.This is equal to minimizing the following objective under the distribution of states induced by π θ .
θ * = arg min θ∈Θ E π θ [L imit (π θ (x a |s v ), x * a ))],(1)
in which the choice of loss function L imit is dependent on specific scenarios.For instance, it may be the expected crossentropy loss for the discrete action space, or the expected MSE loss for the continuous action space.In our case, we select DPO [48] loss due to its proven superior performance to the cross-entropy on aligning models with expert preferences within the discrete language space.Hence, Eq. ( 1) can be extended to the formulation below.
θ * = arg min θ∈Θ −E π θ [L imit (π θ , π ref , s v , x a , x * a )],(2)L imit (•) ≜ log σ β log π θ (x * a |s v ) π ref (x * a |s v ) − β log π θ (x a |s v ) π ref (x a |s v )
where x * a is the action given by an expert while x a is the action given by π θ , σ is the logistic function, and β is a hyperparameter controlling the deviation from π ref , i.e. the reference agent obtained by behavior cloning on a demonstration dataset produced by a rule-based expert in the visual world (see Appendix 9 for complete details).This regularization is essential, as it prevents the agent from deviating too far from the distribution on which the expert is accurate, as well as maintaining the generation diversity and avoiding premature convergence to some easy tasks [48].In practice, the VLM agent π θ is also initialized to π ref for stabilizing the training process.Since environmental dynamics is both unknown and complex, we cannot compute the distribution of states visited by π θ and can only sample it by rolling out the agent.Hence, Eq. ( 2) is a non-i.i.d.supervised learning problem due to the dependence of the state distribution on π θ itself, in which naïve behavior cloning faces issues like cumulative error and distribution shift [26].To address this, we employ an interactive IL algorithm, DAgger [51], which provably converges to the optimal agent π θ * .</p>
<p>As discussed in Sec.2.2 and Sec.2.3, we can harness an LLM expert to generate a sequence of actions, serving as x * a in Eq. ( 2).However, these actions may be suboptimal due to sparse environmental feedback [54,68] or defective in-context instructions [9].To collect better teaching signals, we introduce a "retrospective LLM expert" that is composed of two specialized models: an actor (M a ), built upon an API LLM and prompted to generate actions based on the task instruction and state observations; and a critic (M c ), also based on the same LLM, but designed to analyze EMMA's historical interactions and provide reflective feedback.A long-term memory P is maintained to store the feedback generated by M c , which is then used to prompt M a for improved actions.The complete procedure is detailed in Line 7-10 of Alg. 1, and all prompts are provided in Sec.6 of the Appendix.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Environments.We base our experiments on the ALFWorld benchmark [56], a cross-modality simulation platform that encompasses a wide range of embodied household tasks.For + 0.63 + 0.37 Figure 4: Comparison of success rate between EMMA and the retrospective LLM expert.As the number of trials increases, the LLM expert progressively improves its performance through retrospective processes, and also the gap between the two agents decreases, indicating the effectiveness of cross-modality imitation.We separately plot the curve per task type in Fig. 9  each task, ALFWorld integrates a visual environment, rendered by the Ai2Thor simulator [30], with a corresponding textual environment.Textual environments employ the Planning Domain Definition Language (PDDL) [1] to translate each pixel observation from the simulator into an equivalent text-based observation, and then construct an interactive world using the TextWorld engine [10]., "open safe 1", or "heat egg 1 with microwave 1", following a predefined instruction.These actions involve navigating and interacting with the environment.To provide a comprehensive understanding, we have visualized an example of each task type in Fig. 8 of the Appendix.A task in this benchmark may involve interactions with over 10 objects and require more than 30 steps for a human expert to solve, thus challenging an agent's capabilities in long-horizon planning, instruction following, and the utilization of commonsense knowledge.For a fair comparison, we follow the same setting as prior work [41,[54][55][56]73] and evaluate all baselines using 134 out-of-distribution (OOD) tasks.</p>
<p>Baselines.To verify the effectiveness of cross-modality imitation learning, we compare our EMMA with several baselines and SOTA agents using the ALFWorld benchmark with both visual and textual environments.The compared agents can be divided into three categories: vision models, language models, and vision-language models.Concretely, vision models, including ResNet-18 [20] and MCNN-FPN [21], utilize pretrained vision encoders to extract salient features from each pixel observation.The extracted features then serve as input for a Multi-Layer Perceptron (MLP) policy, which is trained by behavior cloning on a pre-collected demonstration dataset.Unlike vision models performing in the visual environment, language models complete the same tasks but in a parallel, text-based environment.BUT-LER [56] employs a transformer seq2seq model enhanced with a pointer softmax mechanism [19].This architecture aggregates previous observations as input to generate textbased actions token-by-token.GPT-BUTLER [41], a variant of the GPT-2 model [45], is initially pretrained on a static demonstration dataset and further finetuned using data collected online.ReAct [73] takes a novel approach by utilizing LLMs to generate reasoning traces and task-specific actions in an interleaved manner.This method aids the agent in developing, tracking, and updating its action plans interactively.Reflexion [54] similarly employs an LLM, but it focuses on reflecting upon environmental feedback.It maintains this reflective text in an episodic memory buffer, enhancing the agent's ability to improve actions in subsequent trials.Similar to Reflexion, a concurrent work, DEPS [61], also corrects errors in previous LLM-generated actions by integrating descriptions of the action execution process and providing self-explanations for the feedback.Moreover, beyond the single-agent framework, AutoGen [64] exhibits the potential to accomplish a broad spectrum of tasks through the cooperation of multiple LLM agents.Finally, we consider a range of vision-language models, such as MiniGPT-4 [76], BLIP-2 [34], LLaMA-Adaptor [17], and InstructBLIP [11], as agents to interact with the visual environment.Unlike pure vision or language models, VLMs are designed to process and integrate both visual and textual data, offering a more holistic understanding of the environment.To align these agents with the specific requirements of the ALFWorld benchmark, we finetune them on a pre-collected demonstration dataset.This finetuning process is crucial as it enables the agent to comprehend and adhere to ALFWorld's unique grammar and to develop a basic "gamesense".</p>
<p>Training Details</p>
<p>The architectural design of EMMA is depicted in Fig. 3.At its core, EMMA employs a Query Transformer (Q-Former) to process visual data.This Q-Former extracts features using a frozen ViT encoder.Its output consists of 32 visual tokens, which are then passed through a linear projection layer before being fed to a frozen LLM decoder.Similar to other VLM agents, EMMA is also finetuned on a pre-collected demonstration dataset, aligning its basic ability with the ALFWorld benchmark.In order to align EMMA with the dynamics of ALFWorld, we train it by imitating an LLM expert (see Alg. 1).We choose text-davinci-003 developed by OpenAI as our LLM expert because of its established capabilities in reasoning and planning [54,61,64,73].In this setup, text-davinci-003 serves dual roles: it serves as an actor, providing EMMA with expert actions, and as a critic, analyzing EMMA's historical trajectories.This analysis generates retrospective feedback, which is then incorporated into the actor's long-term memory, leading to improved actions in future trials.Further details about the hyperparameters and prompts used in our training procedure are available in Table 2 of the Appendix.</p>
<p>Comparison with State of the Art</p>
<p>EMMA sets new SOTA performance in visual ALFWorld.</p>
<p>In this section, we compare EMMA with 12 other representative agents on the ALFWorld benchmark, spanning both visual and textual environments (refer to Table 1).We assess two key metrics: the success rate, which is the percentage of trials completed successfully, and the average number of interaction steps required for task completion, with a lower number indicating higher efficiency.EMMA demonstrates superior performance in both metrics, significantly outperforming all VLM agents in visual environments.This achievement underscores the effectiveness of our crossmodality imitation learning approach, as depicted in the learning curve shown in Fig. 4. Furthermore, EMMA's performance markedly exceeds that of VM agents, highlighting the crucial role of the prior knowledge embedded in VLMs.Intriguingly, EMMA's performance is comparable with LLM agents that operate using perfectly semantic descriptions of visual observations.This is largely attributed to EMMA's training strategy of imitating an expert LLM agent, proving to be more efficient than learning from scratch in a purely visual setting.As a result, EMMA stands out as the only VLM agent that substantially surpasses SOTA LLM agents, such Given a specific noise rate, we crop a random portion of the pixel observation and resize it to a given size as the new observation.Similarly, we randomly replace some tokens in the textual observation with arbitrary ones.</p>
<p>as AutoGen [64] and DEPS [61], in these environments.And its success also directs a potential way to achieve humanlevel performance in the visual environments of ALFWorld.</p>
<p>EMMA is more robust to noisy observations than LLM agents.While LLM agents exhibit a higher success rate with fewer interaction steps in textual environments, as indicated in Table 1, we hypothesize that this superior performance largely relies on their precise semantic abstraction of the environment.However, such an abstraction might not be feasible in real-world applications.To verify this assumption, we set up a more practical scenario where observations are deliberately perturbed at a specific noise rate.We then compare the robustness of EMMA and a SOTA LLM agent, Reflexion, under these noisy observations.To generate noisy observations, a random portion of the visual observation is cropped, resized, and then used to replace the original observation.Similarly, in the textual observation, random tokens are substituted with arbitrary ones.As illustrated in Fig. 5, with the noise rate increasing, EMMA's performance remains significantly more robust than Reflexion.This could be attributed to the vision encoder in the VLM, which is adept at filtering out visual noises.On the other hand, textual noises are directly processed by the LLM, which can substantially impair the performance of LLM-based agents.This finding highlights the potential advantages of VLM agents like EMMA in visual scenarios, in which data is often imperfect and noisy.with a key variation: "EMMA w/o Retrospection", as shown in Fig. 6 (left).This variant of EMMA is trained using the same procedure as the original EMMA but removes the retrospective process.Instead, it relies solely on a plain LLM actor to provide relabeled actions.The results show that EMMA with the retrospective mechanism significantly outperforms its counterpart.This finding is crucial as it indicates that the retrospective process is not just a supplementary feature but a fundamental component of EMMA's architecture that contributes substantially to its enhanced performance.</p>
<p>Ablation Study</p>
<p>EMMA benefits from BC initialization.Through an ablation study, we evaluate the impact of behavior cloning (BC) initialization, a process described in line 3 of Alg. 1. Fig. 6 (middle) demonstrates that EMMA, when deprived of BC initialization, experiences a slight reduction in the average success rate across 134 unseen tasks compared to its original setup.Despite this decrease, EMMA without BC initialization still outperforms other VLM agents, as clearly shown when compared with the results in Table 1.Furthermore, Fig. 10 in the Appendix breaks down the result by task type.It reveals a consistent but slight drop in performance across 5 out of the 6 task types.These results reflect that while BC initialization contributes positively to EMMA's overall performance, it is not critical for achieving the notable results we have reported.DPO enables more effective imitation learning.To evaluate the effectiveness of using DPO loss in Eq. ( 1), we conducted an ablation study with an alternative version of EMMA, referred to "EMMA w/ Cross Entropy (CE) Loss".In this variant, EMMA is optimized using the token-level CE loss, a common objective for finetuning VLMs.The results, as depicted in Fig. 6 (right), reveal that EMMA w/ CE Loss does not achieve the same high success rate as the original EMMA with DPO loss, suggesting that DPO loss contributes to enhancing the upper performance bound of EMMA.In addition, we noted that EMMA w/ CE Loss exhibits faster convergence in the initial training stages than the original EMMA.This premature convergence leads to the agent paying attention to the expert actions from easier tasks, usually addressed in the first few training epochs.This can suppress EMMA's exploration and learning on more complex tasks, resulting in worse performance.</p>
<p>Generalization to Free-form Task Instructions</p>
<p>The ability of AI agents to preciously follow task instructions given in open-vocabulary and free-form text is crucial for real-world problems.To assess this, we conducted an additional experiment focusing on the generalization capabilities of EMMA, an agent trained with template task instructions.In this experiment, we re-evaluated the performance of EMMA and other baseline agents on 134 unseen tasks, using human-annotated instructions instead of template ones.These human-annotated instructions include a large amount of OOD verbs and objects, presenting a more realistic and challenging scenario for the agents to address.</p>
<p>To underscore this challenge, we compared the vocabulary distribution between the template and human-annotated instructions, as shown in Fig. 7 (right).Moreover, we provide a comprehensive analysis of the vocabulary used across both instruction types in Fig. 11 of the Appendix.In Fig. 7 (left), EMMA demonstrates a slight performance decline, while a significant degradation is observed in other baselines.We also note that Reflexion, a SOTA LLM agent, exhibits exceptional generalization to those OOD instructions.According to these empirical results in Fig. 7, we have the following conclusions: (1) EMMA obtains and benefits from the generalization capabilities inherent in the SOTA LLM agent through cross-modality imitation learning; (2) Our work sheds novel insights on using LLM feedback to train more versatile and generalizable embodied agents.</p>
<p>Related Work</p>
<p>Agents based on Foundation Models.Recent research has increasingly focused on harnessing the capabilities of large ✓ ✗ 0.00 (30.0) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0)BLIP-2 [34] ✓ ✗ 0.01 (29.7) 0.04 (29.1) 0.03 (29.5) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0)LLaMA-Adapter [17] ✓ ✗ 0.02 (29.6) 0.04 (29.3) 0.03 (29.4) 0.04 (29.3) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0)InstructBLIP [11] ✓ ✗ 0.01 (29.8) 0.00 (30.0) 0.03 (29.3) 0.00 (30.0) 0.00 (30.0) 0.00 (30.0) 0.00 (30.pre-trained foundation models to build AI agents [65,69].These models (e.g., LLMs), benefiting from their commonsense knowledge inherited from Internet-scale pretraining, are able to reason actions according to descriptions of the external environments.For example, given a set of task instructions, LLMs can be elaborately prompted to perform as agents generating high-level step-by-step plans [7,24,54,64,73], and each step can be parsed into a sequence of robotic actions that are executed via pretrained policies or available APIs [6,36,59].Furthermore, by using VLMs [32], plans can also be conditioned on visual inputs that are transformed into language descriptions [7,16,25,57] or token embeddings aligned with LLMs [13,42,68,77].However, existing foundation models are usually pretrained on static text or text-image datasets and thus may struggle to align with the dynamics of the world.To bridge this gap, we study how to finetune a VLM to be an embodied agent dynamically aligned with the world by distilling the cross-modality knowledge from an LLM expert.The work most closely related to ours is EUREKA [40], which also explores using source information provided by the simulator as the context of an LLM to aid agent training.Instead of directly mimicking the output of the LLM as we did, EUREKA harnesses the coding LLM to generate a desired reward function for a given task and optimizes a policy against the reward function using RL, leading to a more complex and expensive training procedure [47,52,53].</p>
<p>Imitation Learning.Imitation learning is the study of algorithms that improve performance by mimicking an expert's decisions and behaviors.We summarize three main categories of existing methods in the following: (1) behavior cloning (BC), (2) inverse reinforcement learning (IRL), and (3) the combination of imitation and reinforcement learning.The naïve BC [5] ignores the change in distribution and simply trains a policy that only performs well under the distribution of states visited by the expert.Following works, such as dataset aggregation [51] or policy aggregation [28,49], have been proposed to address the limitations of BC.Another line of work, IRL, is a more complicated algorithm framework that learns the reward function from expert demonstrations and then improves the policy using RL with the learned reward.A representative method in this category is generative adversarial imitation learning (GAIL) [23], in which a policy and a discriminator compete with each other in order to maximize the likelihood of the policy's behavior matching the expert.The third category of methods usually leverages an IL policy to initialize RL and continues to boost its performance via online collected data from RL [8,50,58].This simple combination significantly improves RL's sample efficiency and IL's upper performance bound constrained by the expert.Nevertheless, all of the above methods assume that the expert and the imitator understand the world in the same modality, and thus overlook the fact that the complementary knowledge from other modalities often boosts the model's accuracy and generalization dramatically [46,67].</p>
<p>Conclusion</p>
<p>We create EMMA, an Embodied Multi-Modal Agent, by finetuning a VLM in an embodied visual world with interactive imitation learning from an LLM expert in a parallel text world, who produces better actions and retrospective feedback to VLM's trajectories.Such imitation learning exhibits substantial advantages over vision or VLM policies directly finetuned in the visual world or finetuned by behavior cloning of a rule-based expert, and SOTA API VLMs such as GPT-4V(ision).As a result, EMMA achieves a comparable success rate and much better robustness in the noisy visual world than its LLM teacher in the easier text world.Furthermore, EMMA shows powerful generalization to openvocabulary and free-form task instructions, highlighting its potential in real-world scenarios.</p>
<p>Full Prompts for LLM Expert</p>
<p>In this section, we provide all LLM prompts for the training procedure (Alg. 1) of EMMA.We adopt the prompting technique developed by ReAct [73] but ignore the reasoning traces, i.e., "think" steps, when executing imitation learning between EMMA and the LLM actor.After each trial i, the retrospective feedback P i generated by the LLM critic will be appended to long-term memory P. In practice, we bound P by a maximum number of stored feedback (usually set to 1-3) to adhere to the max context length of the LLM.</p>
<p>An example of the prompt adopted by LLM actor for generating actions {environment: You are in the middle of a room.Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1. task instruction: Your task is to: put some spraybottle on toilet.&gt; think: To solve the task, I need to find and take a sparybottle, then put it on toilet.&gt; think: First I need to find a spraybottle.A spraybottle is more likely to appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1)</p>
<p>Collection of Demonstration Dataset</p>
<p>Fine-tuning pretrained VLMs on a pre-collected demonstration dataset via behavior cloning is a critical step, enabling these models to comprehend and follow the unique grammar of ALFWorld as well as to develop a basic "gamesense".However, the number of task instructions in the original ALFWorld [55] is too limited to yield sufficient data for fine-tuning these large pretrained VLMs effectively.Hence, we propose an automated pipeline, which leverages text-davinci-003 and a rule-based planner to generate a large amount of new instructions and their resulting expert demonstrations, respectively.</p>
<p>To generate a diverse set of new task instructions, we harness the in-context learning capabilities of LLM.Our procedure begins with extracting detailed descriptions from the ALFWorld benchmark for each environment, providing comprehensive information on the number and functional attributes of all items.Then, based on the types of room in these environments, we design different prompts that aim at inducing the LLM to generate task instructions aligned with the features of the target environment.An example of these prompts is detailed in Table 3.For each generated task instruction, we gather demonstrations {x task , s t v , x t a } T t=0 using a rule-based planner devised by ALFWorld.It's important to note that this planner operates with an unfair advantage: it considers the environment as fully observable and has complete information of world dynamics, relying on metadata that is not accessible to the agent during training.In summary, our dataset comprises 15247 expert demonstration episodes, amounting to 178585 image-text pairs.Q: environment: You are in the middle of a room.Looking quickly around you, you see a cabinet, a countertop, a cabinet, a countertop, a drawer, a drawer, a drawer, a stoveburner, a stoveburner, a drawer, a stoveburner, a stoveburner, a cabinet, a cabinet, a microwave, a cabinet, a cabinet, a cabinet, a sink, a sinkbasin, a fridge, a toaster, a coffeemachine, a cabinet, a drawer, a drawer, a drawer, a drawer, a shelf, a shelf, a countertop, a shelf, a drawer, and a garbagecan.object dictionary: all of operable objects are listed in the following dictionary with a consistent format {type of operation: {object name: number of objects}}: {'pickupable': {'dishsponge': 3, 'apple': 2, 'butterknife': 3, 'peppershaker': 2, 'saltshaker': 3, 'bowl': 2, 'spatula': 2, 'pot': 3, 'winebottle': 3, 'statue': 2, 'creditcard': 3, 'plate': 2, 'pan': 2, 'kettle': 3, 'soapbottle': 3, 'potato': 3, 'fork': 2, 'bread': 2, 'knife': 3, 'glassbottle': 3, 'book': 1, 'tomato': 1, 'vase': 2, 'egg': 1, 'papertowelroll': 1, 'cup': 1, 'lettuce': 1, 'spoon': 1, 'mug': 1}, 'sliceable': {'apple': 2, 'potato': 3, 'bread': 2, 'tomato': 1, 'egg': 1, 'lettuce': 1}, 'receptacle': {'bowl': 2, 'pot': 3, 'plate': 2, 'pan': 2, 'stoveburner': 4, 'drawer': 9, 'countertop': 3, 'cabinet': 9, 'microwave': 1, 'shelf': 3, 'toaster': 1, 'garbagecan': 1, 'cup': 1, 'fridge': 1, 'coffeemachine': 1, 'sinkbasin': 1, 'mug': 1}, 'canFillWithLiquid': {'bowl': 2, 'pot': 3, 'winebottle': 3, 'kettle': 3, 'glassbottle': 3, 'houseplant': 1, 'cup': 1, 'mug': 1}, 'breakable': {'bowl': 2, 'winebottle': 3, 'statue': 2, 'plate': 2, 'glassbottle': 3, 'window': 1, 'vase': 2, 'egg': 1, 'cup': 1, 'mug': 1}, 'dirtyable': {'bowl': 2, 'pot': 3, 'plate': 2, 'pan': 2, 'cup': 1, 'mug': 1}, 'openable': {'kettle': 3, 'drawer': 9, 'cabinet': 9, 'book': 1, 'microwave': 1, 'fridge': 1}, 'cookable': {'potato': 3}, 'toggleable': {'stoveknob': 4, 'microwave': 1, 'toaster': 1, 'coffeemachine': 1, 'lightswitch': 1, 'faucet': 1}} generate 30 diverse tasks based on the environment description and object dictionary.A: Task 1: pick clean then place in recep, put a clean pan in fridge Task 2: pick and place simple, put a apple in countertop Task 3: pick two obj and place, put two apple in garbagecan Task 4: pick heat then place in recep, put a hot apple on plate Task 5: pick cool then place in recep, put a cool apple in countertop Task 6: pick and place simple, put a mug in coffeemachine Task 7: pick two obj and place, put two creditcard in plate Task 8: pick clean then place in recep, put a clean mug in coffeemachine Task 9: pick heat then place in recep, put a hot mug on plate Task 10: pick cool then place in recep, put a cool mug in plate Task 11: pick and place simple, put a peppershaker in cabinet Task 12: pick two obj and place, put two peppershaker in shelf Task 13: pick clean then place in recep, put a clean fork in pot Task 14: pick heat then place in recep, put a hot fork on bowl Task 15: pick cool then place in recep, put a cool fork in plate Task 16: pick and place simple, put a statue in countertop Task 17: pick two obj and place, put two bowl in microwave Task 18: pick clean then place in recep, put a clean potato in fridge Task 19: pick heat then place in recep, put a hot potato on plate Task 20: pick cool then place in recep, put a cool potato in pot Task 21: pick and place simple, put a egg in countertop Task 22: pick two obj and place, put two bread in microwave Task 23: pick clean then place in recep, put a clean egg in garbagecan Task 24: pick heat then place in recep, put a hot egg on bowl Task 25: pick cool then place in recep, put a cool egg in pan Task 26: pick and place simple, put a pan in stoveburner Task 27: pick two obj and place, put two pot in stoveburner Task 28: pick clean then place in recep, put a clean tomato in coffeemachine Task 29: pick heat then place in recep, put a hot tomato on plate Task 30: pick cool then place in recep, put a cool tomato in plate Q: environment: ... object dictionary: ... generate 30 diverse tasks based on the environment description and object dictionary.A: ...LLM-generated task instructions... Table 3: An example of the prompt for generating new task instructions in the kitchen</p>
<p>5 Figure 1 :
51
Figure 1: Comparison of three VLM-based agents in the visual environment from ALFWorld.Given the task instruction and the current-step observation as inputs, a VLM agent is expected to predict an action, e.g., "go to cabinet 1", towards completing the task.(a) GPT-4V(ision).(b) InstructBLIP finetuned by behavior cloning on a static dataset of demonstrations by a rule-based expert in the visual simulator.(c) Our Embodied Multi-Modal Agent (EMMA) trained via cross-modality interactive imitation learning from an LLM expert.</p>
<p>Figure 2 :
2
Figure 2: An example of tasks generated for the two parallel worlds.A VLM agent in the visual world and an LLM agent in the text world as household robots instructed to clean an apple and then put it into the fridge.Zoom in for more details.</p>
<p>Figure 3 :
3
Figure 3: Embodied Multi-Modal Agent (EMMA) trained by an LLM expert via cross-modality imitation learning.EMMA takes a textual task instruction and pixel observations as its input state per step to generate a sequence of actions using a VLM.Then, we convert each pixel observation into a textual equivalent as the context of an LLM expert to produce improved actions for EMMA to imitate.</p>
<p>Algorithm 1
1
DAgger-DPO with a single task instruction 1: initialize: i = 0, D ← ∅, EMMA π θ , LLM actor M a with a FIFO memory pool P ← ∅, LLM critic M c 2: input: max trials I, training epochs I e , visual and textual envs E v , E l , instruction x task , reference agent π ref 3: Initialize π θ to π ref ▷ Behavior Cloning Initialization 4: while task not completed or i &lt; I do 5:</p>
<p>Fig. 2
2
provides an illustrative example of the tasks featured in ALFWorld.The tasks within the benchmark are categorized into 6 types: Pick &amp; Place, Clean &amp; Place, Heat &amp; Place, Cool &amp; Place, Look in Light, and Pick Two Objects &amp; Place.Each task requires an agent to execute a series of text-based actions, such as "go to safe 1"</p>
<p>Figure 5 :
5
Figure5: Comparison of robustness between VLM and LLM agents.Given a specific noise rate, we crop a random portion of the pixel observation and resize it to a given size as the new observation.Similarly, we randomly replace some tokens in the textual observation with arbitrary ones.</p>
<p>1 ) p u t p la c e m o v e c h il l tu rn w a s h c le a n ta k e h e a t e x a m in e lo o k c o o l c o o k fi ll p ic k w a rm re a d h o ld m ic ro w a v e s ta n d u s e tr a n s fe r th ro w re tu rn a c q u ir e r in s e s e c u re le a v e g e
1
(22.0)  0.72 (21.7) 0.65 (22.7) 0.72 (22.0) 0.56 (23.9) 0.67 (23.3) 0.76 (18.</p>
<p>Figure 7 :
7
Figure7: Generalization to open-vocabulary, free-form task instructions.Left: These instructions are provided by different human annotators using Amazon Mechanical Turk[55].Right: Frequency distribution of verbs for human-annotated and templated task instructions.</p>
<p>take apple 1 from table 1</p>
<p>Welcome to the TextWorld!Your task is to: put a ... ...You arrive at loc 2. On the table 1, you see knife 1, apple 1, cup 1... &gt; You pick up the apple 1 from the table 1. ...You arrive at loc 4. On the basin 1, you see ... &gt;
21clean apple 1 with basin 13</p>
<p>clean apple 1 with basin 1</p>
<p>You clean the apple 1 using the basin 1. ... ...You open the fridge 1.The fridge 1 is open.In it, you see egg 1, cup 2 ... &gt;</p>
<p>put apple 1 in/on fridge 1 LLM Agent in TextWorld
take apple 1from table 1put apple 1in/on fridge 1</p>
<p>Instruction: put a clean apple on fridge.</p>
<p>open cabinet 1 Loss Metadata Gradients pick up pan 2
LLM DecoderLong-termEmbodied Multi-LLM ActorMemoryLinear LayerModal AgentShort-termLLM CriticQ-FormerMemoryQueriesViTAi2Thor Simulator(Visual Env.)</p>
<p>Inference Training Visual State Task ... Textual Action Retrospective LLM Expert Text State + Task Textual Action
VL-alignedTokenizer EmbeddingPretrained and frozen Learnable weightsUpdatable prompts</p>
<p>Retrospective Process Action You arrive at cabinet 1. cabinet 1 is closed. Your task is to: Put a pan on the dining table Your task is to:... Text State + Task Visual State + Task Parallel TextWorld (Textual Env.)</p>
<p>Table 1 :
1
[56]ppendix.Comparison with the state of the arts.<em>-reported in previous work.VMs = vision models, LMs = language models, VLMs = vision-language models."VisualEnv."and "Textual Env." refer to the visual and the parallel textual environments from ALFWorld[56], respectively.✓/✗ denotes the corresponding environment used/not used to deploy the agent.The highest scores for each task in the same type of environment are highlighted in bold.The average interaction steps are given in the (parentheses).EMMA substantially outperforms other SOTA VLM agents in the visual environments, and its success also directs a promising way to achieve human-level performance in ALFWorld.
AgentVisual Textual Env. Env.Avg.PickSuccess Rate with Template Task Instruction Clean Heat CoolLookPick2VMsResNet-18</em> [56] MCNN-FPN<em> [56]✓ ✓✗ ✗0.06 (-) 0.05 (-)------------BUTLER</em> [56]✗✓0.26 (-)0.31 (-)0.41 (-)0.60 (-)0.27 (-)0.12 (-)0.29 (-)GPT-BUTLER [41]✗✓0.69 (18.8) 0.62 (18.4) 0.81 (18.4) 0.85 (12.7) 0.78 (15.6) 0.50 (24.4) 0.47 (26.6)LMsReAct [73] Reflexion [54]✗ ✗✓ ✓0.54 (20.6) 0.71 (18.1) 0.65 (18.8) 0.62 (18.2) 0.44 (23.2) 0.28 (23.7) 0.35 (25.5) 0.91 (18.7) 0.96 (17.4) 1.00 (17.0) 0.81 (19.4) 0.83 (21.6) 0.94 (16.9) 0.88 (21.6)DEPS<em> [61]✗✓0.76 (-)0.93 (-)0.50 (-)0.80 (-)1.00 (-)1.00 (-)0.00 (-)AutoGen</em> [64]✗✓0.77 (-)0.92 (-)0.74 (-)0.78 (-)0.86 (-)0.83 (-)0.41 (-)MiniGPT-4 [76]✓✗0.16 (26.9) 0.04 (29.0) 0.00 (30.0) 0.19 (26.3) 0.17 (26.7) 0.67 (17.7) 0.06 (28.9)VLMsBLIP-2 [34] LLaMA-Adapter [17] InstructBLIP [11]✓ ✓ ✓✗ ✗ ✗0.04 (29.5) 0.00 (30.0) 0.06 (29.3) 0.04 (29.9) 0.11 (28.2) 0.06 (29.2) 0.00 (30.0) 0.13 (27.5) 0.17 (26.4) 0.10 (28.6) 0.27 (24.2) 0.22 (26.7) 0.00 (30.0) 0.00 (30.0) 0.22 (26.2) 0.50 (21.5) 0.26 (25.0) 0.23 (27.2) 0.06 (28.9) 0.17 (26.8) 0.00 (30.0)EMMA (Ours)✓✗0.82 (19.5) 0.71 (19.3) 0.94 (17.5) 0.85 (19.6) 0.83 (19.9) 0.88 (19.6) 0.67 (22.4)Human Performance* [55]✓✗0.91 (-)------</p>
<p>Ablation study.left: The ability of LLM expert to reflect on and learn from past actions appears to be a key factor in achieving the impressive results we observed.middle: The "w/o BC initialization" variant uses a pretrained VLM instead of the reference agent πref to initialize EMMA.right: The 'w/ CE Loss' variant replaces the DPO loss (described in Eq. 2) with a token-level cross-entropy (CE) loss.Please refer to Sec. 3.4 for a thorough discussion.
Success Rate in ALFWorld0.2 0.4 0.6 0.80 0.22 0.222 Original EMMA 4 Number of Trials 6 0.45 0.36 0.63 0.36 w/o retrospection80.81 0.36100.82 0.37Success Rate in ALFWorld0.2 0.4 0.6 0.80 0.13 0.222 Original EMMA 4 Number of Trials 6 0.45 0.38 0.63 0.54 w/o BC initialization80.81 0.66100.82 0.72Success Rate in ALFWorld0.2 0.4 0.6 0.80 0.22 0.222 Original EMMA 4 Number of Trials 6 0.45 0.56 0.63 0.73 w/ CE Loss80.81 0.76100.82 0.78Figure 6:
Retrospection improves EMMA over time.To assess the importance of the retrospective LLM expert, we present the average success rate of EMMA after each trial and compare it</p>
<p>You open the cabinet 2. The cabinet 2 is open.In it, you see a candle 1, and a spraybottle 2. &gt; think: Now I find a spraybottle (2).Next, I need to take it.&gt; take spraybottle 2 from cabinet 2 You pick up the spraybottle 2 from the cabinet 2. &gt; think: Now I take a spraybottle (2).Next, I need to put it in/on toilet 1. &gt; go to toilet 1 On the toilet 1, you see a soapbottle 2. &gt; put spraybottle 2 in/on toilet 1 You put the spraybottle 2 in/on the toilet 1.} few-shot examples × n You were stuck in a loop in which you continually examined stoveburner 1 instead of heating mug 1 with stoveburner 1.You should have taken mug 1 from countertop 1, then heated it with stoveburner 1, then put it in coffeemachine 1.It did not help to execute two identical actions in a row.You will try to execute a different action if You am stuck in a loop again.}few-shot examples × n
. I can check one by one, starting withcabinet 1.&gt; go to cabinet 1On the cabinet 1, you see a cloth 1, a soapbar 1, a soapbottle 1.&gt; go to cabinet 2The cabinet 2 is closed.&gt; open cabinet 2long-term memory P: [...]target environment: ...task instruction: ...&gt;An example of the prompt adopted by LLM critic for generating retrospection{previous trial: You are in the middle of a room. Looking quickly around you, you see a cabinet 10, acabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, acabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 2, a diningtable 1, a drawer3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, astoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to: heat some mug and put it in coffeemachine.&gt; go to countertop 1On the countertop 1, you see a butterknife 1, a cup 1, a fork 2, and a mug 1.&gt; take mug 1 from countertop 1You pick up the mug 1 from the countertop 1.&gt; go to stoveburner 1On the stoveburner 1, you see a pan 2.&gt; examine stoveburner 1On the stoveburner 1, you see a pan 2.&gt; examine stoveburner 1On the stoveburner 1, you see a pan 2.&gt; examine stoveburner 1On the stoveburner 1, you see a pan 2.&gt; examine stoveburner 1On the stoveburner 1, you see a pan 2.STATUS: FAILretrospection: current trial: ...retrospection:
AcknowledgmentThis work is partially supported by the National Key R&amp;D Program of China under the Grant No. (2023YFE0106300) and (2017YFC0804002), Shenzhen Fundamental Research Program under the Grant No. (JCYJ20200109141235597), and Program for Guangdong Introducing Innovative and Entrepreneurial Teams under Grant No. (2017ZT07X386).. This benchmark adopts diverse household scenes developed by the Ai2Thor[30]environment, in which all objects can be relocated to different positions based on placeable surface areas and class constraints, enabling the procedural generation of a large pool of new tasks via recombining different objects and goal positions.Parallel TextWorldWhile the idea of parallel TextWorld is heavily inspired by previous work[55,56], we have enhanced the TextWorld engine to create text-based equivalents of each visual environment for training and evaluating language-based agents.This enhancement involves utilizing a combination of the PDDL[1]and Fast Downward[22]to maintain and update the textual state of the simulated environments.Based on the metadata provided by the simulator, we represent a visual state as a list of attributes.These attributes detail the relationships among various entities in the environment, such as positions, goals, and objects.Note that all these attributes, variables, and rules are defined within the framework of PDDL.Training DetailsWe provide hyperparameters used for training EMMA in Table2.These hyperparameters are largely derived from those proposed for finetuning InstructBLIP model[11].When training, we only update the parameters of linear projection layer while keeping other components frozen, as done during instruction tuning for many existing work[17,76].We use the AdamW optimizer[39]with a linear warmup of the learning rate, followed by a linear decay with a minimum learning rate of 0.Moreover, we remove the instruction input of Q-Former, which is used in InstructBLIP, and find this improves performance cross all experiments.Our implementation is heavily inspired by the LAVIS library[33]so the training and evaluation processes use the standard procedure provided by LAVIS.Task: Pick Two Objects and PlaceFigure9: Comparison of success rate between EMMA and the LLM expert.As the number of trials increases, the gap between the two agents decreases, and EMMA even outperforms or matches the expert in some tasks (e.g., "Heat and Place" and "Cool and Place"), indicating the effectiveness of cross-modality imitation learning.Frequency distribution of all words for human-annotated and templated task instructions.The diversity of human-annotated instructions presents a significant challenge for the generalization abilities of agents.
Pddl: the planning domain definition language. Constructions Aeronautiques, Adele Howe, Craig Knoblock, Drew Isi, Ashwin Mcdermott, Manuela Ram, Daniel Veloso, David Weld, Sri Wilkins, Anthony Barrett, Dave Christianson, Tech. Rep. 431998Technical Report</p>
<p>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Bob McGrew, Josh Tobin2017</p>
<p>Co-pilot: Collaborative planning and reinforcement learning on sub-task curriculum. Shuang Ao, Tianyi Zhou, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang, 2021</p>
<p>Eat-c: environment-adversarial sub-task curriculum for efficient reinforcement learning. Shuang Ao, Tianyi Zhou, Jing Jiang, Guodong Long, Xuan Song, Chengqi Zhang, ICML. 2022</p>
<p>A framework for behavioural cloning. Michael Bain, Claude Sammut, Machine Intelligence. 1995</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022. 2, 4, 8arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, CoRL2023</p>
<p>Learning to search better than your teacher. Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé, Iii , John Langford, ICML. 2015</p>
<p>Instructzero: Efficient instruction optimization for black-box large language models. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, Tianyi Zhou, arXiv:2306.030822023arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Computer Games Workshop. 201945</p>
<p>InstructBLIP: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, NeurIPS. 2023. 2, 3, 5, 8</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT. 2019</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378NeurIPS. 22023. 2019arXiv preprintCurriculum-guided hindsight experience replay</p>
<p>Drive like a human: Rethinking autonomous driving with large language models. Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, Yu Qiao, arXiv:2307.07162202324arXiv preprint</p>
<p>Physically grounded vision-language models for robotic manipulation. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh, arXiv:2309.025612023arXiv preprint</p>
<p>Llama-adapter v2: Parameter-efficient visual instruction model. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010202383arXiv preprint</p>
<p>Voce: Variational optimization with conservative estimation for offline safe reinforcement learning. Jiayi Guan, Guang Chen, Jiaming Ji, Long Yang, Zhijun Li, NeurIPS. 3622023</p>
<p>Pointing the unknown words. C ¸aglar Gülc ¸ehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio, ACL. 2016</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 2016</p>
<p>Mask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross B Girshick, ICCV. 2017</p>
<p>The fast downward planning system. Malte Helmert, Journal of Artificial Intelligence Research. 32006</p>
<p>Generative adversarial imitation learning. Jonathan Ho, Stefano Ermon, NeurIPS. 2016</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, ICML. 2022</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023. 2, 4, 8arXiv preprint</p>
<p>Imitation learning: A survey of learning methods. Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, Chrisina Jayne, ACM Computing Surveys (CSUR). 5022017</p>
<p>Universal artificial intelligence: Sequential decisions based on algorithmic probability. Marcus Hutter, 2004Springer Science &amp; Business Media</p>
<p>Searchbased structured prediction. Hal Daumé, Iii , John Langford, Daniel Marcu, Mach. Learn. 82009</p>
<p>VIMA: robot manipulation with multimodal prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. 202324ICML</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.05474201741arXiv preprint</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, arXiv:2005.01643202023arXiv preprint</p>
<p>Multimodal foundation models: From specialists to general-purpose assistants. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, arXiv:2309.100202023. 2, 3, 8arXiv preprint</p>
<p>LAVIS: A one-stop library for language-vision intelligence. Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, Steven C H Hoi, 2023. 3Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsSystem Demonstrations3</p>
<p>BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven C H Hoi, ICML. 2023. 2, 3, 5, 8</p>
<p>Boqing Gong, and Tianyi Zhou. Modulewise adaptive distillation for multimodality foundation models. Chen Liang, Jiahui Yu, Ming-Hsuan Yang, Matthew Brown, Yin Cui, Tuo Zhao, 2024</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, ICRA. 2023</p>
<p>Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models. Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, arXiv:2309.062562023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 202323arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Zhu, arXiv:2310.12931Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. 2023arXiv preprint</p>
<p>Language models are few-shot butlers. Vincent Micheli, Franc ¸ois, Fleuret , EMNLP. 20215</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, arXiv:2305.150212023. 2, 4, 8arXiv preprint</p>
<p>GPT-4V(ision) system card. 2023OpenAI</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 52019</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. 202183</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.182902023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, NeurIPS. 242024</p>
<p>Efficient reductions for imitation learning. Stéphane Ross, Drew Bagnell, AISTATS. 2010</p>
<p>Reinforcement and imitation learning via interactive no-regret learning. Stephane Ross, Andrew Bagnell, arXiv:1406.59792014arXiv preprint</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey J Gordon, Drew Bagnell, AISTATS. 2011. 2, 4, 8</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, Philipp Moritz, ICML. 2015</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, NeurIPS2023. 3, 4, 5, 6, 8</p>
<p>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, CVPR. 2020. 5, 8, 3, 6</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, ICLR. 2021. 2, 3, 4, 581</p>
<p>Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling internet-scale vision-language models into embodied agents. Theodore Sumers, Kenneth Marino, ICML. 2023. 2, 4, 8</p>
<p>Truncated horizon policy search: Combining reinforcement learning &amp; imitation learning. Wen Sun, J Andrew Bagnell, Byron Boots, In ICLR. 82018</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, arXiv:2401.10529A comprehensive benchmark for multimodal large language model reasoning over image sequences. 2024arXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.01560202356arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, NeurIPS2022</p>
<p>Intelligent agents: Theory and practice. Michael Wooldridge, Nicholas R Jennings, The knowledge engineering review. 1021995</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.0815520236arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, K Y Kenneth, Zhenguo Wong, Hengshuang Li, Zhao, arXiv:2310.01412Drivegpt4: Interpretable end-to-end autonomous driving via large language model. 202324arXiv preprint</p>
<p>The modality focusing hypothesis: Towards understanding crossmodal knowledge distillation. Zihui Xue, Zhengqi Gao, Sucheng Ren, Hang Zhao, ICLR, 2023. 8</p>
<p>Octopus: Embodied vision-language programmer from environmental feedback. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, arXiv:2310.085882023. 3, 4, 8arXiv preprint</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, Dale Schuurmans, arXiv:2303.041292023arXiv preprint</p>
<p>Pareto policy pool for model-based offline reinforcement learning. Yijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, Yuhui Shi, ICLR, 2022. 2</p>
<p>Continual task allocation in meta-policy network via sparse prompting. Yijun Yang, Tianyi Zhou, Jing Jiang, Guodong Long, Yuhui Shi, ICML. 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, ICLR. 2023. 3, 5, 68</p>
<p>Judging llm-as-a-judge with mtbench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté, arXiv:2402.07876Policy improvement using language feedback models. 2024arXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023. 2, 3, 5, 8arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, CoRL. 2023. 2, 3, 4, 8</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1415 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1415</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1415</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-f864d4d2267abba15eb43db54f58286aef78292b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f864d4d2267abba15eb43db54f58286aef78292b" target="_blank">Offline Reinforcement Learning as One Big Sequence Modeling Problem</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work explores how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1415.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1415.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based autoregressive world model that represents trajectories as discrete token sequences of per-dimension states, actions, immediate rewards, and reward-to-go, and is used for long-horizon prediction and planning via beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer decoder (GPT-like) that models the joint distribution over trajectories by discretizing each continuous state/action dimension into tokens and autoregressively predicting sequences of (s_t^i, a_t^j, r_t, R_t). Includes reward-to-go tokens and is trained with teacher-forcing on logged trajectories; planning is performed via beam search (likelihood-maximizing for imitation/goal conditioning, reward-maximizing with reward-to-go or Q-guidance for offline RL). Architecture used in experiments: 4 transformer layers, 4 attention heads, token embedding dim 128, per-dimension vocabulary V (100 in reported hyperparams).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive (autoregressive discrete) world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline reinforcement learning, imitation learning, goal-conditioned RL, long-horizon dynamics prediction (humanoid, locomotion benchmarks, AntMaze navigation, MiniGrid/4-rooms)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Log-likelihood of held-out true states under the model's per-timestep predicted state marginals (discretized token likelihood); qualitative visual fidelity of sampled rollouts; comparison to a discrete oracle (max log-likelihood given discretization).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitatively accurate 100-step rollouts that are visually indistinguishable from ground-truth for humanoid; substantially lower compounding error vs. a probabilistic feedforward ensemble (PETS) in Figure 3. Task results: D4RL locomotion average returns — TT (quantile) 78.9, TT (uniform) 72.6 (normalized scores, Table 1); AntMaze with Q-guidance TT(+Q) average 84.0 (Table 2). Training: ≈80 epochs on one NVIDIA V100 taking ~6–12 hours per model (dataset-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partly interpretable: internal attention patterns can be visualized and reveal strategies (e.g., Markovian attention to previous transition or striated attention to specific dimensions across multiple timesteps), but overall the model remains a black-box neural network with distributed representations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of attention maps/heads during sequence prediction (attention heatmaps), qualitative inspection of predicted rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: single V100 GPU for ~6–12 hours per model (80 epochs) as reported; model size small in experiments (4 layers, 4 heads, embedding dim 128). Inference/planning: beam search with beam width up to 256 and planning horizon 15 (reported hyperparams), which can require up to multiple seconds per action selection when context windows grow large (precluding many real-time control uses). Vocabulary size used: 100 bins per continuous dimension in reported runs.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More expensive at inference time than conventional single-step feedforward dynamics models (seconds vs. millisecond-scale single-step predictions) but reduces or removes need for ensembles or explicit pessimism in offline RL; training/runtime is heavier but yields much better long-horizon predictive fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Competitive or superior to state-of-the-art offline methods on benchmarks: D4RL locomotion (see Table 1: TT quantile average 78.9 vs. CQL 77.6 and Decision Transformer 74.7), AntMaze navigation when combined with a Q-function (TT(+Q) average 84.0, outperforming IQL and CQL baselines in those tasks). Achieves high imitation returns (≈104% Hopper, 109% Walker2d relative to expert in reported setups).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High long-horizon predictive fidelity translates into practical utility for planning: accurate long rollouts enable beam-search-based planning to produce competitive offline RL policies, and combining TT planning with an external Q-function improves performance in sparse-reward, long-horizon tasks (AntMaze). Discretization choices affect task utility (quantile often better when variable ranges are large).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between fidelity and computational efficiency: TT provides superior long-horizon fidelity at the cost of slower inference and higher compute; discretization imposes an upper bound on precision (uniform vs. quantile discretization trade-offs), and beam-search planning risks myopia unless guided by reward-to-go or Q-values. Smaller Transformer architectures used here trade off capacity and compute; more capacity could improve fidelity but increase cost.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Per-dimension discretization of continuous inputs (uniform or quantile); autoregressive tokenization ordering (states, actions, reward, reward-to-go); Transformer decoder (causal attention) with small configuration (4 layers, 4 heads); include reward-to-go tokens for value heuristics; plan with beam search and optionally augment with learned Q-function for sparse rewards; sample actions from top-k (k_act) tokens and observations greedily (k_obs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to probabilistic feedforward ensembles (e.g., PETS) and single-step Gaussian models, TT has much better long-horizon predictive accuracy (reduced compounding error) while being computationally heavier; versus model-free offline methods (CQL, BRAC) and other sequence approaches (Decision Transformer), TT achieves comparable or superior offline RL performance on locomotion and, when combined with Q-functions, on AntMaze navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends: (1) combine TT planning with a learned Q-function (dynamic programming) for sparse-reward/long-horizon tasks, (2) use quantile discretization when state/action ranges are large to avoid loss of precision, (3) consider computationally-efficient Transformer variants to reduce planning latency, and (4) tune beam search hyperparameters (beam width, horizon, top-k sampling) to balance compute and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline Reinforcement Learning as One Big Sequence Modeling Problem', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1415.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1415.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PETS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic ensemble (PETS) feedforward dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble of probabilistic feedforward dynamics models that predict single-step Gaussian next-state distributions (diagonal covariance) and are used for model-based planning via trajectory sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning in a handful of trials using probabilistic dynamics models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Probabilistic feedforward ensemble (PETS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ensemble of fully-connected neural networks that predict next-state Gaussian distributions with diagonal covariance; planning performed via trajectory sampling and ensemble uncertainty for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic feedforward dynamics model ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based planning for continuous control (humanoid, locomotion benchmarks used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Single-step prediction error / likelihood; accumulated long-horizon error measured by likelihood of true states under multi-step rollout marginals (as used in comparisons in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Excellent single-step error but poor long-horizon accuracy in the tested humanoid environment: the authors tuned ensembles and architectures but could not produce accurate predicted rollouts beyond a few dozen steps; qualitatively exhibited strong compounding error compared to the Trajectory Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural ensemble; uncertainty estimates via ensemble variance offer some interpretability about model confidence but internal features are not explicitly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Ensemble variance / uncertainty quantification; no attention-based visualization (not applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference requires running multiple ensemble members (number of models tuned by authors), generally cheaper per-step than Transformer-based beam search planning but more expensive than a single feedforward model; specific compute numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Faster inference per step than Transformer-based planning (no beam search over long sequences) and typically used with short rollouts to avoid compounding errors; however, long-horizon performance is inferior compared to TT, forcing mitigation strategies (short horizon, ensembles).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>In the paper's qualitative and quantitative comparisons (Figures 2 and 3), PETS produced unrealistic/physically implausible long rollouts while TT produced accurate 100-step rollouts; no explicit numeric task scores for PETS provided in main tables of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High single-step fidelity does not translate to good long-horizon predictive utility in high-dimensional, long-horizon tasks because errors compound quickly; ensembles and short-horizon planning partially mitigate but limit planning horizon and utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off of cheap single-step inference and uncertainty estimation vs. poor long-horizon generalization; ensembles mitigate but increase compute and still suffer compounding errors at long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Fully-connected networks predicting diagonal-covariance Gaussian transitions; use of ensembles to capture epistemic uncertainty; short planning horizons to reduce error accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Trajectory Transformer, PETS has superior single-step efficiency and similar or better uncertainty quantification per step, but much worse multi-step rollout fidelity; compared to single-model Gaussian predictors it benefits from ensemble robustness but still succumbs to long-horizon compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Best used with ensembles and short planning horizons or combined with techniques to limit model exploitation; not optimal for very long-horizon prediction tasks without further augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline Reinforcement Learning as One Big Sequence Modeling Problem', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1415.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1415.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Markovian-TT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Markovian variant of the Trajectory Transformer (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of the Trajectory Transformer with a truncated context window that prevents attention to more than one timestep in the past, effectively enforcing Markovian predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Markovian Transformer variant (truncated context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same Transformer architecture as TT but with attention context truncated to disallow attending to more than the immediately preceding timestep; still uses per-dimension discretization and autoregressive tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based Markovian (single-step-context) world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-horizon prediction in fully-observed and partially-observed control environments (humanoid experiments used in ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Log-likelihood of true states under predicted per-timestep marginals (same evaluation as TT).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Performs similarly to the full Trajectory Transformer in fully-observed environments, suggesting architecture and autoregressive discretization drive much of the empirical advantage; in partially-observed settings it underperforms the full TT, indicating long-horizon conditioning matters under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Attention patterns reveal a Markovian strategy (predictions attending mainly to previous transition), making behavior somewhat interpretable in this regime.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of attention maps during prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Potentially slightly cheaper at inference because of the smaller attended context, but concrete runtime figures not provided; training regime same as TT.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Similar training/inference costs to TT in practice in the experiments, but may be more efficient if context size is reduced; similar single-step behavior but worse under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On fully-observed tasks, produces long-horizon predictions comparable to full TT; on partially-observed tasks, full TT with longer context gives superior predictions and thus better downstream planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows that large part of the benefit arises from expressivity and autoregressive discretization rather than long temporal context when the environment is fully observable, but long context is beneficial under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Truncating context reduces capacity to leverage long-horizon information (hurting partially-observed tasks) while potentially reducing compute; there is a trade-off between context length (fidelity under partial observability) and compute/latency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Explicit truncation of attention context to enforce Markovian dependency; keeps other TT design elements (discretization, reward-to-go) intact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs closer to TT than to single-step feedforward ensemble in fully-observed settings, supporting that autoregressive discretization and Transformer parameterization contribute materially to long-horizon accuracy; in partially-observed settings the full-context TT is superior.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Use truncated (Markovian) variant when environment is fully observed and low-latency is required; use full-context TT when partial observability or long-range dependencies are important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline Reinforcement Learning as One Big Sequence Modeling Problem', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1415.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1415.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-step Gaussian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully-connected single-step Gaussian dynamics models (diagonal covariance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common model-based RL dynamics parameterization: a feedforward neural network predicts the next-state distribution as a Gaussian with diagonal covariance, trained on one-step transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning in a handful of trials using probabilistic dynamics models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single-step Gaussian feedforward dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A fully-connected neural network that, given current state and action, outputs parameters (mean and diagonal covariance) of a Gaussian distribution over the next state; typically used as a Markovian, single-step predictive model in model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic single-step dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL, planning with short rollouts in continuous control domains (locomotion, manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Single-step MSE or log-likelihood of next-state predictions; long-horizon fidelity evaluated via multi-step rollouts but often degrades due to compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Typically low single-step error but poor multi-step/long-horizon accuracy due to compounding errors; paper reports that such models have excellent single-step errors but poor long-horizon accuracy compared to TT (qualitative statement, no numeric MSE provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural network; uncertainty modeled only via predicted covariance (often diagonal) and/or ensembles for epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specific in this paper beyond standard uncertainty outputs (covariance) or ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally cheap at inference: single-forward pass per step; often used with short rollouts to maintain efficiency in planning loops.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Faster and lower-latency than Transformer-based sequence models for single-step predictions, but suffers in long-horizon predictive fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Useful for short-horizon planning and sample-efficient learning; degrades substantially in long-horizon, high-dimensional tasks where compounding errors dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High single-step fidelity does not guarantee utility for long-horizon planning; models are effective when planning horizons are kept short or when combined with other mitigation strategies (ensembles, conservatism).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Cheap, low-latency predictions versus poor long-horizon generalization; ensembles improve robustness but increase computational cost and do not fully solve compounding error.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Diagonal-covariance Gaussian outputs, fully-connected architectures, optionally ensembles for uncertainty; single-step Markovian modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Inferior to Trajectory Transformer for long-horizon multi-step prediction; more efficient at inference and simpler to train, commonly used in practical model-based RL when short horizons suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Best suited for short-horizon planning or when combined with additional mechanisms (ensembles, constrained optimization, online corrections) to mitigate compounding error; not ideal as-is for very long-horizon prediction without augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Offline Reinforcement Learning as One Big Sequence Modeling Problem', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. <em>(Rating: 2)</em></li>
                <li>Autoregressive dynamics models for offline policy evaluation and optimization. <em>(Rating: 2)</em></li>
                <li>Decision Transformer: Reinforcement learning via sequence modeling <em>(Rating: 1)</em></li>
                <li>Model-ensemble trust-region policy optimization <em>(Rating: 2)</em></li>
                <li>When to trust your model: Model-based policy optimization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1415",
    "paper_id": "paper-f864d4d2267abba15eb43db54f58286aef78292b",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "TT",
            "name_full": "Trajectory Transformer",
            "brief_description": "A Transformer-based autoregressive world model that represents trajectories as discrete token sequences of per-dimension states, actions, immediate rewards, and reward-to-go, and is used for long-horizon prediction and planning via beam search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Trajectory Transformer",
            "model_description": "Transformer decoder (GPT-like) that models the joint distribution over trajectories by discretizing each continuous state/action dimension into tokens and autoregressively predicting sequences of (s_t^i, a_t^j, r_t, R_t). Includes reward-to-go tokens and is trained with teacher-forcing on logged trajectories; planning is performed via beam search (likelihood-maximizing for imitation/goal conditioning, reward-maximizing with reward-to-go or Q-guidance for offline RL). Architecture used in experiments: 4 transformer layers, 4 attention heads, token embedding dim 128, per-dimension vocabulary V (100 in reported hyperparams).",
            "model_type": "transformer-based predictive (autoregressive discrete) world model",
            "task_domain": "Offline reinforcement learning, imitation learning, goal-conditioned RL, long-horizon dynamics prediction (humanoid, locomotion benchmarks, AntMaze navigation, MiniGrid/4-rooms)",
            "fidelity_metric": "Log-likelihood of held-out true states under the model's per-timestep predicted state marginals (discretized token likelihood); qualitative visual fidelity of sampled rollouts; comparison to a discrete oracle (max log-likelihood given discretization).",
            "fidelity_performance": "Qualitatively accurate 100-step rollouts that are visually indistinguishable from ground-truth for humanoid; substantially lower compounding error vs. a probabilistic feedforward ensemble (PETS) in Figure 3. Task results: D4RL locomotion average returns — TT (quantile) 78.9, TT (uniform) 72.6 (normalized scores, Table 1); AntMaze with Q-guidance TT(+Q) average 84.0 (Table 2). Training: ≈80 epochs on one NVIDIA V100 taking ~6–12 hours per model (dataset-dependent).",
            "interpretability_assessment": "Partly interpretable: internal attention patterns can be visualized and reveal strategies (e.g., Markovian attention to previous transition or striated attention to specific dimensions across multiple timesteps), but overall the model remains a black-box neural network with distributed representations.",
            "interpretability_method": "Visualization of attention maps/heads during sequence prediction (attention heatmaps), qualitative inspection of predicted rollouts.",
            "computational_cost": "Training: single V100 GPU for ~6–12 hours per model (80 epochs) as reported; model size small in experiments (4 layers, 4 heads, embedding dim 128). Inference/planning: beam search with beam width up to 256 and planning horizon 15 (reported hyperparams), which can require up to multiple seconds per action selection when context windows grow large (precluding many real-time control uses). Vocabulary size used: 100 bins per continuous dimension in reported runs.",
            "efficiency_comparison": "More expensive at inference time than conventional single-step feedforward dynamics models (seconds vs. millisecond-scale single-step predictions) but reduces or removes need for ensembles or explicit pessimism in offline RL; training/runtime is heavier but yields much better long-horizon predictive fidelity.",
            "task_performance": "Competitive or superior to state-of-the-art offline methods on benchmarks: D4RL locomotion (see Table 1: TT quantile average 78.9 vs. CQL 77.6 and Decision Transformer 74.7), AntMaze navigation when combined with a Q-function (TT(+Q) average 84.0, outperforming IQL and CQL baselines in those tasks). Achieves high imitation returns (≈104% Hopper, 109% Walker2d relative to expert in reported setups).",
            "task_utility_analysis": "High long-horizon predictive fidelity translates into practical utility for planning: accurate long rollouts enable beam-search-based planning to produce competitive offline RL policies, and combining TT planning with an external Q-function improves performance in sparse-reward, long-horizon tasks (AntMaze). Discretization choices affect task utility (quantile often better when variable ranges are large).",
            "tradeoffs_observed": "Trade-off between fidelity and computational efficiency: TT provides superior long-horizon fidelity at the cost of slower inference and higher compute; discretization imposes an upper bound on precision (uniform vs. quantile discretization trade-offs), and beam-search planning risks myopia unless guided by reward-to-go or Q-values. Smaller Transformer architectures used here trade off capacity and compute; more capacity could improve fidelity but increase cost.",
            "design_choices": "Per-dimension discretization of continuous inputs (uniform or quantile); autoregressive tokenization ordering (states, actions, reward, reward-to-go); Transformer decoder (causal attention) with small configuration (4 layers, 4 heads); include reward-to-go tokens for value heuristics; plan with beam search and optionally augment with learned Q-function for sparse rewards; sample actions from top-k (k_act) tokens and observations greedily (k_obs).",
            "comparison_to_alternatives": "Compared to probabilistic feedforward ensembles (e.g., PETS) and single-step Gaussian models, TT has much better long-horizon predictive accuracy (reduced compounding error) while being computationally heavier; versus model-free offline methods (CQL, BRAC) and other sequence approaches (Decision Transformer), TT achieves comparable or superior offline RL performance on locomotion and, when combined with Q-functions, on AntMaze navigation.",
            "optimal_configuration": "Paper recommends: (1) combine TT planning with a learned Q-function (dynamic programming) for sparse-reward/long-horizon tasks, (2) use quantile discretization when state/action ranges are large to avoid loss of precision, (3) consider computationally-efficient Transformer variants to reduce planning latency, and (4) tune beam search hyperparameters (beam width, horizon, top-k sampling) to balance compute and performance.",
            "uuid": "e1415.0",
            "source_info": {
                "paper_title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "PETS",
            "name_full": "Probabilistic ensemble (PETS) feedforward dynamics model",
            "brief_description": "An ensemble of probabilistic feedforward dynamics models that predict single-step Gaussian next-state distributions (diagonal covariance) and are used for model-based planning via trajectory sampling.",
            "citation_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models.",
            "mention_or_use": "use",
            "model_name": "Probabilistic feedforward ensemble (PETS)",
            "model_description": "Ensemble of fully-connected neural networks that predict next-state Gaussian distributions with diagonal covariance; planning performed via trajectory sampling and ensemble uncertainty for robustness.",
            "model_type": "probabilistic feedforward dynamics model ensemble",
            "task_domain": "Model-based planning for continuous control (humanoid, locomotion benchmarks used for comparison)",
            "fidelity_metric": "Single-step prediction error / likelihood; accumulated long-horizon error measured by likelihood of true states under multi-step rollout marginals (as used in comparisons in the paper).",
            "fidelity_performance": "Excellent single-step error but poor long-horizon accuracy in the tested humanoid environment: the authors tuned ensembles and architectures but could not produce accurate predicted rollouts beyond a few dozen steps; qualitatively exhibited strong compounding error compared to the Trajectory Transformer.",
            "interpretability_assessment": "Black-box neural ensemble; uncertainty estimates via ensemble variance offer some interpretability about model confidence but internal features are not explicitly interpretable.",
            "interpretability_method": "Ensemble variance / uncertainty quantification; no attention-based visualization (not applicable).",
            "computational_cost": "Inference requires running multiple ensemble members (number of models tuned by authors), generally cheaper per-step than Transformer-based beam search planning but more expensive than a single feedforward model; specific compute numbers not provided in this paper.",
            "efficiency_comparison": "Faster inference per step than Transformer-based planning (no beam search over long sequences) and typically used with short rollouts to avoid compounding errors; however, long-horizon performance is inferior compared to TT, forcing mitigation strategies (short horizon, ensembles).",
            "task_performance": "In the paper's qualitative and quantitative comparisons (Figures 2 and 3), PETS produced unrealistic/physically implausible long rollouts while TT produced accurate 100-step rollouts; no explicit numeric task scores for PETS provided in main tables of this paper.",
            "task_utility_analysis": "High single-step fidelity does not translate to good long-horizon predictive utility in high-dimensional, long-horizon tasks because errors compound quickly; ensembles and short-horizon planning partially mitigate but limit planning horizon and utility.",
            "tradeoffs_observed": "Trade-off of cheap single-step inference and uncertainty estimation vs. poor long-horizon generalization; ensembles mitigate but increase compute and still suffer compounding errors at long horizons.",
            "design_choices": "Fully-connected networks predicting diagonal-covariance Gaussian transitions; use of ensembles to capture epistemic uncertainty; short planning horizons to reduce error accumulation.",
            "comparison_to_alternatives": "Compared to Trajectory Transformer, PETS has superior single-step efficiency and similar or better uncertainty quantification per step, but much worse multi-step rollout fidelity; compared to single-model Gaussian predictors it benefits from ensemble robustness but still succumbs to long-horizon compounding errors.",
            "optimal_configuration": "Best used with ensembles and short planning horizons or combined with techniques to limit model exploitation; not optimal for very long-horizon prediction tasks without further augmentation.",
            "uuid": "e1415.1",
            "source_info": {
                "paper_title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Markovian-TT",
            "name_full": "Markovian variant of the Trajectory Transformer (ablation)",
            "brief_description": "An ablation of the Trajectory Transformer with a truncated context window that prevents attention to more than one timestep in the past, effectively enforcing Markovian predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Markovian Transformer variant (truncated context)",
            "model_description": "Same Transformer architecture as TT but with attention context truncated to disallow attending to more than the immediately preceding timestep; still uses per-dimension discretization and autoregressive tokenization.",
            "model_type": "transformer-based Markovian (single-step-context) world model",
            "task_domain": "Long-horizon prediction in fully-observed and partially-observed control environments (humanoid experiments used in ablation).",
            "fidelity_metric": "Log-likelihood of true states under predicted per-timestep marginals (same evaluation as TT).",
            "fidelity_performance": "Performs similarly to the full Trajectory Transformer in fully-observed environments, suggesting architecture and autoregressive discretization drive much of the empirical advantage; in partially-observed settings it underperforms the full TT, indicating long-horizon conditioning matters under partial observability.",
            "interpretability_assessment": "Attention patterns reveal a Markovian strategy (predictions attending mainly to previous transition), making behavior somewhat interpretable in this regime.",
            "interpretability_method": "Visualization of attention maps during prediction.",
            "computational_cost": "Potentially slightly cheaper at inference because of the smaller attended context, but concrete runtime figures not provided; training regime same as TT.",
            "efficiency_comparison": "Similar training/inference costs to TT in practice in the experiments, but may be more efficient if context size is reduced; similar single-step behavior but worse under partial observability.",
            "task_performance": "On fully-observed tasks, produces long-horizon predictions comparable to full TT; on partially-observed tasks, full TT with longer context gives superior predictions and thus better downstream planning utility.",
            "task_utility_analysis": "Shows that large part of the benefit arises from expressivity and autoregressive discretization rather than long temporal context when the environment is fully observable, but long context is beneficial under partial observability.",
            "tradeoffs_observed": "Truncating context reduces capacity to leverage long-horizon information (hurting partially-observed tasks) while potentially reducing compute; there is a trade-off between context length (fidelity under partial observability) and compute/latency.",
            "design_choices": "Explicit truncation of attention context to enforce Markovian dependency; keeps other TT design elements (discretization, reward-to-go) intact.",
            "comparison_to_alternatives": "Performs closer to TT than to single-step feedforward ensemble in fully-observed settings, supporting that autoregressive discretization and Transformer parameterization contribute materially to long-horizon accuracy; in partially-observed settings the full-context TT is superior.",
            "optimal_configuration": "Use truncated (Markovian) variant when environment is fully observed and low-latency is required; use full-context TT when partial observability or long-range dependencies are important.",
            "uuid": "e1415.2",
            "source_info": {
                "paper_title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Single-step Gaussian",
            "name_full": "Fully-connected single-step Gaussian dynamics models (diagonal covariance)",
            "brief_description": "Common model-based RL dynamics parameterization: a feedforward neural network predicts the next-state distribution as a Gaussian with diagonal covariance, trained on one-step transitions.",
            "citation_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models.",
            "mention_or_use": "mention",
            "model_name": "Single-step Gaussian feedforward dynamics model",
            "model_description": "A fully-connected neural network that, given current state and action, outputs parameters (mean and diagonal covariance) of a Gaussian distribution over the next state; typically used as a Markovian, single-step predictive model in model-based RL.",
            "model_type": "probabilistic single-step dynamics model",
            "task_domain": "Model-based RL, planning with short rollouts in continuous control domains (locomotion, manipulation).",
            "fidelity_metric": "Single-step MSE or log-likelihood of next-state predictions; long-horizon fidelity evaluated via multi-step rollouts but often degrades due to compounding errors.",
            "fidelity_performance": "Typically low single-step error but poor multi-step/long-horizon accuracy due to compounding errors; paper reports that such models have excellent single-step errors but poor long-horizon accuracy compared to TT (qualitative statement, no numeric MSE provided here).",
            "interpretability_assessment": "Black-box neural network; uncertainty modeled only via predicted covariance (often diagonal) and/or ensembles for epistemic uncertainty.",
            "interpretability_method": "None specific in this paper beyond standard uncertainty outputs (covariance) or ensembles.",
            "computational_cost": "Computationally cheap at inference: single-forward pass per step; often used with short rollouts to maintain efficiency in planning loops.",
            "efficiency_comparison": "Faster and lower-latency than Transformer-based sequence models for single-step predictions, but suffers in long-horizon predictive fidelity.",
            "task_performance": "Useful for short-horizon planning and sample-efficient learning; degrades substantially in long-horizon, high-dimensional tasks where compounding errors dominate.",
            "task_utility_analysis": "High single-step fidelity does not guarantee utility for long-horizon planning; models are effective when planning horizons are kept short or when combined with other mitigation strategies (ensembles, conservatism).",
            "tradeoffs_observed": "Cheap, low-latency predictions versus poor long-horizon generalization; ensembles improve robustness but increase computational cost and do not fully solve compounding error.",
            "design_choices": "Diagonal-covariance Gaussian outputs, fully-connected architectures, optionally ensembles for uncertainty; single-step Markovian modelling.",
            "comparison_to_alternatives": "Inferior to Trajectory Transformer for long-horizon multi-step prediction; more efficient at inference and simpler to train, commonly used in practical model-based RL when short horizons suffice.",
            "optimal_configuration": "Best suited for short-horizon planning or when combined with additional mechanisms (ensembles, constrained optimization, online corrections) to mitigate compounding error; not ideal as-is for very long-horizon prediction without augmentation.",
            "uuid": "e1415.3",
            "source_info": {
                "paper_title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models.",
            "rating": 2
        },
        {
            "paper_title": "Autoregressive dynamics models for offline policy evaluation and optimization.",
            "rating": 2
        },
        {
            "paper_title": "Decision Transformer: Reinforcement learning via sequence modeling",
            "rating": 1
        },
        {
            "paper_title": "Model-ensemble trust-region policy optimization",
            "rating": 2
        },
        {
            "paper_title": "When to trust your model: Model-based policy optimization",
            "rating": 2
        }
    ],
    "cost": 0.01628775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Offline Reinforcement Learning as One Big Sequence Modeling Problem</h1>
<p>Michael Janner Qiyang Li Sergey Levine<br>University of California at Berkeley<br>{janner, qcli}@berkeley.edu svlevine@eecs.berkeley.edu</p>
<h4>Abstract</h4>
<p>Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.</p>
<h2>1 Introduction</h2>
<p>The standard treatment of reinforcement learning relies on decomposing a long-horizon problem into smaller, more local subproblems. In model-free algorithms, this takes the form of the principle of optimality (Bellman, 1957), a recursion that leads naturally to the class of dynamic programming methods like $Q$-learning. In model-based algorithms, this decomposition takes the form of single-step predictive models, which reduce the problem of predicting high-dimensional, policy-dependent state trajectories to that of estimating a comparatively simpler, policy-agnostic transition distribution.</p>
<p>However, we can also view reinforcement learning as analogous to a sequence generation problem, with the goal being to produce a sequence of actions that, when enacted in an environment, will yield a sequence of high rewards. In this paper, we consider the logical extreme of this analogy: does the toolbox of contemporary sequence modeling itself provide a viable reinforcement learning algorithm? We investigate this question by treating trajectories as unstructured sequences of states, actions, and rewards. We model the distribution of these trajectories using a Transformer architecture (Vaswani et al., 2017), the current tool of choice for capturing long-horizon dependencies. In place of the trajectory optimizers common in model-based control, we use beam search (Reddy, 1977), a heuristic decoding scheme ubiquitous in natural language processing, as a planning algorithm.</p>
<p>Posing reinforcement learning, and more broadly data-driven control, as a sequence modeling problem handles many of the considerations that typically require distinct solutions: actor-critic algorithms require separate actors and critics, model-based algorithms require predictive dynamics models, and offline RL methods often require estimation of the behavior policy (Fujimoto et al., 2019). These</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 (Architecture) The Trajectory Transformer trains on sequences of (autoregressively discretized) states, actions, and rewards. Planning with the Trajectory Transformer mirrors the sampling procedure used to generate sequences from a language model.
components estimate different densities or distributions, such as that over actions in the case of actors and behavior policies, or that over states in the case of dynamics models. Even value functions can be viewed as performing inference in a graphical model with auxiliary optimality variables, amounting to estimation of the distribution over future rewards (Levine, 2018). All of these problems can be unified under a single sequence model, which treats states, actions, and rewards as simply a stream of data. The advantage of this perspective is that high-capacity sequence model architectures can be brought to bear on the problem, resulting in a more streamlined approach that could benefit from the same scalability underlying large-scale unsupervised learning results (Brown et al., 2020).
We refer to our model as a Trajectory Transformer (Figure 1) and evaluate it in the offline regime so as to be able to make use of large amounts of prior interaction data. The Trajectory Transformer is a substantially more reliable long-horizon predictor than conventional dynamics models, even in Markovian environments for which the standard model parameterization is in principle sufficient. When decoded with a modified beam search procedure that biases trajectory samples according to their cumulative reward, the Trajectory Transformer attains results on offline RL benchmarks that are competitive with the best prior methods designed specifically for that setting. Additionally, we describe how variations of the same decoding procedure yield a model-based imitation learning method, a goal-reaching method, and, when combined with dynamic programming, a state-of-the-art planner for sparse-reward, long-horizon tasks. Our results suggest that the algorithms and architectural motifs that have been widely applicable in unsupervised learning carry similar benefits in RL.</p>
<h1>2 Related Work</h1>
<p>Recent advances in sequence modeling with deep networks have led to rapid improvement in the effectiveness of such models, from LSTMs and sequence-to-sequence models (Hochreiter \&amp; Schmidhuber, 1997; Sutskever et al., 2014) to Transformer architectures with self-attention (Vaswani et al., 2017). In light of this, it is tempting to consider how such sequence models can lead to improved performance in RL, which is also concerned with sequential processes (Sutton, 1988). Indeed, a number of prior works have studied applying sequence models of various types to represent components in standard RL algorithms, such as policies, value functions, and models (Bakker, 2002; Heess et al., 2015a; Chiappa et al., 2017; Parisotto et al., 2020; Parisotto \&amp; Salakhutdinov, 2021; Kumar et al., 2020b). While such works demonstrate the importance of such models for representing memory (Oh et al., 2016), they still rely on standard RL algorithmic advances to improve performance. The goal in our work is different: we aim to replace as much of the RL pipeline as possible with sequence modeling, so as to produce a simpler method whose effectiveness is determined by the representational capacity of the sequence model rather than algorithmic sophistication.
Estimation of probability distributions and densities arises in many places in learning-based control. This is most obvious in model-based RL, where it is used to train predictive models that can then be used for planning or policy learning (Sutton, 1990; Silver et al., 2008; Fairbank, 2008; Deisenroth \&amp; Rasmussen, 2011; Lampe \&amp; Riedmiller, 2014; Heess et al., 2015b; Janner et al., 2020; Amos et al., 2021). However, it also figures heavily in offline RL, where it is used to estimate conditional distributions over actions that serve to constrain the learned policy to avoid out-of-</p>
<p>distribution behavior that is not supported under the dataset (Fujimoto et al., 2019; Kumar et al., 2019a; Ghasemipour et al., 2021); imitation learning, where it is used to fit an expert's actions to obtain a policy (Ross \&amp; Bagnell, 2010; Ross et al., 2011); and other areas such as hierarchical RL (Peng et al., 2017; Co-Reyes et al., 2018; Jiang et al., 2019). In our method, we train a single high-capacity sequence model to represent the joint distribution over sequences of states, actions, and rewards. This serves as both a predictive model and a behavior policy (for imitation) or behavior constraint (for offline RL).</p>
<p>Our approach to RL is most closely related to prior model-based methods that plan with a learned model (Chua et al., 2018; Wang \&amp; Ba, 2020). However, while these prior methods typically require additional machinery to work well, such as ensembles in the online setting (Kurutach et al., 2018; Buckman et al., 2018; Malik et al., 2019) or conservatism mechanisms in the offline setting (Yu et al., 2020; Kidambi et al., 2020; Argenson \&amp; Dulac-Arnold, 2021), our method does not require explicit handling of these components. Modeling the states and actions jointly already provides a bias toward generating in-distribution actions, which avoids the need for explicit pessimism (Fujimoto et al., 2019; Kumar et al., 2019a; Ghasemipour et al., 2021; Nair et al., 2020; Jin et al., 2021; Yin et al., 2021; Dadashi et al., 2021). Our method also differs from most prior model-based algorithms in the dynamics model architecture used, with fully-connected networks parameterizing diagonalcovariance Gaussian distributions being a common choice (Chua et al., 2018), though recent work has highlighted the effectiveness of autoregressive state prediction (Zhang et al., 2021) like that used by the Trajectory Transformer. In the context of recently proposed offline RL algorithms, our method can be interpreted as a combination of model-based RL and policy constraints (Kumar et al., 2019a; Wu et al., 2019), though our approach does not require introducing such constraints explicitly. In the context of model-free RL, our method also resembles recently proposed work on goal relabeling (Andrychowicz et al., 2017; Rauber et al., 2019; Ghosh et al., 2021; Paster et al., 2021) and reward conditioning (Schmidhuber, 2019; Srivastava et al., 2019; Kumar et al., 2019b) to reinterpret all past experience as useful demonstrations with proper contextualization.</p>
<p>Concurrently with our work, Chen et al. (2021) also proposed an RL approach centered around sequence prediction, focusing on reward conditioning as opposed to the beam-search-based planning used by the Trajectory Transformer. Their work further supports the possibility that a high-capacity sequence model can be applied to reinforcement learning problems without the need for the components usually associated with RL algorithms.</p>
<h1>3 Reinforcement Learning and Control as Sequence Modeling</h1>
<p>In this section, we describe the training procedure for our sequence model and discuss how it can be used for control. We refer to the model as a Trajectory Transformer for brevity, but emphasize that at the implementation level, both our model and search strategy are nearly identical to those common in natural language processing. As a result, modeling considerations are concerned less with architecture design and more with how to represent trajectory data - potentially consisting of continuous states and actions - for processing by a discrete-token architecture (Radford et al., 2018).</p>
<h3>3.1 Trajectory Transformer</h3>
<p>At the core of our approach is the treatment of trajectory data as an unstructured sequence for modeling by a Transformer architecture. A trajectory $\boldsymbol{\tau}$ consists of $T$ states, actions, and scalar rewards:</p>
<p>$$
\boldsymbol{\tau}=\left(\mathbf{s}<em 1="1">{1}, \mathbf{a}</em>}, r_{1}, \mathbf{s<em 2="2">{2}, \mathbf{a}</em>}, r_{2}, \ldots, \mathbf{s<em T="T">{T}, \mathbf{a}</em>\right)
$$}, r_{T</p>
<p>In the event of continuous states and actions, we discretize each dimension independently. Assuming $N$-dimensional states and $M$-dimensional actions, this turns $\tau$ into sequence of length $T(N+M+1)$ :</p>
<p>$$
\boldsymbol{\tau}=\left(\ldots, s_{t}^{1}, s_{t}^{2}, \ldots, s_{t}^{N}, a_{t}^{1}, a_{t}^{2}, \ldots, a_{t}^{M}, r_{t}, \ldots\right) \quad t=1, \ldots, T
$$</p>
<p>Subscripts on all tokens denote timestep and superscripts on states and actions denote dimension (i.e., $s_{t}^{i}$ is the $i^{\text {th }}$ dimension of the state at time $t$ ). While this choice may seem inefficient, it allows us to model the distribution over trajectories with more expressivity without simplifying assumptions such as Gaussian transitions.</p>
<p>Algorithm 1 Beam search
1: Require Input sequence $\mathbf{x}$, vocabulary $\mathcal{V}$, sequence length $T$, beam width $B$
2: Initialize $Y_{0}={()}$
3: for $t=1, \ldots, T$ do
4: $\mathcal{C}<em t-1="t-1">{t} \leftarrow\left{\mathbf{y}</em>} \circ y \mid \mathbf{y<em t-1="t-1">{t-1} \in Y</em>\right} \quad / /$ candidate single-token extensions
5: $\quad Y_{t} \leftarrow \underset{Y \subseteq \mathcal{C}}\right.$ and $\left.y \in \mathcal{V<em _theta="\theta">{t},|Y|=B}{\operatorname{argmax}} \log P</em>) \quad / / B$ most likely sequences from candidates
6: end for
7: Return $\underset{\mathbf{y} \in Y_{T}}{\operatorname{argmax}} \log P_{\theta}(\mathbf{y} \mid \mathbf{x})$}(Y \mid \mathbf{x</p>
<p>We investigate two simple discretization approaches:</p>
<ol>
<li>Uniform: All tokens for a given dimension correspond to a fixed width of the original continuous space. Assuming a per-dimension vocabulary size of $V$, the tokens for state dimension $i$ cover uniformly-spaced intervals of width $\left(\max \mathbf{s}^{i}-\min \mathbf{s}^{i}\right) / V$.</li>
<li>Quantile: All tokens for a given dimension account for an equal amount of probability mass under the empirical data distribution; each token accounts for 1 out of every $V$ data points in the training set.</li>
</ol>
<p>Uniform discretization has the advantage that it retains information about Euclidean distance in the original continuous space, which may be more reflective of the structure of a problem than the training data distribution. However, outliers in the data may have outsize effects on the discretization size, leaving many tokens corresponding to zero training points. The quantile discretization scheme ensures that all tokens are represented in the data. We compare the two empirically in Section 4.2.</p>
<p>Our model is a Transformer decoder mirroring the GPT architecture (Radford et al., 2018). We use a smaller architecture than those typically used in large-scale language modeling, consisting of four layers and four self-attention heads. (A full architectural description is provided in Appendix A.) Training is performed with the standard teacher-forcing procedure (Williams \&amp; Zipser, 1989) used to train sequence models. Denoting the parameters of the Trajectory Transformer as $\theta$ and induced conditional probabilities as $P_{\theta}$, the objective maximized during training is:
$\mathcal{L}(\tau)=\sum_{t=1}^{T}\left(\sum_{i=1}^{N} \log P_{\theta}\left(s_{t}^{i} \mid \mathbf{s}<em _t="&lt;t">{t}^{&lt;i}, \boldsymbol{\tau}</em>}\right)+\sum_{j=1}^{M} \log P_{\theta}\left(a_{t}^{j} \mid \mathbf{a<em t="t">{t}^{&lt;j}, \mathbf{s}</em>}, \boldsymbol{\tau<em _theta="\theta">{&lt;t}\right)+\log P</em>}\left(r_{t} \mid \mathbf{a<em t="t">{t}, \mathbf{s}</em>}, \boldsymbol{\tau<em _t="&lt;t">{&lt;t}\right)\right)$,
in which we use $\boldsymbol{\tau}</em>}$ to denote a trajectory from timesteps 0 through $t-1, \mathbf{s<em t="t">{t}^{&lt;i}$ to denote dimensions 0 through $i-1$ of the state at timestep $t$, and similarly for $\mathbf{a}</em>$ to train parameters $\theta$.}^{&lt;j}$. We use the Adam optimizer (Kingma $\&amp; \mathrm{Ba}, 2015)$ with a learning rate of $2.5 \times 10^{-4</p>
<h1>3.2 Planning with Beam Search</h1>
<p>We now describe how sequence generation with the Trajectory Transformer can be repurposed for control, focusing on three settings: imitation learning, goal-conditioned reinforcement learning, and offline reinforcement learning. These settings are listed in increasing amount of required modification on top of the sequence model decoding approach routinely used in natural language processing.
The core algorithm providing the foundation of our planning techniques, beam search, is described in Algorithm 1 for generic sequences. Following the presentation in Meister et al. (2020), we have overloaded $\log P_{\theta}(\cdot \mid \mathbf{x})$ to define the likelihood of a set of sequences in addition to that of a single sequence: $\log P_{\theta}(Y \mid x)=\sum_{\mathbf{y} \in Y} \log P_{\theta}(\mathbf{y} \mid \mathbf{x})$. We use ( ) to denote the empty sequence and $\circ$ to represent concatenation.</p>
<p>Imitation learning. When the goal is to reproduce the distribution of trajectories in the training data, we can optimize directly for the probability of a trajectory $\boldsymbol{\tau}$. This situation matches the goal of sequence modeling exactly and as such we may use Algorithm 1 without modification by setting the conditioning input $\mathbf{x}$ to the current state $\mathbf{s}<em _t="&lt;t">{t}$ (and optionally previous history $\boldsymbol{\tau}</em>$ ).
The result of this procedure is a tokenized trajectory $\boldsymbol{\tau}$, beginning from a current state $\mathbf{s}<em t="t">{t}$, that has high probability under the data distribution. If the first action $\mathbf{a}</em>$ in the sequence is enacted and beam</p>
<p>search is repeated, we have a receding horizon-controller. This approach resembles a long-horizon model-based variant of behavior cloning, in which entire trajectories are optimized to match those of a reference behavior instead of only immediate state-conditioned actions. If we set the predicted sequence length to be the action dimension, our approach corresponds exactly to the simplest form of behavior cloning with an autoregressive policy.</p>
<p>Goal-conditioned reinforcement learning. Transformer architectures feature a "causal" attention mask to ensure that predictions only depend on previous tokens in a sequence. In the context of natural language, this design corresponds to generating sentences in the linear order in which they are spoken as opposed to an ordering reflecting their hierarchical syntactic structure (see, however, Gu et al. 2019 for a discussion of non-left-to-right sentence generation with autoregressive models). In the context of trajectory prediction, this choice instead reflects physical causality, disallowing future events to affect the past. However, the conditional probabilities of the past given the future are still well-defined, allowing us to condition samples not only on the preceding states, actions, and rewards that have already been observed, but also any future context that we wish to occur. If the future context is a state at the end of a trajectory, we decode trajectories with probabilities of the form:</p>
<p>$$
P_{\theta}\left(s_{t}^{i} \mid \mathbf{s}<em _t="&lt;t">{t}^{&lt;i}, \boldsymbol{\tau}</em>\right)
$$}, \mathbf{s}_{T</p>
<p>We can use this directly as a goal-reaching method by conditioning on a desired final state $\mathbf{s}<em T="T">{T}$. If we always condition sequences on a final goal state, we may leave the lower-diagonal attention mask intact and simply permute the input trajectory to $\left{\mathbf{s}</em>}, \mathbf{s<em 2="2">{1}, \mathbf{s}</em>\right}$. By prepending the goal state to the beginning of a sequence, we ensure that all other predictions may attend to it without modifying the standard attention implementation. This procedure for conditioning resembles prior methods that use supervised learning to train goal-conditioned policies (Ghosh et al., 2021) and is also related to relabeling techniques in model-free RL (Andrychowicz et al., 2017). In our framework, it is identical to the standard subroutine in sequence modeling: inferring the most likely sequence given available evidence.}, \ldots, \mathbf{s}_{T-1</p>
<p>Offline reinforcement learning. The beam search method described in Algorithm 1 optimizes sequences for their probability under the data distribution. By replacing the log-probabilities of transitions with the predicted reward signal, we can use the same Trajectory Transformer and search strategy for reward-maximizing behavior. Appealing to the control as inference graphical model (Levine, 2018), we are in effect replacing a transition's log-probability in beam search with its log-probability of optimality.
Using beam-search as a reward-maximizing procedure has the risk of leading to myopic behavior. To address this issue, we augment each transition in the training trajectories with reward-to-go: $R_{t}=\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r_{t^{\prime}}$ and include it as an additional quantity, discretized identically to the others, to be predicted after immediate rewards $r_{t}$. During planning, we then have access to value estimates from our model to add to cumulative rewards. While acting greedily with respect to such Monte Carlo value estimates is known to suffer from poor sample complexity and convergence to suboptimal behavior when online data collection is not allowed, we only use this reward-to-go estimate as a heuristic to guide beam search, and hence our method does not require the estimated values to be as accurate as in methods that rely solely on the value estimates to select actions.
In offline RL, reward-to-go estimates are functions of the behavior policy that collected the training data and do not, in general, correspond to the values achieved by the Trajectory Transformer-derived policy. Of course, it is much simpler to learn the value function of the behavior policy than that of the optimal policy, since we can simply use Monte Carlo estimates without relying on Bellman updates. A value function for an improved policy would provide a better search heuristic, though requires invoking the tools of dynamic programming. In Section 4.2 we show that the simple reward-to-go estimates are sufficient for planning with the Trajectory Transformer in many environments, but that improved value functions are useful in the most challenging settings, such as sparse-reward tasks.
Because the Trajectory Transformer predicts reward and reward-to-go only every $N+M+1$ tokens, we sample all intermediate tokens according to model log-probabilities, as in the imitation learning and goal-reaching settings. More specifically, we sample full transitions $\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\right)$ using likelihood-maximizing beam search, treat these transitions as our vocabulary, and filter sequences of transitions by those with the highest cumulative reward plus reward-to-go estimate.}, r_{t}, R_{t</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 (Prediction visualization) A qualitative comparison of length-100 trajectories generated by the Trajectory Transformer and a feedforward Gaussian dynamics model from PETS, a state-of-the-art planning algorithm Chua et al. (2018). Both models were trained on trajectories collected by a single policy, for which a true trajectory is shown for reference. Compounding errors in the single-step model lead to physically implausible predictions, whereas the Transformer-generated trajectory is visually indistinguishable from those produced by the policy acting in the actual environment. The paths of the feet and head are traced through space for depiction of the movement between rendered frames.</p>
<p>We have taken a sequence-modeling route to what could be described as a fairly simple-looking modelbased planning algorithm, in that we sample candidate action sequences, evaluate their effects using a predictive model, and select the reward-maximizing trajectory. This conclusion is in part due to the close relation between sequence modeling and trajectory optimization. There is one dissimilarity, however, that is worth highlighting: by modeling actions jointly with states and sampling them using the same procedure, we can prevent the model from being queried on out-of-distribution actions. The alternative, of treating action sequences as unconstrained optimization variables that do not depend on state (Nagabandi et al., 2018), can more readily lead to model exploitation, as the problem of maximizing reward under a learned model closely resembles that of finding adversarial examples for a classifier (Goodfellow et al., 2014).</p>
<h1>4 Experiments</h1>
<p>Our experimental evaluation focuses on (1) the accuracy of the Trajectory Transformer as a longhorizon predictor compared to standard dynamics model parameterizations and (2) the utility of sequence modeling tools - namely beam search - as a control algorithm in the context of offline reinforcement learning, imitation learning, and goal-reaching.</p>
<h3>4.1 Model Analysis</h3>
<p>We begin by evaluating the Trajectory Transformer as a long-horizon policy-conditioned predictive model. The usual strategy for predicting trajectories given a policy is to rollout with a single-step model, with actions supplied by the policy. Our protocol differs from the standard approach not only in that the model is not Markovian, but also in that it does not require access to a policy to make predictions - the outputs of the policy are modeled alongside the states encountered by that policy. Here, we focus only on the quality of the model's predictions; we use actions predicted by the model for an imitation learning method in the next subsection.</p>
<p>Trajectory predictions. Figure 2 depicts a visualization of predicted 100-timestep trajectories from our model after having trained on a dataset collected by a trained humanoid policy. Though model-based methods have been applied to the humanoid task, prior works tend to keep the horizon</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 (Compounding model errors) We compare the accuracy of the Trajectory Transformer (with uniform discretization) to that of the probabilistic feedforward model ensemble (Chua et al., 2018) over the course of a planning horizon in the humanoid environment, corresponding to the trajectories visualized in Figure 2. The Trajectory Transformer has substantially better error compounding with respect to prediction horizon than the feedforward model. The discrete oracle is the maximum log likelihood attainable given the discretization size; see Appendix B for a discussion.
intentionally short to prevent the accumulation of model errors (Janner et al., 2019; Amos et al., 2021). The reference model is the probabilistic ensemble implementation of PETS (Chua et al., 2018); we tuned the number of models within the ensemble, the number of layers, and layer sizes, but were unable to produce a model that predicted accurate sequences for more than a few dozen steps. In contrast, we see that the Trajectory Transformer's long-horizon predictions are substantially more accurate, remaining visually indistinguishable from the ground-truth trajectories even after 100 predicted steps. To our knowledge, no prior model-based RL algorithm has demonstrated predicted rollouts of such accuracy and length on tasks of comparable dimensionality.</p>
<p>Error accumulation. A quantitative account of the same finding is provided in Figure 3, in which we evaluate the model's accumulated error versus prediction horizon. Standard predictive models tend to have excellent single-step errors but poor long-horizon accuracy, so instead of evaluating a test-set single-step likelihood, we sample 1000 trajectories from a fixed starting point to estimate the per-timestep state marginal predicted by each model. We then report the likelihood of the states visited by the reference policy on a held-out set of trajectories under these predicted marginals. To evaluate the likelihood under our discretized model, we treat each bin as a uniform distribution over its specified range; by construction, the model assigns zero probability outside of this range.
To better isolate the source of the Transformer's improved accuracy over standard single-step models, we also evaluate a Markovian variant of our same architecture. This ablation has a truncated context window that prevents it from attending to more than one timestep in the past. This model performs similarly to the trajectory Transformer on fully-observed environments, suggesting that architecture differences and increased expressivity from the autoregressive state discretization play a large role in the trajectory Transformer's long-horizon accuracy. We construct a partially-observed version of the same humanoid environment, in which each dimension of every state is masked out with $50 \%$ probability (Figure 3 right), and find that, as expected, the long-horizon conditioning plays a larger role in the model's accuracy in this setting.</p>
<p>Attention patterns. We visualize the attention maps during model predictions in Figure 4. We find two primary attention patterns. The first is a discovered Markovian strategy, in which a state prediction attends overwhelmingly to the previous transition. The second is qualitatively striated, with the model attending to specific dimensions in multiple prior states for each state prediction. Simultaneously, the action predictions attend to prior actions more than they do prior states. The action dependencies contrast with the usual formulation of behavior cloning, in which actions are a function of only past states, but is reminiscent of the action filtering technique used in some planning algorithm to produce smoother action sequences (Nagabandi et al., 2019).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 (Attention patterns) We observe two distinct types of attention masks during trajectory prediction. In the first, both states and actions are dependent primarily on the immediately preceding transition, corresponding to a model that has learned the Markov property. The second strategy has a striated appearance, with state dimensions depending most strongly on the same dimension of multiple previous timesteps. Surprisingly, actions depend more on past actions than they do on past states, reminiscent of the action smoothing used in some trajectory optimization algorithms Nagabandi et al., 2019). The above masks are produced by a first- and third-layer attention head during sequence prediction on the hopper benchmark; reward dimensions are omitted for this visualization. ${ }^{1}$</p>
<h1>4.2 Reinforcement Learning and Control</h1>
<p>Offline reinforcement learning. We evaluate the Trajectory Transformer on a number of environments from the D4RL offline benchmark suite (Fu et al., 2020), including the locomotion and AntMaze domains. This evaluation is the most difficult of our control settings, as reward-maximizing behavior is the most qualitatively dissimilar from the types of behavior that are normally associated with unsupervised modeling - namely, imitative behavior. Results for the locomotion environments are shown in Table 1. We compare against five other methods spanning other approaches to data-driven control: (1) behavior-regularized actor-critic (BRAC; Wu et al. 2019) and conservative $Q$-learning (CQL; Kumar et al. 2020a) represent the current state-of-the-art in model-free offline RL; model-based offline planning (MBOP; Argenson \&amp; Dulac-Arnold 2021) is the best-performing prior offline trajectory optimization technique; decision transformer (DT; Chen et al. (2021)) is a concurrently-developed sequence-modeling approach that uses return-conditioning instead of planning; and behavior-cloning (BC) provides the performance of a pure imitative method.
The Trajectory Transformer performs on par with or better than all prior methods (Table 1). The two discretization variants of the Trajectory Transformer, uniform and quantile, perform similarly on all environments except for HalfCheetah-Medium-Expert, where the large range of the velocities prevents the uniform discretization scheme from recovering the precise actuation required for enacting the expert policy. As a result, the quantile discretization approach achieves a return of more than twice that of the uniform discretization.</p>
<p>Combining with $Q$-functions. Though Monte Carlo value estimates are sufficient for many standard offline RL benchmarks, in sparse-reward and long-horizon settings they become too uninformative to guide the beam-search-based planning procedure. In these problems, the value estimate from the Transformer can be replaced with a $Q$-function trained via dynamic programming. We explore this combination by using the $Q$-function from the implicit $Q$-learning algorithm (IQL; Kostrikov et al. 2021) on the AntMaze navigation tasks (Fu et al., 2020), for which there is only a sparse reward upon reaching the goal state. These tasks evaluate temporal compositionality because they require stitching together multiple zero-reward trajectories in the dataset to reach a designated goal.
AntMaze results are provided in Table 2. Q-guided Trajectory Transformer planning outperforms all prior methods on all maze sizes and dataset compositions. In particular, it outperforms the IQL method from which we obtain the $Q$-function, underscoring that planning with a $Q$-function as a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Environment</th>
<th>BC</th>
<th>MBOP</th>
<th>BRAC</th>
<th>CQL</th>
<th>DT</th>
<th>TT (uniform)</th>
<th>TT (quantile)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Med-Expert</td>
<td>HalfCheetah</td>
<td>59.9</td>
<td>105.9</td>
<td>41.9</td>
<td>91.6</td>
<td>86.8</td>
<td>$40.8 \pm 2.3$</td>
<td>$95.0 \pm 0.2$</td>
</tr>
<tr>
<td>Med-Expert</td>
<td>Hopper</td>
<td>79.6</td>
<td>55.1</td>
<td>0.9</td>
<td>105.4</td>
<td>107.6</td>
<td>$106.0 \pm 0.28$</td>
<td>$110.0 \pm 2.7$</td>
</tr>
<tr>
<td>Med-Expert</td>
<td>Walker2d</td>
<td>36.6</td>
<td>70.2</td>
<td>81.6</td>
<td>108.8</td>
<td>108.1</td>
<td>$91.0 \pm 2.8$</td>
<td>$101.9 \pm 6.8$</td>
</tr>
<tr>
<td>Medium</td>
<td>HalfCheetah</td>
<td>43.1</td>
<td>44.6</td>
<td>46.3</td>
<td>44.0</td>
<td>42.6</td>
<td>$44.0 \pm 0.31$</td>
<td>$46.9 \pm 0.4$</td>
</tr>
<tr>
<td>Medium</td>
<td>Hopper</td>
<td>63.9</td>
<td>48.8</td>
<td>31.3</td>
<td>58.5</td>
<td>67.6</td>
<td>$67.4 \pm 2.9$</td>
<td>$61.1 \pm 3.6$</td>
</tr>
<tr>
<td>Medium</td>
<td>Walker2d</td>
<td>77.3</td>
<td>41.0</td>
<td>81.1</td>
<td>72.5</td>
<td>74.0</td>
<td>$81.3 \pm 2.1$</td>
<td>$79.0 \pm 2.8$</td>
</tr>
<tr>
<td>Med-Replay</td>
<td>HalfCheetah</td>
<td>4.3</td>
<td>42.3</td>
<td>47.7</td>
<td>45.5</td>
<td>36.6</td>
<td>$44.1 \pm 0.9$</td>
<td>$41.9 \pm 2.5$</td>
</tr>
<tr>
<td>Med-Replay</td>
<td>Hopper</td>
<td>27.6</td>
<td>12.4</td>
<td>0.6</td>
<td>95.0</td>
<td>82.7</td>
<td>$99.4 \pm 3.2$</td>
<td>$91.5 \pm 3.6$</td>
</tr>
<tr>
<td>Med-Replay</td>
<td>Walker2d</td>
<td>36.9</td>
<td>9.7</td>
<td>0.9</td>
<td>77.2</td>
<td>66.6</td>
<td>$79.4 \pm 3.3$</td>
<td>$82.6 \pm 6.9$</td>
</tr>
<tr>
<td>Average</td>
<td></td>
<td>47.7</td>
<td>47.8</td>
<td>36.9</td>
<td>77.6</td>
<td>74.7</td>
<td>72.6</td>
<td>78.9</td>
</tr>
</tbody>
</table>
<p>Table 1 (Offline reinforcement learning) The Trajectory Transformer (TT) performs on par with or better than the best prior offline reinforcement learning algorithms on D4RL locomotion (v2) tasks. Results for TT variants correspond to the mean and standard error over 15 random seeds (5 independently trained Transformers and 3 trajectories per Transformer). We detail the sources of the performance for other methods in Appendix C.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 (Offline averages) A plot showing the average per-algorithm performance in Table 1, with bars colored according to a crude algorithm categorization. In this plot, "Trajectory Transformer" refers to the quantile discreization variant.
search heuristic can be less susceptible to errors in the $Q$-function than policy extraction. However, because the $Q$-guided planning procedure still benefits from the temporal compositionality of both dynamic programming and planning, it outperforms return-conditioning approaches, such as the Decision Transformer, that suffer due to the lack of complete demonstrations in the AntMaze datasets.</p>
<p>Imitation and goal-reaching. We additionally plan with the Trajectory Transformer using standard likelihood-maximizing beam search, as opposed to the return-maximizing version used for offline RL. We find that after training the model on datasets collected by expert policies (Fu et al., 2020), using beam search as a receding-horizon controller achieves an average normalized return of $104 \%$ and $109 \%$ in the Hopper and Walker2d environments, respectively, using the same evaluation protocol of 15 runs described as in the offline RL results. While this result is perhaps unsurprising, as behavior cloning with standard feedforward architectures is already able to reproduce the behavior of the expert policies, it demonstrates that a decoding algorithm used for language modeling can be effectively repurposed for control.</p>
<p>Finally, we evaluate the goal-reaching variant of beam-search, which conditions on a future desired state alongside previously encountered states. We use a continuous variant of the classic four rooms environment as a testbed (Sutton et al., 1999). Our training data consists of trajectories collected by a pretrained goal-reaching agent, with start and goal states sampled uniformly at random across the state space. Figure 6 depicts routes taken by the the planner. Anti-causal conditioning on a future state allows for beam search to be used as a goal-reaching method. No reward shaping, or rewards of any sort, are required; the planning method relies entirely on goal relabeling. An extension of this experiment to procedurally-generated maps is described in Appendix F.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Environment</th>
<th>BC</th>
<th>CQL</th>
<th>IQL</th>
<th>DT</th>
<th>TT $(+Q)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Umaze</td>
<td>AntMaze</td>
<td>54.6</td>
<td>74.0</td>
<td>87.5</td>
<td>59.2</td>
<td>$100.0 \pm 0.0$</td>
</tr>
<tr>
<td>Medium-Play</td>
<td>AntMaze</td>
<td>0.0</td>
<td>61.2</td>
<td>71.2</td>
<td>0.0</td>
<td>$93.3 \pm 6.4$</td>
</tr>
<tr>
<td>Medium-Diverse</td>
<td>AntMaze</td>
<td>0.0</td>
<td>53.7</td>
<td>70.0</td>
<td>0.0</td>
<td>$100.0 \pm 0.0$</td>
</tr>
<tr>
<td>Large-Play</td>
<td>AntMaze</td>
<td>0.0</td>
<td>15.8</td>
<td>39.6</td>
<td>0.0</td>
<td>$66.7 \pm 12.2$</td>
</tr>
<tr>
<td>Large-Diverse</td>
<td>AntMaze</td>
<td>0.0</td>
<td>14.9</td>
<td>47.5</td>
<td>0.0</td>
<td>$60.0 \pm 12.7$</td>
</tr>
<tr>
<td>Average</td>
<td></td>
<td>10.9</td>
<td>44.9</td>
<td>63.2</td>
<td>11.8</td>
<td>84.0</td>
</tr>
</tbody>
</table>
<p>Table 2 (Combining with $Q$-functions) Performance on the sparse-reward AntMaze (v0) navigation task. Using a $Q$-function as a search heuristic with the Trajectory Transformer (TT $(+Q)$ ) outperforms policy extraction from the $Q$-function (IQL) and return-conditioning approaches like the Decision Transformer (DT). We report means and standard error over 15 random seeds for TT $(+Q)$; baseline results are taken from Kostrikov et al. (2021).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 (Goal-reaching) Trajectories collected by TTO with anti-causal goal-state conditioning in a continuous variant of the four rooms environment. Trajectories are visualized as curves passing through all encountered states, with color becoming more saturated as time progresses. Note that these curves depict real trajectories collected by the controller and not sampled sequences. The starting state is depicted by $\boldsymbol{\otimes}$ and the goal state by $\boldsymbol{\otimes}$. Best viewed in color.</p>
<h1>5 Discussion and Limitations</h1>
<p>We have presented a sequence modeling view on reinforcement learning that enables us to derive a single algorithm for a diverse range of problem settings, unifying many of the standard components of reinforcement learning algorithms (such as policies, models, and value functions) under a single sequence model. The algorithm involves training a sequence model jointly on states, actions, and rewards and sampling from it using a minimally modified beam search. Despite drawing from the tools of large-scale language modeling instead of those normally associated with control, we find that this approach is effective in imitation learning, goal-reaching, and offline reinforcement learning.
However, prediction with Transformers is currently slower and more resource-intensive than prediction with the types of single-step models often used in model-based control, requiring up to multiple seconds for action selection when the context window grows too large. This precludes real-time control with standard Transformers for most dynamical systems. While the beam-search-based planner is conceptually an instance of model-predictive control, and as such could be applicable wherever model-based RL is, in practice the slow planning also makes online RL experiments unwieldy. (Computationally-efficient Transformer architectures (Tay et al., 2021) could potentially cut runtimes down substantially.) Further, we have chosen to discretize continuous data to fit a standard architecture instead of modifying the architecture to handle continuous inputs. While we found this design to be much more effective than conventional continuous dynamics models, it does in principle impose an upper bound on prediction precision.
This paper is an investigation of a minimal type of algorithm that can be applied to RL problems. While one of the interesting implications of our results is that RL problems can be reframed as supervised learning tasks with an appropriate choice of model, the most practical instantiation of this idea may come from combinations with dynamic programming techniques, as suggested by the effectiveness of the Trajectory Transformer with $Q$-guided planning.</p>
<h1>Code References</h1>
<p>We used the following open-source libraries for this work: NumPy (Harris et al., 2020), PyTorch (Paszke et al., 2019), and minGPT (Karpathy, 2020).</p>
<h2>Acknowledgements</h2>
<p>We thank Ethan Perez and Max Kleiman-Weiner for helpful discussions and Ben Eysenbach for feedback on an early draft. M.J. thanks Karthik Narasimhan for early inspiration about parallels between language modeling and model-based reinforcement learning. This work was partially supported by computational resource donations from Microsoft. M.J. is supported by fellowships from the National Science Foundation and the Open Philanthropy Project.</p>
<h2>References</h2>
<p>Amos, B., Stanton, S., Yarats, D., and Wilson, A. G. On the model-based stochastic value gradient for continuous reinforcement learning. In Conference on Learning for Dynamics and Control, 2021.</p>
<p>Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. Hindsight experience replay. In Advances in Neural Information Processing Systems. 2017.</p>
<p>Argenson, A. and Dulac-Arnold, G. Model-based offline planning. In International Conference on Learning Representations, 2021.</p>
<p>Bakker, B. Reinforcement learning with long short-term memory. In Neural Information Processing Systems, 2002.</p>
<p>Bellman, R. Dynamic Programming. Dover Publications, 1957.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H. Sample-efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision Transformer: Reinforcement learning via sequence modeling. arXiv preprint arXiv:2106.01345, 2021.</p>
<p>Chiappa, S., Racaniere, S., Wierstra, D., and Mohamed, S. Recurrent environment simulators. In International Conference on Learning Representations, 2017.</p>
<p>Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems. 2018.</p>
<p>Co-Reyes, J., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In International Conference on Machine Learning, 2018.</p>
<p>Dadashi, R., Rezaeifar, S., Vieillard, N., Hussenot, L., Pietquin, O., and Geist, M. Offline reinforcement learning with pseudometric learning. In International Conference on Machine Learning, 2021.</p>
<p>Deisenroth, M. and Rasmussen, C. E. PILCO: A model-based and data-efficient approach to policy search. In International Conference on Machine Learning, 2011.</p>
<p>Fairbank, M. Reinforcement learning by value gradients. arXiv preprint arXiv:0803.3539, 2008.
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.</p>
<p>Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, 2019.</p>
<p>Ghasemipour, S. K. S., Schuurmans, D., and Gu, S. S. EMaQ: Expected-max Q-learning operator for simple yet effective offline and online RL. 2021.</p>
<p>Ghosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C. M., Eysenbach, B., and Levine, S. Learning to reach goals via iterated supervised learning. In International Conference on Learning Representations, 2021.</p>
<p>Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.</p>
<p>Gu, J., Liu, Q., and Cho, K. Insertion-based Decoding with Automatically Inferred Generation Order. Transactions of the Association for Computational Linguistics, 2019.</p>
<p>Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357-362, 2020.</p>
<p>Heess, N., Hunt, J. J., Lillicrap, T., and Silver, D. Memory-based control with recurrent neural networks. In Neural Information Processing Systems Deep Reinforcement Learning Workshop, 2015a.</p>
<p>Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, 2015b.</p>
<p>Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.</p>
<p>Janner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, 2019.</p>
<p>Janner, M., Mordatch, I., and Levine, S. $\gamma$-models: Generative temporal difference learning for infinite-horizon prediction. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Jiang, Y., Gu, S., Murphy, K., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. In Advances in Neural Information Processing Systems, 2019.</p>
<p>Jin, Y., Yang, Z., and Wang, Z. Is pessimism provably efficient for offline RL? In International Conference on Machine Learning, 2021.</p>
<p>Karpathy, A. minGPT: A minimal pytorch re-implementation of the openai gpt training, 2020. URL https://github.com/karpathy/minGPT.</p>
<p>Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. MOReL: Model-based offline reinforcement learning. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.</p>
<p>Kostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.</p>
<p>Kumar, A., Fu, J., Tucker, G., and Levine, S. Stabilizing off-policy Q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, 2019a.</p>
<p>Kumar, A., Peng, X. B., and Levine, S. Reward-conditioned policies. arXiv preprint arXiv:1912.13465, 2019b.</p>
<p>Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative Q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems, 2020a.</p>
<p>Kumar, S., Parker, J., and Naderian, P. Adaptive transformers in RL. arXiv preprint arXiv:2004.03761, 2020b.</p>
<p>Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. Model-ensemble trust-region policy optimization. In International Conference on Learning Representations, 2018.</p>
<p>Lampe, T. and Riedmiller, M. Approximate model-assisted neural fitted Q-iteration. In International Joint Conference on Neural Networks, 2014.</p>
<p>Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.</p>
<p>Malik, A., Kuleshov, V., Song, J., Nemer, D., Seymour, H., and Ermon, S. Calibrated model-based deep reinforcement learning. In International Conference on Machine Learning, 2019.</p>
<p>Meister, C., Cotterell, R., and Vieira, T. If beam search is the answer, what was the question? In Empirical Methods in Natural Language Processing, 2020.</p>
<p>Nagabandi, A., Kahn, G., S. Fearing, R., and Levine, S. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In International Conference on Robotics and Automation, 2018.</p>
<p>Nagabandi, A., Konoglie, K., Levine, S., and Kumar, V. Deep Dynamics Models for Learning Dexterous Manipulation. In Conference on Robot Learning, 2019.</p>
<p>Nair, A., Dalal, M., Gupta, A., and Levine, S. Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.</p>
<p>Oh, J., Chockalingam, V., Lee, H., et al. Control of memory, active perception, and action in Minecraft. In International Conference on Machine Learning, 2016.</p>
<p>Parisotto, E. and Salakhutdinov, R. Efficient transformers in reinforcement learning using actorlearner distillation. In International Conference on Learning Representations, 2021.</p>
<p>Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, 2020.</p>
<p>Paster, K., McIlraith, S. A., and Ba, J. Planning from pixels using inverse dynamics models. In International Conference on Learning Representations, 2021.</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems. 2019.</p>
<p>Peng, X. B., Berseth, G., Yin, K., and Van De Panne, M. DeepLoco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics, 2017.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. 2018.</p>
<p>Rauber, P., Ummadisingu, A., Mutz, F., and Schmidhuber, J. Hindsight policy gradients. In International Conference on Learning Representations, 2019.</p>
<p>Reddy, R. Speech understanding systems: Summary of results of the five-year research effort at Carnegie Mellon University, 1977.</p>
<p>Ross, S. and Bagnell, D. Efficient reductions for imitation learning. In International Conference on Artificial Intelligence and Statistics, 2010.</p>
<p>Ross, S., Gordon, G., and Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics, 2011 .</p>
<p>Schmidhuber, J. Reinforcement learning upside down: Don’t predict rewards--just map them to actions. arXiv preprint arXiv:1912.02875, 2019.</p>
<p>Silver, D., Sutton, R. S., and Müller, M. Sample-based learning and search with permanent and transient memories. In International Conference on Machine Learning, 2008.</p>
<p>Srivastava, R. K., Shyam, P., Mutz, F., Jaśkowski, W., and Schmidhuber, J. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.</p>
<p>Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, 2014.</p>
<p>Sutton, R. S. Learning to predict by the methods of temporal differences. Machine Learning, 1988.
Sutton, R. S. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In International Conference on Machine Learning, 1990.</p>
<p>Sutton, R. S., Precup, D., and Singh, S. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 1999.</p>
<p>Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2021.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Wang, T. and Ba, J. Exploring model-based planning with policy networks. In International Conference on Learning Representations, 2020.</p>
<p>Williams, R. J. and Zipser, D. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1989.</p>
<p>Wu, Y., Tucker, G., and Nachum, O. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.</p>
<p>Yin, M., Bai, Y., and Wang, Y.-X. Near-optimal offline reinforcement learning via double variance reduction. arXiv preprint arXiv:2102.01748, 2021.</p>
<p>Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma, T. MOPO: Model-based offline policy optimization. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Zhang, M. R., Paine, T., Nachum, O., Paduraru, C., Tucker, G., ziyu wang, and Norouzi, M. Autoregressive dynamics models for offline policy evaluation and optimization. In International Conference on Learning Representations, 2021.</p>
<h1>Appendix A Model and Training Specification</h1>
<p>Architecture and optimization details. In all environments, we use a Transformer architecture with four layers and four self-attention heads. The total input vocabulary of the model is $V \times(N+$ $M+2)$ to account for states, actions, rewards, and rewards-to-go, but the output linear layer produces logits only over a vocabulary of size $V$; output tokens can be interpreted unambiguously because their offset is uniquely determined by that of the previous input. The dimension of each token embedding is 128 . Dropout is applied at the end of each block with probability 0.1 .
We follow the learning rate scheduling of (Radford et al., 2018), increasing linearly from 0 to $2.5 \times 10^{-4}$ over the course of 2000 updates. We use a batch size of 256.</p>
<p>Hardware. Model training took place on NVIDIA Tesla V100 GPUs (NCv3 instances on Microsoft Azure) for 80 epochs, taking approximately 6-12 hours (varying with dataset size) per model on one GPU.</p>
<h2>Appendix B Discrete Oracle</h2>
<p>The discrete oracle in Figure 3 is the maximum log-likelihood attainable by a model under the uniform discretization granularity. For a single state dimension $i$, this maximum is achieved by a model that places all probability mass on the correct token, corresponding to a uniform distribution over an interval of size</p>
<p>$$
\frac{r_{i}-\ell_{i}}{V}
$$</p>
<p>The total log-likelihood over the entire state is then given by:</p>
<p>$$
\sum_{i=1}^{N} \log \frac{V}{r_{i}-\ell_{i}}
$$</p>
<h2>Appendix C Baseline performance sources</h2>
<p>Offline reinforcement learning The performance of MOPO is taken from Table 1 in Yu et al. (2020). The performance of MBOP is taken from Table 1 in Argenson \&amp; Dulac-Arnold (2021). The performance of BC is taken from Table 1 in Kumar et al. (2020a). The performance of CQL is taken from Table 1 in Kostrikov et al. (2021).</p>
<h2>Appendix D Datasets</h2>
<p>The D4RL dataset (Fu et al., 2020) used in our experiments is under the Creative Commons Attribution 4.0 License (CC BY). The license information can be found at
https://github.com/rail-berkeley/d4rl/blob/master/README.md under the "Licenses" section.</p>
<h1>Appendix E Beam Search Hyperparameters</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Beam width</th>
<th style="text-align: left;">maximum number of hypotheses retained during beam search</th>
<th style="text-align: right;">256</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Planning horizon</td>
<td style="text-align: left;">number of transitions predicted by the model during</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Vocabulary size</td>
<td style="text-align: left;">number of bins used for autoregressive discretization</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">Context size</td>
<td style="text-align: left;">number of input $\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}, \mathbf{r<em t="t">{t}, R</em>\right)$ transitions</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">$\boldsymbol{k}_{\text {obs }}$</td>
<td style="text-align: left;">top- $k$ tokens from which observations are sampled</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">$\boldsymbol{k}_{\text {act }}$</td>
<td style="text-align: left;">top- $k$ tokens from which actions</td>
<td style="text-align: right;">20</td>
</tr>
</tbody>
</table>
<p>Beam width and context size are standard hyperparameters for decoding Transformer language models. Planning horizon is a standard trajectory optimization hyperparameter. The hyperparameters $k_{\text {obs }}$ and $k_{\text {act }}$ indicate that actions are sampled from the most likely $20 \%$ of action tokens and next observations are decoded greedily conditioned on previous observations and actions.
In many environments, the beam width and horizon may be reduced to speed up planning without affecting performance. Examples of these configurations are provided in the reference implementation: github.com/jannerm/trajectory-transformer.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 (Goal-Reaching in MiniGrid) Example paths of the Trajectory Transformer planner in the MiniGrid-MultiRoom-N4-S5. Lock symbols indicate doors.</p>
<h1>Appendix F Goal-Reaching on Procedurally-Generated Maps</h1>
<p>The method evaluated here and the experimental setup is identical to that described in Section 3.2 (Goal-conditioned reinforcement learning), with one distinction: because the map changes each episode, the Transformer model has an additional context embedding that is a function of the current observation image. This embedding is the output of a small convolutional neural network and is added to the token embeddings analogously to the treatment of position embeddings. The agent position and goal state are not included in the map; these are provided as input tokens as described in Section 3.2.</p>
<p>The action space of this environment is discrete. There are seven actions, but only four are required to complete the tasks: turning left, turning right, moving forward, and opening a door. The training data is a mixture of trajectories from a pre-trained goal-reaching policy and a uniform random policy.
$94 \%$ of testing goals are reached by the model on held-out maps. Example paths are shown in Figure 7.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ More attention visualizations can be found at trajectory-transformer.github.io/attention&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
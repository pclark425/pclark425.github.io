<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-265281606</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.10117v1.pdf" target="_blank">Automatic Engineering of Long Prompts</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable capabilities in solving complex open-domain tasks, guided by comprehensive instructions and demonstrations provided in the form of prompts. However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort. Recent research has explored automatic prompt engineering for short prompts, typically consisting of one or a few sentences. However, the automatic design of long prompts remains a challenging problem due to its immense search space. In this paper, we investigate the performance of greedy algorithms and genetic algorithms for automatic long prompt engineering. We demonstrate that a simple greedy approach with beam search outperforms other methods in terms of search efficiency. Moreover, we introduce two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in our search algorithm. Our results show that the proposed automatic long prompt engineering algorithm achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard, highlighting the significance of automating prompt designs to fully harness the capabilities of LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long CoT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long prompts with chain-of-thought demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts comprising an instruction plus multiple demonstration examples with human-written chain-of-thought explanations (often thousands of tokens and >20 sentences); used as the base format for Big-Bench Hard tasks and subject to automated rephrasing/tuning in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (PaLM 2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2 family model used for evaluation of tuned hard prompts (API model named text-bison in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Big-Bench Hard (BBH) — selected 8 tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-class classification tasks from BBH where each example includes a question and the prompt provides task description + multiple demos with chain-of-thought and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long natural-language prompt containing: Task Description + multiple demo examples each with chain-of-thought reasoning and final answer (few-shot with demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Preserves format markers such as 'Question.' and 'Answer.'; prompts contain instruction and many demos with CoT; initial human-crafted prompts often thousands of tokens; tuning allowed rephrasing of mutable sentences but kept delimiters and QA markers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average +9.2% absolute accuracy on full evaluation set (train+test) across 8 BBH tasks; average +8.2% absolute on test set</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original human-designed BBH prompts (used as initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+9.2% absolute (average, full eval); +8.2% absolute (average, test)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Search budget 50 evaluations on training set per run; beam (pool) size = 4; evaluation model temperature = 0; LLM-Mutator (PaLM 2-L) temperature = 0.5; training/test split 50%/50% of 250 examples per task.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence rephrasing (selected)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rewriting selected sentences in a long prompt via LLM mutator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Targeted rephrasing of a few chosen sentences in a long prompt (keeping semantic meaning) using an LLM-based mutator; shown to produce large accuracy gains on specific BBH tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (PaLM 2 family) evaluated; PaLM 2-L used as LLM-Mutator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>text-bison used for final evaluation; instruction-tuned PaLM 2-L used to generate candidate sentence rewrites (LLM-Mutator).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Logical Deduction - Five Objects (BBH task)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Logical deduction over descriptions of ordered objects (multi-class classification).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long CoT few-shot prompt; specific sentences in the instruction/CoT demos were rephrased while keeping QA delimiters.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Only mutable sentences (excluding fixed 'Question.'/'Answer.' format tokens) were rephrased; guided by beam-search greedy algorithm and LLM mutator with in-context history examples when available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Original prompt: 38.8% accuracy (combined). Revised prompt (found at iteration 48): 56.0% combined (reported also as train 57.9%, test 54.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>38.8% combined accuracy (original prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+17.2% absolute combined accuracy (38.8% -> 56.0%)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>50 evaluations on training set, beam size 4, LLM-Mutator temperature 0.5, main-model temperature 0 for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal Fallacies rephrasing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Targeted prompt rephrasing for Formal Fallacies task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic rephrasing of sentences in the Formal Fallacies BBH prompt produced substantial gains; the paper reports concrete train/test and combined accuracy improvements for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (PaLM 2 family) evaluated; PaLM 2-L used as LLM-Mutator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluation on text-bison; mutator queries to PaLM 2-L to propose semantically equivalent rewrites.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Formal Fallacies (BBH task)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify whether an argument is a deductively valid argument or a formal fallacy; requires logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long CoT few-shot prompt with instruction and demos; tuning allowed modification of instruction and chain-of-thought sentences but preserved QA delimiters.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Mutations included subtle sentence rewrites; one mutation produced a semantically different sentence and led to overfitting on train but hurt test in that case; results reported for combined, train and test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Original combined accuracy: 60%. New combined accuracy found during search: 76% (train 92.1%, test 83.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>60% combined (original prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+16.0% absolute combined accuracy (60% -> 76%)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>50 evaluations on training set; beam size 4; LLM-Mutator temp 0.5; main-model temp 0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evolve 'step-by-step'</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-sentence evolution of the 'Let's think step by step' chain-of-thought trigger</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method that optimizes only the single CoT trigger sentence(s) (e.g., 'Let's think step by step') within a long prompt; the paper evaluates this baseline and finds it insufficient for large gains on long prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (PaLM 2 family) evaluated; baseline optimizer from prior work used for single-sentence evolution</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluation model is text-bison; the baseline optimization method (state-of-the-art single sentence optimizer) is applied to all 'Let's think step by step' occurrences in the CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBH tasks (general baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same BBH tasks used elsewhere; baseline only modifies the canonical CoT trigger sentence(s) in demos/instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long CoT prompt where only one sentence (CoT trigger) is mutated across the whole prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Baseline targets only the 'Let's think step by step' sentence(s) inside chain-of-thought sections rather than rephrasing other instruction or CoT sentences; applied across all CoT occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as failing to achieve substantial improvements for long prompt tuning (inferior to the full long-prompt tuning method).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original prompt performance (varies by task); single-sentence evolution did not substantially improve over original in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Applied state-of-the-art single-sentence optimization method (Yang et al. baseline) to all CoT trigger sentences; compared under 50-evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7429.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam-search greedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy algorithm with beam (top-k) search for prompt editing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search algorithm that maintains a pool of the k best prompts and updates the pool as new candidates are generated (immediate update), shown to converge faster and yield higher accuracy than vanilla greedy and genetic algorithms in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (PaLM 2 family) evaluated; PaLM 2-L used as mutator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Beam-search greedy search procedure used to propose and evaluate prompt variants with LLM-based mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBH tasks used in paper (8 selected tasks for main comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-class BBH tasks where long prompts are searched/tuned under a strict evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Search-based prompt rewriting: mutate one sentence at a time using LLM mutator and maintain beam of top-k prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>algorithm / prompt optimization strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Pool (beam) size k=4 in experiments; immediate pool update after each candidate generation; compared to genetic algorithm with same pool size; evaluated under 50-evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (learning curves over iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Beam-search greedy outperforms vanilla greedy and genetic algorithm in convergence and final accuracy (Figure 3); qualitative and averaged improvements reported across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla greedy (store top-1) and Genetic Algorithm (batch offspring then select top-k)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: faster convergence and higher end accuracy than both greedy and GA under the same budget (see Figure 3); no single overall numeric delta provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pool size = 4; GA baseline generated 8 offspring per generation; 50 evaluations budget; LLM-Mutator temp 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7429.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>History-guided mutation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>History-guided LLM mutator with retrieved in-context examples and contextual bandit sentence selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two techniques that use search history to guide mutations: (1) in-context example retrieval for the mutator (include prior beneficial rewrites as examples) and (2) Lin-UCB contextual bandit to prioritize which sentences to mutate; both ablated and shown to contribute to performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L (LLM-Mutator) used for guided mutation; text-bison used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 2-L used to produce candidate rewrites when provided with curated in-context history examples; T5 encoder used to compute sentence embeddings for retrieval and contextual bandit features.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBH tasks (ablation in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same BBH tasks; ablation isolates contribution from guided mutation and contextual bandit sentence selection.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt editing guided by history: include prior positive (or inverted negative) rewrite pairs as in-context examples; use Lin-UCB to pick sentences to mutate.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style + optimization strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Retrieve up to 4 relevant history entries using a T5 sentence encoder; require L2 distance < 0.5 for inclusion; Lin-UCB uses features ϕ(s) from T5, α = 0.05, randomization P = 0.5 for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation (Table 3) shows both history-guided mutation and contextual bandit sentence selection improve final accuracy relative to ablated variants (no numerical values quoted in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Same beam-search greedy without history-guidance / without contextual bandit selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Positive contribution reported in ablation (Table 3); exact numeric deltas are in Table 3 (not fully listed in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>T5 encoder for embeddings (normalized), retrieve top-4 history; Lin-UCB ridge regression (λ), α=0.05, exploration probability P=0.5; search budget 50 evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7429.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format preservation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preservation of explicit QA formatting tokens during tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>During prompt tuning the paper preserves explicit format markers such as 'Question.' and 'Answer.' while allowing rephrasing of other sentences, to keep problem presentation consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison (PaLM 2 family) evaluated; PaLM 2-L used for mutation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The evaluation pipeline retained question/answer delimiters and only allowed mutation of other sentence content.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBH tasks (all used prompts retain format tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-class questions presented with QA delimiters and CoT demos; tuning modifies semantic content but keeps structure.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>QA-style long prompts with fixed delimiters (e.g., 'Question.', 'Answer.') plus demos and chain-of-thought; these delimiters are retained during search.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Tuning allowed rewriting of instruction and chain-of-thought demo sentences but not the 'Question.'/'Answer.' tokens or overall example ordering; this constraint intended to preserve interpretability and reduce overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Implicit: all experimental gains reported (e.g., average +9.2% full eval) were obtained while preserving these format tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original prompts with same format tokens</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Not separately quantified; format preservation was part of the experimental protocol used to obtain reported gains.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Format tokens retained; mutable sentences determined per prompt (Table 1 lists number of mutable sentences per task).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7429.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7429.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Small edit sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM performance to small prompt modifications</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited observation (and motivating phenomenon) that minor prompt modifications (adding/removing a few tokens or rephrasing) can significantly change LLM performance; used to motivate automated prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general (referenced studies and motivating phenomenon)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not a single task in this paper but a cross-cutting observation motivating prompt search.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>any prompt format; small token edits or rephrasings</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper cites literature (e.g., Liu et al., 2023; Zhu et al., 2023; Jiang et al., 2020) that demonstrates sensitivity of LLMs to small prompt changes; leveraged as background to justify search over semantically-equivalent rewrites.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>various (literature-cited)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Engineering of Long Prompts', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Promptbreeder: Self-referential self-improvement via prompt evolution <em>(Rating: 2)</em></li>
                <li>Connecting large language models with evolutionary algorithms yields powerful prompt optimizers <em>(Rating: 2)</em></li>
                <li>Gps: Genetic prompt search for efficient few-shot learning <em>(Rating: 2)</em></li>
                <li>Instruction induction: From few examples to natural language task descriptions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7429",
    "paper_id": "paper-265281606",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Long CoT prompts",
            "name_full": "Long prompts with chain-of-thought demonstrations",
            "brief_description": "Prompts comprising an instruction plus multiple demonstration examples with human-written chain-of-thought explanations (often thousands of tokens and &gt;20 sentences); used as the base format for Big-Bench Hard tasks and subject to automated rephrasing/tuning in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (PaLM 2 family)",
            "model_description": "Instruction-tuned PaLM 2 family model used for evaluation of tuned hard prompts (API model named text-bison in experiments).",
            "model_size": null,
            "task_name": "Big-Bench Hard (BBH) — selected 8 tasks",
            "task_description": "Multi-class classification tasks from BBH where each example includes a question and the prompt provides task description + multiple demos with chain-of-thought and final answers.",
            "problem_format": "Long natural-language prompt containing: Task Description + multiple demo examples each with chain-of-thought reasoning and final answer (few-shot with demonstrations).",
            "format_category": "prompt style",
            "format_details": "Preserves format markers such as 'Question.' and 'Answer.'; prompts contain instruction and many demos with CoT; initial human-crafted prompts often thousands of tokens; tuning allowed rephrasing of mutable sentences but kept delimiters and QA markers.",
            "performance_metric": "accuracy",
            "performance_value": "Average +9.2% absolute accuracy on full evaluation set (train+test) across 8 BBH tasks; average +8.2% absolute on test set",
            "baseline_performance": "Original human-designed BBH prompts (used as initialization)",
            "performance_change": "+9.2% absolute (average, full eval); +8.2% absolute (average, test)",
            "experimental_setting": "Search budget 50 evaluations on training set per run; beam (pool) size = 4; evaluation model temperature = 0; LLM-Mutator (PaLM 2-L) temperature = 0.5; training/test split 50%/50% of 250 examples per task.",
            "statistical_significance": null,
            "uuid": "e7429.0",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Sentence rephrasing (selected)",
            "name_full": "Rewriting selected sentences in a long prompt via LLM mutator",
            "brief_description": "Targeted rephrasing of a few chosen sentences in a long prompt (keeping semantic meaning) using an LLM-based mutator; shown to produce large accuracy gains on specific BBH tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (PaLM 2 family) evaluated; PaLM 2-L used as LLM-Mutator",
            "model_description": "text-bison used for final evaluation; instruction-tuned PaLM 2-L used to generate candidate sentence rewrites (LLM-Mutator).",
            "model_size": null,
            "task_name": "Logical Deduction - Five Objects (BBH task)",
            "task_description": "Logical deduction over descriptions of ordered objects (multi-class classification).",
            "problem_format": "Long CoT few-shot prompt; specific sentences in the instruction/CoT demos were rephrased while keeping QA delimiters.",
            "format_category": "prompt style",
            "format_details": "Only mutable sentences (excluding fixed 'Question.'/'Answer.' format tokens) were rephrased; guided by beam-search greedy algorithm and LLM mutator with in-context history examples when available.",
            "performance_metric": "accuracy",
            "performance_value": "Original prompt: 38.8% accuracy (combined). Revised prompt (found at iteration 48): 56.0% combined (reported also as train 57.9%, test 54.0%).",
            "baseline_performance": "38.8% combined accuracy (original prompt)",
            "performance_change": "+17.2% absolute combined accuracy (38.8% -&gt; 56.0%)",
            "experimental_setting": "50 evaluations on training set, beam size 4, LLM-Mutator temperature 0.5, main-model temperature 0 for decoding.",
            "statistical_significance": null,
            "uuid": "e7429.1",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Formal Fallacies rephrasing",
            "name_full": "Targeted prompt rephrasing for Formal Fallacies task",
            "brief_description": "Automatic rephrasing of sentences in the Formal Fallacies BBH prompt produced substantial gains; the paper reports concrete train/test and combined accuracy improvements for this task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (PaLM 2 family) evaluated; PaLM 2-L used as LLM-Mutator",
            "model_description": "Evaluation on text-bison; mutator queries to PaLM 2-L to propose semantically equivalent rewrites.",
            "model_size": null,
            "task_name": "Formal Fallacies (BBH task)",
            "task_description": "Classify whether an argument is a deductively valid argument or a formal fallacy; requires logical reasoning.",
            "problem_format": "Long CoT few-shot prompt with instruction and demos; tuning allowed modification of instruction and chain-of-thought sentences but preserved QA delimiters.",
            "format_category": "prompt style",
            "format_details": "Mutations included subtle sentence rewrites; one mutation produced a semantically different sentence and led to overfitting on train but hurt test in that case; results reported for combined, train and test splits.",
            "performance_metric": "accuracy",
            "performance_value": "Original combined accuracy: 60%. New combined accuracy found during search: 76% (train 92.1%, test 83.1%).",
            "baseline_performance": "60% combined (original prompt)",
            "performance_change": "+16.0% absolute combined accuracy (60% -&gt; 76%)",
            "experimental_setting": "50 evaluations on training set; beam size 4; LLM-Mutator temp 0.5; main-model temp 0.",
            "statistical_significance": null,
            "uuid": "e7429.2",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Evolve 'step-by-step'",
            "name_full": "Single-sentence evolution of the 'Let's think step by step' chain-of-thought trigger",
            "brief_description": "Baseline method that optimizes only the single CoT trigger sentence(s) (e.g., 'Let's think step by step') within a long prompt; the paper evaluates this baseline and finds it insufficient for large gains on long prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-bison (PaLM 2 family) evaluated; baseline optimizer from prior work used for single-sentence evolution",
            "model_description": "Evaluation model is text-bison; the baseline optimization method (state-of-the-art single sentence optimizer) is applied to all 'Let's think step by step' occurrences in the CoT prompts.",
            "model_size": null,
            "task_name": "BBH tasks (general baseline comparison)",
            "task_description": "Same BBH tasks used elsewhere; baseline only modifies the canonical CoT trigger sentence(s) in demos/instruction.",
            "problem_format": "Long CoT prompt where only one sentence (CoT trigger) is mutated across the whole prompt.",
            "format_category": "prompt style",
            "format_details": "Baseline targets only the 'Let's think step by step' sentence(s) inside chain-of-thought sections rather than rephrasing other instruction or CoT sentences; applied across all CoT occurrences.",
            "performance_metric": "accuracy",
            "performance_value": "Reported as failing to achieve substantial improvements for long prompt tuning (inferior to the full long-prompt tuning method).",
            "baseline_performance": "Original prompt performance (varies by task); single-sentence evolution did not substantially improve over original in experiments.",
            "performance_change": null,
            "experimental_setting": "Applied state-of-the-art single-sentence optimization method (Yang et al. baseline) to all CoT trigger sentences; compared under 50-evaluation budget.",
            "statistical_significance": null,
            "uuid": "e7429.3",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Beam-search greedy",
            "name_full": "Greedy algorithm with beam (top-k) search for prompt editing",
            "brief_description": "A search algorithm that maintains a pool of the k best prompts and updates the pool as new candidates are generated (immediate update), shown to converge faster and yield higher accuracy than vanilla greedy and genetic algorithms in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (PaLM 2 family) evaluated; PaLM 2-L used as mutator",
            "model_description": "Beam-search greedy search procedure used to propose and evaluate prompt variants with LLM-based mutations.",
            "model_size": null,
            "task_name": "BBH tasks used in paper (8 selected tasks for main comparisons)",
            "task_description": "Multi-class BBH tasks where long prompts are searched/tuned under a strict evaluation budget.",
            "problem_format": "Search-based prompt rewriting: mutate one sentence at a time using LLM mutator and maintain beam of top-k prompts.",
            "format_category": "algorithm / prompt optimization strategy",
            "format_details": "Pool (beam) size k=4 in experiments; immediate pool update after each candidate generation; compared to genetic algorithm with same pool size; evaluated under 50-evaluation budgets.",
            "performance_metric": "accuracy (learning curves over iterations)",
            "performance_value": "Beam-search greedy outperforms vanilla greedy and genetic algorithm in convergence and final accuracy (Figure 3); qualitative and averaged improvements reported across runs.",
            "baseline_performance": "Vanilla greedy (store top-1) and Genetic Algorithm (batch offspring then select top-k)",
            "performance_change": "Qualitative: faster convergence and higher end accuracy than both greedy and GA under the same budget (see Figure 3); no single overall numeric delta provided in text.",
            "experimental_setting": "Pool size = 4; GA baseline generated 8 offspring per generation; 50 evaluations budget; LLM-Mutator temp 0.5.",
            "statistical_significance": null,
            "uuid": "e7429.4",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "History-guided mutation",
            "name_full": "History-guided LLM mutator with retrieved in-context examples and contextual bandit sentence selection",
            "brief_description": "Two techniques that use search history to guide mutations: (1) in-context example retrieval for the mutator (include prior beneficial rewrites as examples) and (2) Lin-UCB contextual bandit to prioritize which sentences to mutate; both ablated and shown to contribute to performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L (LLM-Mutator) used for guided mutation; text-bison used for evaluation",
            "model_description": "PaLM 2-L used to produce candidate rewrites when provided with curated in-context history examples; T5 encoder used to compute sentence embeddings for retrieval and contextual bandit features.",
            "model_size": null,
            "task_name": "BBH tasks (ablation in Table 3)",
            "task_description": "Same BBH tasks; ablation isolates contribution from guided mutation and contextual bandit sentence selection.",
            "problem_format": "Prompt editing guided by history: include prior positive (or inverted negative) rewrite pairs as in-context examples; use Lin-UCB to pick sentences to mutate.",
            "format_category": "prompt style + optimization strategy",
            "format_details": "Retrieve up to 4 relevant history entries using a T5 sentence encoder; require L2 distance &lt; 0.5 for inclusion; Lin-UCB uses features ϕ(s) from T5, α = 0.05, randomization P = 0.5 for exploration.",
            "performance_metric": "accuracy",
            "performance_value": "Ablation (Table 3) shows both history-guided mutation and contextual bandit sentence selection improve final accuracy relative to ablated variants (no numerical values quoted in text excerpt).",
            "baseline_performance": "Same beam-search greedy without history-guidance / without contextual bandit selection",
            "performance_change": "Positive contribution reported in ablation (Table 3); exact numeric deltas are in Table 3 (not fully listed in text excerpt).",
            "experimental_setting": "T5 encoder for embeddings (normalized), retrieve top-4 history; Lin-UCB ridge regression (λ), α=0.05, exploration probability P=0.5; search budget 50 evaluations.",
            "statistical_significance": null,
            "uuid": "e7429.5",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Format preservation",
            "name_full": "Preservation of explicit QA formatting tokens during tuning",
            "brief_description": "During prompt tuning the paper preserves explicit format markers such as 'Question.' and 'Answer.' while allowing rephrasing of other sentences, to keep problem presentation consistent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison (PaLM 2 family) evaluated; PaLM 2-L used for mutation",
            "model_description": "The evaluation pipeline retained question/answer delimiters and only allowed mutation of other sentence content.",
            "model_size": null,
            "task_name": "BBH tasks (all used prompts retain format tokens)",
            "task_description": "Multi-class questions presented with QA delimiters and CoT demos; tuning modifies semantic content but keeps structure.",
            "problem_format": "QA-style long prompts with fixed delimiters (e.g., 'Question.', 'Answer.') plus demos and chain-of-thought; these delimiters are retained during search.",
            "format_category": "prompt style / format",
            "format_details": "Tuning allowed rewriting of instruction and chain-of-thought demo sentences but not the 'Question.'/'Answer.' tokens or overall example ordering; this constraint intended to preserve interpretability and reduce overfitting.",
            "performance_metric": "accuracy",
            "performance_value": "Implicit: all experimental gains reported (e.g., average +9.2% full eval) were obtained while preserving these format tokens.",
            "baseline_performance": "Original prompts with same format tokens",
            "performance_change": "Not separately quantified; format preservation was part of the experimental protocol used to obtain reported gains.",
            "experimental_setting": "Format tokens retained; mutable sentences determined per prompt (Table 1 lists number of mutable sentences per task).",
            "statistical_significance": null,
            "uuid": "e7429.6",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Small edit sensitivity",
            "name_full": "Sensitivity of LLM performance to small prompt modifications",
            "brief_description": "Cited observation (and motivating phenomenon) that minor prompt modifications (adding/removing a few tokens or rephrasing) can significantly change LLM performance; used to motivate automated prompt engineering.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": null,
            "task_name": "general (referenced studies and motivating phenomenon)",
            "task_description": "Not a single task in this paper but a cross-cutting observation motivating prompt search.",
            "problem_format": "any prompt format; small token edits or rephrasings",
            "format_category": "prompt style",
            "format_details": "Paper cites literature (e.g., Liu et al., 2023; Zhu et al., 2023; Jiang et al., 2020) that demonstrates sensitivity of LLMs to small prompt changes; leveraged as background to justify search over semantically-equivalent rewrites.",
            "performance_metric": "various (literature-cited)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": null,
            "statistical_significance": null,
            "uuid": "e7429.7",
            "source_info": {
                "paper_title": "Automatic Engineering of Long Prompts",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Promptbreeder: Self-referential self-improvement via prompt evolution",
            "rating": 2,
            "sanitized_title": "promptbreeder_selfreferential_selfimprovement_via_prompt_evolution"
        },
        {
            "paper_title": "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
            "rating": 2,
            "sanitized_title": "connecting_large_language_models_with_evolutionary_algorithms_yields_powerful_prompt_optimizers"
        },
        {
            "paper_title": "Gps: Genetic prompt search for efficient few-shot learning",
            "rating": 2,
            "sanitized_title": "gps_genetic_prompt_search_for_efficient_fewshot_learning"
        },
        {
            "paper_title": "Instruction induction: From few examples to natural language task descriptions",
            "rating": 1,
            "sanitized_title": "instruction_induction_from_few_examples_to_natural_language_task_descriptions"
        }
    ],
    "cost": 0.01583875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Engineering of Long Prompts
16 Nov 2023</p>
<p>Cho-Jui Hsieh cjhsieh@google.com 
Si Si 
Felix X Yu felixyu@google.com 
Inderjit S Dhillon 
Automatic Engineering of Long Prompts
16 Nov 202389D1D17DF117066B626075439A492191arXiv:2311.10117v1[cs.AI]
Large language models (LLMs) have demonstrated remarkable capabilities in solving complex open-domain tasks, guided by comprehensive instructions and demonstrations provided in the form of prompts.However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort.Recent research has explored automatic prompt engineering for short prompts, typically consisting of one or a few sentences.However, the automatic design of long prompts remains a challenging problem due to its immense search space.In this paper, we investigate the performance of greedy algorithms and genetic algorithms for automatic long prompt engineering.We demonstrate that a simple greedy approach with beam search outperforms other methods in terms of search efficiency.Moreover, we introduce two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in our search algorithm.Our results show that the proposed automatic long prompt engineering algorithm achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard, highlighting the significance of automating prompt designs to fully harness the capabilities of LLMs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have exhibited remarkable capabilities when trained on large datasets, demonstrating the ability to comprehend complex and lengthy instructions for diverse tasks without the need for fine-tuning (Wei et al., 2022a;Brown et al., 2020;Chowdhery et al., 2022;Ouyang et al., 2022).Several prompt design principles have emerged in recent years, suggesting that incorporating more complex instructions, demonstrations, and chain-of-thought reasoning into prompts can boost the performance on challenging tasks (Brown et al., 2020;Wei et al., 2022b), including those involving mathematical problem-solving (Cobbe et al., 2021) and reasoning (Suzgun et al., 2022;Srivastava et al., 2022).However, effective prompts for tackling complex tasks often contain thousands of tokens, posing challenges in designing and optimizing them.Figure 1 demonstrates a long prompt for a task in Big Bench Hard (Suzgun et al., 2022), which contains an instruction and several demos, each with humanwritten chain-of-thoughts.</p>
<p>Numerous studies have demonstrated the sensitivity of LLMs to prompts, revealing that minor modifications, such as adding or removing a few tokens or rephrasing the prompt, can significantly impact LLM performance (Liu et al., 2023;Zhu et al., 2023;Jiang et al., 2020).Therefore, prompt design has become a labor-intensive endeavor.Further complicating the matter is the rapid evolution of LLMs, rendering prompts crafted for a specific LLM ineffective when applied to a newer version of LLM.This highlights the need for automatic prompt engineering techniques.</p>
<p>While automatic prompt engineering has been studied recently, existing research (Deng et al., 2022;Xu et al., 2022;Guo et al., 2023;Fernando et al., 2023) focuses on optimizing short instructions with one or a few sentences.These methods either evolve prompts by searching for word replacements or utilize LLMs to rewrite the entire prompt (Xu et al., 2022;Fernando et al., 2023;Guo et al., 2023;Yang et al., 2023).It is challenging to apply them for evolving a long prompt like Figure 1.For long prompts, word replacement-based search faces an immense search space, while rewriting the entire prompt using a single LLM query is extremely difficult.Although it is possible to apply these methods to individual sentences within a prompt, we have observed that tuning a single sentence may not sufficiently improve the performance of long prompts (see Section 4).Therefore, Figure 1: Left panel: the average accuracy improvements of the proposed method over 3 runs with 50 iterations on the training set.We report the accuracy on the whole set (including training and test sets).More detailed results can be found in Table 2. Right panel: An example of a long prompt in BBH (Disambiguation task) which consists of an instruction, several demo examples and chain-of-thought reasoning.We show that by rewriting a few selected sentences in this long prompt with our proposed method, we can improve the accuracy by more than 10%.</p>
<p>how to automatically find better long pompts and what's the potential performance gain by tuning long prompts remain unanswered in the literature.</p>
<p>The main operation in our automatic prompt engineering technique is to replace a sentence by a semantically equivalent sentence, which can be done by queries to LLMs.By comparing the pros and cons of greedy algorithm and genetic algorithm, two representative algorithms for discrete optimization, we show that a simple greedy algorithm with beam search is more effective.Further, for long prompt engineering it is important to identify which sentence to change, and to what direction.We thus propose two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in our algorithm.</p>
<p>Our contributions can be summarized below:</p>
<p>• To the best of our knowledge, this paper presents the first formal discussion of automatic long prompt engineering, demonstrating substantial performance gains on multiple tasks.</p>
<p>• We propose a greedy algorithm with beam search that can rapidly optimize prompts.Furthermore, a novel guided mutation method is introduced to enhance convergence.</p>
<p>• We conducted experiments on the Big Bench Hard (BBH) benchmark (Suzgun et al., 2022;Srivastava et al., 2022), where prompts comprise thousands of tokens, including instructions and chain-of-thought reasoning.Our results demonstrate that the proposed automatic long prompt engineering method significantly enhances performance.Notably, we achieved an average of 9.2% absolute accuracy improvements on 8 tasks selected from BBH, as shown in Figure 1, with only 50 evaluations on the training set.</p>
<p>Related Work</p>
<p>The remarkable ability of large language models (LLMs) to perform complex tasks without finetuning through prompting has significantly broadened their applicability.As a result, designing effective prompts to fully exploit the capabilities of LLMs has become an important topic.Many principal ways for prompt design have been studied recently (Reynolds and McDonell, 2021;Brown et al., 2020;Wei et al., 2022b;Wang et al., 2022aWang et al., , 2023a)).For instance, a well-designed prompt may include a system prompt (e.g., "you are an AI programming assistant"), an instruction prompt outlining the task, multiple contextual examples, and a chain-of-thoughts reasoning section that explains the step-by-step thought process behind the examples.By incorporating these elements, effective prompts often have tens of sentences and thousands of tokens.</p>
<p>An orthogonal line of previous work has explored soft-prompt tuning, a technique that optimizes prompts within a continuous embedding space using standard continuous optimization algorithms (Lester et al., 2021;Zhang et al., 2021;Wang et al., 2022b).While capable of achieving satisfactory performance, soft-prompts lack interpretability and cannot be applied via LLM APIs.Additionally, these parameter-efficient fine-tuning methods demand large training sets, making them unsuitable for applications with limited data, such as those with only tens or hundreds of samples.</p>
<p>Given the limited availability of training data (&lt;1000 samples), our focus lies in exploring strategies for optimizing hard prompts, which are semantically equivalent to the original prompts but yield superior performance.In the context of automated prompt engineering, the literature considers two primary settings.The first setting, which aligns with our work, assumes the existence of an initial human-crafted prompt and aims to refine or improve it to achieve enhanced performance.Several discrete search algorithms have been proposed for this setting: (Xu et al., 2022) employs a genetic algorithm for prompt tuning, utilizing back translation, cloze tasks, and sentence continuation to mutate the initial instruction.More recently, (Fernando et al., 2023;Guo et al., 2023) proposed leveraging LLMs for mutation and crossover operations in evolutionary searches, while (Yang et al., 2023) demonstrated the optimization capabilities of LLMs in generating improved prompt variations based on previous fitness scores.However, these prompt evolution techniques are designed for short sentences or paragraphs within a long prompt.For example, many of them try to evolve only the instruction part or the sentence "Let's think step by step" in the prompt.Our work aims to provide complete freedom to evolve the entire long prompt, opening up more avenues for improvement but also introducing challenges in determining how and where to change the original prompt.</p>
<p>Another setting focuses on automatic prompt generation without a pre-existing prompt.(Honovich et al., 2022) demonstrated the ability of LLMs to generate brief task descriptions when provided with input-output pairs.Building upon this technique and incorporating random search, (Zhou et al., 2022) proposed an automatic prompt engineering (APE) algorithm capable of generating prompts from given pairs.(Pryzant et al., 2023) explored the utilization of input-output pair feedback to refine instructions, while (Chen et al., 2023) developed a continuous relaxation approach employing Bayesian optimization for search within a continuous space.</p>
<p>Proposed Method</p>
<p>In this paper, we address the challenge of automatic long prompt engineering for language models.Given a language model L and an initial prompt p init designed by human for a particular task, our goal is to refine the prompt to achieve superior performance.To enhance the prompt, we are provided with a limited training set (e.g., 100 input-output pairs) {(x i , y i )} n i=1 for performance evaluation.Specifically, for each sample we conduct prediction by L([p, x i ]) and assess its agreement with the corresponding ground truth label y i .We define score(p) as the performance metric of prompt p on the training set.We consider the scenario where the number of available samples n is limited, which is a common situation where individuals lack sufficient data for model fine-tuning but can still utilize the data to design an improved hard prompt.We will evaluate generalization performance on a hold-out test set that is not used in prompt search.</p>
<p>In this section, we will first introduce the search space of our algorithm and an exploration of how to use LLM to navigate this space (Section 3.1).We then delve into the proposed greedy algorithm with beam search, highlighting its advantages over both vanilla greedy algorithms and genetic algorithms in Section 3.2.Finally, Section 3.3 shows how to leverage search history to guide the proposed framework to improve its performance.</p>
<p>Search space</p>
<p>Our goal is to generate a new prompt that is semantically similar to the original prompt while achieving enhanced performance.Further, we avoid introducing non-interpretable tokens, such as adversarial triggers (Zou et al., 2023).These restrictions offer two advantages: 1) the prompts discovered by our algorithm are interpretable, facilitating the verification that the prompt still performs the intended task, and 2) since we are only provided with a limited set (e.g., 100) of training samples, constraining the search space can mitigate the issue of overfitting.We then allow each sentence to be rephrased while preserving its semantic meaning.Since LLMs excel at sentence rephrasing, particularly when provided with prompts like "Generate a variation of the following instruction while keeping the semantic meaning," we employ an LLM-Mutator equipped with such a prompt to identify alternative formulations for each sentence.The prompt we used for vanilla LLM mutator can be found in Figure 2.An improved version of LLM-Mutator will be introduced later in Section 3.3.</p>
<p>Search algorithm</p>
<p>A straightforward approach to conducting the search using the LLM-Mutator is the following greedy algorithm.At each iteration, we randomly select a sentence, denoted by s (i) , and utilize the LLM-Mutator to generate an alternative sentence s ′ (i) .We then replace s (i) by s ′ (i) in the old prompt p if the new resulting prompt improves the performance.However, we observed that this vanilla greedy approach can be easily trapped in local optima.In our problem, the training set is so limited that sometimes a detrimental modification is not reflected in the training score.As a result, unfavorable edit are sometimes accepted due to insufficient evaluation, which hurts the prompt's future improvement.</p>
<p>To address this issue, we propose conducting a beam search by maintaining a pool of k topperforming prompts, denoted as p * 1 , p * 2 , . . ., p * k .At each iteration, we randomly select one of these k prompts and refine the chosen prompt.We then evaluate this new candidate and maintain the top-k prompt pool.This approach ensures that even with the introduction of detrimental edits, recovery from such errors is still possible as we retain not only the top prompt.Our experiments reveal that this approach leads to significantly improved training and test performance compared to the pure greedy algorithm, as demonstrated in Figure 3.</p>
<p>It is worth noting that our method is closely related to the Genetic Algorithm (GA).GA is a widely recognized method for discrete optimization involving black-box functions.Assuming P = {p 1 , . . ., p k } represents the solutions in the current pool, GA applies mutation and crossover operations to this pool to generate a set of newly proposed candidates
P ′ = {p ′ 1 , . . . , p ′ k }.
The fitness scores (performance of the solutions) of these newly proposed candidates are then evaluated.Subsequently, only the top k solutions in P ∪ P ′ are retained before proceeding to the next iteration.</p>
<p>While GA possesses a high exploration capability, its convergence rate is considerably slower during the initial stages.Given the vastness of our search space and the reasonably good quality of the human-written initial prompt, a majority of mutations and crossovers performed in the first few iterations yield poorly performing candidates.Evaluating each of these candidates is computationally expensive, even though it can enhance solution diversity.</p>
<p>Our algorithm is effectively a "greedy" version of GA.We maintain a candidate set of size k and update the candidate pool immediately upon generating each new candidate, rather than waiting for the evaluation of the entire generation of offspring before updating the pool.As a result, our solution pool remains more up-to-date, leading to faster convergence compared to GA. Figure 3 further verifies this observation, demonstrating the slower initial growth of GA's learning curve.</p>
<p>History-guided search</p>
<p>Randomly rephrasing a sentence in a prompt is akin to conducting a random mutation within the space of semantically equivalent sentences, potentially requires a significant amount of trials to obtain a good solution.In this subsection, we demonstrate how the search history can be employed to guide the mutation in a more purposeful direction.</p>
<p>Guided mutation for a single sentence.At iteration T , we can denote the search history as {(s before t , s after t , r t )} T −1 t=1 , where s before t is the sentence before mutation, s after t is the sentence after mutation, and the reward r t indicates the change in the score (r t is positive if the mutation enhances the performance, and negative if it deteriorates performance).Assume s T is the current sentence that is selected for mutation, we can then use the history to guide the mutation towards more positive reward.For example, if we know rephrasing the sentence "So, it is true that Lesley is a great-grandfather of Leroy" to "Therefore Leslie is Leroy's greatgrandfather."can improve the performance, then for another sentence "So, it is true that everyone who is an ancestor of Dana is a stepbrother of Brian" we may want to rewrite it as "Therefore everyone that is an ancestor of Dana is Brian's stepbrother."</p>
<p>It has been shown that LLMs are able to learn from in-context examples (Zhang et al., 2021).Therefore, we propose to let the LLM-Mutator incontext learn from the history.This can be done by listing history in the prompt: if r t &gt; 0, we include an in-context example s before t ⇒ s after t , and if r t &lt; 0 we include s after t ⇒ s before t .The prompt for LLM-Evolver is shown in Figure 4.</p>
<p>Since the history can be long and it has been shown that putting too many in-context learning examples can be harmful for the performance, we only retrieve a small set of relevant history entries when rephrasing a sentence.Specifically, when evolving s T , we compute the similarity be- ) and ϕ is a sentence encoder (we use a T5 (Raffel et al., 2020) sentence encoder in our experiments).We then only select entries that pass a certain threshold to include as in-context examples.</p>
<p>Guided sampling for sentence selection.A challenge in automatic long prompt engineering lies in identifying which sentences in the prompt should be modified to improve performance.Our experiments demonstrate that altering only a few sentences in a long prompt can significantly enhance its effectiveness, but the changes need to be at right places.It is thus beneficial to bias the sampling distribution towards selecting more impactful sentences for modification.</p>
<p>We model sentence selection as a contextual bandit problem (Langford and Zhang, 2007;Li et al., 2010).In contextual bandit problem, at each iteration a learner is faced with m arms associante with feature vectors x 1 , . . ., x m , and the learner is trying to pull an arm at each round to optimize the overall reward.In our case, m arms correspond to the m sentences s (1) , . . ., s (m) in a prompt, and features can be obtained by ϕ(s (i) ) where ϕ is a text encoder.Utilizing feature information is crucial because if modifying a particular sentence can lead to performance improvement, it is likely that modifying similar sentences will also yield positive results.</p>
<p>We then adopt the Lin-UCB algorithm (Li et al., 2010), a widely recognized algorithm for contextual bandit problems, to guide sentence selection.Given the history {(s before t , s after t , r t )} T −1 t=1 , we compute a linear estimator of the underlying reward model
w * T = (H T H + λI) −1 H T r,(1)
where H is a (T − 1)-by-d matrix with each row being ϕ(s before t ), and r = [r 1 , . . ., r T −1 ] T .This is simply a solution of a ridge regression with feature matrix H and reward r.The following UCB rule is then used to select the sentence for mutation:
arg max i ϕ(s (i) ) T w * T + α ϕ(s (i) ) T A −1 ϕ(s (i) ),
(2) where A = H T H + λI.Since this Lin-UCB estimation may not be accurate, we let the algorithm has probability P to choose a arm by purely random and 1 − P to choose the sentence based on Eq. (2).</p>
<p>Experimental Results</p>
<p>In this section, we present empirical evidence demonstrating that the proposed long prompt tuning method can significantly enhance performance on the Big-Bench Hard (BBH) benchmark (Suzgun et al., 2022;Srivastava et al., 2022).We then conduct ablation studies to analyze the proposed algorithm's effectiveness and show qualitative results on how the proposed algorithm refines humanwritten prompts.</p>
<p>Experimental Settings</p>
<p>We consider the prompt developed in (Suzgun et al., 2022)  For the main experiments, we evaluated the performance on the text-bison model and utilized the instruction-tuned PaLM 2-L model as the LLM mutator.Both models belong to the PaLM 2-model family (Anil et al., 2023).Following the previous prompt tuning works, we set temperature as 0. However, to enhance the diversity of sentence mutation, we set the temperature of the LLM-Mutator to 0.5.</p>
<p>Main results</p>
<p>We compare the proposed algorithm with the following baselines:</p>
<p>• Original Prompt: Performance of the original human-designed prompt developed in (Suzgun et al., 2022), which also serves as the initialization for other tuning methods.Table 2: Main results on BBH benchmark.The search budget is limited to 50 evaluations over the training set for each method.We ran each experiment 3 times and report the mean and standard deviation.</p>
<p>• Genetic Algorithm: An implementation of the genetic algorithm for long prompt tuning.To facilitate a more comparable comparison with our method, we set the pool size to 4. At each step, 8 new candidates are generated by randomly mutating and performing crossover on the top candidates within the pool.</p>
<p>• Evolve "step-by-step": Several recent prompt tuning studies have explored the concept of evolving a prompt using a single sentence, such as the 'Let's think step by step' sentence employed in chain-of-thought prompts (Kojima et al., 2022).We adopted one of the state-of-the-art methods for single sentence optimization (Yang et al., 2023) to optimize all of the 'Let's think step by step' sentences within the chain-of-thought prompt.</p>
<p>• Greedy: A simple greedy approach mentioned in Section 3.2, where we only store the top performing candidate in the pool and generating new candidate on top of it.</p>
<p>For our method, we use the same hyper-parameters for all the experiments.We set the pool size (beam size) to 4 for all experiments.When applying guided mutation in Section 3.3, we use a T5 encoder to encode both the current entry and the history and normalize the embeddings to have a unit ℓ 2 norm.We retrieve only the top 4 history entries and require the ℓ 2 distance between the encoded history and the current sentence to be below 0.5.When applying the sentence selection algorithm described in Section 3.3, we set α = 0.05 in Eq. ( 2) and set P = 0.5.Note that those hyper-parameters are not well tuned.</p>
<p>For this experiment, we limited the computational budget to 50 evaluations on the training set and reported the training, test, and combined accuracy achieved by each method.All the experiments are run three times and we report the mean and standard deviation.The results are summarized in Table 2. Our proposed algorithm outperformed the other methods, demonstrating significant improvements in accuracy across all tasks.We also compute the improvements of accuracy on the whole dataset (including both training and testing) and report the accuracy gain in Figure 1.</p>
<p>Across all 8 tasks, our algorithm achieves an average of 8.2% gain in test accuracy and 9.2% gain in the accuracy of full evaluation set (train + test).Among these tasks, we achieve the largest performance gain (18.45%) on the logical deduction task, and the smallest gain (2.45%) on reasoning about colored objects (Color Reasoning).One potential reason of small performance gain on color reasoning is that the original prompt already achieves very high accuracy, so there is not much room for improvements.</p>
<p>Comparing the baselines, it becomes evident that evolving a single sentence (Evolve 'step-by-step') fails to achieve substantial improvements in long prompt tuning.This is mainly due to the fact that long prompt tuning typically involves over 20 sentences with detailed instructions and explanations, and thus naively modifying "Let's think step by step" is unlikely to lead to significant enhancements.Furthermore, the Genetic Algorithm and Greedy Algorithm each exhibit their own limitations, as discussed in Section 3.2, which are reflected in their inferior performance compared to our method.</p>
<p>Despite being able to significantly boost the performance, we also observe some degree of overfitting in our search procedure.Although the proposed beam search method can partially mitigate overfitting (see Section 3.2), we still observe higher training accuracy than testing in most cases.However, as the performance gain is substantial, the prompts found by the algorithm are still significantly better than the original one, even on the test set.</p>
<p>Ablation Study</p>
<p>We conduct an ablation study on the two techniques introduced in Section 3.3: the history-guided mutation and the contextual bandit algorithm for sentence selection.The results are presented in Table 3.We can observe that both components are contributing to the final performance of the model.</p>
<p>Qualitative results</p>
<p>One important benefit of automatic hard prompt engineering is that the resulting prompts remain interpretable by humans, allowing users to easily verify the modifications.We provide some successful examples found by our search in Table 4 and  Table 5.In each table, we show only the different parts of the initial prompt from BBH and the changes made by our method.</p>
<p>The first example demonstrated in Table 4 is for the logic deduction task on five objects.The initial prompt achieves 38.8% accuracy while the revised prompt found at iteration 48 improves the performance to 57.9% train accuracy and 54.0% test accuracy.We observed that most of the changes involve minor revisions to the original sentence without altering its meaning.These seemingly insignificant modifications can lead to substantial improvements in LLM accuracy, showcasing the important of automatic long prompt engineering.</p>
<p>The second prompt in Table 5 is for the Formal Fallacies task, designed to identify whether a given logic statement is valid.The initial prompt achieves 60% accuracy while the revised prompt found at iteration 91 improves the train accuracy to 92.1% and test accuracy to 83.1%.Similar to the previous case, most of the changes involve minor revisions.In this case, we also want to highlight a potential limitation of the proposed method that could be addressed in future work.In the sentence marked as *, the revised sentence is not semantically equivalent to the original one.The original and revised sentences represent different logical statements (original sentence: not A → B; new sentence: B → not A).However, the LLM appears incapable of detecting this subtle distinction, leading to an erroneous rephrasing.Although this change leads to improve training accuracy, it actually hurts test accuracy.Therefore, incorrect mutations can lead to overfitting, and developing strategies to mitigate these errors during the search process would be an interesting area of future research.</p>
<p>Conclusions, Limitations, and Future Work</p>
<p>We study the problem of automatic prompt engineering for long prompts, often comprising thousands of tokens.We investigate the performance of the standard greedy algorithm and genetic algorithm, and develop a novel search algorithm that yields superior performance.With only 50 evaluations on the training set, our method achieves an average absolute accuracy improvement of 9.2% across 8 tasks from Big Bench Hard.This demonstrates the significant potential benefits of automatic long prompt tuning and underscores the importance of this emerging area.</p>
<p>As the first paper focusing on automatic engineering of the entire long prompts, we identify several limitations of the current methods which can lead to interesting future research:</p>
<p>• The current algorithm relies on using another LLM to rephrase a sentence.As illustrated in Section 4.4, this LLM-mutator may introduce errors during sentence rewriting, particularly for intricate sentences (e.g., CoT in complicated logical deduction tasks).Therefore, improving the "correctness" of LLM-Mutator is an interesting future area of research, which has not been fully addressed in our work as well as other recent studies (Fernando et al., 2023;Guo et al., 2023).A: Let's think things through one step at a time.</p>
<p>(2) Eli finished below Amy: "(above) ?Amy ?Eli ?(below)".</p>
<p>(2) Amy was above Eli: "(above) ?Amy ?Eli ?(below)".Eli finished last.</p>
<p>Eli came in last place.Q: The following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a shelf, there are three books: a white book, a green book, and an orange book.The green book is to the right of the white book.The orange book is the rightmost.Q: Each paragraph below describes three objects arranged in a fixed order, and the statements are logically consistent in each paragraph.</p>
<p>There are three books on a shelf: a white, green, and orange book.The green book is to the right of the white book, and the orange book is in the far right position.</p>
<p>A: Let's think step by step.</p>
<p>A: Let's think through things one step at a time.The white book is the leftmost.</p>
<p>The white book is at the far left.A: Let's think step by step.</p>
<p>A: Let's think one step at a time.Table 4: An example that our method improves the performance on Logical Deduction Five Objects from 38.8% to 56%.might be beneficial to manipulate multiple sentences simultaneously during mutation or consolidate multiple sentences into a single one.An automated mechanism for carrying out this process would be an interesting direction for enhancing our method.</p>
<p>• Although our algorithm is able to find a good solution with less than 100 evaluations on the training set, the cost is still not negligible especially when tuning prompts using APIs with cost or rate limits.Employing early stopping techniques, where the evaluation of poorly performing candidates are terminated early, could potentially reduce the number of queries.</p>
<p>• Although automatic prompt engineering can achieve significant gain, the search space of hard prompt has limited representation power which hinders further performance improvements.It has been shown that soft prompt tuning has limited representation power (Wang et al., 2023b), and our search space is a small subset of soft prompts.Therefore, when provided with sufficient data, computational resources, and white-box access to the LLM, (parameter-efficient) fine-tuning may still achieve superior performance.</p>
<p>Figure 2 :
2
Figure 2: The vanilla LLM-Mutator used in our search.</p>
<p>Figure 3 :
3
Figure 3: Comparison between beam-search greedy, greedy, and genetic algorithm.This is the average results over three runs.Beam-search greedy outperforms both greedy algorithm and genetic algorithm.</p>
<p>Figure 4 :
4
Figure 4: The guided LLM-mutator used in our search.</p>
<p>for the BBH tasks, where prompts consist
DatasetNumber of WordsNumber of Mutable SentencesCausal Judgement67820Salient Translation74534Disambiguation67137Formal Fallacies74132Hyperbaton53919Dyck Language67922Color Reasoning43517Logical Five46125Table 1: Initial prompt statistics for each dataset.When calculating the "number of mutable sen-tences," we only consider sentences that can bemodified during the search phase.of two parts: Task Description and Demos. TheTask Description provides instructions describingthe task, while each demo includes the question,a chain-of-thoughts demonstration illustrating theproblem-solving process step-by-step, and the finalanswer. During prompt tuning, the format (e.g.,"Question. ", "Answer. ") are retained, while thetuning algorithm is allowed to modify any otherparts including the instruction part and the chain-of-thoughts part. Table 1 summarizes the statisticsof the datasets used in this study. The full promptsfor each data can be downloaded from https://github.com/suzgunmirac/BIG-Bench-Hard.Each task consists of 250 samples, randomlydivided into 50% for training and 50% for test-ing. Only training samples are utilized for prompttuning. All tasks are multi-class classification prob-lems, and we report accuracy on training data, testdata, and the combined dataset when comparingdifferent tuning methods.</p>
<p>Table 3 :
3
Ablation Study on the two components introduced in Section 3.3.
• Similar to any other training or tuning algo-rithms, automatic prompt engineering can suf-fer from overfitting to the training data, asdiscussed in Section 4. This overfitting can bepotentially alleviated by applying additionalregularization techniques, such as imposingsparsity constraints by reducing the numberof modified sentences. However, further re-search is required to identify the most ef-fective regularization strategies for automaticprompt engineering.• In the current implementation, we break downthe long prompt into individual sentences andmodify one sentence at a time. However, it
AcknowledgementsWe thank Ruochen Wang, Vineet Gupta, Kedar Dhamdhere, Daliang Li for valuable discussions and feedback.Original promp: 60% accuracy New prompt found at iteration 91: 76% accuracy (train 92.1%, test 83.1%)Distinguish deductively valid arguments from formal fallacies.Identify deductively valid arguments from formal fallacies.So, it is true that Lesley is a great-grandfather of Leroy.So it's true that Lesley is Leroy's great-grandfather.Whoever is not a great-grandfather of Clyde is a stepbrother of Brian: If X = NOT (great-grandfather(Clyde)), then X = stepbrother(Brian).* If someone is a stepbrother of Brian then they are not a great-grandfather of Clyde.Furthermore, by (1), we have if X = NOT (great-grandfather(Clyde)), then X = stepbrother(Brian).Additionally, according to (1) we have that if X = NOT (great-grandfather(Clyde)), then X = stepbrother(Brian).By the transitive relation rule in first-order logic, we then have: if X = ancestor(Dana), then X = stepbrother(Brian).Using the transitive relation rule in first-order logic, we have: if X = ancestor(Dana), then X = stepbrother(Brian).Let's see whether the Hypothesis can be deduced from the arguments (1) and (2) by logical reasoning?Let's see whether the Hypothesis is a logical consequence of the arguments (1) and (2)? So, from (1) and (2), we cannot necessarily deduce the Hypothesis.So, given (1) and (2), we are not always entitled to infer the Hypothesis.Table5: An example demonstrating how our method changes the original prompt.By conducting these changes the combined accuracy on Formal Fallacies can be improved from 60% to 76%.Interestingly, there's one line
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Advances in neural information processing systems. 33</p>
<p>Instructzero: Efficient instruction optimization for blackbox large language models. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, Tianyi Zhou, arXiv:2306.030822023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, Zhiting Hu, arXiv:2205.12548Rlprompt: Optimizing discrete text prompts with reinforcement learning. 2022arXiv preprint</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim Rocktäschel, arXiv:2309.167972023arXiv preprint</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang, arXiv:2309.08532Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. 2023arXiv preprint</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, Omer Samuel R Bowman, Levy, arXiv:2205.107822022arXiv preprint</p>
<p>How can we know what language models know? Transactions of the Association for Computational Linguistics. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 20208</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>The epochgreedy algorithm for contextual multi-armed bandits. Advances in neural information processing systems. John Langford, Tong Zhang, 200720</p>
<p>Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameterefficient prompt tuning. Brian Lester, arXiv:2104.08691arXiv preprint</p>
<p>A contextual-bandit approach to personalized news article recommendation. Lihong Li, Wei Chu, John Langford, Robert E Schapire, Proceedings of the 19th international conference on World wide web. the 19th international conference on World wide web2010</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 2021</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chainof-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023aarXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022aarXiv preprint</p>
<p>Universality and limitations of prompt tuning. Yihan Wang, Jatin Chauhan, Wei Wang, Cho-Jui Hsieh, arXiv:2305.187872023barXiv preprint</p>
<p>Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, Sanjiv Kumar, arXiv:2211.00635Preserving in-context learning ability in large language model finetuning. 2022barXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Gps: Genetic prompt search for efficient few-shot learning. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang, arXiv:2210.170412022arXiv preprint</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Differentiable prompt makes pre-trained language models better fewshot learners. Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2108.131612021arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. 2023arXiv preprint</p>
<p>Universal and transferable adversarial attacks on aligned language models. Andy Zou, Zifan Wang, Zico Kolter, Matt Fredrikson, arXiv:2307.150432023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
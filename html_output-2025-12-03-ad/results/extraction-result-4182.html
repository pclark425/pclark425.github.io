<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4182 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4182</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4182</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-264490903</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.17064v1.pdf" target="_blank">math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories</a></p>
                <p><strong>Paper Abstract:</strong> As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few. While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS. By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called \emph{math-PVS}, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4182.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4182.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>math-PVS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>math-PVS framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses document OCR and large language models to convert scientific publications (PDF → LaTeX) into formal PVS theories, enabling interactive theorem proving and discovery of formal relationships across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>math-PVS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An end-to-end framework that ingests scientific PDFs (using Nougat OCR), converts them to LaTeX, feeds the LaTeX (definitions, theorems, summaries) to an LLM (GPT-4/ChatGPT4) to auto-generate PVS theories, and then uses human-in-the-loop interactions with the PVS proof assistant (typechecker, grind tactic, interactive proving) to refine, check, and prove formalized statements. Includes an explicit abstraction prompting step to request LLMs produce simplified, self-contained abstractions of definitions to reduce omitted background details. Iterative loop: OCR → extract/summary → LLM-based abstraction → LLM-generated PVS code → human edits and PVS verification/interactive proving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / ChatGPT4 (for autoformalization); Nougat (for OCR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (formal/theoretical), symbolic dynamics, computer science (theorem proving/formal methods)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Demonstration on 1 paper (Delvenne et al. 2006); framework described as intended to process a large corpus (planned, unspecified size)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Formal mathematical relationships/theorems (topological and structural relationships, e.g., homeomorphisms and bijections between symbolic spaces and Cantor space); not focused on empirical quantitative/physical scaling laws</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Theorem reproduced and formalized: "Every effective symbolic space is effectively homeomorphic to an effective subset of the Cantor space" and related existence/ bijection lemmas mapping symbolic_space → Binary_Sequence (formalized as mapping_exists_theorem and supporting lemmas in PVS).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Pipeline-based extraction: OCR PDF → LaTeX with Nougat; extract/summarize definitions and theorems from LaTeX; prompt LLM (GPT-4/ChatGPT4) to produce abstractions and generate PVS theory code from the extracted LaTeX; manual editing and iterative interaction with PVS to typecheck and complete proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Validation via mechanized PVS environment: typechecking, interactive theorem proving (including the grind tactic), manual inspection and edits; proofs completed and checked in PVS with the assistance of human users.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative performance metrics reported (no accuracy/precision/recall/F1 or numerical success rates).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported numerically; qualitative statement: LLM-generated PVS theories were "very close" to syntactic and semantic correctness but contained errors requiring manual fixes (e.g., PVS syntax mistakes).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM reasoning shortcomings on mathematical/logical tasks; syntactic and minor semantic errors in auto-generated PVS code; omission of background lemmas and proof details commonly omitted in papers; substantial human-in-the-loop effort required for abstraction, simplification, editing, and proof completion; need for fine-tuning on PVS preludes and libraries to reduce errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No formal comparison to baselines reported. The paper mentions planned fine-tuning on PVS/NasaLib and potential integration with CoProver but provides no empirical baseline comparison to human-only or other automated methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4182.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4182.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nougat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nougat (Neural Optical Understanding for Academic Documents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Visual Transformer based OCR system that converts PDF scientific documents into LaTeX (or other markup) to make mathematical content machine-accessible for downstream NLP/LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nougat: Neural optical understanding for academic documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nougat (OCR component of math-PVS pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Visual Transformer OCR used to convert PDFs containing mathematics into LaTeX markup so that mathematical definitions, theorems and equations can be processed by LLMs; used as the first step in the math-PVS pipeline to produce machine-readable LaTeX from PDF inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Nougat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical/scientific literature processing (broad across STEM domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to process the demonstration paper; authors state plans to process a large corpus (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not an extractor of laws itself; enables downstream extraction by converting document images to machine-readable LaTeX (facilitates extraction of formal mathematical relationships/theorems).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>None provided in paper specifically as Nougat outputs; Nougat produced LaTeX of the Delvenne et al. paper which was used downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Optical character recognition and visual transformer model to map PDF content to LaTeX markup for downstream LLM ingestion.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described in detail for Nougat in this paper; Nougat output was used as input and presumed validated by downstream LLM and human edits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>OCR errors and LaTeX recovery errors are possible (paper notes need to simplify LaTeX for PVS parsing); downstream pipeline requires cleaning and human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct comparison to other OCR/markup conversion tools reported in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4182.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4182.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / ChatGPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 / ChatGPT4 (OpenAI large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based foundation model used in math-PVS to (a) abstract and summarize mathematical definitions/theorems and (b) generate PVS theory code from LaTeX-extracted content, with human-in-the-loop correction and PVS verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based autoformalization (GPT-4 / ChatGPT4)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper uses GPT-4 / ChatGPT4 to: (1) summarize and abstract extracted LaTeX definitions/theorems (explicit prompt requesting core abstraction without extraneous details), (2) generate PVS theory code implementing definitions (e.g., mappings, Cantor space, bijectivity), and (3) iterate with a human user to fix syntax and semantic issues; prompts included direct definitions and requests to output PVS code and abstractions; the workflow emphasizes human feedback loops and use of PVS to verify/complete proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / ChatGPT4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics/formal methods; applied to symbolic dynamics example from theoretical computer science and mathematics literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to one concrete research paper in evaluation (Delvenne et al. 2006); described as intended to scale to many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Formal mathematical theorems and structural relationships (existence of bijections, homeomorphisms between symbolic spaces and Cantor space); the focus is formal relationships/theorems rather than empirical numerical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Auto-generated PVS representations of definitions and theorems such as bijective/injective/surjective predicates, Cantor_Space definition, and mapping_exists_theorem: ∀ (ss: symbolic_space): ∃ (f: [symbolic_space → Binary_Sequence]): bijective?(f).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text-based parsing of LaTeX output from Nougat; prompting LLM to produce abstractions and directly emit PVS code implementing definitions and theorems; human edits and PVS verification complete the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>PVS typechecking and interactive theorem proving used to validate and complete formal proofs; human inspection and manual edits used to correct LLM output. The paper documents completing proofs in PVS after editing generated theories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>None quantitative; only qualitative statements about closeness to valid PVS code and need for fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified; qualitative: LLM outputs were close to syntactically/semantically correct but contained errors (e.g., small PVS syntax mistakes) requiring manual correction.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLMs lack robust logical and mathematical reasoning (cited); autoformalization omissions (background lemmas) are common; syntactic PVS errors; manual editing/abstraction required; plan to fine-tune LLMs on PVS prelude and NASA PVS libraries to reduce errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No empirical comparison to other autoformalization systems reported; mentions future work on fine-tuning and integration with CoProver.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4182.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4182.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoProver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoProver (recommender system for proof construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proof-construction recommender system mentioned as a potential future integration to automatically generate lemmas from extracted theories and assist in proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CoProver: A recommender system for proof construction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoProver (future integration mention)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a system that could be combined with math-PVS to automatically generate lemmas from the extracted theories and help automate proof generation; not used in the current demonstrations but cited as future work to reduce manual effort.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated theorem proving / formal methods</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Mentioned as future integration; not evaluated on a corpus here.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Assists in generating lemmas and constructing formal proofs (theorem-proving support rather than extracting empirical quantitative laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>None in this paper (future integration proposed).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Not implemented here; described as a complementary tool to generate lemmas from formalized theories.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Only mentioned; no limitations or empirical evaluation provided within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared within this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Nougat: Neural optical understanding for academic documents <em>(Rating: 2)</em></li>
                <li>Advancing mathematics by guiding human intuition with AI <em>(Rating: 2)</em></li>
                <li>CoProver: A recommender system for proof construction <em>(Rating: 2)</em></li>
                <li>Decidability and universality in symbolic dynamical systems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4182",
    "paper_id": "paper-264490903",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "math-PVS",
            "name_full": "math-PVS framework",
            "brief_description": "A pipeline that uses document OCR and large language models to convert scientific publications (PDF → LaTeX) into formal PVS theories, enabling interactive theorem proving and discovery of formal relationships across papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "math-PVS",
            "system_description": "An end-to-end framework that ingests scientific PDFs (using Nougat OCR), converts them to LaTeX, feeds the LaTeX (definitions, theorems, summaries) to an LLM (GPT-4/ChatGPT4) to auto-generate PVS theories, and then uses human-in-the-loop interactions with the PVS proof assistant (typechecker, grind tactic, interactive proving) to refine, check, and prove formalized statements. Includes an explicit abstraction prompting step to request LLMs produce simplified, self-contained abstractions of definitions to reduce omitted background details. Iterative loop: OCR → extract/summary → LLM-based abstraction → LLM-generated PVS code → human edits and PVS verification/interactive proving.",
            "model_name": "GPT-4 / ChatGPT4 (for autoformalization); Nougat (for OCR)",
            "model_size": null,
            "scientific_domain": "Mathematics (formal/theoretical), symbolic dynamics, computer science (theorem proving/formal methods)",
            "number_of_papers": "Demonstration on 1 paper (Delvenne et al. 2006); framework described as intended to process a large corpus (planned, unspecified size)",
            "law_type": "Formal mathematical relationships/theorems (topological and structural relationships, e.g., homeomorphisms and bijections between symbolic spaces and Cantor space); not focused on empirical quantitative/physical scaling laws",
            "law_examples": "Theorem reproduced and formalized: \"Every effective symbolic space is effectively homeomorphic to an effective subset of the Cantor space\" and related existence/ bijection lemmas mapping symbolic_space → Binary_Sequence (formalized as mapping_exists_theorem and supporting lemmas in PVS).",
            "extraction_method": "Pipeline-based extraction: OCR PDF → LaTeX with Nougat; extract/summarize definitions and theorems from LaTeX; prompt LLM (GPT-4/ChatGPT4) to produce abstractions and generate PVS theory code from the extracted LaTeX; manual editing and iterative interaction with PVS to typecheck and complete proofs.",
            "validation_approach": "Validation via mechanized PVS environment: typechecking, interactive theorem proving (including the grind tactic), manual inspection and edits; proofs completed and checked in PVS with the assistance of human users.",
            "performance_metrics": "No quantitative performance metrics reported (no accuracy/precision/recall/F1 or numerical success rates).",
            "success_rate": "Not reported numerically; qualitative statement: LLM-generated PVS theories were \"very close\" to syntactic and semantic correctness but contained errors requiring manual fixes (e.g., PVS syntax mistakes).",
            "challenges_limitations": "LLM reasoning shortcomings on mathematical/logical tasks; syntactic and minor semantic errors in auto-generated PVS code; omission of background lemmas and proof details commonly omitted in papers; substantial human-in-the-loop effort required for abstraction, simplification, editing, and proof completion; need for fine-tuning on PVS preludes and libraries to reduce errors.",
            "comparison_baseline": "No formal comparison to baselines reported. The paper mentions planned fine-tuning on PVS/NasaLib and potential integration with CoProver but provides no empirical baseline comparison to human-only or other automated methods.",
            "uuid": "e4182.0"
        },
        {
            "name_short": "Nougat",
            "name_full": "Nougat (Neural Optical Understanding for Academic Documents)",
            "brief_description": "A Visual Transformer based OCR system that converts PDF scientific documents into LaTeX (or other markup) to make mathematical content machine-accessible for downstream NLP/LLM processing.",
            "citation_title": "Nougat: Neural optical understanding for academic documents",
            "mention_or_use": "use",
            "system_name": "Nougat (OCR component of math-PVS pipeline)",
            "system_description": "Visual Transformer OCR used to convert PDFs containing mathematics into LaTeX markup so that mathematical definitions, theorems and equations can be processed by LLMs; used as the first step in the math-PVS pipeline to produce machine-readable LaTeX from PDF inputs.",
            "model_name": "Nougat",
            "model_size": null,
            "scientific_domain": "Mathematical/scientific literature processing (broad across STEM domains)",
            "number_of_papers": "Used to process the demonstration paper; authors state plans to process a large corpus (unspecified)",
            "law_type": "Not an extractor of laws itself; enables downstream extraction by converting document images to machine-readable LaTeX (facilitates extraction of formal mathematical relationships/theorems).",
            "law_examples": "None provided in paper specifically as Nougat outputs; Nougat produced LaTeX of the Delvenne et al. paper which was used downstream.",
            "extraction_method": "Optical character recognition and visual transformer model to map PDF content to LaTeX markup for downstream LLM ingestion.",
            "validation_approach": "Not described in detail for Nougat in this paper; Nougat output was used as input and presumed validated by downstream LLM and human edits.",
            "performance_metrics": "None reported in this paper.",
            "success_rate": "Not reported.",
            "challenges_limitations": "OCR errors and LaTeX recovery errors are possible (paper notes need to simplify LaTeX for PVS parsing); downstream pipeline requires cleaning and human intervention.",
            "comparison_baseline": "No direct comparison to other OCR/markup conversion tools reported in this paper.",
            "uuid": "e4182.1"
        },
        {
            "name_short": "GPT-4 / ChatGPT4",
            "name_full": "GPT-4 / ChatGPT4 (OpenAI large language model)",
            "brief_description": "A transformer-based foundation model used in math-PVS to (a) abstract and summarize mathematical definitions/theorems and (b) generate PVS theory code from LaTeX-extracted content, with human-in-the-loop correction and PVS verification.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM-based autoformalization (GPT-4 / ChatGPT4)",
            "system_description": "The paper uses GPT-4 / ChatGPT4 to: (1) summarize and abstract extracted LaTeX definitions/theorems (explicit prompt requesting core abstraction without extraneous details), (2) generate PVS theory code implementing definitions (e.g., mappings, Cantor space, bijectivity), and (3) iterate with a human user to fix syntax and semantic issues; prompts included direct definitions and requests to output PVS code and abstractions; the workflow emphasizes human feedback loops and use of PVS to verify/complete proofs.",
            "model_name": "GPT-4 / ChatGPT4",
            "model_size": null,
            "scientific_domain": "Mathematics/formal methods; applied to symbolic dynamics example from theoretical computer science and mathematics literature",
            "number_of_papers": "Applied to one concrete research paper in evaluation (Delvenne et al. 2006); described as intended to scale to many papers.",
            "law_type": "Formal mathematical theorems and structural relationships (existence of bijections, homeomorphisms between symbolic spaces and Cantor space); the focus is formal relationships/theorems rather than empirical numerical laws.",
            "law_examples": "Auto-generated PVS representations of definitions and theorems such as bijective/injective/surjective predicates, Cantor_Space definition, and mapping_exists_theorem: ∀ (ss: symbolic_space): ∃ (f: [symbolic_space → Binary_Sequence]): bijective?(f).",
            "extraction_method": "Text-based parsing of LaTeX output from Nougat; prompting LLM to produce abstractions and directly emit PVS code implementing definitions and theorems; human edits and PVS verification complete the pipeline.",
            "validation_approach": "PVS typechecking and interactive theorem proving used to validate and complete formal proofs; human inspection and manual edits used to correct LLM output. The paper documents completing proofs in PVS after editing generated theories.",
            "performance_metrics": "None quantitative; only qualitative statements about closeness to valid PVS code and need for fixes.",
            "success_rate": "Not quantified; qualitative: LLM outputs were close to syntactically/semantically correct but contained errors (e.g., small PVS syntax mistakes) requiring manual correction.",
            "challenges_limitations": "LLMs lack robust logical and mathematical reasoning (cited); autoformalization omissions (background lemmas) are common; syntactic PVS errors; manual editing/abstraction required; plan to fine-tune LLMs on PVS prelude and NASA PVS libraries to reduce errors.",
            "comparison_baseline": "No empirical comparison to other autoformalization systems reported; mentions future work on fine-tuning and integration with CoProver.",
            "uuid": "e4182.2"
        },
        {
            "name_short": "CoProver",
            "name_full": "CoProver (recommender system for proof construction)",
            "brief_description": "A proof-construction recommender system mentioned as a potential future integration to automatically generate lemmas from extracted theories and assist in proof generation.",
            "citation_title": "CoProver: A recommender system for proof construction",
            "mention_or_use": "mention",
            "system_name": "CoProver (future integration mention)",
            "system_description": "Referenced as a system that could be combined with math-PVS to automatically generate lemmas from the extracted theories and help automate proof generation; not used in the current demonstrations but cited as future work to reduce manual effort.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Automated theorem proving / formal methods",
            "number_of_papers": "Mentioned as future integration; not evaluated on a corpus here.",
            "law_type": "Assists in generating lemmas and constructing formal proofs (theorem-proving support rather than extracting empirical quantitative laws).",
            "law_examples": "None in this paper (future integration proposed).",
            "extraction_method": "Not implemented here; described as a complementary tool to generate lemmas from formalized theories.",
            "validation_approach": "Not described in this paper.",
            "performance_metrics": "Not reported in this paper.",
            "success_rate": "Not reported.",
            "challenges_limitations": "Only mentioned; no limitations or empirical evaluation provided within this paper.",
            "comparison_baseline": "Not compared within this work.",
            "uuid": "e4182.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Nougat: Neural optical understanding for academic documents",
            "rating": 2,
            "sanitized_title": "nougat_neural_optical_understanding_for_academic_documents"
        },
        {
            "paper_title": "Advancing mathematics by guiding human intuition with AI",
            "rating": 2,
            "sanitized_title": "advancing_mathematics_by_guiding_human_intuition_with_ai"
        },
        {
            "paper_title": "CoProver: A recommender system for proof construction",
            "rating": 2,
            "sanitized_title": "coprover_a_recommender_system_for_proof_construction"
        },
        {
            "paper_title": "Decidability and universality in symbolic dynamical systems",
            "rating": 2,
            "sanitized_title": "decidability_and_universality_in_symbolic_dynamical_systems"
        }
    ],
    "cost": 0.011678749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories
25 Oct 2023</p>
<p>Hassen Saidi hassen.saidi@sri.com 
Computer Science Laboratory
SRI International
94025Menlo ParkCA</p>
<p>Susmit Jha susmit.jha@sri.com 
Computer Science Laboratory
SRI International
94025Menlo ParkCA</p>
<p>Tuhin Sahai tuhin.sahai@sri.com 
Applied Sciences Laboratory
SRI International
94025Menlo ParkCA</p>
<p>math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories
25 Oct 20239DDDC51BCCCBD5830F152940AEF005A2arXiv:2310.17064v1[cs.AI]
As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process.Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models.This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers.Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS.By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called math-PVS, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.</p>
<p>Introduction</p>
<p>The growth (both in model size and size of training datasets) of transformer-based foundation models is quickly revolutionizing a large number of applications such as natural language processing, image generation and modification, and multi-modal inference.Recently, the National Academy of Sciences, Engineering, and Medicine released a workshop report [1] that emphasizes the importance of constructing and deploying AI agents that aid in mathematical endeavors.The advances in machine learning (ML) and AI methods has already aided in generating theorems for outstanding conjectures and discovering new connections between disparate concepts [2].</p>
<p>Note that using computers to assist in generating proofs does have historical precedence.Examples include the four color theorem [3] and the proof of the existence of a chaotic attractor in the Lorenz system using interval arithmetic [4].These efforts used computers to exhaustively search a large (combinatorial) search space and demonstrate the robustness of chaotic attractors respectively.Recent efforts, however, aim to elevate the use of computing platforms to that of an assistant or a peerparticipant in the creative process of mathematical research.Given that large language models (LLMs) do not directly perform well on problems in logic and mathematics [5], there are multiple efforts to integrate theorem proving systems with foundation models, see [1,6,7] and references therein.</p>
<p>The majority of these efforts are focused on formalization of mathematics from the ground up, wherein the LLM proof assistants are customized to prove theorems in particular domains.A number of these efforts use datasets such as MATH, GSM8K, and datasets in [8,9,10] that are limited to high school grade mathematics and mathematics Olympiad questions.Our goal is to explore the use of LLMs on formalizing mathematics used at the frontier of research, and to examine the possibility of developing an LLM-based research assistant that can review mathematical reasoning in a research paper.It is widely accepted that LLMs lack even basic reasoning capability and fail at even spatial reasoning [11].Consequently, we posit that any serious AI mathematics research assistant capable of reviewing research papers and assisting in proving theorems needs to be a combination of proof assistants such as prototype verification system (PVS) [12] and LLMs that can interface between the textual description in research papers and the formal specification in PVS.</p>
<p>PVS [12] is a mechanized environment for formal specification and verification and consists of a specification language based on classical, typed higher-order logic, a large number of predefined theories, a type checker, and an interactive theorem prover.PVS type system supports sub-typing and dependent types.Moreover, the PVS prelude offers foundational theories.NasaLib [13] is a large collection of PVS libraries maintained by NASA.Currently, NasaLib consists of 63 top-level libraries, containing about 38K proven formulas in total.They encode basic mathematical theories such as topology, graph theory, group theory, and probability theory to name a few, as well as useful theories for reasoning about critical systems.</p>
<p>Our framework (see Fig. 1) is able ingest the vast amounts of scientific literature in the form of PDF files.The ingested data is directly converted to PVS code by a LLM as follows: (a) we generate the mathematical data by processing a large corpus of scientific mathematical publications using Facebook's Nougat [14] framework to convert the information from PDF to L A T E Xformat, (b) the L A T E Xcode is converted to PVS theories using LLMs, and (c) the user interacts with the LLM and PVS systems and iterates with them to refine the theories and proofs with the aim of discovering new proofs, define conjectures, check existing proofs, and uncover hidden connections between disparate works.This integration of the PVS with LLMs opens up the possibility of automatically extracting PVS formalization of mathematical theorems and proofs in existing research papers into PVS which can be useful paper reviews and the aforementioned tasks.</p>
<p>A Prompting Framework for Generating PVS Theories</p>
<p>We demonstrate our framework by reproducing the core arguments for a proposition and its associated proof from an existing paper [15] that relates the evolution of Turing machines to symbolic dynamics.Note that symbolic dynamics are a standard tool in study of temporally evolving nonlinear systems.Additionally, the authors develop a generalized definition of computational universality that extends beyond Turing machines, counter machines, and tag systems.This work generalizes the notion of computational universality to cellular automata and subshifts.The authors define an effective symbolic space (ESS) and exploit it as described below.Definition 1.An effective symbolic space is a pair (X, P ), where X is a symbolic space and P : N → 2 X is an injective function whose range is the set of all clopen sets of X, such that the intersection and complementation of clopen sets are computable operations.This means that there exist computable functions f : N → N and g : N × N → N such that X \ P n = P f (n) and P n ∩ P m = P g(n,m) .</p>
<p>They show that cellular automata and Turing machines (ignoring the blank symbol) are examples of effective symbolic systems and then go on to prove universality theorems using this construct.We ingest the above paper (in PDF format) using the Nougat software [14] and feed the resulting L A T E Xcode to OpenAI's GPT-4 for automatic generation of PVS theories.In particular, we used the definition of effective symbolic spaces above along with the following definition to prove a simplified version of the first proposition in [15] (presented below for completeness).Definition 2. Let (X, P ) and (Y, Q) be two effective symbolic spaces.An effective continuous map is a continuous map h :
X → Y such that h −1 (Q n ) = P k(n) , for some computable map k : N → N. If h is
bijective then it is an effective homeomorphism, and (X, P ) is said to be effectively homeomorphic to (Y, Q).Theorem 1.Every effective symbolic space is effectively homeomorphic to an effective subset of the Cantor space.Every perfect effective symbolic space is effectively homeomorphic to the Cantor space.</p>
<p>We note that we use definitions of Cantor spaces [16], homeomorphisms [17], and other mathematical terminology from publicly available lectures notes and Wikipedia/Scholarpedia articles.In the case that these documents are available in PDF format, we again use the Nougat software to convert key portions to L A T E Xformat that is then converted to PVS theories using GPT-4.</p>
<p>We note that, as shown in Fig. 1, both the transformer-based PVS theory generation and its use in PVS presently requires human intervention.The user was also involved in simplifying the theories for PVS parsing, typechecking, and applying the grind tactic, a catch-all strategy that is frequently used to automatically complete a proof in PVS.Moreover, we note that these interactions were very similar to those that one may have with collaborators or advanced student researchers.</p>
<p>Using LLM for Abstraction</p>
<p>The human input required for our math-PVS framework as illustrated in Figure 1 is to use LLMs to simplify and construct abstractions of the original input that capture the underlying core concepts.Similar to the formalization of a complex system or algorithm in a theorem prover, the abstraction step is required here to map mathematical concepts described in the paper of interest, to concepts grounded in basic mathematical constructs such as set theory.While the formalization of the original input in PVS can be automated using LLMs, the resulting PVS theorems might require extensive proof details and background mathematical knowledge that are typically omitted in most mathematical papers.By using LLMs to automatically generate abstractions of the input, the resulting concepts can be mapped directly to PVS theories and subsequently proved by theorem proving techniques without the need for ingestion and processing of copious amounts of background mathematical knowledge and theories.As the corpus of mathematical knowledge grows in math-PVS, we expect this need for abstraction to reduce.Figure 2 illustrates the prompt used to abstract the mathematical definitions extracted from the paper [15].The prompt explicitly requests the LLM to generate an abstraction that captures the main concepts without the inclusion of specific details, thereby enabling the foundation model to generate a self-contained and consistent PVS code.</p>
<p>Evaluation</p>
<p>We now describe how our approach can be applied to review the paper [15] described in the previous section.The first step in our process is to use Meta's Nougat [14] to generate T E Xfrom the original PDF file.Nougat is a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language such as L A T E X.This makes the mathematical knowledge accessible and enables us to use LLMs for autoformalization into PVS.The next step is to extract and summarize the definitions and theorems from the recovered Latex.The core theory described in the paper under study [15] can be summarized as follows: Definition 3. Given two sets X and Y , a mapping h is a one-to-one mapping between its elements (h(x) = y) such that x ∈ X and y ∈ Y and the inverse mapping exists h −1 (y) = x.Definition 4. A Cantor space is the set of infinite numbers on the interval [0, 1] that can be represented as a sequence a 0 , a 1 , . . .such that a i ∈ {0, 1} Definition 5. A symbolic space is a set such that there exists a one-to-one map to some subset of integers greater than 0. Definition 6.An effective symbolic space is a pair (X, P ) where X is a symbolic space and P is a one-to-one mapping P : X → 2 X .Here 2 X is the power set of all possible combinations of subsets of X. Theorem.Every effective symbolic space has a homeomorphic mapping to some subset of a cantor space.</p>
<p>We note that the above definitions are not entirely complete, however, they capture the central concepts associated with the proof that enable one to create the mappings between the spaces.This is consistent with a mathematical collaborator that is participating in the theorem proving activity.We proceed by simply pasting definitions and theorems from the summary as prompts without giving any context.Figure 3 show how a definition of a bijective mapping is used as a prompt, and the output of ChatGPT4 when prompted to generate a PVS theory implementing the definition.For further details we point the reader to the appendix.</p>
<p>Limitations.The generated PVS theory is very close to a valid PVS theory that is syntactically correct (can be parsed) and semantically correct (can be typechecked).But the autoformalization by LLMs is not perfect and without errors.For instance, theory Mappings should be Mappings: THEORY.The importing statement should be after begin.Also, set_theory in PVS is simply sets defined in the prelude.These syntactic errors are easily fixed by providing ChatGPT4 with a few examples of PVS syntax.In future work, we plan to address these issues by fine tuning ChatGPT4 using the PVS prelude and the PVS NASA library [13] which have large quantities of PVS code.We will also explore combining this work with CoProver [18] to automatically generate lemmas from the extracted theories, and automate proofs generation.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we explored the transformative potential of integrating LLMs and proof assistants such as PVS.By automating the extraction and formalization of mathematical content from research papers, we provide a promising approach for academic review and knowledge discovery.This fusion of AI and formal mathematical methods paves the way for more robust, collaborative research practices for scientific discovery.We envision the following uses for our LLM-based math-PVS theorem proving framework.</p>
<ol>
<li>Automated reviewing of papers by journals and reviewers for theorems and their proofs.2. Discovery of connections between unrelated subareas of mathematics enabling the extensions of existing theories to new settings.3. Construction of an assistant that removes the mechanical portions of theorem proving, thereby enabling the mathematician researcher to focus on the creative aspects of the work and the "bigger picture".</li>
</ol>
<p>In future work, we believe that several steps in the process will be automated, reducing the workload on the scientific user of the math-PVS framework.Using the extensive PVS NASA library, we plan to fine-tune a foundation model using existing PVS theories to reduce the number of errors in PVS theory code generation.</p>
<p>Moreover, we plan to process a large corpus of mathematical literature using Nougat and generate extensive math-PVS theories that will reduce the workload of the user.</p>
<p>From this point, we ask ChatGPT4 to generate PVS theories formalizing the various definitions and theorems.The resulting PVS theories are manually edited to produce a single theory merging the various generated theories.In particular, (1) fix syntax errors in the generated PVS code, and (2) change the theorem from proving a surjective mapping to a bijective mapping, and (3) added two lemmas that allow us to prove the main result.</p>
<p>Figure 1 :
1
Figure 1: Overall workflow called math-PVS for data generation from scientific publications, LLMbased PVS code generation, and interactive theorem proving.</p>
<p>Figure 2 :
2
Figure 2: Prompting ChatGPT4 to provide abstraction of the original mathematical definitions and theorem</p>
<p>Figure 3 :
3
Figure 3: Prompting ChatGPT4 with a bijective mapping definition, and the resulting PVS theory</p>
<dl>
<dt>1 ℕ</dt>
<dt>1</dt>
<dd>type = [ → ] injective(ℎ: ): bool = ∀ ( 1 ,  2 : ): ℎ( 1 ) = ℎ( 2 ) implies  1 =  2 surjective(ℎ: ): bool = ∀ (: ): ∃ (: ): ℎ() =  bijective(ℎ: ): bool = injective(ℎ) and surjective(ℎ) H_INV: type = [ → ] inverse_mapping(ℎ: , h_inv: H_INV): bool = ∀ (: , : ): (ℎ() =  implies h_inv() = ) and (h_inv() =  implies ℎ() = ) bijective_if_has_inverse: lemma ∀ (ℎ: , h_inv: H_INV): injective(ℎ) and inverse_mapping(ℎ, h_inv) implies bijective(ℎ) ZeroOne: type = {: ℕ |  = 0 or  = 1} Binary_Sequence: type = [ℕ → sequence[ZeroOne]] Cantor_Space: type = setof[Binary_Sequence] CS: Cantor_Space symbolic_space_element: type symbolic_space: type = setof[symbolic_space_element] &gt;0 : type = {: ℕ |  &gt; 0} positive_integers: type = setof[ℕ &gt;0 ] injective_map: [symbolic_space → positive_integers] injective_map_axiom: axiom ∀ ( 1 ,  2 : symbolic_space):  1 ≠  2 implies injective_map( 1 ) ≠ injective_map( 2 ) PSet_symbolic_space: type = setof[setof[symbolic_space]] : [symbolic_space → PSet_symbolic_space] injective_P_axiom: axiom ∀ ( 1 ,  2 : symbolic_space):  1 ≠  2 implies ℙ( 1 ) ≠ ℙ( 2 ) effective_symbolic_space: type = [#ss_field: symbolic_space, pss_field: PSet_symbolic_space#] effective_symbolic_space_axiom(ss: symbolic_space): effective_symbolic_space = (#ss_field := ss, pss_field := ℙ(ss)#) subset_Cantor_Space: type = setof[Cantor_Space] injective(: [effective_symbolic_space → subset_Cantor_Space]): bool = ∀ ( 1 ,  2 : effective_symbolic_space):  1 ≠  2 implies ( 1 ) ≠ ( 2 ) bijective_map(: [symbolic_space → subset_Cantor_Space]): bool = ∀ ( 1 ,  2 : symbolic_space):  1 =  2 implies ( 1 ) = ( 2 ) MapLemma1: lemma ∃ (: [symbolic_space → ℕ]): bijective?()MapLemma2: lemma ∃ (: [ℕ → Binary_Sequence]): bijective?()mapping_exists_theorem: theorem ∀ (ss: symbolic_space): ∃ (: [symbolic_space → Binary_Sequence]): bijective?()end Mappings</dd>
</dl>
<p>A Appendix: ChatGPT4 Prompts, PVS theory, and PVS Proof This appendix contains specific details of the example outlined in the main body of the paper.In particular, we provide the following information:1.The ChatGPT4 prompts and responses for the example reported in the main body of the paper.2. The PVS theory generated by ChatGPT4 for the example in the main body of the paper.3. edited and final PVS theory for the example in the main body of the paper.4. The proof of the main theorem in PVS after it has been abstracted to the core concepts.The first set of interactions with ChatGPT4 are used to input the original definitions and proposition from the paper, we then use ChatGPT4 to summarize the information, abstract it, and generate a graph that maps all the concepts introduced in the definitions along with their dependencies.The second set of interactions with ChatGPT4 is used to generate PVS theories from the definitions and summaries and prove the primary abstracted version of the proposition of interest.
Artificial Intelligence to Assist Mathematical Reasoning: Proceedings of a Workshop. Washington, DCThe National Academies Press2023National Academies of Sciences, Engineering, and Medicine</p>
<p>Advancing mathematics by guiding human intuition with AI. Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, Nature. 60078872021</p>
<p>Every planar map is four colorable. I Kenneth, Wolfgang Appel, Haken, 1989American Mathematical Soc98</p>
<p>The Lorenz attractor exists. Warwick Tucker, Comptes Rendus de l'Académie des Sciences-Series I-Mathematics. 328121999</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, Edwards, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan Leike. 2023</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, 2023</p>
<p>Leandojo: Theorem proving with retrievalaugmented language models. Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, 2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, 2022</p>
<p>Responsible reasoning with large language models and the impact of proper nouns. Sumit Kumar, Jha , Rickard Ewetz, Alvaro Velasquez, Susmit Jha, Workshop on Trustworthy and Socially Responsible Machine Learning. NeurIPS2022. 2022</p>
<p>Pvs: A prototype verification system. Sam Owre, John M Rushby, Natarajan Shankar, International Conference on Automated Deduction. Springer1992</p>
<p>. Research Nasa Langley, Center, PVS NASA library. 2020. October 27, 2023</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, 2023</p>
<p>Decidability and universality in symbolic dynamical systems. Jean-Charles Delvenne, Petr Kurka, Vincent Blondel, Fundamenta Informaticae. 7442006</p>
<p>Principles of mathematical analysis. Walter Rudin, 1953McGraw Hill</p>
<p>Nonlinear oscillations, dynamical systems, and bifurcations of vector fields. John Guckenheimer, Philip Holmes, 2013Springer Science &amp; Business Media42</p>
<p>CoProver: A recommender system for proof construction. Eric Yeh, Briland Hitaj, Sam Owre, Maena Quemener, Natarajan Shankar, 2023</p>
<p>Verbose proof for mapping_exists_theorem. mapping_exists_theorem: {1} ∀ (ss: symbolic_space): ∃ (𝑓: [symbolic_space → Binary_Sequence]): bijective?(𝑓) mapping_exists_theorem</p>
<p>bijective?(𝑓) Rule? (lemma "MapLemma1. ∀ (ss: symbolic_space): ∃ (𝑓: [symbolic_space → Binary_Sequence</p>
<p>Applying MapLemma1 mapping_exists_theorem: {-1} ∃. symbolic_space → ℕ]): bijective?(𝑓</p>
<p>bijective?(𝑓) Rule? (lemma "MapLemma2. ∀ (ss: symbolic_space): ∃ (𝑓: [symbolic_space → Binary_Sequence</p>
<p>Applying MapLemma2 mapping_exists_theorem: {-1} ∃ (𝑓. ℕ → Binary_Sequence]): bijective?(𝑓) [-2] ∃ (𝑓: [symbolic_space → ℕ]): bijective?(𝑓</p>
<p>mapping_exists_theorem: [-1] ∃ (𝑓: [ℕ → Binary_Sequence]): bijective?(𝑓) [-2] ∃ (𝑓:bijective?(𝑓) Rule? (skeep) Skolemizing and keeping names of the universal formula in (+ -). bijective?(𝑓) Rule? (skeep) Skolemizing and keeping names of the universal formula in (+ -). mapping_exists_theorem: {-1} bijective?(𝑓) [-2] ∃ (𝑓: [symbolic_space → ℕ]): bijective?(𝑓</p>
<p>∃ ( 𝑓, bijective?(𝑓) Rule? (skeep) Skolemizing and keeping names of the universal formula in (+ -). symbolic_space → Binary_Sequence. 1 mapping_exists_theorem: [-1] bijective?(𝑓) {-2} bijective?(𝑓 ′</p>
<p>∃ ( 𝑓, bijective?(𝑓) Rule? (inst 1 "f o f!1") Instantiating the top quantifier in 1 with the terms: 𝑓 ∘ 𝑓 ′. symbolic_space → Binary_Sequence. mapping_exists_theorem: [-1] bijective?(𝑓) [-2] bijective?(𝑓 ′ ) {1} bijective?(𝑓 ∘ 𝑓 ′ ) Rule? (assert</p>
<p>Simplifying, rewriting, and recording with decision procedures. mapping_exists_theorem: [-1] bijective?(𝑓) [-2] bijective?(𝑓 ′</p>
<p>𝑓 ∘ 𝑓 ′ ). </p>
<p>Using lemma composition_bijective. Rule? (use "composition_bijective[symbolic_space,nat,Binary_Sequence. symbolic_space, ℕ, Binary_Sequence], This completes the proof of mapping_exists_theorem</p>            </div>
        </div>

    </div>
</body>
</html>
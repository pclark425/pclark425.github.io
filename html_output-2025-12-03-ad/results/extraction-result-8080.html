<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8080 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8080</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8080</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-5f018ce52d471fc3b3e5f17f54ea49502a85f7f3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f018ce52d471fc3b3e5f17f54ea49502a85f7f3" target="_blank">Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</a></p>
                <p><strong>Paper Venue:</strong> NLLP</p>
                <p><strong>Paper TL;DR:</strong> Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation, and shows that conventional LLM-based evaluation methods can be better aligned with Lawyers’ evaluations, by combining with Eval-RAG.</p>
                <p><strong>Paper Abstract:</strong> While large language models (LLMs) have demonstrated significant capabilities in text generation, their utilization in areas requiring domain-specific expertise, such as law, must be approached cautiously. This caution is warranted due to the inherent challenges associated with LLM-generated texts, including the potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the validity of generated texts based on the related document that are collected by the retriever. In other words, Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation. Our experimental results on Korean Legal Question-Answering (QA) tasks show that conventional LLM-based evaluation methods can be better aligned with Lawyers’ evaluations, by combining with Eval-RAG. In addition, our qualitative analysis show that Eval-RAG successfully finds the factual errors in LLM-generated texts, while existing evaluation methods cannot.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8080.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8080.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FairEval evaluator using GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the FairEval LLM-based evaluation method with GPT-3.5 as the evaluating model, applied to score Korean legal QA answers and compared against human lawyer judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are not fair evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100) drawn from Law&Good client consultations</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-3.5 used as the model invoked by the FairEval evaluation protocol (no further size/training details provided in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts) providing human grading scores (benchmark), scores scaled 1–10</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.0432</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced sensitivity to factual errors/hallucinations; lack of domain-specific training data; inability to reflect recent or specific legal documents</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Very low correlation with human (lawyer) scores for GPT-3.5; adding retrieval (Eval-RAG) improved correlation but baseline FairEval (GPT-3.5) aligns poorly with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Outperforms classical lexical metrics (claimed in related work); can be improved via retrieval augmentation to better align with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>FairEval prompts used with GPT-3.5; scores for each answer scaled 1–10; single retrieved document allowed (due to prompt length limits); correlations computed between evaluator scores and human grading using Pearson, Spearman, Kendall over 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8080.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval-RAG (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FairEval combined with Eval-RAG using GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FairEval evaluation prompts augmented with a single retrieved relevant document (Eval-RAG) while using GPT-3.5 as the evaluator, evaluated against human lawyer scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are not fair evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-3.5 used as evaluator; prompt extended to include retrieved document (Eval-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.1156</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>still limited sensitivity compared to expert humans though improved; limited by single retrieved document and prompt length</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Correlation with human judgments improves relative to FairEval without retrieval, indicating retrieval helps expose factual errors; overall agreement remains low for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Retrieval augmentation increases alignment with human experts compared to bare LLM-based evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>FairEval prompt augmented with one retrieved document produced by a retriever; scores 1–10; correlation metrics computed across 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8080.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FairEval evaluator using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the FairEval LLM-based evaluation method with GPT-4 as the evaluating model, compared against human lawyer grading on Korean legal QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are not fair evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used as the model invoked by FairEval (no additional model-size/training specifics provided in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.4972</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced sensitivity to factual errors unless provided relevant documents; potential domain knowledge gaps for specialized legal nuance without retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 FairEval shows moderate correlation with lawyers (substantial improvement over GPT-3.5); still can disagree with lawyers on factual/legal points unless evaluator is given retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Stronger alignment with human experts compared to GPT-3.5 FairEval; more reliable baseline LLM evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>FairEval prompts with GPT-4; scores 1–10; correlations computed (Pearson/Spearman/Kendall) between model and lawyer scores over 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8080.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval-RAG (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FairEval combined with Eval-RAG using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FairEval evaluation prompts augmented with a retrieved legal document (Eval-RAG) and evaluated using GPT-4, which yielded higher agreement with human lawyers than FairEval alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used as evaluator with prompt augmented by a single retrieved document (Eval-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5923</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>remaining limitations tied to retriever coverage (only one doc returned); evaluator still depends on retrieval quality</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Combining FairEval with retrieval substantially increases correlation with lawyer judgments (best reported alignment); Eval-RAG helps the LLM detect factual/legal errors that bare LLM evaluation missed.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When augmented with retrieval, an LLM judge (GPT-4) aligns more closely with domain experts while retaining scalability of automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>FairEval prompt extended to include one retrieved document; retriever returns closest (q,d) match; scores 1–10; Pearson/Spearman/Kendall correlation computed over 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8080.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatEval (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatEval evaluator using GPT-3.5 (multi-agent debate style evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the ChatEval multi-agent LLM-based evaluation method with GPT-3.5, used to rate legal QA answers and compared against human lawyer grading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chateval: Towards better llm-based evaluators through multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-3.5 used within the ChatEval multi-agent evaluation framework (no additional model specs provided)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.1091</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>poor alignment with domain experts; fails to detect factual/legal errors based solely on internal knowledge; limited domain sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Low correlation with human grading for GPT-3.5 ChatEval; adding retrieval did not consistently improve alignment for GPT-3.5 in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Multi-agent debate style may improve robustness in some settings (claimed by ChatEval prior work), but in this legal dataset benefits are limited for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ChatEval protocol with GPT-3.5; single retrieved document used in ChatEval-RAG variant; scores 1–10; correlations computed against lawyer scores over 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8080.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatEval-RAG (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatEval combined with Eval-RAG using GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatEval evaluation prompts augmented with a retrieved document and using GPT-3.5 as the evaluator; compared against human lawyer grading on Korean legal QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chateval: Towards better llm-based evaluators through multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-3.5 used within ChatEval prompts augmented with one retrieved document</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.0905</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>limited improvement or slight decrease in alignment for GPT-3.5 variant; sensitive to retrieval and prompt-length constraints</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>For GPT-3.5 ChatEval, adding retrieval did not reliably increase correlation with lawyers and in this experiment slightly decreased Pearson correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Potential for improved deliberation-style evaluation, but gains depend on base model capability and retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ChatEval multi-agent prompts with addition of one retrieved document for the -RAG variant; scores 1–10; single retrieved doc due to prompt length; correlations computed across 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8080.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatEval (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatEval evaluator using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the ChatEval multi-agent LLM-based evaluator with GPT-4, evaluated against lawyer grading on Korean legal QA; shows moderate agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chateval: Towards better llm-based evaluators through multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used within ChatEval evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.4467</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>may still misjudge factual/legal correctness without explicit retrieved evidence; limited domain-specific sensitivity in some cases</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 ChatEval achieves moderate correlation with lawyers, but can still disagree on factual legal points; diagnostic example shows ChatEval (GPT-4) sometimes favored a fluent but incorrect GPT answer unless given the retrieved legal text.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Better baseline alignment with human experts than GPT-3.5; multi-agent evaluation can leverage model capabilities to approach human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ChatEval with GPT-4; scores 1–10; correlations computed (Pearson/Spearman/Kendall) against lawyer scores for 100 queries; example comparisons provided illustrating failure modes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8080.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatEval-RAG (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatEval combined with Eval-RAG using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatEval evaluator with prompts augmented by a retrieved relevant legal document, using GPT-4 as the judge, which increases agreement with human lawyers relative to bare ChatEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chateval: Towards better llm-based evaluators through multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used as the evaluator with the ChatEval protocol; prompt augmented by a single retrieved document (Eval-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5103</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>residual dependence on retrieval quality; limited by single retrieved document and prompt length constraints; may still miss nuanced legal interpretation beyond retrieved text</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Adding retrieval yields better alignment with lawyer judgments and allows the LLM judge to correctly identify factual/legal errors (example: ChatEval-RAG (GPT-4) matched lawyer judgment identifying rag answer as correct and gpt answer as incorrect).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Combining powerful LLM evaluators with retrieval yields improved alignment with human domain experts while retaining automated scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ChatEval prompts augmented with one retrieved legal document; single-document retrieval due to prompt limits; outputs compared to human lawyer scoring (1–10); correlations computed across 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8080.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8080.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation with Retrieval-Augmented Generation (Eval-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The retrieval-augmented evaluation method proposed in this paper that appends a retrieved relevant document to LLM-based evaluator prompts to improve detection of factual/legal errors and alignment with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Legal question-answering evaluation (scoring answers)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Divorce-related Korean legal QA queries (n=100); retriever built from 287 QA pairs, 84 legal provisions, 240 cases</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5 and GPT-4 (used in combination with existing evaluators such as FairEval and ChatEval)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Eval-RAG is an augmentation to existing LLM-based evaluators: the evaluator prompt includes the single retrieved document returned by a sentence-embedding-based retriever (text-embedding-ada-002 used for embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Lawyers (domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (Spearman rho and Kendall tau also reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>depends on retriever coverage and prompt length (only one document returned); retrieval quality limits ability to detect errors not present in indexed docs</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Eval-RAG improves LLM-evaluator alignment with human lawyer judgments and successfully surfaces factual/legal errors that bare LLM evaluators missed (illustrated by a case where ChatEval-RAG (GPT-4) aligned with lawyers while ChatEval without RAG did not).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>By providing relevant evidence, Eval-RAG helps LLM judges detect hallucinations and factual errors, increasing agreement with human experts while preserving automation and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Retriever built from three sources (QA pairs, provisions, cases); embedding model text-embedding-ada-002 and vector DB used; retriever returns the single most similar question-document pair (d); evaluator prompts include Q, A, and d (Table 2 style); scores scaled 1–10; correlations vs human grading measured via Pearson, Spearman, Kendall over 100 queries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chateval: Towards better llm-based evaluators through multi-agent debate <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as you desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8080",
    "paper_id": "paper-5f018ce52d471fc3b3e5f17f54ea49502a85f7f3",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "FairEval (GPT-3.5)",
            "name_full": "FairEval evaluator using GPT-3.5",
            "brief_description": "Application of the FairEval LLM-based evaluation method with GPT-3.5 as the evaluating model, applied to score Korean legal QA answers and compared against human lawyer judgments.",
            "citation_title": "Large language models are not fair evaluators",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100) drawn from Law&Good client consultations",
            "judge_model_name": "GPT-3.5",
            "judge_model_details": "GPT-3.5 used as the model invoked by the FairEval evaluation protocol (no further size/training details provided in the paper)",
            "human_evaluator_type": "Lawyers (domain experts) providing human grading scores (benchmark), scores scaled 1–10",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.0432,
            "reported_loss_aspects": "reduced sensitivity to factual errors/hallucinations; lack of domain-specific training data; inability to reflect recent or specific legal documents",
            "qualitative_findings": "Very low correlation with human (lawyer) scores for GPT-3.5; adding retrieval (Eval-RAG) improved correlation but baseline FairEval (GPT-3.5) aligns poorly with human judgments.",
            "advantages_of_llm_judge": "Outperforms classical lexical metrics (claimed in related work); can be improved via retrieval augmentation to better align with human experts.",
            "experimental_setting": "FairEval prompts used with GPT-3.5; scores for each answer scaled 1–10; single retrieved document allowed (due to prompt length limits); correlations computed between evaluator scores and human grading using Pearson, Spearman, Kendall over 100 queries.",
            "uuid": "e8080.0"
        },
        {
            "name_short": "FairEval-RAG (GPT-3.5)",
            "name_full": "FairEval combined with Eval-RAG using GPT-3.5",
            "brief_description": "FairEval evaluation prompts augmented with a single retrieved relevant document (Eval-RAG) while using GPT-3.5 as the evaluator, evaluated against human lawyer scores.",
            "citation_title": "Large language models are not fair evaluators",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-3.5",
            "judge_model_details": "GPT-3.5 used as evaluator; prompt extended to include retrieved document (Eval-RAG)",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.1156,
            "reported_loss_aspects": "still limited sensitivity compared to expert humans though improved; limited by single retrieved document and prompt length",
            "qualitative_findings": "Correlation with human judgments improves relative to FairEval without retrieval, indicating retrieval helps expose factual errors; overall agreement remains low for GPT-3.5.",
            "advantages_of_llm_judge": "Retrieval augmentation increases alignment with human experts compared to bare LLM-based evaluator.",
            "experimental_setting": "FairEval prompt augmented with one retrieved document produced by a retriever; scores 1–10; correlation metrics computed across 100 queries.",
            "uuid": "e8080.1"
        },
        {
            "name_short": "FairEval (GPT-4)",
            "name_full": "FairEval evaluator using GPT-4",
            "brief_description": "Application of the FairEval LLM-based evaluation method with GPT-4 as the evaluating model, compared against human lawyer grading on Korean legal QA.",
            "citation_title": "Large language models are not fair evaluators",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT-4 used as the model invoked by FairEval (no additional model-size/training specifics provided in the paper)",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.4972,
            "reported_loss_aspects": "reduced sensitivity to factual errors unless provided relevant documents; potential domain knowledge gaps for specialized legal nuance without retrieval",
            "qualitative_findings": "GPT-4 FairEval shows moderate correlation with lawyers (substantial improvement over GPT-3.5); still can disagree with lawyers on factual/legal points unless evaluator is given retrieved evidence.",
            "advantages_of_llm_judge": "Stronger alignment with human experts compared to GPT-3.5 FairEval; more reliable baseline LLM evaluator.",
            "experimental_setting": "FairEval prompts with GPT-4; scores 1–10; correlations computed (Pearson/Spearman/Kendall) between model and lawyer scores over 100 queries.",
            "uuid": "e8080.2"
        },
        {
            "name_short": "FairEval-RAG (GPT-4)",
            "name_full": "FairEval combined with Eval-RAG using GPT-4",
            "brief_description": "FairEval evaluation prompts augmented with a retrieved legal document (Eval-RAG) and evaluated using GPT-4, which yielded higher agreement with human lawyers than FairEval alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT-4 used as evaluator with prompt augmented by a single retrieved document (Eval-RAG)",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.5923,
            "reported_loss_aspects": "remaining limitations tied to retriever coverage (only one doc returned); evaluator still depends on retrieval quality",
            "qualitative_findings": "Combining FairEval with retrieval substantially increases correlation with lawyer judgments (best reported alignment); Eval-RAG helps the LLM detect factual/legal errors that bare LLM evaluation missed.",
            "advantages_of_llm_judge": "When augmented with retrieval, an LLM judge (GPT-4) aligns more closely with domain experts while retaining scalability of automated evaluation.",
            "experimental_setting": "FairEval prompt extended to include one retrieved document; retriever returns closest (q,d) match; scores 1–10; Pearson/Spearman/Kendall correlation computed over 100 queries.",
            "uuid": "e8080.3"
        },
        {
            "name_short": "ChatEval (GPT-3.5)",
            "name_full": "ChatEval evaluator using GPT-3.5 (multi-agent debate style evaluation)",
            "brief_description": "Application of the ChatEval multi-agent LLM-based evaluation method with GPT-3.5, used to rate legal QA answers and compared against human lawyer grading.",
            "citation_title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-3.5",
            "judge_model_details": "GPT-3.5 used within the ChatEval multi-agent evaluation framework (no additional model specs provided)",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.1091,
            "reported_loss_aspects": "poor alignment with domain experts; fails to detect factual/legal errors based solely on internal knowledge; limited domain sensitivity",
            "qualitative_findings": "Low correlation with human grading for GPT-3.5 ChatEval; adding retrieval did not consistently improve alignment for GPT-3.5 in this dataset.",
            "advantages_of_llm_judge": "Multi-agent debate style may improve robustness in some settings (claimed by ChatEval prior work), but in this legal dataset benefits are limited for GPT-3.5.",
            "experimental_setting": "ChatEval protocol with GPT-3.5; single retrieved document used in ChatEval-RAG variant; scores 1–10; correlations computed against lawyer scores over 100 queries.",
            "uuid": "e8080.4"
        },
        {
            "name_short": "ChatEval-RAG (GPT-3.5)",
            "name_full": "ChatEval combined with Eval-RAG using GPT-3.5",
            "brief_description": "ChatEval evaluation prompts augmented with a retrieved document and using GPT-3.5 as the evaluator; compared against human lawyer grading on Korean legal QA.",
            "citation_title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-3.5",
            "judge_model_details": "GPT-3.5 used within ChatEval prompts augmented with one retrieved document",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.0905,
            "reported_loss_aspects": "limited improvement or slight decrease in alignment for GPT-3.5 variant; sensitive to retrieval and prompt-length constraints",
            "qualitative_findings": "For GPT-3.5 ChatEval, adding retrieval did not reliably increase correlation with lawyers and in this experiment slightly decreased Pearson correlation.",
            "advantages_of_llm_judge": "Potential for improved deliberation-style evaluation, but gains depend on base model capability and retrieval quality.",
            "experimental_setting": "ChatEval multi-agent prompts with addition of one retrieved document for the -RAG variant; scores 1–10; single retrieved doc due to prompt length; correlations computed across 100 queries.",
            "uuid": "e8080.5"
        },
        {
            "name_short": "ChatEval (GPT-4)",
            "name_full": "ChatEval evaluator using GPT-4",
            "brief_description": "Application of the ChatEval multi-agent LLM-based evaluator with GPT-4, evaluated against lawyer grading on Korean legal QA; shows moderate agreement with humans.",
            "citation_title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT-4 used within ChatEval evaluation framework",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.4467,
            "reported_loss_aspects": "may still misjudge factual/legal correctness without explicit retrieved evidence; limited domain-specific sensitivity in some cases",
            "qualitative_findings": "GPT-4 ChatEval achieves moderate correlation with lawyers, but can still disagree on factual legal points; diagnostic example shows ChatEval (GPT-4) sometimes favored a fluent but incorrect GPT answer unless given the retrieved legal text.",
            "advantages_of_llm_judge": "Better baseline alignment with human experts than GPT-3.5; multi-agent evaluation can leverage model capabilities to approach human judgments.",
            "experimental_setting": "ChatEval with GPT-4; scores 1–10; correlations computed (Pearson/Spearman/Kendall) against lawyer scores for 100 queries; example comparisons provided illustrating failure modes.",
            "uuid": "e8080.6"
        },
        {
            "name_short": "ChatEval-RAG (GPT-4)",
            "name_full": "ChatEval combined with Eval-RAG using GPT-4",
            "brief_description": "ChatEval evaluator with prompts augmented by a retrieved relevant legal document, using GPT-4 as the judge, which increases agreement with human lawyers relative to bare ChatEval.",
            "citation_title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT-4 used as the evaluator with the ChatEval protocol; prompt augmented by a single retrieved document (Eval-RAG)",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": 0.5103,
            "reported_loss_aspects": "residual dependence on retrieval quality; limited by single retrieved document and prompt length constraints; may still miss nuanced legal interpretation beyond retrieved text",
            "qualitative_findings": "Adding retrieval yields better alignment with lawyer judgments and allows the LLM judge to correctly identify factual/legal errors (example: ChatEval-RAG (GPT-4) matched lawyer judgment identifying rag answer as correct and gpt answer as incorrect).",
            "advantages_of_llm_judge": "Combining powerful LLM evaluators with retrieval yields improved alignment with human domain experts while retaining automated scalability.",
            "experimental_setting": "ChatEval prompts augmented with one retrieved legal document; single-document retrieval due to prompt limits; outputs compared to human lawyer scoring (1–10); correlations computed across 100 queries.",
            "uuid": "e8080.7"
        },
        {
            "name_short": "Eval-RAG",
            "name_full": "Evaluation with Retrieval-Augmented Generation (Eval-RAG)",
            "brief_description": "The retrieval-augmented evaluation method proposed in this paper that appends a retrieved relevant document to LLM-based evaluator prompts to improve detection of factual/legal errors and alignment with human experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
            "evaluation_task": "Legal question-answering evaluation (scoring answers)",
            "dataset_name": "Divorce-related Korean legal QA queries (n=100); retriever built from 287 QA pairs, 84 legal provisions, 240 cases",
            "judge_model_name": "GPT-3.5 and GPT-4 (used in combination with existing evaluators such as FairEval and ChatEval)",
            "judge_model_details": "Eval-RAG is an augmentation to existing LLM-based evaluators: the evaluator prompt includes the single retrieved document returned by a sentence-embedding-based retriever (text-embedding-ada-002 used for embeddings).",
            "human_evaluator_type": "Lawyers (domain experts)",
            "agreement_metric": "Pearson r (Spearman rho and Kendall tau also reported)",
            "agreement_score": null,
            "reported_loss_aspects": "depends on retriever coverage and prompt length (only one document returned); retrieval quality limits ability to detect errors not present in indexed docs",
            "qualitative_findings": "Eval-RAG improves LLM-evaluator alignment with human lawyer judgments and successfully surfaces factual/legal errors that bare LLM evaluators missed (illustrated by a case where ChatEval-RAG (GPT-4) aligned with lawyers while ChatEval without RAG did not).",
            "advantages_of_llm_judge": "By providing relevant evidence, Eval-RAG helps LLM judges detect hallucinations and factual errors, increasing agreement with human experts while preserving automation and scalability.",
            "experimental_setting": "Retriever built from three sources (QA pairs, provisions, cases); embedding model text-embedding-ada-002 and vector DB used; retriever returns the single most similar question-document pair (d); evaluator prompts include Q, A, and d (Table 2 style); scores scaled 1–10; correlations vs human grading measured via Pearson, Spearman, Kendall over 100 queries.",
            "uuid": "e8080.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "GPTScore: Evaluate as you desire",
            "rating": 1
        }
    ],
    "cost": 0.0136725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA</h1>
<p>Cheol Ryu Seolhwa Lee Subeen Pang Chanyeol Choi<br>Linq Labs<br>{cheol.ryu, seolhwa.lee, subeen.pang, jacob.choi}@getlinq.com<br>Hojun Choi Myeonggee Min<br>Jy-yong Sohn*<br>Law\&amp;Good<br>Yonsei University<br>{hojun.choi,mgmin}@lawandgood.com jysohn1108@yonsei.ac.kr</p>
<h4>Abstract</h4>
<p>While large language models (LLMs) have demonstrated significant capabilities in text generation, their utilization in areas requiring domain-specific expertise, such as law, must be approached cautiously. This caution is warranted due to the inherent challenges associated with LLM-generated texts, including the potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the validity of generated texts based on the related document that are collected by the retriever. In other words, Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation. Our experimental results on Korean Legal Question-Answering (QA) tasks show that conventional LLM-based evaluation methods can be better aligned with Lawyers' evaluations, by combining with EvalRAG. In addition, our qualitative analysis show that Eval-RAG successfully finds the factual errors in LLM-generated texts, while existing evaluation methods cannot.</p>
<h2>1 Introduction</h2>
<p>Recent advances of large language models (LLMs) are remarkable, in various natural language generative tasks such as translation or summarization. However, LLMs are prone to generating hallucinated text (Ji et al., 2023), i.e., the texts generated by LLMs are sometimes having factual errors, which implies the unreliability of the LLM-generated text. Consequently, it is necessary to develop a proper system of evaluating LLMgenerated text, for using LLMs in real-world applications.</p>
<p>Various methods are proposed for evaluating natural language generation (NLG) (Papineni et al., 2002; Lin, 2004; Zhang et al., 2020). However, it is known that such classical evaluation methods</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The proposed Eval-RAG method for Legal QA task. Suppose an LLM generates answer $A$ for the query $Q$. Eval-RAG is a framework for evaluating the quality of the answer, using retriever $R$ and evaluator $E$. First, $R$ retrieves the document $d$ relevant to the query $Q$. Second, $E$ evaluates the score of $(Q, A)$ based on the contents of the retrieved document $d$.
have room for improvement compared with the human evaluation, especially for evaluating question answering (QA) tasks (Liu et al., 2023).</p>
<p>To address this issue of classical methods, recent approaches considered evaluating LLM-generated texts by another LLM (Chan et al., 2023; Liu et al., 2023; Wang et al., 2023b). Although the LLMbased evaluation methods outperforms classical methods, they are suffering from innate issues of LLMs, such as the lack of training data on specialized domains and the inability to reflect recent data.</p>
<p>Main Contributions In this paper, we propose Eval-RAG, a retrieval-based method evaluating LLM-generated texts. Eval-RAG first retrieves the document relevant to the generation task, and then evaluates the texts based on the relevant document. This method can be considered as applying retrieval-augmented generation (RAG) concept on the existing LLM-based evaluation systems. Our experimental results on Korean legal QA tasks show that Eval-RAG outperforms existing evaluation methods, in terms of higher correlation with the human evaluation. Our qualitative analysis also shows that Eval-RAG overcomes the limitation of existing LLM-based evaluation methods which cannot judge the hallucination of LLMs.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The pipeline of retriever.</p>
<h2>2 Method</h2>
<p>Here we introduce the proposed Eval-RAG framework depicted in Fig. 1, which has two main components: the retriever $R$ and the evaluator $E$. Once an LLM generates answer $A$ for the query $Q$, EvalRAG evaluates whether answer $A$ is proper or not for the query $Q$. First, the retriever $R$ finds legal documents $d$ that are relevant to the query $Q$. Second, the evaluator $E$ evaluates the validity of answer based on the related document $d$.</p>
<p>Retriever Fig. 2 illustrates how retriever $R$ retrieves the documents $d$ related with the query $Q$. This process contains three steps: data collection, sentence embedding, and query processing. In Step 1, we collect three different types of publicly available legal documents: (1) legal question-answer pairs, (2) legal provisions and (3) legal cases (precedents). Since our focus is given to QA tasks, we convert these documents into (question, document containing the answer) format, denoted by $(q, d)$, in the following manner. Given the legal questionanswer $(q, a)$ pairs, we extract related document $d$ from the answer $a$. For other documents $d$ (legal provisions and legal cases), we use LLMs to generate relevant questions $q$. Table 1 shows the examples of question-document pair $(q, d)$ for each data source, which are translated from the Korean using DeepL ${ }^{1}$. In Step 2, the question-document pairs $\left{\left(q_{i}, d_{i}\right)\right}<em i="i">{i=1}^{N}$ collected in step 1 are stored in a vector database (Johnson et al., 2019) by using a sentence embedding model, which converts a sentence $q$ into an $n$-dimensional vector. In Step 3 , given query $Q$ and the set of questions $\left{q</em>\right}<em i="i">{i=1}^{N}$ collected in step 2, the retriever $R$ first finds the question $q</em>$.}$ that is most similar to $Q$ (in the sentence embedding space) and returns the relevant document $d_{i</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Evaluator The quality of answer $A$ to a given query $Q$ is evaluated based on the relevant document $d$, as shown in Fig. 1. Recall that existing LLM-based evaluators (Chan et al., 2023; Liu et al., 2023; Wang et al., 2023b) ask LLMs for the evaluation, where the prompts include $Q$ and $A$, along with the evaluation criteria and methods for evaluation. The evaluator proposed by us is built on top of these methods, by adding the relevant document $d$ (delivered from retriever $R$ ) in the prompt. This allows $E$ to evaluate based on the relevant documents. Note that our method (evaluating LLMs based on related documents) is motivated by the RAG methods (which generates texts with the aid of retrieved documents), and thus we call our evaluation method as Eval-RAG. Table 2 provides an example of how an associated document $d$ is added to the prompt of the existing method called FairEval (Wang et al., 2023b). It is worth noting that Eval-RAG can be combined with any existing LLM-based evaluation methods.</p>
<h2>3 Experiments</h2>
<p>We tested the performances of Eval-RAG and baselines for Korean legal QA tasks, specifically focused on the questions related to divorce.</p>
<p>Experimental Setup The divorce-related QA pairs used for the retriever are collected from Korea Legal Aid Corporation ${ }^{2}$, while the legal provisions and cases mentioned in the QA pairs are gathered from Korea Legislation Research Institute ${ }^{3}$. The number of QA pairs, provisions, and cases used in the experiment were 287, 84, and 240, respectively. we used GPT-4 to generate relevant questions for each provision and case. The query set $(n=100)$ used in the experiment was selected from actual client consultation cases at the law firm Law\&amp;Good ${ }^{4}$. For each query, we prepared three types of answers: an actual answer from a legal professional (Lawyer), an answer generated by assigning the role of a lawyer to ChatGPT (gpt), and an answer generated with the inclusion of relevant documents (rag). We used OpenAI's 'text-embedding-ada-002' sentence embedding model to store questions in vector database.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>q: Does divorce evidence have an expiration date?
a: Yes, there is an expiration date for divorce evidence. Article 841 of the Civil Act of Republic of Korea stipulates that divorce evidence is valid for six months from the date the spouse becomes aware of the evidence and two years from the date of the grounds for filing.
d: Article 841 of the Civil Act (Extinction of Right to Apply for Divorce due to Unchastity) The spouse may not file for a divorce when the spouse has given a prior consent or an ex post facto forgiveness, or when six months have passed since the date the spouse becomes aware of the event or two years have passed since the date of the event.
q: I want to know what I can or should do with the items etc. that I gave to and received from the was-to-be-spouse as engagement gifts in case of a disengagement.
provision: Article 806 of the Civil Code (Termination of Engagement and Right to Claim Damages) Upon termination of an engagement, either party may claim compensation for damages from the negligent other party.
d: Article 806 of the Civil Act (Dissolution of Matrimonial Engagement and Claims for Damages) Upon the termination of an engagement, a party may claim compensation for damages from the negligent other party.
q: I would like to raise a claim for division of property on divorce upon the severance pay of the other party/spouse as well. Is this possible?
case: Case No. 2013MU2250 (Main Issues and Holdings) Whether the future retirement benefits are subject to the division of property on divorce when one of the divorcees is still employed at the time of divorce and has not yet retired(affirmative) and the scope of such division.
d: Case No. 2013MU2250 (Summary of Decision) Because it is required to work for a certain period of time to receive retirement benefits, as far as the cooperation of the spouse is recognized to have contributed to such work, the retirement benefits may be subject to a division of property.</p>
<p>Table 1: Examples of (question, relevant document) pair, denoted by $(q, d)$, for three types of data sources: (1) legal question-answer pair $(q, a),(2)$ legal provision, and (3) legal case.</p>
<p>You are a helpful and precise assistant for checking the quality of the answer. Please evaluate the answer based on stated relevant law of South Korea.
[Question][question]
[The Start of Assistant 1's Answer] [answer_1] [The End of Assistant 1's Answer]
[The Start of Assistant 2's Answer] [answer_2] [The End of Assistant 2's Answer]
[The Start of Relevant Law of South Korea][document][The End of Relevant Law of South Korea]</p>
<p>Table 2: The prompt of Eval-RAG based on FairEval (Wang et al., 2023b). Black sentences are originally designed by FairEval, while blue sentences are designed by Eval-RAG.</p>
<p>Methods \&amp; Performance Metrics We compare the performances of different methods: (1) the latest LLM-based evaluators, ChatEval (Chan et al., 2023) and FairEval (Wang et al., 2023b), (2) these latest evaluators combined with Eval-RAG, and (3) the human grading evaluated by the lawyers, where all scores are scaled with minimum score 1 and the maximum score 10 . Due to the limit of the available prompt length of the evaluator, the retriever returns only one relevant document of the question closest to the query. The human grading for the three types of answers for each query is considered as the benchmark, and we measured the correlation between this benchmark and each method, using Pearson, Spearman, and Kendall correlation coefficients.</p>
<p>Result Table 3 compares different evaluation methods, in terms of the correlation between the score of each method and the human grading. One can confirm that existing evaluators (FairEval and ChatEval) can be improved when combined with Eval-RAG, for both GPT-3.5 and GPT-4.</p>
<p>Table 4 provides qualitative comparison of evaluation methods, for a specific query $Q$. The table on the top shows query $Q$, the answer $A$ generated by GPT and RAG (denoted by $(g p t)$ and $(r a g)$ ), and the related document $d$. The bottom table compares the evaluation of $(g p t)$ and $(r a g)$ for different evaluation methods: Lawyer (human evaluation), ChatEval and ChatEval combined with Eval-RAG.</p>
<p>Here, $(g p t)$ and $(r a g)$ have opposing views on the divorce-related capital gains tax. Note that the document $d$ and the lawyer's evaluation indicate that $(r a g)$ is correct and $(g p t)$ is incorrect.</p>
<p>Table 3: Correlation between the score of each evaluation method and the human grading. FairEval-RAG means FairEval combined with Eval-RAG.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluator</th>
<th style="text-align: center;">Pearson</th>
<th style="text-align: center;">Spearman</th>
<th style="text-align: center;">Kendall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FairEval (GPT-3.5)</td>
<td style="text-align: center;">0.0432</td>
<td style="text-align: center;">0.0515</td>
<td style="text-align: center;">0.0457</td>
</tr>
<tr>
<td style="text-align: center;">FairEval-RAG (GPT-3.5)</td>
<td style="text-align: center;">0.1156</td>
<td style="text-align: center;">0.1264</td>
<td style="text-align: center;">0.1108</td>
</tr>
<tr>
<td style="text-align: center;">FairEval (GPT-4)</td>
<td style="text-align: center;">0.4972</td>
<td style="text-align: center;">0.4999</td>
<td style="text-align: center;">0.4299</td>
</tr>
<tr>
<td style="text-align: center;">FairEval-RAG (GPT-4)</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 2 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 4 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 9 1}$</td>
</tr>
<tr>
<td style="text-align: center;">ChatEval (GPT-3.5)</td>
<td style="text-align: center;">0.1091</td>
<td style="text-align: center;">0.0896</td>
<td style="text-align: center;">0.0768</td>
</tr>
<tr>
<td style="text-align: center;">ChatEval-RAG (GPT-3.5)</td>
<td style="text-align: center;">0.0905</td>
<td style="text-align: center;">0.0816</td>
<td style="text-align: center;">0.0735</td>
</tr>
<tr>
<td style="text-align: center;">ChatEval (GPT-4)</td>
<td style="text-align: center;">0.4467</td>
<td style="text-align: center;">0.4541</td>
<td style="text-align: center;">0.4115</td>
</tr>
<tr>
<td style="text-align: center;">ChatEval-RAG (GPT-4)</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 2 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 1 8}$</td>
</tr>
</tbody>
</table>
<p>However, ChatEval's evaluation, based on LLM's knowledge, fails to recognize that ( $g p t$ ) is incorrect. In contrast, Eval-RAG, referencing the necessary document $d$ for evaluation, assesses (rag) as the more appropriate answer. This observation is consistent across both the GPT-3.5 and GPT-4 models. More specifically, only the evaluation of ChatEval-RAG (GPT-4) accurately states that ( $g p t$ ) is incorrect based on document $d$.</p>
<h2>4 Related Work</h2>
<p>QA and LLM Question answering (QA) is a task of providing answers to a given question, which is explored in last decades (Rogers et al., 2023; Zhu et al., 2021; Bolotova et al., 2022). Recently, there are studies on QA for specific domains such as law (Ravichander et al., 2019; Wang et al., 2023a; Louis et al., 2023), and studies on long-form question answering (LFQA) which involves searching external documents relevant to a given query, and then generating paragraph-length answers (Krishna et al., 2021; Wang et al., 2022; Xu et al., 2023). The area of QA is greatly advanced using large language models (LLMs) (OpenAI, 2023). However, LLMs have several limitations, such as the generation of hallucinated text (Ji et al., 2023). One way of overcoming the limitations of LLMs is to us the Retrieval Augmented Generation (RAG) method, which integrates the retrieval module (which retrieves information from some data source) and the text generation module (which generates texts based on the retrieved information) in a unified framework (Lewis et al., 2020). RAG methods have recently demonstrated strong performance in QA (Mialon et al., 2023; Lazaridou et al., 2022; Khattab et al., 2023).</p>
<p>NLG Metrics Various types of metrics were developed to measure the quality of natural language generation (NLG). Bilingual Evaluation Under- study (BLEU) (Papineni et al., 2002) is a widely used metric for evaluating machine translation systems. It measures the similarity between the machine-generated output and the reference translation based on $n$-gram precision. Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004) is a family of metrics for evaluating text summarization systems. ROUGE measures the similarity between the machine-generated summary and the reference summary based on $n$-gram overlap and sentence-level overlap. BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are also used for QA evaluation, but are not suitable because they are designed as translations and summaries, respectively. BERTScore (Zhang et al., 2020) is a evaluation metric for similarity between two sentences, by using pre-trained BERT models. BERTScore is shown to outperform other metrics in QA tasks. However, BERTscore is not reliable when the texts contain domain-specific knowledge due to the limitations of BERT model (Chalkidis et al., 2020).</p>
<p>LLM evaluation Recently, evaluating generated texts using LLMs is proposed (Fu et al., 2023; Liu et al., 2023; Wang et al., 2023b; Chan et al., 2023). G-Eval (Liu et al., 2023) proposes probabilityweighting schemes to calibrate the evaluation score from LLM. FairEval (Wang et al., 2023b) proposes a new evaluation methodology, motivated by the observation that the evaluation results can vary depending on the order of inputs in the prompt. ChatEval (Chan et al., 2023) proposes a novel approach where multiple agents engage in a discussion to evaluate the generated text. Our paper is built on top of these LLM-based evaluations, and outperforms these baselines by using retrieval-based-evaluation.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed Eval-RAG, a new method of evaluating LLM-generated texts, with the aid of retrieved documents that are related with the target task. Our experiments on Korean Legal QA tasks show that existing LLM-based evaluations can be better aligned with ground-truth human evaluation, when combined with Eval-RAG.</p>
<h2>6 Ethical considerations</h2>
<p>The example we tested the performance might include sensitive case information of clients. However, it is essential to emphasize that this ethical concern is alleviated, as the samples in question have been withheld from public disclosure.</p>
<p>Q: We would like to know whether the transfer of real estate ownership from one party to the other through a division of property on divorce is subject to the capital gains tax.
(gpt): The transfer of real estate ownership from one party to the other in a divorce constitutes a transfer of property for valuable consideration. Capital gains tax is a tax incurred under the Income Tax Act when properties such as real estate, etc. are transferred. However, the transfer of real estate resulting from the division of property in divorce may be exempted from capital gains tax under Article 9 of the Income Tax Act.
(rag): According to the Supreme Court precedent Case No. 2002DU6422, the division of property constitutes a partition of the article jointly owned and the legal principles on a partition of the article jointly owned shall be applied. The partition of the article jointly owned is understood to be a change in the form of ownership by concentrating the co-owned share which was exercised in proportion to the co-owned share in the article jointly owned into an ownership of a specific part of the article and making it subsist only on that specific part of the article. For this reason, the division of property in a divorce is deemed to be a transfer of property for valuable consideration and thereby is not subject to capital gains tax.
d: Case No. 96NU14401 (Summary of Decision) A division of property in a divorce by agreement to liquidate the de facto marital property is substantially a partition of the article jointly owned in light of its legal nature, the object and scope of such division. Therefore, legal principles on a partition of the article jointly owned shall be apply mutatis mutandis, and the transfer of property by the result of such division is not a transfer of property for valuable consideration which is subject to capital gains tax.</p>
<p>Lawyer : The score of $(g p t)=4$, the score of $(r a g)=8$
ChatEval (GPT-3.5) : After carefully reviewing the responses of both AI assistants, I found that ( $g p t$ ) provided a more accurate and detailed answer. ( $g p t$ ) correctly stated that transferring ownership of a property during divorce proceedings is considered a transfer of assets and may be subject to transfer income tax. Moreover, ( $g p t$ ) provided the specific conditions that must be met for the transfer to be exempt from transfer income tax. In contrast, (rag) incorrectly stated that transferring ownership of a property during divorce proceedings is not considered a transfer of assets and therefore not subject to transfer income tax. Therefore, the score of $(g p t)=8$, The score of $(r a g)=5$.
ChatEval-RAG (GPT-3.5) : Based on the stated relevant law of South Korea, (rag)'s answer is more accurate and relevant to the question. The law clearly states that the division of property in divorce is not considered a transfer of ownership and therefore not subject to transfer income tax. (rag) correctly cites a relevant court case to support this. ( $g p t$ )'s answer is also helpful, but it does not provide as much detail on the specific law and court case. Therefore, the score of $(g p t)=7$, the score of $(r a g)=9$.
ChatEval (GPT-4) : ( $g p t$ ) provided a detailed and accurate response, explaining the conditions under which the transfer of property due to divorce can be exempt from transfer income tax. This is in line with the Transfer Income Tax Law in South Korea. (rag), while citing a specific case, seems to have misunderstood the nature of property division in a divorce and its tax implications. Therefore, the score of $(g p t)=9$, the score of $(r a g)=6$.
ChatEval-RAG (GPT-4) : Based on the stated relevant law of South Korea, Case 2002DU6422, the division of property during a divorce does not constitute a paid transfer of assets. (rag) correctly pointed this out and provided an accurate interpretation of the law. On the other hand, ( $g p t$ ) incorrectly stated that the division of property during a divorce constitutes a paid transfer of assets and could be subject to capital gains tax. However, ( $g p t$ ) did provide detailed information about the conditions for exemption from capital gains tax, which could be useful information for the user. The score of $(g p t)=6$, the score of $(r a g)=9$.</p>
<p>Table 4: (Top): An example query $Q$, the answers generated by GPT and RAG, and related document $d$. (Bottom): Evaluation of generated texts by Lawyer, ChatEval (Chan et al., 2023) and ChatEval-RAG, a short-hand notation for combining ChatEval with Eval-RAG. The evaluation with Eval-RAG aligns better with the evaluation of Lawyer.</p>
<h2>References</h2>
<p>Valeriia Bolotova, Vladislav Blinov, Falk Scholer, W. Bruce Croft, and Mark Sanderson. 2022. A nonfactoid question-answering taxonomy. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, page 1196-1207. Association for Computing Machinery.</p>
<p>Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: The muppets straight out of law school. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 28982904. Association for Computational Linguistics.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTScore: Evaluate as you desire.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12).</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2023. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems, volume 33, pages 9459-9474. Curran Associates, Inc.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using GPT-4 with better human alignment.</p>
<p>Antoine Louis, Gijs van Dijck, and Gerasimos Spanakis. 2023. Interpretable long-form legal question answering with retrieval-augmented large language models.</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey.</p>
<p>OpenAI. 2023. GPT-4 technical report.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, page 311-318. Association for Computational Linguistics.</p>
<p>Abhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton, and Norman Sadeh. 2019. Question answering for privacy policies: Combining computational and legal perspectives. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4947-4958. Association for Computational Linguistics.</p>
<p>Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2023. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. ACM Comput. Surv., 55(10).</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are not fair evaluators.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators.</p>
<p>Shufan Wang, Fangyuan Xu, Laure Thompson, Eunsol Choi, and Mohit Iyyer. 2022. Modeling exemplification in long-form question answering via retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2079-2092. Association for Computational Linguistics.</p>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT.</p>
<p>Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://deepl.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://www.klac.or.kr
${ }^{3}$ https://www.klri.re.kr
${ }^{4}$ https://www.lawandgood.com&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
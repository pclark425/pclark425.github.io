<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8014 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8014</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8014</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-5ac80c35214f8d49625cb1c1d899846a65ef0599</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5ac80c35214f8d49625cb1c1d899846a65ef0599" target="_blank">Exploring Scientific Hypothesis Generation with Mamba</a></p>
                <p><strong>Paper Venue:</strong> NLP4SCIENCE</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the use of Mamba for scientific hypothesis generation and preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation.</p>
                <p><strong>Paper Abstract:</strong> Generating scientifically grounded hypotheses is a challenging frontier task for generative AI models in science. The difficulty arises from the inherent subjectivity of the task and the extensive knowledge of prior work required to assess the validity of a generated hypothesis. Large Language Models (LLMs), trained on vast datasets from diverse sources, have shown a strong ability to utilize the knowledge embedded in their training data. Recent research has explored using transformer-based models for scientific hypothesis generation, leveraging their advanced capabilities. However, these models often require a significant number of parameters to manage Long sequences, which can be a limitation. State Space Models, such as Mamba, offer an alternative by effectively handling very Long sequences with fewer parameters than transformers. In this work, we investigate the use of Mamba for scientific hypothesis generation. Our preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation. We have made our code available here: https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-with-Mamba</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8014.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8014.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (Longest Common Subsequence variant of ROUGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic text-overlap metric measuring similarity between generated and reference text using longest common subsequence (LCS)-based precision/recall/F1; used here to evaluate how closely generated hypotheses match target sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ROUGE: A package for automatic evaluation of summaries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5, Mamba, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5 (60.5M, 223M, 738M), Mamba (130M, 370M, 790M), GPT-4 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / target sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-L automatic overlap scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes LCS-based overlap between generated hypothesis (system output) and gold target sentence, yielding precision/recall/F1; reported here as R-L (likely F1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-L score (reported as decimal fractions, e.g., 0.227)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>LCS-based F1-like score in range 0.0–1.0 (higher is better); reported per model/dataset subset.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciMON Challenging subset and SciMON Gold subset (ACL Anthology / S2ORC-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Challenging: T5-60.5M 0.178, T5-223M 0.197, T5-738M 0.223; Mamba-130M 0.176, Mamba-370M 0.219, Mamba-790M 0.227; GPT-4 FS 0.146. Gold subset: T5-60.5M 0.184, T5-223M 0.217, T5-738M 0.243; Mamba-130M 0.191, Mamba-370M 0.237, Mamba-790M 0.242; GPT-4 FS 0.143.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automatic metrics penalized GPT-4 (longer, freer-form outputs) relative to fine-tuned models; no direct human-generated baseline comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Automatic overlap metrics are shallow for open-ended hypothesis generation and can penalize longer/creative outputs that do not match reference wording.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8014.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore (SciBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore (using SciBERT encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic similarity metric that computes token-level similarity using contextual embeddings (here SciBERT) between generated and reference texts; used to evaluate semantic closeness of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertscore: Evaluating text generation with bert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5, Mamba, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5 (60.5M, 223M, 738M), Mamba (130M, 370M, 790M), GPT-4 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / target sentence generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore with SciBERT encoder</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes similarity between generated and reference tokens using contextual embeddings (SciBERT), summarizing via precision/recall/F1-like scores to assess semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore (reported as decimal fractions, e.g., 0.683)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Embedding-based similarity score in range 0.0–1.0 (higher is better); SciBERT used as encoder here.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciMON Challenging subset and SciMON Gold subset (ACL Anthology / S2ORC-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Challenging: T5-60.5M 0.514, T5-223M 0.604, T5-738M 0.663; Mamba-130M 0.523, Mamba-370M 0.628, Mamba-790M 0.683; GPT-4 FS 0.614. Gold subset: T5-60.5M 0.524, T5-223M 0.627, T5-738M 0.684; Mamba-130M 0.562, Mamba-370M 0.631, Mamba-790M 0.695; GPT-4 FS 0.627.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>BERTScore trends similar to ROUGE: larger models score higher; Mamba comparable to T5; GPT-4 lower on some automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding-based scores capture semantic similarity but still may not reflect true novelty, scientific validity, or usefulness of hypotheses.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8014.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge (Claude-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-judge evaluation using Claude-3.5 with reference-guided prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses an LLM (Claude-3.5) prompted in a reference-guided style to classify generated hypotheses as 'effective' or 'ineffective' based on relevance, novelty, scientific soundness, and clarity; output returned in JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5 (judge); evaluated models: T5, Mamba, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Claude-3.5 (judge) ; evaluated models: T5 (various), Mamba (various), GPT-4 FS</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis classification/evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-judge binary classification (effective / ineffective)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide judge LLM with context, seed, generated suggestion, and reference examples; judge assesses relevance, novelty, scientific soundness, and clarity and outputs rating ('effective' or 'ineffective') plus justification in JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary classification aggregated as accuracy (% of outputs rated 'effective' vs. gold or compared across models); per-instance JSON judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion (percentage) of model outputs rated 'effective' by Claude-3.5 (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciMON challenge set / gold subset (used on generated outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Prompt included five examples of both 'effective' and 'ineffective' hypotheses to guide Claude-3.5; structured prompt and reference-guided style to increase human-machine agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Claude-3.5 evaluation accuracy reported for GPT-4: 76% (model rankings consistent with human raters), general pattern: GPT-4 highest, Mamba comparable to T5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Claude-3.5 judgments showed similar patterns to human evaluations (consistency reported), but no direct comparison to human-generated hypotheses beyond agreement patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Potential for LLM self-enhancement bias mitigated by using Claude instead of GPT-4; still reliant on the judge LLM's internal priors and reference examples; binary labeling may be coarse.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8014.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert binary evaluation (effective/ineffective) on four criteria</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Five NLP experts independently rated generated hypotheses as 'effective' or 'ineffective' based on relevance, novelty, scientific validity, and clarity; raters were blind and outputs shuffled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5, Mamba, GPT-4 (outputs evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5 (various sizes), Mamba (various sizes), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert binary rating on four criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Experts read context and seed term and then label each generated hypothesis as 'effective' if it is relevant, novel, scientifically plausible, and clear; otherwise 'ineffective'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy / proportion of outputs rated 'effective' (reported as %), per-model aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of evaluated hypotheses labeled 'effective' by the human panel; binary labels aggregated across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>100 randomly sampled questions from the SciMON Challenge set</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Five NLP experts (graduate-level education) rated 100 instances, blind to system condition; structured questionnaire based on four criteria (relevance, novelty, scientific validity, clarity).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human-evaluation accuracy for GPT-4: 68%; overall pattern similar to Claude-3.5 judgments (GPT-4 highest). No full per-model breakdown provided in text beyond summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Humans and Claude-3.5 produced similar model ranking patterns; no direct comparison to hypotheses authored by human researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Binary effective/ineffective coarse-grained; small number of expert raters and limited sample (100 instances) may limit generalizability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8014.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Novelty Boosting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Novelty Boosting (SciMON novelty optimization workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-generation iterative procedure that compares generated ideas to a reference corpus and, when too similar, prompts the model to revise the idea to increase novelty until a similarity threshold is satisfied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciMON generation module (finetuned T5 or in-context GPT-3.5/GPT-4); also applied to Mamba in this work</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (T5 sizes; GPT-3.5/4 few-shot; Mamba sizes 130M–790M)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis generation and novelty enhancement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative novelty thresholding (similarity-based novelty enforcement)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Retrieve similar items from corpus, compute similarity to generated idea, and if similarity exceeds a threshold, instruct model to update/modify idea to increase novelty; repeat until novelty criterion met.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty measured implicitly via similarity scores (semantic-similarity) and a binary pass/fail w.r.t. threshold; no numeric novelty score reported.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Novelty operationalized as generated idea having similarity below a predetermined threshold to nearest corpus items (binary stop condition); exact similarity function/threshold not numerically specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciMON training corpus / reference corpus (ACL Anthology-derived retrieval dataset of ~59k papers, >374k sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Described as part of SciMON pipeline; no standalone quantitative results for novelty metric reported in this paper beyond qualitative use and claim that novelty iteration was applied before evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Not directly compared to human-generated novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Novelty determined by similarity threshold can be sensitive to retrieval quality and similarity metric; exact thresholds and similarity functions not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8014.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Inspiration Machines Optimized for Novelty (SciMON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework to generate literature-informed, novel research ideas using retrieval of inspirations (semantic neighbors, KG neighbors, citation neighbors), generation (finetuned T5 / in-context GPT), and iterative novelty boosting; used as experimental backbone in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciMON pipeline applied with T5, GPT-3.5/4, and Mamba as generation modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5 (various), GPT-3.5/4 (few-shot), Mamba (130M–790M)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>framework for hypothesis generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SciMON multi-stage evaluation (automatic metrics, LLM-as-judge, human evaluation) within pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Combine retrieval-grounded generation, in-context contrastive fine-tuning, iterative novelty boosting, and evaluate outputs using ROUGE/BERTScore, LLM-as-judge, and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Multiple: ROUGE-L, BERTScore, LLM-judge accuracy, human judge accuracy, long-text QA/NLI metrics (accuracy/F1) depending on subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See individual metric definitions (ROUGE, BERTScore, human accuracy, F1/accuracy for QA/NLI datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciMON dataset derived from S2ORC ACL Anthology (67,408 papers; training <2021, validation 2021, test 2022), retrieval dataset (~59k papers, >374k sentences), gold test set of 194 high-quality instances.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human raters: 5 NLP experts for a 100-instance sample; blind evaluation based on relevance, novelty, scientific validity, clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Framework used to produce the evaluations summarized in this paper (automatic scores and human/judge accuracy reported); Mamba models comparable to T5 on SciMON tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>No direct side-by-side with human-crafted hypotheses; evaluation focused on model-to-reference and human-judge agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset limited to ACL Anthology (NLP domain) reducing cross-domain generalizability; automatic metrics limited for open-ended generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8014.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON Gold Test Set</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON high-quality gold test set (manually curated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated high-quality test subset (194 instances) from the SciMON dataset created by removing cases solvable via surface-level cues and manually annotating to ensure strong relevance between seed and target terms; used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5, Mamba, GPT-4 evaluated on this gold subset</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5 (various), Mamba (various), GPT-4 FS</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark test set for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Gold test set evaluation (automatic metrics and human/LLM judging)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models generate target sentences from background + seed (with target removed); outputs compared to gold targets using ROUGE/BERTScore and judged by Claude-3.5/human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-L and BERTScore on gold subset; also human/LLM-judge labels aggregated to accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ROUGE-L and BERTScore (0–1), accuracy (%) for judge/human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciMON Gold subset (194 instances) drawn from ACL Anthology via S2ORC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>See ROUGE/BERTScore table: e.g., Mamba-790M R-L (GS) 0.242, BERT (GS) 0.695; GPT-4 FS R-L (GS) 0.143, BERT (GS) 0.627.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Gold set small (194), curated to remove easy cues; may still not capture full diversity of plausible scientific hypotheses.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8014.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S2ORC ACL Anthology subset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S2ORC-derived ACL Anthology corpus (SciMON dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dataset constructed from the Semantic Scholar Open Research Corpus consisting of 67,408 ACL Anthology papers (1952–2022) filtered to English with abstracts; split temporally into train/val/test for SciMON.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>S2ORC: The semantic scholar open research corpus</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used to train/evaluate SciMON generation modules (T5 finetuning, Mamba, GPT-4 in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>corpus for hypothesis generation and retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset-driven evaluation (temporal splits; retrieval-grounded generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Paper-level abstracts processed into (Background, Target) pairs; used to train/fine-tune models and to retrieve inspirations for generation; test set comprises 2022 papers to avoid contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used in conjunction with ROUGE/BERTScore and human/LLM judging; not a metric itself.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>S2ORC ACL Anthology subset (67,408 papers; retrieval dataset ~59k papers, >374k sentences; KG with ~197k nodes and 261k relations; citation network 87k titles)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset statistics provided (67,408 papers; retrieval set 59k papers; KG nodes 197k; relations 261k; citation network 87k).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Domain restricted to ACL Anthology (NLP); limits generalizability to other scientific domains and multimodal sciences.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8014.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-text benchmarks (ContractNLI / QuALITY / NarrativeQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ContractNLI, QuALITY, NarrativeQA long-text evaluation datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three long-input benchmarks used to assess models' abilities on question answering and inference over long documents: ContractNLI (document-level NLI for contracts), QuALITY (long-form QA), and NarrativeQA (reading comprehension over long narratives).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mamba, T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mamba (130M, 570M, 790M), T5 (Small, Base, Large)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / long-context understanding (proxy tasks for hypothesis-generation capability)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>capability evaluation (long-context QA / NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Accuracy (ContractNLI, QuALITY) and F1 (NarrativeQA) on long-input tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models answer questions or perform inference given very long input contexts; performance reported via accuracy or F1 depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) for ContractNLI and QuALITY; F1 score for NarrativeQA.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy: percentage correct (0–100%); F1: harmonic mean of precision and recall (reported as %).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ContractNLI, QuALITY, NarrativeQA (and scrolls variants referenced e.g., scrolls_narrativeqa, scrolls_quality, scrolls_contractnli)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table: Mamba-790M ContractNLI 11.86% acc, QuALITY 24.30% acc, NarrativeQA F1 13.81%; T5-Large ContractNLI 35.97% acc, QuALITY 24.98% acc, NarrativeQA 1.63 F1. Indicates Mamba better on longest NarrativeQA, T5 better on shorter inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Compared model families (Mamba vs T5) rather than to human performance; Mamba better on very long inputs, T5 better on shorter ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>These datasets are proxy tasks for long-context reasoning; performance on these benchmarks may not directly translate to quality of scientific hypothesis generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8014.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8014.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning benchmarks (MATHQA / MMLE / MMLUSR / GPQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General in-context learning tasks: MATHQA, MMLE, MMLUSR, GPQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot in-context learning benchmarks used to compare model ICL capabilities (math reasoning and QA) across model sizes; used here to compare Mamba and T5 in general ICL behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mamba, T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mamba (130M, 570M, 790M), T5-Small, T5-Base, T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / model generalization and in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>capability evaluation (few-shot ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Accuracy on several in-context learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide a few demonstrations and measure the model's accuracy on held-out examples for math and QA tasks to understand in-context learning strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) per dataset</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage correct (0–100%) on held-out in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATHQA, MMLE, MMLUSR, GPQA (general ICL suites used in related work and experiments B.1)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table: Mamba-790M MATHQA 25.56%, MMLE 23.74%, MMLUSR 23.38%, GPQA 25.00%; T5 variants showed comparable ranges (T5-Large MATHQA 22.51%, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Mamba performs on par with T5 on these ICL tasks; no human baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>ICL benchmark performance is task-specific and may not directly predict quality of open-ended hypothesis generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>ROUGE: A package for automatic evaluation of summaries <em>(Rating: 2)</em></li>
                <li>Bertscore: Evaluating text generation with bert. <em>(Rating: 2)</em></li>
                <li>Mamba: Linear-time sequence modeling with selective state spaces <em>(Rating: 2)</em></li>
                <li>S2ORC: The semantic scholar open research corpus <em>(Rating: 2)</em></li>
                <li>Contractnli: A dataset for document-level natural language inference for contracts <em>(Rating: 1)</em></li>
                <li>Quality: Question answering with long input texts, yes! <em>(Rating: 1)</em></li>
                <li>The narrativeqa reading comprehension challenge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8014",
    "paper_id": "paper-5ac80c35214f8d49625cb1c1d899846a65ef0599",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "ROUGE-L",
            "name_full": "ROUGE-L (Longest Common Subsequence variant of ROUGE)",
            "brief_description": "An automatic text-overlap metric measuring similarity between generated and reference text using longest common subsequence (LCS)-based precision/recall/F1; used here to evaluate how closely generated hypotheses match target sentences.",
            "citation_title": "ROUGE: A package for automatic evaluation of summaries",
            "mention_or_use": "use",
            "model_name": "T5, Mamba, GPT-4",
            "model_size": "T5 (60.5M, 223M, 738M), Mamba (130M, 370M, 790M), GPT-4 (few-shot)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "hypothesis / target sentence generation",
            "evaluation_method_name": "ROUGE-L automatic overlap scoring",
            "evaluation_method_description": "Computes LCS-based overlap between generated hypothesis (system output) and gold target sentence, yielding precision/recall/F1; reported here as R-L (likely F1).",
            "evaluation_metric": "ROUGE-L score (reported as decimal fractions, e.g., 0.227)",
            "metric_definition": "LCS-based F1-like score in range 0.0–1.0 (higher is better); reported per model/dataset subset.",
            "dataset_or_benchmark": "SciMON Challenging subset and SciMON Gold subset (ACL Anthology / S2ORC-derived)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Challenging: T5-60.5M 0.178, T5-223M 0.197, T5-738M 0.223; Mamba-130M 0.176, Mamba-370M 0.219, Mamba-790M 0.227; GPT-4 FS 0.146. Gold subset: T5-60.5M 0.184, T5-223M 0.217, T5-738M 0.243; Mamba-130M 0.191, Mamba-370M 0.237, Mamba-790M 0.242; GPT-4 FS 0.143.",
            "comparison_to_human_generated": false,
            "comparison_results": "Automatic metrics penalized GPT-4 (longer, freer-form outputs) relative to fine-tuned models; no direct human-generated baseline comparison provided.",
            "limitations_noted": "Automatic overlap metrics are shallow for open-ended hypothesis generation and can penalize longer/creative outputs that do not match reference wording.",
            "uuid": "e8014.0"
        },
        {
            "name_short": "BERTScore (SciBERT)",
            "name_full": "BERTScore (using SciBERT encoder)",
            "brief_description": "A semantic similarity metric that computes token-level similarity using contextual embeddings (here SciBERT) between generated and reference texts; used to evaluate semantic closeness of generated hypotheses.",
            "citation_title": "Bertscore: Evaluating text generation with bert.",
            "mention_or_use": "use",
            "model_name": "T5, Mamba, GPT-4",
            "model_size": "T5 (60.5M, 223M, 738M), Mamba (130M, 370M, 790M), GPT-4 (few-shot)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "hypothesis / target sentence generation",
            "evaluation_method_name": "BERTScore with SciBERT encoder",
            "evaluation_method_description": "Computes similarity between generated and reference tokens using contextual embeddings (SciBERT), summarizing via precision/recall/F1-like scores to assess semantic similarity.",
            "evaluation_metric": "BERTScore (reported as decimal fractions, e.g., 0.683)",
            "metric_definition": "Embedding-based similarity score in range 0.0–1.0 (higher is better); SciBERT used as encoder here.",
            "dataset_or_benchmark": "SciMON Challenging subset and SciMON Gold subset (ACL Anthology / S2ORC-derived)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Challenging: T5-60.5M 0.514, T5-223M 0.604, T5-738M 0.663; Mamba-130M 0.523, Mamba-370M 0.628, Mamba-790M 0.683; GPT-4 FS 0.614. Gold subset: T5-60.5M 0.524, T5-223M 0.627, T5-738M 0.684; Mamba-130M 0.562, Mamba-370M 0.631, Mamba-790M 0.695; GPT-4 FS 0.627.",
            "comparison_to_human_generated": false,
            "comparison_results": "BERTScore trends similar to ROUGE: larger models score higher; Mamba comparable to T5; GPT-4 lower on some automatic metrics.",
            "limitations_noted": "Embedding-based scores capture semantic similarity but still may not reflect true novelty, scientific validity, or usefulness of hypotheses.",
            "uuid": "e8014.1"
        },
        {
            "name_short": "LLM-as-judge (Claude-3.5)",
            "name_full": "LLM-as-judge evaluation using Claude-3.5 with reference-guided prompt",
            "brief_description": "Uses an LLM (Claude-3.5) prompted in a reference-guided style to classify generated hypotheses as 'effective' or 'ineffective' based on relevance, novelty, scientific soundness, and clarity; output returned in JSON.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5 (judge); evaluated models: T5, Mamba, GPT-4",
            "model_size": "Claude-3.5 (judge) ; evaluated models: T5 (various), Mamba (various), GPT-4 FS",
            "scientific_domain": "computer science / NLP",
            "theory_type": "hypothesis classification/evaluation",
            "evaluation_method_name": "LLM-as-judge binary classification (effective / ineffective)",
            "evaluation_method_description": "Provide judge LLM with context, seed, generated suggestion, and reference examples; judge assesses relevance, novelty, scientific soundness, and clarity and outputs rating ('effective' or 'ineffective') plus justification in JSON.",
            "evaluation_metric": "Binary classification aggregated as accuracy (% of outputs rated 'effective' vs. gold or compared across models); per-instance JSON judgments.",
            "metric_definition": "Proportion (percentage) of model outputs rated 'effective' by Claude-3.5 (0–100%).",
            "dataset_or_benchmark": "SciMON challenge set / gold subset (used on generated outputs)",
            "human_evaluation_details": "Prompt included five examples of both 'effective' and 'ineffective' hypotheses to guide Claude-3.5; structured prompt and reference-guided style to increase human-machine agreement.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Claude-3.5 evaluation accuracy reported for GPT-4: 76% (model rankings consistent with human raters), general pattern: GPT-4 highest, Mamba comparable to T5.",
            "comparison_to_human_generated": false,
            "comparison_results": "Claude-3.5 judgments showed similar patterns to human evaluations (consistency reported), but no direct comparison to human-generated hypotheses beyond agreement patterns.",
            "limitations_noted": "Potential for LLM self-enhancement bias mitigated by using Claude instead of GPT-4; still reliant on the judge LLM's internal priors and reference examples; binary labeling may be coarse.",
            "uuid": "e8014.2"
        },
        {
            "name_short": "Human Expert Evaluation",
            "name_full": "Human expert binary evaluation (effective/ineffective) on four criteria",
            "brief_description": "Five NLP experts independently rated generated hypotheses as 'effective' or 'ineffective' based on relevance, novelty, scientific validity, and clarity; raters were blind and outputs shuffled.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5, Mamba, GPT-4 (outputs evaluated)",
            "model_size": "T5 (various sizes), Mamba (various sizes), GPT-4",
            "scientific_domain": "computer science / NLP",
            "theory_type": "hypothesis evaluation",
            "evaluation_method_name": "Human expert binary rating on four criteria",
            "evaluation_method_description": "Experts read context and seed term and then label each generated hypothesis as 'effective' if it is relevant, novel, scientifically plausible, and clear; otherwise 'ineffective'.",
            "evaluation_metric": "Accuracy / proportion of outputs rated 'effective' (reported as %), per-model aggregated.",
            "metric_definition": "Percentage of evaluated hypotheses labeled 'effective' by the human panel; binary labels aggregated across samples.",
            "dataset_or_benchmark": "100 randomly sampled questions from the SciMON Challenge set",
            "human_evaluation_details": "Five NLP experts (graduate-level education) rated 100 instances, blind to system condition; structured questionnaire based on four criteria (relevance, novelty, scientific validity, clarity).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human-evaluation accuracy for GPT-4: 68%; overall pattern similar to Claude-3.5 judgments (GPT-4 highest). No full per-model breakdown provided in text beyond summary.",
            "comparison_to_human_generated": false,
            "comparison_results": "Humans and Claude-3.5 produced similar model ranking patterns; no direct comparison to hypotheses authored by human researchers.",
            "limitations_noted": "Binary effective/ineffective coarse-grained; small number of expert raters and limited sample (100 instances) may limit generalizability.",
            "uuid": "e8014.3"
        },
        {
            "name_short": "Iterative Novelty Boosting",
            "name_full": "Iterative Novelty Boosting (SciMON novelty optimization workflow)",
            "brief_description": "A post-generation iterative procedure that compares generated ideas to a reference corpus and, when too similar, prompts the model to revise the idea to increase novelty until a similarity threshold is satisfied.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "use",
            "model_name": "SciMON generation module (finetuned T5 or in-context GPT-3.5/GPT-4); also applied to Mamba in this work",
            "model_size": "various (T5 sizes; GPT-3.5/4 few-shot; Mamba sizes 130M–790M)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "hypothesis generation and novelty enhancement",
            "evaluation_method_name": "Iterative novelty thresholding (similarity-based novelty enforcement)",
            "evaluation_method_description": "Retrieve similar items from corpus, compute similarity to generated idea, and if similarity exceeds a threshold, instruct model to update/modify idea to increase novelty; repeat until novelty criterion met.",
            "evaluation_metric": "Novelty measured implicitly via similarity scores (semantic-similarity) and a binary pass/fail w.r.t. threshold; no numeric novelty score reported.",
            "metric_definition": "Novelty operationalized as generated idea having similarity below a predetermined threshold to nearest corpus items (binary stop condition); exact similarity function/threshold not numerically specified in paper.",
            "dataset_or_benchmark": "SciMON training corpus / reference corpus (ACL Anthology-derived retrieval dataset of ~59k papers, &gt;374k sentences)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Described as part of SciMON pipeline; no standalone quantitative results for novelty metric reported in this paper beyond qualitative use and claim that novelty iteration was applied before evaluations.",
            "comparison_to_human_generated": false,
            "comparison_results": "Not directly compared to human-generated novelty metrics.",
            "limitations_noted": "Novelty determined by similarity threshold can be sensitive to retrieval quality and similarity metric; exact thresholds and similarity functions not reported here.",
            "uuid": "e8014.4"
        },
        {
            "name_short": "SciMON framework",
            "name_full": "Scientific Inspiration Machines Optimized for Novelty (SciMON)",
            "brief_description": "A framework to generate literature-informed, novel research ideas using retrieval of inspirations (semantic neighbors, KG neighbors, citation neighbors), generation (finetuned T5 / in-context GPT), and iterative novelty boosting; used as experimental backbone in this paper.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "use",
            "model_name": "SciMON pipeline applied with T5, GPT-3.5/4, and Mamba as generation modules",
            "model_size": "T5 (various), GPT-3.5/4 (few-shot), Mamba (130M–790M)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "framework for hypothesis generation and evaluation",
            "evaluation_method_name": "SciMON multi-stage evaluation (automatic metrics, LLM-as-judge, human evaluation) within pipeline",
            "evaluation_method_description": "Combine retrieval-grounded generation, in-context contrastive fine-tuning, iterative novelty boosting, and evaluate outputs using ROUGE/BERTScore, LLM-as-judge, and human experts.",
            "evaluation_metric": "Multiple: ROUGE-L, BERTScore, LLM-judge accuracy, human judge accuracy, long-text QA/NLI metrics (accuracy/F1) depending on subtask.",
            "metric_definition": "See individual metric definitions (ROUGE, BERTScore, human accuracy, F1/accuracy for QA/NLI datasets).",
            "dataset_or_benchmark": "SciMON dataset derived from S2ORC ACL Anthology (67,408 papers; training &lt;2021, validation 2021, test 2022), retrieval dataset (~59k papers, &gt;374k sentences), gold test set of 194 high-quality instances.",
            "human_evaluation_details": "Human raters: 5 NLP experts for a 100-instance sample; blind evaluation based on relevance, novelty, scientific validity, clarity.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Framework used to produce the evaluations summarized in this paper (automatic scores and human/judge accuracy reported); Mamba models comparable to T5 on SciMON tasks.",
            "comparison_to_human_generated": false,
            "comparison_results": "No direct side-by-side with human-crafted hypotheses; evaluation focused on model-to-reference and human-judge agreement.",
            "limitations_noted": "Dataset limited to ACL Anthology (NLP domain) reducing cross-domain generalizability; automatic metrics limited for open-ended generation.",
            "uuid": "e8014.5"
        },
        {
            "name_short": "SciMON Gold Test Set",
            "name_full": "SciMON high-quality gold test set (manually curated)",
            "brief_description": "A curated high-quality test subset (194 instances) from the SciMON dataset created by removing cases solvable via surface-level cues and manually annotating to ensure strong relevance between seed and target terms; used for evaluation.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "use",
            "model_name": "T5, Mamba, GPT-4 evaluated on this gold subset",
            "model_size": "T5 (various), Mamba (various), GPT-4 FS",
            "scientific_domain": "computer science / NLP",
            "theory_type": "benchmark test set for hypothesis generation",
            "evaluation_method_name": "Gold test set evaluation (automatic metrics and human/LLM judging)",
            "evaluation_method_description": "Models generate target sentences from background + seed (with target removed); outputs compared to gold targets using ROUGE/BERTScore and judged by Claude-3.5/human raters.",
            "evaluation_metric": "ROUGE-L and BERTScore on gold subset; also human/LLM-judge labels aggregated to accuracy.",
            "metric_definition": "ROUGE-L and BERTScore (0–1), accuracy (%) for judge/human labels.",
            "dataset_or_benchmark": "SciMON Gold subset (194 instances) drawn from ACL Anthology via S2ORC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "See ROUGE/BERTScore table: e.g., Mamba-790M R-L (GS) 0.242, BERT (GS) 0.695; GPT-4 FS R-L (GS) 0.143, BERT (GS) 0.627.",
            "comparison_to_human_generated": false,
            "comparison_results": "Not applicable.",
            "limitations_noted": "Gold set small (194), curated to remove easy cues; may still not capture full diversity of plausible scientific hypotheses.",
            "uuid": "e8014.6"
        },
        {
            "name_short": "S2ORC ACL Anthology subset",
            "name_full": "S2ORC-derived ACL Anthology corpus (SciMON dataset)",
            "brief_description": "Dataset constructed from the Semantic Scholar Open Research Corpus consisting of 67,408 ACL Anthology papers (1952–2022) filtered to English with abstracts; split temporally into train/val/test for SciMON.",
            "citation_title": "S2ORC: The semantic scholar open research corpus",
            "mention_or_use": "use",
            "model_name": "Used to train/evaluate SciMON generation modules (T5 finetuning, Mamba, GPT-4 in-context)",
            "model_size": "N/A (dataset)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "corpus for hypothesis generation and retrieval",
            "evaluation_method_name": "Dataset-driven evaluation (temporal splits; retrieval-grounded generation)",
            "evaluation_method_description": "Paper-level abstracts processed into (Background, Target) pairs; used to train/fine-tune models and to retrieve inspirations for generation; test set comprises 2022 papers to avoid contamination.",
            "evaluation_metric": "Used in conjunction with ROUGE/BERTScore and human/LLM judging; not a metric itself.",
            "metric_definition": null,
            "dataset_or_benchmark": "S2ORC ACL Anthology subset (67,408 papers; retrieval dataset ~59k papers, &gt;374k sentences; KG with ~197k nodes and 261k relations; citation network 87k titles)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Dataset statistics provided (67,408 papers; retrieval set 59k papers; KG nodes 197k; relations 261k; citation network 87k).",
            "comparison_to_human_generated": false,
            "comparison_results": "Not applicable.",
            "limitations_noted": "Domain restricted to ACL Anthology (NLP); limits generalizability to other scientific domains and multimodal sciences.",
            "uuid": "e8014.7"
        },
        {
            "name_short": "Long-text benchmarks (ContractNLI / QuALITY / NarrativeQA)",
            "name_full": "ContractNLI, QuALITY, NarrativeQA long-text evaluation datasets",
            "brief_description": "Three long-input benchmarks used to assess models' abilities on question answering and inference over long documents: ContractNLI (document-level NLI for contracts), QuALITY (long-form QA), and NarrativeQA (reading comprehension over long narratives).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mamba, T5",
            "model_size": "Mamba (130M, 570M, 790M), T5 (Small, Base, Large)",
            "scientific_domain": "NLP / long-context understanding (proxy tasks for hypothesis-generation capability)",
            "theory_type": "capability evaluation (long-context QA / NLI)",
            "evaluation_method_name": "Accuracy (ContractNLI, QuALITY) and F1 (NarrativeQA) on long-input tasks",
            "evaluation_method_description": "Models answer questions or perform inference given very long input contexts; performance reported via accuracy or F1 depending on dataset.",
            "evaluation_metric": "Accuracy (%) for ContractNLI and QuALITY; F1 score for NarrativeQA.",
            "metric_definition": "Accuracy: percentage correct (0–100%); F1: harmonic mean of precision and recall (reported as %).",
            "dataset_or_benchmark": "ContractNLI, QuALITY, NarrativeQA (and scrolls variants referenced e.g., scrolls_narrativeqa, scrolls_quality, scrolls_contractnli)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table: Mamba-790M ContractNLI 11.86% acc, QuALITY 24.30% acc, NarrativeQA F1 13.81%; T5-Large ContractNLI 35.97% acc, QuALITY 24.98% acc, NarrativeQA 1.63 F1. Indicates Mamba better on longest NarrativeQA, T5 better on shorter inputs.",
            "comparison_to_human_generated": false,
            "comparison_results": "Compared model families (Mamba vs T5) rather than to human performance; Mamba better on very long inputs, T5 better on shorter ones.",
            "limitations_noted": "These datasets are proxy tasks for long-context reasoning; performance on these benchmarks may not directly translate to quality of scientific hypothesis generation.",
            "uuid": "e8014.8"
        },
        {
            "name_short": "In-context learning benchmarks (MATHQA / MMLE / MMLUSR / GPQA)",
            "name_full": "General in-context learning tasks: MATHQA, MMLE, MMLUSR, GPQA",
            "brief_description": "Standard few-shot in-context learning benchmarks used to compare model ICL capabilities (math reasoning and QA) across model sizes; used here to compare Mamba and T5 in general ICL behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mamba, T5",
            "model_size": "Mamba (130M, 570M, 790M), T5-Small, T5-Base, T5-Large",
            "scientific_domain": "NLP / model generalization and in-context learning",
            "theory_type": "capability evaluation (few-shot ICL)",
            "evaluation_method_name": "Accuracy on several in-context learning tasks",
            "evaluation_method_description": "Provide a few demonstrations and measure the model's accuracy on held-out examples for math and QA tasks to understand in-context learning strengths.",
            "evaluation_metric": "Accuracy (%) per dataset",
            "metric_definition": "Percentage correct (0–100%) on held-out in-context examples.",
            "dataset_or_benchmark": "MATHQA, MMLE, MMLUSR, GPQA (general ICL suites used in related work and experiments B.1)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table: Mamba-790M MATHQA 25.56%, MMLE 23.74%, MMLUSR 23.38%, GPQA 25.00%; T5 variants showed comparable ranges (T5-Large MATHQA 22.51%, etc.).",
            "comparison_to_human_generated": false,
            "comparison_results": "Mamba performs on par with T5 on these ICL tasks; no human baselines reported.",
            "limitations_noted": "ICL benchmark performance is task-specific and may not directly predict quality of open-ended hypothesis generation.",
            "uuid": "e8014.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2
        },
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries",
            "rating": 2
        },
        {
            "paper_title": "Bertscore: Evaluating text generation with bert.",
            "rating": 2
        },
        {
            "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "rating": 2
        },
        {
            "paper_title": "S2ORC: The semantic scholar open research corpus",
            "rating": 2
        },
        {
            "paper_title": "Contractnli: A dataset for document-level natural language inference for contracts",
            "rating": 1
        },
        {
            "paper_title": "Quality: Question answering with long input texts, yes!",
            "rating": 1
        },
        {
            "paper_title": "The narrativeqa reading comprehension challenge",
            "rating": 1
        }
    ],
    "cost": 0.01819975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring Scientific Hypothesis Generation with Mamba</h1>
<p>Miaosen Chai ${ }^{1 <em>}$, Emily Herron ${ }^{2 </em>}$, Erick Cervantes ${ }^{3}$, Tirthankar Ghosal ${ }^{2}$<br>${ }^{1}$ University of Southern California<br>${ }^{2}$ Oak Ridge National Laboratory ${ }^{3}$ Texas A\&amp;M International University<br>miaosenc@usc.edu, {herronej, ghosalt}@ornl.gov, Erickcervantes@dusty.tamiu.edu</p>
<h4>Abstract</h4>
<p>Generating scientifically grounded hypotheses is a challenging frontier task for generative AI models in science. The difficulty arises from the inherent subjectivity of the task and the extensive knowledge of prior work required to assess the validity of a generated hypothesis. Large Language Models (LLMs), trained on vast datasets from diverse sources, have shown a strong ability to utilize the knowledge embedded in their training data. Recent research has explored using transformer-based models for scientific hypothesis generation, leveraging their advanced capabilities. However, these models often require a significant number of parameters to manage long sequences, which can be a limitation. State Space Models, such as Mamba, offer an alternative by effectively handling very long sequences with fewer parameters than transformers. In this work, we investigate the use of Mamba for scientific hypothesis generation. Our preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation. We have made our code available here: https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-withMamba</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have emerged as a cornerstone in artificial intelligence, particularly in scientific discovery. These models have been increasingly integrated into scientific hypothesis and idea generation, transforming traditional approaches to research. Traditionally, the process of scientific hypothesis generation has involved a complex interplay of the scientific method and inductive reasoning, requiring meticulous observation, literature review, and identification of knowledge gaps.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>This process, while crucial, is time-consuming and labor-intensive, relying heavily on researchers' expertise and creativity.</p>
<p>LLMs offer unique capabilities that address many challenges inherent in traditional scientific inquiry. They excel at processing vast amounts of text, identifying intricate patterns, and drawing upon an extensive knowledge base. This allows them to mitigate cognitive biases, efficiently identify research gaps, and generate a broad spectrum of hypotheses, including unconventional and crossdisciplinary ideas. Their ability to handle complexity makes them particularly valuable for addressing intricate, interdisciplinary problems, potentially accelerating the pace of scientific discovery. (Banker et al., 2023; Zhou et al., 2024; Park et al., 2023; O'Brien et al., 2024)</p>
<p>Scientific Inspiration Machines Optimized for Novelty (SciMON) (Wang et al., 2024) represents a leading approach in LLM-based scientific hypothesis generation. It utilizes an LLM-based generation module and a novel iterative novelty boosting mechanism to produce ideas that are both innovative and grounded in existing literature. However, SciMON still faces limitations in generating outputs that match the depth and utility of real scientific papers. To address these challenges, we have integrated a new LLM architecture called Mamba (Gu and Dao, 2023) into SciMON's generation module. Mamba, based on selective state space models, combines the strengths of Transformer and recurrent architectures. It introduces a selection mechanism for content-based reasoning and selective information processing within a simplified neural network design. This integration aims to enhance SciMON's ability to generate more novel, technically sophisticated, and practically useful scientific ideas.</p>
<p>Our work provides a comprehensive comparison of Mamba and Transformer-based models in scientific hypothesis generation tasks. We evaluate</p>
<p>Mamba's performance on general in-context learning benchmarks and long-context tasks, assess its capabilities in downstream hypothesis generation, and investigate its potential as a baseline model for scientific hypothesis generation. Throughout our study, we ensure reproducibility by providing detailed experimental setup information, including datasets, benchmark versions, and implementation scripts.</p>
<h2>2 Related Work</h2>
<p>Recent research has explored the potential of Large Language Models (LLMs) in scientific hypothesis and idea generation, employing various approaches from direct prompting to more complex frameworks. (Park et al., 2023) and (Banker et al., 2023) investigated the capabilities of GPT-3 and GPT4 in generating hypotheses across diverse fields such as materials chemistry, physics, quantum information, and social psychology. While these models demonstrated broad knowledge and interdisciplinary insights, they often produced scientifically inaccurate outputs, highlighting the need for refined approaches.</p>
<p>More sophisticated methods have emerged, integrating inter-domain translation, iterative processes, and adversarial techniques. The FieldSHIFT framework (O'Brien et al., 2024), for instance, utilized GPT-4 to translate concepts between neuroscience and developmental biology, successfully generating novel hypotheses and demonstrating potential for identifying symmetries across scientific domains. HypoGeniC (Zhou et al., 2024) employed a multi-armed bandit-inspired reward function to iteratively improve hypotheses, outperforming few-shot prompting across multiple tasks. In astronomy, (Ciucă et al., 2023) applied adversarial prompting using multiple GPT-4 instances to generate, critique, and refine hypotheses, significantly improving their quality.</p>
<p>Further advancements in LLM-based hypothesis generation have incorporated multi-agent approaches, causal graphs, knowledge graph-based retrieval augmentation, and novelty optimization. Qi et al. (2023) developed a collaborative framework where LLM agents serve different roles (analyst, engineer, scientist, critic) in the hypothesis generation process. Tong et al. (2023) combined causal graphs extracted from psychology articles with LLMs to generate psychological hypotheses matching the novelty of human experts. The Sci-</p>
<p>MON framework (Wang et al., 2024) generates novel research directions based on background contexts and a seed term used to constrain and guide the hypothesis space for the model. It employs an iterative novelty optimization workflow and various retrieval augmentations. GPT-4 produced the best results within this framework, although generated ideas still fell short of scientific literature in terms of depth.</p>
<p>While previous work has primarily utilized Transformer-based models, this study leverages Mamba (Gu and Dao, 2023), a sequence modeling architecture based on selective state space models. Mamba has demonstrated comparable or superior performance to Transformer-based architectures, particularly with long sequences. By implementing our approach within the SciMON framework, we aim to capitalize on Mamba's strengths for improved hypothesis generation in scientific contexts, potentially addressing limitations observed in previous LLM-based approaches.</p>
<h2>3 Methodology</h2>
<p>As mentioned, our methodology is inspired by the SciMON model. For our benchmarking study with Mamba, we use the similar experimental framework as SciMON.</p>
<h3>3.1 SciMON Model and Dataset Description</h3>
<p>We make use of the recently released SciMON (Scientific Inspiration Machines Optimized for Novelty) model (Wang et al., 2024), designed to generate novel, literature-informed scientific ideas in the field of Natural Language Processing (NLP). The system begins by extracting problems, motivations, and proposed ideas from scientific papers accessed through the ACL Anthology ${ }^{1}$. The dataset is derived from the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020), comprising 67,408 ACL Anthology papers published between 1952 and 2022. Papers were filtered to include only those in English with available abstracts. The dataset is divided temporally: the training set includes papers before 2021, the validation set contains papers from 2021, and the test set comprises papers from 2022. For our experiments, we use model checkpoints trained on data preceding 2022 to avoid the risk of data contamination. The papers are processed using several information extraction (IE) and natural language processing tools:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Use of IE to obtain literature data: background, proposed ideas (target), and seed terms.</p>
<ol>
<li>PL-Marker (Ye et al., 2022), pretrained on SciERC (Luan et al., 2018), extracts entities (Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic Terms) and their relationships, focusing on used-for relations.</li>
<li>SciCo (Cattan et al., 2021) performs coreference resolution for entity normalization.</li>
<li>Scispacy (Neumann et al., 2019) expands abbreviations to their full forms.</li>
<li>A sentence classification model by Cohan et al. (2019) categorizes abstract sentences into Background, Method, Objective, Other, and Result.</li>
</ol>
<p>In SciMON, a seed term refers to a key concept or keyword that serves as the starting point for generating hypotheses, while the target sentence is the desired output that articulates a potential scientific idea or goal. SciMON takes a seed term and a background context as inputs and generates a corresponding target sentence as output. To train the model, paper abstracts are categorized into Background sentences (B) and Target sentences (T), forming (B, T) training pairs. The Target sentences are selected from the Methods and Objectives sections of the papers. From these, seed terms (typically Tasks) and target terms (typically Methods) are extracted to form input-output pairs. During evaluation, target information is removed. Figure 1 illustrates this process. To ensure dataset quality, we retain only high-confidence outputs from the IE models. The evaluation indicates high precision rates for most preprocessing steps, except for relation extraction. Overall, $79.7 \%$ of instances passed all preprocessing steps, which constitute the challenging dataset. For evaluation, SciMON creates a high-quality gold test set containing 194
instances by removing test cases where models can rely on surface-level background information to infer the ground truth. The remaining instances are then manually annotated to ensure a strong relevance between seed and target terms. At the core of SciMON is its inspiration retrieval module, which retrieves relevant inspirations from three external sources:</p>
<ol>
<li>Semantic Neighbors: Finds similar problems and ideas in the training set based on sentence embeddings.</li>
<li>Knowledge Graph (KG) neighbors: Retrieves related concepts from a background knowledge graph built from the text dataset. The background KG has more than 197 k nodes and 261 k relations.</li>
<li>Citation Neighbors: Identifies relevant paper titles from the citation network of the input paper. The citation networks contain 87 k paper titles.</li>
</ol>
<p>SciMON's generation module utilizes either finetuned T5 language models or in-context learning with GPT-3.5 or GPT-4 LLMs. When fine-tuning the T5 models, an in-context contrastive objective is employed to discourage the models from simply copying their inputs. The in-context contrastive objective is calculated by taking negative examples from the input text and computing an InfoNCE loss (van den Oord et al., 2019) over the hidden states of the decoder with the objective of maximizing the probability of the ground truth against those of in-text negatives. Both the contrastive loss and cross-entropy loss optimized during fine-tuning. During the generation phase, the input contexts are combined with the inspirations retrieved from the previous module. The next phase in the pipeline is Iterative Novelty Boosting. This process begins with an idea generated by the generation module and retrieves similar ideas from the reference corpus or training dataset. The ideas are compared using a similarity threshold. If the generated ideas are too similar to existing ones, the model is instructed to update the idea to improve its novelty. This process is repeated until a sufficient degree of novelty is achieved. To evaluate the effectiveness of SciMON, both automated metrics such as ROUGE and BERTScore were employed, as well as extensive human evaluation. The human evaluation assessed the relevance, novelty, clarity, and scientific reason-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Using the Mamba architecture, the model generates ideas based on background context and literature inspirations, enhancing novelty by repeatedly comparing them to related work.
ableness of the generated ideas, providing a comprehensive assessment of the framework's performance in generating novel scientific ideas. In total, the retrieval dataset includes 59 k papers with over 374 k sentences, allowing SciMON to ground its idea generation in a broad spectrum of research, enabling it to generate novel and literature-informed scientific ideas in the field of NLP.</p>
<h3>3.2 Mamba Architecture</h3>
<p>The Mamba architecture (Gu and Dao, 2023) represents a significant advancement in sequence modeling, introducing selective state-space models (SSMs) to achieve linear time processing of long sequences. At the core of Mamba's design is a novel selection mechanism that enables dynamic focusing on or filtering out of inputs, effectively compressing contexts into smaller states. This approach strikes a balance between effectiveness and efficiency in sequence processing, making it particularly suitable for hypothesis generation in scientific contexts. The key innovation in Mamba lies in its selective SSM layer, which modifies traditional SSMs by making multiple parameters ( $\Delta$, B, C) functions of its inputs. This feature empowers the model to perform content-based reasoning and selectively propagate or forget information along the sequence length dimension. To implement this mechanism efficiently, Mamba employs a hardware-aware parallel algorithm that leverages the memory hierarchy of GPUs. Structurally,</p>
<p>Mamba consists of simplified and heterogeneous blocks. Each block incorporates elements inspired by existing SSM models with MLPs, as found in modern neural networks. A typical Mamba block includes an input linear projection, a convolutional layer, the selective SSM layer, and a linear projection output layer. These blocks are stacked and interleaved with normalization and residual connections throughout the complete architecture, as illustrated in Figure 3. This design represents a simplification of previous SSM architectures by eliminating separate MLP blocks and combining various components into one repeating unit. Mamba distinguishes itself from other state-of-the-art sequence models by avoiding the use of attention mechanisms and standalone MLP blocks. These attributes enable Mamba to achieve state-of-the-art performance across various applications and modalities, including language, audio, and genomics. As demonstrated in Section 4.2, Mamba outperforms other models on language modeling tasks and downstream evaluations. While previous work has primarily utilized Transformer-based models, leveraging Mamba within the SciMON model aims to capitalize on its strengths for improved hypothesis generation in scientific contexts. Mamba's ability to handle long sequences efficiently is particularly advantageous for processing extensive scientific literature and data. Mamba scales better than other models as sequence length increases, potentially addressing limitations observed in previous LLM-based approaches. Furthermore, Mamba boasts inference times up to five times faster than Transformer models and exhibits linear scaling in sequence length (Gu and Dao, 2023). This efficiency is crucial for rapid hypothesis generation and iterative refinement in scientific research. The model's ability to selectively focus on relevant information while filtering out noise could lead to more precise and contextually appropriate hypotheses. By implementing Mamba within the SciMON model, we aim to leverage its unique architecture for enhanced scientific reasoning. The model's demonstrated success in language modeling and its ability to capture long-range dependencies make it a promising approach for efficient and effective hypothesis generation, potentially surpassing the capabilities of previous Transformer-based models in scientific contexts.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Mamba block we use for SciMON. Background and seed serve as input to the model.</p>
<h2>4 Experiments \&amp; Discussion</h2>
<p>We select T5 (Raffel et al., 2019) and GPT-4 as our baseline models to compare with Mamba. We fine-tune various sizes of T5, Mamba models and use a few short GPT-4 in parallel, with the finetuning process taking between 1 to 3 hours using eight H100 GPUs. We present three evaluations: one using the automated metrics and the other with LLM-as-judge (Claude-3.5), following up with a long-text evaluation and finally an evaluation of generated output by a human.</p>
<h3>4.1 Automatic Evaluation</h3>
<p>It is crucial to recognize that the open-ended nature of scientific hypothesis generation poses challenges for automatic evaluations, as semantically comparing outputs from SciMON to the ground truth can be constrained and shallow. Despite these limitations, automated metrics like ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019) still offer valuable insights. We conduct an automatic evaluation for the outputs generated through the novelty iteration with the Challenging and Gold datasets(\$3)</p>
<p>Results Our findings indicate that both fine-tuned T5 and Mamba models show improved performance with increased model size, as evidenced by higher ROUGE-L (Lin, 2004) and BERTScore (Zhang et al., 2019) metrics in Table 1. Generally, Mamba models perform on par with T5 models of similar sizes, with the Mamba-790M model achieving the highest overall scores for three evaluations. However, Mamba does not show a considerable difference compared to T5, as indicated by the results from the original paper (Gu and Dao, 2023).</p>
<p>Additionally, GPT-4 underperformed compared to both T5 and Mamba in few-shot settings, likely because GPT-4 generates longer outputs that do not adhere to the shallow structured templates followed by T5 and Mamba, which are penalized by automatic evaluation metrics. This suggests that human judgment is necessary for a more accurate evaluation.</p>
<table>
<thead>
<tr>
<th>Model - SciMon</th>
<th>R-L</th>
<th>BERT</th>
<th>R-L (GS)</th>
<th>BERT (GS)</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5 - 60.5 m</td>
<td>0.178</td>
<td>0.514</td>
<td>0.184</td>
<td>0.524</td>
</tr>
<tr>
<td>T5 - 223 m</td>
<td>0.197</td>
<td>0.604</td>
<td>0.217</td>
<td>0.627</td>
</tr>
<tr>
<td>T5 - 738 m</td>
<td>0.223</td>
<td>0.663</td>
<td>0.243</td>
<td>0.684</td>
</tr>
<tr>
<td>Mamba - 130 m</td>
<td>0.176</td>
<td>0.523</td>
<td>0.191</td>
<td>0.562</td>
</tr>
<tr>
<td>Mamba - 370 m</td>
<td>0.219</td>
<td>0.628</td>
<td>0.237</td>
<td>0.631</td>
</tr>
<tr>
<td>Mamba - 790 m</td>
<td>0.227</td>
<td>0.683</td>
<td>0.242</td>
<td>0.695</td>
</tr>
<tr>
<td>GPT-4 FS</td>
<td>0.146</td>
<td>0.614</td>
<td>0.143</td>
<td>0.627</td>
</tr>
</tbody>
</table>
<p>Table 1: Automatic results for the challenging (left) and gold (right) subsets. $R-L$ denotes ROUGE-L. BERT denotes BERTScore with SciBERT as its encoder. GS denotes as Gold subsets.</p>
<h3>4.2 LLM As a Judge</h3>
<p>To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework ( Xu et al., 2024b). We utilize a reference-guided style prompt, as introduced by previous research (Zheng et al., 2023), which has been shown to increase the agreement between human and machine evaluations by providing a reference solution.</p>
<p>We design a structured prompt that directs Claude-3.5 to judge the effectiveness of a hypothesis. Specifically, Claude-3.5 is asked to assess the relevance of the hypothesis to the background context and seed term, ensuring sufficient novelty without overly mirroring the provided information, verifying the scientific soundness, and evaluating the clarity and coherence of the language. See the detailed prompt(C). Based on these criteria, Claude-3.5 assigns a rating of either "ineffective" or "effective" to each generated hypothesis, along with a brief justification for its decision, formatted in JSON for consistent analysis. To assist the LLM in making more accurate assessments, we provide it with five examples of both "ineffective" and "effective" hypotheses.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input:</td>
<td style="text-align: center;">Seed Term: Hierarchical table dataset Context: Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4:</td>
<td style="text-align: center;">In that context, a suitable task for a hierarchical table dataset could be Hierarchical Table Reasoning. This task involves understanding, interpreting, and reasoning over complex hierarchical structures present in tables, such as nested rows and columns, hierarchical indexing, and implicit relationships between cells. Extra generated reasoning: The task is well-suited for this dataset because it addresses the problem of understanding complex relationships within the hierarchical structure.</td>
</tr>
<tr>
<td style="text-align: center;">T5:</td>
<td style="text-align: center;">A task that can be used for hierarchical table dataset is semantic parsing</td>
</tr>
<tr>
<td style="text-align: center;">Mamba:</td>
<td style="text-align: center;">Question answering systems are an effective tool for reasoning with hierarchical tables</td>
</tr>
<tr>
<td style="text-align: center;">Ground Truth:</td>
<td style="text-align: center;">Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and produce better descriptions in NLG</td>
</tr>
</tbody>
</table>
<p>Table 2: Example of SCIMON outputs with different models. T5 denotes T5-large. Mamba denotes Mamba-790m.</p>
<h3>4.3 Human Evaluation</h3>
<p>To validate the effectiveness of Claude-3.5, we recruit five NLP experts, each with graduate-level education, to independently rate the outputs. We selected 100 questions at random from the Challenge set and developed a structured questionnaire (Figure 5) for the experts to evaluate the hypotheses. Experts rated each hypothesis as either effective or ineffective based on four key criteria: relevance, novelty, scientific validity, and clarity which is the same as the prompt instruction for Claude-3.5 (C). To ensure objectivity, the raters were blind to the conditions, and the system outputs were randomly shuffled across the instances.</p>
<p>Results We find that both Claude-3.5 and human evaluations yield similar patterns in the performance of the models. GPT-4 achieves the highest scores in both evaluations, with an accuracy of $76 \%$ in the Claude-3.5 evaluation and $68 \%$ in the human evaluation. This consistency across evaluation methods highlights GPT-4's strong capability in generating hypotheses that align with key criteria such as relevance, novelty, scientific validity, and clarity. Given GPT-4's larger model size, its superior performance is expected. However, Mamba does not significantly outperform the transformerbased T5, likely due to the nature of the SciMON task, which does not fully exploit Mamba's longcontext potential. The average input length in this task is less than $10^{2}$ tokens, which favors models with stronger in-context learning abilities like T5. Although we hypothesize that Mamba's strengths would be more apparent in tasks requiring longer
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human and Claude 3.5 Sonnet evaluations of generated scientific hypothesis. The y-axis represents the accuracy(\%).</p>
<p>contexts, the dataset preprocessing used by the SciMON authors prevents us from directly testing this hypothesis within this context.</p>
<p>To further explore this, we conduct a set of long-context experiments in NLP, ordering tasks by input length: scrolls_narrativeqa (longest), scrolls_quality, and scrolls_contractnli (shortest). Our findings (Table 4) indicate that T5 models excel at tasks with smaller input sizes, with T5Large achieving the highest accuracy of $35.97 \%$ on scrolls_contractnli. Conversely, Mamba models perform significantly better with larger input lengths, as evidenced by Mamba-790M attaining the highest F1 score of 13.81 on scrolls_narrativeqa. However, Mamba models exhibit instability on tasks with smaller inputs, as shown by the nonconverging training loss when scaling to large-sized models. Similar instability has been observed in Mamba's performance on the ImageNet dataset</p>
<p>(Xu et al., 2024a), but the underlying cause remains unclear. This issue is likely related to the current instantiation of Mamba, which may suffer from vanishing and exploding gradients. This suggests that while Mamba does not outperform excessively on current tasks, Mamba may be more effective for scientific hypothesis generation under long-input settings. Also, the linear scaling with sequence length benefits Mamba for faster reference. However, future experiments are needed to demonstrate the performance of the Mamba architecture on a large scale.</p>
<h2>5 Limitations and Future Work</h2>
<p>While this study provides valuable insights, it is important to acknowledge its limitations and potential areas for future research. The architecture of SciMON introduces certain constraints that affect the scope and generalizability of our findings. One key limitation is the data scope, as SciMON's dataset is exclusively composed of ACL Anthology papers from S2ORC. This specialized focus may limit the applicability of our results to other scientific domains, particularly those that rely on multimodal data such as visual representations in biology or chemical structures in materials science.</p>
<p>Our comparative model analysis was restricted to an empirical comparison between Mamba and Transformer-based models under constrained parameter sizes. Future work could benefit from more extensive comparisons involving larger parameter settings, which may reveal additional insights into the relative performance of these models in hypothesis generation tasks.</p>
<p>Furthermore, the rapid pace of development in state space models presents new opportunities for advancing hypothesis generation capabilities. Recent innovations such as Jamba (Lieber et al., 2024), Samba (Ren et al., 2024), and TTT (Sun et al., 2024) were not included in our analysis but represent promising avenues for future research. Investigating these emerging models could potentially uncover novel approaches to improve the efficiency and effectiveness of scientific hypothesis generation.</p>
<h2>6 Memorization</h2>
<p>Given that LLMs are trained on extensive datasets, including potentially the same sources used for evaluation, there is a risk that the models may reproduce memorized content rather than generating
novel hypotheses. So, we conduct a memorization check to ensure the validation of our experiments.</p>
<ol>
<li>(Raffel et al., 2019) shows that T5 is pretrained on C4 which was crawled from web prior to April 2019.</li>
<li>Mamba uses the Pile dataset (Gao et al., 2020), and follows the training recipe described in (Brown et al., 2020).</li>
<li>The GPT-4 checkpoint used in this study is primarily based on data collected before September 2021, with only a minimal amount of more recent data included during both pretraining and post-training stages (Wang et al., 2024). Given that the evaluation focuses on papers published in 2022, the chance that these papers are part of GPT-4's pretraining dataset is considerably low.
Furthermore, a manual review of GPT-4's outputs is conducted from SciMON using a gold set composed of 2022 ACL Anthology papers. This review specifically looks for instances where GPT-4 might reproduce detailed information, such as method names, or generate text that closely mirrors the original papers. The findings show no significant evidence of memorization.</li>
</ol>
<h2>7 Conclusion</h2>
<p>Our study provides insights into the application of language models, particularly the Mamba architecture, for scientific hypothesis generation within the SciMON model. Comparative analysis reveals that Mamba models perform comparably to T5 models of similar sizes, with Mamba-790M achieving the highest scores in automatic evaluations. GPT-4, however, outperforms both in human and LLMbased evaluations, demonstrating superior capability in generating relevant, novel, and scientifically valid hypotheses. Mamba exhibits strength in processing longer input sequences, suggesting potential for complex scientific reasoning tasks. However, it shows instability with smaller inputs, indicating areas for improvement. These findings highlight the potential of state space models in advancing scientific hypothesis generation, despite limitations such as the use of only ACL Anthology papers and restricted parameter sizes in our analysis. Future research should focus on expanding the dataset to diverse scientific domains, investigating larger</p>
<p>parameter settings and emerging state space models, developing specialized benchmarks for longsequence processing, and addressing Mamba's instability with smaller inputs. While Mamba shows promise, particularly for long-context tasks, further research is needed to fully harness its potential and address limitations. As language models evolve, their integration into scientific workflows holds great promise for accelerating hypothesis generation and innovation across diverse fields. This research represents a significant step towards leveraging advanced language models to expand the frontiers of scientific inquiry and knowledge generation.</p>
<h2>Acknowledgement</h2>
<p>This research used resources of the Oak Ridge Leadership Computing Facility (OLCF), which is a DOE Office of Science User Facility at the Oak Ridge National Laboratory supported by the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.</p>
<h2>References</h2>
<p>Sachin Banker, Promothesh Chatterjee, Himanshu Mishra, and Arul Mishra. 2023. Machine-assisted social psychology hypothesis generation.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Ma teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165.</p>
<p>Arie Cattan, Sophie Johnson, Daniel S. Weld, Ido Dagan, Iz Beltagy, Doug Downey, and Tom Hope. 2021. Scico: Hierarchical cross-document coreference for scientific concepts. ArXiv, abs/2104.08809.</p>
<p>Ioana Ciucă, Yuan-Sen Ting, Sandor Kruk, and Kartheik Iyer. 2023. Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. Preprint, arXiv:2306.11648.</p>
<p>Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Dan Weld. 2019. Pretrained language models for sequential sentence classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International</p>
<p>Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3693-3699, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027.</p>
<p>Riccardo Grazzi, Julien N. Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. 2024. Is mamba capable of in-context learning? ArXiv, abs/2402.03170.</p>
<p>Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv, abs/2312.00752.</p>
<p>Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2017. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328.</p>
<p>Yuta Koreeda and Christopher D. Manning. 2021. Contractnli: A dataset for document-level natural language inference for contracts. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Haim Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avshalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. 2024. Jamba: A hybrid transformer-mamba language model. ArXiv, abs/2403.19887.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and robust models for biomedical natural language processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319-327, Florence, Italy. Association for Computational Linguistics.</p>
<p>Thomas O'Brien, Joel Stremmel, Léo Pio-Lopez, Patrick McMillen, Cody Rasmussen-Ivey, and Michael Levin. 2024. Machine learning for hypothesis generation in biology and medicine: exploring the latent space of neuroscience and developmental bioelectricity. Digital Discovery, 3:249-263.</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Sam Bowman. 2021. Quality: Question answering with long input texts, yes! In North American Chapter of the Association for Computational Linguistics.</p>
<p>Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. 2024. Can mamba learn how to learn? a comparative study on in-context learning tasks. ArXiv, abs/2402.04248.</p>
<p>Yang Jeong Park, Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, and Ju Li. 2023. Can chatgpt be used to generate scientific hypotheses? Preprint, arXiv:2304.12208.</p>
<p>Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. 2023. Large language models are zero shot hypothesis proposers. Preprint, arXiv:2311.05965.</p>
<p>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. 2024. Samba: Simple hybrid state space models for efficient unlimited context language modeling. ArXiv, abs/2406.07522.</p>
<p>Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. 2024. Learning to (learn at test time): Rnns with expressive hidden states.</p>
<p>Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, and Kaiping Peng. 2023. Automating psychological hypothesis generation with ai: Large language models meet causal graph.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. Representation learning with contrastive predictive coding. Preprint, arXiv:1807.03748.</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2024. Scimon: Scientific inspiration machines optimized for novelty. Preprint, arXiv:2305.14259.</p>
<p>Rui Xu, Shu Yang, Yihui Wang, Yu Cai, Bo Du, and Hao Chen. 2024a. Visual mamba: A survey and new outlooks. Preprint, arXiv:2404.18861.</p>
<p>Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang. 2024b. Pride and prejudice: Llm amplifies self-bias in selfrefinement.</p>
<p>Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904-4917, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica. 2023. Judging lim-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685.</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. 2024. Hypothesis generation with large language models. Preprint, arXiv:2404.04326.</p>
<h2>A Human Evaluation</h2>
<p>To assess the effectiveness of Claude-3.5, we recruit five NLP experts, all of whom have graduatelevel education, to independently evaluate the outputs by using the following questionnaire.</p>
<h2>Evaluating LLM-Generated Scientific Hypothesis</h2>
<p>You are participating in an evaluation of hypotheses generated by an AI assistant designed to enhance scientific research. These hypotheses are intended to propose novel approaches or insights for computer science research questions. Instructions:</p>
<ul>
<li>Background Context - You will receive a summary of the challenge and recent advancements, setting the stage for the hypotheses AI will generate.</li>
<li>Seed Term - You will receive a key concept or focal point related to the research challenge. This term directs the AI in generating hypotheses specifically relevant to the field's ongoing inquiries.</li>
</ul>
<p>Your task is to evaluate the candidate suggestions by labeling them as effective or ineffective. You need to assess the overall quality of each candidate by taking into account factors:</p>
<ol>
<li>Relevance: How well does the hypothesis relate to the context and the seed term provided?</li>
<li>Novelty: Does the hypothesis offer a new perspective or solution distinct from existing research?</li>
<li>Scientific Validity: Is the hypothesis plausible and justifiable with current scientific knowledge?</li>
<li>Clarity: Is the hypothesis articulated in a clear and understandable manner?</li>
</ol>
<p>Read the following example that illustrates the task:
Context: the task of converting a natural language question into an executable sql query, known as text - to - sql, is an important branch of semantic parsing. the state - of - the - art graph - based encoder has been successfully used in this task but does not model the question syntax well.
Seed term: diverse relational edge embedding
System Outputs:</p>
<ul>
<li>We propose a novel technique, Diverse Relational Edge Embedding (DREE ), to address this problem.</li>
<li>We propose a novel graph-based encoder that uses a diverse relational edge embedding to model the question syntax.</li>
<li>Diverse relational edge embedding is important for text-to-SQL parsing because it can help the parser to better understand the question syntax.</li>
</ul>
<p>Result:</p>
<ul>
<li>Ineffective</li>
<li>Effective</li>
<li>Effective</li>
</ul>
<p>Figure 5: Human evaluation instructions</p>
<h2>B Additional Experiments</h2>
<h2>B. 1 In-context Learning</h2>
<p>Modern attention-based LLMs exhibit remarkable in-context learning (ICL) capabilities, enabling them to learn new tasks effectively with only a few demonstrations. Research indicates that Mamba performs on par with Transformers in standard regression ICL tasks and surpasses them in tasks such as sparse parity learning (Park et al., 2024). Additionally, (Grazzi et al., 2024) found that Mamba
incrementally optimizes its internal representations in a manner similar to transformer models, which aids in solving ICL problems. This adaptability suggests that Mamba can be effectively compared to Transformers in few-shot and fine-tuning settings with comparable data and training time due to its ICL, which serves as the basis for our experiment's design.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MATHQA (acc) \%</th>
<th style="text-align: center;">MMLE (acc) \%</th>
<th style="text-align: center;">MMLUSR (acc) \%</th>
<th style="text-align: center;">GPQA (acc) \%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Mamba-130M</td>
<td style="text-align: center;">23.38</td>
<td style="text-align: center;">22.82</td>
<td style="text-align: center;">23.05</td>
<td style="text-align: center;">25.00</td>
</tr>
<tr>
<td style="text-align: center;">Mamba-570M</td>
<td style="text-align: center;">24.32</td>
<td style="text-align: center;">22.95</td>
<td style="text-align: center;">22.96</td>
<td style="text-align: center;">24.78</td>
</tr>
<tr>
<td style="text-align: center;">Mamba-790M</td>
<td style="text-align: center;">25.56</td>
<td style="text-align: center;">23.74</td>
<td style="text-align: center;">23.38</td>
<td style="text-align: center;">25.00</td>
</tr>
<tr>
<td style="text-align: center;">T5-Small</td>
<td style="text-align: center;">23.64</td>
<td style="text-align: center;">23.07</td>
<td style="text-align: center;">23.49</td>
<td style="text-align: center;">24.78</td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">22.18</td>
<td style="text-align: center;">22.93</td>
<td style="text-align: center;">22.96</td>
<td style="text-align: center;">25.00</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large</td>
<td style="text-align: center;">22.51</td>
<td style="text-align: center;">22.94</td>
<td style="text-align: center;">22.94</td>
<td style="text-align: center;">25.45</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for General In-Context Learning Tasks</p>
<h2>B. 2 Long-Text Evaluation</h2>
<p>We selected three datasets, ranging from $10^{2}$ to $10^{6}$ words per input, to test the model's ability in question answering and natural language inference, which are the basic ability for a scientific hypothesis generation model: ContractNLI $\left(10^{2}\right.$ to $\left.10^{3.5}\right)$ (Koreeda and Manning, 2021), QuALITY $\left(10^{3.3}\right.$ to $10^{3.7}$ ) (Pang et al., 2021), and Narrative $\left(10^{3.5}\right.$ to $10^{6}$ ) (Kociský et al., 2017). The first two tasks use accuracy scores and are designed to answer specific questions based on long science and literature documents, while the latter uses F1 score for evaluation, generating results using the continuation probabilities returned by the model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Contract NLI (acc) \%</th>
<th style="text-align: center;">QuALITY (acc) \%</th>
<th style="text-align: center;">NarrativeQA (f1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mamba-130M</td>
<td style="text-align: center;">14.46</td>
<td style="text-align: center;">24.11</td>
<td style="text-align: center;">8.79</td>
</tr>
<tr>
<td style="text-align: left;">Mamba-570M</td>
<td style="text-align: center;">10.22</td>
<td style="text-align: center;">24.88</td>
<td style="text-align: center;">11.31</td>
</tr>
<tr>
<td style="text-align: left;">Mamba-790M</td>
<td style="text-align: center;">11.86</td>
<td style="text-align: center;">24.30</td>
<td style="text-align: center;">$\mathbf{1 3 . 8 1}$</td>
</tr>
<tr>
<td style="text-align: left;">T5-Small</td>
<td style="text-align: center;">30.76</td>
<td style="text-align: center;">23.97</td>
<td style="text-align: center;">2.26</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base</td>
<td style="text-align: center;">32.88</td>
<td style="text-align: center;">23.97</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">$\mathbf{3 5 . 9 7}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 9 8}$</td>
<td style="text-align: center;">1.63</td>
</tr>
</tbody>
</table>
<p>Table 4: Results for Long-Text Evaluation</p>
<h1>C LLM Prompt</h1>
<p>This is prompt for Claude: Your goal in this task is to rank idea suggestions written by LLM. The LLM helps its users write paper abstracts by generating sentences with proposals for new ideas or questions to consider. You are first given:</p>
<ol>
<li>A context which describes relevant background in a specific area of interest.</li>
<li>A seed term that should be a focus of the generated scientific idea.</li>
<li>An idea suggestion generated by LLMs written in the form of a paper abstract (SUGGESTION).</li>
</ol>
<p>Consider the following factors in your evaluation:</p>
<ol>
<li>Is the suggestion relevant to the context and seed term?</li>
<li>Is the suggestion sufficiently novel, not overly copying the context?</li>
<li>Is the suggestion scientifically sound?</li>
<li>Is the language clear and coherent?</li>
</ol>
<p>Assign a rating as either "effective" or "ineffective", where:</p>
<ul>
<li>"effective" = The SUGGESTION is sufficiently novel, relevant, scientifically sound, and clear.</li>
<li>"ineffective" = The SUGGESTION lacks novelty, relevance, scientific soundness, or clarity.</li>
</ul>
<p>Provide your rating and a brief justification for your assessment.
Return your output in JSON format only with the keys "justification" and "rating":
{
"justification": "<your brief justification>",
"suggestion": "ineffective&lt;/ effective&gt;"
}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://aclanthology.org&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
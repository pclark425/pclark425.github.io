<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8401 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8401</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8401</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-265456551</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.14737v1.pdf" target="_blank">Positional Description Matters for Transformers Arithmetic</a></p>
                <p><strong>Paper Abstract:</strong> Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8401.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8401.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT2-small (124M, 12-layer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The primary model used in experiments: a 12-layer, 124M-parameter GPT2-small architecture trained from random initialization or fine-tuned from pretrained weights in different experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT2-small transformer (12 layers, ~124M parameters). Experiments include randomly initialized training (e.g., 300 epochs, lr 2e-5 for multiplication; 100 epochs, lr 2e-5 for dialogue/addition experiments) and fine-tuning from pretrained weights for some addition experiments (e.g., recursive scratchpad). Tokenization ensured one token per digit by inserting a space before each digit.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit multiplication (up to 15 digits), multi-digit addition and length extrapolation (2-15+ digits), digit reversion, addition in natural language dialogue context</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Models learn digit-wise arithmetic strategies when given formats that expose digit alignment; otherwise, they over-rely on absolute positional information and memorization of short patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tuning on synthetic datasets with controlled formats (padding, reverse product, random spaces, recursive scratchpad), ablations removing absolute positional embeddings, adding random (hash) embeddings per token/head, data augmentations (n×1 and first-step % examples), and pretrained vs random-init comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multiplication: with padding + reversed-product format on a 300k-sample training set, the randomly initialized GPT2-small achieves near-perfect accuracy up to 12-digit factors and correct direct output for many 15×15 cases (paper: 'essentially perfect up to 12 digits' and strong performance on 15-digit max; see Table 5). Addition length extrapolation: models trained on 2-10 digit addends: Basic format test accuracies on 9-13 digits = [1.00, 1.00, 0.00, 0.00, 0.00]; Random Space = [1.00, 1.00, 0.99, 0.09, 0.00]; Recursive Scratchpad = [1.00, 1.00, 1.00, 0.96, 0.55] for 9–13 digits respectively (Table 6). Digit reversion / positional ablation: deleting positional embedding improved generalization by about ~2 digits on regular data (figures described). Dialogue + addition: mixing dialogue and addition datasets, models with absolute pos. embeddings do not utilize Basic-format arithmetic data well; Random Space / NoPosEmbed / RandomEmbed setups substantially improve dialogue-context addition accuracy (see Table 9 subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Overreliance on absolute positional encodings leading to zero extrapolation beyond trained lengths; omission of digits in step-by-step scratchpad outputs (models truncating digit-wise steps to training length); poor performance on repetitive-digit cases when positional information is removed (especially for models trained from scratch); degraded performance on much larger digit lengths (e.g., poor results at 20-digit max in un-tuned runs).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Interventions (padding, reversed product) that standardize digit positions dramatically improved multiplication, indicating models exploit positional alignment. Ablation of positional embeddings increased length generalization, showing positional embeddings are a key mechanism models rely on. Random (hash) embedding per-token/head restores distinguishability across positions and recovers/exceeds generalization in some tasks (figures show improved accuracies). Data-format interventions (recursive scratchpad, random spaces) that reduce absolute positional cues force learning of digit-wise recursive algorithms and increase extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Removing positional embedding helps generalization on regular data but hurts models trained from scratch on repetitive-digit data (pretrained weights alleviate this); even with successful interventions (padding + reverse), scaling to much larger lengths (e.g., 20 digits) still failed in these runs; naive mixing of arithmetic and dialogue data without format or embedding adjustments did not enable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8401.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Absolute Positional Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absolute Positional Embedding (standard Transformer positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard per-position positional embeddings in GPT2 which provide absolute token-position signals; shown to be heavily relied upon by models to solve arithmetic tasks and to limit length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard GPT2 positional embeddings present in experiments; authors ablate (remove) them in several setups.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit addition, digit reversion, addition-in-dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Provides absolute-position markers so the model can learn to read digits at fixed absolute positions (enables simple positional-pattern memorization rather than learning recursive digit algorithms).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation/removal of positional embedding to test impact; comparisons between models with and without positional embeddings, on regular and repetitive-digit test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On digit reversion and addition tasks, removing positional embedding improved generalization by ~2 digits on regular data (Figure 1a). However, on repetitive-digit data, models trained from scratch without positional embedding performed worse (Figure 1b), while pretrained models still generalized well to long repetitive inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When present, absolute positional embeddings enable the model to 'memorize' position-indexed digit patterns and fail to extrapolate to longer lengths; when removed, models trained from scratch can no longer distinguish identical tokens at different positions (hurting repetitive-digit cases).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct ablation experiments: removing absolute positional embeddings increased length generalization for digit reversion and some addition tasks; but it created new failure modes on repetitive inputs unless compensated by pretraining or random embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Removal alone is insufficient: models trained from scratch without positional encoding perform poorly on repetitive-digit inputs; pretrained initialization mitigates this. Therefore positional information must be replaced by another stable representation for robust generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8401.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random (Hash) Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random per-token hash embedding (random tags appended to token embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed alternative positional signal: per-token random Gaussian vectors (split across attention heads) regenerated each epoch/testing to let the model distinguish identical tokens at different positions without deterministic absolute positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implementation: choose n_hash-dimensional Gaussian vector, split into n_head parts and set last n_hash/n_head dimensions of input embedding for each head to that head's vector; keep remaining dims unchanged; decode using first (n_embed - n_hash)/n_head dims. New random vectors drawn each epoch and at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>digit reversion, multi-digit addition, dialogue + addition integration</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Provides per-token distinct tags so identical digit tokens at different positions are distinguishable, enabling models to avoid absolute-position memorization while retaining position-aware discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Applied in models with positional embedding removed; compared performance to both models with absolute positional embeddings and to pure no-posembed models (figures show comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Random embedding increased generalization on digit reversion regular and repetitive data (Figure 2a). For addition, random embedding produced generalization comparable to recursive scratchpad and outperformed plain no-posembed in some settings (Figure 2b). In dialogue+addition experiments, random embedding achieved good pure-addition accuracy and competitive dialogue-context performance (see Table 9 subs).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Models with random embedding sometimes underperform no-posembed in dialogue contexts, and the method requires choosing hash dimensions and per-head split which may affect stability; not a panacea for all failure modes (e.g., very-large-digit scaling still challenging).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains in length generalization when replacing absolute positional embeddings with random embedding and removing posembed; the improvement indicates the model was previously overusing absolute position signals and benefits from distinct per-token tags that encourage algorithmic solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Random embedding does not always beat removing positional embeddings in dialogue settings; pretrained vs random-init differences impact effectiveness. Random embedding still relies on the model learning to use the tag-space appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8401.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Padding + Reverse Product</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-padding inputs plus reversing product digits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-format intervention that pads multiplicands to equal length and reverses the output product's digit order so the model computes from least-significant digit first, which simplifies carry propagation and standardizes positional rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training datasets: 300k samples, digits per factor sampled uniformly from {1..n}, padded to fixed n length; product reversed so least significant digit appears first; trained randomly initialized GPT2-small for 300 epochs (lr 2e-5) in multiplication experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>large-number multiplication (up to 15-digit factors tested)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Standardizes digit positions so a single learned rule can apply across lengths, and reversing lets the model emit least-significant digits first minimizing need for future-carry knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compare Basic format vs AddPadding vs ReverseProduct vs AddPadding+ReverseProduct; measure test accuracy across digit pairs (Tables 4, 5, 14).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Padding markedly improved accuracy; reversing product further improved it. Using both padding and reversed product allowed GPT2-small to compute up to 15×15 multiplication accurately in their runs (Table 5 shows many 1.00 accuracies up to and including many 15-digit tests; paper states 'essentially perfect up to 12 digits' and strong performance to 15). Without padding the model failed at 4×4 in baseline settings.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without padding, model struggles to find consistent rule across variable-length inputs; even with padding+reverse, scaling to max 20-digit cases in the authors' runs was unsuccessful (needs more data/fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large empirical accuracy gains when using padding and reversed outputs show that providing consistent positional layout and an LSB-first computation order allows the transformer to learn digit-wise multiplication algorithmically rather than memorizing short patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Improvements depend on dataset composition (e.g., presence of many n×1 samples); still insufficient to guarantee correct results at much larger lengths or with different digit distributions unless training/training-hyperparameters are scaled/adjusted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8401.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Space Format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-space-inserted data format (disrupts absolute position cues)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-format intervention that inserts random spaces between characters/tokens of numbers with probability 0.3 to reduce reliance on absolute positional indices and encourage alternative strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied in addition experiments: training on 120k samples of 2–10 digit addends where random spaces are placed between characters (n_s, n'_s randomly from {1..5}). Both pretrained and random-init models were tested; training regimes described in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit addition and addition-in-dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Breaks absolute position patterns so model must rely on local or repeated cues for digit correspondence or learn true recursive addition steps.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compare Basic vs Random Space vs Recursive Scratchpad formats when training on 2-10 digit additions and test generalization to 9–13 digits (Table 6). Also used in dialogue + addition mixing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On 9–13 digit test following training on 2–10 digits: Random Space accuracies = [1.00, 1.00, 0.99, 0.09, 0.00] for 9–13 digits (Table 6), showing some limited extrapolation (one or two digits) beyond training length. In mixed dialogue experiments Random Space combined with no-posembed or random-embedding improved integration.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Generalization beyond a small margin (1–2 digits) still limited; Random Space alone does not reach the strong extrapolation achieved by recursive scratchpad or random-embedding ablations in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Compared to Basic, Random Space decreased the model's ability to exploit absolute position and led to partial length extrapolation and better integration with dialogue contexts, supporting the claim that absolute positional dependence was a key failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Still fails for longer extrapolations (e.g., 12+ digits) unless combined with other interventions (e.g., recursive scratchpad, removing positional embedding, or random embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8401.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recursive Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive scratchpad data format (repeated redundant local info per digit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scratchpad-style format that, at each digit step, records the remaining digits of both addends (reversed) and repeats already computed partial sums, increasing redundancy so the model learns the recursive/additive step rather than positional memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used in addition experiments: pretrained GPT2-small fine-tuned for five epochs on 120k samples in recursive-scratchpad format, with digits reversed in addends.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit addition and length extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Encourages learning of recursive digit-by-digit algorithmic steps by providing repeated, local context per step (redundancy) and reversed order to place least-significant digit first.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Train/finetune models on Recursive Scratchpad and measure extrapolation to 9–13 digits; compare to Basic and Random Space formats (Table 6). Also evaluate pretrained vs random-init effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Strong length extrapolation: on 9–13 digit tests after training on 2–10 digits, Recursive Scratchpad accuracies = [1.00, 1.00, 1.00, 0.96, 0.55] for 9–13 digits respectively (Table 6), significantly better than other formats.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without pretraining, recursive scratchpad was less effective; some failures remain at higher digit counts (e.g., 13 digits only 0.55 accuracy in that experiment). Models sometimes omit digits when using naive scratchpad formats.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High extrapolation accuracy compared to Basic and Random Space indicates that repeated, localized information per digit helps the transformer learn the true recursive carry-add algorithm rather than memorizing positions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Effectiveness depends on pretraining: recursive scratchpad helps only when starting from pretrained weights; training from scratch did not give same extrapolation gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8401.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional Ablation (NoPosEmbed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation: Removal of absolute positional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation study removing absolute positional embeddings from GPT2 to test whether models' arithmetic solutions rely on absolute position signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ablation applied to both pretrained and randomly initialized GPT2-small variants; multiple experiments including digit reversion and addition tasks; training/finetuning regimes vary (e.g., 10 epochs for some reversion tests).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>digit reversion, addition extrapolation, dialogue-integration</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Forces model to avoid using absolute position indices; requires learning alternative positional encodings or representations (e.g., relative or random tags).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Direct deletion of positional embedding and evaluation vs baseline with posembed; compare pretrained vs scratch models and test on regular and repetitive-digit data (Figures 1, 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Deleting pos. embedding improved generalization by ~2 digits on regular digit-reversion tasks for both pretrained and random-init models (Figure 1a). On repetitive-digit testbeds, pretrained models still generalized well up to 16 digits while random-init models performed poorly (Figure 1b). Dialogue experiments show NoPosEmbed combined with recursive/random-space formats enhances dialogue integration.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Models trained from scratch without positional embedding struggled heavily on repetitive-digit inputs because tokens are indistinguishable without positional cues; also introduced stability issues requiring other compensations (random embedding or pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation directly changes behavior: improvements in some extrapolation metrics when removed show prior reliance on absolute positions; simultaneous degradation on repetitive data indicates need for replacement mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Removing positional embedding is not uniformly beneficial and must be paired with other techniques (random embedding or pretraining) to avoid new failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8401.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>n×1 Augmentation and % First-step Operation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>n×1 augmentation and first-step (%) operation (dataset interventions to expose subproblem structure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two dataset-level interventions: (1) increase prevalence of n×1 multiplications (one factor single-digit) and (2) introduce a '%' operator representing a×(b mod 10) to explicitly expose and link the base sub-step of multiplication to full m-digit multiplications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training with 300k multiplication samples where for every 3 datapoints authors altered the second factor to be single-digit (n×1 augmentation) or replaced * by % every 3 datapoints to show first-step multiplication. Reversed-product format used in these augmentation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit multiplication learning and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Aim to teach the model that full m-digit multiplication decomposes into repeated n×1 multiplications; the % token explicitly signals the sub-step to encourage compositional/algorithmic reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Dataset augmentation experiments comparing baseline, n×1-every-3, and % first-step-every-3 variants; measured accuracy on 5- and 6-maximum-digit testbeds (Tables 10a-10d and text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Augmenting with n×1 every 3 datapoints improved accuracy somewhat over baseline reversed-product/no-padding runs; introducing % first-step examples gave a further improvement (still less than the gains from padding). Exact numeric tables (10a–10d) in paper show incremental improvements for 5- and 6-digit maximum datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>These augmentations help but are not as effective as padding+reverse; dataset heterogeneity increases difficulty for model to identify when % vs * should be applied, requiring the model to learn to interpret multiple operation types.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical improvements indicate that exposing subproblem structure (n×1 and explicit first-step examples) helps the model learn to decompose harder multiplication problems into known sub-steps, supporting a modular/algorithmic representation hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>While helpful, these dataset changes alone do not match the large improvements seen with padding+reverse; also require careful mixing ratio (authors note performance degrades if too few n×1 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8401.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error modes & failure analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error types and failure modes observed across arithmetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated description of common model failures in arithmetic tasks reported in the paper: overfitting to absolute positions, omission/truncation of digits in scratchpads, poor extrapolation, and sensitivity to repetitive digits or dataset composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observations span many experimental settings (multiplication, addition, digit reversion, dialogue integration) with both pretrained and randomly initialized GPT2-small variants.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>as above (multiplication, addition, digit reversion, addition-in-dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Failures often stem from reliance on positional heuristics rather than algorithmic digit-wise processing; models may memorize frequent short patterns in training distribution instead of learning general carry propagation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Failure cases demonstrated via targeted test sets (longer lengths than trained, repetitive-digit inputs, dialogue contexts) and ablations (removing posembed, changing formats). The paper lists explicit failure examples for Basic, Random Space, and Recursive Scratchpad formats (Appendix C/D).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: Basic addition trained 2–10 digits: 11–13-digit test accuracy often 0; Random Space gave small gains (e.g., 11-digit ~0.99 but 12-digit 0.09); Recursive Scratchpad gave much better (12-digit 0.96, 13-digit 0.55). Multiplication without padding fails around 4-digit factors; padding+reverse produced near-perfect up to 12 digits and strong at 15 in reported runs. Dialogue models often fail 4–5 digit additions without arithmetic data or format/embedding adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Omitting digits (scratchpad truncation), truncation to training-length steps, incorrect carries leading to local digit errors, inability to generalize to longer lengths, poor performance on repetitive-digit inputs when positional cues are removed, instability when absolute positions removed without alternative tagging.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Repeated observations that changes reducing absolute pos cues (random space, no-posembed + random tags, recursive scratchpad) alter failure modes and can substantially improve some extrapolation tasks; concrete failure transcripts and tables provided in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No single fix solves all failure modes; e.g., removing posembed helps extrapolation but hurts repetitive inputs unless pretrained or using random-embedding; padding helps multiplication but needs careful sampling/augmentation to scale to larger digit counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8401.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Integration with Natural Language (Dialogue + Addition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Addition in natural language setting (mixing dialogue data with arithmetic data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments assessing whether arithmetic capability learned on synthetic arithmetic-only data transfers to arithmetic problems embedded in dialogue/natural-language context and how format/positional interventions affect this integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models trained from scratch for 50–100 epochs (lr 2e-5) on mixtures of dialogue data (generated by GPT-3.5) and addition data (Basic or Random Space formats). Evaluated on dialogue prompts like 'Student: Hi, what is the sum of #a and #b?'.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>2–5 digit addition in dialogue contexts; pure addition tests also run</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>When arithmetic data and dialogue formats differ in positional patterns, models with absolute positional embedding fail to connect the two; removing pos embedding or using Random Space/random embeddings encourages transfer by making arithmetic patterns less tied to absolute positions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compare Dia-only, Dia+Basic, Dia+RandomSpace datasets across models with PosEmbed, NoPosEmbed, and RandEmbed; measure dialogue-context accuracy and pure-addition accuracy (Table 9 subs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Without arithmetic data, dialogue-only models struggled on 4–5 digit additions. Adding Basic-format arithmetic data did not help models with absolute pos embeddings. Random Space arithmetic data and either removing positional embedding or adding random embedding improved transfer: e.g., Dia+RandomSpace with NoPosEmbed shows strong accuracies in many digit settings (see Table 9). Specific numbers vary across sub-tables for 50 vs 100 epochs and pure vs dialogue contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Naively mixing arithmetic data with dialogue fails when formats differ; models with absolute positional embeddings do not learn to apply arithmetic rule in dialogue (format mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical comparisons show that format-matching or removing/altering positional encodings is necessary for mixing to succeed; random embedding and Random Space help models learn transferrable arithmetic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No single configuration uniformly best: e.g., NoPosEmbed may outperform RandEmbed in dialogue contexts slightly, while RandEmbed can outperform NoPosEmbed in pure addition contexts; pretraining and training duration affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8401.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8401.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in introduction as an example of a large LLM with strong capabilities but still struggling on some multi-digit arithmetic (e.g., 4-digit multiplication).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a large state-of-the-art LLM (no experiments in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>not experimentally evaluated in this paper (mentioned as comparator)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mentioned qualitatively as struggling with 4-digit multiplication in other work; no numeric metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Cited as example of limited arithmetic capability on certain multi-digit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Length generalization in arithmetic transformers <em>(Rating: 2)</em></li>
                <li>Randomized positional encodings boost length generalization of transformers <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 1)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8401",
    "paper_id": "paper-265456551",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT2-small",
            "name_full": "GPT2-small (124M, 12-layer Transformer)",
            "brief_description": "The primary model used in experiments: a 12-layer, 124M-parameter GPT2-small architecture trained from random initialization or fine-tuned from pretrained weights in different experiments in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "GPT2-small transformer (12 layers, ~124M parameters). Experiments include randomly initialized training (e.g., 300 epochs, lr 2e-5 for multiplication; 100 epochs, lr 2e-5 for dialogue/addition experiments) and fine-tuning from pretrained weights for some addition experiments (e.g., recursive scratchpad). Tokenization ensured one token per digit by inserting a space before each digit.",
            "arithmetic_task_type": "multi-digit multiplication (up to 15 digits), multi-digit addition and length extrapolation (2-15+ digits), digit reversion, addition in natural language dialogue context",
            "mechanism_or_representation": "Models learn digit-wise arithmetic strategies when given formats that expose digit alignment; otherwise, they over-rely on absolute positional information and memorization of short patterns.",
            "probing_or_intervention_method": "Fine-tuning on synthetic datasets with controlled formats (padding, reverse product, random spaces, recursive scratchpad), ablations removing absolute positional embeddings, adding random (hash) embeddings per token/head, data augmentations (n×1 and first-step % examples), and pretrained vs random-init comparisons.",
            "performance_metrics": "Multiplication: with padding + reversed-product format on a 300k-sample training set, the randomly initialized GPT2-small achieves near-perfect accuracy up to 12-digit factors and correct direct output for many 15×15 cases (paper: 'essentially perfect up to 12 digits' and strong performance on 15-digit max; see Table 5). Addition length extrapolation: models trained on 2-10 digit addends: Basic format test accuracies on 9-13 digits = [1.00, 1.00, 0.00, 0.00, 0.00]; Random Space = [1.00, 1.00, 0.99, 0.09, 0.00]; Recursive Scratchpad = [1.00, 1.00, 1.00, 0.96, 0.55] for 9–13 digits respectively (Table 6). Digit reversion / positional ablation: deleting positional embedding improved generalization by about ~2 digits on regular data (figures described). Dialogue + addition: mixing dialogue and addition datasets, models with absolute pos. embeddings do not utilize Basic-format arithmetic data well; Random Space / NoPosEmbed / RandomEmbed setups substantially improve dialogue-context addition accuracy (see Table 9 subsets).",
            "error_types_or_failure_modes": "Overreliance on absolute positional encodings leading to zero extrapolation beyond trained lengths; omission of digits in step-by-step scratchpad outputs (models truncating digit-wise steps to training length); poor performance on repetitive-digit cases when positional information is removed (especially for models trained from scratch); degraded performance on much larger digit lengths (e.g., poor results at 20-digit max in un-tuned runs).",
            "evidence_for_mechanism": "Interventions (padding, reversed product) that standardize digit positions dramatically improved multiplication, indicating models exploit positional alignment. Ablation of positional embeddings increased length generalization, showing positional embeddings are a key mechanism models rely on. Random (hash) embedding per-token/head restores distinguishability across positions and recovers/exceeds generalization in some tasks (figures show improved accuracies). Data-format interventions (recursive scratchpad, random spaces) that reduce absolute positional cues force learning of digit-wise recursive algorithms and increase extrapolation.",
            "counterexamples_or_challenges": "Removing positional embedding helps generalization on regular data but hurts models trained from scratch on repetitive-digit data (pretrained weights alleviate this); even with successful interventions (padding + reverse), scaling to much larger lengths (e.g., 20 digits) still failed in these runs; naive mixing of arithmetic and dialogue data without format or embedding adjustments did not enable transfer.",
            "uuid": "e8401.0",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Absolute Positional Embedding",
            "name_full": "Absolute Positional Embedding (standard Transformer positional encodings)",
            "brief_description": "Standard per-position positional embeddings in GPT2 which provide absolute token-position signals; shown to be heavily relied upon by models to solve arithmetic tasks and to limit length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Standard GPT2 positional embeddings present in experiments; authors ablate (remove) them in several setups.",
            "arithmetic_task_type": "multi-digit addition, digit reversion, addition-in-dialogue",
            "mechanism_or_representation": "Provides absolute-position markers so the model can learn to read digits at fixed absolute positions (enables simple positional-pattern memorization rather than learning recursive digit algorithms).",
            "probing_or_intervention_method": "Ablation/removal of positional embedding to test impact; comparisons between models with and without positional embeddings, on regular and repetitive-digit test sets.",
            "performance_metrics": "On digit reversion and addition tasks, removing positional embedding improved generalization by ~2 digits on regular data (Figure 1a). However, on repetitive-digit data, models trained from scratch without positional embedding performed worse (Figure 1b), while pretrained models still generalized well to long repetitive inputs.",
            "error_types_or_failure_modes": "When present, absolute positional embeddings enable the model to 'memorize' position-indexed digit patterns and fail to extrapolate to longer lengths; when removed, models trained from scratch can no longer distinguish identical tokens at different positions (hurting repetitive-digit cases).",
            "evidence_for_mechanism": "Direct ablation experiments: removing absolute positional embeddings increased length generalization for digit reversion and some addition tasks; but it created new failure modes on repetitive inputs unless compensated by pretraining or random embedding.",
            "counterexamples_or_challenges": "Removal alone is insufficient: models trained from scratch without positional encoding perform poorly on repetitive-digit inputs; pretrained initialization mitigates this. Therefore positional information must be replaced by another stable representation for robust generalization.",
            "uuid": "e8401.1",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Random (Hash) Embedding",
            "name_full": "Random per-token hash embedding (random tags appended to token embeddings)",
            "brief_description": "A proposed alternative positional signal: per-token random Gaussian vectors (split across attention heads) regenerated each epoch/testing to let the model distinguish identical tokens at different positions without deterministic absolute positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Implementation: choose n_hash-dimensional Gaussian vector, split into n_head parts and set last n_hash/n_head dimensions of input embedding for each head to that head's vector; keep remaining dims unchanged; decode using first (n_embed - n_hash)/n_head dims. New random vectors drawn each epoch and at test time.",
            "arithmetic_task_type": "digit reversion, multi-digit addition, dialogue + addition integration",
            "mechanism_or_representation": "Provides per-token distinct tags so identical digit tokens at different positions are distinguishable, enabling models to avoid absolute-position memorization while retaining position-aware discrimination.",
            "probing_or_intervention_method": "Applied in models with positional embedding removed; compared performance to both models with absolute positional embeddings and to pure no-posembed models (figures show comparisons).",
            "performance_metrics": "Random embedding increased generalization on digit reversion regular and repetitive data (Figure 2a). For addition, random embedding produced generalization comparable to recursive scratchpad and outperformed plain no-posembed in some settings (Figure 2b). In dialogue+addition experiments, random embedding achieved good pure-addition accuracy and competitive dialogue-context performance (see Table 9 subs).",
            "error_types_or_failure_modes": "Models with random embedding sometimes underperform no-posembed in dialogue contexts, and the method requires choosing hash dimensions and per-head split which may affect stability; not a panacea for all failure modes (e.g., very-large-digit scaling still challenging).",
            "evidence_for_mechanism": "Empirical gains in length generalization when replacing absolute positional embeddings with random embedding and removing posembed; the improvement indicates the model was previously overusing absolute position signals and benefits from distinct per-token tags that encourage algorithmic solutions.",
            "counterexamples_or_challenges": "Random embedding does not always beat removing positional embeddings in dialogue settings; pretrained vs random-init differences impact effectiveness. Random embedding still relies on the model learning to use the tag-space appropriately.",
            "uuid": "e8401.2",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Padding + Reverse Product",
            "name_full": "Zero-padding inputs plus reversing product digits",
            "brief_description": "A data-format intervention that pads multiplicands to equal length and reverses the output product's digit order so the model computes from least-significant digit first, which simplifies carry propagation and standardizes positional rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Training datasets: 300k samples, digits per factor sampled uniformly from {1..n}, padded to fixed n length; product reversed so least significant digit appears first; trained randomly initialized GPT2-small for 300 epochs (lr 2e-5) in multiplication experiments.",
            "arithmetic_task_type": "large-number multiplication (up to 15-digit factors tested)",
            "mechanism_or_representation": "Standardizes digit positions so a single learned rule can apply across lengths, and reversing lets the model emit least-significant digits first minimizing need for future-carry knowledge.",
            "probing_or_intervention_method": "Compare Basic format vs AddPadding vs ReverseProduct vs AddPadding+ReverseProduct; measure test accuracy across digit pairs (Tables 4, 5, 14).",
            "performance_metrics": "Padding markedly improved accuracy; reversing product further improved it. Using both padding and reversed product allowed GPT2-small to compute up to 15×15 multiplication accurately in their runs (Table 5 shows many 1.00 accuracies up to and including many 15-digit tests; paper states 'essentially perfect up to 12 digits' and strong performance to 15). Without padding the model failed at 4×4 in baseline settings.",
            "error_types_or_failure_modes": "Without padding, model struggles to find consistent rule across variable-length inputs; even with padding+reverse, scaling to max 20-digit cases in the authors' runs was unsuccessful (needs more data/fine-tuning).",
            "evidence_for_mechanism": "Large empirical accuracy gains when using padding and reversed outputs show that providing consistent positional layout and an LSB-first computation order allows the transformer to learn digit-wise multiplication algorithmically rather than memorizing short patterns.",
            "counterexamples_or_challenges": "Improvements depend on dataset composition (e.g., presence of many n×1 samples); still insufficient to guarantee correct results at much larger lengths or with different digit distributions unless training/training-hyperparameters are scaled/adjusted.",
            "uuid": "e8401.3",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Random Space Format",
            "name_full": "Random-space-inserted data format (disrupts absolute position cues)",
            "brief_description": "A data-format intervention that inserts random spaces between characters/tokens of numbers with probability 0.3 to reduce reliance on absolute positional indices and encourage alternative strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Applied in addition experiments: training on 120k samples of 2–10 digit addends where random spaces are placed between characters (n_s, n'_s randomly from {1..5}). Both pretrained and random-init models were tested; training regimes described in Appendix.",
            "arithmetic_task_type": "multi-digit addition and addition-in-dialogue",
            "mechanism_or_representation": "Breaks absolute position patterns so model must rely on local or repeated cues for digit correspondence or learn true recursive addition steps.",
            "probing_or_intervention_method": "Compare Basic vs Random Space vs Recursive Scratchpad formats when training on 2-10 digit additions and test generalization to 9–13 digits (Table 6). Also used in dialogue + addition mixing experiments.",
            "performance_metrics": "On 9–13 digit test following training on 2–10 digits: Random Space accuracies = [1.00, 1.00, 0.99, 0.09, 0.00] for 9–13 digits (Table 6), showing some limited extrapolation (one or two digits) beyond training length. In mixed dialogue experiments Random Space combined with no-posembed or random-embedding improved integration.",
            "error_types_or_failure_modes": "Generalization beyond a small margin (1–2 digits) still limited; Random Space alone does not reach the strong extrapolation achieved by recursive scratchpad or random-embedding ablations in some cases.",
            "evidence_for_mechanism": "Compared to Basic, Random Space decreased the model's ability to exploit absolute position and led to partial length extrapolation and better integration with dialogue contexts, supporting the claim that absolute positional dependence was a key failure mode.",
            "counterexamples_or_challenges": "Still fails for longer extrapolations (e.g., 12+ digits) unless combined with other interventions (e.g., recursive scratchpad, removing positional embedding, or random embedding).",
            "uuid": "e8401.4",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Recursive Scratchpad",
            "name_full": "Recursive scratchpad data format (repeated redundant local info per digit)",
            "brief_description": "A scratchpad-style format that, at each digit step, records the remaining digits of both addends (reversed) and repeats already computed partial sums, increasing redundancy so the model learns the recursive/additive step rather than positional memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Used in addition experiments: pretrained GPT2-small fine-tuned for five epochs on 120k samples in recursive-scratchpad format, with digits reversed in addends.",
            "arithmetic_task_type": "multi-digit addition and length extrapolation",
            "mechanism_or_representation": "Encourages learning of recursive digit-by-digit algorithmic steps by providing repeated, local context per step (redundancy) and reversed order to place least-significant digit first.",
            "probing_or_intervention_method": "Train/finetune models on Recursive Scratchpad and measure extrapolation to 9–13 digits; compare to Basic and Random Space formats (Table 6). Also evaluate pretrained vs random-init effect.",
            "performance_metrics": "Strong length extrapolation: on 9–13 digit tests after training on 2–10 digits, Recursive Scratchpad accuracies = [1.00, 1.00, 1.00, 0.96, 0.55] for 9–13 digits respectively (Table 6), significantly better than other formats.",
            "error_types_or_failure_modes": "Without pretraining, recursive scratchpad was less effective; some failures remain at higher digit counts (e.g., 13 digits only 0.55 accuracy in that experiment). Models sometimes omit digits when using naive scratchpad formats.",
            "evidence_for_mechanism": "High extrapolation accuracy compared to Basic and Random Space indicates that repeated, localized information per digit helps the transformer learn the true recursive carry-add algorithm rather than memorizing positions.",
            "counterexamples_or_challenges": "Effectiveness depends on pretraining: recursive scratchpad helps only when starting from pretrained weights; training from scratch did not give same extrapolation gains.",
            "uuid": "e8401.5",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Positional Ablation (NoPosEmbed)",
            "name_full": "Ablation: Removal of absolute positional embeddings",
            "brief_description": "An ablation study removing absolute positional embeddings from GPT2 to test whether models' arithmetic solutions rely on absolute position signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Ablation applied to both pretrained and randomly initialized GPT2-small variants; multiple experiments including digit reversion and addition tasks; training/finetuning regimes vary (e.g., 10 epochs for some reversion tests).",
            "arithmetic_task_type": "digit reversion, addition extrapolation, dialogue-integration",
            "mechanism_or_representation": "Forces model to avoid using absolute position indices; requires learning alternative positional encodings or representations (e.g., relative or random tags).",
            "probing_or_intervention_method": "Direct deletion of positional embedding and evaluation vs baseline with posembed; compare pretrained vs scratch models and test on regular and repetitive-digit data (Figures 1, 2).",
            "performance_metrics": "Deleting pos. embedding improved generalization by ~2 digits on regular digit-reversion tasks for both pretrained and random-init models (Figure 1a). On repetitive-digit testbeds, pretrained models still generalized well up to 16 digits while random-init models performed poorly (Figure 1b). Dialogue experiments show NoPosEmbed combined with recursive/random-space formats enhances dialogue integration.",
            "error_types_or_failure_modes": "Models trained from scratch without positional embedding struggled heavily on repetitive-digit inputs because tokens are indistinguishable without positional cues; also introduced stability issues requiring other compensations (random embedding or pretraining).",
            "evidence_for_mechanism": "Ablation directly changes behavior: improvements in some extrapolation metrics when removed show prior reliance on absolute positions; simultaneous degradation on repetitive data indicates need for replacement mechanisms.",
            "counterexamples_or_challenges": "Removing positional embedding is not uniformly beneficial and must be paired with other techniques (random embedding or pretraining) to avoid new failure modes.",
            "uuid": "e8401.6",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "n×1 Augmentation and % First-step Operation",
            "name_full": "n×1 augmentation and first-step (%) operation (dataset interventions to expose subproblem structure)",
            "brief_description": "Two dataset-level interventions: (1) increase prevalence of n×1 multiplications (one factor single-digit) and (2) introduce a '%' operator representing a×(b mod 10) to explicitly expose and link the base sub-step of multiplication to full m-digit multiplications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Training with 300k multiplication samples where for every 3 datapoints authors altered the second factor to be single-digit (n×1 augmentation) or replaced * by % every 3 datapoints to show first-step multiplication. Reversed-product format used in these augmentation experiments.",
            "arithmetic_task_type": "multi-digit multiplication learning and generalization",
            "mechanism_or_representation": "Aim to teach the model that full m-digit multiplication decomposes into repeated n×1 multiplications; the % token explicitly signals the sub-step to encourage compositional/algorithmic reuse.",
            "probing_or_intervention_method": "Dataset augmentation experiments comparing baseline, n×1-every-3, and % first-step-every-3 variants; measured accuracy on 5- and 6-maximum-digit testbeds (Tables 10a-10d and text).",
            "performance_metrics": "Augmenting with n×1 every 3 datapoints improved accuracy somewhat over baseline reversed-product/no-padding runs; introducing % first-step examples gave a further improvement (still less than the gains from padding). Exact numeric tables (10a–10d) in paper show incremental improvements for 5- and 6-digit maximum datasets.",
            "error_types_or_failure_modes": "These augmentations help but are not as effective as padding+reverse; dataset heterogeneity increases difficulty for model to identify when % vs * should be applied, requiring the model to learn to interpret multiple operation types.",
            "evidence_for_mechanism": "Empirical improvements indicate that exposing subproblem structure (n×1 and explicit first-step examples) helps the model learn to decompose harder multiplication problems into known sub-steps, supporting a modular/algorithmic representation hypothesis.",
            "counterexamples_or_challenges": "While helpful, these dataset changes alone do not match the large improvements seen with padding+reverse; also require careful mixing ratio (authors note performance degrades if too few n×1 examples).",
            "uuid": "e8401.7",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Error modes & failure analysis",
            "name_full": "Error types and failure modes observed across arithmetic experiments",
            "brief_description": "Aggregated description of common model failures in arithmetic tasks reported in the paper: overfitting to absolute positions, omission/truncation of digits in scratchpads, poor extrapolation, and sensitivity to repetitive digits or dataset composition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Observations span many experimental settings (multiplication, addition, digit reversion, dialogue integration) with both pretrained and randomly initialized GPT2-small variants.",
            "arithmetic_task_type": "as above (multiplication, addition, digit reversion, addition-in-dialogue)",
            "mechanism_or_representation": "Failures often stem from reliance on positional heuristics rather than algorithmic digit-wise processing; models may memorize frequent short patterns in training distribution instead of learning general carry propagation rules.",
            "probing_or_intervention_method": "Failure cases demonstrated via targeted test sets (longer lengths than trained, repetitive-digit inputs, dialogue contexts) and ablations (removing posembed, changing formats). The paper lists explicit failure examples for Basic, Random Space, and Recursive Scratchpad formats (Appendix C/D).",
            "performance_metrics": "Examples: Basic addition trained 2–10 digits: 11–13-digit test accuracy often 0; Random Space gave small gains (e.g., 11-digit ~0.99 but 12-digit 0.09); Recursive Scratchpad gave much better (12-digit 0.96, 13-digit 0.55). Multiplication without padding fails around 4-digit factors; padding+reverse produced near-perfect up to 12 digits and strong at 15 in reported runs. Dialogue models often fail 4–5 digit additions without arithmetic data or format/embedding adaptations.",
            "error_types_or_failure_modes": "Omitting digits (scratchpad truncation), truncation to training-length steps, incorrect carries leading to local digit errors, inability to generalize to longer lengths, poor performance on repetitive-digit inputs when positional cues are removed, instability when absolute positions removed without alternative tagging.",
            "evidence_for_mechanism": "Repeated observations that changes reducing absolute pos cues (random space, no-posembed + random tags, recursive scratchpad) alter failure modes and can substantially improve some extrapolation tasks; concrete failure transcripts and tables provided in appendices.",
            "counterexamples_or_challenges": "No single fix solves all failure modes; e.g., removing posembed helps extrapolation but hurts repetitive inputs unless pretrained or using random-embedding; padding helps multiplication but needs careful sampling/augmentation to scale to larger digit counts.",
            "uuid": "e8401.8",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Integration with Natural Language (Dialogue + Addition)",
            "name_full": "Addition in natural language setting (mixing dialogue data with arithmetic data)",
            "brief_description": "Experiments assessing whether arithmetic capability learned on synthetic arithmetic-only data transfers to arithmetic problems embedded in dialogue/natural-language context and how format/positional interventions affect this integration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_description": "Models trained from scratch for 50–100 epochs (lr 2e-5) on mixtures of dialogue data (generated by GPT-3.5) and addition data (Basic or Random Space formats). Evaluated on dialogue prompts like 'Student: Hi, what is the sum of #a and #b?'.",
            "arithmetic_task_type": "2–5 digit addition in dialogue contexts; pure addition tests also run",
            "mechanism_or_representation": "When arithmetic data and dialogue formats differ in positional patterns, models with absolute positional embedding fail to connect the two; removing pos embedding or using Random Space/random embeddings encourages transfer by making arithmetic patterns less tied to absolute positions.",
            "probing_or_intervention_method": "Compare Dia-only, Dia+Basic, Dia+RandomSpace datasets across models with PosEmbed, NoPosEmbed, and RandEmbed; measure dialogue-context accuracy and pure-addition accuracy (Table 9 subs).",
            "performance_metrics": "Without arithmetic data, dialogue-only models struggled on 4–5 digit additions. Adding Basic-format arithmetic data did not help models with absolute pos embeddings. Random Space arithmetic data and either removing positional embedding or adding random embedding improved transfer: e.g., Dia+RandomSpace with NoPosEmbed shows strong accuracies in many digit settings (see Table 9). Specific numbers vary across sub-tables for 50 vs 100 epochs and pure vs dialogue contexts.",
            "error_types_or_failure_modes": "Naively mixing arithmetic data with dialogue fails when formats differ; models with absolute positional embeddings do not learn to apply arithmetic rule in dialogue (format mismatch).",
            "evidence_for_mechanism": "Empirical comparisons show that format-matching or removing/altering positional encodings is necessary for mixing to succeed; random embedding and Random Space help models learn transferrable arithmetic representations.",
            "counterexamples_or_challenges": "No single configuration uniformly best: e.g., NoPosEmbed may outperform RandEmbed in dialogue contexts slightly, while RandEmbed can outperform NoPosEmbed in pure addition contexts; pretraining and training duration affect outcomes.",
            "uuid": "e8401.9",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4 (mentioned)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "Mentioned in introduction as an example of a large LLM with strong capabilities but still struggling on some multi-digit arithmetic (e.g., 4-digit multiplication).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Referenced as a large state-of-the-art LLM (no experiments in this paper).",
            "arithmetic_task_type": "not experimentally evaluated in this paper (mentioned as comparator)",
            "mechanism_or_representation": "",
            "probing_or_intervention_method": "",
            "performance_metrics": "Mentioned qualitatively as struggling with 4-digit multiplication in other work; no numeric metrics reported in this paper.",
            "error_types_or_failure_modes": "Cited as example of limited arithmetic capability on certain multi-digit tasks.",
            "evidence_for_mechanism": "",
            "counterexamples_or_challenges": "",
            "uuid": "e8401.10",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2
        },
        {
            "paper_title": "Length generalization in arithmetic transformers",
            "rating": 2
        },
        {
            "paper_title": "Randomized positional encodings boost length generalization of transformers",
            "rating": 2
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 1
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 1
        }
    ],
    "cost": 0.0186705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Positional Description Matters for Transformers Arithmetic
22 Nov 2023</p>
<p>Ruoqi Shen shenr3@cs.washington.edu 
University of Washington</p>
<p>Sébastien Bubeck 
Ronen Eldan 
Yin Tat Lee 
Yuanzhi Li 
Yi Zhang </p>
<p>Microsoft Research</p>
<p>Microsoft Research</p>
<p>Positional Description Matters for Transformers Arithmetic
22 Nov 202313B981EE8A5F4705DC2F264FA335E53BarXiv:2311.14737v1[cs.CL]
Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities -which paradoxically include remarkable coding abilities.We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers.Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently.We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context.For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication.In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).</p>
<p>Introduction</p>
<p>In the world of Large Language Models (LLMs) advancements, arithmetic operations stand as an important yet frequently underestimated challenge.The emergence and triumph of models like GPT-4 (OpenAI, 2023;Bubeck et al., 2023) have had a transformative impact on various sectors, illuminating new potentials in Natural Language Processing and more.However, as we delve deeper into the diverse applications of these models, arithmetic tasks continually pose obstacles (Dziri et al., 2023), e.g., even GPT-4's struggles with tasks such as 4-digit multiplication.</p>
<p>Arithmetic tasks differ significantly from typical natural language tasks.The primary distinction lies in their execution: arithmetic operations might demand intricate intermediate steps internally, whereas most other tasks might not need such extensive internal processing.Furthermore, they possess a distinct data format, being based on a concise vocabulary with vast potential combinations.They also showcase more predictable patterns, and each token in an arithmetic sentence can hold equal significance.This contrasts with other tasks where omitting some non-essential words might not affect the overall meaning.Given the stark differences between arithmetic and other tasks, it is uncertain whether there is a straightforward way to bolster a language model's proficiency in arithmetic.Specifically, it's unclear if the prevailing architecture-tailored mainly for natural language tasks-can efficiently and accurately tackle arithmetic tasks.Moreover, this uniqueness of arithmetic also presents an opportunity: the structured nature of arithmetic, with its transparent steps and definitive outcomes, offers an ideal framework for a deeper understanding of the models.Addressing the challenges of arithmetic tasks and enhancing the arithmetic proficiency of LLMs can also contribute to a deeper understanding of their strengths and limitations.</p>
<p>In this work, we investigate the capabilities of language models concerning arithmetic operations, emphasizing techniques related to efficient position information utilization.Before delving into our approaches, we first identify the important challenges that arithmetic tasks face.Three such challenges, central to this study, are:</p>
<p>Complicated Calculation Large-number multiplication and similar arithmetic tasks often involve intricate intermediate steps.Human solutions without using a calculator typically involve digit-wise multiplication, followed by summation.However, allowing a model to record each step can be verbose and inefficient for larger numbers.We investigate the feasibility of enabling a small transformer to directly output the product of large multiplication tasks.</p>
<p>Length Extrapolation Arithmetic data, unlike natural language data, typically exhibits highly regular patterns.As a result, models often depend on absolute position information to solve such tasks (Lee et al., 2023).For instance, in an addition operation like a 1 b 1 c 1 + a 2 b 2 c 2 , aligning digits in corresponding positions (e.g., a 1 in position 1 and a 2 in position 5) presents the simplest solution.However, for a four-digit addition task like a 1 b 1 c 1 d 1 + a 2 b 2 c 2 d 2 that hasn't appeared in the training, it's unclear how to handle d 1 at position 4.</p>
<p>Arithmetic and Language Integration</p>
<p>The poor performance of transformers on arithmetic data may be partly due to the lack of arithmetic data in the training set.However, it's uncertain whether simply supplementing the model with arithmetic data will resolve the problem.It's unclear if the model can successfully integrate arithmetic and natural language data due to their substantial differences.</p>
<p>We tackle the above challenges through two types of approaches, both aimed at utilizing position information more efficiently.A first approach is to alter the positional encoding directly.In this work, we explore an alternative positional encoding, namely randomized embedding, which is simple yet efficient for arithmetic tasks.A less direct approach for better position information utilization is to modify the representation of the arithmetic data to leverage standard positional encoding differently.We investigate how altering the data format can lead the model to learn the arithmetic task differently and exhibit varying properties.</p>
<p>In this work, we focus on small models of a GPT2-small size (124M).Our findings reveal that even such modest-sized models can adeptly execute intricate arithmetic tasks.This underscores not only the capability of the transformer architecture to handle arithmetic but also highlights that a small subset of model parameters can integrate arithmetic proficiency into language models, without affecting the model's capacity on other tasks.</p>
<p>We study large-number multiplication in Section 2, length generalization in Section 3 and arithmetic and language integration in Section 4. In this work, we tackle the three challenges outlined separately.However, in practice, we would need a single model that is able to show all the properties we want.This can be done by combining the approaches used in this paper, which we leave as a future work.For the purposes of this paper, we've maintained consistency in data size, model size, and training epochs, though it's conceivable that our observed outcomes could be achieved with reduced data sizes, smaller models, and fewer training epochs.</p>
<p>Related Works Several recent works have studied using transformers to solve arithmetic tasks.Charton (2021Charton ( , 2022) ) studied using transformers to do linear algebra.Nogueira et al. (2021) studied how the surface form of a number affects learning.Zhang et al. (2022) studied variable assignment task.Qian et al. (2022) demonstrated the limitation of language models on arithmetic tasks.Hanna et al. (2023) studied the ability of GPT2 on arithmetic tasks from an interpretation viewpoint.Dziri et al. (2023) showed that even fine-tuned GPT3 has trouble performing 3-digit multiplication.Yang et al. (2023) trained a model of size 2B to perform arithmetic tasks and beat the performance of GPT4, but the accuracy obtained is not perfect even for 5-digit numbers.Lee et al. (2023) focused on the sample efficiency of using various data formats for arithmetic tasks and also studied the challenges we address in this paper, focusing on small numbers such as 3-digit addition and 2-digit multiplication.We are not aware of any previous work that is able to output the product of two 15-digit number multiplication, essentially perfect up to 12-digit, as demonstrated in our paper.Lee et al. (2023) also illustrates a model's ability to learn arithmetic and language simultaneously, but the two types of data remain separated.We refer the readers to Testolin (2023) for a survey on the recent progress in using neural networks to do arithmetic tasks.</p>
<p>A long list of works has focused on length generalization of transformers by modifying the positional encoding or model architecture, including Su et al. (2021), Press et al. (2021), Kiyono et al. (2021), Csordás et al. (2021), Li and McClelland (2022), Kazemnejad et al. (2023), Ruoss et al. (2023).Jelassi et al. (2023) shows that relative position embedding (Su et al., 2021), the encoder-only model can generalize to significantly longer lengths on arithmetic tasks.</p>
<p>To solve math problems using transformers, Uesato et al. ( 2022</p>
<p>Large Number Multiplication</p>
<p>Multiplication entails a sequence of intermediate steps, especially when dealing with large numbers.Modern language models like GPT-4 often find it challenging to handle these extensive multiplications (see Table 1).One test we can do is to ask the model to output the product directly, without using a scratchpad.We believe studying how the model can output the answer directly, bypassing intermediary steps, is an important research direction because in practice, outputting every step can be laborious and time-consuming.More importantly, always outputting the full steps can also prevent the model from using the most efficient method to solve the problem.In Section 2.1, we show a 12-layer transformer can output the product of 15 × 15-multiplication directly, demonstrating the immense potential of transformers.Constraining the model to use the scratchpad can force the model to adopt suboptimal strategies.While it can be hard for the model to learn to output the answers directly without using a scratchpad, our experiment indicates that given the right dataset and training regimen, it is feasible.</p>
<p>Large number multiplication is complicated, so it can be hard for the model to detect the rules for multiplication if we train the model directly with complicated multiplication tasks.However, there exist simple cases such as one-digit multiplications.By starting with these straightforward cases, the model can initially grasp rules from the basics and then extrapolate to more complex situations.For our initial attempt, we included a lot of small-number multiplication in our datset.Our aim was to ensure the model had ample exposure to basic multiplications, enabling it to grasp multiplication rules.We create a dataset with 300k samples on 1-to-n-digit multiplication.We generate the two numbers in a way such that the number of digits of the two numbers is sampled uniformly randomly from {1, ..., n}.Although this uniform distribution ensures a balanced representation of numbers of different lengths, our emphasis leans towards smaller numbers.For example, our training set consists of around 8k single-digit number times a single-digit number, but there are only 100 different one-digit multiplications, so there will be a lot of repeated single-digit multiplication in our training set.On the contrary, the training set contains only less than 0.0002% of 5-digit times 5-digit numbers.In the "Basic" format, the multiplier, multiplicant, and their product are presented straightforwardly.For instance, for two numbers 73866 and 1001, we write down "7 3 8 6 6 * 1 0 0 1 # 7 3 9 3 9 8 6 6".1 We show in Table 2 the performance of a randomly initialized GPT2-small trained for 300 epochs when n = 5 and in Table 13 the performance when n = 10.The model performs well on 1-2-digit multiplication, but very poorly on large numbers.Notably, we see a trend that the model performs poorly when the sum of the number of digits in the two factors is greater than 5.When the sum is smaller than 5, the training set includes more than 10% of all possible number combinations, leading to uncertainty regarding whether the model's proficiency with smaller numbers stems from genuine understanding or mere memorization.</p>
<p>Our findings show that emphasizing the small numbers is not enough for the model to perform well on large numbers.As the next step, we will focus on modifying the simple case, where the model can grasp the rule, so that the model can extrapolate to the hard cases efficiently.In Section 2.1 and Section A.1, we will present two distinct approaches designed to help the model draw connections between simpler and more complex tasks.These two approaches follow different principles and we hope they can inspire innovative simple case formulations not only for this multiplication task but for other tasks as well.</p>
<p>Padding</p>
<p>For datapoints on multiplications of numbers with different numbers of digits, the position of the product sign varies.Consequently, the model needs to figure out the position of the product sign first and then perform the operation based on the relative position.This makes the rule of operation unnecessarily hard.A simple modification we can adopt is to add zero-padding to the training samples so that all the numbers are given in the same length.In this way, all multiplications will follow one rule no matter how many the number of digits the two factors have.If the maximum number of digits for the factors is n, we pad 0 so that both factors contain n digits and the product contains 2n digit.</p>
<p>In addition, to make the task even easier, we can reverse the digits in the product.The rationale behind this is that to get the most significant digit of the product, we need to compute the carry from each digit accurately but to get the least significant digit, we only need to use the least significant digits of the two factors.As a result, starting with the least significant digit and progressing to the most significant digit is more straightforward.This intuitive approach has been used in previous works such as Lee et al. (2023).</p>
<p>Data Format</p>
<p>Example Basic 7 3 8 6 6 * 1 0 0 1 # 7 3 9 3 9 8 6 6 Reverse Product 7 3 8 6 6 * 1 0 0 1 # 6 6 8 9 3 9 3 7 Add Padding 7 3 8 6 6 * 0 1 0 0 1 # 0 0 7 3 9 3 9 8 6 6 Add Padding + Reverse Product 7 3 8 6 6 * 0 1 0 0 1 # 6 6 8 9 3 9 3 7 0 0 Table 4: Testing accuracy for models trained on data with padding and(or) reversed product when the maximum number of digits is 5.</p>
<p>In Table 3, we present examples of our data format.We give more details on the dataset and the setup in Appendix B. The accuracy by GPT2-small on 300k samples achieved using padding and/or reversed product for multiplications with a maximum of 5 and 10 digits is detailed in Table 4 and Table 14 respectively.The results indicate that padding markedly boosts accuracy while reversing the product further elevates it to near perfection.Utilizing both padding and reversed product allows us to accurately compute up to 15 × 15 multiplications, as shown in Table 5.This is a remarkable enhancement when compared to the non-padded data format, which encountered difficulties even with 4 × 4 multiplication.The benefit of padding is that it standardizes the format between basic and more complex cases, enabling them to be addressed by a singular rule and enhancing the link between them.</p>
<p>However, when evaluating accuracy for a maximum of 20 digits in Table 15, the results for larger numbers are unsatisfactory.We did not fine-tune the parameters in our experiment, so it is possible we can achieve high accuracy for even more digits if we use a larger training set, a more optimal digit distribution, or a more fine-tuned learning rate, etc.</p>
<p>Length Extrapolation</p>
<p>In this section, we tackle a different challenge from the previous section, length extrapolation.While relying on position information can help complicated arithmetic tasks, overreliance on position can hurt generalization to additional digits.Based on the idea of reducing the reliance on absolute positional information, in Section 3.1, we delve into various data formats that can help generalization to additional digits, and in section 3.2, we investigate the role of vanilla positional embedding in arithmetic tasks and explore alternative positional embedding that better suits the needs of arithmetic tasks.</p>
<p>Data format</p>
<p>In this section, we explore the impact of different data formats on the generalization performance of models when faced with additional digits in the addition task.We propose two distinct data formats that aid in improving the models' ability to generalize.One straightforward data format is a chain-of-thought (Wei et al., 2022) style scratchpad.In this format, we first write down the two addends of the addition, followed by the digit-wise summation steps and the final sum.However, as expected, this format struggles to generalize to numbers longer than those encountered during training.A common mistake made by the model is omitting digits while recording the digit-wise summation steps.To address this issue, we explore new data formats based on two key ideas.The first idea involves introducing random spaces between characters in the data.By doing so, we make it more challenging for the model to rely on absolute positional embedding to solve the task.This disruption encourages the model to consider other cues beyond the absolute positional information.</p>
<p>The second idea is based on repeating more information for each digit-wise step.This approach allows the model to access additional information, enabling it to learn the actual steps rather than solely relying on memorizing positional patterns.The increased redundancy makes it harder for the model to overfit to positional information.We found that data formats based on both of these ideas significantly improve generalization performance.By incorporating random spaces and increasing information repetition, the models gain the ability to better handle numbers with more digits and exhibit enhanced generalization performance.</p>
<p>We test our two ideas on three data formats.Table 7 shows the examples of these three data formats, where random space is based on the first idea and recursive scratchpad is based on the second idea.We give the formal definition of the data formats and the setup in Appendix C.1.We show in Table 6 the accuracy of the model on the three types of data formats.We further give a few failure examples of the models trained on each data format in Table 16.Our experimental results corroborate our conjectures.</p>
<p>In the "Basic" data format, when trained on addends up to 10 digits, the model fails to generalize numbers exceeding 10 digits.If the model is given two addends that exceed this length, it simply omits some digits and outputs a result with only 10 steps.However, incorporating random spaces into the training set compels the model to move away from relying on absolute positional embedding since it can no longer retrieve the digits from fixed positions.Despite the model's accurate prediction only extending by one digit, this progression represents a significant improvement, demonstrating a phase transition from a complete lack of generalization to some degree of it.We observe an even more significant improvement in generalization performance when we increase the information provided in each digit-wise step.This suggests that adding more information can encourage the model to learn the fundamental recursive steps required for addition, as opposed to overfitting to positional information.</p>
<p>Data Format</p>
<p>9 Digits 10 Digits 11 Digits 12 Digits 13 Digits Basic 1.00 1.00 0.00 0.00 0.00 Random Space 1.00 1.00 0.99 0.09 0.00 Recursive Scratchpad 1.00 1.00 1.00 0.96 0.55</p>
<p>Table 6: Testing accuracies on 9-13-digit-addition of models trained on the three data formats of 2-10-digit-addition.</p>
<p>Data Format Example Basic 2 3 9 + 8 2 1 : 0 + 9 + 1 = 1 0, 1 + 3 + 2 = 6, 0 + 2 + 8 = 1 0, 1 0 6 0 Random Space 2 3 9 + 8 2 1 : 0 + 9 + 1 = 1 0, 1 + 3 + 2 = 6, 0 + 2 + 8 = 1 0, 1 0 6 0 Recursive Scratchpad 2 3 9 + 8 2 1 : 0 + 9 + 1 = 1 0 , = 0 , 3 2 + 2 8 : 1 + 3 + 2 = 6 , = 6 0 , 2 + 8 : 0 + 2 + 8 = 1 0 , = 0 6 0 , = 1 0 6 0 In addition, we would like to make the following remarks.Pretrained vs. Randomly Initialized We found that in this task, using a pretrained model is important for "recursive scratchpad".Without using a pretrained model, "recursive scratchpad" won't help generalization to additional digits.However, it does not make much difference for "random space".For both pretrained and randomly initialized models, "basic" does not generalize to additional digits.We will have more discussion on training from scratch on the addition task in Section 3.2.</p>
<p>Reverse the order of the digits in the addends For "Recursive Scratchpad", we found that reversing the order of the digits of the addends can help the generalization performance.However, reversing the order of both the addends and the sum will not help as much.</p>
<p>Positional Embedding</p>
<p>As we discussed in Section 3.1, the data format can greatly influence a model's dependency on positional information, which subsequently affects its generalization capacity.In this section, we directly examine positional embedding by studying its limitations and exploring potential alternatives.</p>
<p>To better understand the significance of positional embedding, we first consider a simpler task: given a number, the model outputs its digits in reverse order.For instance, if the input number is 12345, the output should be 54321.We evaluate the model's performance on numbers that are longer than those in the training set and investigate challenging cases such as numbers with many repeated digits.We give the formal definition of the two types of data in Appendix C.2.</p>
<p>As an initial step, we eliminated the positional embedding of the GPT2-small while leaving the rest of the architecture intact.It appears that for both the pre-trained model and the model trained from scratch, the removal of positional embedding enhances the generalization capacity across more digits.We show in Figure 1 the test accuracy of both models on regular and repetitive data.Figure 1a indicates that upon deletion of the positional embedding, both models exhibit an improvement in generalization by approximately two digits on the regular data.While we don't observe a significant accuracy discrepancy between the two models on regular data, their performance on repetitive data varies considerably.As shown in Figure 1b, the repetitive data does not pose a difficult challenge for the model with positional embedding.However, it becomes notably difficult for the model trained from scratch, which achieves low accuracy even with 9-digit data.In contrast, it's relatively simple for the pre-trained model, which manages to achieve perfect accuracy with 16-digit data.We speculate that the underlying reason is the inability to differentiate repetitive data  aside from their respective positions.Without absolute positional embedding, the models must resort to alternative methods to encode positional information.Given that the pre-trained model already contains various useful pre-trained components, it has greater flexibility to address this issue.</p>
<p>We propose a simple solution targeting this issue.We mark each token using a random tag so that the model can easily use the tag to distinguish the same tokens appearing at different positions.We call this component a random embedding.We are able to show that this random tag can not only improve the generalization performance on this simple task of digit reversion but also on the more complicated task of addition.</p>
<p>Random Embedding For any chosen hash dimension n hash , we generate a n hash -dimensional random Gaussian vector with mean 0 and identity covariance.Then, we split the Gaussian vector into n head many vectors {h i } n head i=1 each with dimension n hash /n head , set the last n hash /n head dimensions of the input embedding of each head to be h i and keep the remaining (n embed − n hash )/n head dimensions unchanged.After the final layer, we use only the first (n embed − n hash )/n head dimension of each head to decode.We use newly generated random vectors for each epoch and during testing.In Figure 2, we demonstrate the improved generalization capacity of GPT2-small equipped with random embedding.Figure 2a shows that adding random embedding increases the generalization capacity on both the regular data and the repetitive data in the digit reversion task.</p>
<p>Back to the more complicated task of addition, we show in Figure 2b that if we simply delete the positional embedding, the random initialized model does not perform well.If we keep the positional embedding, the model does not generalize to more digits.The random embedding shows significant improvement by achieving about the same generalization capacity as the "Recursive Scratchpad" data format as we show in Section 3.1.</p>
<p>Addition in Natural Language Setting</p>
<p>In the previous sections, we focused on the case where the training data consists of solely arithmetic data.However, in practice, we need to do the arithmetic operations in the natural language setting.Training data consisting exclusively of arithmetic data is usually easy to collect as it can be generated programmatically in large quantities.On the contrary, obtaining arithmetic information embedded in natural language is a more arduous task due to its rarity in natural language content.Consequently, it is important to understand if training on purely arithmetic data can equip the model with the ability to perform arithmetic tasks within a natural language setting.</p>
<p>In this section, we explore a task that involves mixing natural language data with purely arithmetic data to investigate the model's ability to integrate both data types.The natural language data in this case includes dialogues on solving addition problems, with a substantial amount of samples for easy addition questions and a smaller portion for difficult ones.Such dataset structure reflects the real-world challenge of readily collecting easy tasks, while struggling to find natural language data that solves more complex problems.Alongside this, we incorporate purely arithmetic data, which is always correct and can be effortlessly produced using computer programs.Our primary objective is to examine whether the accurate arithmetic data can help the model solve the complex tasks embedded in the natural language context.</p>
<p>Our experiments show that in our setting, training solely on the natural language data can't guarantee an accurate solution to the difficult problems due to the lack of difficult samples in the training set and the errors present.If we naively mix the arithmetic data with the natural language data, we don't see a significant boost in accuracy, which shows that integrating the two types of data is challenging if they follow different formats.One obvious difficulty arises when the arithmetic data follows a certain pattern; the model can easily learn the arithmetic task relying on the positional information.However, when the arithmetic task appears in the natural language context, it won't follow the same positional pattern, causing the model to struggle to connect the two types of data.Overreliance on positional embedding is a recurring issue when using transformers for arithmetic tasks, and this represents the main challenge we discuss in Section 3. In Section 3, we tackle this issue from two aspects: data format and alternative position embedding.We show in our experiments that similar ideas can be applied to the integration of natural language and arithmetic data, thus facilitating the merging of these two types of data.</p>
<p>We use three types of data formats, formally defined in Appendix D with examples shown in Table 9.Our dialogue dataset contains a large number of 2-3-digit addition, but not enough 4-5-digit addition while the addition data set containd a large number of both 2-3-digit addition and 4-5-digit addition.We compare in Table 9 models trained on datasets that combine dialogue data with addition data (Dia+Basic and Dia+RandomSpace) to those trained solely on dialogue data (Dia).We show the results for random initialized models trained 50 epochs and 100 epochs.Without any arithmetic data, models trained exclusively on dialogue struggle to accurately perform 4-5-digit addition.This confirms our hypothesis, given that the dialogue lacks a sufficient number of correct 4-5-digit examples.With arithmetic data, for models with the absolute position embedding, "Basic" doesn't significantly enhance their ability to tackle addition tasks within the dialogue prompt.In contrast, using "Random space", removing the absolute position embedding, and integrating random embedding all improve the model's ability to leverage addition data in supporting dialogue-based addition tasks.For models that exclude absolute position embedding, as well as those with random embedding, the testing accuracy for "Basic" and "Random space" is similar when trained for long enough.Nevertheless, models can learn the "Random space" format slightly faster, as shown in thing.The answer is 726.Addition Data -Basic 4 8 + 4 = 5 2 3 7 5 + 2 6 1 = 6 3 6 5 0 5 1 + 8 5 3 9 = 1 3 5 9 0 Addition Data -Random Space 4 8 4 5 2 3 7 5 2 6 1 6 3 6 5 0 5 1 8 5 3 9 1 3 5 9 0 Table 9: Testing accuracy for models with and without the positional embedding and with the random embedding on the dataset solely consists of dialogue data and the data set consists of a mix of dialogue data and addition data.</p>
<p>and Table 9b.Models without position embedding exhibit slightly better accuracy compared to those with random embedding in dialogue contexts.Conversely, models with random embedding outperform those lacking position embedding in pure addition scenarios, as highlighted in Table 9c and Table 9d.</p>
<p>In conclusion, to allow language and arithmetic integration, we need either data format modification such as random space, or position embedding modification such as excluding absolute positional embedding or adding random embedding.Our conclusions here align with those in Section 3.For models with absolute position embedding, the "Basic" format is less effective due to its highly predictable pattern, allowing models to overly depend on positional information.Removing position embedding addresses this, but can create new stability issues as the model needs alternative ways to interpret position data.Introducing random embedding can offset the drawbacks of removing position embedding, resulting in a more stable performance.Testolin, A. (2023).Can neural networks do arithmetic?a survey on the elementary numerical skills of state-of-the-art deep learning models.arXiv preprint arXiv:2303.07735.</p>
<p>Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. (2022).Solving math word problems with process-and outcome-based feedback.arXiv preprint arXiv:2211.14275.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022).Chain-ofthought prompting elicits reasoning in large language models.Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Yang, Z., Ding, M., Lv, Q., Jiang, Z., He, Z., Guo, Y., Bai, J., and Tang, J. ( 2023 In multiplication, n × 1-multiplication serves as a very important simple case.If our dataset does not contain enough n × 1-multiplication, the models will not be able to learn multiplication successfully even with padding and reversed product.We show in Figure 11 how decreasing the ratio of n × 1-multiplication hurts the model's performance.With this insight, we amplify the presence of n × 1 multiplication in the training set, hypothesizing that this could potentially improve multiplication accuracy, even in the absence of padding.As an initial step, for every three data points of the dataset, we modify the second factor of the multiplication to a single-digit number.In other words, given the original dataset, we choose 1/3 of the datapoints a * b and replace b with b%10.In this way, our dataset consists of a large number of n × 1 multiplication.We give an example of such a dataset with n × 1 multiplication every 3 datapoints in Table 12.Our training datapoints have reversed product but no padding.We show the accuracy on 5-maximum-digit and 6-maximum-digit in Table 10a and 10b.A comparison between Table 10a and Table 4 reveals that this augmentation does somewhat enhance the model accuracy.</p>
<p>To use the simple n × 1-multiplication data, the model must recognize that n × m-multiplication can be decomposed into m n × 1 multiplication and use the knowledge gained from n × 1-multiplication to solve the problem.To better bridge the understanding between n × 1 and n × m multiplications, we undertook a more bold adjustment to our dataset.Rather than merely incorporating more n × 1 multiplications, we introduced an entirely new type of data that explicitly tells the model that n × 1 multiplication is a foundational step for n × m multiplication.To this end, we introduce a new operation % where for two numbers a and b, we define a%b = a × (b mod 10).In this way, we can view % as guidance on the first step of solving a * b.We change the operation from * to % for every 3 datapoints in our training set.We show in Table 12 examples of data with first-step multiplication % every 3 datapoints.We show in Table 10c and 10d the accuracy on 5-maximum-digit and 6-maximum-digit.We see a further performance improvement from Table 10a and Table 10b.</p>
<p>While the enhancements observed from introducing the % operation might not be as pronounced as those achieved with padding, as discussed in Section 2.1, the outcomes of this experiment are both intriguing and enlightening.When comparing the two experiments -one adding additional n × 1 multiplications, and the other integrating first-step multiplications -the latter introduces a more intricate dataset.This dataset comprises two markedly distinct types of data, requiring the models to discern the function of the novel %  Example n × 1 multiplication every 3 datapoints 6 5 1 2 5 * 6 # 0 5 7 0 9 3 (with reversed product) 5 1 4 * 5 9 6 9 # 6 6 0 8 6 0 3 3 8 6 3 1 * 6 1 6 # 6 9 6 6 9 7 3 2 2 2 * 9 # 8 9 1 7 9 8 0 4 * 8 8 8 9 2 # 8 6 1 7 3 9 3 9 0 7  1 9 6 6 5 * 9 7 0 # 0 5 0 5 7 0 9 1  2 6 4 9 * 6 # 4 9 8 5 1  4 3 5 0 * 8 8 1 5 # 0 5 2 5 4 3 8 3  2 0 2 0 * 9 9 8 9 # 0 8 7 7 7 1 0 2  6 2 2 7 4 * 5 # 0 7 3 1 1 3 ... First-step multiplication every 3 datapoints 6 5 1 2 5 * 1 5 3 0 6 % 0 5 7 0 9 3 (with reversed product) 5 1 4 * 5 9 6 9 # 6 6 0 8 6 0 3 3 8 6 3 1 * 6 1 6 # 6 9 6 6 9 7 3 2 2 2 * 8 9 % 8 9 1 7 9 8 0 4 * 8 8 8 9 2 # 8 6 1 7 3 9 3 9 0 7 1 9 6 6 5 * 9 7 0 # 0 5 0 5 7 0 9 1 2 6 4 9 * 6 7 9 6 % 4 9 8 5 1 4 3 5 0 * 8 8 1 5 # 0 5 2 5 4 3 8 3 2 0 2 0 * 9 9 8 9 # 0 8 7 7 7 0 2 6 2 2 7 4 * 9 5 % 0 7 3 1 1 3 ... connections between the simple case and the harder case can be an even more essential step.Exposure to the harder problem when presenting the simple case can be an efficient way to associate the two.Our findings suggest that showing solutions to sub-components of a complex issue can be a more effective instructional approach than merely presenting elementary problems.Indeed, constructing datasets around these subproblems might be more straightforward than building an array of incrementally challenging tasks, when aiming for a specific complexity level.For example, if we want to solve high-school math problems, we can easily collect a dataset consisting of high-school examination problems, but it can be hard to collect a series of problems with increasing difficulty that culminates at the high school level.Using primary school math as foundational problems might not guarantee a seamless transition in complexity or methodology.Instead, decomposing high school problems into their core sub-issues may offer a more coherent learning trajectory.As a result, subproblems of the hard problems can be an efficient simple case.</p>
<p>B Additional Details for Section 2</p>
<p>Setup For all experiments in this section, our training data set consists of 300k samples, where the number of digits each factor has is sampled independently from a uniform distribution on {1, ..., n}.We call such a dataset a n-maximum-digit dataset.We train a random initialized GPT2-small for 300 epochs with a learning rate 2e − 5. We test on 100 samples for each (m 1 , m 2 ) combination, where m 1 and m 2 are the number of digits in the two factors.We replace L with 50 random numbers.We generate L in the following way.For 2-to-3-digit addition, we generate each element of L i.i.d 2-digit numbers uniformly with probability 0.3, and i.i.d.3-digit numbers uniformly with probability 0.7.For 4-to-5-digit addition, we generate each element of L i.i.d 4-digit numbers uniformly with probability 0.5, and i.i.d.4-digit numbers uniformly with probability 0.5.We generate around 12200 dialogues on 2-to-3-digit addition and 1040 dialogues on 4-to-5-digit addition.In this way, our natural language data contains many more 2-to-3-digit numbers than 4-to-5-digit numbers.Our training data contains only one dialogue in one sentence.Although we specify in the prompt the format, around 3% of the data does not follow the prompt and output the dialogue in two lines or output a separator between two dialogues.For the 4-to-5-digit addition, there is an error rate of 0.2%.We did not correct these errors to reflect the noisy data collected in practice.</p>
<p>Addition Data We study two types of arithmetic data.2. Random Space For two numbers a and b, we first write down a n ...a 1 , followed by n s many random spaces.Then, we write down b m ...b 1 , followed n ′ s many random spaces.Finally, we write down the sum s l ...s 1 .n s and n ′ s are uniform numbers from {1, ..., 5}.</p>
<p>For each type, generate 120k samples.</p>
<p>Setup We compare the performance of the models trained on purely dialogue data and those trained the mixture of dialogue and addition data.To test the model in the dialogue context, we prompt the model using a simple student question, "Student: Hi, what is the sum of #a and #b?", where we replace #a and #b with the randomly generated numbers.To test the model in the pure arithmetic context we prompt the model using "#a + #b" or "#a #b" depending on the data format.We check the accuracy of the teacher's response on 100 samples for each number of digits.We compare GPT2-small models with positional embedding, without positional embedding and with random embedding (see Section 3.2 for definition).The models with random embedding do not have positional embedding.For all the runs, we train from a randomly initialized model for 100 epochs with a learning rate 2e-5.</p>
<p>),Cobbe et al. (2021) and Lightman et al. (2023) used verifier and feedback.Zhou et al. (2022) used advanced prompting technique.</p>
<p>( a )
a
Test accuracy on regular data.(b) Test accuracy on repetitive data.</p>
<p>Figure 1 :
1
Figure1: Comparison of pretrained model and trained from the scratch model with and without absolute positional embedding on 100 regular testing samples and repetitive samples.We use pretrained and random initialized GPT2small with/without the positional embedding and fine-tune/train for 10 epochs with a learning rate 2e-5.</p>
<p>( a )
a
Test accuracy on repetitive data in the digit reversion task for models trained from randomly initialized weights.(b)Test accuracy on addition with "Basic" step task for models trained from randomly initialized weights.</p>
<p>Figure 2 :
2
Figure 2: Comparison of trained from scratch model with and without hash embedding on 100 regular testing samples and repetitive samples.We use random initialized GPT2-small (124M) without the positional embedding and train for 25 epochs with a learning rate 1e-5.2</p>
<p>1.</p>
<p>Basic For two numbers a and b with n and m digits, a n ...a 1 and b m ...b 1 .We write down a n ...a 1 + b m ...b 1 = s l s l−1 ...s 1 , where s l ...s 1 is the sum of a and b.</p>
<p>Table 3 :
3
Examples of the data format for multiplication.
(a) Add Padding(b) Reverse Product(c) Reverse Product + Add Padding# Digits12345# Digits12345# Digits1234511.001.001.001.000.9911.001.001.001.001.0011.001.001.001.001.0021.000.990.990.940.8821.001.001.001.001.0021.001.001.001.001.0031.000.990.860.820.7531.001.001.000.970.2831.001.001.001.001.0040.990.960.790.730.6841.001.000.970.280.0241.001.001.001.001.0051.000.900.720.620.5951.000.990.290.060.0251.001.001.001.001.00</p>
<p>Table 5 :
5
Testing accuracy for 15-maximum-digit with padding and reversed product.</p>
<h1>Digits12345678910111213141511.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0021.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0031.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0041.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0051.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0061.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0071.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0081.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0091.001.001.001.001.001.001.001.001.001.001.001.001.001.001.00101.001.001.001.001.001.001.001.000.991.001.001.001.001.000.99111.001.001.001.001.001.001.001.001.001.001.001.001.001.000.99121.001.001.001.001.001.001.001.000.991.001.001.001.000.990.99131.001.001.001.001.001.001.001.001.001.000.990.990.950.950.95141.001.001.001.001.001.001.001.001.001.000.991.000.980.950.93151.001.001.001.001.001.001.001.001.001.000.960.980.950.980.84</h1>
<p>Table 7 :
7
Examples of the data format for adding 239 and 821.</p>
<p>Table 9a
9a
Excuse me, can you help me with something?I need to add two numbers, 842 and 62. Teacher: Of course, let me do the calculation for you.The answer is 904.Student: Good morning!Can you help me with a math problem?I need to find the sum of 324 and 402.Teacher: Good morning!Sure
Data FormatExamplesDialogue Data (Dia)Student:</p>
<p>Table 8 :
8
Examples of the natural language and arithmetic data used.Testing accuracy in the dialogue context for models trained for 100 epochs.Testing accuracy in the dialogue context for models trained for 50 epochs.Testing accuracy in the pure addition context for models trained for 100 epochs.Testing accuracy in the pure addition context for models trained for 50 epochs
DiaDia+BasicDia+Random Space#Digit234523452345PosEmbed0.940.920.610.131.00.960.680.121.00.990.910.82NoPosEmbed0.90.970.670.110.991.00.990.990.960.991.00.99RandEmbed0.90.850.570.071.01.00.970.861.01.00.960.91(a) DiaDia+BasicDia+Random Space#Digit234523452345PosEmbed0.970.960.620.140.990.940.640.070.990.980.930.77NoPosEmbed0.830.960.470.081.01.00.980.61.01.00.990.76RandEmbed0.770.690.130.01.00.980.910.360.990.990.890.83(b) Dia+BasicDia+Random Space#Digit23452345PosEmbed0.991.00.980.981.01.01.01.0NoPosEmbed0.20.080.040.010.950.960.960.27RandEmbed0.991.01.01.01.01.01.01.0(c) Dia+BasicDia+Random Space#Digit23452345PosEmbed0.991.01.00.991.01.01.01.0NoPosEmbed0.910.770.40.220.981.00.980.78RandEmbed0.991.01.00.980.981.00.990.97(d)</p>
<p>Table 10 :
10
). Gpt can solve mathematical problems without a calculator.arXiv preprint arXiv:2309.03241.Testing accuracies for models trained on dataset with n × 1-multiplication every 3 datapoints and first-step multiplication every 3 datapoints.
Zhang, Y., Backurs, A., Bubeck, S., Eldan, R., Gunasekar, S., and Wagner, T. (2022). Unveiling transformerswith lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301.Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. (2022). Teaching algorithmicreasoning via in-context learning. arXiv preprint arXiv:2211.09066.</p>
<p>Table 12 :
12
Examples of datasets with n × 1 multiplication every 3 datapoints and first-step multiplication every 3 datapoints.</p>
<p>the teacher answers it in different ways.Only show the dialogues one by one.Do not show any numbering or anything else.Only use numbers in the list L.</p>
<h1>Digits123456789101112131415161718192011.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0021.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.4631.001.001.001.001.001.001.001.000.991.001.001.000.991.001.001.001.001.000.720.03</h1>
<p>In this paper, for every dataset used, a space is inserted before each digit. This ensures the tokenizer tokenizes each digit as an individual token.
The accuracies of "No PosEmbed" differ slightly from the corresponding accuracies in Figure1because the accuracies are measured from different runs with different learning rates and numbers of epochs.
For dataset with 1-digit weight = α, we generate the dataset in a way such that for every mutliplicaiton, we first generate the number of digits for the two factors independently sampled from {1, ..., 10} with weight {α, 1, 1, ..., 1}.operation and subsequently correlate this operation to the first phase of the * operation.Remarkably, the model adeptly forges this connection, resulting in enhanced performance.This shows that while including simple cases is crucial for the model to solve the hard problem, fostering, where a i represents the i-th digit of the first addend a, b i represents the i-th digit of the second addend b, c i represents the carry from the previous digit, and s i represents the sum of a i , b i , and c i .Finally, we write down the sum of the two numbers.2. Random Space: We introduce random spaces between any two characters in the sentence in the form of "Basic".For each position, we generate a random space with a probability of 0.3.3. Recursive Scratchpad: We modify the basic format so that before writing down the digit-wise sum a i + b i + c i = s i , we first record the remaining digits of a and b that haven't been included in their digit-wise sums in the reversed order, denoted as a i ...a n and b i ...b n .Then, we write down the digit-wise sum for digit i, followed by the digit-wise sums obtained so far, s i ...s 2 s 1 .In addition, we reverse the order of the digits in the two addends.Data Generation For the training data, we first choose the number of digits for the two addends, denoted as n, uniformly random from {2, ..., 10}.Then, given n, we generate the two addends independently using a uniform distribution on [0, 1, ..., 10 n − 1].Then, for each pair of addends, we generate the complete scratchpad steps according to the chosen data format.We generate a total of 120k samples independently following this process.For the testing data, for each number of digits n, we sample two addends uniformly random from [10 n−1 , ..., 10 n − 1] independently.Setup We fine-tune pretrained GPT2-small(124M) for five epochs with learning rate 2e − 5 on data in the three types of data formats in Table7.The testing set consists of 100 n-digit plus n digit additions with n ∈ 9, 10, ...13.C.2 Additional Details for Section 3.2Data Generation For the training data, we first choose the number of digits, denoted as n, uniformly random from {2, ..., 10}.Then, given n, we generate number independently using from uniform distribution on [0, 1, ..., 10 n − 1].We generate a total of 120k samples independently following this process.We consider two types of testing data, to generate the regular testing data, for each digit i, we sample the number uniformly random from [10 n−1 , ..., 10 n − 1] independently.To generate the testing data with repetitive digits, for each digit i, we sample a digit from [0, 1, ..., 9] and repeat the sampled digit i times to form a number with i digits.D Additional Details for Section 4Formally, we have the following two types of data.4 1.00 1.00 1.00 0.81 0.12 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5 1.00 1.00 1.00 0.05 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7 1.00 1.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 8 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13 1.00 1.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 14 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 15 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  Data Format Failure Cases Basic (11 digits) 9 0 8 9 4 0 3 0 2 8 7 + 6 7 9 2 4 1 6 0 2 8 1 : 0 + 7 + 1 = 8 , 0 + 2 + 8 = 1 0 , 1 + 0 + 0 = 1 , 0 + 3 + 6 = 9 , 0 + 0 + 1 = 1 , 0 + 4 + 4 = 8 , 0 + 9 + 2 = 1 1 , 1 + 8 + 9 = 1 8 , 1 + 0 + 7 = 8 , 0 + 9 + 6 = 1 5 , 1 + 0 + 0 = 1, : 1 5 8 8 1 8 1 9 1 0 8 Basic (11 digits) 9 7 4 0 4 8 4 9 0 7 0 + 6 3 2 4 3 4 8 2 9 9 7 : 0 + 0 + 7 = 7 , 0 + 0 + 9 = 9 , 0 + 9 + 2 = 1 1 , 1 + 4 + 8 = 1 3 , 1 + 8 + 4 = 1 3, 1 + 4 + 3 = 8 , 0 + 0 + 4 = 4 , 0 + 4 + 2 = 6 , 0 + 7 + 3 = 1 0 , 1 + 9 + 6 = 1 6 , 1 + 0 + 0 = 1, : 1 6 0 6 4 8 3 3 1 9 7 Random Space 2 3 6 6 7 6 8 3 6 0 8 9 + 1 7 0 7 1 2 0 5 9 6 3 0 : 0 + 9 + 0 = 9, (12 digits) 0 + ... (Empty spaces) Recursive Scratchpad 7 3 7 8 0 5 5 5 9 0 1 8 9 + 8 7 8 1 7 8 2 4 6 5 9 7 9 : 0 + 7 + 8 = 1 5, = 5, (13 digits) 3 7 8 0 5 5 5 9 0 1 8 9 + 7 8 1 7 8 2 4 6 5 9 7 9 : 1 + 3 + 7 = 1 1, = 1 5, 7 8 0 5 5 5 9 0 1 8 9 + 8 1 7 8 2 4 6 5 9 7 9 : 1 + 7 + 8 = 1 6, = 6 1 5, 8 0 5 5 5 9 0 1 8 9 + 1 7 8 2 4 6 5 9 7 9 : 1 + 8 + 1 = 1 0, = 0 6 1 5, 0 5 5 5 9 0 1 8 9 + 7 8 2 4 6 5 9 7 9 : 1 + 0 + 7 = 8, = 8 0 6 1 5, 5 5 5 9 0 1 8 9 + 8 2 4 6 5 9 7 9 : 0 + 5 + 8 = 1 3, = 3 8 0 6 1 5, 5 5 9 0 1 8 9 + 2 4 6 5 9 7 9 : 1 + 5 + 2 = 8, = 8 3 8 0 6 1 5, 5 9 0 1 8 9 + 4 6 5 9 7 9 : 0 + 5 + 4 = 9, = 9 8 3 8 0 6 1 5, 9 0 1 8 9 + 6 5 9 7 9 : 0 + 9 + 6 = 1 5, = 5 9 8 3 8 0 6 1 5, 0 1 8 9 + 5 9 7 9 : 1 + 0 + 5 = 6, = 6 5 9 8 3 8 0 6 1 5, 1 8 9 + 9 7 9 : 0 + 1 + 9 = 1 0, = 0 6 5 9 8 3 8 0 6 1 5, 8 9 + 7 9 : 1 + 8 + 7 = 1 6, = 6 0 6 5 9 8 3 8 0 6 1 5, 9 + 9 : 1 + 9 + 9 = 1 9, = 9 6 5 9 8 3 8 0 6 1 5, = 1 9 6 5 9 8 3 8 0 6 1 5 Dialogue Data We use GPT3.5 to generate the natural language data.We use the following prompt: Create 20 dialogues between a student and a teacher where the student asks the teacher the sum of two numbers.Only use two numbers!An example of such dialogue is "Student: Hi, can you help me add two numbers, 34 and 432?Teacher: Sure.466 is the answer."The teacher and the student are very talkative, so the dialogue contains longer sentences than my example.Please make sure your number is random and that different dialogues have different styles.The student asks the question in different ways in each dialogue and
S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>F Charton, arXiv:2112.01898Linear algebra with transformers. 2021arXiv preprint</p>
<p>What is my math transformer doing?-three results on interpretability and generalization. F Charton, arXiv:2211.001702022arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>R Csordás, K Irie, J Schmidhuber, arXiv:2110.07732The neural data router: Adaptive control flow in transformers improves systematic generalization. 2021arXiv preprint</p>
<p>N Dziri, X Lu, M Sclar, X L Li, L Jian, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, arXiv:2305.18654Faith and fate: Limits of transformers on compositionality. 2023arXiv preprint</p>
<p>How does gpt-2 compute greater-than?. M Hanna, O Liu, A Variengien, arXiv:2305.00586Interpreting mathematical abilities in a pre-trained language model. 2023arXiv preprint</p>
<p>S Jelassi, S Ascoli, C Domingo-Enrich, Y Wu, Y Li, F Charton, arXiv:2306.15400Length generalization in arithmetic transformers. 2023arXiv preprint</p>
<p>The impact of positional encoding on length generalization in transformers. A Kazemnejad, I Padhi, K N Ramamurthy, P Das, S Reddy, arXiv:2305.194662023arXiv preprint</p>
<p>S Kiyono, S Kobayashi, J Suzuki, K Inui, arXiv:2109.05644Shape: Shifted absolute position embedding for transformers. 2021arXiv preprint</p>
<p>Teaching arithmetic to small transformers. N Lee, K Sreenivasan, J D Lee, K Lee, D Papailiopoulos, arXiv:2307.033812023arXiv preprint</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Y Li, J L Mcclelland, arXiv:2210.004002022arXiv preprint</p>
<p>H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.20050Let's verify step by step. 2023arXiv preprint</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, J Lin, arXiv:2102.130192021arXiv preprint</p>
<p>Openai, arXiv:2309.05463Gpt-4 technical report. 2023arXiv preprint</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. O Press, N A Smith, M Lewis, arXiv:2108.124092021arXiv preprint</p>
<p>Limitations of language models in arithmetic and symbolic induction. J Qian, H Wang, Z Li, S Li, X Yan, arXiv:2208.050512022arXiv preprint</p>
<p>A Ruoss, G Delétang, T Genewein, J Grau-Moya, R Csordás, M Bennani, S Legg, J Veness, arXiv:2305.16843Randomized positional encodings boost length generalization of transformers. 2023arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, Y Lu, S Pan, A Murtadha, B Wen, Y Liu, arXiv:2104.098642021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
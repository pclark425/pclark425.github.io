<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9326 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9326</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9326</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-263829186</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.04270v3.pdf" target="_blank">A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9326.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9326.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HoC_prompt_descriptiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt descriptiveness for Hallmarks of Cancer (HoC) text classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicitly providing class names and textual definitions in the prompt substantially improves zero-shot LLM classification on the HoC dataset; removing definitions or even the class names causes large drops in F1 for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HoC (Hallmarks of Cancer) text classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sentence-level classification of biomedical sentences into one of 10 'hallmarks of cancer' categories or 'empty' if none apply.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompts containing the full list of 10 HoC classes plus their definitions (descriptive prompt). Variations tested: (A) only the names of the 10 classes without definitions; (B) neither the names nor definitions (i.e., less descriptive).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison across prompt variants: full descriptive prompt (baseline) vs (A) names only vs (B) no names/definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Descriptive zero-shot prompt (baseline): F1 = 59.26 (GPT-3.5, Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Names-only prompt: F1 = 46.93; No-class-names prompt: F1 = 38.20 (both for GPT-3.5, reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-12.33 F1 (names-only vs descriptive); -21.06 F1 (no-names vs descriptive) for GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors conclude that for classification tasks, providing descriptive prompts (explicit class names and definitions) supplies task structure and disambiguating information that helps LLMs map inputs to labels; removing these guidance cues degrades performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>HoC has 10 classes; evaluation used hybrid human+automatic parsing. Prompt variations were created by removing definitions and/or class names; reported F1 values are for GPT-3.5 zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9326.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDI_descriptive_prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Descriptive prompting for drug–drug interaction (DDI) relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing more descriptive prompts (including definitions of interaction types) improved performance for some LLMs (notably GPT-3.5 and Claude-2) on the DDI relation extraction dataset with four label types; the effect was model-dependent (PaLM-2 did not benefit).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, Claude-2, PaLM-2 (evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5: null, Claude-2: null, PaLM-2: null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DDI (Drug-Drug Interaction) relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify drug–drug interaction pairs and classify interaction type (mechanism, effect, advice, int) from biomedical text.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot generative prompt with/without descriptive additions; for the DDI dataset prompts included definitions of the four interaction types (i.e., more descriptive).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison: descriptive prompt (with definitions of interaction types) vs less-descriptive prompts (no definitions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With descriptive prompts, GPT-3.5 achieved the best across Precision, Recall, and F1 among tested LLMs and outperformed other LLMs on this dataset (reported as 'state-of-the-performance' for GPT-3.5 in text). Exact metric values not tabulated in the main text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Descriptive prompts helped GPT-3.5 and Claude-2; the same descriptive prompts were not helpful for PaLM-2 (PaLM-2 performed worse with descriptive prompts in this dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More descriptive prompts disambiguated the small label set and definitions, enabling models that can utilize such task-level guidance (GPT-3.5, Claude-2) to map generative output to relation labels; benefits are model-dependent, possibly due to differing pretraining or instruction-following capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>DDI is a small dataset with ~500 training samples; relation evaluation was done via human evaluation (all LLM responses manually annotated). Prompts included definitions of the four interaction types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9326.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BC5CDR_prompt_and_zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt design and zero-shot gains on BC5CDR chemical–disease relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot PaLM-2 (with designed prompts) outperformed the fine-tuned BioGPT SOTA on BC5CDR by a substantial margin (reported improvement of 17.61% F1 for best PaLM-2 run), demonstrating that prompt-driven zero-shot inference can surpass small-data fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 (also compared with Claude-2, LLaMA-2, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BC5CDR chemical–disease relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract relations between chemicals and diseases (drug-induced side-effects) from biomedical passages.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt tailored to relation extraction (instructions and passage concatenation); prompts for this dataset were carefully constructed to elicit relation pairs from passages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison between zero-shot LLMs (PaLM-2, Claude-2, LLaMA-2, GPT-3.5) and a fine-tuned SOTA (BioGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot PaLM-2 achieved the best F1 among LLMs and outperformed the fine-tuned BioGPT SOTA on this dataset by 17.61% F1 (text explicitly reports this improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Other LLMs (Claude-2, LLaMA-2, GPT-3.5) did not reach the same F1; PaLM-2 was notably better and even comparable to or above fine-tuned BioGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+17.61% F1 (best PaLM-2 zero-shot vs BioGPT fine-tuned baseline) on BC5CDR as reported</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that when training data is small (<1000 samples), large pretraining plus appropriate prompting can make zero-shot LLMs more effective than models fine-tuned only on the small dataset; prompt design that reflects task structure aids this.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>BC5CDR has 500 test/validation splits shown; relation outputs were evaluated by human annotators. Prompts were dataset-specific (see Table 2 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9326.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>context_length_summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of input context length on summarization performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Varying the input context length (2000, 5000, 10000 words) affects LLM summarization performance in a model-dependent way: increasing context length decreased PaLM-2 performance, and provided no substantial gains for GPT-3.5 or Claude-2; a 2000-word context worked well overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2, GPT-3.5, Claude-2 (separate experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Biomedical article/dialog/question/answer summarization (various datasets including BioLaySumm, iCliniq, HealthCareMagic, MEDIQA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate concise summaries (lay summaries, abstracts, question/answer summaries) from biomedical texts of varying lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot generative prompts with varying input context lengths: PaLM-2: 2000 and 5000 words; GPT-3.5: 2000, 5000, 10000 words; Claude-2: 2000, 5000, and full document (Claude supports much longer context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison across multiple context-window sizes for each model; LLaMA-2 excluded due to smaller context (≈3000 words).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that increasing context length decreased PaLM-2 performance across summarization tasks; GPT-3.5 and Claude-2 showed no substantive gains by increasing context. Overall, 2000-word input gave good ROUGE and BERTScore results and was cost/time efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>2000-word context served as practical baseline; longer contexts (5000/10000) led to worse ROUGE/BERTScore for PaLM-2 and no reliable improvement for GPT-3.5/Claude-2.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Citing 'Lost in the middle' (Liu et al., 2023a), the authors hypothesize LLMs lose or fail to use information that appears in the middle of very long contexts, which can reduce performance as context grows; longer context also raises cost and latency without guaranteed gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Input context lengths examined: 2000, 5000, 10000 words (depending on model limits). Evaluations used ROUGE and BERTScore; full-document input only possible for Claude-2. Summary datasets included BioLaySumm (eLife/PLOS), iCliniq, HealthCareMagic, MeQSum, MEDIQA variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9326.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>few_shot_effects_claude2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting effects (Claude-2 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding 1 or 3 few-shot examples to prompts has mixed effects: for some tasks and datasets few-shot improves performance (e.g., KD-DTI 1-shot, BC5CDR 3-shot, MediQA-2019 1-shot, PubMedQA 3-shot, some summarization tasks), but in many tasks (NER, Entity Linking and others) few-shot either provided no benefit or degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks: relation extraction (KD-DTI, BC5CDR), question answering (PubMedQA, MediQA-2019), NER, entity linking, summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various biomedical discriminative and generative tasks evaluated under 0-shot, 1-shot, and 3-shot prompt variations.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompt format: include 1 or 3 task examples at the start of the prompt, followed by task description and test sample. Claude-2 selected due to support for very long contexts (100k tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>0-shot vs 1-shot vs 3-shot prompt formats (same examples added into prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported improvements for a subset of tasks: KD-DTI (relation) improved at 1-shot (F1 improvement), BC5CDR improved at 3-shot (F1), MediQA-2019 (QA) improved at 1-shot (Accuracy), PubMedQA improved at 3-shot (Accuracy), and some summarization gains observed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>In many other tasks (notably NER and entity linking) few-shot decreased or did not improve performance compared to 0-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that few-shot benefits are inconsistent; adding examples increases context length (which can harm models with limited context) and may bias outputs; few-shot examples must be high-quality and well-chosen to help. Findings align with prior work (Ye et al., 2023) showing few-shot can sometimes worsen performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot examples were prepended to prompts; Claude-2 used for these experiments due to large context capacity. Reported shot counts were 1-shot and 3-shot; evaluation across multiple datasets (see Table 16).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9326.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>finetune_llama2_7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning small open LLM (LLaMA-2-7B) on small biomedical datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning the LLaMA-2-7B-chat model on small biomedical training sets (several hundred examples) produced larger gains than few-shot prompting and in some cases outperformed zero/few-shot closed-source LLMs and achieved a new SOTA on a summarization task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B-chat (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple small-data tasks: relation extraction (DDI, BC5CDR), question answering (PubMedQA), summarization (MeQSum), etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark biomedical tasks where available fine-tuning data are small (hundreds of training samples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Parameter fine-tuning on the specific dataset (supervised fine-tuning) rather than prompt-only methods; LLaMA-2-7B was fine-tuned for 3 epochs with lr = 2e-5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Fine-tuned LLaMA-2-7B vs zero-shot and few-shot LLMs (GPT-3.5, PaLM-2, Claude-2, LLaMA-2 zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned LLaMA-2-7B outperformed zero-shot and few-shot LLMs in most examined small-data tasks; it set a new state-of-the-art in the summarization dataset and achieved near-SOTA on PubMedQA despite only using ~500 training samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Fine-tuned LLaMA-2-7B beat zero/few-shot LLM runs in most tasks (exception: GPT-3.5 had stronger recall/F1 in DDI in some cases), and matched or approached fully fine-tuned domain-specific SOTA models despite far fewer training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuning adapts model parameters to task distribution more effectively than in-context few-shot examples, especially on small but task-specific datasets; even relatively small fine-tuning datasets produced meaningful gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tuning settings: 3 epochs, learning rate 2e-5; datasets used for fine-tuning included PubMedQA (450 samples), MeQSum (500), DDI (500), BC5CDR (664). Evaluation compared to zero/few-shot LLM runs and existing SOTA models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9326.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NER_BIO_format_limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of BIO-format prompts for NER with generative LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Asking LLMs to output BIO-style token-level labels led to poor NER performance: models could explain the BIO scheme but generally failed to produce task examples in the expected BIO format and achieved much lower F1 than fine-tuned BioBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, PaLM-2, Claude-2, LLaMA-2-13B (evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-2: 13B where specified; others: null</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Biomedical Named Entity Recognition (NER) across multiple datasets (BC2GM, JNLPBA, BC4CHEMD, BC5CDR, NCBI, LINNAEUS, s800)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Token-level sequence labeling of biomedical texts into entity types (e.g., gene, disease, chemical) using BIO tags.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompts instructing LLMs to tag each token using BIO scheme; prompts included a description of BIO format and asked for token-by-token output lines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot BIO-format generative prompt vs fine-tuned BioBERT (SOTA) which uses supervised sequence labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>All LLMs performed significantly worse than BioBERT (SOTA) across precision, recall, and F1; Claude-2 was best among LLMs but still far below SOTA (statistically significant difference, p ≤ .05). Exact per-model F1s are reported in Table 9 (e.g., top LLM F1s in 20s–40s range vs SOTA much higher).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generative zero-shot BIO-format prompting produced markedly lower F1 than fine-tuned BioBERT; LLaMA-2 performed poorest among the LLMs tested.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Generative LLMs struggle with strict sequence-labeling output formats like BIO; while they can reason about the BIO scheme, they fail to reliably produce token-level structured outputs. The absence of task examples in pretraining (task example extraction found no contamination) could further explain poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt included line-by-line tokens and asked for 'token: tag' per line; hybrid evaluation applied (parsing script then human review). Data contamination checks showed LLMs could not extract task examples for NER datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9326.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9326.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>data_contamination_impacts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of dataset contamination / presence of task examples in pretraining on zero-shot performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When LLMs could extract task examples or had membership matches (indications of dataset content in pretraining), those models sometimes performed better on corresponding datasets, suggesting contamination can affect apparent zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (PaLM-2, GPT-3.5, Claude-2, LLaMA-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Contamination detection across multiple tasks/datasets (Relation Extraction KD-DTI/DDI, QA PubMedQA, LitCovid, summarization datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analysis of whether task examples or gold references were present in LLM pretraining using task example extraction and membership inference methods.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Contamination analysis methods: (i) Task Example Extraction for discriminative tasks (can a model reproduce a task example?), (ii) Membership Inference for generative tasks (does model output exactly match a gold reference?).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Interpret performance with/without detected contamination: datasets where extraction/membership was YES vs NO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Where task examples could be extracted: KD-DTI (PaLM-2 could extract examples); DDI (GPT-3.5 and Claude-2 could extract examples); PubMedQA (LLaMA-2 and Claude-2 showed possibility); LitCovid (PaLM-2 showed possibility). In some of these cases, the same models achieved unusually strong performance on those datasets (e.g., LLaMA-2 on PubMedQA). Exact metrics tied to contamination not always quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Presence of dataset examples or exact summaries in pretraining likely inflates zero-shot performance (i.e., contamination can explain unexpectedly strong results). Authors use this to caution interpretation of zero-shot gains and to partially explain dataset-specific model advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Followed Li & Flanigan (2023) contamination checks: task example extraction for discriminative tasks, membership inference for generative summarization; results summarized in Table 18 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Lost in the middle: How language models use long contexts <em>(Rating: 2)</em></li>
                <li>Task contamination: Language models may not be few-shot anymore <em>(Rating: 2)</em></li>
                <li>A comprehensive capability analysis of gpt-3 and gpt-3.5 series models <em>(Rating: 1)</em></li>
                <li>A comprehensive evaluation of ChatGPT on benchmark datasets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9326",
    "paper_id": "paper-263829186",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "HoC_prompt_descriptiveness",
            "name_full": "Prompt descriptiveness for Hallmarks of Cancer (HoC) text classification",
            "brief_description": "Explicitly providing class names and textual definitions in the prompt substantially improves zero-shot LLM classification on the HoC dataset; removing definitions or even the class names causes large drops in F1 for GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0613)",
            "model_size": null,
            "task_name": "HoC (Hallmarks of Cancer) text classification",
            "task_description": "Sentence-level classification of biomedical sentences into one of 10 'hallmarks of cancer' categories or 'empty' if none apply.",
            "presentation_format": "Zero-shot prompts containing the full list of 10 HoC classes plus their definitions (descriptive prompt). Variations tested: (A) only the names of the 10 classes without definitions; (B) neither the names nor definitions (i.e., less descriptive).",
            "comparison_format": "Comparison across prompt variants: full descriptive prompt (baseline) vs (A) names only vs (B) no names/definitions.",
            "performance": "Descriptive zero-shot prompt (baseline): F1 = 59.26 (GPT-3.5, Table 8).",
            "performance_comparison": "Names-only prompt: F1 = 46.93; No-class-names prompt: F1 = 38.20 (both for GPT-3.5, reported in text).",
            "format_effect_size": "-12.33 F1 (names-only vs descriptive); -21.06 F1 (no-names vs descriptive) for GPT-3.5",
            "explanation_or_hypothesis": "Authors conclude that for classification tasks, providing descriptive prompts (explicit class names and definitions) supplies task structure and disambiguating information that helps LLMs map inputs to labels; removing these guidance cues degrades performance substantially.",
            "null_or_negative_result": false,
            "experimental_details": "HoC has 10 classes; evaluation used hybrid human+automatic parsing. Prompt variations were created by removing definitions and/or class names; reported F1 values are for GPT-3.5 zero-shot.",
            "uuid": "e9326.0",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DDI_descriptive_prompting",
            "name_full": "Descriptive prompting for drug–drug interaction (DDI) relation extraction",
            "brief_description": "Providing more descriptive prompts (including definitions of interaction types) improved performance for some LLMs (notably GPT-3.5 and Claude-2) on the DDI relation extraction dataset with four label types; the effect was model-dependent (PaLM-2 did not benefit).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, Claude-2, PaLM-2 (evaluated separately)",
            "model_size": "GPT-3.5: null, Claude-2: null, PaLM-2: null",
            "task_name": "DDI (Drug-Drug Interaction) relation extraction",
            "task_description": "Identify drug–drug interaction pairs and classify interaction type (mechanism, effect, advice, int) from biomedical text.",
            "presentation_format": "Zero-shot generative prompt with/without descriptive additions; for the DDI dataset prompts included definitions of the four interaction types (i.e., more descriptive).",
            "comparison_format": "Comparison: descriptive prompt (with definitions of interaction types) vs less-descriptive prompts (no definitions).",
            "performance": "With descriptive prompts, GPT-3.5 achieved the best across Precision, Recall, and F1 among tested LLMs and outperformed other LLMs on this dataset (reported as 'state-of-the-performance' for GPT-3.5 in text). Exact metric values not tabulated in the main text excerpt.",
            "performance_comparison": "Descriptive prompts helped GPT-3.5 and Claude-2; the same descriptive prompts were not helpful for PaLM-2 (PaLM-2 performed worse with descriptive prompts in this dataset).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "More descriptive prompts disambiguated the small label set and definitions, enabling models that can utilize such task-level guidance (GPT-3.5, Claude-2) to map generative output to relation labels; benefits are model-dependent, possibly due to differing pretraining or instruction-following capabilities.",
            "null_or_negative_result": null,
            "experimental_details": "DDI is a small dataset with ~500 training samples; relation evaluation was done via human evaluation (all LLM responses manually annotated). Prompts included definitions of the four interaction types.",
            "uuid": "e9326.1",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BC5CDR_prompt_and_zero-shot",
            "name_full": "Prompt design and zero-shot gains on BC5CDR chemical–disease relation extraction",
            "brief_description": "Zero-shot PaLM-2 (with designed prompts) outperformed the fine-tuned BioGPT SOTA on BC5CDR by a substantial margin (reported improvement of 17.61% F1 for best PaLM-2 run), demonstrating that prompt-driven zero-shot inference can surpass small-data fine-tuned models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 (also compared with Claude-2, LLaMA-2, GPT-3.5)",
            "model_size": null,
            "task_name": "BC5CDR chemical–disease relation extraction",
            "task_description": "Extract relations between chemicals and diseases (drug-induced side-effects) from biomedical passages.",
            "presentation_format": "Zero-shot prompt tailored to relation extraction (instructions and passage concatenation); prompts for this dataset were carefully constructed to elicit relation pairs from passages.",
            "comparison_format": "Comparison between zero-shot LLMs (PaLM-2, Claude-2, LLaMA-2, GPT-3.5) and a fine-tuned SOTA (BioGPT).",
            "performance": "Zero-shot PaLM-2 achieved the best F1 among LLMs and outperformed the fine-tuned BioGPT SOTA on this dataset by 17.61% F1 (text explicitly reports this improvement).",
            "performance_comparison": "Other LLMs (Claude-2, LLaMA-2, GPT-3.5) did not reach the same F1; PaLM-2 was notably better and even comparable to or above fine-tuned BioGPT.",
            "format_effect_size": "+17.61% F1 (best PaLM-2 zero-shot vs BioGPT fine-tuned baseline) on BC5CDR as reported",
            "explanation_or_hypothesis": "Authors suggest that when training data is small (&lt;1000 samples), large pretraining plus appropriate prompting can make zero-shot LLMs more effective than models fine-tuned only on the small dataset; prompt design that reflects task structure aids this.",
            "null_or_negative_result": false,
            "experimental_details": "BC5CDR has 500 test/validation splits shown; relation outputs were evaluated by human annotators. Prompts were dataset-specific (see Table 2 in the paper).",
            "uuid": "e9326.2",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "context_length_summarization",
            "name_full": "Effect of input context length on summarization performance",
            "brief_description": "Varying the input context length (2000, 5000, 10000 words) affects LLM summarization performance in a model-dependent way: increasing context length decreased PaLM-2 performance, and provided no substantial gains for GPT-3.5 or Claude-2; a 2000-word context worked well overall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2, GPT-3.5, Claude-2 (separate experiments)",
            "model_size": null,
            "task_name": "Biomedical article/dialog/question/answer summarization (various datasets including BioLaySumm, iCliniq, HealthCareMagic, MEDIQA variants)",
            "task_description": "Generate concise summaries (lay summaries, abstracts, question/answer summaries) from biomedical texts of varying lengths.",
            "presentation_format": "Zero-shot generative prompts with varying input context lengths: PaLM-2: 2000 and 5000 words; GPT-3.5: 2000, 5000, 10000 words; Claude-2: 2000, 5000, and full document (Claude supports much longer context).",
            "comparison_format": "Comparison across multiple context-window sizes for each model; LLaMA-2 excluded due to smaller context (≈3000 words).",
            "performance": "Authors report that increasing context length decreased PaLM-2 performance across summarization tasks; GPT-3.5 and Claude-2 showed no substantive gains by increasing context. Overall, 2000-word input gave good ROUGE and BERTScore results and was cost/time efficient.",
            "performance_comparison": "2000-word context served as practical baseline; longer contexts (5000/10000) led to worse ROUGE/BERTScore for PaLM-2 and no reliable improvement for GPT-3.5/Claude-2.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Citing 'Lost in the middle' (Liu et al., 2023a), the authors hypothesize LLMs lose or fail to use information that appears in the middle of very long contexts, which can reduce performance as context grows; longer context also raises cost and latency without guaranteed gains.",
            "null_or_negative_result": true,
            "experimental_details": "Input context lengths examined: 2000, 5000, 10000 words (depending on model limits). Evaluations used ROUGE and BERTScore; full-document input only possible for Claude-2. Summary datasets included BioLaySumm (eLife/PLOS), iCliniq, HealthCareMagic, MeQSum, MEDIQA variants.",
            "uuid": "e9326.3",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "few_shot_effects_claude2",
            "name_full": "Few-shot prompting effects (Claude-2 experiments)",
            "brief_description": "Adding 1 or 3 few-shot examples to prompts has mixed effects: for some tasks and datasets few-shot improves performance (e.g., KD-DTI 1-shot, BC5CDR 3-shot, MediQA-2019 1-shot, PubMedQA 3-shot, some summarization tasks), but in many tasks (NER, Entity Linking and others) few-shot either provided no benefit or degraded performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-2",
            "model_size": null,
            "task_name": "Multiple tasks: relation extraction (KD-DTI, BC5CDR), question answering (PubMedQA, MediQA-2019), NER, entity linking, summarization",
            "task_description": "Various biomedical discriminative and generative tasks evaluated under 0-shot, 1-shot, and 3-shot prompt variations.",
            "presentation_format": "Few-shot prompt format: include 1 or 3 task examples at the start of the prompt, followed by task description and test sample. Claude-2 selected due to support for very long contexts (100k tokens).",
            "comparison_format": "0-shot vs 1-shot vs 3-shot prompt formats (same examples added into prompt).",
            "performance": "Reported improvements for a subset of tasks: KD-DTI (relation) improved at 1-shot (F1 improvement), BC5CDR improved at 3-shot (F1), MediQA-2019 (QA) improved at 1-shot (Accuracy), PubMedQA improved at 3-shot (Accuracy), and some summarization gains observed.",
            "performance_comparison": "In many other tasks (notably NER and entity linking) few-shot decreased or did not improve performance compared to 0-shot.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors note that few-shot benefits are inconsistent; adding examples increases context length (which can harm models with limited context) and may bias outputs; few-shot examples must be high-quality and well-chosen to help. Findings align with prior work (Ye et al., 2023) showing few-shot can sometimes worsen performance.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot examples were prepended to prompts; Claude-2 used for these experiments due to large context capacity. Reported shot counts were 1-shot and 3-shot; evaluation across multiple datasets (see Table 16).",
            "uuid": "e9326.4",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "finetune_llama2_7b",
            "name_full": "Fine-tuning small open LLM (LLaMA-2-7B) on small biomedical datasets",
            "brief_description": "Fine-tuning the LLaMA-2-7B-chat model on small biomedical training sets (several hundred examples) produced larger gains than few-shot prompting and in some cases outperformed zero/few-shot closed-source LLMs and achieved a new SOTA on a summarization task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B-chat (fine-tuned)",
            "model_size": "7B",
            "task_name": "Multiple small-data tasks: relation extraction (DDI, BC5CDR), question answering (PubMedQA), summarization (MeQSum), etc.",
            "task_description": "Benchmark biomedical tasks where available fine-tuning data are small (hundreds of training samples).",
            "presentation_format": "Parameter fine-tuning on the specific dataset (supervised fine-tuning) rather than prompt-only methods; LLaMA-2-7B was fine-tuned for 3 epochs with lr = 2e-5.",
            "comparison_format": "Fine-tuned LLaMA-2-7B vs zero-shot and few-shot LLMs (GPT-3.5, PaLM-2, Claude-2, LLaMA-2 zero-shot).",
            "performance": "Fine-tuned LLaMA-2-7B outperformed zero-shot and few-shot LLMs in most examined small-data tasks; it set a new state-of-the-art in the summarization dataset and achieved near-SOTA on PubMedQA despite only using ~500 training samples.",
            "performance_comparison": "Fine-tuned LLaMA-2-7B beat zero/few-shot LLM runs in most tasks (exception: GPT-3.5 had stronger recall/F1 in DDI in some cases), and matched or approached fully fine-tuned domain-specific SOTA models despite far fewer training examples.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Fine-tuning adapts model parameters to task distribution more effectively than in-context few-shot examples, especially on small but task-specific datasets; even relatively small fine-tuning datasets produced meaningful gains.",
            "null_or_negative_result": false,
            "experimental_details": "Fine-tuning settings: 3 epochs, learning rate 2e-5; datasets used for fine-tuning included PubMedQA (450 samples), MeQSum (500), DDI (500), BC5CDR (664). Evaluation compared to zero/few-shot LLM runs and existing SOTA models.",
            "uuid": "e9326.5",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "NER_BIO_format_limitation",
            "name_full": "Limitations of BIO-format prompts for NER with generative LLMs",
            "brief_description": "Asking LLMs to output BIO-style token-level labels led to poor NER performance: models could explain the BIO scheme but generally failed to produce task examples in the expected BIO format and achieved much lower F1 than fine-tuned BioBERT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, PaLM-2, Claude-2, LLaMA-2-13B (evaluated separately)",
            "model_size": "LLaMA-2: 13B where specified; others: null",
            "task_name": "Biomedical Named Entity Recognition (NER) across multiple datasets (BC2GM, JNLPBA, BC4CHEMD, BC5CDR, NCBI, LINNAEUS, s800)",
            "task_description": "Token-level sequence labeling of biomedical texts into entity types (e.g., gene, disease, chemical) using BIO tags.",
            "presentation_format": "Zero-shot prompts instructing LLMs to tag each token using BIO scheme; prompts included a description of BIO format and asked for token-by-token output lines.",
            "comparison_format": "Zero-shot BIO-format generative prompt vs fine-tuned BioBERT (SOTA) which uses supervised sequence labeling.",
            "performance": "All LLMs performed significantly worse than BioBERT (SOTA) across precision, recall, and F1; Claude-2 was best among LLMs but still far below SOTA (statistically significant difference, p ≤ .05). Exact per-model F1s are reported in Table 9 (e.g., top LLM F1s in 20s–40s range vs SOTA much higher).",
            "performance_comparison": "Generative zero-shot BIO-format prompting produced markedly lower F1 than fine-tuned BioBERT; LLaMA-2 performed poorest among the LLMs tested.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Generative LLMs struggle with strict sequence-labeling output formats like BIO; while they can reason about the BIO scheme, they fail to reliably produce token-level structured outputs. The absence of task examples in pretraining (task example extraction found no contamination) could further explain poor performance.",
            "null_or_negative_result": true,
            "experimental_details": "Prompt included line-by-line tokens and asked for 'token: tag' per line; hybrid evaluation applied (parsing script then human review). Data contamination checks showed LLMs could not extract task examples for NER datasets.",
            "uuid": "e9326.6",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "data_contamination_impacts",
            "name_full": "Effect of dataset contamination / presence of task examples in pretraining on zero-shot performance",
            "brief_description": "When LLMs could extract task examples or had membership matches (indications of dataset content in pretraining), those models sometimes performed better on corresponding datasets, suggesting contamination can affect apparent zero-shot performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (PaLM-2, GPT-3.5, Claude-2, LLaMA-2)",
            "model_size": null,
            "task_name": "Contamination detection across multiple tasks/datasets (Relation Extraction KD-DTI/DDI, QA PubMedQA, LitCovid, summarization datasets)",
            "task_description": "Analysis of whether task examples or gold references were present in LLM pretraining using task example extraction and membership inference methods.",
            "presentation_format": "Contamination analysis methods: (i) Task Example Extraction for discriminative tasks (can a model reproduce a task example?), (ii) Membership Inference for generative tasks (does model output exactly match a gold reference?).",
            "comparison_format": "Interpret performance with/without detected contamination: datasets where extraction/membership was YES vs NO.",
            "performance": "Where task examples could be extracted: KD-DTI (PaLM-2 could extract examples); DDI (GPT-3.5 and Claude-2 could extract examples); PubMedQA (LLaMA-2 and Claude-2 showed possibility); LitCovid (PaLM-2 showed possibility). In some of these cases, the same models achieved unusually strong performance on those datasets (e.g., LLaMA-2 on PubMedQA). Exact metrics tied to contamination not always quantified.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Presence of dataset examples or exact summaries in pretraining likely inflates zero-shot performance (i.e., contamination can explain unexpectedly strong results). Authors use this to caution interpretation of zero-shot gains and to partially explain dataset-specific model advantages.",
            "null_or_negative_result": null,
            "experimental_details": "Followed Li & Flanigan (2023) contamination checks: task example extraction for discriminative tasks, membership inference for generative summarization; results summarized in Table 18 of the paper.",
            "uuid": "e9326.7",
            "source_info": {
                "paper_title": "A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Lost in the middle: How language models use long contexts",
            "rating": 2,
            "sanitized_title": "lost_in_the_middle_how_language_models_use_long_contexts"
        },
        {
            "paper_title": "Task contamination: Language models may not be few-shot anymore",
            "rating": 2,
            "sanitized_title": "task_contamination_language_models_may_not_be_fewshot_anymore"
        },
        {
            "paper_title": "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models",
            "rating": 1,
            "sanitized_title": "a_comprehensive_capability_analysis_of_gpt3_and_gpt35_series_models"
        },
        {
            "paper_title": "A comprehensive evaluation of ChatGPT on benchmark datasets",
            "rating": 1,
            "sanitized_title": "a_comprehensive_evaluation_of_chatgpt_on_benchmark_datasets"
        }
    ],
    "cost": 0.019306,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks
19 Feb 2024</p>
<p>Israt Jahan israt18@yorku.ca 
Department of Biology
York University</p>
<p>Information Retrieval and Knowledge Management Research Lab
York University Toronto
OntarioCanada</p>
<p>MdTahmid Rahman Laskar tahmid20@yorku.ca 
School of Information Technology
York University</p>
<p>Information Retrieval and Knowledge Management Research Lab
York University Toronto
OntarioCanada</p>
<p>Chun Peng cpeng@yorku.ca 
Department of Biology
York University</p>
<p>Jimmy Xiangji Huang jhuang@yorku.ca 
School of Information Technology
York University</p>
<p>Information Retrieval and Knowledge Management Research Lab
York University Toronto
OntarioCanada</p>
<p>A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks
19 Feb 202436A9C335D6A3F4A39C11B878A6E596B0arXiv:2310.04270v3[cs.CL]
Recently, Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks.However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet.To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks.For this purpose, a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets has been conducted.To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain.Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art models when they were finetuned only on the training set of these datasets.This suggests that pre-training on large text corpora makes LLMs quite specialized even in the biomedical domain.We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task.While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.</p>
<p>Introduction</p>
<p>The rapid growth of language models (Rogers et al., 2021) in the field of Natural Language Processing (NLP) in recent years has led to significant advancements in various domains, including the biomedical domain (Kalyan et al., 2022).Although specialized models like BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) (Lee et al., 2020), BioBART (Bidirectional and Auto-Regressive Transformers for the Biomedical Domain) (Yuan et al., 2022a), and BioGPT (Generative Pre-trained Transformer for Biomedical Text Generation and Mining) (Luo et al., 2022a) have shown promising results in the biomedical domain, they require finetuning1 using domain-specific datasets.This finetuning process can be time-consuming due to the requirement of task-specific large annotated datasets.In contrast, zero-shot2 learning (Wang et al., 2019) enables models to perform tasks without the need for fine-tuning on task-specific datasets.</p>
<p>Large Language Models (LLMs) (Zhao et al., 2023) are a class of natural language processing models that have been trained on vast amounts of textual data, making it possible to understand and generate human-like language.In recent years, LLMs such as ChatGPT 3 have demonstrated impressive performance on a range of language tasks, including text classification, question answering, and text summarization.One area where LLMs are not yet deeply investigated is the biomedical text processing and information retrieval domain.While there are vast amount of textual data available in the field of biomedicine, there still remains a scarcity of annotated datasets in this domain.Thus, it is difficult to build suitable models for biomedical tasks that lack large annotated datasets.In this regard, due to the strong zero-shot capabilities of LLMs across various tasks, LLM-powered automated tools can be useful for researchers and practitioners in the biomedical domain to find relevant information and extract insights from this vast corpus of unannotated data.However, despite being evaluated on various traditional NLP tasks, there is a lack of comprehensive studies that evaluate LLMs in the biomedical domain.To this end, this paper aims to evaluate LLMs across benchmark biomedical tasks.</p>
<p>However, the evaluation of LLMs in the biomedical domain would require a proper understanding of the complex linguistic characteristics of biomedical texts.In addition, LLMs are sensitive to prompts (Liu et al., 2023b;Jahan et al., 2023).Thus, for biomedical tasks, the effective construction of prompts is important to best utilize these LLMs in biomedical applications.Under these circumstances, domain-specific knowledge in the biomedical domain could play a pivotal role in improving the performance of LLMs in biomedical tasks.In this regard, we study how to effectively build prompts for LLMs to simulate common tasks in biomedical research, such as document classification, named entity recognition, relation extraction, text summarization, question answering, etc.</p>
<p>Since technologies in medicine and healthcare are critical, it is important to ensure rigorous evaluation before using LLMs in these domains.Thus, this paper will contribute to the understanding of the capabilities and limitations of LLMs in biomedical text processing and information retrieval.Moreover, with a comprehensive evaluation of various powerful LLMs, this paper would lead to the development of new tools and techniques for researchers in this field, which could pave the way to build new applications in healthcare and biomedicine via leveraging LLMs.The major contributions from this study are summarized below:</p>
<p>• A comprehensive evaluation of various LLMs in the biomedical domain, providing insights into their capabilities and limitations for various tasks.More specifically, this study investigates the zero-shot capabilities of LLMs in the Biomedical domain to address the lack of large annotated datasets in this domain.</p>
<p>• Construction of task-specific prompts by understanding the complex linguistic structure of biomedical texts.Our findings based on the extensive performance analysis of LLMs across various biomedical tasks will help researchers and practitioners when building LLM-based applications for the biomedical domain.</p>
<p>• To pave the way for future research on LLMs in the biomedical domain, we will release the code used for pre-processing and parsing of LLM-generated responses, alongside the data (the prompts constructed for LLMs and the LLM-generated responses) here: https:// github.com/tahmedge/llm-eval-biomed.</p>
<p>Related Work</p>
<p>There are a large number of studies on various biomedical tasks, such as biomedical image analysis (Liu et al., 2023c;Rahman et al., 2021;Morid et al., 2021), biomedical text processing (Cohen and Hersh, 2005;Wang et al., 2021), genomic sequence analysis (O'Brien et al., 2018;Ji et al., 2021b), disease diagnosis (Ali et al., 2021), drug discovery (Shaker et al., 2021;Martinelli, 2022;Pandiyan and Wang, 2022), cancer research (Nguyen et al., 2019), vaccine development (Soleymani et al., 2022), etc. Biomedical text processing is closely related to these tasks as it serves as a critical component and enabler by providing automated methods for extracting information from the vast amount of textual data in the biomedical domain.In this section, we mainly review the existing state-of-the-art approaches for processing large amounts of biomedical textual data, that are the most related to our research.In the following, we first briefly review various language models used in recent years in the biomedical domain, followed by a brief review of the LLMs that have been studied in this paper.</p>
<p>Language Models for the Biomedical Domain</p>
<p>In recent years, the effective utilization of transformer-based (Vaswani et al., 2017) NLP models like BERT (Devlin et al., 2019) and GPT (Radford et al., 2019) have led to significant progress in the biomedical domain (Lee et al., 2020;Alsentzer et al., 2019;Beltagy et al., 2019;Gu et al., 2020;Peng et al., 2019;raj Kanakarajan et al., 2021).</p>
<p>BERT leverages the encoder of the transformer architecture, while GPT leverages the decoder of the transformer.In addition to these models, sequenceto-sequence models like BART (Lewis et al., 2019) that leverage both the encoder and the decoder of the transformer have also emerged as a powerful approach in various text generation tasks in the biomedical domain (Yuan et al., 2022a).It has been observed that domain-specific pre-training of these models on the biomedical text corpora followed by fine-tuning on task-specific biomedical datasets have helped these models to achieve state-of-the-art performance in a variety of Biomedical NLP (BioNLP) tasks (Gu et al., 2021).This led to the development of various language models for the biomedical domain, such as BioBERT (Lee et al., 2020), ClinicalBERT (Alsentzer et al., 2019), BioBART (Yuan et al., 2022a), BioElectra (raj Kanakarajan et al., 2021), BioGPT (Luo et al., 2022a), etc.However, one major limitation of using such fine-tuned models is that they require task-specific large annotated datasets, which is significantly less available in the BioNLP domain in comparison to the general NLP domain.In this regard, having a strong zero-shot model could potentially alleviate the need for large annotated datasets, as it could enable the model to perform well on tasks that it was not exclusively trained on.</p>
<p>Large Language Models</p>
<p>In recent years, large autoregressive decoder-based language models like GPT-3 (Brown et al., 2020) have demonstrated impressive few-shot learning capability.With the success of GPT-3 in few-shot scenarios, a new variant of GPT-3 called the In-structGPT model (Ouyang et al., 2022) has been proposed that leverages the reinforcement learning (Kaelbling et al., 1996) from human feedback (RLHF) mechanism.The resulting InstructGPT models (in other words, GPT-3.5) are much better at following instructions than the original GPT-3 model, resulting in an impressive zero-shot performance across various tasks.ChatGPT4 is the latest addition in the GPT-3.5 series models that additionally uses dialog-based instructional data during its training phase.Recently, more decoder-based LLMs such as PaLM5 (Chowdhery et al., 2022;Anil et al., 2023;Singhal et al., 2023), Claude6 , LLaMA7 (Touvron et al., 2023a,b) etc. have been proposed that also achieve impressive performance in a wide range of tasks.All these LLMs including ChatGPT are first pre-trained on a large amount of textual data to predict the next token and then fine-tuned using a process called reinforcement learning from human feedback (RLHF) that leveraged both supervised learning and reinforcement learning techniques.The goal of RLHF was to improve the model's performance and ensure that it provided high-quality responses to user queries.</p>
<p>The supervised learning phase of the RLHF process involved training the model on conversations in which human trainers played both sides: the user and the AI assistant.These conversations were collected from a variety of sources, including chat logs from customer service interactions, social media messages, and chatbots.The supervised learning phase aimed to train the model to produce highquality responses that were contextually relevant to the user's query.Meanwhile, the reinforcement learning phase of the RLHF process aimed to further improve the model's performance by using human trainers to provide feedback on its responses.In this phase, human trainers ranked the responses that the model had created in a previous conversation.These rankings were used to create "reward models" that were used to fine-tune the model further by using several iterations of Proximal Policy Optimization (PPO) (Kaelbling et al., 1996).While these models have demonstrated strong performance in various NLP tasks (Qin et al., 2023;Bang et al., 2023;Yang et al., 2023), they have not been investigated in the biomedical domain yet.To this end, this paper aims to evaluate these powerful LLMs in the biomedical domain.</p>
<p>Biomedical Tasks Description</p>
<p>The biomedical text processing task refers to the use of computational techniques to analyze and extract information from textual data in the field of biomedicine.It can be defined as follows:
T : X → Y (1)
Here, X represents the input text for the given task T , and Y represents the output generated.In the following, the description of the benchmark biomedical text processing tasks that have been studied in this paper along with some examples are demonstrated.</p>
<p>(i) Biomedical Named Entity Recognition: Named Entity Recognition (NER) is the task of identifying named entities like person, location, organization, drug, disease, etc. in a given text (Yadav and Bethard, 2018).In the case of biomedical NER, this task aims to extract the biomedical named entities, such as genes, proteins, diseases, chemicals, etc., from the literature to improve biomedical research.</p>
<p>Example: The patient has been diagnosed with a rare form of cancer and is undergoing chemother-apy treatment with the drug Taxol.</p>
<p>Expected NER classifications:</p>
<p>• NER (Disease): "rare form of cancer".</p>
<p>• NER (Treatment): "chemotherapy".</p>
<p>• NER (Drug): "Taxol".</p>
<p>(ii) Biomedical Relation Extraction: The relation extraction task aims to extract relations between named entities in a given text (Zhong and Chen, 2021).In the biomedical relation extraction task, the aim is to analyze textual data by identifying which gene/variants are responsible for which diseases, which treatment/drug is effective for which disease, as well as identifying drug-drug interactions, etc.</p>
<p>Example:</p>
<p>The patient has been diagnosed with a rare form of cancer and is undergoing chemotherapy treatment with the drug Taxol.</p>
<p>Expected Relation Extractions:</p>
<p>• Relation (Treatment of a Disease): "chemotherapy" is a treatment for "rare form of cancer".</p>
<p>• Relation (Drug used in Treatment): "Taxol" is a drug used in "chemotherapy".</p>
<p>(iii) Biomedical Entity Linking: The entity linking task focuses on linking named entities in a text to their corresponding entries in a knowledge base (Laskar et al., 2022a,b).In the case of the biomedical entity linking task, it involves recognizing and linking biomedical named entities in unstructured text to their correct definitions, e.g., to the corresponding entries in structured knowledge bases or ontologies.</p>
<p>Example: The patient has been diagnosed with a rare form of cancer and is undergoing chemotherapy treatment with the drug Taxol.</p>
<p>Expected Entity Linking: A biomedical entity linking system may link the drug Taxol to the following link: https://chemocare.com/druginfo/taxol.</p>
<p>(iv) Biomedical Text Classification: For a given text, the goal of this task is to classify the text into a specific category.One example to classify a given sentence in one of the 10 hallmarks of cancer taxonomy has been demonstrated below:</p>
<p>Example: "Heterogeneity in DNA damage within the cell population was observed as a function of radiation dose."</p>
<p>Expected Result: Genomic Instability and Mutation.</p>
<p>(v) Biomedical Question Answering: The biomedical question-answering task involves retrieving the relevant answer for the given question related to the biomedical literature, such as scientific articles, medical records, and clinical trials.This task is of great importance as it can help healthcare professionals, researchers, and patients access relevant information quickly and efficiently, which can have a significant impact on patient care, drug development, and medical research.</p>
<p>Example: What is recommended for thalassemia patients ?</p>
<p>• Candidate Answer 1: Chemotherapy may be used to: Cure the cancer, shrink the cancer, and prevent the cancer from spreading.</p>
<p>• Candidate Answer 2: Regular blood transfusions can help provide the body with normal red blood cells containing normal hemoglobin.</p>
<p>Expected Answer: The candidate answer 2 should be retrieved as a relevant answer (Abacha et al., 2019;He et al., 2020).</p>
<p>(vi) Biomedical Text Summarization: The main purpose of the text summarization task is to generate a short concise summary of the given document (El-Kassas et al., 2021).The generation of short summaries of biomedical texts would help reduce the time spent reviewing lengthy electronic health records / patient queries in healthcare forums / doctor-patient conversations, resulting in improving the efficiency of the healthcare system.</p>
<p>Example: Patient is a 62-year-old female with a medical history of hyperlipidemia, osteoarthritis, and previous cerebrovascular accident.She presented with sudden onset of dizziness and palpitations that began a day ago.An electrocardiogram was immediately conducted, which indicated the presence of atrial fibrillation.She was promptly hospitalized for monitoring and commenced on anticoagulation therapy with warfarin and ratecontrolling medications like beta-blockers.</p>
<p>Expected Summary: A 62-year-old female with a history of hyperlipidemia, osteoarthritis, and a previous cerebrovascular accident experienced sudden dizziness and palpitations.An ECG confirmed atrial fibrillation, leading to her hospitalization and treatment with warfarin and beta-blockers.</p>
<p>In this section, we first present our methodology on how we design the prompts for different tasks, followed by describing the LLMs that have been studied in this paper.Afterward, the evaluation pipeline has been demonstrated.An overview of our methodology is also shown in Figure 1.</p>
<p>Prompt Design</p>
<p>For a given test sample X, we first prepare a task instruction T .Then, we concatenate the test sample X with the task instruction T to construct the prompt P .Afterward, the prompt P is given as input to generate the response R. Below, the prompt P that has been constructed for each task depending on the respective dataset has been demonstrated.</p>
<p>(i) NER: For NER, prompts are designed to identify the biomedical named entities in a given text in the BIO format.In our prompts, the description of the BIO format is also added along with the task instructions.For NER, we use the BC2GM (Smith et al., 2008) and JNLPBA (Collier and Kim, 2004) datasets for gene/protein entity recognition, BC4CHEMD (Krallinger et al., 2015) and BC5CDR-CHEM (Li et al., 2016) for drug/chemical entity recognition, BC5CDR-Disease (Li et al., 2016) and NCBI-Disease (Dogan et al., 2014) for disease type entity recognition, LINNAEUS (Gerner et al., 2010) and s800 (Pafilis et al., 2013) for species type entity recognition.The prompts for this task are shown in Table 1.</p>
<p>(ii) Relation Extraction: To identify the possible relation between entities mentioned in a given text, the prompts are designed depending on the dataset.For this purpose, we construct prompts for chemical-disease-relation in the BC5CDR dataset (Li et al., 2016), drug-target-interaction in the KD-DTI dataset (Hou et al., 2022), and drug-druginteraction in the DDI dataset (Herrero-Zazo et al., 2013).The prompts used for these datasets are demonstrated in Table 2.</p>
<p>(iii) Entity Linking: To identify whether LLMs can link named entities to their correct definitions based on their pre-training knowledge, we follow the work of Yuan et al. (Yuan et al., 2022b) for the generative entity linking task by asking LLMs to identify the correct concept names for the named entities.For evaluation, the BC5CDR (Li et al., 2016) dataset for the entity linking of disease/chemical type named entities, the NCBI (Dogan et al., 2014) dataset to link diseases, and the COMETA (Basaldella et al., 2020) dataset to link clinical terms have been used.The sample prompts for this task are shown in Table 3.</p>
<p>(iv) Text Classification: The goal of this task is to classify the type of the given text.In this paper, we use two datasets: (i) the HoC (the Hallmarks of Cancer corpus) dataset (Baker et al., 2016), and (ii) the LitCovid dataset (Chen et al., 2021).The HoC dataset consists of 1580 PubMed abstracts where the goal is to annotate each sentence in the given abstract in one of the 10 currently known hallmarks of cancer.Whereas in the LitCovid dataset, each article is required to be classified in one (or more) of the following 8 categories: Prevention, Treatment, Diagnosis, Mechanism, Case Report, Transmission, Forecasting, and General.Our prompts for these text classification datasets are shown in Table 4.</p>
<p>(v) Question Answering: For the questionanswering task, we also evaluate the performance of LLMs on multiple datasets: (i) the PubMedQA dataset (Jin et al., 2019), and (ii) the MEDIQA-2019 dataset (Abacha et al., 2019).In the Pub-medQA dataset, the question, the reference context, and the answer are given as input to the LLMs to determine whether the answer to the given question can be inferred from the provided reference context with LLMs being prompted to reply either as yes, no, or maybe, as required by the task.In the MEDIQA-2019 dataset, the LLMs are asked to determine whether the retrieved answer for the given question is relevant or not (Laskar et al., 2020).The prompts for this task are shown in Table 5.</p>
<p>(vi) Text Summarization:</p>
<p>The biomedical text summarization task requires the generation of a concise summary of the given biomedical text.To this end, the LLMs are evaluated across a wide range of diverse biomedical summarization tasks, such as healthcare question summarization (MeQ-Sum (Abacha and Demner-Fushman, 2019) and MEDIQA-QS (Abacha et al., 2021) datasets), medical answer summarization (MEDIQA-ANS (Savery et al., 2020) and MEDIQA-MAS (Abacha et al., 2021) datasets), and doctor-patient dialogue summarization (iCliniq and HealthCareMagic datasets (Zeng et al., 2020;Mrini et al., 2021)) to generate short queries for healthcare forums describing patient's medical conditions.In addition, we use vari-Figure 1: An overview of our methodology to evaluate 6 biomedical tasks across 26 datasets in this paper.At first, we construct the prompt for each dataset.Then, we generate the response for each dataset using respective LLMs.Finally, depending on the task, we apply various evaluation techniques.12K / 1K / 1.3K Identify the drug-target interactions in the following passage (along with the interaction type among the following: 'inhibitor', 'agonist', 'modulator', 'activator', 'blocker', 'inducer', 'antagonist', 'cleavage', 'disruption', 'intercalation', 'inactivator', 'bind', 'binder', 'partial agonist', 'cofactor', 'substrate', 'ligand', 'chelator', 'downregulator', 'other', 'antibody', ' Identify the pairs of drug-drug interactions in the passage given below based on one of the following interaction types: (i) mechanism: this type is used to identify drug-drug interactions that are described by their pharmacokinetic mechanism.</p>
<p>(ii) effect: this type is used to identify drug-drug interactions describing an effect.</p>
<p>(iii) advice: this type is used when a recommendation or advice regarding a drug-drug interaction is given.</p>
<p>(iv) int: this type is used when a drug-drug interaction appears in the text without providing any additional information.</p>
<p>[PASSAGE]   ous datasets for biomedical literature summarization (Luo et al., 2022b;Goldsack et al., 2022), such as the Biomedical Text Lay Summarization shared task 2023 (BioLaySumm-2023) datasets (Goldsack et al., 2023).For BioLaySumm-2023, since the gold reference summaries of the test sets are not publicly available as of the writing of this paper, the respective validation sets are used for evaluation.The sample prompts in the summarization datasets are shown in Table 6.</p>
<p>Models</p>
<p>In the following, we describe the 4 popular LLMs that we evaluate in benchmark biomedical datasets and tasks in this paper.</p>
<p>(i) GPT-3.5:GPT-3.5 is an auto-regressive language model based on the transformer (Vaswani et al., 2017) architecture that was pre-trained on a vast amount of textual data via supervised learning alongside reinforcement learning with human feedback.The backbone model behind the first version of ChatGPT was also GPT-3.5, and it is currently one of the base models, behind OpenAI's ChatGPT, alongside GPT-4.The initial training data for GPT-3.5 was obtained from a large corpus of text data that was crawled from the internet.This corpus included a wide range of publicly available text, including articles, books, and websites.Additionally, OpenAI collected data from GPT-3 users to train and fine-tune the model further (Qin et al., 2023;OpenAI, 2023).In this work, we used the OpenAI API for the gpt-3.5-turbo-06138model for GPT-3.5.</p>
<p>(ii) PaLM-2: PaLM-2 (Anil et al., 2023) is also a transformer-based language model that exhibits enhanced multilingual and reasoning capabilities, along with improved computing efficiency.</p>
<p>Evaluation Pipeline</p>
<p>Since LLMs usually generate human-like responses that may sometimes contain unnecessary information while not in a specific format, some tasks are very challenging to evaluate without any human intervention.For instance, in tasks like Relation Extraction, there can be multiple answers.Thus, it would be very difficult to automatically evaluate the performance of LLMs by comparing their response with the gold labels using just an evaluation script.Thus, in this paper, to ensure high-quality evalua-tion, we follow the work of Laskar et al. (Laskar et al., 2023a), where they design different settings for the evaluation of LLMs for different tasks:</p>
<p>i. Automatic Evaluation: Where they evaluate some tasks, such as text summarization via leveraging automatic evaluation scripts.</p>
<p>ii. Human Evaluation: Where they evaluate some discriminative tasks solely by humans, which cannot be evaluated directly based on automatic evaluation scripts.</p>
<p>iii.Hybrid (Human + Automatic) Evaluation:</p>
<p>Where they evaluate some tasks via leveraging both human intervention alongside evaluation scripts.More specifically, this is done by first applying evaluation scripts on the dataset to parse the results from the LLM-generated response, followed by utilizing human intervention if solely depending on the evaluation script cannot parse the results in the expected format.</p>
<p>For discriminative tasks, where parsing of the results from the generated response is required for evaluation, we follow the work of Laskar et al. (Laskar et al., 2023a) and design an evaluation script for the respective dataset to first parse the results and then compare the parsed results with the gold labels.Subsequently, any samples where the script could not parse the result properly were manually reviewed by the human annotators.For NER, Entity Linking, Text Classification, and Question Answering, we evaluate the performance by leveraging this technique (denoted as hybrid evaluation).However, for relation extraction, human intervention is necessary since parsing scripts cannot properly identify the relations found in the generative responses.Thus, for relation extraction, all LLMgenerated responses were manually evaluated by humans.This technique of solely utilizing humans to evaluate LLM-generated response when parsing is not possible was also used in recent literature (Laskar et al., 2023a;Jahan et al., 2023).In our human evaluation, at least two annotators compared the LLM-generated response against the gold labels.Any disagreements were resolved based on discussions between the annotators.</p>
<p>For generative tasks, such as summarization, where the full response generated by LLMs can be used for evaluation instead of parsing the response, we evaluate using automatic evaluation metrics (e.g., ROUGE or BERTScore).</p>
<p>Evaluation Metrics</p>
<p>We use different evaluation metrics for different tasks to ensure a fair comparison of different LLMs with prior state-of-the-art results.For this purpose, the standard evaluation metrics that are used in the literature for benchmarking the performance of different models are selected.Thus, for the relation extraction and named entity recognition tasks, Precision, Recall, and F1 metrics are used, while for entity linking, the Recall@1 metric is used.For Summarization, the ROUGE (Lin, 2004a) and the BERTScore (Zhang et al., 2019) metrics are used.For question answering and text classification, metrics like Accuracy and F1 are used.</p>
<p>Baselines</p>
<p>To compare the performance of the zero-shot LLMs, the current state-of-the-art fine-tuned models are used as the baselines.These baseline models are described below.</p>
<p>(i) BioGPT: The backbone of BioGPT (Luo et al., 2022a) is GPT-2 (Radford et al., 2019), which is a decoder of the transformer (Vaswani et al., 2017).The BioGPT model was trained over PubMed titles and abstracts via leveraging the standard language modeling task.We use the fine-tuned BioGPT models as the baseline for all datasets in the relation extraction task, HoC dataset in the text classification task, and the PubMedQA14 dataset for the question-answering tasks.</p>
<p>(ii) BioBART: It is a sequence-to-sequence model based on the BART (Lewis et al., 2019) architecture where the pre-training process involves reconstructing corrupted input sequences.The main difference between BioBART (Yuan et al., 2022a) and BART is that the former was pre-trained over PubMed abstracts to make it suitable for the biomedical domain tasks.The fine-tuned BioBART model was used as the baseline in all the entity linking datasets and the following biomedical summarization tasks: Dialogue Summarization, Question Summarization, and Answer Summarization.</p>
<p>(iii) BioBERT: It is a domain-specific language representation model (Lee et al., 2020) based on the BERT (Devlin et al., 2019) architecture that was additionally pre-trained on large-scale biomedical corpora (PubMed and PMC abstracts).The fine-tuned BioBERT model achieved state-of-theart performance across different biomedical NER datasets and so it was used as the baseline for all NER datasets in this paper.In addition, it was used as the baseline in the LitCovid dataset for text classifcation.</p>
<p>(iv) ALBERT with disease knowledge infused:</p>
<p>The ALBERT (Lan et al., 2019) model is a variant of the BERT (Devlin et al., 2019)  (vi) PRIMERA: It is a pre-trained model (Xiao et al., 2022) designed to enhance multi-document summarization.It proposes a new pre-training strategy for multi-document summarization by leveraging the longformer-encoder-decoder (Beltagy et al., 2020) for pre-training.In this work, the fine-tuned PRIMERA model is used as the baseline in the Readability-Controlled Summarization task since it is the current state-of-the-art in this task.</p>
<p>Results</p>
<p>In this section, the results for LLMs in various tasks are presented.At first, we present our results in the Relation Extraction task where we utilize human evaluation.Then, we demonstrate our findings in Text Classification, Question Answering, Entity Linking, and NER datasets where hybrid evaluation is conducted.Finally, we present our findings in the Summarization datasets where automatic evaluation is utilized.</p>
<p>(i) Relation Extraction: We compare the performance of LLMs with the current state-of-theart fine-tuned BioGPT (Luo et al., 2022a) 7, we find that in the BC5CDR dataset, while LLaMA-2 achieves the highest recall, PaLM-2 performs the best in terms of Precision and F1.Meanwhile, in terms of F1, the zero-shot PaLM-2, Claude-2, and LLaMA-2 model even outperform the prior stateof-the-art fine-tuned BioGPT in this dataset, with an improvement of 17.61% by the best performing PaLM-2.In the KD-DTI dataset, though GPT-3.5 and Claude-2 achieve high recall, their overall F1-score was quite lower than BioGPT and PaLM-2.Meanwhile, zero-shot PaLM-2 again performs much better while achieving almost similar performance in comparison to the fine-tuned BioGPT in terms of the F1 score.In the DDI dataset, GPT-3.5 achieves state-of-the-performance across all three metrics (Precision, Recall, and F1), followed by Claude-2.Since in the DDI dataset, there are only 4 types of labels, more descriptive prompts are used in this dataset (e.g., providing the definition of different interaction types), which helped GPT-3.5 and Claude-2 to achieve better performance.However, more descriptive prompts were not helpful for PaLM-2 in this dataset.Nonetheless, the impressive results achieved by LLMs in comparison to the prior state-of-the-art results in BC5CDR and DDI datasets demonstrate that in datasets having smaller training sets (both datasets have less than 1000 training samples), LLMs are more effective than even fine-tuned models.Meanwhile, in the KD-DTI dataset that has about 12K training samples, most zero-shot LLMs still achieve comparable performance, with PaLM-2 slightly outperforming the state-of-the-art result.More interestingly, while other LLMs achieve quite poor precision scores in the KD-DTI dataset, PaLM-2 even outperforms the current state-of-the-art result in terms of precision.However, based on paired t-test with p ≤ .05, the performance difference between the LLMs and the current fine-tuned SOTA models in terms of F1 is not statistically significant.</p>
<p>(ii) Text Classification: In terms of Text Classification (see Table 8), the LLM generated responses are evaluated based on Hybrid Evaluation.In comparison to the current state-of-theart models fine-tuned on the respective datasets (BioGPT (Luo et al., 2022a) in HoC and BioBERT (Lee et al., 2020) in LitCovid), it is evident that the zero-shot LLMs perform very poorly in comparison to the state-of-the-art fine-tuned baselines in both datasets.In particular, the performance of Claude-2 was much poorer than other LLMs.Among LLMs, GPT-3.5 and PaLM-2 are generally better, with PaLM-2 being the best performing LLM in both the HoC dataset and the LitCovid dataset.The difference in performance between the best performing PaLM-2 and the worst performing Claude-2 is also statistically significant, based on paired t-test, with p ≤ .05.</p>
<p>We also investigate the effect of prompt tuning by evaluating two new prompts that are less descriptive, i.e., without giving definitions of the HoC classes, or without naming the HoC classes.Below our findings for GPT-3.5 based on prompt variations are demonstrated:</p>
<p>(i) Prompting with only the name of each HoC class is given without any definitions, drops the F1 score to 46.93.</p>
<p>(ii) Prompting without explicitly mentioning the name of 10 HoC classes, drops F1 to 38.20.</p>
<p>This indicates that for classification tasks, descriptive prompts are very helpful in improving the performance of LLMs (see Section 5.4.1 for more details).</p>
<p>(iii) Question Answering: For question answering, we evaluate the performance based on Hybrid Evaluation on two datasets (see Table 8).</p>
<p>In terms of the question-answering task in the PubMedQA dataset, we find that the performance of all LLMs is much lower than the current state-ofthe-art BioGPT model.It should be noted that the BioGPT (Luo et al., 2022a) model which achieves the state-of-the-art result in PubmedQA was additionally trained on the PQA-A (211K instances) and PQA-U (61K instances) splits of the Pub-medQA dataset (along with the PQA-L split which is the dedicated training set of this dataset).While comparing the performance of the closed-source LLMs (GPT-3.5,PaLM-2, Claude-2), we find that they perform almost similarly, with none of them achieving more than 60% accuracy.More interestingly, none of these closed-source LLMs could out- perform the LLaMA-2 model that achieves the best performance among LLMs in this dataset.This is an interesting finding since the LLaMA-2 only has 13B parameters, which is much smaller than the closed-source LLMs.To further investigate how LLaMA-2 achieves superior performance in this dataset, we present the confusion matrix using a heatmap based on the prediction made by different LLMs in Figure 2. From the heatmap, we find that all LLMs except LLaMA-2 make mistakes while predicting the "no" type label, as in most cases the LLMs (GPT-3.5,PaLM-2, Claude-2) ended up predicted with the "yes" type label instead, leading to an overall poor accuracy.</p>
<p>In terms of the question-answering task in the MediQA-2019 dataset, we find that the accuracy from the PubMedQA dataset is increased for GPT-3.5 and Claude-2, while being decreased for the LLaMA-2 and PaLM-2; with the zero-shot GPT-3.5 achieving the best accuracy (73.26).The performance of GPT-3.5 is comparable to the current state-of-the-art accuracy of 79.49 (He et al., 2020) by the ALBERT model (Lan et al., 2019) which was additionally trained in question-answering style on 14K biomedical texts consisting of diseaserelated knowledge followed by being fine-tuned on the MediQA-2019 dataset.To further investigate the performance of LLMs in this dataset, we show the confusion matrix in Figure 3 to find that the best performing LLM in the MediQA-2019 dataset, the GPT-3.5 model was able to classify the Relevant and Not Relevant labels more accurately than other LLMs.Moreover, the reason behind PaLM-2 being the worst performer in this dataset is due to the fact that it predicts most instances as Not Relevant.Paired t-test with p ≤ .05demonstrates that the performance difference between the LLMs in question answering is not statistically significant.</p>
<p>(iv) Entity Linking: All the entity linking datasets are evaluated based on the Hybrid Evaluation technique.For entity linking, we find from Table 8 that Claude-2 outperforms all other LLMs in all three entity linking datasets: BC5CDR, Cometa, and NCBI.In BC5CDR and NCBI, while LLaMA-2 is the second best performing model; the PaLM-2 is found to be the second best performer in the Cometa dataset.Nonetheless, the performance of the second best performing models is still quite below in comparison to the Claude-2 model.This finding suggests that Claude-2 is more useful than other models in biomedical entity linking tasks by effectively retrieving the correct definition from its  pre-training knowledge, although its performance is still much below compared to the current finetuned SOTA models, which is also statistically significant, based on paired t-test with p ≤ .05.</p>
<p>(v) NER: Similar to Entity Linking, we also conduct Hybrid Evaluation for NER and find from Table 9 that Claude-2 again outperforms the rest other LLMs across all NER datasets (also in terms of all evaluation metrics: Precision, Recall, and F1).However, the performance of all LLMs is significantly lower than the current SOTA results (based on paired t-test, this difference in performance is statistically significant, with p ≤ .05),with the performance of LLaMA-2 being the poorest.Such limitations of zero-shot LLMs in NER have also been observed in datasets from the general NLP domain (Laskar et al., 2023a).These findings give a strong indication that generative LLMs need further improvement on sequence labeling tasks like NER using the traditional BIO formatting.</p>
<p>(vi) Summarization: We present the results on the following summarization datasets: Dialog Summarization, Question Summarization, and Answer Summarization in Table 10 and compare with BioBART (Yuan et al., 2022a).For evaluation (Laskar et al., 2022c), we use the following two Automatic Evaluation metrics: (i) the widely used ROUGE (Lin, 2004b) metric, and (ii) the BERTScore (Zhang et al., 2019) metric.For BERTScore, we use the RoBERTa-Large (Liu et al., 2019) model for implementation.For all LLMs, the input context length of 2000 words has been used.</p>
<p>We observe that in terms of the ROUGE metric, all LLMs perform much worse than BioBART in datasets that have dedicated training sets, such as iCliniq, HealthCareMagic, and MeQSum.Meanwhile, they perform on par with BioBART in the MEDIQA-QS dataset.Among LLMs, in general, GPT-3.5 is found to be the best performer in these datasets.More importantly, GPT-3.5 outperforms BioBART in both MEDIQA-ANS and MEDIQA-MAS datasets.Note that MEDIQA-ANS, MEDIQA-MAS, and MEDIQA-QS datasets do not have any dedicated training data and GPT-3.5 and other LLMs usually achieve comparable or even better performance in these datasets compared to the BioBART model fine-tuned on other related datasets (Yuan et al., 2022a).This further confirms that zero-shot LLMs are more useful than domain-specific fine-tuned models in biomedical datasets that lack large training data.</p>
<p>We also present our findings on the biomedical lay summarization task in Table 11 and readability controlled summarization task in Table 12.</p>
<p>For the biomedical lay summarization task, we Table 10: Performance on various summarization datasets.Here, 'R-1', 'R-2', 'R-L' and 'B-S' denote 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', and 'BERTScore', respectively.State-of-the-art (SOTA) results are taken from the BioBART (Yuan et al., 2022a) model.Also, LLaMA-2 refers to its 13b version, similar to other tasks.combine both abstract and article together and give as input to the models till the concatenated text reaches the maximum context length.For this task, we compare the performance of the LLMs in eLife and PLOS datasets.Based on the ROUGE scores, the Claude-2 model is found to be the best performing LLM in the eLife dataset with GPT-3.5 being the best-performing one in the PLOS dataset.However, none of the LLMs could outperform the current state-of-the-art in these datasets.
Dataset Model iCliniq HealthCareMagic MeQSum MEDIQA-QS MEDIQA-MAS MEDIQA-ANS R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S GPT-3.
While the performance of the LLMs is quite low in terms of ROUGE, they achieve much higher scores in terms of BERTScore, which is comparable to the state-of-the-art result.This shows a great discrepancy between the lexical matching based traditional ROUGE scoring and the contextual similarity-based BERTScore metric.</p>
<p>The readability-controlled summarization task contains two sub-tasks: (i) abstract writing, and (ii) lay summary writing.Contrary to the previous task (i.e., biomedical lay summarization task), this time we only give an article as input without the abstract, as required by the task.We find that in writing the abstract of the given article, the Claude-2 model performs the best in terms of all ROUGE scores.However, in terms of BERTScore, GPT-3.5 slightly performs better than Claude-2.Interestingly, we find that in terms of the BERTScore, the GPT-3.5 model even outperforms the ROUGE-based SOTA models in both datasets.This further establishes the limitation of using ROUGE as a metric to evaluate LLMs for summarization (Laskar et al., 2023a).</p>
<p>Since the whole document cannot be given as input at once to these LLMs except Claude-2, we also investigate the performance using the following input context lengths (in terms of number of words); PaLM-2: 2000 and 5000, GPT-3.5: 2000, 5000, and10000, andClaude-2: 2000, 5000, and full input document.Since LLaMA-2 has a maximum context length of 4000 tokens (approximately 3000 words15 ), we exclude LLaMA-2 from this study.The results for both tasks, biomedical lay summarization, and readability controlled summarization, can be found in Table 13 and Table 14, respectively.Our experiments reveal that increasing the context length decreases the performance of PaLM-2 in both tasks across all datasets.Moreover, increasing the context length also does not help GPT-3.5 or Claude-2 to gain any substantial performance gain.This can be explained based on the work of Liu et al. (Liu et al., 2023a), where they find that LLMs tend to lose contextual information with the increase in sequence length, and especially they perform poorly in scenarios when they are required to generate responses based on utilizing the information that appears in the middle of the context.</p>
<p>The experimental results in these article summarization datasets demonstrate that using the context length of 2000 is good enough in terms of ROUGE and BERTScore metrics for both abstract and lay summarization.This context length should also be very helpful in terms of usage cost as well as time efficiency in comparison to using longer contexts (Laskar et al., 2023b) Further performance analysis demonstrates that based on the paired t-test with p ≤ .05, the performance difference in terms of the ROUGE score between all the LLMs and the current fine-tuned SOTA models in the summarization datasets is statistically significant, which also happens in terms of BERTScore for all LLMs except GPT-3.5.</p>
<p>Analysis</p>
<p>In this section, we conduct further analysis on the performance of LLMs based on (i) variations in prompts, (ii) few-shot learning, and (iii) finetuning, alongside analyzing the performance of LLMs based on the (iv) possibility of data contamination.Below, the findings based on this analysis are demonstrated.</p>
<p>Effects of Prompt Variation</p>
<p>The effects of prompt tuning in the HoC dataset have been investigated by evaluating the performance of    15).</p>
<p>Thus, our findings demonstrate that more descriptive prompts yield better results.</p>
<p>Effects of Few-Shot Learning</p>
<p>In the previous analysis, it has been found that variations in prompts, especially the utilization of more descriptive prompts could significantly impact the performance of LLMs in zero-shot scenarios.While the main focus of this work was to conduct zero-shot experiments using LLMs to address the lack of large annotated datasets in the biomedical domain, this section demonstrates the effect of the utilization of few-shot examples in the prompts.Since few-shot learning also leads to an increase in the context length, which is a problem for LLMs that have limited context length, in this paper, the Claude-2 model is selected for the few-shot experiments since it can consider significantly much longer contexts (100k tokens) than other LLMs.Thus, using Claude-2 as the LLM for the few-shot learning experiments also helped us to address the context length issue.In the prompt, the few-shot examples are first included, followed by the task descriptions, as demonstrated in Section 4.1.The results from the few-shot experiments across all datasets are shown in Table 16.Though few-shot learning usually leads to improvements in performance, in many tasks, few-shot learning is also found to be ineffective.For instance, Ye et al. (Ye et al., 2023) demonstrated that in many language processing tasks, few-shot learning using LLMs achieves much poorer results in comparison to zero-shot learning.In our experiments, we also find that while few-shot learning is more effective than zero-shot in some tasks (e.g., better in terms of F1 in KD-DTI (1-shot) and BC5CDR (3-shot) for relation extraction 16 , in terms of Accuracy in MediQA-2019 (1-shot) and PubMedQA (3-shot) for question answering, as well as in some summarization datasets), the opposite happens in other tasks as well (e.g., NER, Entity Linking, etc.).Therefore our findings are consistent with Ye et al. (Ye et al., 2023) to reveal that increasing few-shot examples from 0-shot to 1 or 3-shot does not necessarily improve the performance.</p>
<p>To further improve performance with few-shot, the task examples in few-shot prompts are required to be of high quality to ensure better performance while avoiding possible prediction biases towards the task examples.Thus, future work may investi-</p>
<p>Effects of Fine-Tuning</p>
<p>The few-shot learning experiment demonstrates that adding few-shot examples to the prompt does not lead to any performance gain in most biomedical tasks.Thus, in this section, we investigate whether the fine-tuning of LLMs could lead to performance gain.Since the main motivation of this paper is to investigate how LLMs could be used to address the lack of annotated datasets problem in the biomedical domain, only the datasets that have smaller training sets have been used for the fine-tuning experiment.This makes the finetuning experiment to be also consistent with the motivation of this paper which is to investigate the capability of LLMs in zero-shot scenarios in the biomedical domain to address the lack of large annotated dataset issue.For this reason, the Pub-MedQA dataset for question-answering (only 450 training samples), the MeQSum dataset (500 training samples) for summarization, the DDI (500 training samples), and the BC5CDR (664 training samples) datasets for relation extraction have been used for LLM fine-tuning.Nonetheless, many closed-source LLMs (e.g., PaLM-2, Claude-2) do not support fine-tuning, whereas fine-tuning GPT-3.5 significantly increases the cost during inference17 .Thus, the fine-tuning experiment is conducted with a comparatively smaller open-source LLM: the LLaMA-2-7B-Chat18 model and run for 3 epochs with the learning rate 2e − 5.These hyperparameters are selected since they lead to the best performance in the validation set.The results of the fine-tuning experiment are shown in Table 17.From Table 17, it is quite evident that finetuning is more useful than few-shot learning.In general, fine-tuning outperforms all the zero-shot and few-shot LLMs (except GPT-3.5 in the DDI dataset in terms of Recall and F1, even though the fine-tuned version achieves significantly better precision scores).Meanwhile, in the summarization dataset, the fine-tuned LLaMA-2-7B set a new state-of-the-art result.Moreover, it achieves almost similar performance in comparison to the state-of-the-art in the PubMedQA dataset for the question answering task (even though LLaMA-2-7B was only trained on 500 samples, the current state-of-the-art BioGPT (Luo et al., 2022a) model was trained on 270K samples).</p>
<p>Data Contamination Detection Analysis</p>
<p>We follow the work of Li et al. (Li and Flanigan, 2023) to analyze the possibility of the contamination of the datasets that we study in this paper to evaluate various LLMs.For this purpose, we do the following similar19 to their work (Li and Flanigan, 2023).</p>
<p>i. Task Example Extraction: This contamination detection technique checks whether the task example of a particular dataset (evaluated on discriminative tasks, i.e., nonsummarization) can be extracted from the LLMs that we evaluated in this paper.</p>
<p>ii. Membership Inference: This contamination detection technique checks whether the response generated by LLMs in a particular dataset (evaluated on generation tasks, i.e., summarization) is an exact match of any gold labels in that dataset.</p>
<p>The results of the data contamination detection analysis are shown in Table 18.From Table 18, it can be inferred that in the NER datasets, none of the LLMs could extract the task examples.This could be due to the fact that in our experiments, the LLMs were asked to determine the NER tag for each token based on the 'BIO' format.Meanwhile, the LLMs could potentially be pre-trained differently for the NER task.In our analysis, we also find that while LLMs could explain the NER tasks, they cannot generate the task examples for each dataset in the expected 'BIO format'.The experimental results demonstrate that the possible absence of the task examples in the pre-training data could probably be the reason behind LLMs performing very poorly in all NER datasets.A similar trend is also observed in the Entity Linking datasets where no possibility of data contamination is found based on the task extraction analysis technique.</p>
<p>However, in Relation Extraction, task examples could be extracted in the KD-DTI and the DDI datasets (while the task example extraction approach did not lead to the possibility of data contamination in BC5CDR).In the case of the KD-DTI dataset, the best-performing PaLM-2 model could extract task examples, whereas in the DDI dataset, two of the better-performing LLMs, GPT-3.5 and Claude-2, could also extract task examples.This may indicate that the possible presence of task examples in the LLM training data may be responsible for the improved performance of some LLMs in respective datasets.</p>
<p>In terms of the question answering and the text classification datasets, the task example extraction techniques show no possibility of data contamination in MediQA-2019 and HoC datasets.This is quite surprising for GPT-3.5 in the MediQA-2019 dataset since it achieves performance comparable to the state-of-the-art.While for HoC, it is expected since all LLMs perform much poorer than the stateof-the-art.For the other question-answering and text classification datasets, LLaMA-2 and Claude-2 show the possibility of data contamination in the PubMedQA dataset.This may provide some explanations on why smaller LLaMA-2-13b outperforms other much larger LLMs in this dataset.In the LitCovid dataset, we only find that the PaLM-2 model has the possibility of data contamination (it also achieves the best result in comparison to other LLMs in this dataset).</p>
<p>In the summarization datasets, the contamina-Table 16: Experimental Results for Few-Shot Learning.Here, 'Readability-Controlled', 'ROUGE', and 'BERTScore' are denoted by 'RC', 'R', and 'B-S', respectively.'tion detection analysis is conducted based on the membership inference technique which demonstrates that PaLM-2 is more likely to generate some responses similar to the gold reference summaries, as it shows the possibility of membership inference-based contamination in the highest num-  'TEE' and 'MI', respectively; whereas 'NO' indicates that the possibility of contamination is not found, and 'YES' indicates that there is a possibility of contamination found.ber of datasets (4 out of the 10 summarization datasets).We also find that the HealthcareMagic and the MeQSum datasets are reported as contaminated based on membership inference for all four LLMs.However, in none of these datasets, LLMs could beat the state-of-the-art models (with the results being much lower in comparison to the reported state-of-the-art results).It should also be pointed out that the membership inference shows no possibility of contamination in datasets that are released in 2023.</p>
<p>Conclusions and Future Work</p>
<p>In this paper, we evaluate LLMs in six benchmark benchmark biomedical tasks across 26 datasets.We observe that in datasets that have large training data, zero-shot LLMs usually fail to outperform the fine-tuned state-of-the-art models (e.g., BioBERT, BioGPT, BioBART, etc.).However, they consistently outperform the fine-tuned baselines on tasks where the state-of-the-art results were achieved based on fine-tuning only on smaller training sets.While the LLMs that are studied in this paper are massive language models with a billion of parameters, they are trained on diverse domains and so when evaluating their zero-shot capabilities, they usually fail to outperform various state-of-the-art biomedical task specific fine-tuned models.However, fine-tuning these LLMs even on smaller training sets significantly improves their performance.Thus, it could be useful to train biomedical domainspecific LLMs on biomedical corpora to achieve better performance in tasks related to the biological and the medicine domain.Moreover, our findings demonstrate that the performance of these LLMs may vary across different datasets and tasks, as we did not observe a single LLM outperforming others across all datasets and tasks.Thus, our evaluation in this paper could give a good direction for future research as well as real-world usage while utilizing these LLMs to build task-specific biomedical systems.We also demonstrate that LLMs are sensitive to prompts, as variations in prompts led to a noticeable difference in results.Thus, we believe that our evaluation will help future research while constructing the prompts for LLMs for various tasks.</p>
<p>In the future, we will extend our work to investigate the performance of LLMs on more biomedical tasks (Wang et al., 2021), such as medical code assignment (Ji et al., 2021a), drug design (Monteiro al., 2023), healthcare (Alsentzer et al., 2019), protein sequence (Shah et al., 2021), as well as on low-resource languages (Phan et al., 2023) and problems in information retrieval that require opendomain knowledge (Huang et al., 2005;Huang and Hu, 2009;Yin et al., 2010).We will also explore the ethical implications (e.g., privacy concerns (Khalid et al., 2023)) of using LLMs in the biomedical domain.Moreover, we will extend our work to study the multi-modal LLMs (Team et al., 2023;Chen et al., 2023b;Zhang et al., 2023a,b;Moor et al., 2023) in the biomedical image processing tasks alongside also studying whether finetuning smaller open-source LLMs (Fu et al., 2024) could outperform existing fine-tuned state-of-theart models in the biomedical domain.</p>
<p>language model which requires lower memory consumption and a new self-supervised loss function.He at al.,(He et al., 2020) extends its training mechanism by additionally training ALBERT on 14K biomedical texts in a question-answering fashion via infusing disease knowledge which led to the state-of-theart performance in the MediQA-2019 dataset.The LLMs are compared with this disease knowledge infused version of the ALBERT model in this work.(v)FLAN-T5-XL: FLAN-T5(Chung et al., 2022)   is an extension of the T5(Raffel et al., 2020) model.The T5 model treats each tasks as a sequence to sequence problem.While the architecture of FLAN-T5 is similar to the original T5 model, it leverage instruction fine-tuning instead of traditional finetuning.The FLAN-T5-XL that achieves state-ofthe-art performance in the Biomedical Lay Summarization task is used as the baseline in the eLife and the PLOS datasets to compare LLMs in biomedical lay summarization.</p>
<p>model across 3 datasets for the relation extraction task.The LLM generated responses in the relation extraction task are computed based on Human Evaluation.From the results presented in Table</p>
<p>Figure 2 :
2
Figure 2: Confusion Matrix for different models in the PubMedQA dataset.</p>
<p>Figure 3 :
3
Figure 3: Confusion Matrix for different models in the MediQA-2019 dataset.</p>
<p>Table 1 :
1
Sample Prompts in Different Named Entity Recognition (NER) Datasets.
DatasetTypeData SplitPrompt(Train / Valid / Test)BC2GMNER (GENE/PROTEIN)12574 / 2519 / 5038BC4CHEMDNER (DRUG/CHEMICAL)30682 / 30639 / 26364BC5CDR-CHEMNER (DRUG/CHEMICAL)4560 / 4581 / 4797BC5CDR-NER (DISEASE)4560 / 4581 / 4797DiseaseNER (GENE/PROTEIN)14690 / 3856 / 3856JNLPBANER (SPECIES)11935 / 4078 / 7142LINNAEUSNER (DISEASE)5424 / 923 / 940NCBI-DiseaseNER (SPECIES)5733 / 830 / 1630s800
Below, we provide a biomedical text:[TEXT]You need to identify the [ENTITY] type named entities in the above text.To identify the named entities, please tag each token of the given text in the 'BIO' format as either: 'B' or 'I' or 'O' The BIO format stands for Beginning, Inside, Outside.It provides a way to label individual tokens in a given text to indicate whether they are part of a named entity.In the BIO format, each token in a text is labeled with a tag that represents its role in a named entity.For our case, there are three possible tags: B: it indicates that the token is the beginning of the [ENTITY] type named entity (i.e., the first token of a [ENTITY] type named entity).I: it indicates the token is inside a [ENTITY] type named entity (i.e., any token other than the first token of a [ENTITY] type named entity).O: it indicates that the token is outside any named entity.In other words, it is not part of any named entity.Below, each token of the biomedical text is provided (separated by new line).Now please assign the correct tag to each token.Return your result for each token in a newline in the following format -&gt; token: assigned_tag: [LIST OFLINE SEPARATED TOKENS]</p>
<p>Table 2 :
2
Sample Prompts in Different Relation Extraction Datasets.
DatasetTypeData SplitPrompt(Train / Valid / Test)BC5CDRChemical-Disease500 / 500 / 500Identify each pair of drugs and the drug-induced side-effects (e.g., diseases) in the following passage:Relation Extraction[PASSAGE]KD-DTIDrug-TargetRelation Extraction</p>
<p>Table 3 :
3
Sample Prompts in Different Entity Linking Datasets.In the biomedical text given above, what does the entity between the START and the END token refer to?
DatasetTypeData SplitPrompt(Train / Valid / Test)BC5CDREntity Linking (DISEASE/CHEMICAL)9285 / 9515 / 9654[TEXT_S <START> ENTITY <END> TEXT_E]COMETAEntity Linking (CLINICAL TERMS)13489 / 2176 / 4350NCBIEntity Linking (DISEASE)5784 / 787 / 960</p>
<p>Table 4 :
4
Sample Prompts in Different Text Classification Datasets.Sustaining proliferative signaling: Cancer cells can initiate and maintain continuous cell division by producing their own growth factors or by altering the sensitivity of receptors to growth factors.(ii)Evadinggrowth suppressors: Cancer cells can bypass the normal cellular mechanisms that limit cell division and growth, such as the inactivation of tumor suppressor genes and/or insensitivity to antigrowth signals.(iii)Resisting cell death: Cancer cells develop resistance to apoptosis, the programmed cell death process, which allows them to survive and continue dividing.(iv) Enabling replicative immortality: Cancer cells can extend their ability to divide indefinitely by maintaining the length of telomeres, the protective end caps on chromosomes.(v) Inducing angiogenesis: Cancer cells stimulate the growth of new blood vessels, providing the necessary nutrients and oxygen to support their rapid growth.
DatasetTypeData SplitPrompt(Train / Valid / Test)HoCText Classification9972 / 4947 / 4947The 10 hallmarks of cancer taxonomy with their definitions are given below:(i)
(vi)Activating invasion and metastasis: Cancer cells can invade surrounding tissues and migrate to distant sites in the body, forming secondary tumors called metastases.(vii)Deregulating cellular energetic metabolism: Cancer cells rewire their metabolism to support rapid cell division and growth, often relying more on glycolysis even in the presence of oxygen (a phenomenon known as the Warburg effect).(viii)Avoidingimmunedestruction:Cancercellscan avoid detection and elimination by the immune system through various mechanisms, such as downregulating cell surface markers or producing immunosuppressive signals.(ix)Tumorpromotinginflammation:Chronicinflammationcan promote the development and progression of cancer by supplying growth factors, survival signals, and other molecules that facilitate cancer cell proliferation and survival.(x)Genomeinstabilityandmutation:Cancer cells exhibit increased genomic instability, leading to a higher mutation rate, which in turn drives the initiation and progression of cancer.Classify the sentence given below in one of the above 10 hallmarks of cancer taxonomy (if relevant).If cannot be classified, answer as "empty":[SENTENCE]LitCovidText Classification 16126 / 2305 / 4607 Choose the most appropriate topic(s) for the biomedical article on covid-19 given below from the following options: (i) Prevention, (ii) Treatment, (iii) Diagnosis, (iv) Mechanism, (v) Case Report, (vi) Transmission, (vii) Forecasting, and (viii) General.[ARTICLE]</p>
<p>Table 5 :
5
Sample Prompts in Different Question Answering Datasets.
DatasetTypeData SplitPrompt(Train / Valid / Test)PubMedQAQuestion450 / 50 / 500AnsweringQuestion: [QUESTION]Reference context: [REFERENCE CONTEXT]Answer: [ANSWER]MEDIQA-2019Question1701 / 234 / 1107A retrieved answer for the following question is given below. Identify whether the retrievedAnsweringanswer is relevant to the question or not. Answer as 1 if relevant, otherwise answer as 0.Question: [QUESTION]Retrieved Answer: [TEXT]
For the question, the reference context, and the answer given below, is it possible to infer the answer for that question from the reference context?Only reply as either Yes or No or Maybe.</p>
<p>Table 6 :
6
Sample Prompts in Different Text Summarization tasks.
DatasetTypeData SplitPrompt(Train / Valid / Test)iCliniqDialog24851 / 3105 / 3108Write a very short and concise one line summary of theSummarizationfollowing dialogue as an informal question in a healthcareforum:[DIALOGUE]HealthCare MagicDialog181122 / 22641 / 22642Write a very short and concise one line summary of theSummarizationfollowing dialogue as a question in a healthcare forum:[DIALOGUE]MeQSumQuestion500 / -/ 500Rewrite the following question in a short and conciseSummarizationform:[QUESTION]MEDIQA-QSQuestion-/ 50 / 100Rewrite the following question in a short and conciseSummarizationform:[QUESTION]MEDIQA-MASAnswer-/ 50 / 80For the following question, some relevant answers areSummarizationgiven below. Please write down a short concise answerby summarizing the given answers.Question: [QUESTION]Answer 1: [ANSWER1]Answer 2: [ANSWER2]MEDIQA-ANSAnswer-/ -/ 552Write a very short and concise summary of the followingSummarizationarticle based on the question given below:[QUESTION][ARTICLE]BioLaySumm-2023 (PLOS)Lay24773 / 1376 / 142Write down a readable summary of the following biomed-Summarizationical article using less technical terminology (e.g., lay sum-mary) such that it can be understandable for non-expertaudiences:[ABSTRACT + ARTICLE]BioLaySumm-2023 (eLife)Lay4346 / 241 / 142Write down a readable summary of the following biomed-Summarizationical article using less technical terminology (e.g., lay sum-mary) such that it can be understandable for non-expertaudiences:[ABSTRACT + ARTICLE]BioLaySumm-2023 (PLOS)Readability-controlled24773 / 1376 / 142Write down a readable summary of the following biomed-Summarization (Lay Summary)ical article using less technical terminology (e.g., lay sum-mary) such that it can be understandable for non-expertaudiences:[ARTICLE]BioLaySumm-2023 (PLOS)Readability-controlled24773 / 1376 / 142Write down the abstract of the following biomedical arti-Summarization (Abstract)cle:[ARTICLE]</p>
<p>Table 7 :
7
(Luo et al., 2022a)tion Extraction datasets.All SOTA results are taken from the BioGPT(Luo et al., 2022a)model.
Dataset</p>
<p>Table 8 :
8
(Yuan et al., 2022a)Classification, Question Answering (QA), and Entity Linking datasets.The SOTA results for HoC and PubMedQA are taken from the BioGPT(Luo et al., 2022a) model, while we take the SOTA results from Gutiérrez et al. (2020) andHe et al. (2020)for LitCovid and MediQA-2019, respectively.Note that all SOTA results for Entity Linking are taken from the BioBART(Yuan et al., 2022a)model.
Text Classification Dataset Question Answering DatasetEntity Linking DatasetModelHoCLitCovidPubMedQA MediQA-2019 BC5CDR CometaNCBIF1F1AccuracyAccuracyRecall@1 Recall@1 Recall@1GPT-3.559.2629.6354.4073.2654.9043.4552.19PaLM-261.0337.5059.6052.1252.1448.7638.44Claude-234.937.6057.2065.1378.0153.2970.21LLaMA-2-13b41.8211.3461.4056.0166.5240.6759.17State-of-the-Art (SOTA) 85.1286.2078.2079.4993.2681.7789.90</p>
<p>Table 9 :
9
Performance on Named Entity Recognition datasets.SOTA results are from the BioBERT(Lee et al., 2020)  model.Here, 'Precision' and 'Recall' are denoted by 'P' and 'R', respectively.
Model</p>
<p>Table 11 :
11
Performance on the Biomedical Lay Summarization task.State-of-the-Art results are from Sim et al. (2023).
5 30.5 12.8 25.4 89.3 28.1 9.8 24.0 88.9 30.0 12.3 26.2 89.0 30.6 11.6 26.7 89.0 38.9 14.6 22.1 87.9 28.7 10.4 24.4 89.0PaLM-2 21.9 10.2 18.6 87.0 25.9 9.8 22.0 88.3 31.5 14.0 27.7 89.7 29.7 11.5 26.0 90.0 15.3 8.6 13.5 85.2 25.4 12.1 18.9 85.4Claude-2 28.8 11.0 23.7 89.0 24.4 7.4 20.3 88.2 31.7 13.6 27.9 89.9 32.0 13.5 27.7 90.2 13.4 6.2 11.1 85.6 28.6 8.7 17.6 85.9LLaMA-2 20.0 7.2 15.2 85.8 16.7 5.1 12.9 85.3 21.2 7.3 17.1 85.5 23.3 8.6 17.7 86.2 13.7 11.2 13.2 86.6 28.0 9.6 17.4 85.3SOTA 61.1 48.5 59.4 94.1 46.7 26.1 44.2 91.9 55.6 38.1 53.2 93.3 32.0 12.4 29.7 90.3 32.9 11.3 29.3 86.1 21.6 9.3 19.2 85.7DatasetModeleLifePLOSROUGE-1 ROUGE-2 ROUGE-L BERTScore ROUGE-1 ROUGE-2 ROUGE-L BERTScoreGPT-3.533.888.6417.1584.4941.1111.4121.7486.11PaLM-221.553.9212.1481.0329.617.1016.4083.02Claude-239.209.3118.3484.3039.059.2819.5285.03LLaMA-2-13b38.538.6918.1083.1838.5811.1520.1484.69State-of-the-Art49.5014.6046.9085.5050.2019.0046.2086.50</p>
<p>Table 12 :
12
Performance on Readability Controlled Summarization in the PLOS dataset.State-of-the-Art results are from Chen et al. (2023a).
Summarization Type</p>
<p>Table 13 :
13
Performance of different LLMs on Biomedical Lay Summarization datasets based on various input lengths.
Dataset</p>
<p>Table 14 :
14
Performance of different LLMs on Readability Controlled Summarization in the PLOS dataset based on various input lengths.
Summarization Type</p>
<p>Table 15 :
15
Effects of Prompt Variations in GPT-3.5 for the Document Classification Task in the HoC dataset.</p>
<h1>Prompt</h1>
<p>Table 18 :
18
Data Contamination Detection Analysis.Here, 'Task Example Extraction' and 'Membership Inference' are denoted by</p>
<p>Fine-tuning means providing good amount (e.g., thousands of samples) of training examples to re-train a pre-trained language model on a specific task.
2 Zero-shot learning means asking a trained model to complete a task without providing any explicit examples of that particular
task. 3 https://chat.openai.com/
https://openai.com/blog/chatgpt
https://ai.google/discover/palm2/
https://www.claudeai.ai/
https://ai.meta.com/blog/ large-language-model-llama-meta-ai/
https://platform.openai.com/docs/models/ gpt-3-5
https://bard.google.com/
https://cloud.google.com/vertex-ai/docs/ generative-ai/model-reference/text
https://www.anthropic.com/index/claude-2
https://ai.meta.com/llama/
We used the following version of LLaMA-2-13B: https: //huggingface.co/meta-llama/Llama-2-13b-chat-hf, which achieves improved factual correctness than its based version. As we are benchmarking LLMs in the biomedical domain, selecting a more faithful model is prioritized.
In PubMedQA, BioGPT was additionally fine-tuned on more than 270K instances.
https://help.openai.com/en/articles/ 4936856-what-are-tokens-and-how-to-count-them
Few-shot learning leads to a decrease in performance in terms of Recall in comparison to zero-shot learning in all relation extraction dataests.
https://openai.com/blog/ gpt-3-5-turbo-fine-tuning-and-api-updates
https://huggingface.co/meta-llama/ Llama-2-7b-chat-hf
We did not compare the performance of LLMs based on the chronological analysis (which was also used byLi et al.  in (Li and Flanigan,<br />
)) since most of the classification datasets that have been used in this paper came before the data cut-off date of different LLMs.
AcknowledgementWe would like to thank the handling editor and all the five reviewers of the Computers in Biology and Medicine journal for their excellent review comments.This research is supported by the research grant (RGPIN-2020-07157) from the Natural Sciences and Engineering Research Council (NSERC) of Canada, the York Research Chairs (YRC) program, and the Generic (Minor/Startup/Other) research fund of York University.We also acknowledge Compute Canada for providing us with the computing resources to conduct experiments, as well as Anthropic for providing us early access to the Claude-2 API.Task &amp; Dataset GPT-3.5 PaLM-2 Claude-2 LLaMA-2-13B NER TEE TEE TEE TEE BC2GM (2008) No No No No BC4CHEMD (2016) No No No No BC5CDR-chem (2015) No No No No BC5CDR-disease (2014) No No No No JNLPBA (2004) No No No No NCBI-disease (2016) No No No No linnaeus (2010) No No No No s800 (2013) No No No No Relation Extraction TEE TEE TEE TEE BC5CDR (2016) No No No No KD-DTI (2022) No Yes No No DDI (2013) Yes No Yes No Entity Linking TEE TEE TEE TEE BC5CDR No No No No Cometa No No No No NCBI No No No No Question Answering TEE TEE TEE TEE PubMedQA (2019) No No Yes Yes MediQA-2019 (2019) No No No No Text Classification TEE TEE TEE TEE HoC (2016) No No No No LitCovid (2020) No Yes No No Summarization MI MI MI MI iCliniq (2020) No Yes Yes No HealthCareMagic (2020) Yes Yes Yes Yes MeQSum (2019) Yes Yes Yes Yes MEDIQA-QS (2021) No No No No MEDIQA-ANS (2020) No Yes No No MEDIQA-MAS (2021) No No No No eLife (Lay Summ) (2023) No No No No PLOS (Lay Summ) (2023) No No No No PLOS (RC: Abstract) (2023) No No No No PLOS (RC: Lay Summ) (2023) No No No No
Dataset Claude-2 (0-Shot) Claude-2 (1-Shot). </p>
<p>Claude-2 (3-Shot). </p>
<p>SOTA NER Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1. BC2GM 31.95 55.10 40.45</p>
<p>Relation Extraction Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 BC5CDR. </p>
<p>Text Classification F1 F1 F1 F1. </p>
<p>Experimental Results for Fine-Tuning. Here, 'ROUGE' and 'BERTScore' are denoted by 'R' and 'B-S', respectively. Relation Extraction Task QA Task Summarization Task Model BC5CDR DDI PubMedQA MeQSum Precision Recall F1 Precision Recall F1 Accuracy R-1 R-1. R-L B-S , Table. 17</p>
<p>Claude-2 (0-Shot). </p>
<p>LLaMA-2-13b (0-Shot). </p>
<p>Claude-2 (1-Shot). </p>
<p>Claude-2 (3-Shot). </p>
<p>LLaMA-2-7b. Fine-Tuned</p>
<p>On the summarization of consumer health questions. References Asma, Ben Abacha, Dina Demner-Fushman, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Overview of the mediqa 2021 shared task on summarization in the medical domain. Asma Ben Abacha, M' Yassine, Yuhao Rabet, Chaitanya Zhang, Curtis Shivade, Dina Langlotz, Demner-Fushman, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language Processing2021</p>
<p>Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering. Asma Ben Abacha, Chaitanya Shivade, Dina Demner-Fushman, Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared Task2019</p>
<p>Heart disease prediction using supervised machine learning algorithms: Performance analysis and comparison. Bikash Md Mamun Ali, Kawsar Kumar Paul, Francis M Ahmed, Julian Mw Bui, Mohammad Quinn, Moni Ali, Computers in Biology and Medicine. 1361046722021</p>
<p>Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew Mcdermott, arXiv:1904.03323Publicly available clinical bert embeddings. 2019arXiv preprint</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Automatic semantic classification of scientific literature according to the hallmarks of cancer. Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Högberg, Ulla Stenius, Anna Korhonen, Bioinformatics. 3232016</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>COMETA: A corpus for medical entity linking in the social media. Marco Basaldella, Fangyu Liu, Ehsan Shareghi, Nigel Collier, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Ncuee-nlp at biolaysumm task 2: Readabilitycontrolled summarization of biomedical articles using the primera models. Chao-Yi Chen, Jen-Hao Yang, Lung-Hao Lee, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023a</p>
<p>Litcovid: an open database of covid-19 literature. Qingyu Chen, Alexis Allot, Zhiyong Lu, Nucleic acids research. 49D12021</p>
<p>Gpt-4 vision on medical image classification-a case study on covid-19 dataset. Ruibo Chen, Tianyi Xiong, Yihan Wu, Guodong Liu, Zhengmian Hu, Lichang Chen, Yanshuo Chen, Chenxi Liu, Heng Huang, arXiv:2310.184982023barXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>A survey of current work in biomedical text mining. M Aaron, William R Cohen, Hersh, Briefings in bioinformatics. 612005</p>
<p>Introduction to the bio-entity recognition task at jnlpba. Nigel Collier, Jin-Dong Kim, Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP). the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)2004</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies2019</p>
<p>Ncbi disease corpus: a resource for disease name recognition and concept normalization. Rezarta Islamaj Dogan, Robert Leaman, Zhiyong Lu, Journal of biomedical informatics. 472014</p>
<p>Automatic text summarization: A comprehensive survey. Wafaa S El-Kassas, Ahmed A Cherif R Salama, Rafea, Hoda, Mohamed, 2021165113679Expert systems with applications</p>
<p>Xue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khasanova, Cheng Chen, Shashi Bhushan, T N , arXiv:2402.00841Tiny titans: Can smaller large language models punch above their weight in the real world for meeting summarization?. 2024arXiv preprint</p>
<p>Linnaeus: a species name identification system for biomedical literature. Martin Gerner, Goran Nenadic, Casey M Bergman, BMC bioinformatics. 1112010</p>
<p>Biolaysumm 2023 shared task: Lay summarisation of biomedical research articles. Tomas Goldsack, Zheheng Luo, Qianqian Xie, Carolina Scarton, Matthew Shardlow, Sophia Ananiadou, Chenghua Lin, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023</p>
<p>Making science simple: Corpora for the lay summarisation of scientific literature. Tomas Goldsack, Zhihao Zhang, Chenghua Lin, Carolina Scarton, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, arXiv:2007.157792020arXiv preprint</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, ACM Transactions on Computing for Healthcare. 312021</p>
<p>Document classification for covid-19 literature. Jiménez Bernal, Jucheng Gutiérrez, Dongdong Zeng, Ping Zhang, Yu Zhang, Su, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition. Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, James Caverlee, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions. María Herrero-Zazo, Isabel Segura-Bedmar, Paloma Martínez, Thierry Declerck, Journal of biomedical informatics. 4652013</p>
<p>A bayesian learning approach to promoting diversity in ranking for biomedical information retrieval. Yutai Hou, Yingce Xia, Lijun Wu, Shufang Xie, Yang Fan, Jinhua Zhu, Tao Qin, Tie-Yan Liu, Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. the 32nd international ACM SIGIR conference on Research and development in information retrievalXiangji Huang and Qinmin Hu2022. 200938Discovering drug-target interaction knowledge from biomedical literature</p>
<p>York university at TREC 2005: Genomics track. Xiangji Huang, Ming Zhong, Luo Si, Proceedings of the Fourteenth Text REtrieval Conference, TREC 2005. the Fourteenth Text REtrieval Conference, TREC 2005Gaithersburg, Maryland, USA2005. November 15-18, 2005NIST Special Publication. National Institute of Standards and Technology (NIST)</p>
<p>Evaluation of ChatGPT on biomedical tasks: A zero-shot comparison with finetuned generative transformers. Israt Jahan, Md Tahmid Rahman, Chun Laskar, Jimmy Peng, Huang, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Does the magic of bert apply to medical code assignment? a quantitative study. Shaoxiong Ji, Matti Hölttä, Pekka Marttinen, Computers in biology and medicine. 1391049982021a</p>
<p>Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri, Bioinformatics. 37152021b</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Reinforcement learning: A survey. Leslie Pack, Kaelbling Michael L Littman, Andrew W Moore, Journal of artificial intelligence research. 41996</p>
<p>Ammu: a survey of transformer-based biomedical pretrained language models. Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha, Journal of biomedical informatics. 1261039822022</p>
<p>Privacypreserving artificial intelligence in healthcare: Techniques and applications. Nazish Khalid, Adnan Qayyum, Muhammad Bilal, Ala Al-Fuqaha, Junaid Qadir, Computers in Biology and Medicine. 1068482023</p>
<p>The chemdner corpus of chemicals and drugs and its annotation principles. Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, Journal of cheminformatics. 712015</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. 2019</p>
<p>A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Huang, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, Canada2023aAssociation for Computational Linguistics</p>
<p>An auto encoder-based dimensionality reduction technique for efficient entity linking in business phone conversations. Md Tahmid, Rahman Laskar, Cheng Chen, Jonathan Johnston, Xue-Yong Fu, Shashi Bhushan, T N , Simon Corston-Oliver, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022a</p>
<p>Blink with elasticsearch for efficient entity linking in business conversations. Md Tahmid, Rahman Laskar, Cheng Chen, Aliaksandr Martsinovich, Jonathan Johnston, Xue-Yong Fu, Shashi Bhushan Tn, Simon Corston-Oliver, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track2022b</p>
<p>Building real-world meeting summarization systems using large language models: A practical perspective. Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan Tn, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track2023b</p>
<p>Domain adaptation with pre-trained transformers for query-focused abstractive text summarization. Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Xiangji Huang, Computational Linguistics. 4822022c</p>
<p>Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task. Md Tahmid Rahman Laskar, Xiangji Huang, Enamul Hoque, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation Conference2020</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv:2312.16337Task contamination: Language models may not be few-shot anymore. Changmao Li, Jeffrey Flanigan, 2019. 2023arXiv preprint</p>
<p>Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, Zhiyong Lu, 2016. 2016Database</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004a</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004b</p>
<p>Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023aarXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023b</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692RoBERTa: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Zhaoshan Liu, Qiujie Lv, Ziduo Yang, Yifan Li, Chau , Hung Lee, Lei Shen ; Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Computers in Biology and Medicine. 6232023c. 2022aBriefings in Bioinformatics</p>
<p>Readability controllable biomedical document summarization. Zheheng Luo, Qianqian Xie, Sophia Ananiadou, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>Generative machine learning for de novo drug discovery: A systematic review. Dominic D Martinelli, Computers in Biology and Medicine. 1451054032022</p>
<p>Fsm-ddtr: End-to-end feedback strategy for multi-objective de novo drug design using transformers. Tiago O Nelson Rc Monteiro, Ana Pereira, Catarina, José L Machado, Maryam Oliveira, Joel P Abbasi, Arrais, Computers in Biology and Medicine. 1641072852023</p>
<p>Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Machine Learning for Health (ML4H). PMLR2023Med-flamingo: a multimodal medical few-shot learner</p>
<p>A scoping review of transfer learning research on medical image analysis using imagenet. Mohammad Amin Morid, Alireza Borjali, Guilherme Del, Fiol , Computers in biology and medicine. 1281041152021</p>
<p>A gradually soft multi-task and data-augmented approach to medical question understanding. Khalil Mrini, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Walter Chang, Emilia Farcas, Ndapandula Nakashole, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Wnt/β-catenin signalling in ovarian cancer: Insights into its hyperactivation and function in tumorigenesis. Vu Hong, Loan Nguyen, Rebecca Hough, Stefanie Bernaudo, Chun Peng, Journal of ovarian research. 122019</p>
<p>Overview of microrna biogenesis, mechanisms of actions, and circulation. Heyam Jacob O'brien, Yara Hayder, Chun Zayed, Peng, Frontiers in endocrinology. 94022018</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>The species and organisms resources for fast and accurate identification of taxonomic names in text. Evangelos Pafilis, Lucia Sune P Frankild, Sarah Fanini, Christina Faulwetter, Aikaterini Pavloudi, Christos Vasileiadou, Lars Juhl Arvanitidis, Jensen, PloS one. 86e653902013</p>
<p>A comprehensive review on recent approaches for cancer drug discovery associated with artificial intelligence. Sanjeevi Pandiyan, Li Wang, Computers in Biology and Medicine. 1061402022</p>
<p>Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. Yifan Peng, Shankai Yan, Zhiyong Lu, Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared Task2019</p>
<p>Enriching biomedical knowledge for low-resource language through large-scale translation. Long Phan, Tai Dang, Hieu Tran, Trieu Trinh, Vy Phan, Lam Chau, Minh-Thang Luong, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational Linguistics2023</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, arXiv:2302.06476Is chatgpt a general-purpose natural language processing task solver? arXiv preprint. 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Exploring the effect of image enhancement techniques on covid-19 detection using chest xray images. Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin, Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed, M Susu, Muhammad Zughaier, Salman Khan, Computers in biology and medicine. 1321043192021</p>
<p>Bioelectra: pretrained biomedical text encoder using discriminators. Bhuvana Kamal Raj Kanakarajan, Malaikannan Kundumani, Sankarasubbu, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language Processing2021</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works. 20218</p>
<p>Question-driven summarization of answers to consumer health questions. Max Savery, Asma Ben Abacha, Soumya Gayen, Dina Demner-Fushman, Scientific Data. 20207</p>
<p>Gt-finder: Classify the family of glucose transporters with pre-trained bert language models. Syed Muazzam, Ali Shah, Semmy Wellem Taju, Quang-Thai Ho, Yu-Yen Ou, Computers in biology and medicine. 1311042592021</p>
<p>In silico methods and tools for drug discovery. Bilal Shaker, Sajjad Ahmad, Jingyu Lee, Chanjin Jung, Dokyun Na, Computers in biology and medicine. 1371048512021</p>
<p>Csiro data61 team at biolaysumm task 1: Lay summarisation of biomedical research articles using generative models. Xiang Mong Yuan Sim, Maciej Dai, Sarvnaz Rybinski, Karimi, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 2023</p>
<p>Overview of biocreative ii gene mention recognition. Larry Smith, Lorraine K Tanabe, Cheng-Ju Kuo, I Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, Manabu Torii, Genome biology. 922008</p>
<p>An overview of progress from empirical to rational design in modern vaccine development, with an emphasis on computational tools and immunoinformatics approaches. Safoura Soleymani, Amin Tavassoli, Mohammad Reza Housaindokht, Computers in biology and medicine. 1401050572022</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 2017</p>
<p>Pretrained language models in biomedical domain: A systematic survey. Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie Fu, 2021ACM Computing Surveys</p>
<p>A survey of zero-shot learning: Settings, methods, and applications. Wei Wang, Vincent W Zheng, Han Yu, Chunyan Miao, ACM Transactions on Intelligent Systems and Technology (TIST). 1022019</p>
<p>PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization. Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan, 10.18653/v1/2022.acl-long.360Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022</p>
<p>A survey on recent advances in named entity recognition from deep learning models. Vikas Yadav, Steven Bethard, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational Linguistics2018</p>
<p>Exploring the limits of chatgpt for query or aspect-based text summarization. Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, Wei Cheng, arXiv:2302.080812023arXiv preprint</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, arXiv:2303.10420A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. 2023arXiv preprint</p>
<p>A survival modeling approach to biomedical search result diversification using wikipedia. Xiaoshi Yin, Jimmy Xiangji Huang, Xiaofeng Zhou, Zhoujun Li, Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. the 33rd international ACM SIGIR conference on Research and development in information retrieval2010</p>
<p>Biobart: Pretraining and evaluation of a biomedical generative language model. Hongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, Sheng Yu, Proceedings of the 21st Workshop on Biomedical Language Processing. the 21st Workshop on Biomedical Language Processing2022a</p>
<p>Generative biomedical entity linking via knowledge baseguided pre-training and synonyms-aware fine-tuning. Hongyi Yuan, Zheng Yuan, Sheng Yu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022b</p>
<p>Meddialog: Large-scale medical dialogue datasets. Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Large-scale domain-specific pretraining for biomedical vision-language processing. Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, arXiv:2303.009152023aarXiv preprint</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2019</p>
<p>Pmc-vqa: Visual instruction tuning for medical visual question answering. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, Weidi Xie, ; Wayne Xin, Kun Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2305.10415arXiv:2303.18223A survey of large language models. 2023b. 2023arXiv preprint</p>
<p>A frustratingly easy approach for entity and relation extraction. Zexuan Zhong, Danqi Chen, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6796 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6796</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6796</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-278237785</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.00001v1.pdf" target="_blank">Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs’ logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateau-ing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6796.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6796.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source high-performing large language model (LLM) from OpenAI family used in this paper as the base model for fine-tuning on a propositional logic benchmark (Rosetta-PL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art, closed-source generative pre-trained transformer family model used by the authors as the fine-tuning target; described as high-performing on reasoning benchmarks (MMLU, GSM8K, Big Bench Hard).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on Rosetta-PL (25,214 translated Lean problems; subsets of 20,000 and 10,000 also used); pretraining corpora not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised fine-tuning on a custom propositional logic dataset (Rosetta-PL); evaluation contrasted preserved-symbol translations vs arbitrary-symbol translations to probe pattern learning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Rosetta-PL (this work); Lean Workbook and Mini-f2f used as sources/benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Rosetta-PL: a custom propositional-logic benchmark created by translating the Lean Workbook into a bespoke propositional language (25,214 problems). Testing included 'seen' subsets from training and 'unseen' problems sampled from Mini-f2f.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Propositional logical statement evaluation (truth value classification) / pattern learning in a formal propositional language</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (fraction of correctly answered queries); reported with empirical uncertainties</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Seen (translated propositional language average): 95.97% ± 0.33% vs Seen (Lean/untranslated): 76.08% ± 0.36%; Unseen Lean: 99.89% ± 0.06%; Unseen translated (Translation Key 1): 97.56% ± 0.44%; Translation Key 2 (translated) overall: 64.1% ± 0.75%; Average across tests: Translation Key 1 = 92.68%, Translation Key 2 = 80.36%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuned GPT-4o outperforms base GPT-4o on translated examples; base GPT-4o retains pretraining Lean knowledge and sometimes performs better on unseen Lean examples. Translation Key 1 finetuned models outperform Translation Key 2 finetuned models substantially (e.g., Key1 ∼92.7% vs Key2 ∼80.4% average).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning GPT-4o on a propositional language yields very high accuracy on seen translated problems and strong generalization when the translation preserves logical relationships (Translation Key 1). Performance plateaus near ≈20,000 training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model is closed-source (leakage risk); no model size or internal architecture details provided; overfitting risk on larger training sets; poor generalization when translation disrupts logical relationships (Translation Key 2); plateauing performance suggests dataset redundancy or capacity limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6796.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rosetta-PL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rosetta-PL (Propositional Logic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper that translates the Lean Workbook into a custom propositional language to evaluate LLMs' strict logical reasoning and pattern-generalization in a controlled setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (benchmark/dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>dataset/benchmark (propositional logic format)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Derived from Lean Workbook (Ying et al., 2024); 25,214 translated problems total. 'Seen' test subsets (3×500 each) sampled from training, 'unseen' tests (3×200 each) sampled from Mini-f2f.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used as supervised fine-tuning target to elicit LLM internalization of propositional patterns rather than relying on natural-language cues.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Rosetta-PL</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Controlled propositional-language dataset of truth-evaluation problems translated from Lean; designed to remove natural-language ambiguity and require pattern discovery rather than following predefined inference templates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Propositional statement truth classification and pattern generalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy on seen and unseen splits (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fine-tuned GPT-4o: seen translated average 95.97% ± 0.33%; seen Lean 76.08% ± 0.36%; unseen Lean 99.89% ± 0.06%; unseen translated (Key1) 97.56% ± 0.44%; Key2 translated 64.1% ± 0.75%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>When used to fine-tune GPT-4o, Rosetta-PL improved performance on translated pattern recognition relative to base model; preserving logical relationships in the benchmark (Key1) yielded superior gains over arbitrary transformations (Key2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A propositional, formally-translated benchmark isolates logical pattern learning and shows that preserving relational structure during translation markedly improves LLM learning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Possible dataset biases and redundancy; high unseen accuracy may reflect over-representation of particular problem types; translation strategies can create artifacts that either help or hurt generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6796.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translation Key 1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translation Key 1 (Focused Key, preserves logical relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A translation strategy introduced in this work that maps Lean symbols to new symbols while preserving logical relationships and consistent mapping of related symbols, including a scrambling duplication trick to simulate syntactic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (translation/method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>symbol-mapping + text transformation strategy</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Applied to the entire Rosetta-PL training set (25,214 problems) in the primary fine-tuning runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Preserves semantic relationships during translation so that models can learn consistent symbol-to-meaning mappings in the propositional language.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Rosetta-PL (translation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A translated variant of Lean problems which maintains logical relationships between tokens/symbols to encourage learnability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Enabling supervised learning of logical pattern correspondences during propositional truth classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average accuracy with Translation Key 1 across tests: 92.68% (paper reports fine-tuned model achieved seen translated average 95.97%; unseen translated 97.56%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed Translation Key 2 by a large margin (Key1 average 92.68% vs Key2 average 80.36%); fine-tuned Key1 models outperform base GPT-4o on translated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Preserving logical relationships in symbolic translation substantially improves an LLM's ability to learn and generalize formal logical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The scrambling/duplication introduced to mimic syntactic variation could introduce artifacts; success may partly depend on pretraining familiarity with Lean-like syntax; does not guarantee transfer when translation deviates from pretraining structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6796.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translation Key 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translation Key 2 (Random Key, disrupts logical relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A translation strategy that arbitrarily transforms symbols (e.g., shifting ASCII values) and inverts statements around logical operators, intentionally disrupting logical structure to test robustness of LLM pattern learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (translation/method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>arbitrary symbol transformation strategy</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Applied to the Rosetta-PL training set in a dedicated fine-tuning run (25,214 problems transformed arbitrarily).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Removes consistent mapping between symbols and logical roles, testing whether LLMs can learn patterns absent preserved structural relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Rosetta-PL (translation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A translated variant where symbolic and syntactic structures are intentionally scrambled to remove recognizable logical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Propositional truth classification under arbitrary symbol mapping</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average accuracy reported for Translation Key 2: 64.1% ± 0.75% on relevant tests; overall average cited 80.36% when aggregated in some comparisons (paper emphasizes markedly lower accuracy than Key1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performed substantially worse than Translation Key 1; some finetuned models under Key2 performed worse than base GPT-4o on unseen Lean data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Disrupting logical relationships in the training representation severely reduces the model's ability to learn and generalize formal logical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Arbitrary transformations produce low learnability and poor generalization; unrealistic as a training strategy if goal is to teach strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6796.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lean Workbook</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lean Workbook (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale collection of problems formalized in the Lean theorem prover language used as the source dataset for Rosetta-PL translations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lean workbook: A large-scale lean problem set formalized from natural language math problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>formal theorem/problem corpus (Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Source of original logical problems which were translated into the Rosetta-PL propositional language (Ying et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Formalized mathematical logic problems expressed in Lean; used as source material for supervised fine-tuning after translation.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Lean Workbook (source)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A dataset of logical/mathematical problems encoded in Lean used as the canonical formal source for translation into Rosetta-PL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formal theorem/problem statements (converted to propositional truth evaluation after translation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not directly applicable (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as a controlled source to create Rosetta-PL; leveraging a formal-language corpus helps isolate reasoning from natural-language ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pretraining familiarity of GPT-4o with Lean-like syntax may confound disentangling fine-tuning gains from prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6796.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mini-f2f</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mini-f2f (Mini Formal-to-Formal) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-system benchmark of formal olympiad-level mathematics used in this paper as an independent source of 'unseen' problems for generalization testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minif2f: a cross-system benchmark for formal olympiad-level mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>formal theorem/problem corpus (multiple systems)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Used to sample 'unseen' test subsets (200-problem sets) to evaluate generalization of models fine-tuned on Rosetta-PL.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Provides out-of-training-source problems to test transfer/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Mini-f2f</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A cross-system dataset of formal mathematics problems (used here as unseen test data to validate generalization beyond training corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formal problem truth evaluation/generalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy on unseen sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Unseen Lean accuracy reported for fine-tuned GPT-4o: 99.89% ± 0.06%; Unseen translated (Key1) 97.56% ± 0.44% (these unseen sets were sampled from Mini-f2f for testing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuned models generally outperformed base model on unseen translated examples; for unseen Lean, base model sometimes retained advantage depending on translation key.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an independent formal dataset for unseen evaluation helps assess whether fine-tuned representations generalize to novel formal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High unseen accuracy might reflect dataset biases or overlap in problem types; does not fully rule out overfitting to superficial patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6796.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step reasoning traces from LLMs; discussed in the paper as background and prior work relevant to reasoning but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (technique)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompt-engineering technique (not an evaluated model in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting technique</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Elicits intermediate reasoning steps from LLMs to improve multi-step problem solving (cited as effective in natural-language arithmetic and reasoning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mentioned as a prior method that improves reasoning (Wei et al., 2023), but its effectiveness in strictly symbolic/logical contexts remains largely unexplored per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited literature shows CoT can improve arithmetic and reasoning tasks in natural language; authors note its effectiveness in symbolic/formal contexts is underexplored.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not evaluate CoT on Rosetta-PL; unknown effectiveness for strict propositional logic within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6796.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymbCoT (Symbolic Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced framework that integrates symbolic expressions and logic rules directly into chain-of-thought to improve reasoning fidelity; mentioned in related work but not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid prompting/framework approach combining symbolic reasoning steps with chain-of-thought style decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>symbolic+CoT hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Integrates symbolic expressions and logic rules into stepwise reasoning traces to boost fidelity of logical reasoning (cited from Xu et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Referenced as recent work improving reasoning via hybrid symbolic-CoT integration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Symbolic logical reasoning with augmented CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as improving reasoning fidelity by combining symbolic rules and CoT, but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated in Rosetta-PL experiments; external-tool interactions and implementation details are from cited literature, not this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6796.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External Symbolic Solvers (tool-choice effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External symbolic solvers (e.g., Z3, Prover9, Pyke) and tool-choice effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior studies cited in the paper have shown integrating external symbolic solvers can aid LLM reasoning, with tool selection influencing performance substantially; paper references these findings but does not use such tools in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (tools)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic theorem provers and solvers that can be integrated with LLMs (cited works report up to ~50% performance variation depending on tool).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external symbolic solver (neuro-symbolic integration)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>External tool integration (LLM invokes symbolic solvers or uses solver outputs in reasoning chains).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Cited examples include Z3, Prover9, Pyke; referenced as influential in prior works where choice of solver affected reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mentioned in context of prior benchmarks that combine LLMs with symbolic solvers (e.g., LOGIC-LM, LogicBench).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Symbolic logical problem solving, theorem proving aided by external solvers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prior work cited reports up to ~50% variation in performance depending on solver choice (Lam et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Tool choice can cause large variability; integrating symbolic solvers can improve correctness in logic tasks, per cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External symbolic solvers can substantially affect reasoning performance; this paper intentionally avoids such tool integration to isolate pattern learning in a propositional language.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not empirically compare external tool integration vs pure fine-tuning; references indicate dependence on tool selection and implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6796.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6796.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIC-LM / LOGIGLUE / LogicBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIC-LM, LOGIGLUE, LogicBench (related benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior benchmarks referenced that probe logical/symbolic reasoning capabilities of LLMs, often using predefined inference templates, symbolic solvers, or multi-step structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>benchmark suites for logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Various: some integrate external solvers (LOGIC-LM), some use step templates (LOGIGLUE), and some evaluate systematicity (LogicBench).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LOGIC-LM; LOGIGLUE; LogicBench</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks that evaluate LLM logical reasoning: LOGIC-LM (LLMs + symbolic solvers), LOGIGLUE (multi-step reasoning templates), LogicBench (systematic logical reasoning evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Logic puzzles, multi-step inference, systematic logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>These benchmarks often rely on predefined inference steps or external solvers; the authors position Rosetta-PL as complementary by removing predefined steps and testing autonomous discovery of patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Benchmarks often confound language and reasoning steps or depend on external tools; Rosetta-PL aims to isolate pattern learning in a formal propositional language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Lean workbook: A large-scale lean problem set formalized from natural language math problems <em>(Rating: 2)</em></li>
                <li>Minif2f: a cross-system benchmark for formal olympiad-level mathematics <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models <em>(Rating: 2)</em></li>
                <li>Faithful logical reasoning via symbolic chain-of-thought <em>(Rating: 2)</em></li>
                <li>A closer look at logical reasoning with llms: The choice of tool matters <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6796",
    "paper_id": "paper-278237785",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A closed-source high-performing large language model (LLM) from OpenAI family used in this paper as the base model for fine-tuning on a propositional logic benchmark (Rosetta-PL).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A state-of-the-art, closed-source generative pre-trained transformer family model used by the authors as the fine-tuning target; described as high-performing on reasoning benchmarks (MMLU, GSM8K, Big Bench Hard).",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": "Fine-tuned on Rosetta-PL (25,214 translated Lean problems; subsets of 20,000 and 10,000 also used); pretraining corpora not specified in paper.",
            "reasoning_method": "Supervised fine-tuning on a custom propositional logic dataset (Rosetta-PL); evaluation contrasted preserved-symbol translations vs arbitrary-symbol translations to probe pattern learning.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Rosetta-PL (this work); Lean Workbook and Mini-f2f used as sources/benchmarks",
            "benchmark_description": "Rosetta-PL: a custom propositional-logic benchmark created by translating the Lean Workbook into a bespoke propositional language (25,214 problems). Testing included 'seen' subsets from training and 'unseen' problems sampled from Mini-f2f.",
            "task_type": "Propositional logical statement evaluation (truth value classification) / pattern learning in a formal propositional language",
            "performance_metric": "Accuracy (fraction of correctly answered queries); reported with empirical uncertainties",
            "performance_value": "Seen (translated propositional language average): 95.97% ± 0.33% vs Seen (Lean/untranslated): 76.08% ± 0.36%; Unseen Lean: 99.89% ± 0.06%; Unseen translated (Translation Key 1): 97.56% ± 0.44%; Translation Key 2 (translated) overall: 64.1% ± 0.75%; Average across tests: Translation Key 1 = 92.68%, Translation Key 2 = 80.36%",
            "comparison_with_baseline": "Fine-tuned GPT-4o outperforms base GPT-4o on translated examples; base GPT-4o retains pretraining Lean knowledge and sometimes performs better on unseen Lean examples. Translation Key 1 finetuned models outperform Translation Key 2 finetuned models substantially (e.g., Key1 ∼92.7% vs Key2 ∼80.4% average).",
            "key_findings": "Fine-tuning GPT-4o on a propositional language yields very high accuracy on seen translated problems and strong generalization when the translation preserves logical relationships (Translation Key 1). Performance plateaus near ≈20,000 training examples.",
            "limitations": "Model is closed-source (leakage risk); no model size or internal architecture details provided; overfitting risk on larger training sets; poor generalization when translation disrupts logical relationships (Translation Key 2); plateauing performance suggests dataset redundancy or capacity limits.",
            "uuid": "e6796.0",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Rosetta-PL",
            "name_full": "Rosetta-PL (Propositional Logic benchmark)",
            "brief_description": "A benchmark introduced in this paper that translates the Lean Workbook into a custom propositional language to evaluate LLMs' strict logical reasoning and pattern-generalization in a controlled setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (benchmark/dataset)",
            "model_description": "N/A",
            "model_size": null,
            "architecture_type": "dataset/benchmark (propositional logic format)",
            "training_data": "Derived from Lean Workbook (Ying et al., 2024); 25,214 translated problems total. 'Seen' test subsets (3×500 each) sampled from training, 'unseen' tests (3×200 each) sampled from Mini-f2f.",
            "reasoning_method": "Used as supervised fine-tuning target to elicit LLM internalization of propositional patterns rather than relying on natural-language cues.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Rosetta-PL",
            "benchmark_description": "Controlled propositional-language dataset of truth-evaluation problems translated from Lean; designed to remove natural-language ambiguity and require pattern discovery rather than following predefined inference templates.",
            "task_type": "Propositional statement truth classification and pattern generalization",
            "performance_metric": "Accuracy on seen and unseen splits (percentage correct)",
            "performance_value": "Fine-tuned GPT-4o: seen translated average 95.97% ± 0.33%; seen Lean 76.08% ± 0.36%; unseen Lean 99.89% ± 0.06%; unseen translated (Key1) 97.56% ± 0.44%; Key2 translated 64.1% ± 0.75%",
            "comparison_with_baseline": "When used to fine-tune GPT-4o, Rosetta-PL improved performance on translated pattern recognition relative to base model; preserving logical relationships in the benchmark (Key1) yielded superior gains over arbitrary transformations (Key2).",
            "key_findings": "A propositional, formally-translated benchmark isolates logical pattern learning and shows that preserving relational structure during translation markedly improves LLM learning and generalization.",
            "limitations": "Possible dataset biases and redundancy; high unseen accuracy may reflect over-representation of particular problem types; translation strategies can create artifacts that either help or hurt generalization.",
            "uuid": "e6796.1",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Translation Key 1",
            "name_full": "Translation Key 1 (Focused Key, preserves logical relationships)",
            "brief_description": "A translation strategy introduced in this work that maps Lean symbols to new symbols while preserving logical relationships and consistent mapping of related symbols, including a scrambling duplication trick to simulate syntactic variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (translation/method)",
            "model_description": "N/A",
            "model_size": null,
            "architecture_type": "symbol-mapping + text transformation strategy",
            "training_data": "Applied to the entire Rosetta-PL training set (25,214 problems) in the primary fine-tuning runs.",
            "reasoning_method": "Preserves semantic relationships during translation so that models can learn consistent symbol-to-meaning mappings in the propositional language.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Rosetta-PL (translation variant)",
            "benchmark_description": "A translated variant of Lean problems which maintains logical relationships between tokens/symbols to encourage learnability.",
            "task_type": "Enabling supervised learning of logical pattern correspondences during propositional truth classification",
            "performance_metric": "Accuracy",
            "performance_value": "Average accuracy with Translation Key 1 across tests: 92.68% (paper reports fine-tuned model achieved seen translated average 95.97%; unseen translated 97.56%).",
            "comparison_with_baseline": "Outperformed Translation Key 2 by a large margin (Key1 average 92.68% vs Key2 average 80.36%); fine-tuned Key1 models outperform base GPT-4o on translated examples.",
            "key_findings": "Preserving logical relationships in symbolic translation substantially improves an LLM's ability to learn and generalize formal logical patterns.",
            "limitations": "The scrambling/duplication introduced to mimic syntactic variation could introduce artifacts; success may partly depend on pretraining familiarity with Lean-like syntax; does not guarantee transfer when translation deviates from pretraining structures.",
            "uuid": "e6796.2",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Translation Key 2",
            "name_full": "Translation Key 2 (Random Key, disrupts logical relationships)",
            "brief_description": "A translation strategy that arbitrarily transforms symbols (e.g., shifting ASCII values) and inverts statements around logical operators, intentionally disrupting logical structure to test robustness of LLM pattern learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (translation/method)",
            "model_description": "N/A",
            "model_size": null,
            "architecture_type": "arbitrary symbol transformation strategy",
            "training_data": "Applied to the Rosetta-PL training set in a dedicated fine-tuning run (25,214 problems transformed arbitrarily).",
            "reasoning_method": "Removes consistent mapping between symbols and logical roles, testing whether LLMs can learn patterns absent preserved structural relationships.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Rosetta-PL (translation variant)",
            "benchmark_description": "A translated variant where symbolic and syntactic structures are intentionally scrambled to remove recognizable logical patterns.",
            "task_type": "Propositional truth classification under arbitrary symbol mapping",
            "performance_metric": "Accuracy",
            "performance_value": "Average accuracy reported for Translation Key 2: 64.1% ± 0.75% on relevant tests; overall average cited 80.36% when aggregated in some comparisons (paper emphasizes markedly lower accuracy than Key1).",
            "comparison_with_baseline": "Performed substantially worse than Translation Key 1; some finetuned models under Key2 performed worse than base GPT-4o on unseen Lean data.",
            "key_findings": "Disrupting logical relationships in the training representation severely reduces the model's ability to learn and generalize formal logical patterns.",
            "limitations": "Arbitrary transformations produce low learnability and poor generalization; unrealistic as a training strategy if goal is to teach strict logical reasoning.",
            "uuid": "e6796.3",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Lean Workbook",
            "name_full": "Lean Workbook (dataset)",
            "brief_description": "A large-scale collection of problems formalized in the Lean theorem prover language used as the source dataset for Rosetta-PL translations.",
            "citation_title": "Lean workbook: A large-scale lean problem set formalized from natural language math problems",
            "mention_or_use": "use",
            "model_name": "N/A (dataset)",
            "model_description": "N/A",
            "model_size": null,
            "architecture_type": "formal theorem/problem corpus (Lean)",
            "training_data": "Source of original logical problems which were translated into the Rosetta-PL propositional language (Ying et al., 2024).",
            "reasoning_method": "Formalized mathematical logic problems expressed in Lean; used as source material for supervised fine-tuning after translation.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Lean Workbook (source)",
            "benchmark_description": "A dataset of logical/mathematical problems encoded in Lean used as the canonical formal source for translation into Rosetta-PL.",
            "task_type": "Formal theorem/problem statements (converted to propositional truth evaluation after translation)",
            "performance_metric": "Not directly applicable (dataset)",
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Used as a controlled source to create Rosetta-PL; leveraging a formal-language corpus helps isolate reasoning from natural-language ambiguity.",
            "limitations": "Pretraining familiarity of GPT-4o with Lean-like syntax may confound disentangling fine-tuning gains from prior knowledge.",
            "uuid": "e6796.4",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mini-f2f",
            "name_full": "Mini-f2f (Mini Formal-to-Formal) dataset",
            "brief_description": "A cross-system benchmark of formal olympiad-level mathematics used in this paper as an independent source of 'unseen' problems for generalization testing.",
            "citation_title": "Minif2f: a cross-system benchmark for formal olympiad-level mathematics",
            "mention_or_use": "use",
            "model_name": "N/A (dataset)",
            "model_description": "N/A",
            "model_size": null,
            "architecture_type": "formal theorem/problem corpus (multiple systems)",
            "training_data": "Used to sample 'unseen' test subsets (200-problem sets) to evaluate generalization of models fine-tuned on Rosetta-PL.",
            "reasoning_method": "Provides out-of-training-source problems to test transfer/generalization.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Mini-f2f",
            "benchmark_description": "A cross-system dataset of formal mathematics problems (used here as unseen test data to validate generalization beyond training corpus).",
            "task_type": "Formal problem truth evaluation/generalization",
            "performance_metric": "Accuracy on unseen sets",
            "performance_value": "Unseen Lean accuracy reported for fine-tuned GPT-4o: 99.89% ± 0.06%; Unseen translated (Key1) 97.56% ± 0.44% (these unseen sets were sampled from Mini-f2f for testing).",
            "comparison_with_baseline": "Fine-tuned models generally outperformed base model on unseen translated examples; for unseen Lean, base model sometimes retained advantage depending on translation key.",
            "key_findings": "Using an independent formal dataset for unseen evaluation helps assess whether fine-tuned representations generalize to novel formal problems.",
            "limitations": "High unseen accuracy might reflect dataset biases or overlap in problem types; does not fully rule out overfitting to superficial patterns.",
            "uuid": "e6796.5",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step reasoning traces from LLMs; discussed in the paper as background and prior work relevant to reasoning but not used in experiments here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "N/A (technique)",
            "model_description": "A prompt-engineering technique (not an evaluated model in this paper).",
            "model_size": null,
            "architecture_type": "prompting technique",
            "training_data": null,
            "reasoning_method": "Elicits intermediate reasoning steps from LLMs to improve multi-step problem solving (cited as effective in natural-language arithmetic and reasoning tasks).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "",
            "benchmark_description": "Mentioned as a prior method that improves reasoning (Wei et al., 2023), but its effectiveness in strictly symbolic/logical contexts remains largely unexplored per the paper.",
            "task_type": "Multi-step reasoning prompting",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Cited literature shows CoT can improve arithmetic and reasoning tasks in natural language; authors note its effectiveness in symbolic/formal contexts is underexplored.",
            "limitations": "Paper does not evaluate CoT on Rosetta-PL; unknown effectiveness for strict propositional logic within this study.",
            "uuid": "e6796.6",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SymbCoT",
            "name_full": "SymbCoT (Symbolic Chain-of-Thought)",
            "brief_description": "A referenced framework that integrates symbolic expressions and logic rules directly into chain-of-thought to improve reasoning fidelity; mentioned in related work but not used here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "N/A (framework)",
            "model_description": "Hybrid prompting/framework approach combining symbolic reasoning steps with chain-of-thought style decompositions.",
            "model_size": null,
            "architecture_type": "symbolic+CoT hybrid",
            "training_data": null,
            "reasoning_method": "Integrates symbolic expressions and logic rules into stepwise reasoning traces to boost fidelity of logical reasoning (cited from Xu et al., 2024).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "",
            "benchmark_description": "Referenced as recent work improving reasoning via hybrid symbolic-CoT integration.",
            "task_type": "Symbolic logical reasoning with augmented CoT",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Cited as improving reasoning fidelity by combining symbolic rules and CoT, but not evaluated in this paper.",
            "limitations": "Not evaluated in Rosetta-PL experiments; external-tool interactions and implementation details are from cited literature, not this work.",
            "uuid": "e6796.7",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "External Symbolic Solvers (tool-choice effect)",
            "name_full": "External symbolic solvers (e.g., Z3, Prover9, Pyke) and tool-choice effects",
            "brief_description": "Prior studies cited in the paper have shown integrating external symbolic solvers can aid LLM reasoning, with tool selection influencing performance substantially; paper references these findings but does not use such tools in experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "N/A (tools)",
            "model_description": "Symbolic theorem provers and solvers that can be integrated with LLMs (cited works report up to ~50% performance variation depending on tool).",
            "model_size": null,
            "architecture_type": "transformer + external symbolic solver (neuro-symbolic integration)",
            "training_data": null,
            "reasoning_method": "External tool integration (LLM invokes symbolic solvers or uses solver outputs in reasoning chains).",
            "external_tool_used": null,
            "external_tool_description": "Cited examples include Z3, Prover9, Pyke; referenced as influential in prior works where choice of solver affected reasoning performance.",
            "benchmark_name": "",
            "benchmark_description": "Mentioned in context of prior benchmarks that combine LLMs with symbolic solvers (e.g., LOGIC-LM, LogicBench).",
            "task_type": "Symbolic logical problem solving, theorem proving aided by external solvers",
            "performance_metric": null,
            "performance_value": "Prior work cited reports up to ~50% variation in performance depending on solver choice (Lam et al., 2024).",
            "comparison_with_baseline": "Tool choice can cause large variability; integrating symbolic solvers can improve correctness in logic tasks, per cited literature.",
            "key_findings": "External symbolic solvers can substantially affect reasoning performance; this paper intentionally avoids such tool integration to isolate pattern learning in a propositional language.",
            "limitations": "Paper does not empirically compare external tool integration vs pure fine-tuning; references indicate dependence on tool selection and implementation.",
            "uuid": "e6796.8",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LOGIC-LM / LOGIGLUE / LogicBench",
            "name_full": "LOGIC-LM, LOGIGLUE, LogicBench (related benchmarks)",
            "brief_description": "Prior benchmarks referenced that probe logical/symbolic reasoning capabilities of LLMs, often using predefined inference templates, symbolic solvers, or multi-step structured tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "N/A (benchmarks)",
            "model_description": "N/A",
            "model_size": null,
            "architecture_type": "benchmark suites for logical reasoning",
            "training_data": null,
            "reasoning_method": "Various: some integrate external solvers (LOGIC-LM), some use step templates (LOGIGLUE), and some evaluate systematicity (LogicBench).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "LOGIC-LM; LOGIGLUE; LogicBench",
            "benchmark_description": "Benchmarks that evaluate LLM logical reasoning: LOGIC-LM (LLMs + symbolic solvers), LOGIGLUE (multi-step reasoning templates), LogicBench (systematic logical reasoning evaluation).",
            "task_type": "Logic puzzles, multi-step inference, systematic logical reasoning",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "These benchmarks often rely on predefined inference steps or external solvers; the authors position Rosetta-PL as complementary by removing predefined steps and testing autonomous discovery of patterns.",
            "limitations": "Benchmarks often confound language and reasoning steps or depend on external tools; Rosetta-PL aims to isolate pattern learning in a formal propositional language.",
            "uuid": "e6796.9",
            "source_info": {
                "paper_title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Lean workbook: A large-scale lean problem set formalized from natural language math problems",
            "rating": 2,
            "sanitized_title": "lean_workbook_a_largescale_lean_problem_set_formalized_from_natural_language_math_problems"
        },
        {
            "paper_title": "Minif2f: a cross-system benchmark for formal olympiad-level mathematics",
            "rating": 2,
            "sanitized_title": "minif2f_a_crosssystem_benchmark_for_formal_olympiadlevel_mathematics"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models",
            "rating": 2,
            "sanitized_title": "towards_logiglue_a_brief_survey_and_a_benchmark_for_analyzing_logical_reasoning_capabilities_of_language_models"
        },
        {
            "paper_title": "Faithful logical reasoning via symbolic chain-of-thought",
            "rating": 2,
            "sanitized_title": "faithful_logical_reasoning_via_symbolic_chainofthought"
        },
        {
            "paper_title": "A closer look at logical reasoning with llms: The choice of tool matters",
            "rating": 2,
            "sanitized_title": "a_closer_look_at_logical_reasoning_with_llms_the_choice_of_tool_matters"
        }
    ],
    "cost": 0.014643999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning</p>
<p>Shaun Baek shaun.baek@emory.edu 
Emory University</p>
<p>Shaun Esua-Mensah 
Algoverse AI Research</p>
<p>Cyrus Tsui 
Algoverse AI Research</p>
<p>Sejan Vigneswaralingam 
Algoverse AI Research</p>
<p>Abdullah Alali 
Algoverse AI Research</p>
<p>Michael Lu 
Algoverse AI Research</p>
<p>Vasu Sharma 
Algoverse AI Research</p>
<p>Kevin Zhu 
Algoverse AI Research</p>
<p>Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning
76E3863139914C393341030F8B24C1A6
Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning.This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment.We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o).Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model.Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples.These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as OpenAI's GPT models (Brown et al., 2020), Google's Gemini models (Team et al., 2024), and Meta's Llama models (Touvron et al., 2023), are typically trained on high-resource natural languages (e.g., English, Spanish, and Chinese).This focus on high-resource languages disadvantages speakers of low-resource languages, as training models for these languages are more challenging due to their inherent complexity (Team et al., 2022).Furthermore, semantic ambiguity, grammatical complexities, and contextual dependencies in natural languages can limit the capabilities of an LLM in precise logical reasoning.Since natural language often relies on implied meaning, subtle cues, and flexible syntax, models trained primarily on data using these principles * These authors contributed equally to this work.may struggle to follow strict rules needed for logical reasoning (Asher et al., 2023).</p>
<p>To isolate these reasoning abilities from language-specific challenges, we propose the evaluation of LLMs within a controlled setting using formal logical language.Logical languages, characterized by strict syntax and precise semantics, eliminate many of the extraneous factors present in natural languages, allowing us to focus squarely on pattern recognition and problem solving.Although prior benchmarks, such as LOGIGLUE (Luo et al., 2024), provide structured reasoning tasks, these typically rely on predefined reasoning steps, making it challenging to determine whether an LLM can autonomously identify and apply logical rules.In contrast, our benchmark, Rosetta-PL, evaluates whether LLMs can discover logical patterns within a propositional language, thereby measuring reasoning ability without relying on predefined inference steps or extraneous linguistic factors.Research on applying LLMs to logic-based problem solving is relatively scarce, and while chain-ofthought (CoT) prompting has gained popularity in natural language tasks (Wei et al., 2023), its effectiveness in logical or symbolic contexts remains largely unexplored (Creswell et al., 2022).</p>
<p>We address this gap by constructing Rosetta-PL by translating the Lean Workbook dataset (Ying et al., 2024) into our own propositional language and fine-tuning ChatGPT (Brown et al., 2020) using the translated dataset.We evaluate logical accuracy in our custom language while varying training data parameters such as training set size and the method of translation.Our experiments point towards potentially effective training strategies and provide preliminary estimates on the dataset size needed to approach benchmark-level logical understanding.By setting aside language-specific factors, we focus on the relationship between pattern recognition and data requirements, offering insights that impact language training in both high-arXiv:2505.00001v1[cs.CL] 25 Mar 2025 and low-resource settings.</p>
<p>Background</p>
<p>Large Language Models (LLMs) have excelled at tasks involving unstructured natural language, yet their capacity for structured logical reasoning remains underexplored (Creswell et al., 2022).The inherent ambiguities of natural language, such as polysemy and idiomatic expressions, can obscure true reasoning capabilities.In contrast, formal logical languages, defined by strict syntax and unambiguous semantics, offer a controlled testbed for evaluating pattern recognition and rule-based inference (Barcelo et al., 2023).</p>
<p>Propositional logic, a fundamental component of formal logic, employs connectives (e.g., ∧, ∨, ¬) to combine atomic propositions into complex expressions whose truth values are fully determined by their parts (Niu et al., 2024).This clarity makes it an ideal framework for assessing whether LLMs can autonomously learn and generalize logical rules-a skill central to disciplines like mathematics and programming (Nye et al., 2021;Polu and Sutskever, 2020).</p>
<p>Recent benchmarks have begun to probe the symbolic reasoning of LLMs.For example, LOGIC-LM demonstrates that LLMs can solve logic puzzles when aided by external symbolic solvers (Pan et al., 2023).Meanwhile, LOGIGLUE (Luo et al., 2024) andLogic Bench (Parmar et al., 2024) evaluate multi-step reasoning based on predefined inference templates, and chain-of-thought prompting has been shown to improve arithmetic performance (Wei et al., 2023).Other studies have further enriched this landscape: for example, the SymbCoT framework integrates symbolic expressions and logic rules directly into chain-of-thought (CoT) thereby boosting reasoning fidelity (Xu et al., 2024), while research examining the impact of symbolic solver choices has revealed that tool selection (e.g., Z3, Prover9, or Pyke) can cause performance variations of up to 50% (Lam et al., 2024).Furthermore, work on step-by-step symbolic verification has demonstrated that automated checks of intermediate reasoning steps can substantially enhance overall accuracy (Zhang et al., 2024).However, these approaches tend to rely on surface-level statistical correlations rather than genuine discovery of novel logical patterns (Creswell et al., 2022).</p>
<p>To bridge this gap, our work translates natural language logic problems into a propositional lan-guage, thereby eliminating linguistic complexities and focusing solely on intrinsic pattern recognition.Building on formal frameworks such as Lean4 (Ying et al., 2024), we investigate how well LLMs can learn and generalize new logical structures-a capability that also carries implications for improving training strategies in low-resource language settings (Team et al., 2022).</p>
<p>Method</p>
<p>Objective</p>
<p>The primary objective of this experiment is to evaluate the logical accuracy and pattern recognition capabilities of LLMs in a newly created propositional language.By removing linguistic complexities to focus solely on logical problem-solving, we aim to determine how well these models generalize and adapt in a structured, logic-based environment under varying dataset sizes, and whether this process reveals or rectifies discrepancies in their understanding of formal languages.</p>
<p>Dataset</p>
<p>We derived Rosetta-PL from the Lean Workbook (Ying et al., 2024), which is a dataset of logical problems translated into the formal language of Lean.Each problem was translated into our custom propositional language using a predefined translation key, resulting in a training dataset of 25,214 problems.Each dataset entry was written in a conversation-like structure with system, user, assistant, function, and message content, containing a logical problem (a statement) in our custom language and its corresponding truth value, indicating whether the statement is true or not.In contrast to benchmarks such as LOGIGLUE (Luo et al., 2024) and LOGIC-LM (Pan et al., 2023), which focus on logical problems with predefined inference steps, Rosetta-PL is designed to test an LLM's ability to discover new patterns.Unlike Logic Bench (Parmar et al., 2024), which evaluates performance on known logical patterns, our dataset requires the model to infer novel patterns.</p>
<p>Experimental Methodology</p>
<p>Our experimental setup involved building a data pipeline for fine-tuning GPT-4o on formal logical tasks.We opted to use GPT-4o primarily due to its performance on a range of reasoning benchmarks such as MMLU (Massive Multitask Language Understanding), GSM8K, and Big Bench Hard, al-lowing us to compare with one of the highest performers for LLMs in formal logic tasks.Because GPT-4o is closed-source, there is an inherent risk of leakage challenges.However, by translating the Lean Workbook into our own custom propositional language, we altered the original problems in an unorthodox way that makes direct overlap in GPT-4o's training far less likely.</p>
<p>Each entry in our training dataset was verified to conform to the required format-ensuring valid roles such as system, user, and assistant, and is passed on to GPT-4o for fine-tuning.From this same dataset, we also extracted "seen" testing subsets by randomly selecting 500 entries.We also extracted "unseen" testing subsets by randomly selecting 200 problems from an entirely different source: the Minif2f-lean4 dataset (Zheng et al., 2022), which does not overlap with the training dataset.We aim to measure the model's ability to both retain learned information and generalize its logical understanding to novel patterns through the "seen" and "unseen" datasets respectively.</p>
<p>Throughout these experiments, all fine-tuning and testing were conducted using NVIDIA A100 GPUs.Overall, GPT-4o underwent four separate fine-tuning runs, during which we kept parameter settings constant (e.g., learning rate, number of epochs) while varying the size of the training dataset (25,214, 20,000, and 10,000) and which one out of the two translation keys used.These translation keys altered how the logical problems from the Lean Workbook were mapped into our custom language, effectively creating multiple languages with varying logical structures.</p>
<p>Original Example:
xyz : N ⊢ (x 2 + 1) * (y 2 + 1) * (z 2 + 1) = (x + y + z) 2 − 2 * (x * y + y * z + z * x) + (x * y + y * z + z * x) 2 − 2 * x * y * z * (x + y + z) + x 2 * y 2 * z 2 + 1 (1)
Translation Strategies: To investigate the effect of symbolic representation on logical reasoning, we employ two distinct translation strategies.The first strategy maintains the inherent logical relationships by carefully mapping symbols, while the second intentionally disrupts these patterns through arbitrary transformations.These contrasting approaches allow us to assess how preserving or altering logical structure influences model performance.</p>
<p>• Translation Key 1 Strategy (Focused Key):</p>
<p>Translation Key 1 replaces Lean symbols with other symbols (see appendix).This method preserves logical relationships by ensuring that related symbols are consistently mapped.For instance, the symbols "&gt;" and "&lt;" are translated into "»" and "«", respectively, preserving their comparative meaning.This is to mimic spoken language, where symbols and phrases are logically related.Additionally, the sentence structure is encrypted using a scrambling function that adds a reversed duplicate of the sentence at the end, with a few additional symbols in between, in order to mimic the variations in sentence structures across different languages.An example of an entry translated with Key 1 is shown below:
xyz¬N##|−|−|−x ∧ ∧2 ∧ ∧1|−e|−|−|−y ∧ ∧2 ∧ ∧1|− e|−|−|−z ∧ ∧2 ∧ ∧1|− == |−|−|−x ∧ ∧y ∧ ∧z| − ∧ ∧2 2 e|−|−|−xey ∧ ∧yez ∧ ∧zx|− ∧ ∧|−|−|−xey ∧ ∧yez ∧ ∧zex|− ∧ ∧2 2 exeyeze|−|−|−x ∧ ∧y ∧ ∧z| − ∧ ∧x ∧ ∧2ey ∧ ∧2ez ∧ ∧2 ∧ ∧1(2)
• Translation Key 2 Strategy (Random Key): In contrast, this method removes logical structure by shifting the ASCII values of each character by 10, resulting in an entirely arbitrary transformation.As a result, the translated expression loses any recognizable logical patterns.Additionally, statements are inverted around logical operators such as -&gt;, &gt;, &lt;, &gt;=, and &lt;=.For example, an expression of the form "A &gt; B &gt; C" would be translated into "C T(&gt;) B T(&gt;) A", where T(&gt;) represents the transformed version of the "&gt;" symbol.An example of an entry translated with Key 2 is provided below:
"y!z!{!;!\u2125\u000b\u22a3!)y!<em>!3!, !2<em>!+!)z!_!3!,!2</em>!+!){!</em>!3!,!2<em>!&gt;\u000b !!!!)y!,!z!,!{</em>!<em>!3!.!3!+!)y!+!z!,!z!+! {!,!{!+!y<em>!,!)y!+!z!,!z!+!{!,!{!+!y</em>!</em> !3!.!3!+!y!+!z!+!{!+!)y!,!z!,!{*!, \u000b!!!!!!!!y!<em>!3!+!z!</em>!3!+! !,\u000b!!!!!!2"|(3)
Evaluation Procedure: We conducted four finetuning runs on GPT-4o, keeping all hyperparameters constant, and evaluated five models (four finetuned and one base model with no fine-tuning) using 12 distinct datasets.These datasets are organized into two main categories:</p>
<p>• Seen Data: Six datasets were created by randomly selecting problems from the training set-three datasets containing 500 problems each in the original Lean format and three datasets with 500 problems each using the same translation key employed during finetuning.</p>
<p>• Unseen Data: To assess generalization, six additional datasets were formed by randomly selecting 200 problems each from the independent Mini-f2f dataset (Zheng et al., 2022).Like the seen data, these were split into two groups of three datasets: one in Lean and the other using the corresponding translated format.</p>
<p>Overall accuracy was computed by averaging the results across all testing sets, with accuracy defined as the number of correctly answered queries divided by the total number of queries in each set.</p>
<p>Results</p>
<p>Figure 1 displays the comparative performance of four fine-tuned GPT-4o models evaluated on both "seen" and "unseen" datasets.Specifically, models were fine-tuned with 25,214, 20,000, and 10,000 distinct queries using Translation Key 1, and with 25,214 queries using Translation Key 2. Additionally, Lean (untranslated) versions of both testing sets serve as benchmarks.</p>
<p>Our experiments demonstrate that GPT-4o exhibits superior problem-solving performance in our custom propositional language compared to Lean on average.On the "seen" dataset, GPT-4o achieved an average accuracy over all tests in of 95.97% in our propositional language versus 76.08% in Lean, with a small uncertainty of ± 0.33% and ± 0.36% respectively.</p>
<p>In contrast, on the "unseen" dataset, GPT-4o performed better when tested in Lean than in our custom language-attaining 99.89% accuracy with Lean compared to 97.56% with Translation Key 1 (± 0.06% and ± 0.44% respectively).As expected, Translation Key 2 yielded a substantially lower accuracy of 64.1% (± 0.75%) due to its arbitrary mapping.The model was fine-tuned solely on translated data, so it specializes in those patterns, resulting in high performance on seen translated examples but poor performance on seen Lean examples.For unseen data, it falls back on its broader pre-training, which helps it perform better on unseen Lean problems.</p>
<p>Additionally, our experiments indicate that GPT-4o solves problems more accurately with Translation Key 1 than with Translation Key 2, with average accuracies of 92.68% compared to 80.36% respectively-highlighting the importance of preserving logical relationships in the translation process.Table 1 provides a detailed summary of results from testing with Translation Key 1, and Table 3 provides a detailed summary of results from testing with Translation Key 2. Furthermore, training set size influenced performance.Increasing the training set from 10,000 to 20,000 samples improved accuracy by 2.7% on the "seen" dataset and by 0.3% on the "unseen" dataset, while further increases up to 25,214 samples did not yield additional gains.This suggests that the training set size threshold for stable performance lies below 20,000 samples.</p>
<p>For seen data in the custom translated format, the fine-tuned GPT-4o consistently achieves higher accuracy by specializing in the patterns and syntax introduced during fine-tuning, outperforming the base model.In contrast, on seen Lean data, the base GPT-4o retains its general Lean knowledge from pre-training and achieves similar results to the fine-tuned model.</p>
<p>When it comes to unseen data, the fine-tuned GPT-4o expectedly outperforms the base model on unseen translated examples.Table 4 provides a detailed summary of the results from testing using the base GPT-4o model.However, for unseen Lean data, the GPT-4o fine-tuned using Translation Key 2 performed significantly worse than its Translation Key 1 counterparts and also the base models.Focusing on Lean data (untranslated), all 4 finetuned models outperform the base models in both the unseen and seen data, except for the model finetuned in Translation Key 2 which showed worse comparative performance in the unseen lean data.</p>
<p>Tables 1, 3, and 4 provides a detailed summary of all dataset permutations and average performance metrics, shedding light on any potential anomalies.</p>
<p>Discussion</p>
<p>Our findings align with previous studies (Kojima et al., 2023;Wei et al., 2023), demonstrating that the accuracy of logical reasoning depends significantly on prompt formulation and task representation.The use of translation keys in our experiments illustrates that preserving inherent logical relationships-as in Translation Key 1-yields better performance than employing arbitrary mappings.This is analogous to natural language, where inverse or comparable relationships between symbols facilitate comprehension.</p>
<p>Our results also reveal a general trend where accuracy increases with training set size, echoing prior research that shows LLMs can perform well even with limited data (Brown et al., 2020).However, as shown in Figure 1, this trend is not strictly linear.There are occasions where smaller datasets outperformed larger datasets, such as the "seen" dataset in our propositional language having a 0.467% greater accuracy with 10000 samples compared to 20,000 samples.We attribute these fluctuations to certain factors, such as overfitting in larger training sets.Unlike earlier studies that evaluated existing models (liu et al., 2023), our approach using a custom propositional language uncovers unique aspects of pattern recognition in LLMs.</p>
<p>Notably, our analysis revealed that GPT-4o's performance on unseen data is better in Lean than it is in our custom language.We attribute this to GPT-4o's prior exposure to Lean-like syntax during pre-training Lean, as a formal proof assistant, shares structural similarities with theorem-proving and programming languages.In contrast, the custom language, especially under Translation Key 2, disrupted logical structure, thereby impeding generalization.This suggests that fine-tuning benefits significantly when the training data preserves logical consistency, aligning with the model's pretraining experience.This is further reinforced by the observation that models fine-tuned with Translation Key 1 performed better across all testing sets than those finetuned with Translation Key 2. Additionally, the fine-tuned models-especially those with Translation Key 1-consistently exhibited superior performance on both seen and unseen data, and this performance improved with larger training set sizes.This demonstrates GPT's ability to generalize logical information.The LLM extracted logical information from our custom language and used it to improve its logical accuracy in Lean.Notably, it performed better with Translation Key 1-which preserves logical relationships-than with Translation Key 2, which disrupts them.</p>
<p>While distinguishing between these effects is challenging, future work could explore fine-tuning an LLM with minimal exposure to Lean syntax to better understand the impact of pre-training familiarity compared to logical structure preservation.Comparing performance across runs provided insights into whether GPT-4o could robustly handle shifts in symbolic representation and how sensitive its performance is to different training configurations.</p>
<p>Our experiments indicate that GPT-4o's performance plateaus at around 20,000 training examples.This plateau may result from dataset redundancy, model capacity limitations, or the relative simplicity of the tasks.When the dataset contains many similar patterns, the model's exposure to novel challenges is limited, and once key patterns are internalized, additional training yields diminishing returns.</p>
<p>In summary, our findings suggest that GPT-4o can achieve high problem-solving accuracy in a propositional language when fine-tuned appropriately.The choice of translation key, dataset characteristics, and training set size must be managed carefully to mitigate overfitting and ensure robust generalization beyond seen patterns.</p>
<p>Conclusion</p>
<p>Our investigation confirms that fine-tuning GPT-4o on a custom propositional language not only facilitates high-level logical reasoning but also underscores the critical role of maintaining relational integrity within training data.Specifically, our work shows that using structured translation strategies significantly enhances model performance.This improvement is achieved by aligning the training data with the inherent logical patterns familiar from the model's pre-training, allowing GPT-4o to generalize more effectively, particularly when transitioning from seen to unseen examples.</p>
<p>Furthermore, our analysis highlights that an optimally balanced training set is essential: while increased dataset size improves performance up to a threshold (around 20,000 examples), additional data yields diminishing returns, suggesting the need for more efficient data utilization methods.These findings not only validate the importance of structured prompts and contextual cues but also offer practical guidelines for optimizing LLM training in both high-and low-resource language scenarios.</p>
<p>Collectively, our results contribute to a deeper understanding of how targeted data curation and translation methodologies can bolster logical reasoning in large language models.</p>
<p>Future Research</p>
<p>Future work should investigate dataset design principles.The high accuracy observed on our unseen dataset may reflect biases, such as overrepresentation of certain problem types or cultural premises, which should be systematically addressed.Synthetically balanced datasets that incorporate tiered complexity levels (e.g., single-step versus multi-step reasoning) could help disentangle superficial pattern recognition from genuine logical understanding.Additionally, although formatting differences (e.g., brackets versus colons) did not hinder performance in our study, systematic evaluations of robustness to syntactic variations are needed to better assess adaptability in low-resource settings.</p>
<p>A potential path to explore would be foregoing fine-tuning GPT-4o on our custom dataset and instead rely on in-context learning.Because GPT-4o may already have some familiarity with Lean from its pre-training, one could design a prompt that includes a few worked examples of Lean problems alongside a call to an external translator function that converts Lean input into the custom propositional language at inference time.Though this may yield lower accuracy than fine-tuning, it avoids the cost of creating and maintaining a large translation corpus.Evaluating GPT-4o in context can reveal how much of its Lean knowledge can be utilized through prompt engineering alone.</p>
<p>Further research should focus on optimizing translation strategies by developing principled approaches, such as semantic alignment of symbols, to enhance learnability.At the same time, exploring data efficiency methods is critical, as our observed performance plateau at approximately 20,000 training examples suggests that smarter data utilization may both reduce data requirements and improve systematicity.</p>
<p>Figure 1 :
1
Figure 1: Comparison of GPT-4o accuracy across datasets ("Seen" and "Unseen") using different translation keys and varying dataset sizes.</p>
<p>A Appendix
Limits for learning with language models. Nicholas Asher, Swarnadeep Bhar, Akshay Chaturvedi, Julie Hunter, Soumya Paul, arXiv:2306.122132023Preprint</p>
<p>Logical languages accepted by transformer encoders with hard attention. Pablo Barcelo, Alexander Kozachinskiy, Anthony Widjaja Lin, Vladimir Podolskii, arXiv:2310.038172023Preprint</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, 202012</p>
<p>Language models are few-shot learners. arXiv:2005.14165Preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022Preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>A closer look at logical reasoning with llms: The choice of tool matters. Long Hei, Matthew Lam, Ramya Keerthy Thatikonda, Ehsan Shareghi, arXiv:2406.002842024Preprint</p>
<p>Glore: Evaluating logical reasoning of large language models. Zhiyang Hanmeng Liu, Ruoxi Teng, Jian Ning, Qiji Liu, Yue Zhou, Zhang, arXiv:2310.091072023Preprint</p>
<p>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Ming Shen, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, arXiv:2310.008362024Preprint</p>
<p>On grobner-shirshov bases for markov semirings. Xiaohui Niu, Wenxi Li, Zhongzhi Wang, arXiv:2401.057312024Preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, arXiv:2112.001142021Preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.12295arXiv:2404.15522Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, and Chitta Baral. 2024. Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. 2023Preprint</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, arXiv:2009.033932020Preprint</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, arXiv:2312.118052024PreprintAngeliki Lazaridou, and 1331 others</p>
<p>No language left behind: Scaling human-centered machine translation. Marta R Nllb Team, James Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Anna Maillard, Skyler Sun, Guillaume Wang, Al Wenzek, Bapi Youngblood, Loic Akula, Gabriel Mejia Barrault, Prangthip Gonzalez, Hansanti, arXiv:2207.04672202220Preprint</p>
<p>Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, arXiv:2307.09288Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu, arXiv:2405.183572024Preprint</p>
<p>Lean workbook: A large-scale lean problem set formalized from natural language math problems. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen, arXiv:2406.038472024Preprint</p>
<p>Evaluating step-by-step reasoning through symbolic verification. Yi-Fan Zhang, Hanlin Zhang, Li Erran Li, Eric Xing, arXiv:2212.086862024Preprint</p>
<p>Minif2f: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, arXiv:2109.001102022Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
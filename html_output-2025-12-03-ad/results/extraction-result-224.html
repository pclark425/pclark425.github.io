<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-224 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-224</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-224</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0" target="_blank">Advancing LLM Reasoning Generalists with Preference Trees</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Eurus, a suite of large language models (LLMs) optimized for reasoning, and derives a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.</p>
                <p><strong>Paper Abstract:</strong> We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e224.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e224.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UltRAINTERACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UltRAINTERACT (Tree-structured Alignment Data for Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, high-quality alignment dataset introduced in this paper consisting of tree-structured, multi-turn preference trees for challenging reasoning tasks (math, coding, logic), containing diverse reasoning chains, multi-turn interaction trajectories, and paired correct/incorrect actions for preference learning and SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus (SFT / Preference Learning / RM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B (used for different Eurus checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning (math, coding, logical reasoning), multi-turn interaction</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tree-structured multi-turn alignment data: reasoning chains (CoT and modularization), code-as-action trajectories, multi-turn interactions with environment and critique, and paired correct/incorrect actions for preference learning</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>86K instructions; 220K action pairs (multi-turn); paper also reports 287K correct actions used for SFT and an augmentation of 240K single-turn pairs for RM</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>High difficulty, diversity across reasoning patterns and datasets, multi-turn interactions, ground-truth solutions available (to ensure high-quality oversight), code-executable actions, paired correct/incorrect nodes (preference trees)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>various: pass@1 (coding), accuracy on math benchmarks, multi-turn success rate at Turn 5, RM correlation metrics (AutoJ, MT-Bench), aggregated average scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Using UltRAINTERACT in SFT + standard alignment mixtures yields state-of-the-art open-source reasoning performance (Eurus-7B-SFT and Eurus-70B-SFT). Examples reported: Eurus-7B-SFT (with UltRAINTERACT + other alignment data) overall averages reported in the paper (different eval sets) e.g. avg ~46.5 (Table 3) and an alternate aggregate avg 53.6 (Table 5) depending on metric set; RM trained with UltRAINTERACT (Eurus-RM-7B) achieves AutoJ overall 64.5 and MT-Bench 72.9.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT trained on open-source only alignment data: avg 37.0 (Table 5, 'Open-source Only'); 'Ground-truth' variant (replace generated actions with original dataset GT) avg 44.0 (Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Mixing UltRAINTERACT into SFT vs open-source-only SFT yields large absolute gains (example: +16.6 absolute avg in Table 5: 53.6 vs 37.0); used in preference learning and RM also yields improvements in math & multi-turn performance and RM correlation/reranking gains.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Tree-structured, multi-turn, high-quality preference data (UltRAINTERACT) materially improves reasoning and multi-turn interaction abilities when included in SFT and preference/RM training; however, it should be mixed with other alignment data to preserve instruction-following abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e224.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT Mixture (UltraChat / ShareGPT / OpenOrca)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of instruction-conversational alignment datasets (UltraChat, ShareGPT, OpenOrca) used in supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common conversational/instruction datasets mixed into SFT to improve general instruction-following and conversational ability alongside UltRAINTERACT's reasoning data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-7B-SFT (and Eurus-70B-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / general QA / reasoning (when mixed)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>High-quality instruction-chat conversation datasets (human/curated conversational traces) used in mixture for SFT</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Eurus-7B mix: 32K UltraChat + 30K ShareGPT + 50K OpenOrca; Eurus-70B mix: 63K UltraChat + 30K ShareGPT + 70K OpenOrca (counts provided by paper)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>General instruction-following style, conversational diversity, human-like demonstrations (not specialized multi-turn reasoning trees)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>IFEval prompt-level loose score, other aggregate evals</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Used in mixture with UltRAINTERACT to produce best SFT checkpoints; pure UltraInteract-only SFT shows good reasoning but much worse IFEval/instruction-following (e.g., UltraInteract-only IFEval 17.4 vs full-mix Eurus-7B-SFT 44.0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>UltraInteract-only SFT avg 47.7 (good reasoning) but poor instruction-following (IFEval 17.4); open-source-only SFT avg 37.0 (worse reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Mixing UltRAINTERACT with conversational alignment datasets recovers instruction-following while keeping strong reasoning performance (example: IFEval improves from 17.4 to 44.0 in Eurus-7B-SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>UltRAINTERACT should be mixed with general instruction-chat data (UltraChat/ShareGPT/OpenOrca) to achieve both strong reasoning and good instruction-following; training only on UltRAINTERACT hurts instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e224.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preference pairs (UltRAINTERACT + UltraFeedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paired preference data from UltRAINTERACT (multi-turn pairs) augmented with UltraFeedback pairs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paired correct vs incorrect action/trajectory examples used for preference learning experiments (DPO, KTO, NCA) and reward modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-7B / Eurus-70B (preference learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning (multi-turn), coding, math</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Paired preference examples: multi-turn trajectory pairs (correct vs incorrect) plus augmented single-turn NxN pairs for RM</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>UltRAINTERACT multi-turn pairs: 220K; UltraFeedback pairs: 340K; augmented single-turn action pairs: 240K (augmentation procedure described)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>High-quality objective preference labels (correctness against ground-truth), multi-turn comparisons at each turn, paired structure prevents reliance on surface shortcuts, ensures diversity by sampling incorrect actions from different actor models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream pass@1, multi-turn success rate, RM correlation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Used for preference learning and RM; KTO and NCA training on these pairs improved math and multi-turn performance; RM trained on these pairs (with DR objective) improved reranking and correlation (Eurus-RM-7B AutoJ 64.5, MT-Bench 72.9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>No preference learning (SFT only) or preference learning with DPO gave worse or mixed results: e.g., Eurus-7B-SFT avg 46.5; +DPO 44.5 (degradation); +KTO 48.8 (improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Preference learning with KTO/NCA on these pairs yields modest absolute gains in overall averages (e.g., Eurus-7B: +2.3 absolute with KTO), with larger consistent gains in math and multi-turn metrics; DPO on same data decreased overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>High-quality paired multi-turn preference data enables effective preference learning, but the choice of preference-learning algorithm matters: KTO/NCA improve reasoning and multi-turn ability on these pairs, while DPO often degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e224.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Preference Optimization (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference learning algorithm used in the paper's experiments that optimizes a relative-ranking objective; in these experiments DPO decreased absolute rewards for chosen solutions and hurt reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-7B / Eurus-70B (preference learning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning (math, coding), multi-turn tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Paired preference data from UltRAINTERACT (220K multi-turn pairs) and UltraFeedback (340K pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>220K + 340K pairs (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Optimizes relative preference differences (Bradley-Terry derived), does not explicitly increase absolute rewards of chosen items</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>aggregate average across reasoning/coding benchmarks, pass@1, multi-turn success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Eurus-7B + DPO: overall average dropped from 46.5 (SFT) to 44.5 (Table 3). For Eurus-70B DPO training failed (rewards collapsed to -infinity) and hence could not be applied successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Eurus-7B-SFT baseline average 46.5 (no preference learning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>-2.0 absolute average (degradation) for Eurus-7B; failure for Eurus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>DPO's relative-only optimization tends to push down absolute rewards of chosen (correct) reasoning solutions in these reasoning datasets, leading to degraded downstream reasoning performance; DPO may be less suitable for correctness-sensitive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e224.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KTO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KTO (Model alignment as prospect theoretic optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference-learning algorithm evaluated in the paper that, unlike DPO, increases absolute rewards for chosen items and consistently improves math and multi-turn reasoning performance when applied to UltRAINTERACT pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-7B / Eurus-70B (preference learning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning (math, multi-turn), coding</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Paired preference data (UltRAINTERACT multi-turn pairs + UltraFeedback)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>220K + 340K pairs (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Optimizes an objective that increases absolute rewards of chosen solutions (prospect-theoretic style), maintains or raises chosen rewards while lowering rejected ones</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>aggregate average across benchmarks, pass@1, multi-turn success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Eurus-7B + KTO overall avg increased from 46.5 to 48.8 (+2.3); Eurus-70B + KTO increased from 57.1 to 58.4 (+1.3). KTO shows consistent improvements on five math benchmarks and multi-turn evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Eurus-SFT baseline (no preference learning) e.g., 46.5 (Eurus-7B-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Eurus-7B: +2.3 absolute overall avg; larger consistent gains on math/multi-turn subsets</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Algorithms that increase absolute reward values for chosen (correct) reasoning trajectories (like KTO) lead to better reasoning performance than algorithms that only optimize relative ranking (like DPO) on UltRAINTERACT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e224.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NCA (Noise Contrastive Alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preference-learning / alignment algorithm evaluated in the paper (Chen et al., 2024a cited) that, like KTO, generally raises chosen rewards and improves reasoning performance when trained on UltRAINTERACT pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-7B / Eurus-70B (preference learning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning (math, multi-turn), coding</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Paired preference data from UltRAINTERACT and UltraFeedback</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>220K + 340K pairs (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Contrastive-style objective that can increase absolute rewards of chosen data while decreasing rejected rewards</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>aggregate average scores, pass@1, multi-turn success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Eurus-7B + NCA overall avg improved to 48.1 (from 46.5 SFT); Eurus-70B + NCA overall avg improved to 59.0 (from 57.1 SFT). NCA improves math and multi-turn performance though effects vary across other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Eurus-SFT baseline (no preference learning) e.g., 46.5 for 7B</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Eurus-7B: +1.6 to +2-ish absolute overall avg; Eurus-70B: +1.9 absolute overall avg</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Noise-contrastive style preference objectives (NCA) that encourage increasing chosen rewards are effective for reasoning preference learning on UltRAINTERACT, similar to KTO and unlike DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e224.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ground-truth SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SFT using original dataset ground-truth rationales/answers instead of actor-generated actions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where SFT replaces generated actions with original ground-truth rationales/answers (where available); this variant underperforms the UltRAINTERACT-generated-action SFT, showing the importance of the dataset's action-format and decomposition style.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-7B (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning (math, coding), instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Ground-truth solutions/rationales from original datasets (used where available) for SFT</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Not explicitly aggregated beyond dataset sizes; used to replace generated actions in ablation</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Human-provided ground-truth rationales/answers without the tree-structured, code-as-action, multi-turn trajectory format</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>aggregate averages across reasoning/coding/IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Ground-truth SFT average reported as 44.0 (Table 5) versus Eurus-7B-SFT (with UltRAINTERACT actions + mixture) 53.6 in the same table setup</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Eurus-7B-SFT (full pipeline with UltRAINTERACT generated actions + alignment mixtures) avg 53.6</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Ground-truth SFT is worse than UltRAINTERACT-based SFT by -9.6 absolute avg (44.0 vs 53.6) in the reported ablation, indicating the generated-action format and decomposition patterns in UltRAINTERACT are valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Replacing the generated action-format data with plain ground-truth solutions/rationales reduces downstream reasoning performance, suggesting the action/trajectory format and decomposition patterns in UltRAINTERACT contribute to learning reasoning behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e224.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eurus-RM-7B & DR objective</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eurus Reward Model (Eurus-RM-7B) trained with augmented Bradley-Terry + Direct-Reward (DR) term</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward model trained from Eurus-7B-SFT, using UltRAINTERACT (multi-turn and augmented single-turn pairs), UltraFeedback, and UltraSafety; training objective augments Bradley-Terry with a Direct-Reward term to increase absolute reward of chosen solutions and decrease rejected ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Eurus-RM-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reward modeling for reasoning and general chat, reranking of candidate answers</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Reward modeling data: UltRAINTERACT multi-turn pairs (220K), augmented single-turn pairs (240K), UltraFeedback pairs (340K), UltraSafety pairs (~3K)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Approximately 220K + 240K + 340K + 3K pairs as used in training (paper reports these counts)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Mixture balances reasoning-focused pairs (UltRAINTERACT) and broader feedback/safety pairs (UltraFeedback/UltraSafety); objective explicitly encourages larger absolute rewards for chosen data</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>RM correlation metrics (RewardBench splits, AutoJ, MT-Bench), downstream reranking pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Eurus-RM-7B achieved RewardBench Avg ~82.4, AutoJ overall 64.5, MT-Bench 72.9 (outperforming several baselines and sometimes GPT-4 on subsets); reranking yields consistent pass@1 improvements across HumanEval/MBPP/GSM8K/MATH versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Ablation w/o Direct-Reward (w/o E_DR) lowers RewardBench avg to 78.3 and reasoning split from 87.0 to 77.5 (Table 4); other baselines (Starling-RM-34B) have mixed worse reranking performance on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Augmenting BT with DR produced a substantial gain in RM reasoning quality (e.g., RewardBench 'Reasoning' 87.0 vs 77.5 without DR) and improved reranking outcomes on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Training RMs on UltRAINTERACT-augmented pair mixtures and optimizing an objective that increases absolute rewards for chosen solutions significantly improves RM correlation on reasoning tasks and yields stronger reranking performance; the DR term is particularly important for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e224.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e224.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Actor-generated synthetic trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Actor-generated actions and trajectories (GPT-3.5 / GPT-4 critique) used to synthesize training pairs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses actor LLMs (GPT-3.5 Turbo by default, with fallback to stronger models incl. GPT-4 Turbo) to generate diverse action trajectories and uses a strong critique model (GPT-4) plus environment execution to produce observations and critiques used in trajectories and pair construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Data-generation pipeline (actor: GPT-3.5 Turbo / GPT-4 Turbo; critique: GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data generation for reasoning/coding/multi-turn tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Synthetic data: actor-generated reasoning chains and code actions executed in environment (Python interpreter) with execution observations and GPT-4 critique; used to produce correct/incorrect action pairs</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Sampling process: up to 20 actions sampled then repeated up to 3 times with progressively stronger actors; used to collect the 220K multi-turn pairs and correct actions set (287K) in UltRAINTERACT</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Synthetic but grounded via execution and GPT-4 critique; includes diverse planning strategies (CoT, modularization), ensures syntax checks and sampling across actor models to increase diversity and reduce surface shortcuts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: downstream SFT / preference / RM performance after training on data derived from these actors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Using actor-generated trajectories (with GPT-4 critique and execution) enabled creation of UltRAINTERACT which provided large performance improvements in SFT/RM/preference pipelines as reported; the dataset's formats (actions, decompositions) outperform raw ground-truth-only training in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Ground-truth-only SFT and open-source-only SFT baselines (see other entries) performed worse for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Actor-synthesized action-format data (with critique and execution) led to notable lifts in reasoning performance versus using only ground-truth answers/rationales in SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>High-quality synthetic trajectories generated by actors and validated by execution + a strong critique model are an effective way to produce diverse and instructive reasoning training data that improves downstream reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing LLM Reasoning Generalists with Preference Trees', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>Kto: Model alignment as prospect theoretic optimization <em>(Rating: 2)</em></li>
                <li>Noise contrastive alignment of language models with explicit rewards <em>(Rating: 2)</em></li>
                <li>Ultrafeedback: Boosting language models with highquality feedback <em>(Rating: 2)</em></li>
                <li>Openorca: An open dataset of gpt augmented flan reasoning traces <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-224",
    "paper_id": "paper-ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "UltRAINTERACT",
            "name_full": "UltRAINTERACT (Tree-structured Alignment Data for Reasoning)",
            "brief_description": "A large, high-quality alignment dataset introduced in this paper consisting of tree-structured, multi-turn preference trees for challenging reasoning tasks (math, coding, logic), containing diverse reasoning chains, multi-turn interaction trajectories, and paired correct/incorrect actions for preference learning and SFT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Eurus (SFT / Preference Learning / RM experiments)",
            "model_size": "7B / 70B (used for different Eurus checkpoints)",
            "training_stage": "multiple",
            "task_type": "reasoning (math, coding, logical reasoning), multi-turn interaction",
            "is_scientific_domain": false,
            "data_type": "Tree-structured multi-turn alignment data: reasoning chains (CoT and modularization), code-as-action trajectories, multi-turn interactions with environment and critique, and paired correct/incorrect actions for preference learning",
            "data_size": "86K instructions; 220K action pairs (multi-turn); paper also reports 287K correct actions used for SFT and an augmentation of 240K single-turn pairs for RM",
            "data_properties": "High difficulty, diversity across reasoning patterns and datasets, multi-turn interactions, ground-truth solutions available (to ensure high-quality oversight), code-executable actions, paired correct/incorrect nodes (preference trees)",
            "performance_metric": "various: pass@1 (coding), accuracy on math benchmarks, multi-turn success rate at Turn 5, RM correlation metrics (AutoJ, MT-Bench), aggregated average scores",
            "performance_with_data": "Using UltRAINTERACT in SFT + standard alignment mixtures yields state-of-the-art open-source reasoning performance (Eurus-7B-SFT and Eurus-70B-SFT). Examples reported: Eurus-7B-SFT (with UltRAINTERACT + other alignment data) overall averages reported in the paper (different eval sets) e.g. avg ~46.5 (Table 3) and an alternate aggregate avg 53.6 (Table 5) depending on metric set; RM trained with UltRAINTERACT (Eurus-RM-7B) achieves AutoJ overall 64.5 and MT-Bench 72.9.",
            "performance_baseline": "SFT trained on open-source only alignment data: avg 37.0 (Table 5, 'Open-source Only'); 'Ground-truth' variant (replace generated actions with original dataset GT) avg 44.0 (Table 5)",
            "performance_lift": "Mixing UltRAINTERACT into SFT vs open-source-only SFT yields large absolute gains (example: +16.6 absolute avg in Table 5: 53.6 vs 37.0); used in preference learning and RM also yields improvements in math & multi-turn performance and RM correlation/reranking gains.",
            "compares_data_types": true,
            "key_finding": "Tree-structured, multi-turn, high-quality preference data (UltRAINTERACT) materially improves reasoning and multi-turn interaction abilities when included in SFT and preference/RM training; however, it should be mixed with other alignment data to preserve instruction-following abilities.",
            "uuid": "e224.0",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "SFT Mixture (UltraChat / ShareGPT / OpenOrca)",
            "name_full": "Mixture of instruction-conversational alignment datasets (UltraChat, ShareGPT, OpenOrca) used in supervised fine-tuning",
            "brief_description": "Common conversational/instruction datasets mixed into SFT to improve general instruction-following and conversational ability alongside UltRAINTERACT's reasoning data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Eurus-7B-SFT (and Eurus-70B-SFT)",
            "model_size": "7B / 70B",
            "training_stage": "SFT",
            "task_type": "instruction following / general QA / reasoning (when mixed)",
            "is_scientific_domain": false,
            "data_type": "High-quality instruction-chat conversation datasets (human/curated conversational traces) used in mixture for SFT",
            "data_size": "Eurus-7B mix: 32K UltraChat + 30K ShareGPT + 50K OpenOrca; Eurus-70B mix: 63K UltraChat + 30K ShareGPT + 70K OpenOrca (counts provided by paper)",
            "data_properties": "General instruction-following style, conversational diversity, human-like demonstrations (not specialized multi-turn reasoning trees)",
            "performance_metric": "IFEval prompt-level loose score, other aggregate evals",
            "performance_with_data": "Used in mixture with UltRAINTERACT to produce best SFT checkpoints; pure UltraInteract-only SFT shows good reasoning but much worse IFEval/instruction-following (e.g., UltraInteract-only IFEval 17.4 vs full-mix Eurus-7B-SFT 44.0).",
            "performance_baseline": "UltraInteract-only SFT avg 47.7 (good reasoning) but poor instruction-following (IFEval 17.4); open-source-only SFT avg 37.0 (worse reasoning).",
            "performance_lift": "Mixing UltRAINTERACT with conversational alignment datasets recovers instruction-following while keeping strong reasoning performance (example: IFEval improves from 17.4 to 44.0 in Eurus-7B-SFT).",
            "compares_data_types": true,
            "key_finding": "UltRAINTERACT should be mixed with general instruction-chat data (UltraChat/ShareGPT/OpenOrca) to achieve both strong reasoning and good instruction-following; training only on UltRAINTERACT hurts instruction-following.",
            "uuid": "e224.1",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Preference pairs (UltRAINTERACT + UltraFeedback)",
            "name_full": "Paired preference data from UltRAINTERACT (multi-turn pairs) augmented with UltraFeedback pairs",
            "brief_description": "Paired correct vs incorrect action/trajectory examples used for preference learning experiments (DPO, KTO, NCA) and reward modeling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Eurus-7B / Eurus-70B (preference learning)",
            "model_size": "7B / 70B",
            "training_stage": "multiple",
            "task_type": "reasoning (multi-turn), coding, math",
            "is_scientific_domain": false,
            "data_type": "Paired preference examples: multi-turn trajectory pairs (correct vs incorrect) plus augmented single-turn NxN pairs for RM",
            "data_size": "UltRAINTERACT multi-turn pairs: 220K; UltraFeedback pairs: 340K; augmented single-turn action pairs: 240K (augmentation procedure described)",
            "data_properties": "High-quality objective preference labels (correctness against ground-truth), multi-turn comparisons at each turn, paired structure prevents reliance on surface shortcuts, ensures diversity by sampling incorrect actions from different actor models",
            "performance_metric": "downstream pass@1, multi-turn success rate, RM correlation",
            "performance_with_data": "Used for preference learning and RM; KTO and NCA training on these pairs improved math and multi-turn performance; RM trained on these pairs (with DR objective) improved reranking and correlation (Eurus-RM-7B AutoJ 64.5, MT-Bench 72.9).",
            "performance_baseline": "No preference learning (SFT only) or preference learning with DPO gave worse or mixed results: e.g., Eurus-7B-SFT avg 46.5; +DPO 44.5 (degradation); +KTO 48.8 (improvement).",
            "performance_lift": "Preference learning with KTO/NCA on these pairs yields modest absolute gains in overall averages (e.g., Eurus-7B: +2.3 absolute with KTO), with larger consistent gains in math and multi-turn metrics; DPO on same data decreased overall performance.",
            "compares_data_types": true,
            "key_finding": "High-quality paired multi-turn preference data enables effective preference learning, but the choice of preference-learning algorithm matters: KTO/NCA improve reasoning and multi-turn ability on these pairs, while DPO often degrades performance.",
            "uuid": "e224.2",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DPO",
            "name_full": "Direct Preference Optimization (DPO)",
            "brief_description": "A preference learning algorithm used in the paper's experiments that optimizes a relative-ranking objective; in these experiments DPO decreased absolute rewards for chosen solutions and hurt reasoning performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Eurus-7B / Eurus-70B (preference learning experiments)",
            "model_size": "7B / 70B",
            "training_stage": "DPO",
            "task_type": "reasoning (math, coding), multi-turn tasks",
            "is_scientific_domain": false,
            "data_type": "Paired preference data from UltRAINTERACT (220K multi-turn pairs) and UltraFeedback (340K pairs)",
            "data_size": "220K + 340K pairs (as used in experiments)",
            "data_properties": "Optimizes relative preference differences (Bradley-Terry derived), does not explicitly increase absolute rewards of chosen items",
            "performance_metric": "aggregate average across reasoning/coding benchmarks, pass@1, multi-turn success",
            "performance_with_data": "Eurus-7B + DPO: overall average dropped from 46.5 (SFT) to 44.5 (Table 3). For Eurus-70B DPO training failed (rewards collapsed to -infinity) and hence could not be applied successfully.",
            "performance_baseline": "Eurus-7B-SFT baseline average 46.5 (no preference learning)",
            "performance_lift": "-2.0 absolute average (degradation) for Eurus-7B; failure for Eurus-70B",
            "compares_data_types": true,
            "key_finding": "DPO's relative-only optimization tends to push down absolute rewards of chosen (correct) reasoning solutions in these reasoning datasets, leading to degraded downstream reasoning performance; DPO may be less suitable for correctness-sensitive reasoning tasks.",
            "uuid": "e224.3",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "KTO",
            "name_full": "KTO (Model alignment as prospect theoretic optimization)",
            "brief_description": "A preference-learning algorithm evaluated in the paper that, unlike DPO, increases absolute rewards for chosen items and consistently improves math and multi-turn reasoning performance when applied to UltRAINTERACT pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Eurus-7B / Eurus-70B (preference learning experiments)",
            "model_size": "7B / 70B",
            "training_stage": "multiple",
            "task_type": "reasoning (math, multi-turn), coding",
            "is_scientific_domain": false,
            "data_type": "Paired preference data (UltRAINTERACT multi-turn pairs + UltraFeedback)",
            "data_size": "220K + 340K pairs (as used in experiments)",
            "data_properties": "Optimizes an objective that increases absolute rewards of chosen solutions (prospect-theoretic style), maintains or raises chosen rewards while lowering rejected ones",
            "performance_metric": "aggregate average across benchmarks, pass@1, multi-turn success",
            "performance_with_data": "Eurus-7B + KTO overall avg increased from 46.5 to 48.8 (+2.3); Eurus-70B + KTO increased from 57.1 to 58.4 (+1.3). KTO shows consistent improvements on five math benchmarks and multi-turn evaluations.",
            "performance_baseline": "Eurus-SFT baseline (no preference learning) e.g., 46.5 (Eurus-7B-SFT)",
            "performance_lift": "Eurus-7B: +2.3 absolute overall avg; larger consistent gains on math/multi-turn subsets",
            "compares_data_types": true,
            "key_finding": "Algorithms that increase absolute reward values for chosen (correct) reasoning trajectories (like KTO) lead to better reasoning performance than algorithms that only optimize relative ranking (like DPO) on UltRAINTERACT.",
            "uuid": "e224.4",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "NCA",
            "name_full": "NCA (Noise Contrastive Alignment)",
            "brief_description": "A preference-learning / alignment algorithm evaluated in the paper (Chen et al., 2024a cited) that, like KTO, generally raises chosen rewards and improves reasoning performance when trained on UltRAINTERACT pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Eurus-7B / Eurus-70B (preference learning experiments)",
            "model_size": "7B / 70B",
            "training_stage": "multiple",
            "task_type": "reasoning (math, multi-turn), coding",
            "is_scientific_domain": false,
            "data_type": "Paired preference data from UltRAINTERACT and UltraFeedback",
            "data_size": "220K + 340K pairs (as used in experiments)",
            "data_properties": "Contrastive-style objective that can increase absolute rewards of chosen data while decreasing rejected rewards",
            "performance_metric": "aggregate average scores, pass@1, multi-turn success",
            "performance_with_data": "Eurus-7B + NCA overall avg improved to 48.1 (from 46.5 SFT); Eurus-70B + NCA overall avg improved to 59.0 (from 57.1 SFT). NCA improves math and multi-turn performance though effects vary across other tasks.",
            "performance_baseline": "Eurus-SFT baseline (no preference learning) e.g., 46.5 for 7B",
            "performance_lift": "Eurus-7B: +1.6 to +2-ish absolute overall avg; Eurus-70B: +1.9 absolute overall avg",
            "compares_data_types": true,
            "key_finding": "Noise-contrastive style preference objectives (NCA) that encourage increasing chosen rewards are effective for reasoning preference learning on UltRAINTERACT, similar to KTO and unlike DPO.",
            "uuid": "e224.5",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Ground-truth SFT",
            "name_full": "SFT using original dataset ground-truth rationales/answers instead of actor-generated actions",
            "brief_description": "An ablation where SFT replaces generated actions with original ground-truth rationales/answers (where available); this variant underperforms the UltRAINTERACT-generated-action SFT, showing the importance of the dataset's action-format and decomposition style.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Eurus-7B (ablation)",
            "model_size": "7B",
            "training_stage": "SFT",
            "task_type": "reasoning (math, coding), instruction-following",
            "is_scientific_domain": false,
            "data_type": "Ground-truth solutions/rationales from original datasets (used where available) for SFT",
            "data_size": "Not explicitly aggregated beyond dataset sizes; used to replace generated actions in ablation",
            "data_properties": "Human-provided ground-truth rationales/answers without the tree-structured, code-as-action, multi-turn trajectory format",
            "performance_metric": "aggregate averages across reasoning/coding/IFEval",
            "performance_with_data": "Ground-truth SFT average reported as 44.0 (Table 5) versus Eurus-7B-SFT (with UltRAINTERACT actions + mixture) 53.6 in the same table setup",
            "performance_baseline": "Eurus-7B-SFT (full pipeline with UltRAINTERACT generated actions + alignment mixtures) avg 53.6",
            "performance_lift": "Ground-truth SFT is worse than UltRAINTERACT-based SFT by -9.6 absolute avg (44.0 vs 53.6) in the reported ablation, indicating the generated-action format and decomposition patterns in UltRAINTERACT are valuable.",
            "compares_data_types": true,
            "key_finding": "Replacing the generated action-format data with plain ground-truth solutions/rationales reduces downstream reasoning performance, suggesting the action/trajectory format and decomposition patterns in UltRAINTERACT contribute to learning reasoning behaviors.",
            "uuid": "e224.6",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Eurus-RM-7B & DR objective",
            "name_full": "Eurus Reward Model (Eurus-RM-7B) trained with augmented Bradley-Terry + Direct-Reward (DR) term",
            "brief_description": "A reward model trained from Eurus-7B-SFT, using UltRAINTERACT (multi-turn and augmented single-turn pairs), UltraFeedback, and UltraSafety; training objective augments Bradley-Terry with a Direct-Reward term to increase absolute reward of chosen solutions and decrease rejected ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Eurus-RM-7B",
            "model_size": "7B",
            "training_stage": "multiple",
            "task_type": "reward modeling for reasoning and general chat, reranking of candidate answers",
            "is_scientific_domain": false,
            "data_type": "Reward modeling data: UltRAINTERACT multi-turn pairs (220K), augmented single-turn pairs (240K), UltraFeedback pairs (340K), UltraSafety pairs (~3K)",
            "data_size": "Approximately 220K + 240K + 340K + 3K pairs as used in training (paper reports these counts)",
            "data_properties": "Mixture balances reasoning-focused pairs (UltRAINTERACT) and broader feedback/safety pairs (UltraFeedback/UltraSafety); objective explicitly encourages larger absolute rewards for chosen data",
            "performance_metric": "RM correlation metrics (RewardBench splits, AutoJ, MT-Bench), downstream reranking pass@1",
            "performance_with_data": "Eurus-RM-7B achieved RewardBench Avg ~82.4, AutoJ overall 64.5, MT-Bench 72.9 (outperforming several baselines and sometimes GPT-4 on subsets); reranking yields consistent pass@1 improvements across HumanEval/MBPP/GSM8K/MATH versus baselines.",
            "performance_baseline": "Ablation w/o Direct-Reward (w/o E_DR) lowers RewardBench avg to 78.3 and reasoning split from 87.0 to 77.5 (Table 4); other baselines (Starling-RM-34B) have mixed worse reranking performance on some tasks.",
            "performance_lift": "Augmenting BT with DR produced a substantial gain in RM reasoning quality (e.g., RewardBench 'Reasoning' 87.0 vs 77.5 without DR) and improved reranking outcomes on reasoning tasks.",
            "compares_data_types": true,
            "key_finding": "Training RMs on UltRAINTERACT-augmented pair mixtures and optimizing an objective that increases absolute rewards for chosen solutions significantly improves RM correlation on reasoning tasks and yields stronger reranking performance; the DR term is particularly important for reasoning.",
            "uuid": "e224.7",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Actor-generated synthetic trajectories",
            "name_full": "Actor-generated actions and trajectories (GPT-3.5 / GPT-4 critique) used to synthesize training pairs",
            "brief_description": "The paper uses actor LLMs (GPT-3.5 Turbo by default, with fallback to stronger models incl. GPT-4 Turbo) to generate diverse action trajectories and uses a strong critique model (GPT-4) plus environment execution to produce observations and critiques used in trajectories and pair construction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Data-generation pipeline (actor: GPT-3.5 Turbo / GPT-4 Turbo; critique: GPT-4)",
            "model_size": null,
            "training_stage": "multiple",
            "task_type": "data generation for reasoning/coding/multi-turn tasks",
            "is_scientific_domain": false,
            "data_type": "Synthetic data: actor-generated reasoning chains and code actions executed in environment (Python interpreter) with execution observations and GPT-4 critique; used to produce correct/incorrect action pairs",
            "data_size": "Sampling process: up to 20 actions sampled then repeated up to 3 times with progressively stronger actors; used to collect the 220K multi-turn pairs and correct actions set (287K) in UltRAINTERACT",
            "data_properties": "Synthetic but grounded via execution and GPT-4 critique; includes diverse planning strategies (CoT, modularization), ensures syntax checks and sampling across actor models to increase diversity and reduce surface shortcuts",
            "performance_metric": "indirect: downstream SFT / preference / RM performance after training on data derived from these actors",
            "performance_with_data": "Using actor-generated trajectories (with GPT-4 critique and execution) enabled creation of UltRAINTERACT which provided large performance improvements in SFT/RM/preference pipelines as reported; the dataset's formats (actions, decompositions) outperform raw ground-truth-only training in ablations.",
            "performance_baseline": "Ground-truth-only SFT and open-source-only SFT baselines (see other entries) performed worse for reasoning tasks.",
            "performance_lift": "Actor-synthesized action-format data (with critique and execution) led to notable lifts in reasoning performance versus using only ground-truth answers/rationales in SFT.",
            "compares_data_types": true,
            "key_finding": "High-quality synthetic trajectories generated by actors and validated by execution + a strong critique model are an effective way to produce diverse and instructive reasoning training data that improves downstream reasoning performance.",
            "uuid": "e224.8",
            "source_info": {
                "paper_title": "Advancing LLM Reasoning Generalists with Preference Trees",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2
        },
        {
            "paper_title": "Kto: Model alignment as prospect theoretic optimization",
            "rating": 2
        },
        {
            "paper_title": "Noise contrastive alignment of language models with explicit rewards",
            "rating": 2
        },
        {
            "paper_title": "Ultrafeedback: Boosting language models with highquality feedback",
            "rating": 2
        },
        {
            "paper_title": "Openorca: An open dataset of gpt augmented flan reasoning traces",
            "rating": 1
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.023935249999999998,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Advancing LLM Reasoning Generalists with Preference Trees</h1>
<p>Lifan Yuan ${ }^{1,2}$, ${ }^{r}$ Ganqu Cui ${ }^{1 <em>}$, ${ }^{1}$ Hanbin Wang ${ }^{3,4 </em>}$, Ning Ding ${ }^{1,1}$, ${ }^{4}$ Xingyao Wang ${ }^{2}$, Jia Deng ${ }^{5}$, Boji Shan ${ }^{6}$, Huimin Chen ${ }^{1}$, Ruobing Xie ${ }^{7}$, Yankai Lin ${ }^{5}$, Zhenghao Liu ${ }^{3}$, Bowen Zhou ${ }^{1}$, Hao Peng ${ }^{2}$, Zhiyuan Liu ${ }^{1}$, ${ }^{4}$ Maosong Sun ${ }^{1}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ University of Illinois Urbana-Champaign ${ }^{3}$ Northeastern University<br>${ }^{4}$ ModelBest.Inc ${ }^{5}$ Renmin University of China ${ }^{6}$ BUPT ${ }^{7}$ Tencent<br>lifan4@illinois.edu cgq22@mails.tsinghua.edu.cn wanghanbinpanda@gmail.com</p>
<h4>Abstract</h4>
<p>We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a $33.3 \%$ pass@1 accuracy on LeetCode and $32.6 \%$ on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than $13.3 \%$. The strong performance of Eurus can be primarily attributed to UltRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltRAINTERACT, leads to a strong reward model. ${ }^{1}$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Evaluation results on LeetCode and TheoremQA, two challenging OOD coding and math benchmarks with only test sets. Our EURus-7B is comparable with baselines that are 10x larger and EURus-70B is the only one on par with GPT-3.5 Turbo.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Current alignment techniques have significantly advanced the development of open-source large language models (LLMs) that effectively meet user expectations and align with human values (Touvron et al., 2023; Tunstall et al., 2023). On complex reasoning, success has been achieved by specializing models for specific capabilities, such as coding (Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024) and solving math problems (Fu et al., 2023; Yue et al., 2023; Luo et al., 2023a; Toshniwal et al., 2024). However, these models still fall short, by large margins, of the most advanced proprietary models in their all-around capabilities to tackle a diverse range of challenging problems. We conjecture that this performance gap can be primarily attributed to (1) the lack of high-quality alignment data and (2) the underexploration of preference learning techniques for improving models' complex reasoning capabilities. In this paper, we take strides towards bridging this gap by addressing both factors and developing Eurus.</p>
<p>EURUS consists of a suite of LLMs finetuned from Mistral-7B (Jiang et al., 2023a) and CodeLLaMA-70B (Roziere et al., 2023). Across a diverse set of complex reasoning benchmarks that are mostly out-of-distribution (OOD), Eurus achieves state-of-the-art overall performance among all open-source models. In particular, Eurus excels in solving challenging problems that often require sophisticated planning, reasoning, tool integration, and the ability to interact with and learn from the environment and users. As shown in Figure 1, on university-level STEM questions TheoremQA (Chen et al., 2023) and competition-level coding problems LeetCode Contest (Guo et al., 2024a), Eurus-70B significantly outperforms all open-source models, achieving comparable performance to GPT-3.5 Turbo.
EURUS models are trained on UltRAINTERACT, our newly-curated, large-scale, and high-quality alignment data specifically designed to improve LLMs' reasoning capabilities. UltRAINTERACT consists of a diverse set of instructions spanning math, coding, and logical reasoning problems from 12 established datasets. For each instruction, UltRAINTERACT collects a preference tree that includes: (1) Diverse planning strategies in a unified pattern, such as sequential processing (Wei et al., 2022) and tool creation (Qian et al., 2023), followed by executing step-by-step actions formatted in either text or code, to provide divserse reasoning trajectories. (2) Multi-turn interaction trajectories with the environment and the critique, to improve models' capabilities to learn from feedback and correct previous errors (Wang et al., 2023b). (3) Paired correct and incorrect actions organized in tree structures, to facilitate preference learning. In total, UltRAINTERACT contains 86 K instructions and 220 K action pairs, where each pair consists of an instruction, a correct response, and an incorrect one. Conceptually, UltRAINTERACT's data resemble imbalanced binary trees as shown in Figure 2.
UltRAINTERACT can be used in both supervised fine-tuning and preference learning. Our experiments show that, using UltRAINTERACT along with established datasets in instruction fine-tuning already achieves strong performance. UltRAINTERACT further facilitates preference learning for reasoning tasks, improving the performance even further with KTO (Ethayarajh et al., 2024) and NCA (Chen et al., 2024a). Surprisingly, applied to an instruction finetuned Eurus model, DPO (Rafailov et al., 2023) hurts the performance.
Through careful analysis, we provide evidence that the performance in reasoning correlates with the value of rewards of chosen data-a higher final reward often indicates a better reasoning capability. Besides, our investigation suggests that DPO may be less suitable for reasoning tasks than KTO and NCA. Inspired by this fresh finding, we devise a new objective for reward modeling to augment the Bradley-Terry objective (Bradley \&amp; Terry, 1952), explicitly encouraging training to increase the absolute rewards of chosen solution and decrease those of rejected data. Furthermore, UltRAINTERACT leads to our reward model Eurus-RM-7B, which achieves a better correlation with human annotators than all existing models on AutoJ (Li et al., 2023a) and MT-Bench (Zheng et al., 2023), including GPT-4 (OpenAI, 2023). Eurus-RM-7B demonstrates especially strong preference modeling performance on reasoning tasks.
Checkpoints of our Eurus models, accompanying UltRAINTERACT alignment data to reproduce this research, will be publicly available.</p>
<h1>2 UltRAINTERACT: Tree-structured Alignment Data for Reasoning</h1>
<p>Solving complex problems often requires the model's capability in planning and reasoning, integrating with tools, and interacting with and learning from both the environment and the users. This is reflected in UltRAINTERACT's design choices: (1) Its instructions are diverse, challenging, and of a large scale (\$2.1); (2) It provides multi-turn trajectories that solve the input instruction through multiple turns of interaction with and learning from the environment and critique. At each turn, it
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left: CodeActInstruct (Wang et al., 2024) and Code-Feedback (Zheng et al., 2024); Middle: HH-RLHF (Bai et al., 2022); Right: UltRAINTERACT. Each instruction in UltRAINTERACT is constructed as a preference tree.
breaks down the problem into smaller ones ( $\S 2.2$ ). (3) UltRAINTERACT includes pairwise data to facilitate preference learning ( $\S 2.3$ ).
Conceptually, UltRAINTERACT collects a preference tree for each instruction, with the instruction being the root and each action a node (Figure 2). A trajectory is a root-to-leaf path consisting of a sequence of actions. In each preference tree, all nodes of correct actions and all trajectories ending with correct actions can be used for SFT. Paired correct and incorrect nodes or trajectories can be used for preference learning.</p>
<h3>2.1 Instruction Selection Emphasizing Complexity, Quality, and Diversity</h3>
<p>We target three representative reasoning tasks: math problem-solving, code generation, and logical reasoning. The complexity, quality, and diversity of the alignment data are crucial to the model's performance (Liu et al., 2023). Following Wang et al. (2023b), we select challenging problems that GPT-3.5-Turbo fails to solve. We intentionally restrict the selection of the datasets to those with ground-truth solutions, aiming to ensure highquality oversight signals rather than relying on LLM-as-a-judge annotation (Weyssow et al., 2024). Besides, the gold solutions also serve as references for the critique model to generate feedback. To promote UltRAINTERACT's diversity, we pick datasets of different categories. For each dataset, we include distinct reasoning patterns based on question categories or formulations necessary to solve the problems. Table 6 summarizes the datasets selected by UltRAINTERACT. Except for MATH, none of the training datasets is used in our evaluation.</p>
<h3>2.2 Decomposition and Interaction at Each Turn</h3>
<p>Figure 3 provides an illustrative example. In what follows, we connect the actor model with a Python interpreter as the "environment". Unless otherwise specified, we use GPT-3.5 Turbo as the actor model.</p>
<p>Following Wang et al. (2024), the actor model first decomposes the input problem into several sub-problems and then solves each by generating Python code pieces as actions and using the environment to execute them. To promote solution diversity, the actor model randomly samples one reasoning schema in the form of either CoT (Wei et al., 2022) or modularization programming (Qian et al., 2023; Yuan et al., 2023). The actor then generates actions in text or code to solve each sub-problem, with each step being marked by explicit notations.
Multi-turn interactions with the environment are often necessary to solve challenging problems (Wang et al., 2023b). To improve such capabilities of the models, UltRAINTERACT collects trajectories in which the actor model interacts with the environment and a critique model (a proxy for user) and refines its action based on their feedback.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustrative example of an UltRAINTERACT trajectory over two turns. In each turn, the actor model generates step-by-step reasoning chains, and the environment and the critique model provide observations and textual critique respectively.</p>
<p>The environment receives an action from the actor model along with the interaction history, and then the code interpreter returns two kinds of "Observation": (1) Python execution results, either program outputs or error traceback messages; (2) binary feedback, indicating whether the solution is correct or not. Then, the observations along with the history will be passed to a critique model, which locates the errors and provides suggestions for improvements. To avoid potential bias introduced by self-correction (Wang et al., 2023b; Xu et al., 2024), we adopt a stronger model, GPT-4, as the critique and ensure critique quality by providing GPT-4 with ground truth answers as references.
This procedure resembles Wang et al. (2024). However, we adopt more diverse reasoning patterns to teach LLMs to learn rationales rather than simply memorizing answers (Mitra et al., 2023), and learn to create and use tools (Qian et al., 2023; Yuan et al., 2023; Qin et al., 2023). Besides, we believe that it is important for LLMs to learn from the feedback provided by the critique rather than solely from observations of the environment.</p>
<h1>2.3 Preference Trees Facilitates Preference Learning Across Multiple Turns</h1>
<p>Unlike open-ended conversations, where human preference is ambiguous and challenging to specify, many reasoning tasks have clear and objective preferences for correct actions. The preference annotation is threfore an evaluation of the correctness of the solutions conditioning ground truth ones, which come with the datasets in UltRAINTERACT. This eliminates the need for human or LLM-based preference annotation and ensures high data quality. To facilitate preference learning, UltRAINTERACT pairs correct and incorrect actions.
Sampling Paired Correct and Incorrect Actions at Each Turn. For each instruction in UltRAINTERACT, we sample, from the actor model, a pair of correct and incorrect actions following $\S 2.2$. We follow Cui et al. (2023) to sample the pair from different actor models to ensure response diversity. To prevent models from exploiting shortcuts based on surface features, we exclude instances that fail to pass the Python syntax check.
Certain challenging problems in UltRAINTERACT pose difficulties in obtaining correct actions, even using strong actors such as GPT-4, with nearly zero pass@100 accuracies. To improve the pass rates of the actor models while keeping the expense under control, we sequentially take the following steps. (1) Directly sampling 20 actions and randomly keeping a correct one, if any. (2) If no correct action is obtained, we repeat the above process up to three times, progressively switching from more cost-effective models to the strong yet expensive GPT-4 Turbo. (3) For the remaining difficult problems where no correct action is acquired after the previous two steps, we provide the actor with ground-truth rationales and answers, and then apply various techniques to elicit correct actions. The specific information provided and the techniques applied vary depending on the tasks (Appendix A.2).</p>
<p>Tree-structured Action Pairs Across Multiple Turns. After each turn, the correct action concludes its trajectory. We expand the incorrect action into the next turn, and have the actor interact with the environment and the critique to refine its solution ( $\S 2.2$ ). We then repeat the procedures introduced earlier in this section to collect an additional action pair. By expanding the incorrect action, UltRAINTERACT can provide data to help models learn from feedback, and collect multiple action pairs for preference learning across multiple turns.
Conceptually, for every instruction, UltRAINTERACT constructs a binary preference tree with each action being a node (Figure 2). We cap the tree at a maximum of five turns.
Additional Instruction-action Pairs for Challenging Problems. We believe the challenging instructions that make it to step (3) above can provide valuable training signals. Therefore, for a subset of these problems with multiple ground truth solutions, we further sample additional correct actions to cover all ground truths. Accordingly, we further sample incorrect actions to pair with these additional correct actions, so that they can be used in both supervised fine-tuning and preference learning.
With the tree-structured data, UltRAINTERACT enables comparisons at every turn, in contrast to comparing only at the last turn (Bai et al., 2022), and thus can improve the models' interaction ability. Closing this section, Table 1 summarizes some statistics of UltRAINTERACT, and more details are in Appendix A.4.</p>
<p>Table 1: Some statistics of UltRAINTERACT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Type <br> w/ Interaction?</th>
<th style="text-align: center;"># Instructions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Turns per Traj.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Tokens per Traj.</th>
<th style="text-align: center;">Avg. # Traj per Ins.</th>
<th style="text-align: center;">Total <br> # Pairs</th>
<th style="text-align: center;"># Correct Answers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/ Tool?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T1</td>
<td style="text-align: center;">T2</td>
<td style="text-align: center;">T3</td>
<td style="text-align: center;">T4</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">22,928</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10,440</td>
<td style="text-align: center;">4,122</td>
<td style="text-align: center;">1,898</td>
<td style="text-align: center;">904</td>
<td style="text-align: center;">5,564</td>
<td style="text-align: center;">1,750.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">42,780</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2,757</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16,154</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">439.1</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">13,217</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">22,639</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10,708</td>
<td style="text-align: center;">3,521</td>
<td style="text-align: center;">1,459</td>
<td style="text-align: center;">723</td>
<td style="text-align: center;">6,228</td>
<td style="text-align: center;">1,521.9</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">44,750</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">2,083</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16,348</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">538.1</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">12,624</td>
</tr>
<tr>
<td style="text-align: center;">Coding</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20,463</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13,265</td>
<td style="text-align: center;">2,584</td>
<td style="text-align: center;">987</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">3,248</td>
<td style="text-align: center;">1,728.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">18,106</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8,495</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">92,618</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,070.4</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">78,634</td>
</tr>
<tr>
<td style="text-align: center;">Logic</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2,086</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,685</td>
<td style="text-align: center;">298</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">1,299.8</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1,750</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">4,467</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2,453</td>
<td style="text-align: center;">1,674</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1,266.7</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">7,958</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85,918</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">163,671</td>
<td style="text-align: center;">12,199</td>
<td style="text-align: center;">4,756</td>
<td style="text-align: center;">2,014</td>
<td style="text-align: center;">15,063</td>
<td style="text-align: center;">1,201.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">219,819</td>
</tr>
</tbody>
</table>
<h1>3 Eurus: State-of-the-art Open LLMs in Reasoning</h1>
<p>UltRAINTERACT helps us develop Eurus, a suite of LLMs and a reward model (RM).
Supervised Fine-Tuning. Eurus-7B-SFT is fine-tuned from Mistral-7B (Jiang et al., 2023a) and Eurus-70B-SFT from CodeLLaMA-70B (Roziere et al., 2023). First, we perform SFT using all correct actions (287K) in UltRAINTERACT. We find it yields better performance to discard interaction history and train only on correct leaf nodes in each tree. To improve general instruction-following ability, we include into our SFT data mixture UltraChat (Ding et al., 2023), ShareGPT ${ }^{2}$, and OpenOrca (Lian et al., 2023). Please find mixture ratios in Appendix B.</p>
<p>Perference Learning. Based on Eurus-SFT models, we explore three preference learning algorithms, DPO (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), and NCA (Chen et al., 2024a). Differently from SFT, here we include all multi-turn trajectory pairs in our UltRAINTERACT (220K) and include all UltraFeedback (Cui et al., 2023) pairs (340K).
Reward Modeling. Similarly to the preference learning, we use all 220 K multi-turn trajectory pairs from UltRAINTERACT; it is further augmented with the 240 K single-turn action pairs from UltRAINTERACT. More details are in the Appendix B. We include all 340K pairs from UltraFeedback and one pair for each instruction from UltraSafety (Guo et al., 2024b), totaling 3K. Eurus-RM-7B is initialized from Eurus-7B-SFT with a new linear layer.</p>
<p>Our findings in $\S 6$ indicate that the absolute values of rewards make a big difference in the models' reasoning performance. We therefore augment the established Bradley-Terry (BT) objective $\mathcal{L}<em _mathrm_DR="\mathrm{DR">{\mathrm{BT}}$ with an additional term $\mathcal{L}</em>$ to directly increase the reward of the chosen actions for instances from UltRAINTERACT, and decrease those of the rejected ones:}</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Open-source LLM baselines that we compare to.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>General Purpose</td>
<td>Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a), Zephyr-7B- $\beta$ (Tunstall et al., 2023), OpenChat-3.5-1210 (Wang et al., 2023a), Starling-LM-7B- $\alpha$ (Zhu et al., 2023), Mistral-8x7B-Instruct (Jiang et al., 2023a), DeepSeek-LLM-67B-Chat (DeepSeek-AI, 2024), QWen1.5-72B-Chat (Bai et al., 2023)</td>
</tr>
<tr>
<td>Coding</td>
<td>Magicoder-S-DS-6.7B (Wei et al., 2023), OpenCodeInterpreter (OpenCI for short, DS-6.7B/CL-70B) (Zheng et al., 2024), DeepSeek-Coder-33B-Instruct (Guo et al., 2024a), and CodeLLaMA-70B-Instruct(Koziere et al., 2023).</td>
</tr>
<tr>
<td>Math</td>
<td>MAmmoTH-7B-Mistral (Yue et al., 2023), WizardMath-7B-v1.1 (Luo et al., 2023a), OpenMath (Mistral-7B/CodeLLaMA-70B) (Toshniwal et al., 2024).</td>
</tr>
</tbody>
</table>
<p>For instances from other datasets, we train with $\mathcal{L}<em _theta="\theta">{\mathrm{BT}} \cdot \theta$ denotes the reward model's parameters, $r</em>}(\cdot)$ and $r_{\theta}\left(x, y_{r}\right)$ the rewards on the chosen and rejected actions respectively. Our ablation study demonstrates the importance of both $\mathcal{L<em _mathrm_DR="\mathrm{DR">{\mathrm{BT}}$ and $\mathcal{L}</em>$.}</p>
<h1>4 Evaluation of Eurus-7B and Eurus-70B</h1>
<p>Evaluation Setup. We consider both single-turn and multi-turn reasoning. For single-turn evaluation, we consider HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and LeetCode (Guo et al., 2024a) for coding, GSM-Plus (Li et al., 2024), MATH, TheoremQA (Chen et al., 2023), SVAMP (Patel et al., 2021), and ASDiv (Miao et al., 2020) for math, and BBH-Hard (Suzgun et al., 2022) for reasoning. We evaluate with pass@1 accuracy. We also use IFEval (Zhou et al., 2023) to assess the instruction-following ability and report the prompt-level loose score. For multi-turn evaluation, we adopt MINT (Wang et al., 2023b) and only consider the coding and math problems. We report the success rate at Turn 5. Please find further details on evaluation setups and evaluations beyond reasoning in Appendix C.</p>
<p>As shown in Table 2, we compare our Eurus with general-purpose models, and those specialized in coding and math of various sizes. We also summarize the results of GPT-3.5 Turbo and GPT-4 reported in previous works.</p>
<p>Table 3: Overall performance. All test sets except MATH are out-of-distribution to our models and most baselines. MAmmoTH, OpenChat, and Starling-LM have been trained on TheoremQA test sets. We strikethrough the contaminated numbers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Coding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reasoning</th>
<th style="text-align: center;">Ins-Following</th>
<th style="text-align: center;">Multi-Turn</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HumanE</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">LeetC.</td>
<td style="text-align: center;">GSM-Plus</td>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">Theo.QA</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;">BBH</td>
<td style="text-align: center;">IFEval</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$-\mathrm{7B}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.5</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr-7B- $\beta$</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">22.8</td>
</tr>
<tr>
<td style="text-align: center;">OpenChat-LL-1210</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">46.0</td>
</tr>
<tr>
<td style="text-align: center;">Starling-LM-7B- $\alpha$</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: center;">Magicoder-S-DS-6.7B</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: center;">OpenCI-33S-6.7B</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: center;">MAmmoTH-7B-Mistral</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath-7B-v1.1</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: center;">OpenMath-Mistral-7B</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;">Eurus-7B-SFT</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: center;">+ DPO</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">+ KTO</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">+ NCA</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: center;">$-\mathrm{4} / \mathrm{B}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral-8x7B-Instruct</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-Coder-33B-Ins</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: center;">$-\mathrm{70B}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CodeLLaMA-70B-Instruct</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-LM-67B-Chat</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">QWen1.5-72B-Chat</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: center;">OpenCI-CL-70B</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">46.3</td>
</tr>
<tr>
<td style="text-align: center;">OpenMath-CL-70B</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: center;">Eurus-70B-SFT</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: center;">+ KTO</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">58.4</td>
</tr>
<tr>
<td style="text-align: center;">+ NCA</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: center;">Proprietary Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">74.8</td>
</tr>
</tbody>
</table>
<h1>4.1 Results</h1>
<p>Results are shown in Table 3. We summarize the takeaways as follows:
EURUS, both the 7B and 70B variants, achieve the best overall performance among open-source models of similar sizes. Eurus even outperform specialized models in corresponding domains in many cases. Notably, Eurus-7B outperforms baselines that are $5 \times$ larger and Eurus-70B achieves better performance than GPT-3.5 Turbo. Eurus's instruction-following performance is among the best general-purpose models, substantially better than specialized ones.
Preference learning with UltRAINTERACT can further improve the performance, especially in math and the multi-turn ability. KTO and NCA consistently improve the models' performance in all five math benchmarks and mult-turn evaluations, while their effects vary in others. Since SFT models only use the single-turn data from UltRAINTERACT while preference learning uses the multi-turn ones, the improvements in interaction ability should also be attributed to UltRAINTERACT rather than the algorithms alone. Surprisingly, we observe that DPO hurts model performance on most benchmarks. DPO training of our 70B model fails since the rewards go down to $-\infty$. We analyze this phenomenon in $\S 6.1$.</p>
<h2>5 Evaluation of Eurus-RM-7B</h2>
<p>Evaluation Setup. We evaluate Eurus-RM-7B on three RM benchmarks, RewardBench (Lambert et al., 2024), AutoJ (Li et al., 2023a), and MT-Bench (Zheng et al., 2023). Aiming for a more realistic OOD evalation, we exclude the "prior sets" split from RewardBench, since many baselines train on the datasets that this split contains. We compare with PairRM (Jiang et al., 2023b), Starling-RM-7B/34B (Zhu et al., 2023), UltraRM-13B (Cui et al., 2023), GPT-3.5 Turbo, and GPT-4. To further explore EURUS-RM-7B's potential in improving models' performance through reranking, we use it to rerank Mistral-7B-Instruct-v0.2's responses on HumanEval, MBPP, GSM8K, and MATH. We report the results of random sampling, self-consistency, and Starling-RM-34B as baselines.</p>
<h3>5.1 Results</h3>
<p>Table 4 summarizes reward modeling performance, and Figure 4 plots some reranking results with others in Appendix D.1.
EURUS-RM-7B stands out as the best 7B RM overall, and achieves similar or better performance than much larger baselines. Particularly, it outperforms GPT-4 in certain tasks. EURUS-RM-7B achieves a better correlation with human experts than all existing models on AutoJ and MT-Bench, and it achieves comparable performance to the $5 \times$ larger Starling-RM-34B on RewardBench. On RewardBench, Eurus-RM-7B outperforms all baselines on the "Chat-Hard" split while achieving very competitive performance on the "Reasoning" split. Across the AutoJ splits, Eurus-RM-7B outperforms nearly all existing models, with the only exception being GPT-4's results on Coding.
Our training objective is beneficial in improving RM performance on hard problems and reasoning. Table 4 shows that optimizing $\mathcal{L}_{\mathrm{DR}}$ improves RM's reasoning ability, but BT modeling is still beneficial in equipping RM with abilities in general chatting as suggested in the "Chat-Hard" column, though its effect on reasoning may vary.
UltraINTERACT is compatible with other datasets like UltraFeedback and UltraSafety, and mixing these datasets can balance different RM abilities. Improving RM's capabilities in reasoning with UltRAINTERACT does not sacrifice others, which indicates that UltRAINTERACT can be a great ingredient for the training data mixture of reward models.
EURUS-RM-7B improves LLMs' reasoning performance by a large margin through reranking. EURUS-RM-7B consistently improves pass@1 accuracy across all tasks and performs better than $5 \times$ larger baseline Starling-RM-34B. Also, Eurus-RM-7B's reranking performance scales well with #responses per instruction, except a slight decrease in HumanEval</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results on reranking Mistral-7B-Instruct-v0.2's responses. Full results in Table 9.
Table 4: Results on reward modeling benchmarks. UF: UltraFeedback; US: UltraSafety. The best performance in each benchmark is in bold and the second best one is underlined. Most baseline results are from Jiang et al. (2023b) and Lambert et al. (2024).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Reward Bench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AutoJ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MT-Bench</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chat</td>
<td style="text-align: center;">Chat-Hard</td>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Others</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PairRM</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: center;">Starling-RM-7B</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;">Starling-RM-34B</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">60.4</td>
</tr>
<tr>
<td style="text-align: center;">UltraRM-13B</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">56.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT -4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">63.9</td>
</tr>
<tr>
<td style="text-align: center;">EURUS-RM-7B</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;">w/o $\mathcal{E}_{\text {DR }}$</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: center;">w/o $\mathcal{L}_{\text {RT }}$</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">w/o US</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;">w/o UF + US</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">67.2</td>
</tr>
</tbody>
</table>
<p>when increasing response number form 8 to 16 . In contrast, Starling-RM-34B suffers from severe performance drop on HumanEval and it consistently hurts model accuracy on MATH.</p>
<h1>6 Analysis</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward patterns of EURUS-7B preference learning with DPO, KTO, and NCA. For all algorithms, the rewards of rejected data keep decreasing and the margins between chosen and rejected data keep increasing. However, the rewards of chosen data decrease below zero in DPO while keeping increasing and staying positive in KTO and NCA. The absolute values of the reward in the last step (in red) of the three algorithms positively correlate with their performance in Table 3.</p>
<h3>6.1 Explicit Reward as A Proxy? Hypothesis for Preference Learning in Reasoning</h3>
<p>We investigate the reason why DPO behaves differently than KTO and NCA. We start by empirically inspecting the rewards throughout the preference learning process, as shown in Figure 5. Rewards for chosen rejected data both keep decreasing through DPO, though the rewards for chosen data is still higher hence the loss decreases. In KTO and NCA, the rewards of chosen data keep increasing with those of rejected data decreasing.
Therefore, we hypothesize it is the distinction in the trend of rewards that leads to the performance gap between DPO and the other two algorithms. This distinction can be attributed to that DPO, derived from the Bradley-Terry model, only optimizes the relative differences between chosen and rejected data overlooking the absolute values of the rewards. This is a non-issue in alignment with general human values where preference is "relative" and there</p>
<p>can be many valid answers to the same input. However, in reasoning tasks, the space of correct answers is much smaller than that of incorrect ones. Further, we notice that the rewards of chosen data in the last training step follow the ranking order of $\mathrm{KTO}&gt;\mathrm{NCA}&gt;\mathrm{DPO}$, positively correlate with their performance trends. Therefore, we believe that increasing the rewards of the chosen data is especially beneficial in preference learning for reasoning tasks.</p>
<h1>6.2 Ablation Study</h1>
<p>We study the impact of UltRAINTERACT and other open-source alignment data on Eurus-7B-SFT's performance. We consider three settings: (1) With original ground-truth answers, which replaces the generated actions with ground-truth
Table 5: Ablation study of SFT data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Coding</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">BBH</th>
<th style="text-align: center;">IFEval</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eurus-7B-SFT</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">53.6</td>
</tr>
<tr>
<td style="text-align: left;">Ground-truth</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: left;">Open-source Only</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: left;">UltraInteract Only</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">47.7</td>
</tr>
</tbody>
</table>
<p>rationales and answers from the original datasets. If no rationales are available, we use those from UltRAINTERACT. (2) Open-source data only. (3)UltRAINTERACT only. We evaluate with the same setting as $\S 4$ and report the averaged scores. See full results in Appendix E.
In Table 5, Eurus outperforms the "Grouth-truth" model on all tasks, confirming the advantage of UltRAINTERACT's designs of divide-and-conquer and code-as-action patterns, in line with conclusions of concurrent work (Chen et al., 2024b; Wang et al., 2024). Training only on open-source data without UltRAINTERACT greatly hurts the reasoning performance, confirming the effectiveness of UltRAINTERACT. Meanwhile, training only on UltRAINTERACT suffers a performance drop except for BBH, especially in instruction following. We attribute the performance drop to a worse instruction-following ability. This suggests the necessity of mixing UltRAINTERACT with other alignment data for better all-around supervised fine-tuning.</p>
<h2>7 Related Work</h2>
<p>Open LLMs in Reasoning. Open-source LLMs have shown remarkable progress in building specialists that excel in mathematics reasoning (Luo et al., 2023a; Yue et al., 2023; Toshniwal et al., 2024) or coding abilities (Roziere et al., 2023; Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024). On the contrary, mastering general reasoning capabilities still challenges open models, while the most advanced ones (DeepSeek-AI, 2024; Bai et al., 2023; Touvron et al., 2023; Jiang et al., 2024) are well behind proprietary models. More, these cutting-edge open general-purpose models maintain their alignment recipes confidential, which further hinders the replication and development of open-source reasoning models.
Preference Learning for Reasoning. Aligning language models from human or AI preferences has emerged as a prevalent approach in the open-source community (Tunstall et al., 2023; Bai et al., 2023) with the proposal of DPO (Rafailov et al., 2023) and high-quality preference datasets (Cui et al., 2023; Zhu et al., 2023). Different from open-domain chatbots, preference learning is largely underexplored in complex reasoning. Recent research showed performance degradation when applying DPO on reasoning tasks, but some newly proposed algorithms demonstrated a positive effect (Ethayarajh et al., 2024; Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). However, a deep understanding of preference learning, specifically its efficacy on complex reasoning, is not yet established.</p>
<h2>8 Conclusion</h2>
<p>We strive to narrow the huge gap between open-source models and proprietary models from the perspective of alignment. Our work pushes the boundaries of open-source reasoning generalists by (1) releasing a high-quality multi-turn reasoning dataset UltRAINTERACT with preference trees, (2) introducing Eurus-series LLMs which achieve new SOTA on challenging reasoning benchmarks and (3) providing insights on preference learning for reasoning through analysis, leading to new reward modeling objectives as well as a powerful reward model for reasoning.</p>
<h1>References</h1>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proc. of NAACL-HLT, 2019.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. ArXiv preprint, abs/2309.16609, 2023.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv preprint, abs/2204.05862, 2022.</p>
<p>Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39, 1952.</p>
<p>Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. ArXiv preprint, abs/2402.05369, 2024a.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.</p>
<p>Wenhu Chen, Ming Yin, Max W.F. Ku, Yixin Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. ArXiv preprint, abs/2305.12524, 2023.</p>
<p>Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. volume abs/2403.12881, 2024b.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. volume abs/2110.14168, 2021.</p>
<p>Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with highquality feedback. ArXiv preprint, abs/2310.01377, 2023.</p>
<p>DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. ArXiv preprint, abs/2401.02954, 2024.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Conference on Empirical Methods in Natural Language Processing, 2023.</p>
<p>Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. ArXiv preprint, abs/2402.01306, 2024.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In Proceedings of the International Conference on Machine Learning, 2023.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9, 2021.</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. ArXiv preprint, abs/2401.14196, 2024a.</p>
<p>Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Controllable preference optimization: Toward controllable multi-objective alignment. ArXiv preprint, abs/2402.19085, 2024b.</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. ArXiv preprint, abs/2310.06825, 2023a.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thophile Gervet, Thibaut Lavril, Thomas Wang, Timothe Lacroix, and William El Sayed. Mixtral of experts. 2024.</p>
<p>Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. In Annual Meeting of the Association for Computational Linguistics, 2023b.</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Daniel Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hanna Hajishirzi. Rewardbench: Evaluating reward models for language modeling. 2024.</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. ArXiv preprint, abs/2310.05470, 2023a.</p>
<p>Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. ArXiv preprint, abs/2402.19255, 2024.</p>
<p>Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. volume abs/2312.14852, 2023b.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. volume abs/2203.07814, 2022.</p>
<p>Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/Open-Orca/OpenOrca, 2023.</p>
<p>Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. 2023.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In Proceedings of ICLR, 2023.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583, 2023a.</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023b.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English math word problem solvers. In Proc. of ACL, 2020.</p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proc. of ACL, 2022.</p>
<p>Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching small language models how to reason. ArXiv preprint, abs/2311.11045, 2023.</p>
<p>Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. ArXiv preprint, abs/2402.14830, 2024.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proc. of ACL, 2015.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.</p>
<p>Cheng Qian, Chi Han, Yi Ren Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. In Conference on Empirical Methods in Natural Language Processing, 2023.</p>
<p>Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv preprint, abs/2307.16789, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. ArXiv preprint, abs/2305.18290, 2023.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jrmy Rapin, et al. Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950, 2023.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv preprint, abs/2402.03300, 2024.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv preprint, abs/2210.09261, 2022.</p>
<p>Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv: Arxiv-2402.10176, 2024.</p>
<p>Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantn Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023.</p>
<p>Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clmentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. ArXiv preprint, abs/2310.16944, 2023.</p>
<p>Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. ArXiv preprint, abs/2309.11235, 2023a.</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. ArXiv preprint, abs/2309.10691, 2023b.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. ArXiv preprint, abs/2402.01030, 2024.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903, 2022.</p>
<p>Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need, 2023.</p>
<p>Martin Weyssow, Aton Kamanda, and Houari Sahraoui. Codeultrafeedback: An llm-as-ajudge dataset for aligning large language models to coding preferences. ArXiv preprint, abs/2403.09032, 2024.</p>
<p>Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang. Perils of self-feedback: Self-bias amplifies in large language models. ArXiv preprint, abs/2402.11436, 2024.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proc. of EMNLP, 2018.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset requiring logical reasoning. In Proc. of ICLR, 2020.</p>
<p>Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Ren Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating and retrieving from specialized toolsets. ArXiv preprint, abs/2309.17428, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. ArXiv preprint, abs/2309.05653, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv preprint, abs/2306.05685, 2023.</p>
<p>Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. ArXiv preprint, abs/2402.14658, 2024.</p>
<p>Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. ArXiv preprint, abs/2311.07911, 2023.
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness \&amp; harmlessness with rlaif, 2023.</p>
<p>Table 6: UltRAINTERACT covers a diverse set of datasets spanning three tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Datasets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Math</td>
<td style="text-align: left;">GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), MathQA (Amini et al., 2019), <br> NumGlue (Mishra et al., 2022), TabMWP (Lu et al., 2023)</td>
</tr>
<tr>
<td style="text-align: left;">Coding</td>
<td style="text-align: left;">CodeContest (Li et al., 2022), TACO (Li et al., 2023b), WikiTableQuestions (Pasupat \&amp; Liang, <br> 2015), Magicoder-Evol-Instruct (Luo et al., 2023b; Wei et al., 2023)</td>
</tr>
<tr>
<td style="text-align: left;">Logic</td>
<td style="text-align: left;">ReClor (Yu et al., 2020), HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021)</td>
</tr>
</tbody>
</table>
<h1>A Additional Details in UltRAINTERACT Construction</h1>
<h2>A. 1 Dataset Details</h2>
<p>Math. We adopt GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), MathQA (Amini et al., 2019), and NumGLUE (Mishra et al., 2022)for mathematic reasoning, and include TabMWP (Lu et al., 2023) for tabular processing. We retain all the instructions for all datasets except MathQA, NumGLUE, and TabMWP. MathQA divides problems into different categories according to the topics and annotates the formula that indicates the pattern needed to solve each problem. We apply stratified sampling to sample at most five problems for each pattern and prioritize the problems that come from the longtail category. Numglue contains eight different reasoning tasks and we discard Task 5 (Reading Comprehension + Explicit Numerical Reasoning), Task 6 (Reading Comprehension + Implicit Numerical Reasoning), and Task 7 (Quantitative NLI) due to the simplicity Mishra et al. (2022). For TabMWP, we only keep the questions with difficulty levels 4 and 5 since the rest are too easy for current state-of-the-art models.</p>
<p>Code. We focus on programming with Python for the simplicity of integration of the interpreter. We use CodeContest (Li et al., 2022) and TACO (Li et al., 2023b), two competitionlevel coding datasets collected from various online platforms. We filter out the overlapped questions. Note that part of the questions in TACO only contain ground-truth solutions and do not contain test cases for evaluation, hence we apply GPT-4 to generate 12 test case inputs ( 4 basic inputs, 4 edge cases, and 4 large numbers) for each question and then execute the ground-truth solution snippets to produce outputs. Given that the two datasets mainly focus on competition problems that may deviate from real-world daily uses, we exclusively adopt Magicoder-Evol-Instruct (Luo et al., 2023b; Wei et al., 2023), the only dataset in our selection that does not contain test cases or ground-truth solutions. We employ GPT-4 Turbo to judge the correctness of generated code during interaction, and therefore we do not use this dataset for preference learning since we cannot rigorously construct pairs of correct and incorrect actions limited by the evaluation reliability. We also include WikiTableQuestions (Pasupat \&amp; Liang, 2015) for table processing with code.
Logical Reasoning. we use the multi-hop reasoning datasets HotpotQA (Yang et al., 2018) and StrategyQA (Geva et al., 2021), and the logical reasoning dataset ReClor (Yu et al., 2020). We follow the setting of Wang et al. (2023b) and convert HotpotQA to a generation task, removing the contexts and requiring LLMs to search relevant information using Wikipedia API.</p>
<h2>A. 2 Details on Preference Tree Construction</h2>
<p>Models Adopted for Incorrect Action Sampling. We randomly sample one model from Mistral-7B-Instruct-v0.2, DeepSeek-Coder-33B-Instruct, Mixtral-8x7B-Instruct, and DeepSeek-LLM-67B-Chat to generate one incorrect action to pair with each correct one.</p>
<h2>Correct Action Generation Based on Ground Truth Annotations.</h2>
<p>We adopt GPT-3.5 Turbo as the generator to generate correct actions based on ground truth considering the instruction-following ability. We provide different access to the ground truth information for different tasks, specifically: (1) For coding, where test cases are black boxes to reference solutions, we provide full access to the solution codes. The actor model will add step marks and corresponding explanations to the ground-truth code to make it</p>
<p>Table 7: Stats breakdown</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">w/ Tool?</th>
<th style="text-align: center;"># Prompts</th>
<th style="text-align: center;"># Pairs</th>
<th style="text-align: center;"># Correct Answers.</th>
<th style="text-align: center;">Avg. Length</th>
<th style="text-align: center;">Human Annotation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Has Answer?</td>
<td style="text-align: center;">Has Rationale?</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">4,522</td>
<td style="text-align: center;">10,277</td>
<td style="text-align: center;">17,392</td>
<td style="text-align: center;">1,746.7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">7,257</td>
<td style="text-align: center;">10,879</td>
<td style="text-align: center;">15,752</td>
<td style="text-align: center;">823.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7,474</td>
<td style="text-align: center;">22,905</td>
<td style="text-align: center;">34,667</td>
<td style="text-align: center;">1,189.0</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">7,471</td>
<td style="text-align: center;">25,765</td>
<td style="text-align: center;">36,005</td>
<td style="text-align: center;">1,735.0</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MathQA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7,552</td>
<td style="text-align: center;">15,079</td>
<td style="text-align: center;">20,328</td>
<td style="text-align: center;">2,338.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">7,159</td>
<td style="text-align: center;">17,743</td>
<td style="text-align: center;">22,500</td>
<td style="text-align: center;">1,916.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NumGLUE</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3,020</td>
<td style="text-align: center;">3,601</td>
<td style="text-align: center;">5,717</td>
<td style="text-align: center;">1,474.6</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">2,835</td>
<td style="text-align: center;">2,987</td>
<td style="text-align: center;">4,273</td>
<td style="text-align: center;">1,056.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TabMWP</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3,117</td>
<td style="text-align: center;">4,135</td>
<td style="text-align: center;">6,083</td>
<td style="text-align: center;">842.6</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Coding</td>
<td style="text-align: center;">CodeContest</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">8,167</td>
<td style="text-align: center;">44,319</td>
<td style="text-align: center;">44,666</td>
<td style="text-align: center;">2,061.7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TACO</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9,016</td>
<td style="text-align: center;">50,877</td>
<td style="text-align: center;">58,191</td>
<td style="text-align: center;">2,143.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiTableQuestions</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,401</td>
<td style="text-align: center;">1,544</td>
<td style="text-align: center;">1,738</td>
<td style="text-align: center;">1,794.8</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Magicoder-Evol-Instruct</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10,374</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10,238</td>
<td style="text-align: center;">687.1</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Logic</td>
<td style="text-align: center;">Reclor</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">4,467</td>
<td style="text-align: center;">7,958</td>
<td style="text-align: center;">7,231</td>
<td style="text-align: center;">1,266.7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1,182</td>
<td style="text-align: center;">1,009</td>
<td style="text-align: center;">1,230</td>
<td style="text-align: center;">1,333.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">904</td>
<td style="text-align: center;">741</td>
<td style="text-align: center;">968</td>
<td style="text-align: center;">1,256.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>easier to understand, or further refine the code for optimization. (2) For tool-free math problems, to avoid the actor model directly copying the answers to pass the correctness checking, we mask the answer numbers in the rationale before providing it to LLMs. This approach can better ensure response quality since it encourages LLMs to generate responses with complete reasoning chains with each step clearly marked. (3) For program-enhanced math reasoning, we first translate the textual rationale into code. Then, we either directly provide it to the actor model to generate plans, or ask the actor model to convert the code into modularization programming and then make plans to create tools to solve problems.</p>
<h1>A. 3 Data Decomtamination</h1>
<p>We conduct careful decontamination. Firstly, for LeetCode, we apply the Exact Substring Matching Algorithm ${ }^{3}$ to compare with each instruction in the UltRAINTERACT and find no overlaps. For others, we perform 8 -gram exact matching to compare UltRAINTERACT instructions with test sets of the same task. We remove those instructions that overlap 8 grams with any test sample.</p>
<h2>A. 4 Detailed Statistics</h2>
<p>In total, UltRAINTERACT has 86 K instructions and 220 K action pairs. The Total # Pairs does not equal Total # Turns in UltRAINTERACT, since we fail to generate sufficient correct actions for every incorrect action in multi-turn trajectories mainly due to a lack of sufficient ground truth annotations. The total # pairs may not equal # correct answers, either, because it is also difficult and unnecessary to sample incorrect actions for the correct ones for some simple instructions. We present the specific information for each dataset. In particular, we list information on human annotation in each dataset, which plays an important role in correct action generation ( $\S 2.3$ and Appendix A.2). All three steps of correct action sampling methods mentioned in $\S 2.3$ can be applied to datasets that have rationales, while for datasets only containing answers, only the first two steps are applicable. We do not apply any of the three-step methods to generate correct answers for Magicoder, the only dataset without any human annotation, to construct preference pairs.</p>
<h2>B Additional Details on Training Eurus Models</h2>
<p>Supervised Fine-Tuning. We finetune base models for 1 epoch with a 2e-5 learning rate and 0.1 warmup ratio using a cosine scheduler. For Eurus-7B, we mix 32K UltraChat, 30K ShareGPT, and 50K OpenOrca. For For Eurus-70B, we mix 63K UltraChat, 30K ShareGPT, and 70K OpenOrca.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Preference Learning. For hyperparameters, all $\beta$ is set to 0.1 , and $\lambda_{+} / \lambda_{-}$in KTO is set to 1.33 as recommended. We finetune models for 1 epoch with a $5 \mathrm{e}-7$ learning rate and 0.1 warmup ratio using a cosine scheduler.
Reward Modeling. We train RM for 1 epoch with $\mathrm{lr}=1 \mathrm{e}-5$ learning rate. We also use a cosine scheduler with a warmup ratio of 0.1 .</p>
<p>Regarding pair augmentation, we scale up the pairs by matching every correct action for each instruction with one incorrect action of other turns. This leads to NxN pairs of singleturn actions for a trajectory of depth N . We remove the action pairs consisting of nodes at the same turn, as they are already part of the multi-turn trajectory pairs we included. Next, to avoid overfitting on the training set, we only select instructions with $\mathrm{NxN} \leq 10$, and for these instructions, we randomly sample at most 9 pairs with each action occurring no more than 3 times. This leads to an augmentation of 240 k single-turn action pairs.</p>
<h1>C Additional Evaluation Results of Eurus</h1>
<p>Detailed Setup in $\S 4$. For math, we test both textual reasoning and program-enhanced settings and report the best performance of the two. All evaluations are conducted in 0 -shot CoT with two exceptions: BBH uses 3 shots and IFEval does not use CoT. For MINT, we select MATH, TheoremQA, and MMLU-math from "reasoning" as a new "math" split. We also evaluate 5-shot MMLU (Hendrycks et al., 2021a) for STEM knowledge and MT-Bench (Zheng et al., 2023) for conversation abilities to study whether Eurus needs to trade off other capabilities for reasoning.
Results. Results are shown in Table 8.
On MMLU, EURUS outperforms baselines dedicated to coding and math, and achieves higher results than Mistral-Instruct-v0.2 and CodeLLaMA-70B-Instruct, the official aligned versions of our base model built by their authors. Compared to general-purpose baseline models, EURus-7B achieves comparable performance with the top-performance OpenChat and Starling-LM, though EURus-70B does not achieve the same level of performance as other generalpurpose models, which is expected due to the gap in the base models since CodeLLaMA-70B has not been intentionally optimized for knowledge.</p>
<p>Table 8: MMLU and MT-Bench.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">MT-Bench</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\sim 7 \mathrm{~B}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">7.60</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr-7B- $\beta$</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">7.34</td>
</tr>
<tr>
<td style="text-align: center;">OpenChat-3.5-1210</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">7.81</td>
</tr>
<tr>
<td style="text-align: center;">Starling-LM-7B-</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">8.09</td>
</tr>
<tr>
<td style="text-align: center;">Magicodes-S-DS-6.7B</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OpenCI-DS-6.7B</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MAmmoTH-7B-Mistral</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath-7B-v1.1</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OpenMath-Mistral-7B</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">EURus-7B-SFT</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">7.15</td>
</tr>
<tr>
<td style="text-align: center;">+ DPO</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">7.38</td>
</tr>
<tr>
<td style="text-align: center;">+ KTO</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">7.38</td>
</tr>
<tr>
<td style="text-align: center;">+ NCA</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">7.38</td>
</tr>
<tr>
<td style="text-align: center;">$\sim 40$ B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral-8x7B-Instruct</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">8.30</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-Coder-33B-Ins</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\sim 70$ B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CodeLLaMA-70B-Instruct</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-LM-67B-Chat</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QWen1.5-72B-Chat</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">8.61</td>
</tr>
<tr>
<td style="text-align: center;">OpenCI-CL-70B</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OpenMath-CL-70B</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">EURus-70B-SFT</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">7.69</td>
</tr>
<tr>
<td style="text-align: center;">+ KTO</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">7.93</td>
</tr>
<tr>
<td style="text-align: center;">+ NCA</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">7.54</td>
</tr>
<tr>
<td style="text-align: center;">Proprietary Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">7.94</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">8.96</td>
</tr>
</tbody>
</table>
<p>On MT-Bench, we report baseline numbers from the official leaderboard ${ }^{4}$. EURUS matches the performance of mainstream open-source general-purpose models, and EURus-70B-KTO further achieves the score of GPT-3.5 Turbo.</p>
<h2>D Detailed Results on Reward Modeling</h2>
<h2>D. 1 Additional Results on Reranking</h2>
<p>We present the full results on reranking in Table 9, where the conclusions are consistent with those drawn from D: (1) Our reward models always achieve the highest accuracy on all test sets across different N, except when $\mathrm{N}=2$ on HumanEval. (2) Both $\mathcal{L}<em _mathrm_DR="\mathrm{DR">{\mathrm{BT}}$ and $\mathcal{L}</em>$ consistently help improve reranking performance on three test sets except for HumanEval, where removing either of the objectives can prevent the accuracy from dropping when increasing N from 8 to 16. (3) Modeling safety hurts reranking performance in reasoning.}</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 9: Detailed results of reranking Mistral-Instruct-v0.2's responses on coding and math.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">N</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.5</td>
</tr>
<tr>
<td style="text-align: center;">Top Logits</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">16.8</td>
</tr>
<tr>
<td style="text-align: center;">Starling-RM-34B</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;">EURUS-RM-7B</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">17.3</td>
</tr>
<tr>
<td style="text-align: center;">w/o $\mathcal{L}_{\text {Jrq }}$</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;">w/o $\mathcal{L}_{\text {BT }}$</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">w/o US</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: center;">w/o UF + US</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">17.4</td>
</tr>
<tr>
<td style="text-align: center;">Pass@N</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">35.5</td>
</tr>
</tbody>
</table>
<p>Table 10: Ablation Study.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Coding</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ins-Following</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">LeetCode</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">TheoremQA</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BBH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">IFEval</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EURUS-7B-SFT</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">53.6</td>
</tr>
<tr>
<td style="text-align: center;">Ground-Truth</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source Only</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: center;">UltraIntERacy Only</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.7</td>
</tr>
</tbody>
</table>
<p>When removing UltraSafety from the training data, the RM achieves higher accuracies than EURUS-RM-7B except on MBPP.</p>
<h1>E Detailed Ablation Results</h1>
<p>We present the full results of $\S 5$ in Table 10, with detailed metrics on all coding and math datasets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://huggingface.co/spaces/1msys/chatbot-arena-leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
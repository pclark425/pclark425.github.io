<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8205 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8205</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8205</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-271270644</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.13193v2.pdf" target="_blank">Retrieval-Augmented Generation for Natural Language Processing: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG training, including RAG with/without datastore update. Then, we introduce the application of RAG in representative natural language processing tasks and industrial scenarios. Finally, this paper discusses the future directions and challenges of RAG for promoting its development.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8205.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8205.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based autonomous agents (RAG-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-based Autonomous Agents using Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey discussion of autonomous agents that use LLMs as controllers and augment them with external long-term memory/datastores (RAG) and tool-use to perform tasks without continuous human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Natural Language Processing: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-based autonomous agent (RAG-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent architecture that uses an LLM as a controller/brain, often extended with multimodal perception, tool invocation, and an external long-term memory (RAG datastore) to support extended decision-making and problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this survey entry; generally refers to off-the-shelf large language models used as controllers (e.g. GPT-family, Llama-family) without concrete size/variant specified in the discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General autonomous task suite (decision-making, planning, up-to-date info retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents are expected to perform tasks requiring extended context, tool use, domain knowledge, or up-to-date information (e.g., planning, web-informed responses, multi-step problem solving).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / decision-making / information retrieval / tool-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external long-term memory (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retrieval-Augmented Generation (RAG): agent queries an external knowledge datastore (vector index / key-value store) to fetch relevant passages or records which are then integrated into the LLM's generation (via concatenation, latent fusion, or logits fusion); agents can also call web-search tools then use RAG to integrate results.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External documents/knowledge entries, retrieved passages, web search results, or datastore values (documents, chunks, or demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval from a vector database (ANN search) and tool-based web search followed by retrieval/conditioning; retrieved items integrated into prompts or latent states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The survey summarizes the conceptual benefits but does not provide experimental ablations or numerical comparisons for agents in this section; it cites other works for empirical evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG enables agents to access broader and up-to-date knowledge, improving decision-making and problem solving; external memory/datastores let agents be refreshed without re-training the LLM and allow domain specialization by populating the datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Challenges include retrieval quality (choice of keys, embedding model, similarity metrics, ANN trade-offs), efficiency (encoding, ANN search, I/O), when and how to update datastores, and how often to retrain/fine-tune the generator vs. relying on in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8205.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8205.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sun et al. (RAG+agents iterative reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sun et al. — combining retrieval-augmented generation with agents for iterative reasoning (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work (Sun et al. [137] in the survey) that combines RAG with agent-style iterative reasoning to produce final answers by repeated retrieval and reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-augmented iterative reasoning agent (Sun et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agentic process that iteratively retrieves supporting knowledge (e.g., from RAG or knowledge graph) and applies reasoning steps, updating its intermediate state across iterations to refine final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Iterative reasoning for knowledge-grounded tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step inference where intermediate retrievals inform subsequent reasoning (e.g., complex QA or knowledge graph-based reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory / knowledge graph-backed memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Iterative retrieval loop: agent retrieves relevant knowledge (from RAG/graph), reasons with it, then issues further retrievals conditioned on intermediate results to converge to a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Knowledge graph facts or retrieved documents/passages used as evidence during each iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Iterative semantic retrieval guided by intermediate agent states/queries (i.e., retrieval conditioned on previous reasoning steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey only notes the combination; does not report numerical ablations; pointers to the referenced paper are provided for empirical details.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining RAG with agent-style iterative reasoning helps in producing more grounded and evidence-based final results by allowing the agent to refine its queries and use retrieved facts across reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Survey does not list experiment-specific limitations; general challenges include retrieval relevance, iteration stopping criteria, and computational cost of multiple retrieval/reason cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8205.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8205.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT: Towards LLMs as Operating Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system proposing LLMs augmented with structured memory to act like an operating-system-level controller (listed in the survey's references).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemGPT: Towards LLMs as Operating Systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conceptual framework where an LLM manages tasks and resources using persistent memory structures and can orchestrate tool use and background knowledge (survey only cites the work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Agentic task orchestration / persistent task management</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where an LLM acts continuously to manage and perform multiple operations over time using stored memory and tool access.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>persistent agentic control / long-term task management</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit persistent memory (long-term external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Not detailed in the survey; referenced as a system that leverages memory for persistent agent behavior and operating-system-like responsibilities.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Not specified in the survey (likely structured memory representing state, logs, or records in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Not specified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey only cites the paper; no ablation or experimental details are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included in references as an example of architectures that emphasize external/persistent memory for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Survey does not detail limitations of MemGPT; general concerns remain around memory alignment, update frequency, and scaling costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8205.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8205.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhong et al. memory-augmented LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zhong et al. — Augmenting language models with local, long-term, and external memories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that augments language models with three types of retrieval memories (local memory, long-term memory, external memory) to optimize next-token prediction using nearest neighbors from those memories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training Language Models with Memory Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-augmented language model (Zhong et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Language model pre-training/augmentation approach that conditions next-token prediction on nearest-neighbor retrievals from multiple memory sources (local, long-term, external).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling (next-token prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict probability distribution over next tokens given a prefix, augmented by retrieving nearest neighbors from multiple memory sources to improve prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / next-token prediction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multi-tier retrieval memory (local, long-term, external)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Nearest-neighbor retrieval from memory banks (local context memory, long-term memory, external memory) used to calibrate next-token probability distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Neighbor prefixes and their next tokens or surrounding contexts stored in memory banks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic/nearest-neighbor retrieval (k-NN) from the different memory stores during model forwarding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey mentions the approach and its optimization goal (improving next-token distribution) but does not reproduce ablations; the referenced paper contains empirical evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating multi-tier retrieval memories can help calibrate next-token predictions by leveraging nearest neighbors from varied memory scopes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed in survey; implicit challenges include memory selection, alignment with model representations, and computational costs of k-NN retrieval during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8205.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8205.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RETRO / hidden-state memory retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RETRO (Retrieval-Enhanced Transformer) and related hidden-key/value external memory methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RETRO is an early retrieval-enhanced pretraining approach that integrates retrieved chunks via a cross-attention module; related works store hidden attention keys/values externally and retrieve them during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Language Models by Retrieving from Trillions of Tokens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RETRO-style retrieval-augmented generator</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pre-trained generator architecture that adds cross-attention over retrieved passages to the transformer's layers, enabling access to a large external token-level datastore (demonstrated to scale retrieval database in training).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval-augmented transformer variant with cross-attention modules; specific parameter counts not provided in the survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling and downstream generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improves generation/perplexity by attending to retrieved text chunks during model forward passes; used in pretraining and fine-tuning contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external datastore holding text chunks (and in related works, hidden attention keys/values)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Cross-attention latent fusion: retrieved text (or stored hidden keys/values) are encoded and integrated into hidden states via a cross-attention module in transformer layers; some related methods store and retrieve hidden keys/values as memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Text chunks (chunks of tokens) or, in related variants, precomputed hidden attention keys and values stored in memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval of similar chunks (ANN search) and then cross-attention fusion during forward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey highlights RETRO's finding that a very large retrieval database (e.g., trillion-token scale) can allow RETRO to match larger parameter models; specific numeric ablations are cited to the original work but not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Latent (attention-based) retrieval fusion like RETRO can substantially improve performance and allow favorable scaling trade-offs (large datastore can compensate for fewer model parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High complexity of attention mechanism; need for careful datastore construction; less interpretability of latent fusion; engineering complexity for large-scale retrieval during pretraining and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8205.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8205.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain / LLaMAindex (frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain and LLaMAindex retrieval frameworks for building RAG-enabled agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed tool/frameworks that facilitate building RAG systems and LLM-based agents by orchestrating retrieval, prompt assembly, and integration with vector datastores and tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Natural Language Processing: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Framework-enabled LLM agents (LangChain / LLaMAindex)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Middleware and indexing frameworks that let developers integrate external data sources, vector stores, and retrieval steps into LLM prompts or pipelines to build agents and RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various downstream tasks (QA, summarization, agents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>These frameworks are used to assemble retrieval pipelines and prompt templates for tasks requiring external knowledge or tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>infrastructure / multi-task support for retrieval-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external datastore / vector index (retrieval datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Provide connectors to vector databases and tools; orchestrate encoding queries, ANN search, fetching values, and concatenating or integrating retrievals into LLM prompts or latent fusion modules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Indexed text chunks, documents, or arbitrary values stored in key-value stores supported by the frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval (vector search via ANN) and post-processing (reranking, filtering) orchestrated by framework components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey notes practical impact and role in implementation rather than empirical ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Frameworks like LangChain and LLaMAindex significantly ease RAG and agent construction by connecting encoders, vector stores, and LLM prompts, enabling practical agent deployments using external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Survey highlights engineering challenges (efficiency, input length limits, datastore update handling) when using frameworks in real-world agent systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieval-Augmented Generation for Natural Language Processing: A Survey', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MemGPT: Towards LLMs as Operating Systems <em>(Rating: 2)</em></li>
                <li>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph <em>(Rating: 2)</em></li>
                <li>Training Language Models with Memory Augmentation <em>(Rating: 2)</em></li>
                <li>Improving Language Models by Retrieving from Trillions of Tokens <em>(Rating: 2)</em></li>
                <li>Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents <em>(Rating: 2)</em></li>
                <li>A Survey on the Memory Mechanism of Large Language Model based Agents <em>(Rating: 2)</em></li>
                <li>LangChain <em>(Rating: 1)</em></li>
                <li>LLaMAindex <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8205",
    "paper_id": "paper-271270644",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "LLM-based autonomous agents (RAG-enabled)",
            "name_full": "Large Language Model-based Autonomous Agents using Retrieval-Augmented Generation",
            "brief_description": "Survey discussion of autonomous agents that use LLMs as controllers and augment them with external long-term memory/datastores (RAG) and tool-use to perform tasks without continuous human intervention.",
            "citation_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "mention_or_use": "mention",
            "agent_name": "LLM-based autonomous agent (RAG-enabled)",
            "agent_description": "Agent architecture that uses an LLM as a controller/brain, often extended with multimodal perception, tool invocation, and an external long-term memory (RAG datastore) to support extended decision-making and problem solving.",
            "model_name": null,
            "model_description": "Not specified in this survey entry; generally refers to off-the-shelf large language models used as controllers (e.g. GPT-family, Llama-family) without concrete size/variant specified in the discussion.",
            "task_name": "General autonomous task suite (decision-making, planning, up-to-date info retrieval)",
            "task_description": "Agents are expected to perform tasks requiring extended context, tool use, domain knowledge, or up-to-date information (e.g., planning, web-informed responses, multi-step problem solving).",
            "task_type": "multi-step reasoning / decision-making / information retrieval / tool-augmented tasks",
            "memory_used": true,
            "memory_type": "external long-term memory (retrieval-augmented)",
            "memory_mechanism": "Retrieval-Augmented Generation (RAG): agent queries an external knowledge datastore (vector index / key-value store) to fetch relevant passages or records which are then integrated into the LLM's generation (via concatenation, latent fusion, or logits fusion); agents can also call web-search tools then use RAG to integrate results.",
            "memory_representation": "External documents/knowledge entries, retrieved passages, web search results, or datastore values (documents, chunks, or demonstrations).",
            "memory_retrieval_method": "Semantic retrieval from a vector database (ANN search) and tool-based web search followed by retrieval/conditioning; retrieved items integrated into prompts or latent states.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "The survey summarizes the conceptual benefits but does not provide experimental ablations or numerical comparisons for agents in this section; it cites other works for empirical evaluations.",
            "key_findings": "RAG enables agents to access broader and up-to-date knowledge, improving decision-making and problem solving; external memory/datastores let agents be refreshed without re-training the LLM and allow domain specialization by populating the datastore.",
            "limitations_or_challenges": "Challenges include retrieval quality (choice of keys, embedding model, similarity metrics, ANN trade-offs), efficiency (encoding, ANN search, I/O), when and how to update datastores, and how often to retrain/fine-tune the generator vs. relying on in-context learning.",
            "uuid": "e8205.0",
            "source_info": {
                "paper_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Sun et al. (RAG+agents iterative reasoning)",
            "name_full": "Sun et al. — combining retrieval-augmented generation with agents for iterative reasoning (referenced work)",
            "brief_description": "Referenced work (Sun et al. [137] in the survey) that combines RAG with agent-style iterative reasoning to produce final answers by repeated retrieval and reasoning steps.",
            "citation_title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph",
            "mention_or_use": "mention",
            "agent_name": "RAG-augmented iterative reasoning agent (Sun et al.)",
            "agent_description": "An agentic process that iteratively retrieves supporting knowledge (e.g., from RAG or knowledge graph) and applies reasoning steps, updating its intermediate state across iterations to refine final answers.",
            "model_name": null,
            "model_description": null,
            "task_name": "Iterative reasoning for knowledge-grounded tasks",
            "task_description": "Tasks requiring multi-step inference where intermediate retrievals inform subsequent reasoning (e.g., complex QA or knowledge graph-based reasoning).",
            "task_type": "multi-step reasoning / question answering",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory / knowledge graph-backed memory",
            "memory_mechanism": "Iterative retrieval loop: agent retrieves relevant knowledge (from RAG/graph), reasons with it, then issues further retrievals conditioned on intermediate results to converge to a final answer.",
            "memory_representation": "Knowledge graph facts or retrieved documents/passages used as evidence during each iteration.",
            "memory_retrieval_method": "Iterative semantic retrieval guided by intermediate agent states/queries (i.e., retrieval conditioned on previous reasoning steps).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey only notes the combination; does not report numerical ablations; pointers to the referenced paper are provided for empirical details.",
            "key_findings": "Combining RAG with agent-style iterative reasoning helps in producing more grounded and evidence-based final results by allowing the agent to refine its queries and use retrieved facts across reasoning steps.",
            "limitations_or_challenges": "Survey does not list experiment-specific limitations; general challenges include retrieval relevance, iteration stopping criteria, and computational cost of multiple retrieval/reason cycles.",
            "uuid": "e8205.1",
            "source_info": {
                "paper_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT: Towards LLMs as Operating Systems",
            "brief_description": "A referenced system proposing LLMs augmented with structured memory to act like an operating-system-level controller (listed in the survey's references).",
            "citation_title": "MemGPT: Towards LLMs as Operating Systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT (referenced)",
            "agent_description": "Conceptual framework where an LLM manages tasks and resources using persistent memory structures and can orchestrate tool use and background knowledge (survey only cites the work).",
            "model_name": null,
            "model_description": null,
            "task_name": "Agentic task orchestration / persistent task management",
            "task_description": "Tasks where an LLM acts continuously to manage and perform multiple operations over time using stored memory and tool access.",
            "task_type": "persistent agentic control / long-term task management",
            "memory_used": true,
            "memory_type": "explicit persistent memory (long-term external memory)",
            "memory_mechanism": "Not detailed in the survey; referenced as a system that leverages memory for persistent agent behavior and operating-system-like responsibilities.",
            "memory_representation": "Not specified in the survey (likely structured memory representing state, logs, or records in the referenced work).",
            "memory_retrieval_method": "Not specified in the survey.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey only cites the paper; no ablation or experimental details are given here.",
            "key_findings": "Included in references as an example of architectures that emphasize external/persistent memory for LLM agents.",
            "limitations_or_challenges": "Survey does not detail limitations of MemGPT; general concerns remain around memory alignment, update frequency, and scaling costs.",
            "uuid": "e8205.2",
            "source_info": {
                "paper_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Zhong et al. memory-augmented LM",
            "name_full": "Zhong et al. — Augmenting language models with local, long-term, and external memories",
            "brief_description": "Referenced work that augments language models with three types of retrieval memories (local memory, long-term memory, external memory) to optimize next-token prediction using nearest neighbors from those memories.",
            "citation_title": "Training Language Models with Memory Augmentation",
            "mention_or_use": "mention",
            "agent_name": "Memory-augmented language model (Zhong et al.)",
            "agent_description": "Language model pre-training/augmentation approach that conditions next-token prediction on nearest-neighbor retrievals from multiple memory sources (local, long-term, external).",
            "model_name": null,
            "model_description": null,
            "task_name": "Language modeling (next-token prediction)",
            "task_description": "Predict probability distribution over next tokens given a prefix, augmented by retrieving nearest neighbors from multiple memory sources to improve prediction.",
            "task_type": "language modeling / next-token prediction",
            "memory_used": true,
            "memory_type": "multi-tier retrieval memory (local, long-term, external)",
            "memory_mechanism": "Nearest-neighbor retrieval from memory banks (local context memory, long-term memory, external memory) used to calibrate next-token probability distributions.",
            "memory_representation": "Neighbor prefixes and their next tokens or surrounding contexts stored in memory banks.",
            "memory_retrieval_method": "Semantic/nearest-neighbor retrieval (k-NN) from the different memory stores during model forwarding.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey mentions the approach and its optimization goal (improving next-token distribution) but does not reproduce ablations; the referenced paper contains empirical evaluations.",
            "key_findings": "Incorporating multi-tier retrieval memories can help calibrate next-token predictions by leveraging nearest neighbors from varied memory scopes.",
            "limitations_or_challenges": "Not detailed in survey; implicit challenges include memory selection, alignment with model representations, and computational costs of k-NN retrieval during generation.",
            "uuid": "e8205.3",
            "source_info": {
                "paper_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RETRO / hidden-state memory retrieval",
            "name_full": "RETRO (Retrieval-Enhanced Transformer) and related hidden-key/value external memory methods",
            "brief_description": "RETRO is an early retrieval-enhanced pretraining approach that integrates retrieved chunks via a cross-attention module; related works store hidden attention keys/values externally and retrieve them during inference.",
            "citation_title": "Improving Language Models by Retrieving from Trillions of Tokens",
            "mention_or_use": "mention",
            "agent_name": "RETRO-style retrieval-augmented generator",
            "agent_description": "Pre-trained generator architecture that adds cross-attention over retrieved passages to the transformer's layers, enabling access to a large external token-level datastore (demonstrated to scale retrieval database in training).",
            "model_name": null,
            "model_description": "Retrieval-augmented transformer variant with cross-attention modules; specific parameter counts not provided in the survey excerpt.",
            "task_name": "Language modeling and downstream generation tasks",
            "task_description": "Improves generation/perplexity by attending to retrieved text chunks during model forward passes; used in pretraining and fine-tuning contexts.",
            "task_type": "language modeling / generation",
            "memory_used": true,
            "memory_type": "external datastore holding text chunks (and in related works, hidden attention keys/values)",
            "memory_mechanism": "Cross-attention latent fusion: retrieved text (or stored hidden keys/values) are encoded and integrated into hidden states via a cross-attention module in transformer layers; some related methods store and retrieve hidden keys/values as memory.",
            "memory_representation": "Text chunks (chunks of tokens) or, in related variants, precomputed hidden attention keys and values stored in memory.",
            "memory_retrieval_method": "Semantic retrieval of similar chunks (ANN search) and then cross-attention fusion during forward passes.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey highlights RETRO's finding that a very large retrieval database (e.g., trillion-token scale) can allow RETRO to match larger parameter models; specific numeric ablations are cited to the original work but not reproduced here.",
            "key_findings": "Latent (attention-based) retrieval fusion like RETRO can substantially improve performance and allow favorable scaling trade-offs (large datastore can compensate for fewer model parameters).",
            "limitations_or_challenges": "High complexity of attention mechanism; need for careful datastore construction; less interpretability of latent fusion; engineering complexity for large-scale retrieval during pretraining and inference.",
            "uuid": "e8205.4",
            "source_info": {
                "paper_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LangChain / LLaMAindex (frameworks)",
            "name_full": "LangChain and LLaMAindex retrieval frameworks for building RAG-enabled agents",
            "brief_description": "Surveyed tool/frameworks that facilitate building RAG systems and LLM-based agents by orchestrating retrieval, prompt assembly, and integration with vector datastores and tools.",
            "citation_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "mention_or_use": "mention",
            "agent_name": "Framework-enabled LLM agents (LangChain / LLaMAindex)",
            "agent_description": "Middleware and indexing frameworks that let developers integrate external data sources, vector stores, and retrieval steps into LLM prompts or pipelines to build agents and RAG systems.",
            "model_name": null,
            "model_description": null,
            "task_name": "Various downstream tasks (QA, summarization, agents)",
            "task_description": "These frameworks are used to assemble retrieval pipelines and prompt templates for tasks requiring external knowledge or tool use.",
            "task_type": "infrastructure / multi-task support for retrieval-augmented tasks",
            "memory_used": true,
            "memory_type": "external datastore / vector index (retrieval datastore)",
            "memory_mechanism": "Provide connectors to vector databases and tools; orchestrate encoding queries, ANN search, fetching values, and concatenating or integrating retrievals into LLM prompts or latent fusion modules.",
            "memory_representation": "Indexed text chunks, documents, or arbitrary values stored in key-value stores supported by the frameworks.",
            "memory_retrieval_method": "Semantic retrieval (vector search via ANN) and post-processing (reranking, filtering) orchestrated by framework components.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey notes practical impact and role in implementation rather than empirical ablations.",
            "key_findings": "Frameworks like LangChain and LLaMAindex significantly ease RAG and agent construction by connecting encoders, vector stores, and LLM prompts, enabling practical agent deployments using external memory.",
            "limitations_or_challenges": "Survey highlights engineering challenges (efficiency, input length limits, datastore update handling) when using frameworks in real-world agent systems.",
            "uuid": "e8205.5",
            "source_info": {
                "paper_title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MemGPT: Towards LLMs as Operating Systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph",
            "rating": 2,
            "sanitized_title": "thinkongraph_deep_and_responsible_reasoning_of_large_language_model_with_knowledge_graph"
        },
        {
            "paper_title": "Training Language Models with Memory Augmentation",
            "rating": 2,
            "sanitized_title": "training_language_models_with_memory_augmentation"
        },
        {
            "paper_title": "Improving Language Models by Retrieving from Trillions of Tokens",
            "rating": 2,
            "sanitized_title": "improving_language_models_by_retrieving_from_trillions_of_tokens"
        },
        {
            "paper_title": "Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents",
            "rating": 2,
            "sanitized_title": "memory_matters_the_need_to_improve_longterm_memory_in_llmagents"
        },
        {
            "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
            "rating": 2,
            "sanitized_title": "a_survey_on_the_memory_mechanism_of_large_language_model_based_agents"
        },
        {
            "paper_title": "LangChain",
            "rating": 1
        },
        {
            "paper_title": "LLaMAindex",
            "rating": 1,
            "sanitized_title": "llamaindex"
        }
    ],
    "cost": 0.019826749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Retrieval-Augmented Generation for Natural Language Processing: A Survey
19 Jul 2024</p>
<p>Shangyu Wu 
Ying Xiong 
Yufei Cui 
Haolun Wu 
Can Chen 
Ye Yuan 
Lianming Huang 
Tei-Wei Kuo 
Nan Guan 
Chun Jason 
Xue Mbzuai </p>
<p>City University of Hong Kong
MBZUAI</p>
<p>McGill University
Mila</p>
<p>McGill University
Mila</p>
<p>McGill University
Mila</p>
<p>McGill University
Mila</p>
<p>City University of Hong Kong Xue Liu McGill University
Mila</p>
<p>National Taiwan University</p>
<p>City University of Hong Kong</p>
<p>Retrieval-Augmented Generation for Natural Language Processing: A Survey
19 Jul 2024EE85C8D14509E14E845D20015D0D53A3arXiv:2407.13193v2[cs.CL]
Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge.However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise.The appearance of retrievalaugmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs.This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions.Besides, tutorial codes are provided for implementing the representative techniques in RAG.This paper further discusses the RAG training, including RAG with/without datastore update.Then, we introduce the application of RAG in representative natural language processing tasks and industrial scenarios.Finally, this paper discusses the future directions and challenges of RAG for promoting its development.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) [71,108,114,138,164] have achieved significant advancements in recent years and have become the cornerstone of various applications in the field of natural language processing (NLP).These LLMs are typically pre-trained on a large amount of natural language corpus and then fine-tuned on the specific downstream tasks' datasets.Recent works [3,53,106,117] demonstrate the success of LLMs can be explained by the fact that language models act as knowledge bases, which refers to implicitly storing the knowledge learned from training datasets in the parameters as internal memory and generating responses by retrieving answers from memory.To store more knowledge for better generation performance, existing works generally enlarge the memory capacity by increasing the volume of parameters [1,11,54,78].</p>
<p>Although existing LLMs have shown great power, there are still several challenges hindering the development of LLMs.One of the most prominent challenges is the hallucination problem [23,69,70], which refers to the tendency of LLMs to generate responses that are coherent and fluent but factually incorrect.Another big challenge is the knowledge update issue.To update the knowledge stored in the LLMs' internal memory [106,146,168], it is necessary to retrain/fine-tune LLMs with new data, which is a costly process.Another challenge for general LLMs is lacking of domain-specific expertise [20,133,134,166].Training a domain-specific LLM, however, demands considerable manpower for dataset collection.</p>
<p>To address these challenges, recent works [10,49,87] have proposed leveraging an external knowledge database to augment LLMs, known as retrieval-augmented generation (RAG).By supplying LLMs with retrieved relevant factual information, the hallucination problem can be alleviated to some extent.Besides, the knowledge update issue can also be addressed by updating the external knowledge database, which can augment LLMs with up-to-date knowledge.RAG can also convert a general LLM into a domain-specific LLM by constructing and utilizing a domain-specific knowledge database.Therefore, RAG plays an important role in augmenting the functionality of LLMs, making them more accurate, knowledgeable, and reliable in a wide range of applications.</p>
<p>Contributions: This paper reviews all techniques involved in RAG for natural language processing.Although there are several survey papers for RAG [41,59,88,162,171], our survey still has some key insights, (1) This paper systematically introduces each component of RAG, including details about the retriever from building to querying, and techniques of the retrieval fusions with tutorial codes.(2) This paper exhibits different RAG training strategies, including RAG with/without datastore update.(3) This paper further discusses the applications of RAG on downstream NLP tasks and practical NLP scenarios.(4) This paper finally identifies promising future directions for exploring and main challenges for addressing.</p>
<p>The remainder of this paper is organized as follows.Section 2 gives an overview of RAG.Section 3 and Section 4 comprehensively introduce all technical details used in retrievers and retrieval fusions.Section 6 presents how to train the RAG with/without new knowledge.Section 7 presents the techniques used in representative NLP tasks.Section 8 shows the applications of RAG in practical NLP scenarios.Section 9 discusses the future directions of RAG.Section 10 makes a final conclusion of this paper.</p>
<p>Latent Fusion</p>
<p>Logits-based Fusion Inputs</p>
<p>OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION</p>
<p>This section gives an overview of RAG for NLP.As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions.Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs.The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality.The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc.The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc.</p>
<p>Retrieval Fusions aims to leverage the retrieved information to augment the generation.These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion.The query-based fusion augments inputs with retrievals before feeding them into the generators.The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits.The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance.</p>
<p>Generator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators.The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124].The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93].Those generators generate responses or make predictions.</p>
<p>The workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs;</p>
<p>(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.</p>
<p>RETRIEVER</p>
<p>Figure 2 shows the two stages for using the retriever, which involves first building the retriever and then querying the retriever.The following sections will introduce details about each stage.</p>
<p>Building the Retriever</p>
<p>This section will explain how to build a retriever using a large natural language corpus.As shown in Figure 2 (a), the process involves three steps: chunking corpus, encoding chunks, and building the vector database.Specifically, building the vector database includes building the ANN index and storing the data with key-value pairs.(a) Building the retriever.</p>
<p>Postprocessing
504 519 995 𝑁𝑁 1 1 𝑁𝑁 2 1 𝑁𝑁 1 2 𝑁𝑁 2 2 𝑁𝑁 1 3 𝑁𝑁 2 3 𝑁𝑁 1 1 𝑁𝑁 5 1 𝑁𝑁 2 2 𝑁𝑁 1 2 𝑁𝑁 3 3 𝑁𝑁 2 3
Figure 2: Two stages of using the retriever.</p>
<p>Chunking Corpus.</p>
<p>Chunking techniques generally refer to dividing large documents into small text chunks [10,13,43,65,111], which is an indispensable key step in the process of building the retriever.The intuitions behind chunking techniques are, (1) The texts or embeddings used for the indexing should be semantically independent, containing one core idea for models to encode.Short texts are more likely to be ambiguous, for example, the word "apple" can refer to a fruit or a company.(2) Encoding a long sequence document would result in considerable resource overheads when using existing transformer-based models, while processing shorter text chunks can significantly accelerate the encoding process and save memory costs.Therefore, the main challenge of the chunking techniques is to find the best chunking size to make a better tradeoff between text semantics and encoding efficiency.</p>
<p>To solve the above challenge, there are three key points that need to be considered when determining the chunking size:</p>
<p>(1) Task's property.Different tasks may benefit from different kinds of retrieval chunks.For example, question-answer tasks may prefer short phrases, while summarization tasks may prefer long documents.(2) Encoder's property.Different encoder models have varying encoding capabilities on texts with different lengths.For example, models in the sentence-transformer [125] behave better on a single sentence, while the text-embedding-ada-002 [113] is good at longer texts.(3) Query's property.The length of the user's queries should be aligned with the chunking size, which implicitly aligns the amount of contextual information in chunks with that in queries, thus improving the relevance between queries and retrievals.For example, a retrieval database built on short phrases may be useless for queries with long documents.Overall, there is no golden rule for determining the chunking size, and it depends on the specific RAG scenarios.</p>
<p>There are basically three types of chunking techniques, including the chunking with fixed length, the semantic chunking, and the content-based chunking.Chunking with fixed length is the simplest way to split documents sequentially using a length hyperparameter.The semantic chunking cuts documents based on semantics, such as the period character or the newline character that represents the end of the sentence.Existing state-of-the-art natural language processing toolkits, such as NLTK [112] and spaCy [33], have provided convenient sentence-cutting methods.The contentbased chunking segments documents according to the unique structural characteristics.For example, electronic medical records can be easily segmented based on the sections, or programming codes can be segmented based on function blocks.</p>
<p>Encoding Chunks.</p>
<p>Encoding refers to numericalizing textual chunks as vector representations (embeddings).These embeddings generally capture the semantics of the chunks, enabling the retriever to perform similarity searches based on content relevance rather than just keyword matching.</p>
<p>According to the sparsity of the embeddings, there are two kinds of encoding methods, i.e., sparse encoding and dense encoding.The sparse encoding represents text by creating high-dimensional vectors where most elements are zero.The basic sparse encoding is one-hot encoding [50], which represents a word with a highdimensional vector as large as the vocabulary table size but only marks the value corresponding to the presence of the word as one.The embeddings produced by such encodings are called the one-hot vector.Other common sparse encodings include:</p>
<p>(1) Bag of Words (BoW) [51].This encoding improves onehot encoding by replacing the zero-one counting with the frequency counting.However, BoW ignores the syntax and word order in the documents and focuses on statistical information, thus only expressing limited semantics.(2) Term Frequency-Inverse Document Frequency (TF-IDF) [120].This encoding not only counts the occurrence (frequency) of each word but also adjusts these counts based on how common the word is across all documents (inverse Algorithm 3.1 Building the retriever.</p>
<p>Input: A natural language corpus  = { 1 , . . .,   } for building the knowledge database, an encoder E for encoding chunks.Output: The index I and the key-value store S.</p>
<p>1: K = { }, V = { }; document frequency).TF-IDF helps emphasize words that are more descriptive of the document's content.Sparse encoding is an efficient way to encode textual chunks.However, such encodings may not capture deeper semantic meanings.</p>
<p>The dense encoding generates vectors where each dimension can capture a range of semantic features, and most elements are nonzero floating points.The dense embeddings are generally produced by (deep) neural network models,</p>
<p>(1) BERT [29] and Variants.Bidirectional Encoder Representation from Transformers (BERT) is a typical pre-trained transformer model, generating dense semantic embeddings that capture the contextual information.Other BERT variants, such as RoBERTa [99], DistilBERT [128], and ELEC-TRA [19], further improve the semantic representations with advanced learning techniques.(2) Siamese Encoders.This is a type of neural network designed to learn the similarity between inputs, which is usually trained with contrastive learning.Existing state-ofthe-art siamese encoders are DPR [79], SimCSE [40].(3) LLM-based Encoders.This type of encoder benefits from the powerful representation capability of LLMs.LLMs, which contain billions of parameters and are pre-trained on vast amounts of data covering a wide range of topics, have advanced semantic language understanding capabilities.Typical LLM-based encoders are text-embedding-ada-002 [113], bge-embedding [156], mxbai-embedding [130].Compared to sparse encoding, dense encoding leverages deep neural networks, especially transformers [140], to capture broader linguistic and semantic information.Currently, such encodings are widely used in most semantic representation scenarios.</p>
<p>3.1.3Building the Index.Indexing in the vector database aims to accelerate the search process for data similar to high-dimensional query embedding.Unlike common indexing in databases, indexing in the vector database mainly focuses on supporting efficient approximate nearest neighbor (ANN) search [32,47,77]  quality and search efficiency.To solve the challenge, there are various specific optimizations in both algorithmic aspects and systematic aspects to be explored, including choices of similarity metrics, dimension reduction (DR) on embeddings, advanced ANN indexing, system-level optimizations, hardware-aware optimization, and so on.Due to the page limits, this section discusses the optimizations that significantly affect the search quality and efficiency.</p>
<p>Choice of Similarity Metrics.The similarity metrics are the basic components in the retriever, which measures the degree of relevance between query embeddings and chunk embeddings.The similarity metrics would affect the search quality.Typical similarity metrics include cosine similarity, Euclidean similarity, Manhattan distance, and Jaccard similarity.</p>
<p>Dimension Reduction on Embeddings.Reducing the dimensionality of embeddings can improve search efficiency but at the risk of harming the semantic representations.The basic but effective dimension reduction (DR) is the principal component analysis (PCA).The PCA is a simple statistical technique that transforms the original data into a new coordinate system while retaining the most important features.Another popular and advanced dimension reduction is locality-sensitive hashing (LSH).LSH significantly reduces the dimensionality by mapping the data into buckets but preserves the similarity of the original input data.The intuition behind LSH is that the nearest neighbors will be mapped into the same buckets.Unlike LSH, product quantization (PQ) [68] is another popular and effective DR technique for ANN search.The core idea of the PQ is to divide the high-dimensional space into smaller, independently quantized subspaces.Each subspace creates a codebook of different quantized integers to form the representative and compact vectors.The above techniques enable efficient storage and fast approximate search but may lose semantic information.Recent work [17] proposed a new technique named AutoCompressor that reduces the dimension of embeddings by compressing the original context into semantically shorter embeddings.</p>
<p>Advanced ANN Indexing.ANN Indexing generally refers to the methods or structures used to organize and manage data so that the approximate-nearest-neighbor search process is optimized for retrieval quality and retrieval efficiency.This paper will introduce several advanced ANN indexing techniques.</p>
<p>(1) The InVerted File system with Product Quantization (IVFPQ) [32] is a simple but effective indexing framework that combines two powerful techniques to enable an efficient and scalable ANN search process.The main idea of IVFPQ is first to cluster the data for coarse-grained partition</p>
<p>Retrieval fusions in RAG</p>
<p>Query-based Fusions</p>
<p>Logits-based Fusions Latent Fusions</p>
<p>Text Concatenation Feature Concatenation REALM [49] RAG [87] REINA [145] RALM [122] FID [66] RETRO-PROMPT [14] LUMEN [25] Ensemble Calibration kNN-LM [81] kNN-MT [80] kNN-Adapter [64] Robust-kNN-MT [72] Source-Context [89] Attention Weighted Addition RETRO [10] Enc-Dec [93] LONGMEM [147] EAE [38] ReFusion [153] Figure 3: The categories of fusion methods in RAG.</p>
<p>and then to compress the data within each cluster into subvectors for fine-grained quantization.The coarse-grained clustering (the IVF component) significantly reduces the search space, while the fine-grained quantization (the PQ component) ensures a high retrieval performance.</p>
<p>(2) The Hierarchical Navigable Small World (HNSW) [102] uses a hierarchical graph structure to perform ANN search in high-dimensional spaces efficiently.Specifically, HNSW treats high-dimensional vectors as nodes and connects them with their nearest neighbors.The multi-layer graph structure is determined probabilistically to ensure fewer nodes at higher layers for efficient search.(3) Tree-based Indexing aims to organize high-dimensional vectors in tree-liked structures, such as KD-Trees [123], Ball Trees [62] and VP-Trees [98].Typical tree-based indexing is Approximate Nearest Neighbors Oh Yeah (Annoy) [135], which uses a forest of trees built based on random projections to separate the vector space into multiple hyperplanes for efficient ANN search.</p>
<p>3.1.4</p>
<p>Building the Datastore with Key-Value Pairs.The datastore used in the vector database is a specialized database that stores and manages data as a collection of key-value pairs, where keys are the unique identifier of high-dimensional embeddings and values are the domain-specific knowledge.Since the amount of the data stored in the datastore may be quite large, the storage engine, such as LMDB [100] or RocksDB [35], should be capable of efficient retrieval and data persistence.The key point in the datastore for ANN search is what should be used to store as values.For example, for question-answer tasks, when adding retrievals to prompts, the naive but effective way is to store the question embedding as the key and question-answer pairs as the value.This can help the generation process as retrievals are used as demonstrations for models.Recent works have proposed various state-of-the-art vector databases, including the indexing and the datastore, such as Milvus [46,143], FAISS [32,77], LlamaIndex [95], etc.</p>
<p>3.1.5Code Demonstrations.Algorithm 3.1 shows detailed steps to build the retriever.Lines 2-8 present the chunking and the encoding process for a natural language corpus containing multiple documents.In line 6, algorithm 3.1 takes the concatenation of the current chunk and the next chunk as the value.Notably, the choice of value can vary for different tasks.Another practical issue is that the memory cost of all keys and values may exceed the memory capacity of the server in the practical scenario.Thus, it is recommended that the keys and values persist in the storage if necessary.</p>
<p>Querying the Retriever</p>
<p>This section will explain how to query the pre-built retriever, which basically includes three steps as shown in Figure 2(b): encoding queries, ANN search, and post-processing.</p>
<p>Encoding Queries and ANN Search.</p>
<p>To align with the prebuilt embedding space, the retriever uses the same encoder to encode queries during the querying stage.The ANN search refers to leveraging the pre-built indexing and datastore to find similar data via approximate nearest neighbor searching algorithms and then retrieve the corresponding values.</p>
<p>Searching the index refers to searching the pre-built index, finding the top-k nearest neighbors, and returning the unique identifiers of k nearest neighbors.The nearest neighbor search process depends on indexing algorithms or structures.Taking IVFPQ as an example, the search process first compares the query embedding with cluster embeddings and selects several candidate clusters for further search.Then, within each cluster, the search process performs the same product quantization on the query embedding and finds the top-k nearest neighbors based on the distance.Finally, the search process merges all nearest neighbor candidates and re-orders all candidates for the final top-k nearest neighbors.</p>
<p>Retrieving values from datastore refers to fetching the corresponding values based on the key identifiers of nearest neighbors.</p>
<p>Post-Processing.</p>
<p>The post-processing involves a set of techniques after the initial retrieval step.These techniques aim to refine, enhance, or adapt the retrievals based on the specific task objectives.This section will list some typical post-processing techniques.</p>
<p>Reranking aims to reorder the retrieved knowledge based on task-specific objectives.The intuition is that the knowledge is retrieved based on task-agnostic metrics, such as Euclidean distance.Existing reranking methods [18,56,85,141] mostly design different architectures or strategies to reorder the retrieved knowledge.</p>
<p>Code Demonstrations.</p>
<p>After building the retriever, this section demonstrates the detailed steps of querying the retriever to obtain the top- nearest neighbor knowledge in algorithm 3.2, including encoding the query (line 1), performing the approximate nearest neighbor search (line 2), and fetching the knowledge for fusion (line 3).These three steps depend on the specific APIs of encoders, indexing, and datastore.After obtaining the top- retrievals, optimizations for post-processing are applied (line 4).</p>
<p>RETRIEVAL FUSIONS</p>
<p>Retrieval fusions refer to how to leverage the retrieved knowledge to improve generators' performance.Basically, there are three types of retrieval fusions: query-based fusions, logits-based fusions, and latent fusions.Figure 3 shows the detailed categorization of fusions and representative works of each retrieval fusion in RAG.</p>
<p>Query-based Fusion</p>
<p>The simplest and most direct fusion technique is query-based fusion, which integrates the retrieved information with input queries to generate responses.The query-based fusion can be further categorized into two sub-classes according to the type of concatenated information, i.e., text concatenation and feature concatenation.</p>
<p>Text concatenation involves performing query-based fusion with raw texts, making it particularly suitable for contemporary LLMs like GPT-4.These models function as black-box systems with limited interaction capabilities, typically offering only API access to users.Existing works [49,87,122] directly concatenate the input with the top- retrieved sentences/documents to form the query for generators.To better use the in-context learning capability of LLMs, some works [34,90,141,145] design effective prompt templates to integrate retrieved information and inputs.To address the issue of lengthy inputs after concatenating retrievals, recent studies [5,96,101,150,159] have introduced methods for assigning importance weights to elements within the retrieved knowledge base and filtering out less relevant contexts based on these weights.</p>
<p>The feature concatenation involves merging the encoded retrievals with the input features.A simple yet effective approach is FID [66], which first encodes the retrieved passages into sparse or dense representations and then takes the concatenated features as the input for a generator.The state-of-the-art performance of the FID demonstrates the efficacy of feature concatenation.The follow-up works [25,48,67,97,127] further improve the FID by jointly tuning the retriever and the encoder, which can enhance the retrieved knowledge's representations.Besides, Chen et al. [14] concatenate the representations of related knowledge as demonstrations for prompt learning, yielding better generalization.</p>
<p>Algorithm 4.1 presents how to leverage query-based fusions to fuse retrieved knowledge.For those using text concatenation [49,122], algorithm 4.1 first concatenates the retrieved texts and inputs (line 2), then feeds the concatenated input into the generator.Notably, since there is a limit to the maximum input length of existing language models, concatenating too many retrievals would result in a truncation of the concatenated input, which may cut the given input.Therefore, designing the prompt template is the key step for this branch of work.For those using feature concatenation [48,66], algorithm 4.1 first leverages an encoder to obtain the feature (line 5), then concatenates the feature of input and retrievals (line 6), finally passes the concatenated feature into a decoder model (line 7).This branch of work generally incurs high memory costs due to the long sequence length.</p>
<p>Logits-based Fusion</p>
<p>The logits-based fusion refers to incorporating the retrieved knowledge into the output layers.Basically, retrieved knowledge would be fed into the same model to obtain the logits for enhancing or calibrating the predictions.Therefore, logits-based fusion can be categorized into two branches, i.e., ensemble-based fusion and calibration-based fusion.</p>
<p>Ensemble-based fusion treats the logits from the retrieved knowledge as part of an ensemble of predictions.Such ensemble-based fusion can significantly improve the generalization and robustness of the model [80,81,158].One notable work of ensemble-based fusion is kNN-LM [81], which aggregates the logits of the top- nearest neighbors' targets and then interpolates the final predictions.Similar to kNN-LM, Khandelwal et al. [80] propose kNN-MT to enhance the machine translation using retrievals' logits, which is also followed by a branch of works [64,172] ℎ  0 = ;</p>
<p>3:</p>
<p>for  from 1 to  do 4:</p>
<p>ℎ   = M   (ℎ   −1 ); Different from ensemble-based fusion, calibration-based fusion uses the logits from the retrieved knowledge as a form of calibration for the model's predictions.Specifically, Jiang et al. [72] propose a confidence-enhanced kNN-MT that refines the kNN distribution and interpolation weights with the neural machine translation confidence.Li et al. [89] propose to leverage the source context to calibrate the retrieval-augmented neural machine translation.Algorithm 4.2 demonstrates the detailed steps of using the logitsbased fusion to integrate the retrieved knowledge.This branch of work first treats retrievals as similar data to augment the model (lines 2-4).For ensemble, algorithm 4.2 leverages a hyperparameter to fuse the retrieval logits and the output logits (line 6).For calibration, algorithm 4.2 dynamically determines the parameter based on the retrieval logits and the output logits (line 8).Then, algorithm 4.2 performs the same fusion with the computed parameter (line 9).</p>
<p>Latent Fusion</p>
<p>The latent fusion investigates merging the retrieved knowledge into the hidden states of generators for a better generation.Based on the introduction method, latent fusion can be further classified into two categories: attention-based and weighted-addition.</p>
<p>One notable contribution of attention-based fusion is the Retrieval-Enhanced Transformer (RETRO) [10].RETRO represents a pioneering effort in pre-training retrieval-based LLMs, introducing a new cross-attention module to integrate retrieved knowledge directly into the model's hidden states.A significant finding from this work is demonstrating a scaling law for the retrieval database, where RETRO, with a 2 trillion token database, attains performance comparable to that of major models like GPT-3 and Jurassic-1, albeit with 25 times fewer parameters.Customizing the transformer model in RETRO highlights the potential of pre-trained, retrieval-enhanced architectures in improving the efficiency and scalability of LLMs.</p>
<p>In addition to RETRO, other studies [12,26,93,151,154] have contributed to the field by leveraging new attention modules to introduce external knowledge.Typically, Li et al. [93] have extended the RETRO model by decoupling the context encoding from the model inference.Wu et al. [154], Wang et al. [147] store the hidden attention keys and values into external memory and retrieve the knowledge from the memory using an attention mechanism.</p>
<p>Due to the high complexity of the attention mechanism, another branch of work adopts lightweight (weighted) additions to introduce retrieved knowledge.Fevry et al. [38] propose the EAE model that retrieves top- related entities' embeddings from a learnable external memory and adds entities' embeddings to the hidden states of the model.Wu et al. [153] propose ReFusion, which explores various learnable reranking schemes to first re-weight the retrieved knowledge's embeddings, then use weighted addition to incorporate them into the hidden states of the model.Those approaches signify a growing trend towards models that dynamically select and integrate relevant knowledge, paving the way for more sophisticated and nuanced language generation and understanding.</p>
<p>Algorithm 4.3 shows the steps of using latent fusion to introduce the retrieved knowledge into the hidden states of the generator.For attention-based latent fusion, algorithm 4.3 first encodes the retrievals with the output states of the attention module (line 5), then uses a cross-attention module to fuse the retrieval features into the hidden state (line 6).Different from attention-based latent fusion, weighted-addition-based latent fusion adopts a more lightweight way to incorporate retrieved knowledge (lines 10-19).Algorithm 4.3 first encodes the retrievals before feeding them into the generator (line 11), which can be done offline and directly stored as values in the datastore.Then, algorithm 4.3 learns a set of weights to add the retrieval features on the hidden states of generators (line 15).</p>
<p>GENERATORS</p>
<p>This section introduces representative generators and retrievalaugmented generators, which are generally pre-trained on large datasets.Existing generators are mostly large language models that adopt or modify the transformer-based architecture [140].For example, Llama-series models [138,139], GPT-series models [11,114,118,119], and Gemini-series models [4,108,124] remove all encoder modules, retaining only the decoder module, which includes an attention module and a feed-forward network module.Other advanced techniques, such as root mean square layer normalization [165], rotary position embedding [136], and group query attention mechanisms [2], have been incorporated into the design of existing large language models to enhance their performance.</p>
<p>Retrieval-augmented generators typically incorporate new modules into the architecture of existing large language models.They are also pre-trained on a large dataset and an external knowledge database constructed from a vast natural language corpus.These  generators mostly leverage latent fusions to incorporate the knowledge into the hidden states of large language models [10,93,154], which has been discussed in Section 4.3.</p>
<p>Datastore</p>
<p>RAG TRAINING</p>
<p>This section introduces RAG training, which can be categorized into two main classes: RAG without datastore update and RAG with datastore update.The former refers to the case where only trainable parameters in each module of RAG would be updated, and the knowledge in the datastore would remain the same during the training stage.The latter refers to the case where the knowledge in the datastore would be updated, then each module's parameters in RAG would be updated in a similar way as the former case.</p>
<p>RAG without Datastore Update</p>
<p>The goal of training RAG without datastore update is to update the knowledge stored in the short-term memory of generators based on the existing knowledge datastore.As shown in Figure 4 (a)-(c), there are three training cases, i.e., training the retriever, training the generator, and jointly training the retriever and generator.</p>
<p>Training retriever.</p>
<p>Considering the case of no datastore update, training the retriever generally refers to training the retriever encoder and rebuilding the indexing.Since sparse encodings typically rely on statistical methods without parameters, training the encoder pertains only to dense encoding methods.Different training methods may have different goals, such as improving the semantic representations, accelerating the encoding process, or learning the domain-specific representations.The first two goals are often achieved by replacing the original encoder with a more powerful or tiny encoder, such as DistilBERT [128] or TinyBERT [75].The last requires training the original encoder on the domain-specific corpus using contrastive learning.After training the retriever encoder, the embeddings that serve as keys in the vector database will also change.Thus, all indexes should be rebuilt with new embeddings.Besides, if the encoder remains unchanged, the indexing can be updated using new ANN searching algorithms or re-tuning the hyperparameters.After the retriever is trained, it can be directly incorporated into the RAG without updating the generator.</p>
<p>Training generator.</p>
<p>Training the generator involves updating its parameters or those in the retrieval fusion modules.Since the generator is generally an LLM, training the LLM is a resource-and time-consuming process.Fortunately, several parameter-efficient fine-tuning techniques, such as LoRA [57], are proposed to address the fine-tuning problem of LLMs.Although the parameters in the retrieval fusion modules are less than those in the generator, only fine-tuning those parameters may encounter some training problems, such as low convergence and overfitting.Jointly tuning the parameters in the generator and the retrieval fusion modules is a better way to train the generator and the retrieval fusion modules if there are sufficient and powerful resources.</p>
<p>6.1.3</p>
<p>Jointly training the retriever and generator.Apart from independently training the retriever and the generator, jointly training the retriever and the generator can be another good choice for better performance on downstream tasks.The key to this case is to ensure the differentiability from the input to the output during the forward process.Typically, complex indexes, such as FAISS [32], are not a suitable choice during the fine-tuning stage.Existing works generally leverage the complex indexes to pre-select a small subset of nearest neighbors as candidates, then choose the final top- nearest neighbors by performing the matrix-multiplication operations.</p>
<p>Joint training is an end-to-end optimization that can lead to better coordination between the retriever and the generator and improve the contextual understanding of the generator.</p>
<p>RAG with Datastore Update</p>
<p>As shown in Figure 4 (d), this scenario involves two stages: updating the knowledge database, then training the retriever and the generator.There are three cases for updating the knowledge database, i.e., updating with trainable embeddings, updating with new values, and updating with new corpus.In the first case, values generally are trainable embeddings and are simultaneously/asynchronously updated with parameters in the RAG [14].The last two cases usually refer to updating the knowledge database with up-to-date information.Taking question-answer corpus as an example, updating with new values refers to updating the answer to existing questions, while updating with new corpus refers to adding new questionanswer pairs.To update the value of existing keys requires first querying the existing key-value pairs and then performing in-place updates.For a new corpus, the datastore first needs to perform insertion operations, then rebuilds or updates the indexes for new keys.After updating the datastore, training the retriever and the generator is similar to RAG without datastore update.However, this training step is not always a necessary step, benefiting to the in-context learning capability of LLMs.</p>
<p>TASKS</p>
<p>This section lists several classical tasks in the NLP domain and introduces advanced RAG techniques used to solve these tasks.</p>
<p>Language Modeling</p>
<p>Language modeling is the task that requires the prediction of the probability distribution of the next word or character given a sequence of words or characters, which is also named the next-token prediction task.Language modeling has become the fundamental task for pre-training large language models, which can measure the models' generation using the perplexity metric.The formal definition is as follows: given such a sequence of tokens  1 , . . .,   called Prefix, the language modeling task aims to model its probability via next-token prediction,
𝑝 (𝑥 1 , . . . , 𝑥 𝑛 ) = 𝑝 (𝑥 1 ) • 𝑛 𝑖=2 𝑝 (𝑥 𝑖 |𝑥 1 , . . . , 𝑥 𝑖 −1 ),(1)
where the conditional probabilities  (  | 1 , . . .,   −1 ) are modeled by a parameterized language model.Recent works mainly leverage RAG further to improve language modeling capability in the pre-training stage.A branch of works [10,93,147,154] modifies the architecture of generators by adding a new cross-attention module in each transformer block for introducing retrieval knowledge.The intuition of those works is that given the similar Prefixes and their next tokens (retrieving stage), the pre-trained model can calibrate the model's prediction using the cross-attention module to capture the pattern between the next token and prefix (model forwarding stage).Zhong et al. [173] propose to augment the language model with three types of retrieval memories/databases (local memory, long-term memory, and external memory) and optimize the next-token probability distribution with nearest neighbors retrieved from the memories/databases.Another branch of works [49,64,81,122,160] focuses on augmenting the inputs or outputs of generators with retrievals.Guu et al. [49] and Ram et al. [122] concatenate the retrieved knowledge with inputs and feed the retrieval-augmented inputs into the generators.Other works [64,81,160] fuse the logits of inputs as well as retrievals at the final output layer and generate the final probability distribution based on the interpolated results.Those works believe that the concatenated/fused retrievals can provide useful context information on inputs/outputs to improve models' robustness during the pre-training stage.Besides, Doostmohammadi et al. [31] focus on pre-training models with a semantic retriever (BM25) and achieve a better language modeling performance.</p>
<p>Machine Translation</p>
<p>Machine translation (MT) leverages computational linguistics algorithms to translate text or speech from one language to another automatically.The goal of MT is to produce an accurate and fluent translation, preserving the meaning of the original text while adhering to the grammatical and stylistic norms of the target language.MT systems have evolved from rule-based machine translation (RBMT) to statistical machine translation (SMT) and, more recently, to neural machine translation (NMT).In particular, NMT methods have significantly improved translation quality by leveraging deep learning techniques, which thus will be the focus of this section.</p>
<p>RAG techniques can further enhance MT by incorporating external knowledge into the translation process.The simplest way is to concatenate the similar translation examples into the inputs or fuse the logits of similar translation examples at the output layer.For example, some works [16,145] retrieve similar translations according to the source text and concatenate corresponding target texts or pairs of source and target texts as examples into inputs.Other works [56,80,172] feed the retrieved source text into the models and obtain the logits of the next target tokens, then aggregate all logits to generate the final predictions.Moreover, Jiang et al. [72] and Li et al. [89] use the logits of retrieved examples to calibrate the aggregated logits, improving the robustness of the generation.Another branch of works [173,174] injects external knowledge into the objective function during the training stage, refining the representation space with similar translations.Besides, Cai et al. [12] encode similar translations and store them as the translation memory, then introduce the knowledge from memory with a cross-attention module.Instead of improving the performance, a branch of work focuses on accelerating the generation efficiency on MT tasks, such as searching from a pre-built subset [27,107] or a dynamic datastore [22], searching by chunks [104].</p>
<p>Text Summarization</p>
<p>Text summarization is the process of condensing a larger text document into a shorter version, preserving key information and the overall message.This task can be broadly categorized into two types: extractive summarization, which involves selecting and compiling parts of the original text, and abstractive summarization, which entails rewriting the essence of the text in a new, concise form.The goal is to produce a coherent and fluent summary that encapsulates the most critical information from the source material.</p>
<p>RAG techniques can significantly enhance text summarization tasks by leveraging external knowledge and similar documents to inform the summarization process.[16,36,90,145] simply concatenates the retrieved similar summaries into inputs to generate summarizations.Instead of concatenating texts, other works fuse features at the intermediate layers by cross-attention [9], or at the output layers by logits ensemble [56].Besides, Jiang et al. [74] argue that retrieving for every generation may not always be the best choice and propose to retrieve external knowledge during the generation process adaptively.</p>
<p>Question Answering</p>
<p>Question Answering (QA) is a fundamental task in NLP that involves building systems capable of automatically answering human questions in natural language.QA systems can be broadly classified into two categories: open-domain, where the system answers questions about virtually anything, and closed-domain, focusing on a specific area of knowledge.The primary challenge in QA is understanding the question's intent and retrieving accurate, relevant information from a vast collection of data to provide a concise answer.Due to the page limits, this paper only discusses the works of open-domain QA systems.</p>
<p>RAG techniques combine information retrieval with model-based generation, which is highly suitable for QA systems.In particular, open-domain QA systems usually first require searching for knowledge from the Internet or large-scale databases, then generate the corresponding answers according to the retrieved knowledge.Naturally, given similar questions and corresponding answers as demonstrations which are concatenated into inputs [60,90,145], generators in RAG can learn the pattern between questions and answers and infer what answers should be.For some specific QA tasks where a set of reference documents is given, retrievers in RAG would retrieve the relevant documents for concatenation, and then generators in RAG would read the context then generate the final answers via the self-attention mechanism [6,49,86,122], which is similar to solving a reading comprehension problem.Besides, Fabbri et al. [34] focus on designing effective templates for re-organizing the concatenated contexts.Baek et al. [7] leverage the knowledge graph to retrieve the related facts for the input questions, then feed their concatenation and inputs into the generators.Instead of directly concatenating texts, another branch of works focuses on joining the retrieval embeddings with input embeddings for the encoder-decoder models [25,66,67,127].</p>
<p>Some works incorporate the external knowledge in the hidden states or the final logits of generators.For the fusion in the hidden states, the key is what kind of knowledge should be injected, such as entities [26,38], chunks [10,142], documents [15].For the fusion in the logits, most works combine the logits of retrievals and inputs by ensemble techniques [49,87,110,131].</p>
<p>Instead of designing different knowledge fusions for QA systems, existing works also improve QA systems with RAG from other aspects.Some works [45,94,116] use retrieved questionanswering pairs as extra training data.Some works optimize the retriever module, e.g., improving the keys' representation when building the retriever database [121], replacing the indexing with a pre-trained ranking model [163], or enabling retrieving phrases with two queries [109].Other works focus on accelerating the generation efficiency of RAG.Jong et al. [24] propose the layer-sparse cross-attention to speed up the decoding.Some works [6,74,149] observe that the retrievals may not always provide useful information during the generation process and learn to determine when to retrieve.Moreover, Sun et al. [137] combine the RAG with agents to iteratively reason the final results.</p>
<p>Information Extraction</p>
<p>Information Extraction (IE) is a critical task in NLP to automatically extract structured information from unstructured and semistructured text sources.This task encompasses several sub-tasks, including Named Entity Recognition (NER), Entity Linking (EL), Coreference Resolution (CR), Relation Extraction (RE), etc.The goal is to identify and classify key elements from text and understand the relationships between them, thereby converting textual data into a structured format amenable to analysis and interpretation.</p>
<p>With RAG techniques, addressing IE tasks can be significantly improved in terms of not only performance but also interpretability.In NER tasks, Wang et al. [148] first retrieve similar sentences and then concatenate the ranked retrievals for better semantic representations.Ren et al. [126] show that naive RAG may not address Event Argument Extraction (EAE) tasks.Thus, they adopt a sampling-based method to guarantee the same distribution of event labels between retrievals and inputs then concatenate retrieval texts into inputs for better performance in EAE tasks.Table augmentation is also a challenging task, which requires extracting information from tables.Glass et al. [42] propose to extract information in a retrieval-augmented manner.</p>
<p>Text Classification</p>
<p>Text classification tasks are common in NLP applications.Sentiment analysis, a prominent text classification task in NLP, entails identifying and categorizing the emotional tone conveyed in a text.For example, given a sentence of "I love to watch movies", the analysis models should determine whether it has a positive attitude or a negative attitude.The attitude in sentiment analysis can range from positive to negative or can be neutral, nuanced, and even mixed.The sentiment analysis task is crucial for understanding consumer feedback, monitoring brand reputation, and gaining insights into public opinion on various issues.</p>
<p>RAG techniques can significantly enhance sentiment analysis with different external knowledge fusion strategies.Li et al. [90] concatenate the retrieved options and corresponding prompt-based labels with input options.Other works [14,48] concatenate the retrieval embeddings with input embeddings before feeding them into the decoder.Some works fuse the retrieval features into the hidden states of generators via cross-attention [15,147] or rankingbased addition [153].Besides, other works focus on fusing the logits of retrievals with the output logit using ensemble techniques [161,167].Except for knowledge fusions, Min et al. [109] enable locating knowledge in phrases more accurately via two queries.</p>
<p>Dialogue Systems</p>
<p>Dialogue systems, also known as conversational agents or chatbots, are designed to simulate conversation with human users, either in text or speech form.These systems can be categorized into two main types: task-oriented systems [61], which assist users in completing specific tasks such as booking tickets or ordering food, and opendomain systems, which aim to carry on a general conversation on a wide range of topics [132].The core challenge in developing effective dialogue systems lies in understanding user intent, maintaining context, and generating coherent, relevant responses.</p>
<p>Existing works improve the dialogue system with RAG mostly via the concatenation-based methods.Some works [16,83,92] concatenate the retrieved history conversations with current inputs.Other works [16,37,97] first leverage an encoder to encode the history responses, then feed the concatenated embeddings into a decoder to generate new responses.</p>
<p>APPLICATIONS 8.1 LLM-based Autonomous Agents</p>
<p>LLM-based autonomous agents are intelligent software systems which leverages the power of LLMs to perform tasks without the need for continuous human intervention [91,144,155].These agents use LLMs as a brain or controller [63], and extend their abilities through multimodal perception [157], tool utilization [129] and external memory [115].Especially, external long-term memory for agents functions as the knowledge datastore in RAG, which provides agents with the capability to incorporate external knowledge over extended periods.Therefore, applying RAG would be benefit to access a broader range of information, improving agents' decisionmaking and problem-solving abilities [169].This section explores how LLM-based agents can leverage RAG from two perspectives.</p>
<p>Using RAG to Retrieve from External Memory.LLM-based agents can utilize RAG to access and retrieve information from their own external memory [52,105,170].This external memory serves as a knowledge base that the agent can draw upon to enhance its understanding and decision-making.When faced with a query or a task, the agent can use RAG to retrieve relevant information from this memory, which is then integrated into the generation process of the LLM.This allows the agent to produce responses or solutions that are informed by a wider range of knowledge, leading to more accurate and contextually relevant outcomes.</p>
<p>The ability to tap into a vast external memory enables the agent to continuously learn and adapt based on new information, making it more effective over time.Using Tools to Search the Web and RAG for Up-to-Date Information.In addition to retrieving information from its own memory, an LLM-based agent can use tools to search the web for the most current information [129].This capability is particularly useful for tasks that require up-to-date knowledge, such as news summarization, market analysis, or responding to rapidly evolving situations.Once the agent retrieves the latest information from the web, it can use RAG to integrate this data into its generation process.By combining the LLM's natural language understanding with real-time data from the web, the agent can generate responses that are not only contextually relevant but also reflect the latest developments.This approach enhances the agent's ability to provide accurate and timely information, improving its effectiveness in dynamic environments.</p>
<p>In both cases, RAG plays a crucial role in augmenting the capabilities of LLM-based agents by enabling them to access and leverage a wider range of information, whether it's from their own external memory or from real-time sources on the web.This leads to more informed decision-making and enhances the overall performance of the agents.</p>
<p>Frameworks</p>
<p>Frameworks like Langchain [84] and LLaMAindex [95] pose significant impact on enhancing the practical implementation of RAG.Langchain and LLaMAindex exemplify the integration of sophisticated retrieval mechanisms with generative models, facilitating the seamless incorporation of external data into the language generation process.This section will introduce these two representative RAG frameworks in details.</p>
<p>Langchain is a framework designed to augment the capabilities of language models by integrating them with external knowledge sources and databases.It acts as a middleware that facilitates the interaction between language models and various data retrieval systems, enabling more informed and accurate generation of responses.The core functionality of Langchain involves orchestrating the flow of information from external databases into the generative process of language models, enhancing their ability to leverage context and specific knowledge in their responses.This integration plays a crucial role in enabling language models to perform tasks that require access to up-to-date or detailed information that is not contained within the model's initial training data.</p>
<p>LLaMAindex is a specialized data framework that focuses on organizing and indexing vast amounts of data to improve the retrieval capabilities of language models.This framework supports efficient querying mechanisms, allowing language models to quickly access relevant information from a structured repository.LLaMAindex is designed to be highly scalable and can handle diverse data types, from text documents to structured databases.The indexed data supports a wide range of applications, from simple fact retrieval to complex analytical tasks, making it an indispensable tool for enhancing the information retrieval phase in language models.Both Langchain and LLaMAindex are deeply connected to the concept of RAG.Langchain enhances RAG by providing a structured way for language models to interact with external databases and knowledge sources during the generation process.On the other hand, LLaMAindex serves as a powerful backend for RAG systems by ensuring that the retrieval process is both fast and relevant.Together, Langchain and LLaMAindex enhance the capabilities of RAG by ensuring that the language models are not only generating text based on their internal knowledge but are also capable of pulling in external data to provide responses that are contextually enriched and informationally robust.</p>
<p>DISCUSSION AND FUTURE DIRECTION</p>
<p>Despite the success of the RAG for natural language processing, there are some challenges that should be considered.This paper highlights these challenges to inspire future research and provides possible future research directions in RAG for NLP.</p>
<p>Retrieval Quality</p>
<p>The retrieval quality refers to improving the relevance of the information retrieved in RAG, which involves the following four key factors to be designed.The first consideration is determining the optimal key to use in the vector database.This process typically involves subjective decision-making and requires human effort to design effectively.The naive idea is to choose inputs for the given tasks, treating each task as a QA problem.</p>
<p>The second is the choice of embedding model.After determining the key, the next step is leveraging embedding models to convert text into vector representations.Models such as BERT [29], RoBERTa [99], or domain-specific embeddings can be crucial to determine how well nuances and contextual meanings are captured.Adapting the embedding model to better suit specific types of data or queries can significantly enhance retrieval quality.This requires training the model on domain-specific corpora that include the types of queries and documents the system will encounter.</p>
<p>Thirdly, designing effective similarity metrics is also crucial to improve retrieval quality.The goal of similarity metrics is to measure the relevance between the query and the retrieved information.Some classical similarity metrics, such as cosine similarity or Euclidean distance, used for ranking in the recommender system can also be used in RAG [44].Apart from these metrics, some works explored more complex similarity metrics, such as optimal transport distance [21], to obtain a task-specific similarity.</p>
<p>Finally, approximate nearest neighbor (ANN) searching is also a key step in determining what knowledge should be returned as nearest neighbors.Advanced ANN searching aims to accelerate the retrieval efficiency at the cost of sacrificing the retrieval quality.Choosing a suitable ANN algorithm, such as product quantization [68] or HNSW [102], requires a good trade-off between retrieval efficiency and retrieval quality.All of these factors collectively contribute to the retrieval quality of the retriever.</p>
<p>RAG Efficiency</p>
<p>RAG efficiency is crucial for downstream NLP applications, which limits the volume of data that can be retrieved.There are two simple ways to guarantee RAG efficiency without new algorithms, i.e., reducing the volume of data or adding more powerful computing and memory resources.However, the former may impact the retrieval quality, while the latter requires more resource cost.</p>
<p>RAG efficiency encompasses the efficiency of the retriever and the efficiency of retrieval fusions.Retriever efficiency refers to the time cost of retrieving relevant information, which can be divided into three parts, i.e., encoding time, ANN searching time, and data fetching time of the datastore.It is unnecessary to jointly optimize all three components as the bottleneck would vary from different database sizes.For smaller retrieval databases, such as those with fewer than 1 million entries, the encoding phase is often the primary bottleneck, as the vector database can be all stored in the memory.Several topics, such as model quantization [8,82], distillation [30,75], or model pruning [39], are used to accelerate the encoding.</p>
<p>In contrast, for larger databases, the time cost of searching in the index and fetching data from the datastore becomes the major bottleneck, as the searching is over a considerable amount of data, and the fetching involves I/O overheads.In this case, efficient ANN searching algorithms [32,47,77] and system-level optimizations [73,76] are the main focus.</p>
<p>Retrieval fusion efficiency, which aims to enhance the inference efficiency when integrating retrievals, is worth to be optimized for improving the RAG efficiency.For example, the computational overhead of query-based fusion is often non-negligible due to the long sequence length.Some works, such as Fid-light [55] and Re-Fusion [153], mainly target reducing the computations while integrating the retrieved information.</p>
<p>Choices of Fusions</p>
<p>This paper introduces three kinds of retrieval fusions, where each fusion is worth further exploring.Query-based fusions concatenate the texts or embeddings of retrieved knowledge with inputs.These methods have better interpretability and are easy to apply even only when the API of LLMs is provided.However, concatenation leads to a long sequence of inputs, thus resulting in a large computational overhead in the attention and truncation of inputs.Some works [5,153] aim to improve efficiency when integrating retrievals, while others [9,147] focus on improving the efficiency when increasing the model input length.</p>
<p>Conversely, latent-based fusions amalgamate information at a deeper, more abstract level, which may capture more nuanced relationships between the retrieved information and the query.However, these fusions significantly lack interpretability and often require pre-training or fine-tuning to adjust the retrieval embeddings or reweight the retrievals.Therefore, enhancing the interpretability of such latent-based fusions is also worth exploring in the future.</p>
<p>Logits-based fusions incorporate information at the decision level, thereby offering a potentially more flexible and robust integration of data from various sources.Nonetheless, these fusions may oversimplify the fusion process, diminishing the richness of the retrieved information by reducing them to logit values.Meanwhile, such fusions require performing all inference of retrievals, which is also a time-consuming process.</p>
<p>Apart from applying one kind of fusion in practical applications, combining different fusions is also worth exploring for better performance.These fusion methods are not mutually exclusive, as they focus on augmenting the different stages of generators, i.e., inputs, hidden states, and outputs.Besides, during the generation, when to fuse retrieved knowledge is also a significant problem worthy of further exploration [103].</p>
<p>RAG Training</p>
<p>As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update.For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG.This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies.</p>
<p>For RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations.Although the time cost of the update operation in datastore cannot be ignored, some works [14] reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation.Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added.Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios.Recently, some efficient training strategies [28,57] have been proposed to accelerate the fine-tuning process, which can be taken into considerations.</p>
<p>Cross-Modality Retrieval</p>
<p>Retrieving cross-modality information in NLP tasks can greatly enhance the quality and richness of the representations, leading to improved performance.First, cross-modality information, such as combining text with images, videos, or audio, provides a richer context to the content [58].For instance, when language is ambiguous, accompanying images can clarify meanings difficult to convey through text alone.Second, different modalities can contribute various types of information that are not accessible from a single source.For example, visual data can provide spatial, color, and action cues, while textual data can offer detailed descriptions, emotions, or abstract concepts.Combining these can lead to a more comprehensive understanding of the data.Moreover, Models trained on multi-modal data typically exhibit increased robustness and generalizability [152].These models are adept at associating information across diverse inputs, mitigating overfitting to the peculiarities of a single modality.This attribute is particularly valuable in real-world applications of NLP, such as in autonomous vehicles, where systems must interpret textual information from signs or dialogues and sensory data from the surrounding environment to make informed decisions.Furthermore, multi-modal data can resolve ambiguities that cannot be resolved within a single modality.For example, the phrase "bank" can refer to either a financial institution or the side of a river, and visual context can help disambiguate this.Last, human communication is inherently multi-modal, incorporating elements such as gestures, facial expressions, and tone of voice.Systems capable of processing multiple modes of communication can interact with humans in a manner that is both more natural and intuitive.In conclusion, integrating cross-modality information in RAG for NLP tasks not only enhances the richness and quality of data representations but also significantly improves the systems' comprehension, interaction capabilities, and adaptability to diverse applications.</p>
<p>CONCLUSION</p>
<p>In this survey, we delve into the development of RAG within the field of natural language processing.First, this paper introduces the components of RAG and their functionalities.Subsequently, this paper elaborates on each step involved in retriever, discussing the diverse techniques.Furthermore, this paper categorizes the retrieval fusions, evaluating the strengths and weaknesses inherent of each retrieval fusion techniques.Besides, this paper discusses the RAG training, including RAG with/without datastore update.Then, this paper explores how RAG can be adapted for various NLP tasks and provides practical applications of RAG in real-world scenarios.Conclusively, this paper identifies ongoing challenges and suggests directions for future research to foster advancements in this evolving area.</p>
<p>Figure 1 :
1
Figure 1: The overview of retrieval-augmented generation for natural language processing.</p>
<p>Figure 4 :
4
Figure 4: Different RAG training strategies with/without datastore update.</p>
<p>2: for   ∈  do
3:𝑐 1 𝑖 , . . . , 𝑐 𝑚 𝑖 = 𝐶ℎ𝑢𝑛𝑘 (𝑑 𝑖 );/<em> Split each data 𝑑 𝑖 </em>/4:for 𝑗 from 1 to 𝑚 do5:𝑒𝑗 𝑖 = E (𝑐𝑗 𝑖 );/<em> Encode each chunk 𝑐𝑗 𝑖 </em>/6:Add 𝑒𝑗 𝑖 into K and 𝑐𝑗 𝑖 + 𝑐𝑗 +1 𝑖into V; /<em> Take next chunk as anexampless </em>/7:The K and V persist in the storage (e.g., SSD) if necessary;8:end for9: end for10: Build the index I with K;11: Store K and V into the key-value store S;12: return I and S;</p>
<p>rather than transaction operations like insertion, deletion, and update.The key challenge of indexing is making a good trade-off between search Algorithm 3.2 Query the retriever.Input: A query input , an encoder E for encoding chunks, the index I, the key-value store S, the parameter .{ 1 , . . .,   } = S.ℎ ( { 1 , . . .,   } ); /<em> Fetch the values of the neighbors </em>/ 4: {  1 , . . .,    } =  ( { 1 , . . .,   } ) 5: return {  1 , . . .,    };
Output: Top-𝑘 nearest neighbor knowledge.1: 𝑒 = E (𝑞);2: {𝑖𝑑𝑥 1 , . . . , 𝑖𝑑𝑥 𝑘 } = I.𝑆𝑒𝑎𝑟𝑐ℎ (𝑒, 𝑘 );/<em> Search the top-𝑘 nearestneighbors </em>/3:</p>
<p>Algorithm 4.1 Query-based Fusions.Input: A query input , top- nearest neighbor knowledge { 1 , . . .,   }, an encoder E  and a decoder D  for feature concatenation, the generator G for text concatenation.Output: Generated response .  = E  (),    = E  (  ),  ∈ {1, . . .,  };   =   ⊕   1 ⊕ . . .⊕    ; /<em> Concatenate embeddings of neighbors and query </em>/
3:𝑦 = G (𝑥 );4: else5:6:7:𝑦 = D 𝑓 (𝑒 𝑥 )8: end if9: return 𝑦;
1: if Use the text concatenation then 2:  =  1 ⊕ . . .⊕   ⊕ ; /<em> Concatenate neighbor texts and query </em>/</p>
<p>Algorithm 4.2 Logits-based Fusions.Input: A query input , top- nearest neighbor knowledge { 1 , . . .,   }, the generator G. Output: Generated response .  =  (  ,   1 , . . .,    )  =       + (1 −   )  ; 10: end if 11: return ;
3:𝑦 𝑣 𝑗 = G (𝑣 𝑗 )4: end for5: if Use ensemble then6:𝑦 = 𝜆 𝑗 𝑦 𝑣 𝑗 + (1 − 𝜆)𝑦 𝑞 ;7: else8:9:
1:   = G (); 2: for  from 1 to  do</p>
<p>.</p>
<p>Algorithm 4.3 Latent Fusions.Input: A query input , top- nearest neighbors { 1 , . . .,   }, the encoder E, the generator G containing  pairs of modules { ( M  1 , M  1 ), . ..},where M   and M   are the attention module and the FFN module at layer , M   is the cross-attention module used in attention-based latent fusions.Output: Generated response .</p>
<p>1: if Use the attention then 2:</p>
<p>5 :
5
  1 , . . .,    = E ( 1 , . . .,   , ℎ   )   ,   1 , . . .,    ); /<em> Use a cross-attention module to incorporate external knowledge </em>/   1 , . . .,    = E ( 1 , . . .,   )
6: 𝑖 (ℎ 7: ℎ 𝑅 𝑖 = M 𝐶 ℎ 𝐹 𝑖 = M 𝐹 𝑖 (ℎ 𝑅 𝑖 )8:end for9:𝑦 = 𝐿𝑀_𝐻 𝐸𝐴𝐷 (ℎ 𝐹 𝑙 )10: else11:12:ℎ 𝐹 0 = 𝑞;13:for 𝑖 from 1 to 𝑙 do14: 15:ℎ 𝐴 𝑖 = M 𝐴 𝑖 (ℎ 𝐹 𝑖 −1 ); ℎ 𝑅 𝑖 = ℎ 𝐴 𝑖 + 1 𝑘 𝑗 𝑤 𝑗 𝑒 𝑣 𝑗/<em> Use a weighted sum mechanism tofuse the retrieved knowledge </em>/16:ℎ 𝐹 𝑖 = M 𝐹 𝑖 (ℎ 𝑅 𝑖 )17:end for18:𝑦 = 𝐿𝑀_𝐻 𝐸𝐴𝐷 (ℎ 𝐹 𝑙 )19: end if20: return 𝑦;</p>
<p>Jointly training retriever and generator. RAG without Datastore Update RAG with Datastore Update
RetrieverInputsRetrieverInputsEncoder IndexingRetrieverGenerators Retrieval FusionsEncoder IndexingGenerators Retrieval FusionsDatastoreOutputsDatastoreOutputsa. Training retriever.b. Training generator.TrainableEmbeddingsforwardRetrieverInputsbackwardEncoderRetrievalNew ValuesQuerydatabase operation trainableIndexingFusions GeneratorsNew CorpusInsertionUpdateOutputsd. Updating datastore, then training generator.
c.</p>
<p>Exploring the Limits of Large Scale Pre-training. Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi, The Tenth International Conference on Learning Representations (ICLR). 2022</p>
<p>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language Processing2023Association for Computational Linguistics</p>
<p>A Review on Language Models as Knowledge Bases. Badr Alkhamissi, Millicent Li, Asli Celikyilmaz, Mona T Diab, Marjan Ghazvininejad, CoRR abs/2204.060312022. 2022</p>
<p>. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Sygnowski, 2023. 202311805Gemini: A Family of Highly Capable Multimodal Models. CoRR abs/2312</p>
<p>LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs. Adnan Md, Biplob Arefeen, Srimat Debnath, Chakradhar, CoRR abs/2309.008412023. 2023</p>
<p>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, CoRR abs/2310.115112023. 2023</p>
<p>Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. Jinheon Baek, Alham Fikri Aji, Amir Saffari, CoRR abs/2306.041362023. 2023</p>
<p>BinaryBERT: Pushing the Limit of BERT Quantization. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R Lyu, Irwin King, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP). the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP)Association for Computational Linguistics2021</p>
<p>Unlimiformer: Long-Range Transformers with Unlimited Length Input. Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R Gormley, Advances in Neural Information Processing Systems. 202336NeurIPS</p>
<p>Improving Language Models by Retrieving from Trillions of Tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Oriol Irving, Simon Vinyals, Karen Osindero, Jack W Simonyan, Erich Rae, Laurent Elsen, Sifre, Proceedings of the 39th International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research). the 39th International Conference on Machine Learning (ICML) ( Machine Learning Research)2022162</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish202033Language Models are Few-Shot Learners. NeurIPS</p>
<p>Neural Machine Translation with Monolingual Translation Memory. Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingACL/IJCNLP2021</p>
<p>SeDR: Segment Representation Learning for Long Documents Dense Retrieval. Junying Chen, Qingcai Chen, Dongfang Li, Yutao Huang, CoRR abs/2211.108412022. 2022</p>
<p>Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen, Advances in Neural Information Processing Systems. 202235NeurIPS</p>
<p>Decouple knowledge from paramters for plug-and-play language modeling. Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, Rui Yan, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, Rui Yan, Advances in Neural Information Processing Systems. 202336NeurIPS</p>
<p>Adapting Language Models to Compress Contexts. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)2023</p>
<p>Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering. Yung-Sung Chuang, Wei Fang, Shang-Wen, Wen-Tau Li, James R Yih, Glass, Findings of the Association for Computational Linguistics (ACL). Association for Computational Linguistics2023</p>
<p>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, 8th International Conference on Learning Representations (ICLR). 2020OpenReview.net</p>
<p>Pierre Colombo, Pessoa Telmo, Malik Pires, Dominic Boudiaf, Rui Culver, Caio Melo, Corro, F T André, Fabrizio Martins, Vera Esposito, Sofia Lúcia Raposo, Michael Morgado, Desa, abs/2403.03883SaulLM-7B: A pioneering Large Language Model for Law. 2024. 2024</p>
<p>Retrieval-Augmented Multiple Instance Learning. Yufei Cui, Ziquan Liu, Yixin Chen, Yuchen Lu, Xinyue Yu, Xue ( Steve, ) Liu, Tei-Wei Kuo, Miguel Rodrigues, Chun Jason Xue, Antoni B Chan, Advances in Neural Information Processing Systems. 202336NeurIPS</p>
<p>Simple and Scalable Nearest Neighbor Machine Translation. Yuhan Dai, Zhirui Zhang, Qiuzhi Liu, Qu Cui, Weihua Li, Yichao Du, Tong Xu, The Eleventh International Conference on Learning Representations (ICLR). 2023</p>
<p>Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better. David Dale, Elena Voita, Loïc Barrault, Marta R Costa, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)Association for Computational Linguistics-jussà. 2023</p>
<p>FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. Jong Michiel De, Yury Zemlyanskiy, Joshua Ainslie, Nicholas Fitzgerald, Sumit Sanghai, Fei Sha, William W Cohen, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>Pre-computed memory or onthe-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. Jong Michiel De, Yury Zemlyanskiy, Nicholas Fitzgerald, Joshua Ainslie, Sumit Sanghai, Fei Sha, William W Cohen, Proceedings of the 40th International Conference on Machine Learning (ICML). the 40th International Conference on Machine Learning (ICML)2023</p>
<p>Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. Jong Michiel De, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, William W Cohen, The Tenth International Conference on Learning Representations (ICLR). 2022</p>
<p>Subset Retrieval Nearest Neighbor Machine Translation. Hiroyuki Deguchi, Taro Watanabe, Yusuke Matsui, Masao Utiyama, Hideki Tanaka, Eiichiro Sumita, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>QLoRA: Efficient Finetuning of Quantized LLMs. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202336NeurIPS</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies (NAACL-HLT2019</p>
<p>SKD-BERT: Compressing BERT via Stochastic Knowledge Distillation. Zixiang Ding, Guoqing Jiang, Shuai Zhang, Lin Guo, Wei Lin, Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI). AAAI Press2023</p>
<p>Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. Ehsan Doostmohammadi, Tobias Norlund, Marco Kuhlmann, Richard Johansson, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou, CoRR abs/2401.08281The Faiss library. 2024. 2024</p>
<p>Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering. Alexander R Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)2020</p>
<p>. Facebook, 2013</p>
<p>Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies. Angela Fan, Claire Gardent, CoRR abs/2204.058792022. 2022</p>
<p>Augmenting Transformers with KNN-Based Composite Memory for Dialog. Angela Fan, Claire Gardent, Chloé Braud, Antoine Bordes, Trans. Assoc. Comput. Linguistics. 92021. 2021</p>
<p>Entities as Experts: Sparse Memory Access with Entity Supervision. Thibault Févry, Baldini Livio, Nicholas Soares, Eunsol Fitzgerald, Tom Choi, Kwiatkowski, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Compressing Large-Scale Transformer-Based Models: A Case Study on BERT. Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, Marianne Winslett, Trans. Assoc. Comput. Linguistics. 92021. 2021</p>
<p>SimCSE: Simple Contrastive Learning of Sentence Embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2021</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, CoRR abs/2312.109972023. 2023</p>
<p>Ankita Rajaram Naik, Gaetano Rossiello, and Alfio Gliozzo. 2023. Retrieval-Based Transformer for Table Augmentation. R Michael, Xueqing Glass, Wu, Findings of the Association for Computational Linguistics (ACL). </p>
<p>Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension. Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, Dong Yu, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)Association for Computational Linguistics2020</p>
<p>A Survey of Accuracy Evaluation Metrics of Recommendation Tasks. Asela Gunawardana, Guy Shani, J. Mach. Learn. Res. 102009. 2009</p>
<p>Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing. Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL). the 57th Conference of the Association for Computational Linguistics (ACL)2019</p>
<p>Manu: A Cloud Native Vector Database Management System. Rentong Guo, Xiaofan Luan, Long Xiang, Xiao Yan, Xiaomeng Yi, Jigao Luo, Qianya Cheng, Weizhi Xu, Jiarui Luo, Frank Liu, Zhenshan Cao, Yanliang Qiao, Ting Wang, Bo Tang, Charles Xie, Proc. VLDB Endow. VLDB Endow2022. 202215</p>
<p>Accelerating Large-Scale Inference with Anisotropic Vector Quantization. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar, Proceedings of the 37th International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research). the 37th International Conference on Machine Learning (ICML) ( Machine Learning Research)2020119</p>
<p>Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks. Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, Yang Liu, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>Retrieval Augmented Language Model Pre-Training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research). the 37th International Conference on Machine Learning (ICML) ( Machine Learning Research)2020119</p>
<p>Digital design and computer architecture. David Harris, Sarah Harris, 2010Morgan Kaufmann</p>
<p>Distributional structure. S Zellig, Harris, 1954. 195410</p>
<p>Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents. Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, Dustin Dannenhauer, Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20232</p>
<p>Can Language Models Act as Knowledge Bases at Scale?. Qiyuan He, Yizhong Wang, Wenya Wang, CoRR abs/2402.142732024. 2024</p>
<p>Training Compute-Optimal Large Language Models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre2022. 2022</p>
<p>FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. Sebastian Hofstätter, Jiecao Chen, Karthik Raman, Hamed Zamani, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)ACM2023</p>
<p>Simple and Effective Retrieve-Edit-Rerank Text Generation. Nabil Hossain, Marjan Ghazvininejad, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)2020</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, The Tenth International Conference on Learning Representations (ICLR). 2022</p>
<p>Multimodal Named Entity Recognition and Relation Extraction with Retrieval-Augmented Strategy. Xuming Hu, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)ACM20233488</p>
<p>Yucheng Hu, Yuxing Lu, CoRR abs/2404.19543RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing. 2024. 2024</p>
<p>RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan, Chang , Bryan Catanzaro, CoRR abs/2308.079222023. 2023</p>
<p>Learning Retrieval Augmentation for Personalized Dialogue Generation. Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, Lilian H Y Tang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2023</p>
<p>Lightweight-Yet-Efficient: Revitalizing Ball-Tree for Point-to-Hyperplane Nearest Neighbor Search. Qiang Huang, Anthony K H Tung, 39th IEEE International Conference on Data Engineering (ICDE). IEEE2023</p>
<p>Understanding the planning of LLM agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, CoRR abs/2402.027162024. 2024</p>
<p>kNN-Adapter: Efficient Domain Adaptation for Black-Box Language Models. Yangsibo Huang, Daogao Liu, Zexuan Zhong, Weijia Shi, Yin Tat, Lee , CoRR abs/2302.108792023. 2023</p>
<p>Chunk-based Decoder for Neural Machine Translation. Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li, Ming Zhou, Naoki Yoshinaga, Masaru Kitsuregawa, Weijia Jia, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL). the 55th Annual Meeting of the Association for Computational Linguistics (ACL)Association for Computational Linguistics2017</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. Gautier Izacard, Edouard Grave, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European Chapterthe Association for Computational Linguistics2021</p>
<p>Atlas: Few-shot Learning with Retrieval Augmented Language Models. Gautier Izacard, S H Patrick, Maria Lewis, Lucas Lomeli, Fabio Hosseini, Timo Petroni, Jane Schick, Armand Dwivedi-Yu, Sebastian Joulin, Edouard Riedel, Grave, J. Mach. Learn. Res. 24432023. 2023</p>
<p>Product Quantization for Nearest Neighbor Search. Hervé Jégou, Matthijs Douze, Cordelia Schmid, IEEE Trans. Pattern Anal. Mach. Intell. 332011. 2011</p>
<p>Survey of Hallucination in Natural Language Generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung, ACM Comput. Surv. 55382023. 2023</p>
<p>RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding. Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, Pascale Fung, Findings of the Association for Computational Linguistics (ACL). Association for Computational Linguistics2023</p>
<p>. Albert Q Jiang, Mistral 7B. CoRRAlexandre Sablayrolles, Mistral 7B. CoRRArthur Mensch, Mistral 7B. CoRRChris Bamford, Mistral 7B. CoRRDevendra Singh Chaplot, Mistral 7B. CoRRDiego De Las, Mistral 7B. CoRRFlorian Casas, Mistral 7B. CoRRGianna Bressand, Mistral 7B. CoRRGuillaume Lengyel, Mistral 7B. CoRRLucile Lample, Mistral 7B. CoRRSaulnier, Mistral 7B. CoRRabs/2310.06825Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023. 2023</p>
<p>Towards Robust k-Nearest-Neighbor Machine Translation. Hui Jiang, Ziyao Lu, Fandong Meng, Chulun Zhou, Jie Zhou, Degen Huang, Jinsong Su, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design. Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, Tim Kraska, CoRR abs/2403.056762024. 2024</p>
<p>Active Retrieval Augmented Generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>TinyBERT: Distilling BERT for Natural Language Understanding. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Findings of the Association for Computational Linguistics (EMNLP) (Findings of ACL). Association for Computational Linguistics2020. 2020</p>
<p>RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. Zili Chao Jin, Xuanlin Zhang, Fangyue Jiang, Xin Liu, Xuanzhe Liu, Xin Liu, Jin, CoRR abs/2404.124572024. 2024</p>
<p>Billion-Scale Similarity Search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Trans. Big Data. 72021. 2021</p>
<p>Scaling Laws for Neural Language Models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, CoRR abs/2001.083612020. 2020</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Nearest Neighbor Machine Translation. Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, CoRR abs/2010.007102020. 2020</p>
<p>Generalization through Memorization: Nearest Neighbor Language Models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, The 8th International Conference on Learning Representations (ICLR). 2020</p>
<ol>
<li>I-BERT: Integer-only BERT Quantization. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, Kurt Keutzer, Proceedings of the 38th International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research). the 38th International Conference on Machine Learning (ICML) ( Machine Learning Research)139</li>
</ol>
<p>Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. Brendan King, Jeffrey Flanigan, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>. Langchain, 2023</p>
<p>Internet-augmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, CoRR abs/2203.051152022. 2022</p>
<p>When to Read Documents or QA History: On Unified and Selective Open-domain QA. Kyungjae Lee, Sang-Eun Han, Findings of the Association for Computational Linguistics (ACL). 2023Seung-won Hwang, and Moontae Lee</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. S H Patrick, Ethan Lewis, Aleksandra Perez, Fabio Piktus, Vladimir Petroni, Naman Karpukhin, Heinrich Goyal, Mike Küttler, Wen-Tau Lewis, Tim Yih, Sebastian Rocktäschel, Douwe Riedel, Kiela, Advances in Neural Information Processing Systems. 202033NeurIPS</p>
<p>A Survey on Retrieval-Augmented Text Generation. Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu, CoRR abs/2202.011102022. 2022</p>
<p>Revisiting Source Context in Nearest Neighbor Machine Translation. Xuanhong Li, Peng Li, Po Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)2023</p>
<p>Unified Demonstration Retriever for In-Context Learning. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, CoRR abs/2401.05459Ya-Qin Zhang, and Yunxin Liu. 2024. Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. 2024</p>
<p>Retrieve &amp; Memorize: Dialog Policy Learning with Multi-Action Memory. Yunhao Li, Yunyi Yang, Xiaojun Quan, Jianxing Yu, Findings of the Association for Computational Linguistics (ACL/IJCNLP. 2021</p>
<p>Decoupled Context Processing for Context Augmented Language Modeling. Zonglin Li, Ruiqi Guo, Sanjiv Kumar, Advances in Neural Information Processing Systems. 202235NeurIPS</p>
<p>Unsupervised Cross-Task Generalization via Retrieval Augmentation. Kangmin Bill Yuchen Lin, Chris Tan, Beiwen Miller, Xiang Tian, Ren, Advances in Neural Information Processing Systems. 202235NeurIPS</p>
<p>Jerry Liu, 10.5281/zenodo.1234LlamaIndex. 2022</p>
<p>TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian, Findings of the Association for Computational Linguistics (EMNLP). 2023</p>
<p>RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation. Shuai Liu, Hyundong Cho, Marjorie Freedman, Xuezhe Ma, Jonathan , Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)May. 2023</p>
<p>Fast nearest neighbor searching based on improved VP-tree. Shi-Guang Liu, Yin-Wei Wei, Pattern Recognit. Lett. 602015. 2015</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, CoRR abs/1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019. 2019</p>
<p>Improving Retrieval-Augmented Large Language Models via Data Importance Learning. Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang, CoRR abs/2307.030272023. 2023</p>
<p>Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. A Yury, Dmitry A Malkov, Yashunin, IEEE Trans. Pattern Anal. Mach. Intell. 422020. 2020</p>
<p>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Chunkbased Nearest Neighbor Machine Translation. Henrique Pedro, Zita Martins, Marinho, F T André, Martins, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang, CoRR abs/2403.16971AIOS: LLM Agent Operating System. 2024. 2024</p>
<p>Locating and Editing Factual Associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 202235NeurIPS</p>
<p>Fast Nearest Neighbor Machine Translation. Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, Jiwei Li, Findings of the Association for Computational Linguistics (ACL). 2022</p>
<p>. Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Cristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, CoRR abs/2403.08295Gemma: Open Models Based on Gemini Research and Technology. 2024. 2024</p>
<p>Nonparametric Masked Language Modeling. Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-Tau Yih, Hannaneh Hajishirzi, Luke Zettlemoyer, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>Meta-training with Demonstration Retrieval for Efficient Fewshot Learning. Aaron Mueller, Kanika Narang, Lambert Mathias, Qifan Wang, Hamed Firooz, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>Graph-and surface-level sentence chunking. Ewa Muszynska, Proceedings of the ACL 2016 Student Research Workshop. the ACL 2016 Student Research WorkshopAssociation for Computational Linguistics2016</p>
<p>Text-Emb-Ada. 2022OpenAI</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, CoRR abs/2310.08560MemGPT: Towards LLMs as Operating Systems. 2023. 2023</p>
<p>Retrieval-guided Counterfactual Generation for QA. Bhargavi Paranjape, Matthew Lamm, Ian Tenney, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). the 60th Annual Meeting of the Association for Computational Linguistics (ACL)2022</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, S H Patrick, Anton Lewis, Yuxiang Bakhtin, Alexander H Wu, Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAI blog. 2018. 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>Anand Rajaraman, Jeffrey David Ullman, Data Mining. Cambridge University Press2011</p>
<p>What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, Amir Globerson, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>In-Context Retrieval-Augmented Language Models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, Trans. Assoc. Comput. Linguistics. 112023. 2023</p>
<p>Revisiting kd-tree for Nearest Neighbor Search. Parikshit Ram, Kaushik Sinha, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD). Ankur Teredesai, Vipin Kumar, Ying Li, Rómer Rosales, Evimaria Terzi, George Karypis, the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)ACM2019</p>
<p>. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross Mcilroy, Melvin Johnson, Johan Schalkwyk, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, 2024. 2024Eli Collins, Eliza Rutherford, Erica Moreira,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR abs/2403.05530</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation. Yubing Ren, Yanan Cao, Ping Guo, Fang Fang, Wei Ma, Zheng Lin, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering. Devendra Singh Sachan, Siva Reddy, William L Hamilton, Chris Dyer, Dani Yogatama, Advances in Neural Information Processing Systems. NeurIPS202134</p>
<p>Distil-BERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, CoRR abs/1910.011082019. 2019</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202336NeurIPS</p>
<p>Darius Koenig, Julius Lipp, Sean Lee, Aamir Shakir, Open Source Strikes Bread -New Fluffy Embeddings Model. 2024</p>
<p>REPLUG: Retrieval-Augmented Black-Box Language Models. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, CoRR abs/2301.126522023. 2023</p>
<p>Retrieval Augmentation Reduces Hallucination in Conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, Findings of the Association for Computational Linguistics (EMNLP). Association for Computational Linguistics2021</p>
<p>Large Language Models Encode Clinical Knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schärli, Aakanksha Chowdhery, ; R Webster, Gregory S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nature. Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan6202023. 2023</p>
<p>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska ; Nenad, Yun Tomasev, Renee Liu, Christopher Wong, S Sara Semturs, Joelle K Mahdavi, Dale R Barral, Gregory S Webster, Yossi Corrado, Matias, CoRR abs/2305.09617Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level Medical Question Answering with Large Language Models. Blaise Agüera y Arcas2023</p>
<p>RoFormer: Enhanced transformer with Rotary Position Embedding. Jianlin Su, H M Murtadha, Yu Ahmed, Shengfeng Lu, Wen Pan, Yunfeng Bo, Liu, Neurocomputing. 5681270632024. 2024</p>
<p>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, Jian Guo, CoRR abs/2307.076972023. 2023</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, CoRR abs/2302.139712023. 2023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurélien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. NeurIPS201730</p>
<p>FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc V Le, Thang Luong, CoRR abs/2310.032142023. 2023</p>
<p>Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. Boxin Wang, Wei Ping, Peng Xu, Lawrence Mcafee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, Bryan Catanzaro, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)2023</p>
<p>Milvus: A Purpose-Built Vector Data Management System. Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang, Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, Charles Xie, SIGMOD '21: International Conference on Management of Data. ACM2021</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, Frontiers Comput. Sci. 181863452024. 2024</p>
<p>Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data. Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, Michael Zeng, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). the 60th Annual Meeting of the Association for Computational Linguistics (ACL)2022</p>
<p>Knowledge Editing for Large Language Models: A Survey. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li, CoRR abs/2310.162182023. 2023</p>
<p>Augmenting Language Models with Long-Term Memory. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, Advances in Neural Information Processing Systems. 202336NeurIPS</p>
<p>Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning. Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingACL/IJCNLP2021</p>
<p>Self-Knowledge Guided Retrieval Augmentation for Large Language Models. Yile Wang, Peng Li, Maosong Sun, Yang Liu, Findings of the Association for Computational Linguistics (EMNLP). 2023</p>
<p>Learning to Filter Context for Retrieval-Augmented Generation. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig, CoRR abs/2311.083772023. 2023</p>
<p>Retrieval-based Controllable Molecule Generation. Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard G Baraniuk, Anima Anandkumar, The Eleventh International Conference on Learning Representations. 2023ICLR</p>
<p>Multimodal Generative Models for Scalable Weakly-Supervised Learning. Mike Wu, Noah D Goodman, Advances in Neural Information Processing Systems 31 (NeurIPS). 2018</p>
<p>ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. Shangyu Wu, Ying Xiong, Yufei Cui, Xue Liu, Buzhou Tang, Tei-Wei Kuo, Chun Jason, Xue , CoRR abs/2401.029932024. 2024</p>
<p>Memorizing Transformers. Yuhuai Wu, Markus Norman Rabe, Delesley Hutchins, Christian Szegedy, The Tenth International Conference on Learning Representations (ICLR). 2022</p>
<p>Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: A Survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, CoRR abs/2309.078642023</p>
<p>Peitian Zhang, and Niklas Muennighof. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. Shitao Xiao, Zheng Liu, CoRR abs/2309.075972023</p>
<p>Large Multimodal Agents: A Survey. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, Guanbin Li, CoRR abs/2402.151162024. 2024</p>
<p>EARA: Improving Biomedical Semantic Textual Similarity with Entity-Aligned Attention and Retrieval Augmentation. Ying Xiong, Xin Yang, Linjing Liu, Ka-Chun Wong, Qingcai Chen, Yang Xiang, Buzhou Tang, Findings of the Association for Computational Linguistics (EMNLP). Association for Computational Linguistics2023</p>
<p>RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, CoRR abs/2310.044082023. 2023</p>
<p>Why do Nearest Neighbor Language Models Work. F Frank, Uri Xu, Graham Alon, Neubig, Proceedings of the 40th International Conference on Machine Learning (ICML) (Proceedings of Machine Learning Research). the 40th International Conference on Machine Learning (ICML) ( Machine Learning Research)2023202</p>
<p>Retrieval-Augmented Few-shot Text Classification. Guoxin Yu, Lemao Liu, Haiyun Jiang, Shuming Shi, Xiang Ao, Findings of the Association for Computational Linguistics (EMNLP). 2023</p>
<p>Evaluation of Retrieval-Augmented Generation: A Survey. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu, CoRR abs/2405.074372024. 2024</p>
<p>Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In. Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>GLM-130B: An Open Bilingual Pre-trained Model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, The Eleventh International Conference on Learning Representations (ICLR). 2023OpenReview.net</p>
<p>Root Mean Square Layer Normalization. Biao Zhang, Rico Sennrich, Advances in Neural Information Processing Systems. 201932</p>
<p>HuatuoGPT, Towards Taming Language Model to Be a Doctor. Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li, Findings of the Association for Computational Linguistics (EMNLP). Association for Computational Linguistics2023</p>
<p>ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models. Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang, Changyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi, Yiran Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, CoRR abs/2401.01286A Comprehensive Study of Knowledge Editing for Large Language Models. Jun Zhou, and Huajun Chen. 2024. 2024</p>
<p>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian De Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei, CoRR abs/2404.01230LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models. 2024. 2024</p>
<p>A Survey on the Memory Mechanism of Large Language Model based Agents. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, CoRR abs/2404.135012024. 2024</p>
<p>Retrieval-Augmented Generation for AI-Generated Content: A Survey. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui, CoRR abs/2402.194732024. 2024</p>
<p>Adaptive Nearest Neighbor Machine Translation. Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua Luo, Jiajun Chen, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingACL/IJCNLP2021</p>
<p>Training Language Models with Memory Augmentation. Zexuan Zhong, Tao Lei, Danqi Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation. Wenhao Zhu, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1023 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1023</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1023</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-268875925</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.02577v2.pdf" target="_blank">Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering</a></p>
                <p><strong>Paper Abstract:</strong> We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1023.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1023.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO-CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization with Curriculum Learning (PPO-CL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PPO agent trained with a five-phase predefined curriculum that progressively increases environment complexity and applies reward engineering to optimize multi-criteria container-emptying decisions in a simulated waste-sorting benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO-CL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep reinforcement learning agent using Proximal Policy Optimization (PPO) trained through a structured five-phase curriculum and reward engineering (Gaussian, custom, precision rewards) to learn container-emptying policies under safety, energy, and volume objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (industrial benchmark digital twin)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Container management (ContainerGym) - plastic sorting facility final-stage benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated digital twin of a waste-sorting facility container management task (ContainerGym). Eleven containers collect material with per-container stochastic fill rates (random walk with drift) and noisy sensors; two processing units (PUs) with processing-time functions; actions are either do-nothing or empty a specific container which then requires an available PU. Constraints include safety hard limits (max volume 40), container-to-PU accessibility, delayed rewards (some containers take up to ~300 timesteps to reach ideal volumes), and class imbalance (rare rewarding emptying actions). Complexity is increased through stochastic filling, limited PUs, long horizons, and noisy observations; variation is introduced across curriculum phases via deterministic → stochastic fill rates, changes to number of PUs (many PUs → constrained PUs), episode length, timestep duration, and reward function variants.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of containers (11), number of PUs (final: 2), episode length (up to 600 timesteps), timestep duration (60 s), delayed reward horizon (up to ~300 timesteps), stochastic filling modeled as random walk with drift and Gaussian noise, safety hard limit (max volume = 40 units), class imbalance between emptying actions and do-nothing actions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (final test environment: 11 containers, 2 PUs, long horizons, stochastic dynamics, delayed rewards, and safety constraints); varied across curriculum (low → high).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Deterministic vs stochastic fill rates (phases 1-3 deterministic, phases 4-5 stochastic), resource-constraint variation (phase1: many PUs, later phases: 2 PUs), episode length and timestep changes (short episodes 25 timesteps at 30s → 600 timesteps at 60s), and reward-function variation (Simple Gaussian, Custom, Precision).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varied: low in early curriculum phases (deterministic, many PUs), high in later phases (stochastic dynamics, constrained PUs, longer horizons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average inference volume deviation (percent), total emptying actions per rollout, PU utilization (proxy for energy), safety violation rate (percentage of actions/rollouts exceeding volume limit), cumulative reward (reported but de-emphasized).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Avg. inference volume deviation = 3.55% ± 2.33 (across 15 rollouts, best seed); Emptying actions per rollout = 56; PU utilization: 12% less than Optimal Analytic Agent and 24% less than PPO-volume-criteria agent (relative reductions reported); Safety violations = 1.7% (percentage of actions/rollouts exceeding max volume 40).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly describes and experiments with a relationship: high environment complexity (stochastic fill rates, limited PUs, long horizons, delayed rewards, class imbalance) makes from-scratch training fail (high safety violations and poor timing). Curriculum learning that gradually increases complexity (starting deterministic, many PUs, short horizons) and varying reward precision enables transfer and stabilizes learning; introducing higher variation (stochastic dynamics, constrained resources) later requires fine-tuning (freeze/unfreeze value and policy networks) and reduced KL-constraints. The trade-offs discussed: low-complexity/low-variation training speeds early learning and reduces unsafe exploration, while high-complexity/high-variation final training is necessary for real-world performance but is harder and requires careful reward shaping and constrained exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (predefined five-phase curriculum) combined with reward engineering (Simple Gaussian, Custom, Precision), staged freezing of networks (freeze policy, train value net in Phase 4), and action masking at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained with a multi-phase budget: phases explicitly: Phase 2 = 1,000,000 timesteps (precision reward), Phase 3 ≈ 1,000,000 timesteps (penalized actions), Phase 4 = 500,000 timesteps (stochastic dynamics, policy frozen), Phase 5 = 500,000 timesteps (fine-tuning) — total ≈ 3,000,000 environment timesteps across the curriculum (Phase 1 budget not clearly reported in text/table).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A structured curriculum that gradually increases environment complexity and refines rewards enables PPO to learn safe, energy-efficient container-emptying policies that vanilla PPO trained from scratch cannot; curriculum-trained PPO (PPO-CL) achieves lower average volume deviation (3.55% ± 2.33), fewer emptying actions (56 vs 62), substantially lower PU utilization (12%-24% reduction vs baselines), and dramatically fewer safety violations (1.7% vs 27.11% for vanilla PPO). Reward shaping (precision reward, positional and termination rewards) and staged adaptation (freeze value/policy) are critical to handle long-delayed rewards, class imbalance, and stochastic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1023.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1023.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO-volume</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization trained on Volume Objective (vanilla PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline PPO agent trained from scratch to optimize only the volume criterion using a Simple Gaussian reward; it fails to learn safe and well-timed policies in the full-complexity environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO-volume criteria (vanilla PPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard PPO agent trained only on the volume optimization objective (Simple Gaussian reward) without curriculum; trained from scratch on complex environment dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (industrial benchmark digital twin)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Container management (ContainerGym) - plastic sorting facility final-stage benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same ContainerGym environment as PPO-CL: 11 containers, stochastic/deterministic fill rates depending on training regime, 2 PUs in final test environment, delayed rewards, noisy sensor inputs, safety limit at 40 units, class imbalance with rare rewarding emptying actions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>11 containers; test environment: 2 PUs; long horizons (up to 600 timesteps), delayed positive rewards up to ~300 timesteps for some containers; stochastic fill modeled as random walk with drift and Gaussian noise; safety limit (40 units); class imbalance (few positive emptying events).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (trained from scratch on high-complexity environment in baseline experiments and found to perform poorly).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>No curriculum variation — trained directly on full-complexity environment (stochastic dynamics, resource constraints, long horizons) as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (exposed to full environment complexity and variation during training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average inference volume deviation (percent), total emptying actions per rollout, safety violation rate, PU utilization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Avg. inference volume deviation = 15.16% ± 10.80; Emptying actions per rollout = 62; Safety violations = 27.11% (percentage of actions/rollouts exceeding max volume 40); PU utilization: baseline higher (PPO-CL uses 24% less PU-time than this agent).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicitly discussed: training on the fully complex, highly variable environment from scratch leads to poor learning (high variance in emptying decisions, high safety violations) due to delayed rewards and class imbalance. The authors contrast this with curriculum training, which alleviates these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training (vanilla PPO trained from scratch on target objective without curriculum), Simple Gaussian reward.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained from scratch for comparable budgets as PPO-CL in evaluation but failed to obtain sufficient safe samples due to frequent safety-violating terminations; specific total training timesteps for baseline runs not separately enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla PPO trained from scratch on the full-complexity environment struggled: it exhibits large timing variance, high average volume deviation (≈15.16%), high safety violation rate (~27.1%), and higher PU utilization (worse energy efficiency). This demonstrates that high environment complexity and variation (delayed rewards, stochasticity, class imbalance) impede learning without curriculum/structured training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1023.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1023.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimal-Analytic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimal Analytic Agent (hand-crafted policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-expert hand-crafted policy that empties containers at their precise target volumes when possible and uses an emergency threshold to avoid overflow; used as a non-learning baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Optimal Analytic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deterministic, hand-crafted decision policy (non-learning) designed by experts to empty containers at target volumes when possible and perform emergency emptying at a conservative threshold (volume 37) to avoid overflow; does not model interactions/competition for PUs explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>deterministic (simulated) agent / analytic baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Container management (ContainerGym) - plastic sorting facility final-stage benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same test environment as the RL agents (11 containers, 2 PUs, stochastic filling in final evaluation), but the analytic agent applies fixed logic (empties at target volumes or at emergency threshold 37) and does not adapt to container interactions or stochastic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>11 containers; 2 PUs in test environment; uses deterministic rules rather than learning; safety emergency threshold = 37 (below physical limit 40) hard-coded.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (the analytic agent simplifies operation via conservative rules rather than learning to handle full interactions under stochastic dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>No training-induced variation; analytic agent is tested under the same environment variations as learned agents but does not adapt to stochasticity or changing dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>exposed to high variation at test time but agent policy is fixed (no adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average inference volume deviation (percent), total emptying actions per rollout, PU utilization proxy (energy), safety violation rate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Avg. inference volume deviation = 4.47% ± 2.61; Emptying actions per rollout = 62; Safety violations = 0% (due to hard-coded emergency threshold of 37); PU utilization: PPO-CL used 12% less than this analytic agent (i.e., analytic PU utilization is higher).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Used as a benchmark to show trade-off between safety and efficiency: the analytic agent attains zero safety violations via conservative rules but is less energy efficient (higher PU utilization) and does not exploit inter-container scheduling to reduce resource use — illustrating a trade-off between adaptivity in variable environments and strict conservative safety.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>No learning (hand-crafted analytic policy).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable (analytic deterministic policy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The analytic policy is perfectly safe (0% safety violations) due to conservative emergency thresholds but is less efficient (higher PU utilization and more emptying actions than PPO-CL). This underscores a trade-off: conservative hand-crafted policies can guarantee safety in varied/stochastic environments but at the cost of energy/resource efficiency and lack of adaptability; curriculum-trained RL can approach zero violations while improving efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ContainerGym: A Real-World reinforcement learning benchmark for resource allocation <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: a framework and survey <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 1)</em></li>
                <li>Safe exploration for reinforcement learning <em>(Rating: 1)</em></li>
                <li>A comprehensive survey on safe reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1023",
    "paper_id": "paper-268875925",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PPO-CL",
            "name_full": "Proximal Policy Optimization with Curriculum Learning (PPO-CL)",
            "brief_description": "A PPO agent trained with a five-phase predefined curriculum that progressively increases environment complexity and applies reward engineering to optimize multi-criteria container-emptying decisions in a simulated waste-sorting benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO-CL",
            "agent_description": "Deep reinforcement learning agent using Proximal Policy Optimization (PPO) trained through a structured five-phase curriculum and reward engineering (Gaussian, custom, precision rewards) to learn container-emptying policies under safety, energy, and volume objectives.",
            "agent_type": "simulated agent (industrial benchmark digital twin)",
            "environment_name": "Container management (ContainerGym) - plastic sorting facility final-stage benchmark",
            "environment_description": "Simulated digital twin of a waste-sorting facility container management task (ContainerGym). Eleven containers collect material with per-container stochastic fill rates (random walk with drift) and noisy sensors; two processing units (PUs) with processing-time functions; actions are either do-nothing or empty a specific container which then requires an available PU. Constraints include safety hard limits (max volume 40), container-to-PU accessibility, delayed rewards (some containers take up to ~300 timesteps to reach ideal volumes), and class imbalance (rare rewarding emptying actions). Complexity is increased through stochastic filling, limited PUs, long horizons, and noisy observations; variation is introduced across curriculum phases via deterministic → stochastic fill rates, changes to number of PUs (many PUs → constrained PUs), episode length, timestep duration, and reward function variants.",
            "complexity_measure": "Number of containers (11), number of PUs (final: 2), episode length (up to 600 timesteps), timestep duration (60 s), delayed reward horizon (up to ~300 timesteps), stochastic filling modeled as random walk with drift and Gaussian noise, safety hard limit (max volume = 40 units), class imbalance between emptying actions and do-nothing actions.",
            "complexity_level": "high (final test environment: 11 containers, 2 PUs, long horizons, stochastic dynamics, delayed rewards, and safety constraints); varied across curriculum (low → high).",
            "variation_measure": "Deterministic vs stochastic fill rates (phases 1-3 deterministic, phases 4-5 stochastic), resource-constraint variation (phase1: many PUs, later phases: 2 PUs), episode length and timestep changes (short episodes 25 timesteps at 30s → 600 timesteps at 60s), and reward-function variation (Simple Gaussian, Custom, Precision).",
            "variation_level": "varied: low in early curriculum phases (deterministic, many PUs), high in later phases (stochastic dynamics, constrained PUs, longer horizons).",
            "performance_metric": "Average inference volume deviation (percent), total emptying actions per rollout, PU utilization (proxy for energy), safety violation rate (percentage of actions/rollouts exceeding volume limit), cumulative reward (reported but de-emphasized).",
            "performance_value": "Avg. inference volume deviation = 3.55% ± 2.33 (across 15 rollouts, best seed); Emptying actions per rollout = 56; PU utilization: 12% less than Optimal Analytic Agent and 24% less than PPO-volume-criteria agent (relative reductions reported); Safety violations = 1.7% (percentage of actions/rollouts exceeding max volume 40).",
            "complexity_variation_relationship": "Yes — the paper explicitly describes and experiments with a relationship: high environment complexity (stochastic fill rates, limited PUs, long horizons, delayed rewards, class imbalance) makes from-scratch training fail (high safety violations and poor timing). Curriculum learning that gradually increases complexity (starting deterministic, many PUs, short horizons) and varying reward precision enables transfer and stabilizes learning; introducing higher variation (stochastic dynamics, constrained resources) later requires fine-tuning (freeze/unfreeze value and policy networks) and reduced KL-constraints. The trade-offs discussed: low-complexity/low-variation training speeds early learning and reduces unsafe exploration, while high-complexity/high-variation final training is necessary for real-world performance but is harder and requires careful reward shaping and constrained exploration.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (predefined five-phase curriculum) combined with reward engineering (Simple Gaussian, Custom, Precision), staged freezing of networks (freeze policy, train value net in Phase 4), and action masking at inference.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Trained with a multi-phase budget: phases explicitly: Phase 2 = 1,000,000 timesteps (precision reward), Phase 3 ≈ 1,000,000 timesteps (penalized actions), Phase 4 = 500,000 timesteps (stochastic dynamics, policy frozen), Phase 5 = 500,000 timesteps (fine-tuning) — total ≈ 3,000,000 environment timesteps across the curriculum (Phase 1 budget not clearly reported in text/table).",
            "key_findings": "A structured curriculum that gradually increases environment complexity and refines rewards enables PPO to learn safe, energy-efficient container-emptying policies that vanilla PPO trained from scratch cannot; curriculum-trained PPO (PPO-CL) achieves lower average volume deviation (3.55% ± 2.33), fewer emptying actions (56 vs 62), substantially lower PU utilization (12%-24% reduction vs baselines), and dramatically fewer safety violations (1.7% vs 27.11% for vanilla PPO). Reward shaping (precision reward, positional and termination rewards) and staged adaptation (freeze value/policy) are critical to handle long-delayed rewards, class imbalance, and stochastic dynamics.",
            "uuid": "e1023.0",
            "source_info": {
                "paper_title": "Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PPO-volume",
            "name_full": "Proximal Policy Optimization trained on Volume Objective (vanilla PPO)",
            "brief_description": "A baseline PPO agent trained from scratch to optimize only the volume criterion using a Simple Gaussian reward; it fails to learn safe and well-timed policies in the full-complexity environment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO-volume criteria (vanilla PPO baseline)",
            "agent_description": "Standard PPO agent trained only on the volume optimization objective (Simple Gaussian reward) without curriculum; trained from scratch on complex environment dynamics.",
            "agent_type": "simulated agent (industrial benchmark digital twin)",
            "environment_name": "Container management (ContainerGym) - plastic sorting facility final-stage benchmark",
            "environment_description": "Same ContainerGym environment as PPO-CL: 11 containers, stochastic/deterministic fill rates depending on training regime, 2 PUs in final test environment, delayed rewards, noisy sensor inputs, safety limit at 40 units, class imbalance with rare rewarding emptying actions.",
            "complexity_measure": "11 containers; test environment: 2 PUs; long horizons (up to 600 timesteps), delayed positive rewards up to ~300 timesteps for some containers; stochastic fill modeled as random walk with drift and Gaussian noise; safety limit (40 units); class imbalance (few positive emptying events).",
            "complexity_level": "high (trained from scratch on high-complexity environment in baseline experiments and found to perform poorly).",
            "variation_measure": "No curriculum variation — trained directly on full-complexity environment (stochastic dynamics, resource constraints, long horizons) as baseline.",
            "variation_level": "high (exposed to full environment complexity and variation during training).",
            "performance_metric": "Average inference volume deviation (percent), total emptying actions per rollout, safety violation rate, PU utilization.",
            "performance_value": "Avg. inference volume deviation = 15.16% ± 10.80; Emptying actions per rollout = 62; Safety violations = 27.11% (percentage of actions/rollouts exceeding max volume 40); PU utilization: baseline higher (PPO-CL uses 24% less PU-time than this agent).",
            "complexity_variation_relationship": "Implicitly discussed: training on the fully complex, highly variable environment from scratch leads to poor learning (high variance in emptying decisions, high safety violations) due to delayed rewards and class imbalance. The authors contrast this with curriculum training, which alleviates these issues.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment training (vanilla PPO trained from scratch on target objective without curriculum), Simple Gaussian reward.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Trained from scratch for comparable budgets as PPO-CL in evaluation but failed to obtain sufficient safe samples due to frequent safety-violating terminations; specific total training timesteps for baseline runs not separately enumerated in text.",
            "key_findings": "Vanilla PPO trained from scratch on the full-complexity environment struggled: it exhibits large timing variance, high average volume deviation (≈15.16%), high safety violation rate (~27.1%), and higher PU utilization (worse energy efficiency). This demonstrates that high environment complexity and variation (delayed rewards, stochasticity, class imbalance) impede learning without curriculum/structured training.",
            "uuid": "e1023.1",
            "source_info": {
                "paper_title": "Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Optimal-Analytic",
            "name_full": "Optimal Analytic Agent (hand-crafted policy)",
            "brief_description": "A domain-expert hand-crafted policy that empties containers at their precise target volumes when possible and uses an emergency threshold to avoid overflow; used as a non-learning baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Optimal Analytic Agent",
            "agent_description": "A deterministic, hand-crafted decision policy (non-learning) designed by experts to empty containers at target volumes when possible and perform emergency emptying at a conservative threshold (volume 37) to avoid overflow; does not model interactions/competition for PUs explicitly.",
            "agent_type": "deterministic (simulated) agent / analytic baseline",
            "environment_name": "Container management (ContainerGym) - plastic sorting facility final-stage benchmark",
            "environment_description": "Same test environment as the RL agents (11 containers, 2 PUs, stochastic filling in final evaluation), but the analytic agent applies fixed logic (empties at target volumes or at emergency threshold 37) and does not adapt to container interactions or stochastic dynamics.",
            "complexity_measure": "11 containers; 2 PUs in test environment; uses deterministic rules rather than learning; safety emergency threshold = 37 (below physical limit 40) hard-coded.",
            "complexity_level": "medium (the analytic agent simplifies operation via conservative rules rather than learning to handle full interactions under stochastic dynamics).",
            "variation_measure": "No training-induced variation; analytic agent is tested under the same environment variations as learned agents but does not adapt to stochasticity or changing dynamics.",
            "variation_level": "exposed to high variation at test time but agent policy is fixed (no adaptation).",
            "performance_metric": "Average inference volume deviation (percent), total emptying actions per rollout, PU utilization proxy (energy), safety violation rate.",
            "performance_value": "Avg. inference volume deviation = 4.47% ± 2.61; Emptying actions per rollout = 62; Safety violations = 0% (due to hard-coded emergency threshold of 37); PU utilization: PPO-CL used 12% less than this analytic agent (i.e., analytic PU utilization is higher).",
            "complexity_variation_relationship": "Used as a benchmark to show trade-off between safety and efficiency: the analytic agent attains zero safety violations via conservative rules but is less energy efficient (higher PU utilization) and does not exploit inter-container scheduling to reduce resource use — illustrating a trade-off between adaptivity in variable environments and strict conservative safety.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "No learning (hand-crafted analytic policy).",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Not applicable (analytic deterministic policy).",
            "key_findings": "The analytic policy is perfectly safe (0% safety violations) due to conservative emergency thresholds but is less efficient (higher PU utilization and more emptying actions than PPO-CL). This underscores a trade-off: conservative hand-crafted policies can guarantee safety in varied/stochastic environments but at the cost of energy/resource efficiency and lack of adaptability; curriculum-trained RL can approach zero violations while improving efficiency.",
            "uuid": "e1023.2",
            "source_info": {
                "paper_title": "Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ContainerGym: A Real-World reinforcement learning benchmark for resource allocation",
            "rating": 2,
            "sanitized_title": "containergym_a_realworld_reinforcement_learning_benchmark_for_resource_allocation"
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: a framework and survey",
            "rating": 2,
            "sanitized_title": "curriculum_learning_for_reinforcement_learning_domains_a_framework_and_survey"
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 1,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "Safe exploration for reinforcement learning",
            "rating": 1,
            "sanitized_title": "safe_exploration_for_reinforcement_learning"
        },
        {
            "paper_title": "A comprehensive survey on safe reinforcement learning",
            "rating": 1,
            "sanitized_title": "a_comprehensive_survey_on_safe_reinforcement_learning"
        }
    ],
    "cost": 0.01335375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
23 Jul 2024</p>
<p>Abhijeet Pendyala 
Ruhr-University Bochum
BochumGermany</p>
<p>Asma Atamna 
Ruhr-University Bochum
BochumGermany</p>
<p>Tobias Glasmachers 
Ruhr-University Bochum
BochumGermany</p>
<p>Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
23 Jul 2024D0E7C8D380DF474767E62E8FAF6C3C1CarXiv:2404.02577v2[cs.LG]Deep reinforcement learningReal-world taskCurriculum learningSustainable waste management
We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility.Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage.A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities.This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy.This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task.Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism.This iterative and adaptable process enables the agent to learn a desired optimal policy.Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.</p>
<p>Introduction</p>
<p>This work introduces a novel reinforcement learning approach to address the critical need for optimization within waste sorting facilities, addressing an uncharted area of research.EU directives emphasize the need for responsible and sustainable recycling of packaging waste.This has led to waste sorting facilities investing in sophisticated infrastructure and automated sorting technologies.Traditional data-driven methods are emerging as key tools within the digitalization of these highthroughput facilities for optimal control.Sorting facilities are designed specifically for different use cases, such as the types of materials to be separated and the scale and the throughput requirements of the plant.They must remain robust to ever-changing material streams, and they should ideally be capable of adapting to changes of the input composition, and even to future design modifications such as changes in layout or modifying sorting machinery.Additionally, ensuring operational safety while optimizing sorting efficiency remains a key challenge.</p>
<p>The Reinforcement Learning (RL) framework allows agents to learn and optimize actions through dynamic interactions with their environments [18].This makes it a powerful paradigm for creating optimal and adaptive control systems, particularly when dealing with uncertain or changing conditions [9,20].RL's ability to adapt based on feedback and reward signals offers great potential for addressing the complexities and ever-evolving nature of waste management processes.The final stage of a waste sorting facility, namely the management of containers filling up with sorted material before being processed into a final product, is readily available as a real-world industrial benchmark RL environment called ContainerGym [12].</p>
<p>This research introduces a curriculum learning approach for training a Proximal Policy Optimization (PPO) algorithm for the container management problem.The approach breaks down the complex learning process into manageable stages, aiming to optimize sorting efficiency and resource utilization within operational constraints.</p>
<p>The motivation behind our approach was that the RL problem at hand is non-trivial.We found that a vanilla baseline like a PPO agent trained from scratch on all the three criteria detailed in section 3 fails to learn a useful control policy.Analysis of training data revealed that the majority of the training episodes terminated prematurely due to violation of safety constraints (volume limit exceeded).Consequently, the agent cannot collect sufficient samples of optimal state-action pairs, trapping them in a sub-optimal solution.Therefore there was a need for a more powerful method.We demonstrate that a nuanced curriculum approach can solve the problem.</p>
<p>This paper is structured as follows.Section 2 reviews relevant research and establishes the motivation for our approach.Section 3 provides a detailed problem description and outlines key challenges.The control task is cast as a reinforcement learning (RL) problem in Section 4. Section 5 explains the design of a nuanced reward mechanism, on top of which Section 6 presents our structured curriculum learning strategy, composed of five distinct phases.Finally, Section 7 presents an experimental evaluation and discusses results, followed by conclusions in Section 8.</p>
<p>Related Work</p>
<p>Curriculum learning in the context of reinforcement learning is a training paradigm where an agent encounters a sequence of tasks carefully designed to accelerate learning.Designing a curriculum allows ML engineers and domain experts to incorporate experience and domain knowledge into the training process.The curriculum might involve modifying reward functions, altering environment dynamics, or changing state or action spaces [19].By strategically guiding the agent's experiences, CL has the potential to promote faster convergence, better generalization, and more efficient learning [1,11,24].Beyond reinforcement learning, CL has been proven to provide a versatile framework with a broad range of applications.In supervised learning, CL has enhanced model performance through the strategic sequencing of training examples [10,24].The gaming industry has employed CL to create progressively challenging levels, boosting agent performance [17,8].Robotics has benefited from CL, as complex tasks could be decomposed into simpler steps, accelerating robotic skill acquisition [3,15].Finally, CL has been used in safe reinforcement learning by gradually exposing the agent to higher-risk scenarios [4,5].For a comprehensive overview of CL methods, see the survey papers [23,16,11].</p>
<p>Our proposed curriculum approach falls within the category of predefined curriculum learning [23].We adopt a Curriculum Through Intermediate Goals strategy [6], decomposing a complex goal state into a sequence of simpler intermediate goals.While many existing approaches concentrate on modifying single aspects of the learning process, such as state distributions [3], reward functions [15,25], or goal generation [2,14], we differentiate our method by strategically modifying multiple facets.We manipulate the underlying Markov Decision Processes (MDPs) of our intermediate tasks, carefully tuning environment dynamics, reward mechanisms, and learning time horizons.This multifaceted approach, combined with our focus on a real-world problem, sets our approach apart from existing methods.</p>
<p>We aim to achieve three key benefits through our curriculum approach: optimization, improved sampling (as alluded to in [26]), and safety.Optimization benefit arises as a curriculum guides an agent towards solutions more efficiently than direct minimization of a non-convex target objective with multiple criteria.Statistically, careful sample allocation within a curriculum can boost performance by facilitating knowledge transfer among tasks, potentially reducing training data needs.For safety, as [13] note, a curriculum allows the reuse of safe policies from simpler tasks, ensuring safety in intermediate stages while the agent progressively develops proficiency for complex environments.</p>
<p>Real Environment and Problem Description</p>
<p>We consider the task of managing containers in a plastic sorting facility.The containers collect pre-sorted material.After accumulating enough material of a certain type, the container is emptied onto a conveyor belt and the material is moved to one of two processing units (PU-1 and PU-2).The setup is illustrated in Figure 1.Containers are continuously filled with material, with each container prescribed to a unique material.The material extraction from the container is always done until it is completely emptied.The difficulty of the task arises from two constraints.First of all, each container has its own unique optimal emptying volume depending on the material type.Emptying the container earlier is possible, but it results in a sub-optimal product and a waste of energy.Emptying later is subject to similar costs, however, processing larger chunks of material is generally beneficial as it saves energy and processing time.The second constraint is that processing the material takes time, and containers can be emptied only if a PU is free.If a container is emptied too far away from its optimal (at higher and or lower value), the deviation in volume that cannot be transformed into a product is again redirected to the corre-sponding container via an energy-intensive auxiliary process.On the other hand, if a container is emptied prematurely and too frequently, the PUs are engaged too frequently causing higher energy usage and also a bottleneck in resource allocation.In other words, the control task is to reduce the cumulative deviation from the ideal volume for each container while managing resources (PUs) efficiently.</p>
<p>A critical safety constraint that needs to be respected is that a container is never allowed to overshoot the physical limit of its maximum bearing volume (40 units).This incurs a high recovery cost including human intervention to stop and restart the facility.and should be avoided at all costs.Therefore, emptying containers close to the physical limit is a risky approach.Other system constraints are that certain containers can only be emptied into PU-1 and others only into PU-2.PU-1 takes in material via only one conveyor belt while PU-2 takes in material via two conveyor belts.The quality of an emptying decision is a balance between three criteria:</p>
<p>-Volume criteria: minimize the cumulative deviation from the ideal emptying volume for each container.-Energy criteria: optimize the energy usage or utilization costs of PUs.</p>
<p>-Safety criteria: adhere to the safety and system constraints.Multiple aspects make this problem challenging:</p>
<p>-Stochastic material flow rate.The material flow rate into the containers is unique for each container, as it depends on the type of material, its density, the time of the day, seasonality, and other factors.In addition, the sensor readings determining volume estimates are very noisy.Incorporating this phenomenon, the flow rates are modelled as a stochastic process.This makes approaching the problem with standard planning approaches quite difficult.-Delayed rewards.Certain containers have very slow filling rates, taking up to 5 hours or 300 simulation timesteps of 60 seconds each to reach their respective ideal volumes, and receive a positive reward associated with the correct emptying action, as well as about 300 preceding 'idle' actions.-PU constraint.The limited availability of the PUs implies that always waiting for containers to fill up to their ideal emptying volume is risky: if too many containers are close to their respective ideal volumes and no PU is available at that time, then a safety constraint is violated and a physical overflow occurs.Therefore, an optimal policy needs to take fill states and fill rates of all containers into account, and possibly empty some containers early or later.-Class imbalance.Emptying decisions can be taken at any time, but the emptying actions close to the ideal volume for each container are rather infrequent, compared to the filling times to reach the respective ideal volumes.In addition, the rate at which containers should be emptied varies between containers.There is a class imbalance between the successful emptying actions (and corresponding rewards) and the do-nothing actions making the distributions of actions highly asymmetric, with important actions being relatively rare.</p>
<p>Reinforcement Learning Problem Formulation</p>
<p>In this section, the container management problem referenced in Section 3 is recast within the framework of a Markov Decision Process (MDP), with a corresponding digital twin designed in Python using gymnasium [21].This adaptation retains the crux of the actual problem while deliberately omitting edge cases such as facility downtime and the duration of minor processes for simplicity.The state space is carefully crafted, incorporating all critical elements required for effective decision-making by the agent.By grounding the model's parameters in authentic data, the MDP offers a precise representation of the real-world problem at hand.</p>
<p>State space.</p>
<p>The system's state at a timestep t (s t ), encompasses several key components.These include container volumes {v i,t } n i=1 bounded by predefined minimum and maximum volumes to maintain operational constraints.{p j,t } 2 j=1 captures the normalized (by timestep) time until each of the two processing units becomes available, ensuring that the system can anticipate and plan for processing availability.{b k,t } n k=1 is a binary representation indicating which containers are currently being emptied, providing immediate insight into the immediate container status, rewards from the previous timestep {r l,t−1 } n l=1 , used to integrate feedback from past decisions, and the ideal volumes {pv m } n m=1 for each container guiding the agent towards maintaining optimal material levels.The indices i, j, k, l, and m denote the container or PU identifiers.This representation is expressed as follows:
s t = ({v i,t } n i=1 , {p j,t } 2 j=1 , {b k,t } n k=1 , {r l,t−1 } n l=1 , {pv m } n m=1 )(1)
This formulation ensures an adequate reflection of the system's dynamics, incorporating both the containers' status and the operational state of PUs, thereby facilitating the decision-making process.</p>
<p>Action space.</p>
<p>At any given time t, the agent has two primary choices: (i) to refrain from emptying any container, effectively taking no action and letting the volume increase, or (ii) to empty a specific container and process its contents.The do-nothing action is denoted by 0, while the action of emptying a container and processing its content is denoted by the index i of the container.Thus, the action a t falls within the set {0, 1, . . ., n}, where n represents the total number of containers.Depending on the availability of the PUs the action taken is either successful or unsuccessful for emptying and is reflected through the volumes, status of PUs and time features in the state, and the reward received.</p>
<p>Environment Dynamics</p>
<p>In this section, the dynamics of the volume of material in the containers, the PU model, as well as the state update are discussed.</p>
<p>Container filling rates.In addressing the variability of container fill rates due to the inconsistent nature of plastic material inflow, which ranges from solid lumps to irregular flows, leading to uneven accumulations within containers, a stochastic model is employed.This model accounts for the erratic yet on average linear increase in volume over time through a random walk mechanism with drift.Specifically, the volume update for container i at time t + 1 is modeled as:
v i,t+1 = max(0, α i + v i,t + ϵ i,t ),(2)
where α i represents the average volume increase rate, and ϵ i,t is a normally distributed random variable representing measurement noise.This approach captures real-world fluctuations in fill rates while simplifying the model to facilitate analysis and simulation.Upon the action of emptying a container, its volume is instantaneously reduced to zero in the subsequent timestep.This simplification contrasts with the gradual volume reduction observed in actual scenarios but aligns with our dataset's indication that the emptying process completes within the duration of a single timestep, set at 60 seconds in this study.This modelling choice effectively bridges the gap between the discrete-time model used for simulation and the continuous nature of the emptying process observed in operational environments.</p>
<p>Processing unit dynamics.The transformation time by a Processing Unit (PU) for material volume v in a container depends linearly on the producible products from v, expressed as ⌊v/b ij ⌋.This is detailed by the function g ij in Equation ( 3), incorporating b ij for product size, β ij as the PU's activation time, and λ ij for time per product.Here, i refers to the container index, and j to the specific PU index, indicating that the parameters are specific to each container-PU combination.</p>
<p>If PUs are occupied, containers awaiting processing continue to accumulate material, highlighting the system's operational constraints and the need for efficient PU management.
g ij (v) = β ij + λ ij ⌊v/b ij ⌋ . (3)</p>
<p>Reward Tuning</p>
<p>In addressing the complexities of dynamic decision-making environments, the formulation of reward functions is a critical component.This section presents the case for Gaussian-based reward mechanisms, chosen for their smoothness and ability to accommodate the infrequent nature of container-emptying actions and the uncertainties within state space parameters.Gaussian rewards, characterized by their resilience to noise, ease of interpretation, and smooth gradient provision, are selected to mitigate the effects of measurement inaccuracies and focus on the aggregate effects of the underlying phenomenon.The strategies are presented in three distinct subsections: Simple Gaussian Reward, Custom Reward, and Precision Reward.</p>
<p>Simple Gaussian reward</p>
<p>The reward r(s t , a t , P U status ) is calculated based on the action taken, the current volume of a container, the volume at the next time step, and whether a PU is free to empty the container implicitly contained in s t .A Gaussian distribution centred around the ideal volume for emptying a given container with parameters defining the peak volume (p), peak height (a), and peak width (w) was used.The complete reward function r(s t , a t , P U status ) is summarized in Algorithm 1.It fosters emptying containers at or close to their optimal volumes, where the width (standard deviation) w controls the required precision encoded by the reward.A wider peak enables fast initial learning, while a narrow peak focuses on fine-tuning.</p>
<p>In principle, this reward encodes everything the agent needs to know about the problem at hand.</p>
<p>Custom Reward</p>
<p>Here the custom reward function built on the Gaussian reward is introduced to navigate the complexities of dynamic resource allocation and operational optimisation.This reward is a sum of action Algorithm 1: Gaussian Reward Function for Emptying Decision input : Action at, Current volume vt, PU availability P U _status, penalty rpen, ideal volume vi, Height of the peak h, Width of the peak w output: Reward rt between rpen and 1
if at &gt; 0 then if vt = 0 or not P U _status then rt ← rpen else rt ← (h − rpen) • exp − (v t −v i ) 2 2w 2
+ rpen rewards plus bonuses, positional rewards, and episode termination rewards to enhance the learning efficiency of the agent.Action Reward, which is nothing but the Simple Gaussian Reward Function, described earlier is used to quantify the efficacy of an action at a timestep based on the container's proximity to an ideal volume.It applies only to (supposedly rare) emptying actions.The conditional nature of this reward ensures that meaningful actions--those contributing to the actual emptying behaviour of the agent-are incentivised, while actions that do not align with constraints are penalised.</p>
<p>Positional Reward is used to encourage the system towards an ideal operational state across all containers, not just the one being acted upon.It's designed to address the challenge of slow filling rates across the containers, which leads to sparse action rewards within the operational timeframe.For example, there are containers for which it takes up to 300 timesteps of 60 seconds (or five hours) to reach the ideal emptying volume.These rewards need to be propagated back through a long history of zero-actions that contributed to emptying at the right moment.By assigning a reward to each container based on its volume relative to an ideal operational state, and importantly, doing so irrespective of the agent's actions at every time step, the reward signal ensures a constant nudge towards optimal efficiency across all containers.</p>
<p>The positional reward for a given container not acted upon at time step t, r positional,j , is calculated as follows:
r positional,j =    1 − (vi−vj,t) vi 0.5 , if v j,t ≤ v i , −0.1, otherwise.(4)
The cumulative positional reward for all containers is calculated as
r positional = n j=1 r positional,j(5)
where n is the total number of containers.Episode Termination Reward is a critical component designed to ensure the agent learns to avoid prematurely ending an episode by reaching an unsafe state.It aims at discouraging the agent from allowing any container to exceed its physical limit of 40 volume units, a crucial safety consideration.</p>
<p>The episode termination reward, r termination , is defined as follows: r termination = 0.2, small positive reward for not ending the episode prematurely, −30, if any container's volume exceeds the safety limit of 40.(6)</p>
<p>Precision Reward</p>
<p>While the Gaussian reward in principle encodes the full goal of emptying at the optimal volume, its smooth and rather flat top can make fine-tuning difficult.Therefore, we introduce an additional incentive for the agent to empty at the ideal volume by rewarding within a narrowly defined optimal range and penalizing deviations:</p>
<p>Methodology</p>
<p>In the section, a scaffold approach to developing reinforcement learning agents is presented.This methodology, encompassing five phases, progressively introduces increased complexity, systematically enhancing agent competency.By employing a phased curriculum learning strategy, the agent's evolution is carefully curated, ensuring a gradual ascension to operational adeptness.In the following, we go through each phase, starting from its goals, and explaining the measures for reaching these goals.</p>
<p>Phase 1: Foundational Training.The goal of this phase is to learn a first non-trivial policy for a simplified version of the task.In particular, the agent must learn to perform the zero-action most of the time, while executing rare but critical emptying actions when a container volume comes close to the optimal volume.In this initial phase, the random walk for filling containers is disabled, resulting in a deterministic and easily predictable environment.Furthermore, there are as many PUs as containers.That's a very unrealistic setting that in effect removes all dependencies between containers reaching peak volume at the same time.Here, the Custom Reward is implemented.Actions are taken every 30 seconds, and episodes are as short as 25 time steps.The initial states are designed such that there are at least 8-11 emptying actions that lead to positive rewards and the rest being do-nothing actions.This helps the initial agent in dealing with the class imbalance problem alluded to in section 3.</p>
<p>Phase 2: Refinement.In this stage, the Precision Reward is added for a budget of 1 million timesteps.All other settings are retained from the first phase.The introduction of Precision Reward aims to refine the agent's accuracy in actions, within a deterministic setting, enhancing the precision in decision-making.</p>
<p>Phase 3: Penalizing Greedy Actions.This training stage aims at altering the policy so that it becomes more energy efficient.This is achieved by slightly penalizing all emptying actions.The reasoning is that PUs are a scarce resource, and emptying a container twice instead of once blocks the PU for an unnecessarily long time, and it also uses more energy than processing more of the same material in one go.This adjustment is intended to augment the agent's greedy nature to accumulate more episodic rewards by prematurely emptying a container too many times as opposed to optimizing for the least number of emptying actions per episode along with precise emptying actions.</p>
<p>Phase 4: Inject Real-world Complexity.Now that the basic behaviour is as intended, the agent is ready to be exposed to the true complexity of the environment.The budget for this phase is 0.5 million timesteps.Timesteps are extended to 60 seconds, and the episodes are up to 600 timesteps long.In addition, the stochastic filling rates and resource constraints are introduced.This results in significant changes in the environment dynamics (noisy dynamics, failing emptying actions if all PUs are busy), which must be accounted for with corresponding changes in the value function estimator.To achieve this, the policy network is frozen and only the value network is trained as both of these are distinct neural nets not sharing network parameters [22].This phase is tailored to tune the agent's value network to the unpredictable and constrained operational dynamics.</p>
<p>Phase 5: Fine-tuning with Reduced KL Constraint.This phase concludes the curriculum, mirroring Phase 4's operational parameters but reinstating the Precision Reward.Gradually unfreezing the policy with a low KL constraint (limiting the Kullback Leibler divergence between action distributions) allows controlled exploration around the learned policy within the complex environment introduced in Phase 4.</p>
<p>To address operational constraints and edge cases effectively, action masking was incorporated after the curriculum learning phases for inference on the test environment.This adaptation ensures alignment with the plant's structural and operational limitations, where the disposition of containers relative to Processing Units (PUs) is dictated by the proximity to conveyor belts.Given this configuration, only specific containers can be targeted for emptying based on their accessibility to certain PUs.Additionally, dynamic reduction of the action space is facilitated through action masking [7], particularly when all PUs are engaged, enhancing decision-making efficiency.</p>
<p>Experimental Evaluation</p>
<p>The goal of this section is to empirically verify the effectiveness of the proposed approach.We aim to achieve this by answering two research questions.</p>
<p>Research question 1: Is the curriculum approach effective?To this end, we compare the PPO-Curriculum learning (PPO-CL) agent trained in five phases for all three criteria agents with our first baseline.This is a naive PPO agent (PPO-volume criteria), optimized only for one of the three criteria, i.e., optimal emptying volumes encoded by training with a Simple Gaussian reward.</p>
<p>Research question 2: Does the proposed approach offer a real-world benefit?For this, we compare PPO-CL with our second baseline, which is a meticulously hand-crafted analytical agent labelled as a Optimal Analytic agent.This agent is close to the semi-automated decision policy currently deployed at the waste sorting facility.It is optimal in the sense that it empties containers at their precise target volumes if possible, and that it performs emergency emptying at a volume of 37 (out of 40) to avoid overflow.However, it does not take interactions of containers competing for PUs into account.</p>
<p>With respect to metrics, cumulative reward values are not necessarily the most relevant performance measure.Therefore we investigate different aspects like safety, energy-saving behavior and timing precision separately, in order to arrive at conclusions that are meaningful not only from an RL perspective but also from an application point of view.</p>
<p>In the spirit of open and reproducible research, we make our source code available via an anonymous repository. 1 The repository contains a script for reproducing all results presented in this section.</p>
<p>Experimental Setup</p>
<p>The PPO-CL (five phases) and PPO-volume criteria agents undergo training within environments with eleven containers and two PUs.These are characterized by a 60-second time-step and a 600episode length, with default settings maintained for other hyperparameters.To evaluate stability, fifteen independent training runs are conducted with distinct random seeds for each agent.Our second baseline, the Optimal Analytic agent operates with a fixed logic.Hence it is designed, not trained.Its main criterion for emptying decisions is the evaluation of the proximity of each container's volume to its ideal state.It also prioritizes actions based on operational conditions such as PU availability and the plant's physical layout (e.g., container alignment with conveyor belts).To ensure a fair comparison, we test the PPO-CL agent and both baselines on the same test environment.</p>
<p>Results</p>
<p>In this section, we present the empirical results.Figure 3 shows a single rollout with the best policy produced by both PPO agents.Our visualization tracks container volumes, actions, and PU utilization over time, offering a more nuanced analysis than reward values alone.The volume chart shows different containers being emptied at around their respective ideal volumes and the frequency of peaks in the time chart for PUs gives a sense of how they compete for the resources.The key insight here is that although the PPO-volume criteria agent seems to show reasonable behaviour, its emptying decisions are not properly timed, as is evident from the large reward fluctuations.In contrast, the PPO-CL agent consistently achieves close-to-optimal rewards.</p>
<p>Empyting Decision Quality Here we compare all three agents for the quality of emptying decisions.For the trained PPO agents, these metrics represent the best out of 15 independent training runs.Table 2 shows the average inference volume deviation (deviation between actual and ideal container volume) across all containers.Figure 6 compares the container-wise percentage volume deviation of all agents.</p>
<p>The first baseline, the PPO-volume criteria agent, performs poorly with the highest deviation.The PPO-CL agent and the analytical agent achieve much smaller deviations, while the former does better than the latter.This is also evident from looking at their performance individually for each container.The PPO-volume criteria agent, although trained only for optimizing volume, has higher than 20% volume deviation for five containers, which is quite excessive.In contrast, the PPO-CL agent has the lowest deviation values for the majority of the containers.</p>
<p>Safety and Energy savings Table 2 also shows the total number of emptying actions.Both baseline agents take the same number of emptying actions, while PPO-CL gets along with fewer actions, consequently emptying containers at higher volumes.This behavior results in better resource utilization, in terms of PU occupation time, and also in terms of energy.</p>
<p>Figure 4 further highlights the PPO-CL agent's strengths in energy efficiency and safety compliance.On the left, we see significant energy conservation: the PPO-CL agent utilizes the PU 12% less than the Optimal Analytic Agent and 24% less than the PPO-volume criteria agent.The right graph underscores the PPO-volume criteria agent's alarming safety violations: 27.11% of its actions exceed the critical volume limit of 40.This highlights the risks of training an RL agent from scratch in complex environments with safety constraints.In contrast, the Optimal Analytic agent exhibits zero violations due to its hard-coded limit of 37, acting as a safety benchmark but lacking adaptability.The PPO-CL agent, however, achieves a remarkable 1.7% violation rate.</p>
<p>Overall, the PPO-CL agent demonstrates a superior balance.The results suggest that phased learning strategies offer distinct advantages in complex, multi-criteria decision-making environments, particularly in industrial settings where precision, adaptability, and efficiency are crucial.</p>
<p>Discussion</p>
<p>To gain deeper insights into the PPO-CL agent's safety, energy efficiency, and volume management, we further analyze the results from the previous section.</p>
<p>The PPO-CL agent's safety adherence, a critical outcome highlighted by the results, likely stems from two key curriculum design elements: the enforcement of safety constraints and the early emphasis on safe exploration.Short episode lengths in initial phases (25 timesteps, as seen in Table 1) encouraged exploration within safe state spaces.Furthermore, the reward structure in Phase 3, which penalizes actions even with positive outcomes, directly incentivizes conservative decisionmaking, contributing to the agent's reduced PU utilization and energy savings.This flexibility in balancing competing objectives makes our framework particularly suitable for complex industrial settings where safety is of utmost importance.To analyze the emptying decision quality of the PPO-CL agent against our two baselines, we examine the empirical cumulative distribution functions (ECDFs) in Figure 5.</p>
<p>ECDFs plot the probability, based on observed data from agent actions, that a container is emptied at or below a given volume.Here, steeper curves signify greater consistency in emptying volumes at which a container is emptied across different instances.Our first baseline, the PPOvolume criteria agent, exhibits higher variance in its ECDF, particularly for slower-filling containers (e.g., C1-40, C1-60, C2-10, C2-20).This points to the difficulty of learning effective policies when rewards are delayed due to extended fill times, an issue hindering from-scratch training as discussed in Section 5.2.In contrast, our second baseline, the Optimal Analytic agent, employs a deterministic logic that leads to frequent preemptive emptying.While this ensures no safety violations, it results in inefficient energy usage.</p>
<p>The PPO-CL agent's ECDFs demonstrate remarkable consistency across all containers, regardless of fill rate.This highlights its ability to manage delayed rewards effectively, leading to optimal energy usage with fewer emptying actions per episode -a key advantage emphasized by our results.Crucially, unlike the PPO-volume criteria agent, the PPO-CL agent consistently avoids reaching the critical volume threshold of 40 (evident from all slow-filling containers), demonstrating its strong safety compliance.</p>
<p>Conclusion</p>
<p>In this work, a curriculum learning approach was introduced, designed to progressively enhance the complexity of the environment for training a Proximal Policy Optimization (PPO) agent to navigate the intricacies of real-world industrial operations.Our model not only achieves a harmonious balance among competing operational goals like volume management, energy conservation, and adherence to stringent safety protocols, but it also paves the way for the broader application of such strategies in advanced, adaptive, multi-criteria decision-making environments.We believe that with some (industrial) domain expertise available, designing a curriculum is an attractive alternative to using excessive computing for solving hard problems, e.g., by training more complex networks or by applying multi-agent approaches.</p>
<p>We view our achievement as a solid basis for future endeavours.Although our agent performs well in most cases, we have not yet pushed its ability to avoid unsafe behaviour to the maximum.This amounts to avoiding future collisions of PU usage requests, which may occur if too many containers reach their ideal volume at the same time.That situation should be counteracted by emptying some containers earlier than usual.The problem is pressing since PUs are expensive units, so facilities are often designed with the smallest possible number of PUs.Designing a systematic solution to this problem will be subject to future research.It may involve forms of (stochastic) real-time planning, as well as further curriculum steps specifically targeted at the collision problem.</p>
<p>For practitioners in the realm of real-world reinforcement learning, we offer two take-home messages.First, it has proven invaluable to evaluate agent performance beyond mere cumulative rewards, using ECDF plots, diverse metrics, and statistical tools.Second, particularly in environments of high complexity, adopting a curriculum-based training approach can help with the learning of meaningful policies, surmounting the limitations faced by vanilla agents.</p>
<p>Fig. 1 :
1
Fig. 1: Layout sketch of a facility with 11 containers and 2 PUs, connected with conveyor belts.The containers are filled from above, with their current fill states indicated by the shaded areas.</p>
<p>Algorithm 2 : 1 Fig. 2 :
212
Fig. 2: Plots showing the various reward functions: Simple Gaussian (left), Custom Reward (centre), and Precise Reward (right).</p>
<p>Fig. 3 :
3
Fig. 3: A single rollout (best agent out of 15) of the PPO-CL (left) and PPO-volume criteria on a test environment with 11 containers.Displayed are the volumes, emptying actions, rewards, and time to process by PU-1 and PU-2.</p>
<p>Fig. 4 :Fig. 5 :
45
Fig. 4: Comparison of key performance metrics across different agents, collected over 15 rollouts of the best policy for both PPO agents.The left figure presents the average total PU utilization across agents.The right figure details the percentage of safety violations</p>
<p>Fig. 6 :
6
Fig. 6: Comparision of percentage volume deviation across all containers, collected over 15 rollouts of the best policy for both PPO agents</p>
<p>Table 1 :
1
Summary of curriculum learning phases and their parameters
Phases Budget TimeSteps (Ts) Ep-len (in Ts) RewardFill. rates Resource Constraint (PUs)Phase-1 1.53025CustomDeterministicNoPhase-2 13025PrecisionDeterministicNoPhase-3 13025PrecisionwithDeterministicNopenalty for posi-tive actionsPhase-4 0.560600Policy frozen andStochasticYesPrecisionPhase-5 0.560600Precision Reward StochasticYes</p>
<p>Table 2 :
2
Performance metrics for both RL agents (best seed out of 15) and analytic agent for 15 rollouts on the same test environment: emptying actions and average inference volume deviation
AgentEmptying Actions Avg. Inf Vol Deviation (± Std Dev)PPO-volume criteria6215.16 ± 10.80Optimal Analytic Agent624.47 ± 2.61PPO-CL563.55 ± 2.33
https://gitlab.com/anonymousppocl1/ppo_paper.git
Acknowledgements: This work was funded by the German federal ministry of economic affairs and climate action through the "ecoKI" grant.
Y Bengio, J Louradour, R Collobert, J Weston, Curriculum learning. </p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, Y Duan, P Abbeel, 2018</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, P Abbeel, 1st Conference on Robot Learning (CoRL. 2017</p>
<p>Safe exploration for reinforcement learning. J Garcia, F Fernandez, European Symposium on Artificial Neural Networks. 2012ESANN</p>
<p>A comprehensive survey on safe reinforcement learning. J García, F Fernández, Journal of Machine Learning Research. 162015</p>
<p>Extending the capabilities of reinforcement learning through curriculum: A review of methods and applications. K Gupta, D Mukherjee, H Najjaran, SN Computer Science. 3128Oct 2021</p>
<p>A closer look at invalid action masking in policy gradient algorithms. S Huang, S Ontañón, Jun 2020</p>
<p>Automated curriculum learning for deep reinforcement learning. N Justesen, P Bontrager, J Togelius, S Risi, Conference on Computational Intelligence and Games (CIG). 2018</p>
<p>Reinforcement learning: A survey. L P Kaelbling, M L Littman, A W Moore, May 19964</p>
<p>Self-paced learning for latent variable models. M P Kumar, B Packer, D Koller, Advances in Neural Information Processing Systems. 2010</p>
<p>Curriculum learning for reinforcement learning domains: a framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, J. Mach. Learn. Res. 211Jan 2020</p>
<p>ContainerGym: A Real-World reinforcement learning benchmark for resource allocation. A Pendyala, J Dettmer, T Glasmachers, A Atamna, Machine Learning, Optimization, and Data Science. SwitzerlandSpringer Nature2024</p>
<p>Safe curriculum learning for optimal flight control of unmanned aerial vehicles with uncertain system dynamics. T Pollack, E J Van Kampen, American Institute of Aeronautics and Astronautics. 2020. Jan 2020AIAA Scitech</p>
<p>Imagination augmented agents for deep reinforcement learning. S Racanière, A K Lampinen, A Santoro, D P Reichert, V Firoiu, T P Lillicrap, Advances in Neural Information Processing Systems. 32NeurIPS 2019. 2019</p>
<p>M Riedmiller, R Hafner, T Lampe, M Neunert, J Degrave, T Wiele, V Mnih, N Heess, J T Springenberg, arXiv:1802.10567Learning by playing -solving sparse reward tasks from scratch. 2018arXiv preprint</p>
<p>Curriculum learning: A survey. P Soviany, R T Ionescu, P Rota, N Sebe, Int. J. Comput. Vis. 1306Jun 2022</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, Z Lin, I Kostrikov, G Synnaeve, A Szlam, R Fergus, 6th International Conference on Learning Representations. ICLR2018</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Automatic curriculum graph generation for reinforcement learning agents. M Svetlik, M Leonetti, J Sinapov, R Shah, N Walker, P Stone, AAAI. 311Feb 2017</p>
<p>Algorithms for Reinforcement Learning. C Szepesvári, 10.1007/978-3-031-01551-92010Springer International Publishing</p>
<p>. M Towers, J K Terry, A Kwiatkowski, J U Balis, G D Cola, T Deleu, M Goulão, A Kallinteris, A Kg, M Krimmel, R Perez-Vicente, A Pierré, S Schulhoff, J J Tai, A T J Shen, O G Younis, 10.5281/zenodo.8127026Gymnasium. Mar 2023</p>
<p>SCC: an efficient deep reinforcement learning agent mastering the game of StarCraft II. X Wang, J Song, P Qi, P Peng, Z Tang, W Zhang, W Li, X Pi, J He, C Gao, H Long, Q Yuan, Dec 2020</p>
<p>A survey on curriculum learning. X Wang, Y Chen, W Zhu, IEEE Trans. Pattern Anal. Mach. Intell. 449Sep 2022</p>
<p>Curriculum learning by transfer learning: Theory and experiments with deep networks. D Weinshall, G Cohen, D Amir, 2018</p>
<p>Training agent for first-person shooter game with actor-critic curriculum learning. Y Wu, Y Tian, International Conference on Learning Representations. 2017</p>
<p>On the statistical benefits of curriculum learning. Z Xu, A Tewari, K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, Proceedings of the 39th International Conference on Machine Learning. Proceedings of Machine Learning Research. S Sabato, the 39th International Conference on Machine Learning. Machine Learning ResearchPMLR2022162</p>            </div>
        </div>

    </div>
</body>
</html>
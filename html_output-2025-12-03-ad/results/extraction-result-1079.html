<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1079 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1079</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1079</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-e0ccc40cdbd81ef83cdf99f4bed486c848e1f090</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e0ccc40cdbd81ef83cdf99f4bed486c848e1f090" target="_blank">Intrinsically motivated model learning for a developing curious agent</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Development and Learning</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that combining the agent's intrinsic rewards with external task rewards enables the agent to learn faster than using external rewards alone, and that the learned model can be used afterward to perform tasks in the domain.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1079.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1079.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEXPLORE-VANIR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEXPLORE with Variance-And-Novelty-Intrinsic-Rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsically motivated, model-based reinforcement learning agent that learns random-forest transition models and uses two intrinsic rewards (variance-based and novelty-based) to drive curiosity-driven exploration and efficient model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TEXPLORE-VANIR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model-based reinforcement learning agent using random forests to predict next-state distributions for each state feature, planning with UCT in the RTMBA real-time architecture, and augmenting the model's reward with two intrinsic rewards: VARIANCE-REWARD (based on KL divergence among trees) and NOVELTY-REWARD (L1 distance to nearest trained state for given action).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A simulated multi-room domain with sequence of rooms, each containing a door, a lock, and possibly a key. Agent state is a 17-dimensional vector (X, Y, room ID, KEY boolean, LOCKED boolean, 12 light sensors detecting three colored lights in four directions). Actions: 6 discrete actions (4 stochastic moves, PRESS lock, PICKUP key). Dynamics include stochastic movement (0.9 intended, 0.05 side each) and stochastic effectiveness for PRESS/PICKUP (0.9). Complexity arises from object interactions (must pick up key before unlocking, one-way doors), stochasticity, and a rich sensory feature vector.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dimensionality (17 features); action set size (6 discrete actions); presence of object-interaction dynamics (pickup then press required to open lock); stochastic transition probabilities (movement: 0.9/0.05/0.05; PICKUP/PRESS success 0.9); multi-room progression and conditional dynamics (requiring sequencing). Model accuracy measured by variational distance between predicted and true next-state distributions averaged over 5000 sampled state-actions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (17-dim state, 6 actions, stochastic and conditional object interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Stochasticity in transitions (movement and action success probabilities), randomized starting states (agent starts in random state in top-left room), variability in room contents (keys may or may not be present), repeated room sequence (indefinite progression) — sampled evaluation over 5000 random state-actions to capture distributional variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (nontrivial stochasticity and randomized initial conditions, but environment structure (rooms/objects) is fixed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Model accuracy: average variational distance between model-predicted next-state distribution and true next-state distribution (computed over 5000 randomly sampled state-actions). Task performance: cumulative external reward over 3000 steps in a room-traversal task. Exploration behavior: counts of specific actions (e.g., PRESS) in particular states over 1000 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Model: TEXPLORE-VANIR achieved the lowest average variational distance among compared methods (exact numeric variational distances not reported), statistically significant improvement (p < 0.025). Task: highest cumulative external reward over 3000 steps when exploiting learned model (exact reward totals not given), statistically significant (p < 0.001). Exploration: significantly more targeted PRESS attempts on relevant objects than a random agent (plots shown; no absolute counts reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs between model generalization, environment complexity, and suitable intrinsic motivations. Key points: (1) The best intrinsic reward depends on both the learning algorithm and domain complexity/variation: e.g., competence-progress (R-IAC) is intended for very large, high-dimensional continuous domains where learning is slow, but performs poorly in the smaller, faster-learning Light World because by the time competence progress is detected the region is already learned. (2) R-MAX style exploration works well for tabular models (high variation in unseen state-actions requires visit-count bonuses) but poorly for generalizing random-forest models. (3) For models that generalize (random forests), a combination of variance-based (to resolve model disagreement) and novelty-based (to explore far-from-training examples) intrinsic rewards yields better sample efficiency and progressive learning of complex dynamics. Overall, complexity (conditional object interactions) increases learning difficulty and demands intrinsic rewards that encourage staged/developing exploration; variation (stochastic transitions, randomized starts) requires probabilistic-model-aware measures (e.g., KL divergence among ensemble predictions) rather than simple error-based bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment intrinsic exploration phase (1000 steps) to learn transition model using intrinsic rewards, followed by a task phase (3000 steps) where the learned model is used greedily with respect to an external reward; RTMBA real-time model-based architecture with parallel model-learning, planning and acting; no curriculum or domain randomization used.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — after 1000 steps of reward-free exploration to learn a transition model, the learned model was used to perform a different task (reward of 10 for moving between rooms) for 3000 steps with greedy action selection; TEXPLORE-VANIR outperformed baselines in cumulative external reward (statistically significant, p < 0.001), indicating the learned model generalized to enable task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Exploration/model-learning phase: 1000 environment steps (agents acted at 2.5 actions/sec). Model accuracy was measured periodically (every 25 steps) using 5000 sampled state-actions; TEXPLORE-VANIR learned a more accurate model than baselines within the 1000-step exploration window (exact number of steps to convergence not numerically specified).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TEXPLORE-VANIR's combination of variance-based intrinsic reward (ensemble KL divergence) and novelty-based intrinsic reward (L1 distance in normalized feature space to nearest trained state for same action) yields more accurate transition models within 1000 steps than several baselines (random action, competence-progress (R-IAC style), prediction-error, R-MAX style bonuses, tabular baselines). The learned models transfer to downstream task performance (3000-step room traversal) yielding higher cumulative reward than baselines. The paper emphasizes that optimal intrinsic motivation depends on both the model learning method and environment complexity/variation: competence-progress is suited to very high-dimensional slow-learning domains, R-MAX suits tabular models, while variance+novelty suits generalizing ensemble models in moderately complex stochastic domains. TEXPLORE-VANIR exhibits developing curiosity, progressing from easier interactions (PICKUP) to harder ones (PRESS on lock) rather than random or exhaustive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically motivated model learning for a developing curious agent', 'publication_date_yy_mm': '2012-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1079.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1079.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Random Action Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that selects actions uniformly at random; used as a control to show the benefit of intrinsic-motivation-driven exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Selects one of the six actions uniformly at random at each timestep; learns no model (unless paired with a tabular model variant in experiments) and serves as a baseline for exploration behavior and naive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Light World domain as above; random agent experiences the same stochastic transitions and object interactions but does not direct exploration toward resolving uncertainty or novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same domain measures (17 features, 6 actions, stochasticity). For random agent, complexity manifests as likelihood of accidentally discovering multi-step object interactions is low.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (same as main environment)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Experience variation arises only from environment stochasticity and randomized initial states; agent does not purposefully vary exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration behavior (counts of PRESS attempts on relevant objects), model baseline comparisons (when paired with tabular model), and cumulative reward in task phase.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performed substantially worse than TEXPLORE-VANIR: tried PRESS on arbitrary states more often than correctly on locks; when compared in task phase, cumulative reward was significantly lower (exact numeric totals not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Used as a baseline to illustrate that unguided exploration fails to focus on complex, conditional interactions (e.g., unlocking requires key) and is less sample-efficient in discovering multi-step behaviors under stochastic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>No learning-driven exploration strategy; pure random action selection for 1000 steps in exploration phase; optionally paired with a tabular model variant in some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not applicable; random agent does not learn a generalizing model and performs poorly when later tasked with the room traversal reward.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very low sample efficiency for discovering conditional object interactions; qualitative plots show substantially fewer correct PRESS attempts on locks compared to TEXPLORE-VANIR over 1000 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random exploration tends to call complex actions arbitrarily and fails to structure exploration from easy to hard skills; demonstrates the value of intrinsic-reward-driven exploration for sample efficiency and targeted skill discovery in stochastic, conditional domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically motivated model learning for a developing curious agent', 'publication_date_yy_mm': '2012-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1079.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1079.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Competence-Progress (R-IAC style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Competence Progress (inspired by R-IAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic-motivation baseline that splits state space into regions, tracks prediction error curves per region, and rewards regions where model error is improving most (i.e., competence progress).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R-IAC: Robust Intrinsically Motivated Exploration and Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Competence-Progress Agent (R-IAC variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A comparison agent implementing competence-progress intrinsic rewards: state space is partitioned into random regions; each region maintains an error curve and intrinsic reward equals slope (improvement) of prediction error; integrated with the same TEXPLORE model learning framework for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates in the Light World domain. The method was originally intended for very large, high-dimensional continuous domains where learning is slow; here it is applied to the 17-dim Light World with relatively quick regional learning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Intended for very high-dimensional continuous state spaces where regional learning progress estimates require many samples; in Light World the domain is smaller so per-region sample budgets are low.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>designed for high complexity domains; in the Light World test it is effectively mismatched (domain not large enough).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is measured indirectly via regional prediction error curves; competence progress measures derivatives of error over time in each region.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>method suited to high variation across large continuous regions; in Light World variation is moderate causing delayed/ineffective progress signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Model accuracy (variational distance) after 1000 steps; downstream cumulative task reward after using learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performed worse than TEXPLORE-VANIR and often poorly in Light World; competence-progress reward did not improve sample efficiency here because regions are learned quickly and progress signals arrive too late (exact numeric results not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper explicitly contrasts this method's suitability: competence-progress is recommended for large/high-dimensional domains where learning is slow; in lower-complexity/more rapidly-learned domains like Light World, competence-progress delays exploration of useful areas and underperforms.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Partitioned-region competence tracking (random regions at start) with intrinsic rewards proportional to competence progress; integrated with TEXPLORE-style model learning for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not reported as effective in this domain; competence-progress did not lead to superior models or task performance in Light World given the available samples.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low in this domain: by the time regional error derivatives signaled progress, the agent often already had sufficient data and didn't benefit from targeted exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Competence-progress intrinsic rewards (R-IAC style) are not universally effective; their utility depends on domain scale and learning speed — effective in very large/high-dimensional slow-learning domains, but ineffective in the comparatively smaller Light World where regions are learned rapidly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically motivated model learning for a developing curious agent', 'publication_date_yy_mm': '2012-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1079.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1079.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prediction-Error Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Region Prediction-Error Intrinsic Reward Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that rewards regions with high prediction error (rather than progress), intended to drive exploration to areas with currently-high model error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Prediction-Error Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses region-based partitioning like Competence-Progress, but intrinsic reward is proportional to current prediction error in region; integrated with TEXPLORE model learning for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same 17-dim Light World domain; prediction-error rewards aim to push agent into regions with high instantaneous model error.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Similar domain complexity metrics (17 features, object interactions, stochastic transitions). Prediction-error method measures instantaneous model error in regions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Captures variation where model currently predicts poorly; depends on partitioning resolution (random regions) and sample coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Model variational distance; downstream cumulative reward when model used for task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performed poorly relative to TEXPLORE-VANIR; authors note that it may not visit the right state-actions within high-error regions, leading to worse model learning (no absolute numbers reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper suggests that simply rewarding high prediction error can fail in structured domains because regions of high error may contain subregions with both informative and uninformative state-actions; without targeted signals (e.g., ensemble disagreement or novelty by distance), the agent may not resolve the right dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Region-based error reward with TEXPLORE model learning; no curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not highlighted as providing useful generalization in the Light World experiments; model quality and downstream task performance were inferior to TEXPLORE-VANIR and R-MAX.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample efficiency compared to TEXPLORE-VANIR; failed to focus exploration on the most informative state-actions inside error regions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically motivated model learning for a developing curious agent', 'publication_date_yy_mm': '2012-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1079.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1079.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R-MAX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R-MAX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tabular model-based RL algorithm that gives optimistic intrinsic rewards (R_max) to state-actions visited fewer than m times, driving exhaustive visitation guarantees in finite MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R-Max - a general polynomial time algorithm for near-optimal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R-MAX</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular model-based RL algorithm that initializes unvisited state-actions with optimistic reward R_max until visited at least m times; used both in pure R-MAX form and as a baseline when combined with the TEXPLORE random-forest model.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to Light World domain. R-MAX encourages aggressive exploration to visit state-actions enough times to build an accurate tabular model.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Tabular state-action visitation counts; in large or continuous state spaces tabular R-MAX is infeasible. In Light World, state space size is large but finite given discretized 17 features and room positions; R-MAX still used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (sufficient for R-MAX baseline but high enough that exhaustive visitation is expensive)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation handled by visit counts per state-action; stochastic transitions mean multiple visits required to estimate transition distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Model variational distance after exploration phase; cumulative external reward during task exploitation; combined intrinsic+external reward experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>R-MAX was the next-best method after TEXPLORE-VANIR in model accuracy and task reward when exploration and exploitation were separated into phases. However, when intrinsic and external rewards were combined, R-MAX's aggressive exploration cost it external reward and it underperformed relative to TEXPLORE-VANIR (exact numeric values not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper argues that R-MAX style bonuses make sense for tabular models in which exhaustive visitation is needed to learn transitions, but are less suited to generalizing models (like random forests) and to settings where exploration-exploitation must be balanced continuously.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Tabular optimistic initialization with visit-count threshold m (R-MAX); also tested R-MAX combined with TEXPLORE model (performed poorly).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>R-MAX produced reasonably accurate tabular models in the Light World given the experimental budget and transferred to task performance when exploration/exploitation were separated, but was less effective when exploration bonuses interfered with obtaining external reward during combined training.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than TEXPLORE-VANIR in this domain when using a generalizing model; needed more visits per state-action to reach similar model accuracy in tabular formulation (exact counts not given).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>R-MAX is appropriate for tabular models requiring exhaustive visitation but is not an ideal intrinsic bonus for generalizing models; aggressive optimistic exploration can harm combined intrinsic+external reward learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically motivated model learning for a developing curious agent', 'publication_date_yy_mm': '2012-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1079.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1079.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tabular Random Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Action with Tabular Model Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Random action selection paired with a tabular model that assumes self-transitions for unvisited state-actions; used to contrast tabular versus generalizing model strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random (Tabular Model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acts randomly but maintains a tabular model initialized to predict self-transitions for unvisited state-actions; used to show effects of naive tabular priors compared to intrinsic-motivation-driven exploration with generalizing models.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light World</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Light World domain; tabular prior biases model predictions for unvisited state-actions and affects downstream task exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Tabular state-action count relative to 17-dim state representation; many unvisited state-actions given finite sample budget.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high; tabular representation struggles due to large effective state-action space.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is handled only indirectly via stochastic observed transitions; unvisited state-actions default to self-transition predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Model accuracy and downstream cumulative task reward when acting greedily on the tabular model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performed worse than TEXPLORE-VANIR and R-MAX; acting randomly with a tabular prior did not lead to competitive task performance (exact numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Demonstrates that naive tabular priors plus random exploration do not effectively handle the complexity and variation present in Light World, motivating generalizing models and intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Random action selection for exploration while collecting tabular counts and transitions; no intrinsic reward guiding exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tabular model with random exploration did not generalize effectively to the downstream room traversal task compared to TEXPLORE-VANIR.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low; many state-actions remain unvisited and tabular self-transition priors bias model away from true dynamics until many visits are collected.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically motivated model learning for a developing curious agent', 'publication_date_yy_mm': '2012-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>R-IAC: Robust Intrinsically Motivated Exploration and Active Learning <em>(Rating: 2)</em></li>
                <li>R-Max - a general polynomial time algorithm for near-optimal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Real time targeted exploration in large domains <em>(Rating: 2)</em></li>
                <li>RTMBA: A real-time modelbased reinforcement learning architecture for robot control <em>(Rating: 1)</em></li>
                <li>Random forests <em>(Rating: 1)</em></li>
                <li>Building portable options: Skill transfer in reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1079",
    "paper_id": "paper-e0ccc40cdbd81ef83cdf99f4bed486c848e1f090",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "TEXPLORE-VANIR",
            "name_full": "TEXPLORE with Variance-And-Novelty-Intrinsic-Rewards",
            "brief_description": "An intrinsically motivated, model-based reinforcement learning agent that learns random-forest transition models and uses two intrinsic rewards (variance-based and novelty-based) to drive curiosity-driven exploration and efficient model learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TEXPLORE-VANIR",
            "agent_description": "A model-based reinforcement learning agent using random forests to predict next-state distributions for each state feature, planning with UCT in the RTMBA real-time architecture, and augmenting the model's reward with two intrinsic rewards: VARIANCE-REWARD (based on KL divergence among trees) and NOVELTY-REWARD (L1 distance to nearest trained state for given action).",
            "agent_type": "simulated agent",
            "environment_name": "Light World",
            "environment_description": "A simulated multi-room domain with sequence of rooms, each containing a door, a lock, and possibly a key. Agent state is a 17-dimensional vector (X, Y, room ID, KEY boolean, LOCKED boolean, 12 light sensors detecting three colored lights in four directions). Actions: 6 discrete actions (4 stochastic moves, PRESS lock, PICKUP key). Dynamics include stochastic movement (0.9 intended, 0.05 side each) and stochastic effectiveness for PRESS/PICKUP (0.9). Complexity arises from object interactions (must pick up key before unlocking, one-way doors), stochasticity, and a rich sensory feature vector.",
            "complexity_measure": "State dimensionality (17 features); action set size (6 discrete actions); presence of object-interaction dynamics (pickup then press required to open lock); stochastic transition probabilities (movement: 0.9/0.05/0.05; PICKUP/PRESS success 0.9); multi-room progression and conditional dynamics (requiring sequencing). Model accuracy measured by variational distance between predicted and true next-state distributions averaged over 5000 sampled state-actions.",
            "complexity_level": "medium-high (17-dim state, 6 actions, stochastic and conditional object interactions)",
            "variation_measure": "Stochasticity in transitions (movement and action success probabilities), randomized starting states (agent starts in random state in top-left room), variability in room contents (keys may or may not be present), repeated room sequence (indefinite progression) — sampled evaluation over 5000 random state-actions to capture distributional variation.",
            "variation_level": "medium (nontrivial stochasticity and randomized initial conditions, but environment structure (rooms/objects) is fixed)",
            "performance_metric": "Model accuracy: average variational distance between model-predicted next-state distribution and true next-state distribution (computed over 5000 randomly sampled state-actions). Task performance: cumulative external reward over 3000 steps in a room-traversal task. Exploration behavior: counts of specific actions (e.g., PRESS) in particular states over 1000 steps.",
            "performance_value": "Model: TEXPLORE-VANIR achieved the lowest average variational distance among compared methods (exact numeric variational distances not reported), statistically significant improvement (p &lt; 0.025). Task: highest cumulative external reward over 3000 steps when exploiting learned model (exact reward totals not given), statistically significant (p &lt; 0.001). Exploration: significantly more targeted PRESS attempts on relevant objects than a random agent (plots shown; no absolute counts reported).",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs between model generalization, environment complexity, and suitable intrinsic motivations. Key points: (1) The best intrinsic reward depends on both the learning algorithm and domain complexity/variation: e.g., competence-progress (R-IAC) is intended for very large, high-dimensional continuous domains where learning is slow, but performs poorly in the smaller, faster-learning Light World because by the time competence progress is detected the region is already learned. (2) R-MAX style exploration works well for tabular models (high variation in unseen state-actions requires visit-count bonuses) but poorly for generalizing random-forest models. (3) For models that generalize (random forests), a combination of variance-based (to resolve model disagreement) and novelty-based (to explore far-from-training examples) intrinsic rewards yields better sample efficiency and progressive learning of complex dynamics. Overall, complexity (conditional object interactions) increases learning difficulty and demands intrinsic rewards that encourage staged/developing exploration; variation (stochastic transitions, randomized starts) requires probabilistic-model-aware measures (e.g., KL divergence among ensemble predictions) rather than simple error-based bonuses.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment intrinsic exploration phase (1000 steps) to learn transition model using intrinsic rewards, followed by a task phase (3000 steps) where the learned model is used greedily with respect to an external reward; RTMBA real-time model-based architecture with parallel model-learning, planning and acting; no curriculum or domain randomization used.",
            "generalization_tested": true,
            "generalization_results": "Yes — after 1000 steps of reward-free exploration to learn a transition model, the learned model was used to perform a different task (reward of 10 for moving between rooms) for 3000 steps with greedy action selection; TEXPLORE-VANIR outperformed baselines in cumulative external reward (statistically significant, p &lt; 0.001), indicating the learned model generalized to enable task performance.",
            "sample_efficiency": "Exploration/model-learning phase: 1000 environment steps (agents acted at 2.5 actions/sec). Model accuracy was measured periodically (every 25 steps) using 5000 sampled state-actions; TEXPLORE-VANIR learned a more accurate model than baselines within the 1000-step exploration window (exact number of steps to convergence not numerically specified).",
            "key_findings": "TEXPLORE-VANIR's combination of variance-based intrinsic reward (ensemble KL divergence) and novelty-based intrinsic reward (L1 distance in normalized feature space to nearest trained state for same action) yields more accurate transition models within 1000 steps than several baselines (random action, competence-progress (R-IAC style), prediction-error, R-MAX style bonuses, tabular baselines). The learned models transfer to downstream task performance (3000-step room traversal) yielding higher cumulative reward than baselines. The paper emphasizes that optimal intrinsic motivation depends on both the model learning method and environment complexity/variation: competence-progress is suited to very high-dimensional slow-learning domains, R-MAX suits tabular models, while variance+novelty suits generalizing ensemble models in moderately complex stochastic domains. TEXPLORE-VANIR exhibits developing curiosity, progressing from easier interactions (PICKUP) to harder ones (PRESS on lock) rather than random or exhaustive exploration.",
            "uuid": "e1079.0",
            "source_info": {
                "paper_title": "Intrinsically motivated model learning for a developing curious agent",
                "publication_date_yy_mm": "2012-11"
            }
        },
        {
            "name_short": "Random Agent",
            "name_full": "Uniform Random Action Agent",
            "brief_description": "A baseline agent that selects actions uniformly at random; used as a control to show the benefit of intrinsic-motivation-driven exploration.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random Agent",
            "agent_description": "Selects one of the six actions uniformly at random at each timestep; learns no model (unless paired with a tabular model variant in experiments) and serves as a baseline for exploration behavior and naive performance.",
            "agent_type": "simulated agent",
            "environment_name": "Light World",
            "environment_description": "Same Light World domain as above; random agent experiences the same stochastic transitions and object interactions but does not direct exploration toward resolving uncertainty or novelty.",
            "complexity_measure": "Same domain measures (17 features, 6 actions, stochasticity). For random agent, complexity manifests as likelihood of accidentally discovering multi-step object interactions is low.",
            "complexity_level": "medium-high (same as main environment)",
            "variation_measure": "Experience variation arises only from environment stochasticity and randomized initial states; agent does not purposefully vary exploration.",
            "variation_level": "medium",
            "performance_metric": "Exploration behavior (counts of PRESS attempts on relevant objects), model baseline comparisons (when paired with tabular model), and cumulative reward in task phase.",
            "performance_value": "Performed substantially worse than TEXPLORE-VANIR: tried PRESS on arbitrary states more often than correctly on locks; when compared in task phase, cumulative reward was significantly lower (exact numeric totals not reported).",
            "complexity_variation_relationship": "Used as a baseline to illustrate that unguided exploration fails to focus on complex, conditional interactions (e.g., unlocking requires key) and is less sample-efficient in discovering multi-step behaviors under stochastic variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "No learning-driven exploration strategy; pure random action selection for 1000 steps in exploration phase; optionally paired with a tabular model variant in some baselines.",
            "generalization_tested": false,
            "generalization_results": "Not applicable; random agent does not learn a generalizing model and performs poorly when later tasked with the room traversal reward.",
            "sample_efficiency": "Very low sample efficiency for discovering conditional object interactions; qualitative plots show substantially fewer correct PRESS attempts on locks compared to TEXPLORE-VANIR over 1000 steps.",
            "key_findings": "Random exploration tends to call complex actions arbitrarily and fails to structure exploration from easy to hard skills; demonstrates the value of intrinsic-reward-driven exploration for sample efficiency and targeted skill discovery in stochastic, conditional domains.",
            "uuid": "e1079.1",
            "source_info": {
                "paper_title": "Intrinsically motivated model learning for a developing curious agent",
                "publication_date_yy_mm": "2012-11"
            }
        },
        {
            "name_short": "Competence-Progress (R-IAC style)",
            "name_full": "Competence Progress (inspired by R-IAC)",
            "brief_description": "An intrinsic-motivation baseline that splits state space into regions, tracks prediction error curves per region, and rewards regions where model error is improving most (i.e., competence progress).",
            "citation_title": "R-IAC: Robust Intrinsically Motivated Exploration and Active Learning",
            "mention_or_use": "use",
            "agent_name": "Competence-Progress Agent (R-IAC variant)",
            "agent_description": "A comparison agent implementing competence-progress intrinsic rewards: state space is partitioned into random regions; each region maintains an error curve and intrinsic reward equals slope (improvement) of prediction error; integrated with the same TEXPLORE model learning framework for fairness.",
            "agent_type": "simulated agent",
            "environment_name": "Light World",
            "environment_description": "Operates in the Light World domain. The method was originally intended for very large, high-dimensional continuous domains where learning is slow; here it is applied to the 17-dim Light World with relatively quick regional learning.",
            "complexity_measure": "Intended for very high-dimensional continuous state spaces where regional learning progress estimates require many samples; in Light World the domain is smaller so per-region sample budgets are low.",
            "complexity_level": "designed for high complexity domains; in the Light World test it is effectively mismatched (domain not large enough).",
            "variation_measure": "Variation is measured indirectly via regional prediction error curves; competence progress measures derivatives of error over time in each region.",
            "variation_level": "method suited to high variation across large continuous regions; in Light World variation is moderate causing delayed/ineffective progress signals.",
            "performance_metric": "Model accuracy (variational distance) after 1000 steps; downstream cumulative task reward after using learned model.",
            "performance_value": "Performed worse than TEXPLORE-VANIR and often poorly in Light World; competence-progress reward did not improve sample efficiency here because regions are learned quickly and progress signals arrive too late (exact numeric results not reported).",
            "complexity_variation_relationship": "Paper explicitly contrasts this method's suitability: competence-progress is recommended for large/high-dimensional domains where learning is slow; in lower-complexity/more rapidly-learned domains like Light World, competence-progress delays exploration of useful areas and underperforms.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Partitioned-region competence tracking (random regions at start) with intrinsic rewards proportional to competence progress; integrated with TEXPLORE-style model learning for experiments.",
            "generalization_tested": false,
            "generalization_results": "Not reported as effective in this domain; competence-progress did not lead to superior models or task performance in Light World given the available samples.",
            "sample_efficiency": "Low in this domain: by the time regional error derivatives signaled progress, the agent often already had sufficient data and didn't benefit from targeted exploration.",
            "key_findings": "Competence-progress intrinsic rewards (R-IAC style) are not universally effective; their utility depends on domain scale and learning speed — effective in very large/high-dimensional slow-learning domains, but ineffective in the comparatively smaller Light World where regions are learned rapidly.",
            "uuid": "e1079.2",
            "source_info": {
                "paper_title": "Intrinsically motivated model learning for a developing curious agent",
                "publication_date_yy_mm": "2012-11"
            }
        },
        {
            "name_short": "Prediction-Error Agent",
            "name_full": "Region Prediction-Error Intrinsic Reward Agent",
            "brief_description": "A baseline that rewards regions with high prediction error (rather than progress), intended to drive exploration to areas with currently-high model error.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Prediction-Error Agent",
            "agent_description": "Agent uses region-based partitioning like Competence-Progress, but intrinsic reward is proportional to current prediction error in region; integrated with TEXPLORE model learning for comparison.",
            "agent_type": "simulated agent",
            "environment_name": "Light World",
            "environment_description": "Same 17-dim Light World domain; prediction-error rewards aim to push agent into regions with high instantaneous model error.",
            "complexity_measure": "Similar domain complexity metrics (17 features, object interactions, stochastic transitions). Prediction-error method measures instantaneous model error in regions.",
            "complexity_level": "medium-high",
            "variation_measure": "Captures variation where model currently predicts poorly; depends on partitioning resolution (random regions) and sample coverage.",
            "variation_level": "medium",
            "performance_metric": "Model variational distance; downstream cumulative reward when model used for task.",
            "performance_value": "Performed poorly relative to TEXPLORE-VANIR; authors note that it may not visit the right state-actions within high-error regions, leading to worse model learning (no absolute numbers reported).",
            "complexity_variation_relationship": "Paper suggests that simply rewarding high prediction error can fail in structured domains because regions of high error may contain subregions with both informative and uninformative state-actions; without targeted signals (e.g., ensemble disagreement or novelty by distance), the agent may not resolve the right dynamics.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Region-based error reward with TEXPLORE model learning; no curriculum.",
            "generalization_tested": false,
            "generalization_results": "Not highlighted as providing useful generalization in the Light World experiments; model quality and downstream task performance were inferior to TEXPLORE-VANIR and R-MAX.",
            "sample_efficiency": "Lower sample efficiency compared to TEXPLORE-VANIR; failed to focus exploration on the most informative state-actions inside error regions.",
            "uuid": "e1079.3",
            "source_info": {
                "paper_title": "Intrinsically motivated model learning for a developing curious agent",
                "publication_date_yy_mm": "2012-11"
            }
        },
        {
            "name_short": "R-MAX",
            "name_full": "R-MAX",
            "brief_description": "A tabular model-based RL algorithm that gives optimistic intrinsic rewards (R_max) to state-actions visited fewer than m times, driving exhaustive visitation guarantees in finite MDPs.",
            "citation_title": "R-Max - a general polynomial time algorithm for near-optimal reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "R-MAX",
            "agent_description": "Tabular model-based RL algorithm that initializes unvisited state-actions with optimistic reward R_max until visited at least m times; used both in pure R-MAX form and as a baseline when combined with the TEXPLORE random-forest model.",
            "agent_type": "simulated agent",
            "environment_name": "Light World",
            "environment_description": "Applied to Light World domain. R-MAX encourages aggressive exploration to visit state-actions enough times to build an accurate tabular model.",
            "complexity_measure": "Tabular state-action visitation counts; in large or continuous state spaces tabular R-MAX is infeasible. In Light World, state space size is large but finite given discretized 17 features and room positions; R-MAX still used as baseline.",
            "complexity_level": "medium (sufficient for R-MAX baseline but high enough that exhaustive visitation is expensive)",
            "variation_measure": "Variation handled by visit counts per state-action; stochastic transitions mean multiple visits required to estimate transition distributions.",
            "variation_level": "medium",
            "performance_metric": "Model variational distance after exploration phase; cumulative external reward during task exploitation; combined intrinsic+external reward experiments.",
            "performance_value": "R-MAX was the next-best method after TEXPLORE-VANIR in model accuracy and task reward when exploration and exploitation were separated into phases. However, when intrinsic and external rewards were combined, R-MAX's aggressive exploration cost it external reward and it underperformed relative to TEXPLORE-VANIR (exact numeric values not reported).",
            "complexity_variation_relationship": "Paper argues that R-MAX style bonuses make sense for tabular models in which exhaustive visitation is needed to learn transitions, but are less suited to generalizing models (like random forests) and to settings where exploration-exploitation must be balanced continuously.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Tabular optimistic initialization with visit-count threshold m (R-MAX); also tested R-MAX combined with TEXPLORE model (performed poorly).",
            "generalization_tested": false,
            "generalization_results": "R-MAX produced reasonably accurate tabular models in the Light World given the experimental budget and transferred to task performance when exploration/exploitation were separated, but was less effective when exploration bonuses interfered with obtaining external reward during combined training.",
            "sample_efficiency": "Less sample-efficient than TEXPLORE-VANIR in this domain when using a generalizing model; needed more visits per state-action to reach similar model accuracy in tabular formulation (exact counts not given).",
            "key_findings": "R-MAX is appropriate for tabular models requiring exhaustive visitation but is not an ideal intrinsic bonus for generalizing models; aggressive optimistic exploration can harm combined intrinsic+external reward learning.",
            "uuid": "e1079.4",
            "source_info": {
                "paper_title": "Intrinsically motivated model learning for a developing curious agent",
                "publication_date_yy_mm": "2012-11"
            }
        },
        {
            "name_short": "Tabular Random Agent",
            "name_full": "Random Action with Tabular Model Baseline",
            "brief_description": "Random action selection paired with a tabular model that assumes self-transitions for unvisited state-actions; used to contrast tabular versus generalizing model strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random (Tabular Model)",
            "agent_description": "Acts randomly but maintains a tabular model initialized to predict self-transitions for unvisited state-actions; used to show effects of naive tabular priors compared to intrinsic-motivation-driven exploration with generalizing models.",
            "agent_type": "simulated agent",
            "environment_name": "Light World",
            "environment_description": "Same Light World domain; tabular prior biases model predictions for unvisited state-actions and affects downstream task exploitation.",
            "complexity_measure": "Tabular state-action count relative to 17-dim state representation; many unvisited state-actions given finite sample budget.",
            "complexity_level": "medium-high; tabular representation struggles due to large effective state-action space.",
            "variation_measure": "Variation is handled only indirectly via stochastic observed transitions; unvisited state-actions default to self-transition predictions.",
            "variation_level": "medium",
            "performance_metric": "Model accuracy and downstream cumulative task reward when acting greedily on the tabular model.",
            "performance_value": "Performed worse than TEXPLORE-VANIR and R-MAX; acting randomly with a tabular prior did not lead to competitive task performance (exact numbers not provided).",
            "complexity_variation_relationship": "Demonstrates that naive tabular priors plus random exploration do not effectively handle the complexity and variation present in Light World, motivating generalizing models and intrinsic rewards.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Random action selection for exploration while collecting tabular counts and transitions; no intrinsic reward guiding exploration.",
            "generalization_tested": false,
            "generalization_results": "Tabular model with random exploration did not generalize effectively to the downstream room traversal task compared to TEXPLORE-VANIR.",
            "sample_efficiency": "Low; many state-actions remain unvisited and tabular self-transition priors bias model away from true dynamics until many visits are collected.",
            "uuid": "e1079.5",
            "source_info": {
                "paper_title": "Intrinsically motivated model learning for a developing curious agent",
                "publication_date_yy_mm": "2012-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "R-IAC: Robust Intrinsically Motivated Exploration and Active Learning",
            "rating": 2
        },
        {
            "paper_title": "R-Max - a general polynomial time algorithm for near-optimal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Real time targeted exploration in large domains",
            "rating": 2
        },
        {
            "paper_title": "RTMBA: A real-time modelbased reinforcement learning architecture for robot control",
            "rating": 1
        },
        {
            "paper_title": "Random forests",
            "rating": 1
        },
        {
            "paper_title": "Building portable options: Skill transfer in reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.015169249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Intrinsically Motivated Model Learning for a Developing Curious Agent</h1>
<p>Todd Hester and Peter Stone<br>Department of Computer Science<br>The University of Texas at Austin<br>{todd, pstone}@cs.utexas.edu</p>
<h4>Abstract</h4>
<p>Reinforcement Learning (RL) agents are typically deployed to learn a specific, concrete task based on a pre-defined reward function. However, in some cases an agent may be able to gain experience in the domain prior to being given a task. In such cases, intrinsic motivation can be used to enable the agent to learn a useful model of the environment that is likely to help it learn its eventual tasks more efficiently. This paper presents the TEXPLORE with Variance-And-Novelty-IntrinsicRewards algorithm (TEXPLORE-VANIR), an intrinsically motivated model-based RL algorithm. The algorithm learns models of the transition dynamics of a domain using random forests. It calculates two different intrinsic motivations from this model: one to explore where the model is uncertain, and one to acquire novel experiences that the model has not yet been trained on. This paper presents experiments demonstrating that the combination of these two intrinsic rewards enables the algorithm to learn an accurate model of a domain with no external rewards and that the learned model can be used afterward to perform tasks in the domain. While learning the model, the agent explores the domain in a developing and curious way, progressively learning more complex skills. In addition, the experiments show that combining the agent's intrinsic rewards with external task rewards enables the agent to learn faster than using external rewards alone.</p>
<h2>I. INTRODUCTION</h2>
<p>Reinforcement Learning (RL) agents could be useful in society because of their ability to learn and adapt to new environments and tasks. Traditionally, RL agents learn to accomplish a specific, concrete task based on a pre-defined reward function. However, in some cases an agent may be able to gain experience in the domain prior to being given this task. For example, a future domestic robot may be placed in a home and only later given various tasks to accomplish. In such cases, intrinsic motivation can be used to enable the agent to learn a useful model of the environment that can help it learn its eventual tasks more efficiently.</p>
<p>Past work on intrinsically motivated agents arises from two different goals [1]. The first goal comes from the active learning community, which uses intrinsic motivation to improve the sample efficiency of RL. Their goal is to help the agent to maximize its knowledge about the world and its ability to control it. The second goal comes from the developmental learning community, and is to enable cumulative, open-ended learning on robots. Our goal is to use intrinsic motivation towards both goals: 1) to improve the sample efficiency of learning, particularly in tasks with little or no external rewards; and 2) to enable the agent to perform open-ended learning without external rewards.</p>
<p>This paper presents an intrinsically motivated modelbased RL algorithm, called TEXPLORE with Variance-And-Novelty-Intrinsic-Rewards (TEXPLORE-VANIR), that uses intrinsic motivation both for improved sample efficiency and to give the agent a curiosity drive. The agent is based on a model-based RL framework and is motivated to learn models of domains without external rewards as efficiently as possible. TEXPLORE-VANIR combines model learning through the use of random forests with two unique intrinsic rewards calculated from this model. The first reward is based on variance in its models' predictions to drive the agent to explore where its model is uncertain. The second reward drives the agent to novel states which are the most different from what its models have been trained on. The combination of these two rewards enables the agent to explore in a developing curious way, learn progressively more complex skills, and learn a useful model of the domain very efficiently.</p>
<p>This paper presents two main contributions:</p>
<p>1) Novel methods for obtaining intrinsic rewards from a random-forest-based model of the world.
2) The TEXPLORE-VANIR algorithm for intrinsically motivated model learning, which has been released opensource as a ROS package: http://www.ros.org/ wiki/rl-texplore-ros-pkg.
Section IV presents experiments showing that TEXPLOREVANIR: 1) learns a model more efficiently than other methods; 2) explores in a developing, curious way; and 3) can use its learned model later to perform tasks specified by a reward function. In addition, it shows that the agent can use the intrinsic rewards in conjunction with external rewards to learn a task faster than if using external rewards alone.</p>
<h2>II. BACKGROUND</h2>
<p>This section presents background on Reinforcement Learning (RL). We adopt the standard Markov Decision Process (MDP) formalism for this work [2]. An MDP is defined by a tuple $\langle S, A, R, T\rangle$, which consists of a set of states $S$, a set of actions $A$, a reward function $R(s, a)$, and a transition function $T\left(s, a, s^{\prime}\right)=P\left(s^{\prime} \mid s, a\right)$. In each state $s \in$ $S$, the agent takes an action $a \in A$. Upon taking this action, the agent receives a reward $R(s, a)$ and reaches a new state $s^{\prime}$, determined from the probability distribution $P\left(s^{\prime} \mid s, a\right)$. Many domains utilize a factored state representation, where the state $s$ is represented by a vector of $n$ state variables: $s=\left\langle x_{1}, x_{2}, \ldots, x_{n}\right\rangle$. A policy $\pi$ specifies for each state which action the agent will take.</p>
<p>The goal of the agent is to find the policy $\pi$ mapping states to actions that maximizes the expected discounted total reward over the agent’s lifetime. The value $Q^{<em>}(s, a)$ of a given state-action pair $(s, a)$ is an estimate of the expected future reward that can be obtained from $(s, a)$ when following policy $\pi$. The optimal value function $Q^{</em>}(s, a)$ provides maximal values in all states and is determined by solving the Bellman equation:</p>
<p>$Q^{<em>}(s, a)=R(s, a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)\max_{a^{\prime}}Q^{</em>}(s^{\prime},a^{\prime}),$ (1)</p>
<p>where $0&lt;\gamma&lt;1$ is the discount factor. The optimal policy $\pi$ is then:</p>
<p>$\pi(s)=\operatorname{argmax}_{a}Q^{*}(s, a).$ (2)</p>
<p>RL methods fall into two general classes: model-based and model-free methods. Model-based RL methods learn a model of the domain by approximating $R(s, a)$ and $P(s^{\prime}|s,a)$ for each state and action. The agent can then calculate a policy (i.e. plan) using this model. Model-free methods update the values of actions only when taking them in the real task. One of the advantages of model-based methods is their ability to plan multi-step exploration trajectories. The agent can plan a policy to reach intrinsic rewards added into its model to drive exploration to interesting state-actions.</p>
<p>This work takes the approach of using a model-based RL algorithm in a domain with no external rewards. This approach can be thought of as a pure exploration problem, where the agent’s goal is simply to learn as much about the world as possible. TEXPLORE-VANIR extends a modelbased RL algorithm called TEXPLORE [3] to use intrinsic motivation to quickly learn an accurate model in domains with no external rewards.</p>
<h2>III. TEXPLORE-VANIR</h2>
<p>Our goal is to develop an intrinsically motivated curious agent using RL. This agent should use intrinsic rewards to 1) efficiently learn a useful model of the domain’s transition dynamics; and 2) explore in a developing curious way. To this end, we have the following desiderata for such an algorithm:</p>
<p>1) The algorithm should be model-based, both to enable multi-step exploration trajectories and to allow the agent to use the learned model later to perform tasks.
2) It should incorporate generalization into its model learning so as to learn the model quickly.
3) It should not be required to visit every state-action, because doing so is intractable in large domains.</p>
<p>This paper presents the TEXPLORE-VANIR algorithm, which has all of these properties. TEXPLORE-VANIR follows the typical approach of a model-based RL agent. It plans a policy using its learned model (including intrinsic rewards), takes actions following that policy, acquiring new experiences which are used to improve its model, and repeats. In order to be applicable to robots, TEXPLORE-VANIR uses the Real-Time Model Based Architecture [4]. This architecture uses approximate planning with UCT [5] and parallelizes the model learning, planning, and acting such that the agent can take actions in real-time at a specified frequency.</p>
<h2>A. Model Learning</h2>
<p>Making the intrinsically motivated agent model-based enables it to: 1) plan multi-step exploration trajectories; 2) learn faster than model-free approaches; and 3) use the learned model to solve tasks given to it after its learning. It is desirable for the model to generalize the learned transition and reward dynamics across state-actions. This generalization enables the model to make predictions about unseen or infrequently visited state-actions, and therefore the agent does not have to visit every state-action. Thus, TEXPLOREVANIR approaches the model learning task as a supervised learning problem, with the current state and action as the input, and the next state as the output to be predicted.</p>
<p>TEXPLORE-VANIR is built upon the TEXPLORE algorithm [3], which uses random forests to learn separate predictions of each of the $n$ state features in the domain. A random forest [6] is a collection of $m$ decision trees, each of which differs because it is trained on a random subset of experiences and has some randomness when choosing splits at the decision nodes. The agent then plans over the average of the predictions made by each tree in the forest. The decision trees work well because they generalize broadly at first, but can be refined with training to make accurate predictions for individual state-actions. Each tree in the forest represents a different hypothesis of the true model of the domain. The variance of the different trees’ predictions can be used as a measure of the uncertainty in the model.</p>
<h2>B. Intrinsic Motivation</h2>
<p>The main contribution of this paper is a method for extending the model-based TEXPLORE algorithm for learning specific RL tasks to the intrinsically motivated TEXPLOREVANIR algorithm. The best intrinsic rewards to use to improve the efficiency of model-learning are highly dependent on the type of model being learned. With the random forest models TEXPLORE uses, we hypothesize that the following two intrinsic motivations will perform the best: 1) preferring to explore areas of the state space where there is a large degree of uncertainty in the model, and 2) preferring regions of the state space that are far from previously explored areas (regardless of how certain the model is).</p>
<p>The variance of the predictions of each of the trees in the forest can be used to motivate the agent towards the state-actions where its models disagree. These state-actions are the ones where there are still multiple hypotheses of the true model of the domain. TEXPLORE-VANIR calculates a measure of the variance in the predictions of each state feature for a given state-action:</p>
<p>$D(s,a)=\sum_{i=1}^{n}\sum_{j=1}^{m}\sum_{k=1}^{m}D_{K L}(P_{j}(x_{i}|s,a)||P_{k}(x_{i}|s,a)),$ (3)</p>
<p>where for every pair of models ($j$ and $k$) in the forest, it sums the KL-divergences between the predicted probability distributions for each feature $i$. $D(s,a)$ measures how much the predictions of the different models disagree. This measure is different than just measuring where the predictions are</p>
<p>noisy, as $D(s, a)$ will be 0 if all the tree models predict the same stochastic outcome distribution. An intrinsic reward proportional to this variance measure, the VARIANCE-REWARD, is incorporated into the agent’s model for planning:</p>
<p>$R(s, a)=v D(s, a),$ (4)</p>
<p>where $v$ is a coefficient determining how big this reward should be.</p>
<p>This reward will drive the agent to the state-actions where its models have not yet converged to a single hypothesis of the world’s dynamics. However, there will still be cases where all of the agent’s models make incorrect predictions. For the random forest model that TEXPLORE-VANIR uses, the model is more likely to be incorrect when it has to generalize its predictions farther from the experiences it is trained on. Therefore, TEXPLORE-VANIR uses a second intrinsic reward based on the $L_{1}$ distance in feature space from a given state-action and the nearest one that the model has been trained on. This distance is calculated separately for each action. For an action $a$, $X_{a}$ is the set of all the states where this action was taken. Then, $\delta(s,a)$ is the $L_{1}$ distance from the given state $s$ to the nearest state where action $a$ has been taken:</p>
<p>$\delta(s,a)=\min_{s_{x}\in X_{a}}\left|s-s_{x}\right|_{1},$ (5)</p>
<p>where each feature is normalized to range from 0 to 1. A reward proportional to this distance, the NOVELTY-REWARD, drives the agent to explore the state-actions that are the most novel compared to the previously seen state-actions:</p>
<p>$R(s,a)=n\delta(s,a),$ (6)</p>
<p>where $n$ is a coefficient determining how big this reward should be. One nice property of this reward is that given enough time, it will drive the agent to explore <em>all</em> the state-actions in the domain, as any unvisited state-action is different in some feature from the visited ones. However, it will start out driving the agent to explore the state-actions that are the most different from ones it has seen.</p>
<p>The TEXPLORE with Variance-And-Novelty-Intrinsic-Rewards algorithm (TEXPLORE-VANIR) is completed by combining these two intrinsic rewards. They can be combined with different weightings of their coefficients ($v$ and $n$), or with an external reward defining a task. A combination of the two intrinsic rewards should drive the agent to learn a model more efficiently, as well as explore in a developing and curious way: seeking out novel and interesting state-actions, while exploring increasingly complex parts of the domain.</p>
<h2>IV. EMPIRICAL RESULTS</h2>
<p>Evaluating the benefits of intrinsic motivation is not as straightforward as evaluating a standard RL agent on a specific task. Rather than attempting to accrue reward on a given task, a curious agent’s goal is better stated as preparing itself for any task. We therefore evaluate TEXPLORE-VANIR in four ways on a complex domain with no external rewards. First, we measure the accuracy of the agent’s learned model in predicting the domain’s transition dynamics. Second, we test whether the learned model can be used to perform tasks in the domain when given a reward function. Third, we examine the agent’s exploration to see if it is exploring in a developing, curious way. Finally, we demonstrate that TEXPLORE-VANIR can combine its intrinsic rewards with external rewards to learn faster than if it was given only external rewards. These results demonstrate that the intrinsic rewards and model learning approach TEXPLORE-VANIR uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</p>
<p>The agent is tested on the Light World domain [7], shown in Figure 1. In this domain, the agent goes through a series of rooms. Each room has a door, a lock, and possibly a key. The agent must go to the lock and press it to open the door, at which point it can then leave the room. It cannot go back through the door in the opposite direction. If a key is present, it must pickup the key before pressing the lock. Open doors, locks, and keys each emit a different color light that the agent can see. The agent has sensors that detect each color light in each cardinal direction. The agent’s state is made up of 17 different features: its X and Y location in the room, the ID of the room it is in, whether it has the KEY, whether the door is LOCKED, as well as the values of the 12 light sensors, which detect each of the three color lights in the four cardinal directions. The agent can take six possible actions: it can move in each of the four cardinal directions, PRESS the lock, or PICKUP the key. The first four actions are stochastic; they move the agent in the intended direction with probability 0.9 and to either side with probability 0.05 each. The PRESS and PICKUP actions are only effective when the agent is on top of the lock and the key, respectively, and then only with probability 0.9. The agent starts in a random state in the top left room in the domain, and can proceed through the rooms indefinitely.</p>
<p>This domain is well-suited for this task because the domain has a rich feature space and complex dynamics. There are simple actions that move the agent, as well as more complex</p>
<p>actions (PICKUP and PRESS) that interact with objects in different ways. There is a progression of the complexity of the uses of these two actions. Picking up the key is easier than pressing the lock, as the lock requires the agent to have already picked up the key and not yet unlocked the door.</p>
<p>Based on informal testing, we set TEXPLORE-VANIR’s parameters to $v=1$ and $n=3$. TEXPLORE-VANIR is tested against the following agents:</p>
<ol>
<li>Agent which selects actions randomly</li>
<li>Agent which is given an intrinsic motivation for regions with more competence progress (based on RIAC [8])</li>
<li>Agent which is given an intrinsic motivation for regions with more prediction errors</li>
<li>Agent which uses R-MAX style rewards (terminal reward of $R_{\text {max }}$ for state-actions with fewer than $m$ visits)</li>
<li>Agent which acts randomly with a tabular model</li>
<li>R-MAX algorithm [9]</li>
</ol>
<p>These six algorithms provide four different ways to explore using TEXPLORE-VANIR’s random forest model, as well two approaches using a tabular model. The tabular model is initialized to predict self-transitions for state-actions that have not been visited.</p>
<p>One of the more well-known intrinsic motivation algorithms is Robust Intelligent Adaptive Curiosity (R-IAC) [8]. R-IAC does not adopt the RL framework, but is similar in many respects. R-IAC splits the state space into regions and learns a model of the transition dynamics in each region. It maintains an error curve for each region and uses the slope of this curve as the intrinsic reward for the agent, driving the agent to explore the areas where its model is improving the most (rewarding competence progress). This approach is intended for very large multi-dimensional continuous domains where learning may take many thousands of steps. We have created a method based on this idea to compare with our approach (the Competence Progress method). This method splits the state space into random regions at the start, maintains error curves in each region, and provides intrinsic rewards based on competence progress within a region. These intrinsic rewards are combined with the same TEXPLORE model learning approach as the other methods. As another comparison, the Prediction Error method uses the same regions, but rewards areas with high prediction error.</p>
<p>All the algorithms are run in the Light World domain for 1000 steps without any external reward. During this phase, the agent is free to play and explore in the domain, all the while learning a model of the dynamics of this world. All of the algorithms use the RTMBA parallel architecture [4] and take 2.5 actions per second.</p>
<p>First, we examine the accuracy of the agent’s learned model. After every 25 steps, 5000 state-actions from the domain are randomly sampled and the variational distance between the model’s predicted next state probabilities are compared with the true next state probabilities. Figure 2 shows the variational distance between these distributions, averaged over the 5000 sampled state-actions. This figure</p>
<p>LightWorld Model Accuracy
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2. Accuracy of each algorithm’s model plotted versus number of steps the agent has taken, averaged over 30 trials and 5000 randomly sampled state-actions. TEXPLORE-VANIR learns the most accurate models.
shows that TEXPLORE-VANIR learns significantly more accurate models than the other methods ( $p&lt;0.025$ ). The next best algorithm is R-MAX. However, using R-MAX style reward with the TEXPLORE model strategy is worse than acting randomly. This result illustrates our point that the best intrinsic reward is dependent on the particular model learning approach that is used. The method rewarding visiting regions with high prediction error performs poorly, possibly because it is not visiting the right state-actions within these regions.</p>
<p>While TEXPLORE-VANIR and R-MAX appear to learn fairly accurate models, it is more important for the algorithms to be accurate in the interesting and useful parts of the domain than for them to be accurate about every state-action. Therefore, we next test if the learned models are useful to perform a task. After the algorithms learned models without rewards for 1000 steps, they are provided with a reward function for a task. The task is for the agent to continue moving through the rooms (requiring it to use the keys and locks). The reward function is a reward of 10 for moving from one room to the next, and a reward of 0 for all other actions. In this second phase, the agents act greedily with respect to their previously learned transition models and the given external reward function with no intrinsic rewards for 3000 steps.</p>
<p>Figure 3 shows the cumulative external reward received by each algorithm over the 3000 steps of the task. Again, TEXPLORE-VANIR performs the best, slightly out-performing R-MAX and significantly out-performing the other methods ( $p&lt;0.001$ ). Learning an accurate transition model appears to lead to good performance on the task, as both TEXPLOREVANIR and R-MAX perform well on the task.</p>
<p>Next, the exploration of the TEXPLORE-VANIR agent is examined. In addition to learning an accurate and useful model, we desire the agent to exhibit a developing curiosity. Precisely, the agent should progressively learn more complex skills in the domain, rather than explore randomly or exhaustively. Figures 4(a) and 4(b) show the cumulative number of times that TEXPLORE-VANIR and the random agent select the PRESS action in various states over 1000 steps in the task with no external rewards, averaged over 30 trials. Comparing</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 4. This plot shows the cumulative number of times that TEXPLORE-VANIR and a Random Agent select the PRESS action in various states over 1000 steps in the task with no external rewards, averaged over 30 trials. Note that the random agent attempts the PRESS action much less than TEXPLORE-VANIR does. TEXPLORE-VANIR starts out trying to PRESS the key, which is the easiest object to find, and eventually does learn to press the lock, but has difficulty learning when to press the lock (it must be with the key but without the door already being open). The agent does not try calling the PRESS action on random states very often. In contrast, the random agent calls PRESS action on random states more often than it calls it correctly on the lock.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Cumulative rewards received by each algorithm over the 3000 steps of the task, averaged over 30 trials. Agents act greedily with respect to their previously learned transition model and the given external reward function. TEXPLORE-VANIR receives the most reward.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Cumulative rewards received by each algorithm, using intrinsic and external rewards combined, over the 3000 steps of the task, averaged over 30 trials. TEXPLORE-VANIR receives the most reward, while the agent using only external rewards performs very poorly.</p>
<p>The two figures show that TEXPLORE-VANIR calls the PRESS action many more times than the random agent. Figure 4(a) also shows that TEXPLORE-VANIR tries PRESS on objects more often than on random states in the domain. In contrast, Figure 4(b) shows that the random agent tries PRESS on arbitrary states more often than it uses it correctly.</p>
<p>Analyzing the exploration of TEXPLORE-VANIR further, Figure 4(a) shows that it initially tries PRESS on the key, which is the easiest object to access, then tries it on the lock, and then on the door. The figure also shows that TEXPLORE-VANIR takes longer to learn the correct dynamics of the lock, as it continues to PRESS the lock incorrectly, either without the key or with the door already unlocked. These plots show that TEXPLORE-VANIR is acting in an intelligent, curious way, trying actions on the objects in order from the easiest to hardest to access, and going back to the lock repeatedly to learn its more complex dynamics.</p>
<p>Finally, not only should the agent's intrinsic rewards be useful when learning in task without external rewards, they should also make an agent in a domain with external rewards learn more efficiently. For this experiment, the algorithms are run for 3000 steps with their intrinsic rewards added to the previously used external reward function that rewards moving between rooms. Instead of an agent acting randomly, we instead have one agent acting using only the external rewards, and one performing Boltzmann, or soft-max, exploration with temperature τ = 0.2. Figure 5 shows the cumulative external reward received by each agent over the 3000 steps of the task. TEXPLORE-VANIR receives significantly more reward than the other algorithms (p &lt; 0.001), followed by R-MAX. Now that exploration and exploitation are no longer separated into separate phases, the exploration of R-MAX is too aggressive and costs it external reward.</p>
<p>These results show that TEXPLORE-VANIR's intrinsic rewards out-perform other exploration approaches and intrinsic motivations combined with the TEXPLORE model. TEXPLORE-VANIR performs similarly to R-MAX when exploration and exploitation are split into separate phases, but out</p>
<p>performs R-MAX significantly when combining intrinsic and external rewards together. TEXPLORE-VANIR explores the domain in a curious manner progressing from state-actions with easier dynamics to those that are more difficult. Finally, in a task with external rewards, TEXPLORE-VANIR can use its intrinsic rewards to speed up learning with respect to an algorithm using only external rewards.</p>
<p>It is important to note that the best intrinsic rewards are dependent on the learning algorithm and the domain. For example, the competence progress rewards used by R-IAC are intended to be used in complex high-dimensional domains where learning is slow. It takes quite a few samples in one region to get an reasonable estimate of the derivative of the error. In the Light World domain, by the time the algorithm has determined error is improving in a region, the agent has already learned a model of that region and no longer needs to explore there. When using other model learning methods, the best intrinsic reward will vary as well, for example, the R-MAX reward works well for a tabular model, but not for a random forest model.</p>
<h2>V. Related Work</h2>
<p>Many model-based RL algorithms use "exploration bonus" intrinsic rewards to drive the agent to explore more efficiently. As one example, R-MAX [9] uses intrinsic rewards to guarantee that it will learn the optimal policy within a bounded number of steps. The algorithm learns a maximumlikelihood tabular model of the task and provides intrinsic rewards to state-actions that have been visited less than $m$ times. These rewards drive the agent to visit each state-action enough times to learn an accurate model.</p>
<p>A few methods provide intrinsic rewards to an agent to drive it to where its model is improving the most. For example, R-IAC [8] rewards regions where the model error is improving the most. An alternative is to learn a separate predictor of the change in model error and use its predicted values as the intrinsic reward to drive exploration [10].</p>
<p>Simsek and Barto [11] present an approach for the pure exploration problem, where there is no concern with receiving external rewards. They provide a Q-LEARNING agent [12] with intrinsic rewards for where its value function is most improving. This reward speeds up the agent's learning of the true task. However, such a reward is not necessary for model-based agents, which perform value function updates by planning on their model. This algorithm requires an external reward, as the intrinsic reward is speeding up the learning of the task defined by the external reward function.</p>
<p>Singh et al. [13] argue that in nature, intrinsic rewards come from evolution and exist to help us perform any task. Agents using intrinsic rewards combined with external rewards should perform better than those using solely external rewards. For two different algorithms and tasks, they search over a broad set of possible task and agent specific intrinsic rewards and find rewards that make the agent learn faster than if it solely used external rewards.</p>
<p>These different approaches demonstrate that the correct intrinsic motivation is dependent on the type of algorithm.</p>
<p>For example, with a Q-LEARNING agent [12], it makes sense to give intrinsic rewards for where the value backups will have the largest effect, as done in [11]. When learning with a tabular model, the agent must gain enough experiences in each state-action to learn an accurate model of it. Thus it makes sense to use intrinsic motivation to drive the agent to acquire these experiences, as done by R-MAX [9]. With a model learning approach that generalizes as TEXPLOREVANIR's does, the best intrinsic rewards are different again.</p>
<h2>VI. CONCLUSION</h2>
<p>This paper presents the TEXPLORE-VANIR algorithm for intrinsically motivated learning, available at http://www. ros.org/wiki/rl-texplore-ros-pkg. This algorithm combines random forest based model learning with two novel intrinsic rewards. One reward drives the agent to where the model is uncertain in its predictions, and the second drives the agent to acquire novel experiences that its model has not been trained on. Experiments show empirically that TEXPLORE-VANIR can learn accurate and useful models in a domain with no external rewards. In addition, TEXPLOREVANIR's intrinsic rewards drive the agent to learn in a developing and curious way, progressing from learning easier to more difficult skills. TEXPLORE-VANIR can also combine its intrinsic rewards with external task rewards to learn a task faster than using external rewards alone. One goal for future work is to extend TEXPLORE-VANIR to work in large continuous state spaces, so that it can apply to some robotic tasks.</p>
<h2>REFERENCES</h2>
<p>[1] M. Lopes and P.-Y. Oudeyer, "Guest editorial: Active learning and intrinsically motivated exploration in robots: Advances and challenges," IEEE Transactions on Autonomous Mental Development (TAMD), vol. 2, no. 2, pp. 65-69, 2010.
[2] R. Sutton and A. Barto, Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press, 1998.
[3] T. Hester and P. Stone, "Real time targeted exploration in large domains," in International Conference on Development and Learning (ICDL), August 2010.
[4] T. Hester, M. Quinlan, and P. Stone, "RTMBA: A real-time modelbased reinforcement learning architecture for robot control," in IEEE International Conference on Robotics and Automation (ICRA), 2012.
[5] L. Kocsis and C. Szepesvári, "Bandit based Monte-Carlo planning," in European Conference on Machine Learning (ECML), 2006.
[6] L. Breiman, "Random forests," Machine Learning, vol. 45, no. 1, pp. $5-32,2001$.
[7] G. Konidaris and A. G. Barto, "Building portable options: Skill transfer in reinforcement learning," in International Joint Conference on Artificial Intelligence (IJCAI), 2007.
[8] A. Baranes and P. Y. Oudeyer, "R-IAC: Robust Intrinsically Motivated Exploration and Active Learning," IEEE Transactions on Autonomous Mental Development (TAMD), vol. 1, no. 3, pp. 155-169, Oct. 2009.
[9] R. Brafman and M. Tennenholtz, "R-Max - a general polynomial time algorithm for near-optimal reinforcement learning," in International Joint Conference on Artificial Intelligence (IJCAI), 2001.
[10] J. Schmidhuber, "Curious model-building control systems," in International Joint Conference on Neural Networks. IEEE, 1991.
[11] O. Şimşek and A. G. Barto, "An intrinsic reward mechanism for efficient exploration," in ICML, 2006, pp. 833-840.
[12] C. Watkins, "Learning from delayed rewards," Ph.D. dissertation, University of Cambridge, 1989.
[13] S. P. Singh, R. L. Lewis, A. G. Barto, and J. Sorg, "Intrinsically motivated reinforcement learning: An evolutionary perspective," IEEE Transactions on Autonomous Mental Development (TAMD), vol. 2, no. 2, pp. 70-82, 2010.</p>            </div>
        </div>

    </div>
</body>
</html>
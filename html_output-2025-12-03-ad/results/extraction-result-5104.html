<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5104 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5104</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5104</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-67590dc371a89bef960b7bd547110f43cbe7196e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/67590dc371a89bef960b7bd547110f43cbe7196e" target="_blank">APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes APOLLO, a simple adaptive Pretraining approach to improve the logical reasoning skills of language models using a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words.</p>
                <p><strong>Paper Abstract:</strong> Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5104.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5104.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APOLLO (RoBERTa-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APOLLO: Adaptive Pretraining of Language Models for Logical Reasoning (applied to RoBERTa-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>APOLLO is a continual pretraining approach that selects Wikipedia sentences containing logical implication keywords (the IMPLICATION dataset) and trains models with selective masked language modeling (s-MLM) over specific POS tags plus an entailment/contradiction sentence classification loss (E-CLS) to improve logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained transformer-based masked language model (RoBERTa) used as the base checkpoint; APOLLO continues pretraining the top transformer layers of RoBERTa-Large using the IMPLICATION corpus with s-MLM and E-CLS objectives before fine-tuning on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice machine reading comprehension datasets focused on logical reasoning: ReClor (graduate admission logical reasoning questions; split into EASY/HARD subsets) and LogiQA (exam-derived logical reading comprehension). Tasks require semantic understanding and higher-order logical inference to select the correct option.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Continual pretraining on an IMPLICATION subset of Wikipedia filtered by logical implication keywords; selective MLM (s-MLM) that masks tokens only from POS tags likely to require higher-order reasoning (ADJ, ADV, CONJ, CCONJ, PART, SCONJ, VERB); an auxiliary entailment vs contradiction sentence classification loss (E-CLS) bootstrapped by keywords; then fine-tune on target datasets. Typically only top transformer layers (two) were trained during continued pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor: Dev 67.2, Test 58.2, Test-E 76.8, Test-H 43.6 (accuracy). LogiQA: Dev 41.6, Test 42.1 (accuracy). These outperform the RoBERTa baseline and compare favorably to many prior methods (see comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>IMPLICATION corpus selection by keywords trades completeness for noise (contains redundant/non-logical sentences); POS-tag-based selective masking depends on availability and quality of POS taggers (limits cross-lingual application); APOLLO with RoBERTa performed lower on ReClor test than some baselines (e.g., MERIt on RoBERTa reported higher on some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to RoBERTa baseline APOLLO improves both tasks; against prior methods (LRReasoner, DAGN, FOCAL REASONER, MERIt) APOLLO (on RoBERTa-Large) outperforms on LogiQA and is comparable but not best on ReClor (some baselines report higher ReClor test performance). The approach is simpler and task-agnostic compared to methods relying on graph structures or contrastive counterfactual augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations (RoBERTa base): Using IMPLICATION vs RANDOM subset improves performance; s-MLM > MLM; adding E-CLS on top of s-MLM gives further gains (IMPLICATION + s-MLM + E-CLS yields ReClor Dev 67.2 / LogiQA Dev 41.6). POS ablation: base POS set performs best; adding nouns degrades performance (predicting nouns requires world knowledge); random masking (equivalent to standard MLM) is worse. Keyword ablation: IMPLICATION-Positive and -Negative individually help, combined gives best results. Best to continue-train top 2 transformer layers; training more lower layers can degrade performance until full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5104.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5104.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APOLLO (DeBERTa-v3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APOLLO: Adaptive Pretraining applied to DeBERTa-v3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same APOLLO continual pretraining pipeline (IMPLICATION dataset, s-MLM, E-CLS) applied to DeBERTa-v3 base checkpoint to improve logical reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-v3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Microsoft DeBERTa-v3 transformer architecture (pretrained) used as the base model for continued APOLLO pretraining; APOLLO fine-tunes the top layers with s-MLM and E-CLS on IMPLICATION before task fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same MRC logical reasoning benchmarks (ReClor, LogiQA) that require inference over text and discriminating entailment/contradiction and higher-order logical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Continual pretraining on IMPLICATION with s-MLM (POS-selective masking) and E-CLS, then fine-tune on target datasets; experiments use top layer adaptation similar to RoBERTa experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor: Dev 76.8, Test 72.8, Test-E 81.8, Test-H 65.7 (accuracy). LogiQA: Dev 48.4, Test 44.4 (accuracy). APOLLO (DeBERTa-v3) outperforms the base DeBERTa-v3 checkpoint on both benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same dataset-noise and POS-dependency limitations apply; although APOLLO improves DeBERTa-v3, absolute gains vary by dataset and may be smaller on some splits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>APOLLO on DeBERTa-v3 outperforms baseline DeBERTa-v3 and other baselines reported in the paper for both ReClor and LogiQA (per Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablation trends observed with RoBERTa hold: IMPLICATION corpus and s-MLM+E-CLS effective; best performance when adapting top layers (2) rather than all layers in most compute-efficient setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5104.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5104.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APOLLO (DeBERTa-v2-xxlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APOLLO: Adaptive Pretraining applied to DeBERTa-v2-xxlarge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>APOLLO applied to a very large DeBERTa-v2 xxlarge checkpoint, continuing pretraining on IMPLICATION with s-MLM and E-CLS to further boost logical reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-v2-xxlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A very large variant of DeBERTa (xxlarge) used as base; APOLLO continues pretraining last layers on IMPLICATION and fine-tunes on logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice reading-comprehension benchmarks emphasising logical deduction, entailment/contradiction recognition, and reasoning over textual premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Same APOLLO pipeline: IMPLICATION selection, s-MLM masking by POS, E-CLS classification, continual pretraining of top transformer layers, then fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor: Dev 81.8, Test 76.5, Test-E 85.2, Test-H 69.6 (accuracy). LogiQA: Dev 49.6, Test 51.0 (accuracy). APOLLO on DeBERTa-v2-xxlarge yields state-of-the-art LogiQA performance reported in paper and competitive ReClor results (slightly lower than MERIt on ReClor Test in the reported comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same limitations about noisy IMPLICATION data and POS-dependency; large models require substantial compute (authors used 8 A100 GPUs, up to ~20 hours for largest model). APOLLO on xxlarge is slightly worse on ReClor Test than MERIt (reported: MERIt DeBERTa-v2-xxlarge Test 78.1 vs APOLLO 76.5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>APOLLO (DeBERTa-v2-xxlarge) outperforms DeBERTa-v2-xxlarge baseline and most other baselines on LogiQA; compared to MERIt (same base) MERIt reports higher ReClor Test accuracy (78.1) while APOLLO is competitive and better on LogiQA (APOLLO LogiQA Test 51.0).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations (dataset/loss/POS choices, effect of trainable layers) reported in the paper generalize to DeBERTa variants: IMPLICATION + s-MLM + E-CLS is the best combination; masking nouns harms logic gains; bootstrapped E-CLS helps beyond s-MLM alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5104.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5104.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MERIt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that generates logically related sentence pairs from Wikipedia using entity/graph meta-paths and trains models with contrastive learning to improve logical reasoning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MERIt (method applied to PLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method constructing sentence pairs from Wikipedia entity graphs and applying contrastive pretraining/objectives to inject logical relations into PLMs; reported in Jiao et al., 2022 and compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA (reported comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reading-comprehension logical reasoning benchmarks; MERIt uses Wikipedia-derived logical pairs for contrastive learning to improve performance on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Graph/meta-path based generation of logically-related sentence pairs from Wikipedia and contrastive learning to pull related pairs together and push apart counterfactuals; requires entity graph construction and counterfactual augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (from Jiao et al., 2022 and Table 1/2): On RoBERTa-Large base: ReClor Dev 67.8, Test 60.7; LogiQA Dev 42.4, Test 41.5. On DeBERTa-v2-xxlarge reported: ReClor Dev 80.6, Test 78.1 (LogiQA results not reported in that source).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Contrastive formulation requires counterfactual data augmentation which can distort factual knowledge; approach is tied to datasets/sources that can provide entity graphs (heavily reliant on Wikipedia); more complex data processing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>MERIt achieves strong ReClor scores (top reported on DeBERTa-v2-xxlarge ReClor Test) and is sometimes better than APOLLO on ReClor Test; APOLLO is simpler and more task-agnostic and achieves better LogiQA results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not exhaustively analyzed in this paper; referenced as a strong baseline. APOLLO authors report ablations showing their simpler IMPLICATION + s-MLM + E-CLS can match or beat MERIt on LogiQA and be competitive on ReClor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5104.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5104.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LRReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-driven context extension and data augmentation for logical reasoning of text (LRReasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that parses symbolic logical structures from task training data to perform logic-driven context extension and data augmentation to improve PLM performance on logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-driven context extension and data augmentation for logical reasoning of text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LRReasoner (method applied to PLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parses logical/symbolic structures from training data (e.g., ReClor) to augment contexts and generate additional training instances tailored to the target task; used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA (comparisons reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading-comprehension logical reasoning datasets requiring inference and symbolic reasoning; LRReasoner uses parsed logical structures to augment examples for the target task.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Task-specific data augmentation via parsing symbolic logical relations and extending context (logic-driven context extension) to make PLMs better at reasoning for that specific dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Table 1, RoBERTa-Large base): ReClor Dev 66.2, Test 62.4, Test-E 81.4, Test-H 47.5; LogiQA Dev 38.1, Test 40.6 (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Data augmentation is specific to the dataset and may not generalize across tasks; requires parsing symbolic structures from task data which may not be available for arbitrary corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>LRReasoner beats the RoBERTa baseline on ReClor; APOLLO is simpler and more generalizable across corpora but LRReasoner sometimes reports higher ReClor Test-E/Test-H splits than APOLLO (RoBERTa-based).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not ablated in this paper (referenced baseline). APOLLO authors argue LRReasoner is more task-specific whereas APOLLO aims for a task-agnostic continual pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5104.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5104.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGN: Discourse-Aware Graph Network for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-neural-network-based approach that encodes discourse structure of text into graph representations and reasons over those graphs for logical reading comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAGN: Discourse-aware graph network for logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DAGN (GNN-based reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method that builds discourse-aware graphs from textual inputs and applies graph neural networks to perform logical reasoning over the constructed graph; used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA (comparisons reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading comprehension benchmarks requiring discourse-level logical reasoning and inference; DAGN models discourse relations explicitly via a graph to enable reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Construct discourse-aware graphs from input texts and apply GNNs to propagate and integrate information for logical inference; different from PLM-only continual pretraining approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Table 1, RoBERTa-Large base reproduction): ReClor Dev 65.2, Test 58.2, Test-E 76.1, Test-H 44.1; LogiQA Dev 35.5, Test 38.7 (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Graph construction and GNN reasoning pipelines are more complex and may be less general across datasets/formats; not the focus of APOLLO which targets PLM pretraining simplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>APOLLO (RoBERTa and DeBERTa variants) outperforms DAGN on LogiQA and is broadly competitive on ReClor; DAGN emphasizes discourse structure while APOLLO uses corpus filtering and selective MLM.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not ablated here; DAGN is included as a representative GNN-based baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5104.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5104.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Focal ReAsoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fact-driven logical reasoning (Focal ReAsoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constructs logical graphs from chains of facts present in a task instance and uses graph neural networks to reason over those graphs to solve logical reading-comprehension questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fact-driven logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Focal ReAsoner (GNN-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructs logical graphs based on fact chains in a task instance and applies GNNs to perform structured reasoning; compared as a baseline in the APOLLO paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor; LogiQA (comparisons reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading comprehension benchmarks requiring reasoning across fact chains; Focal ReAsoner builds and reasons on instance-specific logical graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Instance-level logical graph construction (chain of facts) combined with GNN reasoning; task-specific graph assembly contrasts with APOLLO's corpus-level continual pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Table 1, RoBERTa-Large base): ReClor Dev 66.8, Test 58.9, Test-E 77.1, Test-H 44.6; LogiQA Dev 41.0, Test 40.3 (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Graph construction and reliance on instance-specific structures limit generality; tends to be dataset-specific and more complex to implement than APOLLO's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>APOLLO outperforms Focal ReAsoner on LogiQA (RoBERTa base) and is competitive on ReClor; APOLLO focuses on simple, corpus-agnostic continual pretraining versus instance-level graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not analyzed further in this paper; used as a representative GNN-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning <em>(Rating: 2)</em></li>
                <li>Logic-driven context extension and data augmentation for logical reasoning of text <em>(Rating: 2)</em></li>
                <li>DAGN: Discourse-aware graph network for logical reasoning <em>(Rating: 2)</em></li>
                <li>Fact-driven logical reasoning <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5104",
    "paper_id": "paper-67590dc371a89bef960b7bd547110f43cbe7196e",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "APOLLO (RoBERTa-Large)",
            "name_full": "APOLLO: Adaptive Pretraining of Language Models for Logical Reasoning (applied to RoBERTa-Large)",
            "brief_description": "APOLLO is a continual pretraining approach that selects Wikipedia sentences containing logical implication keywords (the IMPLICATION dataset) and trains models with selective masked language modeling (s-MLM) over specific POS tags plus an entailment/contradiction sentence classification loss (E-CLS) to improve logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-Large",
            "model_description": "A pretrained transformer-based masked language model (RoBERTa) used as the base checkpoint; APOLLO continues pretraining the top transformer layers of RoBERTa-Large using the IMPLICATION corpus with s-MLM and E-CLS objectives before fine-tuning on downstream tasks.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA",
            "task_description": "Multiple-choice machine reading comprehension datasets focused on logical reasoning: ReClor (graduate admission logical reasoning questions; split into EASY/HARD subsets) and LogiQA (exam-derived logical reading comprehension). Tasks require semantic understanding and higher-order logical inference to select the correct option.",
            "method_or_approach": "Continual pretraining on an IMPLICATION subset of Wikipedia filtered by logical implication keywords; selective MLM (s-MLM) that masks tokens only from POS tags likely to require higher-order reasoning (ADJ, ADV, CONJ, CCONJ, PART, SCONJ, VERB); an auxiliary entailment vs contradiction sentence classification loss (E-CLS) bootstrapped by keywords; then fine-tune on target datasets. Typically only top transformer layers (two) were trained during continued pretraining.",
            "performance": "ReClor: Dev 67.2, Test 58.2, Test-E 76.8, Test-H 43.6 (accuracy). LogiQA: Dev 41.6, Test 42.1 (accuracy). These outperform the RoBERTa baseline and compare favorably to many prior methods (see comparisons).",
            "limitations_or_failure_cases": "IMPLICATION corpus selection by keywords trades completeness for noise (contains redundant/non-logical sentences); POS-tag-based selective masking depends on availability and quality of POS taggers (limits cross-lingual application); APOLLO with RoBERTa performed lower on ReClor test than some baselines (e.g., MERIt on RoBERTa reported higher on some splits).",
            "comparison": "Compared to RoBERTa baseline APOLLO improves both tasks; against prior methods (LRReasoner, DAGN, FOCAL REASONER, MERIt) APOLLO (on RoBERTa-Large) outperforms on LogiQA and is comparable but not best on ReClor (some baselines report higher ReClor test performance). The approach is simpler and task-agnostic compared to methods relying on graph structures or contrastive counterfactual augmentation.",
            "ablation_or_analysis_results": "Ablations (RoBERTa base): Using IMPLICATION vs RANDOM subset improves performance; s-MLM &gt; MLM; adding E-CLS on top of s-MLM gives further gains (IMPLICATION + s-MLM + E-CLS yields ReClor Dev 67.2 / LogiQA Dev 41.6). POS ablation: base POS set performs best; adding nouns degrades performance (predicting nouns requires world knowledge); random masking (equivalent to standard MLM) is worse. Keyword ablation: IMPLICATION-Positive and -Negative individually help, combined gives best results. Best to continue-train top 2 transformer layers; training more lower layers can degrade performance until full fine-tuning.",
            "uuid": "e5104.0",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "APOLLO (DeBERTa-v3)",
            "name_full": "APOLLO: Adaptive Pretraining applied to DeBERTa-v3",
            "brief_description": "Same APOLLO continual pretraining pipeline (IMPLICATION dataset, s-MLM, E-CLS) applied to DeBERTa-v3 base checkpoint to improve logical reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTa-v3",
            "model_description": "Microsoft DeBERTa-v3 transformer architecture (pretrained) used as the base model for continued APOLLO pretraining; APOLLO fine-tunes the top layers with s-MLM and E-CLS on IMPLICATION before task fine-tuning.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA",
            "task_description": "Same MRC logical reasoning benchmarks (ReClor, LogiQA) that require inference over text and discriminating entailment/contradiction and higher-order logical relations.",
            "method_or_approach": "Continual pretraining on IMPLICATION with s-MLM (POS-selective masking) and E-CLS, then fine-tune on target datasets; experiments use top layer adaptation similar to RoBERTa experiments.",
            "performance": "ReClor: Dev 76.8, Test 72.8, Test-E 81.8, Test-H 65.7 (accuracy). LogiQA: Dev 48.4, Test 44.4 (accuracy). APOLLO (DeBERTa-v3) outperforms the base DeBERTa-v3 checkpoint on both benchmarks.",
            "limitations_or_failure_cases": "Same dataset-noise and POS-dependency limitations apply; although APOLLO improves DeBERTa-v3, absolute gains vary by dataset and may be smaller on some splits.",
            "comparison": "APOLLO on DeBERTa-v3 outperforms baseline DeBERTa-v3 and other baselines reported in the paper for both ReClor and LogiQA (per Table 2).",
            "ablation_or_analysis_results": "Ablation trends observed with RoBERTa hold: IMPLICATION corpus and s-MLM+E-CLS effective; best performance when adapting top layers (2) rather than all layers in most compute-efficient setting.",
            "uuid": "e5104.1",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "APOLLO (DeBERTa-v2-xxlarge)",
            "name_full": "APOLLO: Adaptive Pretraining applied to DeBERTa-v2-xxlarge",
            "brief_description": "APOLLO applied to a very large DeBERTa-v2 xxlarge checkpoint, continuing pretraining on IMPLICATION with s-MLM and E-CLS to further boost logical reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTa-v2-xxlarge",
            "model_description": "A very large variant of DeBERTa (xxlarge) used as base; APOLLO continues pretraining last layers on IMPLICATION and fine-tunes on logical reasoning tasks.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA",
            "task_description": "Multiple-choice reading-comprehension benchmarks emphasising logical deduction, entailment/contradiction recognition, and reasoning over textual premises.",
            "method_or_approach": "Same APOLLO pipeline: IMPLICATION selection, s-MLM masking by POS, E-CLS classification, continual pretraining of top transformer layers, then fine-tuning.",
            "performance": "ReClor: Dev 81.8, Test 76.5, Test-E 85.2, Test-H 69.6 (accuracy). LogiQA: Dev 49.6, Test 51.0 (accuracy). APOLLO on DeBERTa-v2-xxlarge yields state-of-the-art LogiQA performance reported in paper and competitive ReClor results (slightly lower than MERIt on ReClor Test in the reported comparison).",
            "limitations_or_failure_cases": "Same limitations about noisy IMPLICATION data and POS-dependency; large models require substantial compute (authors used 8 A100 GPUs, up to ~20 hours for largest model). APOLLO on xxlarge is slightly worse on ReClor Test than MERIt (reported: MERIt DeBERTa-v2-xxlarge Test 78.1 vs APOLLO 76.5).",
            "comparison": "APOLLO (DeBERTa-v2-xxlarge) outperforms DeBERTa-v2-xxlarge baseline and most other baselines on LogiQA; compared to MERIt (same base) MERIt reports higher ReClor Test accuracy (78.1) while APOLLO is competitive and better on LogiQA (APOLLO LogiQA Test 51.0).",
            "ablation_or_analysis_results": "Ablations (dataset/loss/POS choices, effect of trainable layers) reported in the paper generalize to DeBERTa variants: IMPLICATION + s-MLM + E-CLS is the best combination; masking nouns harms logic gains; bootstrapped E-CLS helps beyond s-MLM alone.",
            "uuid": "e5104.2",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "MERIt",
            "name_full": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "brief_description": "A prior method that generates logically related sentence pairs from Wikipedia using entity/graph meta-paths and trains models with contrastive learning to improve logical reasoning abilities.",
            "citation_title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "mention_or_use": "mention",
            "model_name": "MERIt (method applied to PLMs)",
            "model_description": "Method constructing sentence pairs from Wikipedia entity graphs and applying contrastive pretraining/objectives to inject logical relations into PLMs; reported in Jiao et al., 2022 and compared in this paper.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA (reported comparisons)",
            "task_description": "Same reading-comprehension logical reasoning benchmarks; MERIt uses Wikipedia-derived logical pairs for contrastive learning to improve performance on these tasks.",
            "method_or_approach": "Graph/meta-path based generation of logically-related sentence pairs from Wikipedia and contrastive learning to pull related pairs together and push apart counterfactuals; requires entity graph construction and counterfactual augmentation.",
            "performance": "Reported (from Jiao et al., 2022 and Table 1/2): On RoBERTa-Large base: ReClor Dev 67.8, Test 60.7; LogiQA Dev 42.4, Test 41.5. On DeBERTa-v2-xxlarge reported: ReClor Dev 80.6, Test 78.1 (LogiQA results not reported in that source).",
            "limitations_or_failure_cases": "Contrastive formulation requires counterfactual data augmentation which can distort factual knowledge; approach is tied to datasets/sources that can provide entity graphs (heavily reliant on Wikipedia); more complex data processing.",
            "comparison": "MERIt achieves strong ReClor scores (top reported on DeBERTa-v2-xxlarge ReClor Test) and is sometimes better than APOLLO on ReClor Test; APOLLO is simpler and more task-agnostic and achieves better LogiQA results in this paper.",
            "ablation_or_analysis_results": "Not exhaustively analyzed in this paper; referenced as a strong baseline. APOLLO authors report ablations showing their simpler IMPLICATION + s-MLM + E-CLS can match or beat MERIt on LogiQA and be competitive on ReClor.",
            "uuid": "e5104.3",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "LRReasoner",
            "name_full": "Logic-driven context extension and data augmentation for logical reasoning of text (LRReasoner)",
            "brief_description": "A method that parses symbolic logical structures from task training data to perform logic-driven context extension and data augmentation to improve PLM performance on logical reasoning tasks.",
            "citation_title": "Logic-driven context extension and data augmentation for logical reasoning of text",
            "mention_or_use": "mention",
            "model_name": "LRReasoner (method applied to PLMs)",
            "model_description": "Parses logical/symbolic structures from training data (e.g., ReClor) to augment contexts and generate additional training instances tailored to the target task; used as a baseline in comparisons.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA (comparisons reported)",
            "task_description": "Reading-comprehension logical reasoning datasets requiring inference and symbolic reasoning; LRReasoner uses parsed logical structures to augment examples for the target task.",
            "method_or_approach": "Task-specific data augmentation via parsing symbolic logical relations and extending context (logic-driven context extension) to make PLMs better at reasoning for that specific dataset.",
            "performance": "Reported (Table 1, RoBERTa-Large base): ReClor Dev 66.2, Test 62.4, Test-E 81.4, Test-H 47.5; LogiQA Dev 38.1, Test 40.6 (accuracy).",
            "limitations_or_failure_cases": "Data augmentation is specific to the dataset and may not generalize across tasks; requires parsing symbolic structures from task data which may not be available for arbitrary corpora.",
            "comparison": "LRReasoner beats the RoBERTa baseline on ReClor; APOLLO is simpler and more generalizable across corpora but LRReasoner sometimes reports higher ReClor Test-E/Test-H splits than APOLLO (RoBERTa-based).",
            "ablation_or_analysis_results": "Not ablated in this paper (referenced baseline). APOLLO authors argue LRReasoner is more task-specific whereas APOLLO aims for a task-agnostic continual pretraining.",
            "uuid": "e5104.4",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DAGN",
            "name_full": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
            "brief_description": "A graph-neural-network-based approach that encodes discourse structure of text into graph representations and reasons over those graphs for logical reading comprehension.",
            "citation_title": "DAGN: Discourse-aware graph network for logical reasoning",
            "mention_or_use": "mention",
            "model_name": "DAGN (GNN-based reasoning model)",
            "model_description": "A method that builds discourse-aware graphs from textual inputs and applies graph neural networks to perform logical reasoning over the constructed graph; used as a baseline in comparisons.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA (comparisons reported)",
            "task_description": "Reading comprehension benchmarks requiring discourse-level logical reasoning and inference; DAGN models discourse relations explicitly via a graph to enable reasoning.",
            "method_or_approach": "Construct discourse-aware graphs from input texts and apply GNNs to propagate and integrate information for logical inference; different from PLM-only continual pretraining approaches.",
            "performance": "Reported (Table 1, RoBERTa-Large base reproduction): ReClor Dev 65.2, Test 58.2, Test-E 76.1, Test-H 44.1; LogiQA Dev 35.5, Test 38.7 (accuracy).",
            "limitations_or_failure_cases": "Graph construction and GNN reasoning pipelines are more complex and may be less general across datasets/formats; not the focus of APOLLO which targets PLM pretraining simplicity.",
            "comparison": "APOLLO (RoBERTa and DeBERTa variants) outperforms DAGN on LogiQA and is broadly competitive on ReClor; DAGN emphasizes discourse structure while APOLLO uses corpus filtering and selective MLM.",
            "ablation_or_analysis_results": "Not ablated here; DAGN is included as a representative GNN-based baseline for comparison.",
            "uuid": "e5104.5",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Focal ReAsoner",
            "name_full": "Fact-driven logical reasoning (Focal ReAsoner)",
            "brief_description": "A method that constructs logical graphs from chains of facts present in a task instance and uses graph neural networks to reason over those graphs to solve logical reading-comprehension questions.",
            "citation_title": "Fact-driven logical reasoning",
            "mention_or_use": "mention",
            "model_name": "Focal ReAsoner (GNN-based)",
            "model_description": "Constructs logical graphs based on fact chains in a task instance and applies GNNs to perform structured reasoning; compared as a baseline in the APOLLO paper.",
            "model_size": null,
            "logical_reasoning_task": "ReClor; LogiQA (comparisons reported)",
            "task_description": "Reading comprehension benchmarks requiring reasoning across fact chains; Focal ReAsoner builds and reasons on instance-specific logical graphs.",
            "method_or_approach": "Instance-level logical graph construction (chain of facts) combined with GNN reasoning; task-specific graph assembly contrasts with APOLLO's corpus-level continual pretraining.",
            "performance": "Reported (Table 1, RoBERTa-Large base): ReClor Dev 66.8, Test 58.9, Test-E 77.1, Test-H 44.6; LogiQA Dev 41.0, Test 40.3 (accuracy).",
            "limitations_or_failure_cases": "Graph construction and reliance on instance-specific structures limit generality; tends to be dataset-specific and more complex to implement than APOLLO's approach.",
            "comparison": "APOLLO outperforms Focal ReAsoner on LogiQA (RoBERTa base) and is competitive on ReClor; APOLLO focuses on simple, corpus-agnostic continual pretraining versus instance-level graph reasoning.",
            "ablation_or_analysis_results": "Not analyzed further in this paper; used as a representative GNN-based baseline.",
            "uuid": "e5104.6",
            "source_info": {
                "paper_title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "rating": 2
        },
        {
            "paper_title": "Logic-driven context extension and data augmentation for logical reasoning of text",
            "rating": 2
        },
        {
            "paper_title": "DAGN: Discourse-aware graph network for logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Fact-driven logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1
        }
    ],
    "cost": 0.01587,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Apollo: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning</h1>
<p>Soumya Sanyal ${ }^{1 <em>}$ Yichong $\mathrm{Xu}^{2}$ Shuohang Wang ${ }^{2}$ Ziyi Yang ${ }^{2}$<br>Reid Pryzant ${ }^{2}$ Wenhao Yu ${ }^{3 </em>}$ Chenguang Zhu ${ }^{2}$ Xiang Ren ${ }^{1}$<br>${ }^{1}$ University of Southern California ${ }^{2}$ Microsoft Cognitive Service Research<br>${ }^{3}$ University of Notre Dame<br>soumyasa@usc.edu</p>
<h4>Abstract</h4>
<p>Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific solutions that are not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA. The code base has been made publicly available. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Logical reasoning is an important ability of humans that helps us in making rational decisions based on known information. It is an important ability for text understanding across various downstream tasks, e.g., in open-domain question answering (Yang et al., 2018; Zhu et al., 2021), machine</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Motivation of Selective Masking. In random masking (Devlin et al., 2019), a word is masked at random. Predicting these words often require more of language understanding than higher-order reasoning (e.g., predicting "would" at the $2^{\text {n-d }}$ [MASK] place). In selective masking, a word is masked if its POS tag is from a specific set. These candidate words are marked in the blue box in the input sentence. Filling these words requires more reasoning (e.g., to predict "more" at the $2^{\text {n-d }}[\mathrm{MASK}]$ place instead of "less", which is also grammatically valid, the model needs a better understanding of the semantics of the sentence).
reading comprehension (MRC) (Baradaran et al., 2022), etc. Recently, there has been an increasing focus on evaluating the logical reasoning abilities of language models by using MRC tasks that specifically require a significant amount of logical reasoning to obtain the correct answer (Yu et al., 2020; Liu et al., 2021). In these datasets, the model needs to understand a given context, reason logically about a question to infer new conclusions, and then select the correct answer from a set of options. With the advent of large pre-trained language models (PLMs) in NLP (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020), understanding and improving the logical reasoning abilities of these models has become even more important as these are increasingly being used across a wide variety of real-world tasks.</p>
<p>There have been some recent works on improving the logical reasoning abilities of PLMs (Wang et al., 2022; Ouyang et al., 2022; Jiao et al., 2022). These works typically generate a dataset containing symbolic structures such as logical graphs from</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of APOLLO. We filter Wikipedia using specific logical keywords to create the IMPLICATION dataset. This is then used for continued pretraining of a model using two loss objectives: selective masked language modeling (s-MLM) loss and entailment classification (E-CLS) loss. Please refer to Section 2 and Figure 3 for more details on the data selection process and loss function designs.
text, logical contrast sets, etc., and then train the LM using custom loss objectives to learn logical reasoning abilities. While the performance improvements achieved by these methods are encouraging, the proposed solutions generally require complex data processing to generate the additional structural information (graphs, contrast data, etc.) required for training the model. For example, Jiao et al. (2022) constructs synthetic context-answer pairs using the entity-level graph from Wikipedia for training the model. Further, the loss functions proposed in these works are very specifically designed in accordance with their respective data augmentation technique and widely differs from the typical masked language modeling loss used for LM pretraining (Devlin et al., 2019). Additionally, some of these works usually require task-specific design choices, which are not necessarily learning generalizable logical reasoning ability that is reusable across different task formats. For example, Wang et al. (2022) parses symbolic logical structures from the training data of a specific dataset, which might not generalize to a new dataset or task. Overall, it is unclear if these highly specific inductive biases are indeed essential for improving the logical reasoning abilities in language models, or if a simpler approach is possible.</p>
<p>On the other hand, prior works (Gururangan et al., 2020) have shown that continual domainadaptive pretraining of PLMs leads to performance gains on downstream tasks. Inspired by this, we propose APOLLO, a continual pretraining-based approach to inject logical reasoning abilities in language models that requires minimal data processing and loss function modifications.</p>
<p>Firstly, we present a simple way of selecting sentences for training a model that is more likely to involve logical implications. We achieve this by defining a set of logical inference keywords and selecting a subset of sentences from a large text
corpus, each containing at least one of these keywords. We hypothesize that PLMs can learn logical reasoning capabilities more easily using such sentences since the premise/conclusions are explicitly stated. We note that in contrast to previous works (Gururangan et al., 2020), our method can select sentences from any general text corpus, eliminating the need for any domain-specific corpus.</p>
<p>Secondly, we modify the masked language modeling (MLM) loss (Devlin et al., 2019) to selectively mask specific words in the sentence, based on their parts-of-speech tags. Prior works (Lad et al., 2022) have shown the benefit of selective masking of words on task-guided fine-tuning. We hypothesize that masking words with parts-of-speech (POS) tags that are related to higher-order reasoning (such as adverbs, conjunctions, etc.) present more challenging masked positions for the PLM to predict. For instance, in Figure 1, we observe that the words marked in blue boxes are more related to reasoning compared to the non-highlighted words that mainly involve knowledge about specific nouns or English grammar.</p>
<p>Lastly, we design a sentence-level classification loss to predict if the reasoning in the sentence describes an entailment in the reasoning process or a contradiction. This enables the model to better understand the differences between positive and negative implications in a sentence, thus improving logical reasoning.</p>
<p>To test APOLLO, we evaluate it on two downstream logical reasoning tasks: ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021), and compare it with other baselines. We achieve state-of-the-art performance on LogiQA and comparable performance on ReClor. We demonstrate that our method generalizes across different model types. Further, we show that using our proposed loss functions does not induce any catastrophic forgetting (Kirkpatrick et al., 2017) of the original language</p>
<p>modeling skills. This demonstrates that our simple, continual pretraining approach is generalizable to different datasets and enables the PLM to acquire strong logical reasoning abilities.</p>
<p>Overall, compared to prior works, our proposed pretraining paradigm for APOLlo 1) Uses sentences from text corpus for training instead of complex data structures such as entity graphs, etc. 2) Uses simple learning objectives that are closer to language modeling compared to the contrastive loss. 3) Is agnostic to both task format and downstream datasets. 4) Achieves state-of-the-art performance on LogiQA.</p>
<h2>2 Method</h2>
<p>In this section, we describe the details of our proposed approach. In APOLLO, we use a keywordbased selection strategy to collect a dataset of reasoning-related sentences called IMPLICATION (\$2.1) and then continue training a pretrained model checkpoint jointly using two loss functions (2.2). This model is then fine-tuned on the training dataset of each task separately for evaluation. A detailed overview of the pipeline is shown in Figure 2.</p>
<h3>2.1 Dataset Selection</h3>
<p>PLMs are typically trained on web data which helps them to learn general language modeling capability. Then, PLMs are finetuned on downstream datasets to specialize on target tasks (Devlin et al., 2019; Radford et al., 2018; Raffel et al., 2020). Here, instead of focusing on a specific task, we want to teach the PLM generalizable logical reasoning abilities. We hypothesize that using training data that contains more logical sentences, rather than generic internet data, should help in improving the reasoning ability of the PLM.</p>
<p>Although creating such a dataset automatically is a challenging task by itself, in APOLLO, we explore a simple and intuitive way to create such a dataset. First, we select specific keywords that are typically encountered in sentences with logical implications. Broadly, we categorize these keywords into two types ${ }^{2}$ :</p>
<ul>
<li>Positive implication (Entailment): These keywords are present in sentences where the reason generally entails the inference. Exam-</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ples of such keywords would be "therefore", "accordingly", etc.</p>
<ul>
<li>Negative implication (Contradiction): The keywords in this category are usually present in sentences where the reason contradicts the inference. For example, keywords such as "but", "although", etc., come under this category.</li>
</ul>
<p>Next, we select sentences from Wikipedia such that they contain at least one of the keywords. We name this filtered version of Wikipedia as the IMPLICATION dataset. While this keyword-based filtering does not necessarily ensure that the sentence has a logical implication, the retained data contains a higher portion of logically rich sentences than the general data. We argue that pretraining on this data helps the PLM to improve logical reasoning skills. Please refer to Appendix A for more details on the list of keywords used to build the IMPLICATION dataset.</p>
<h3>2.2 Learning objectives</h3>
<p>Selective masked language modeling (s-MLM) is a modified version of the masked language modeling (MLM) loss used in BERT (Devlin et al., 2019). In the MLM loss, tokens in a sentence are masked at random and the model learns to predict the masked tokens. While this helps in learning a good language model, not all masked tokens require a similar degree of reasoning to predict them. In the example shown in Figure 3, words such as "were", "the", etc. are decided more by the structure of the English language than any form of reasoning. In contrast, predicting logical words such as "more", "and" and "hence" would require more logical reasoning. Thus, we hypothesize that masking these logical words would likely teach the model to perform reasoning more effectively than masking a word at random.</p>
<p>While finding these exact logical words for a given sentence is a hard problem, in APOLLO we simplify this by using a heuristic approach to consider words that belong to a specific set of parts-ofspeech (POS) tags. More concretely, in s-MLM loss, we only randomly mask words with these 7 SpaCy POS tags (Honnibal and Montani, 2017): ADJ, ADV, CONJ, CCONJ, PART, SCONJ, and VERB. Please refer to Section 4.4 for more empirical results that further justify this choice.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning objectives in APOLLO. The selective masking step masks out words from a specific set of POS tags (the candidate words are shown in blue boxes). The s-MLM loss then predicts these masked words. The E-CLS loss classifies the masked sentence into one of two categories: entailment or contradiction. The overall loss function used in APOLLO is the sum of both these objectives.</p>
<p>Entailment classification (E-CLS) Prior works have shown that semantic-aware sentence-level classification loss can be useful to learn the semantic information (Sun et al., 2020). Inspired by this, in addition to s-MLM, we use another auxiliary loss function that predicts whether a masked sentence contains some reasoning aspects that portray a sense of entailment or contradiction within the sentence. For example, in Figure 3, the sentence is classified as "Entailment", because the phrase "more reflective" is entailed by the phrase "frozen entirely". We note that the input to the model is the same sentence with masked words that is used for s-MLM loss. A model would ideally require strong logical reasoning abilities to understand the sentence and then predict if it refers to an entailment or contradiction. The labels for this loss are bootstrapped using the heuristic of checking the type of implication keyword present in the sentence (refer to Section 2.1 for details). We note that although the keyword is a correlated feature that can be used to predict the label, on average the keyword would be masked out due to our selective masking policy, forcing the model to learn some logical semantics to minimize the loss. Additionally, even if the model predicts a wrong keyword in the sentence, it may still get the relationship between the sentences correctly. Therefore, the classification loss adds a stronger inductive bias specifically about the reasoning semantics in the sentence than the s-MLM loss.</p>
<h3>2.3 Continual Pretraining</h3>
<p>In APOLLO, we combine both s-MLM and E-CLS objectives as a joint loss function to continually
train a pretrained model checkpoint (Figure 2). Unlike prior works (Jiao et al., 2022), we don't need to add MLM loss to avoid catastrophic forgetting, as s-MLM is quite close to the standard MLM objective in format.</p>
<h3>2.4 Finetuning</h3>
<p>As our loss functions are task-format agnostic, we follow Devlin et al. (2019) and add a randomly initialized MLP layer on top of the continually pretrained model. Then, we finetune the combined model on downstream datasets.</p>
<h2>3 Experimental Setup</h2>
<p>In this section, we describe the details of the datasets on which we evaluate APOLLO, the baselines we compare it with, and some implementation details of our training procedure.</p>
<h3>3.1 Datasets</h3>
<p>Following prior works (Jiao et al., 2022), we evaluate APOLLO on two logical reasoning datasets:</p>
<p>ReClor (Yu et al., 2020) is a reading comprehension dataset created from the logical reasoning questions from standardized graduate admission examinations. The test set is divided into two subsets: EASY (test-E) and HARD (test-H), where the EASY set contains instances whose options can be selected correctly without knowing the context and question. The train/dev/test split consists of 4,638/500/1,000 instances, respectively.</p>
<p>LogiQA (Liu et al., 2021) is developed using publicly available logical examination papers for</p>
<p>reading comprehension. The train/dev/test split consists of 7,376/651/651 instances, respectively.</p>
<h3>3.2 Baselines</h3>
<p>We compare the accuracy of APOLLO with the following baselines: LRReasoner (Wang et al., 2022), DAGN (Huang et al., 2021), FOCAL REASONER (Ouyang et al., 2022), and MERIt (Jiao et al., 2022).</p>
<h3>3.3 Implementation Details</h3>
<p>For creating the IMPLICATION dataset, we use the Wikipedia version provided under HuggingFace Datasets (Wolf et al., 2020) as the main corpus. ${ }^{3}$ The list of keywords we use for filtering sentences from Wikipedia are listed in Appendix A. We experiment with RoBERTa-Large (Liu et al., 2019a), DeBERTa-v3 (He et al., 2021), and DeBERTa-v2-xxlarge (He et al., 2020) as the base models for APOLLO. We pretrain the last two layers of the Transformer (Vaswani et al., 2017) layer for 3 epochs, using a batch size of 4096. Please refer to Appendix B for more details on training and finetuning hyperparameters.</p>
<h2>4 Results</h2>
<h3>4.1 Overall Results</h3>
<p>In this section, we compare the performance of APOLLO with prior baselines on the two logical reasoning datasets for different base architectures. The results of using pretrained Roberta-Large as the starting checkpoint for our method are shown in Table 1. We observe that APOLLO outperforms all baselines on LogiQA and performs lower on ReClor than three baselines, although consistently outperforming the RoBERTa baseline. Overall, this demonstrates that our simple continual pretraining approach is indeed strong enough to perform well on logical reasoning tasks as compared to the prior models that depend on much more complex training data and loss function designs.</p>
<p>To test the generality of our approach across different architectures, we use pretrained DeBERTa-v3 and DeBERTa-v2-xxlarge as the base models for continued training. The results of using these models are shown in Table 2. We find that APOLLO outperforms both the baselines on both datasets. Further, we observe that APOLLO performs 1.5\% worse compared to MERIt on ReClor test set. This</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>shows that our continual pretraining process can improve performance across different LM architectures.</p>
<h3>4.2 Performance on GLUE Benchmark</h3>
<p>While improving the logical reasoning abilities of a PLM is important, it is equally important to retain the natural language understanding skills learned during pretraining. To demonstrate that our proposed approach does not lead to catastrophic forgetting, we finetune APOLLO on each dataset of the GLUE benchmark (Wang et al., 2019) and evaluate the finetuned checkpoint on the Dev set. The results are compared with the Dev set results for the RoBERTa model (Liu et al., 2019b) in Table 3. Following Devlin et al. (2019), we omit the evaluation on the problematic WNLI set. Overall, we observe that APOLLO can slightly improve the overall performance on the GLUE benchmark. This demonstrates that our proposed continued pretraining strategy is able to learn better logical reasoning abilities without any catastrophic forgetting of general-purpose language modeling skills, and these logical reasoning capabilities are also beneficial for general natural language understanding.</p>
<h3>4.3 Qualitative Analysis</h3>
<p>In this section, we analyze the effect of continued pretraining on the model's overall faithfulness. Post-hoc interpretability methods such as Integrated Gradients (Sundararajan et al., 2017), are algorithms to determine the importance of words in the input towards predicting a particular class. These importance scores are also referred to as attribution scores. To approximate the impact of continued pretraining, we compute the overall change in attribution scores for the implication keywords, before and after pretraining the model using our proposed datasets and loss functions. Specifically, we compute the sum of the attribution scores for the keywords present in each instance of the validation set. The results are shown in Figure 4. We observe that our proposed pretraining increases the overall attribution score by a significant margin, indicating that the model intrinsically learns these important logical keywords, which is desirable.</p>
<h3>4.4 Ablation Studies</h3>
<p>In this section, we ablate various design choices in constructing the IMPLICATION dataset, and our proposed method. For the ablations involving APOLLO, we use RoBERTa-Large as the base</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ReClor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LogiQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Test-E</td>
<td style="text-align: center;">Test-H</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: left;">DAGN</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: left;">LRReasoner</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">$\mathbf{6 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 5}$</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">40.6</td>
</tr>
<tr>
<td style="text-align: left;">FOCAL REASONER</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">40.3</td>
</tr>
<tr>
<td style="text-align: left;">MERIt</td>
<td style="text-align: center;">$\mathbf{6 7 . 8}$</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">$\mathbf{4 2 . 4}$</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: left;">APOLLO</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">$\mathbf{4 2 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of APOLLO with other baselines on ReClor and LogiQA. All the models are based on the RoBERTa-large model. The results for all the baselines are reported from Jiao et al. (2022). Please refer to Section 4.1 for more details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ReClor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LogiQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Test-E</td>
<td style="text-align: center;">Test-H</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa-v3</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: left;">APOLLO (DeBERTa-v3)</td>
<td style="text-align: center;">$\mathbf{7 6 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa-v2-xxlarge</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">49.8</td>
</tr>
<tr>
<td style="text-align: left;">MERIt (DeBERTa-v2-xxlarge)</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}$</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">$\mathbf{7 2 . 9}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">APOLLO (DeBERTa-v2-xxlarge)</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}$</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">$\mathbf{8 5 . 2}$</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">$\mathbf{4 9 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of APOLLO with other baselines on ReClor and LogiQA with DeBERTa as the base architecture. Results for MERIt are reported from Jiao et al. (2022), which is missing results on LogiQA. Other baselines are reproduced by ourselves. The base models are shown in brackets. Please refer to Section 4.1 for more details.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison plot of the keyword attribution scores between RoBERTa-large and APOLLO. Please refer to 4.3 for more details.
model and the IMPLICATION dataset, if not mentioned separately. All the reported numbers are on the validation set of the downstream task, since we used these ablation studies in our model's design choices.</p>
<p>Effect of datasets and loss functions To study the effect of using IMPLICATION for continued pretraining along with the proposed loss functions, we first create RANDOM, a random subset of Wikipedia of similar size as that of IMPLICATION, and also consider using the standard masked lan-
guage modeling (MLM) loss (Devlin et al., 2019), where any token can be masked at random. The results of the ablation are shown in Table 4. We observe that using the IMPLICATION dataset leads to consistent improvements on both datasets when compared to the RANDOM dataset. Additionally, we find that both the s-MLM and E-CLS loss lead to improvements over MLM loss. Thus, this empirically justifies our choice of the dataset and loss functions proposed here.</p>
<p>Effect of keyword category In this ablation, we study the effect of the keyword categories that we use for filtering Wikipedia. For this, we create two different pretraining datasets IMPLICATIONPositive and IMPLICATION-Negative using the positive and negative implication keywords, respectively (refer to Section 2.1). The total number of sentences in these datasets is 7.5 M and 11.3 M , respectively. Our complete dataset IMPLICATION thus has a total of 18.3 M sentences. The results of the ablation are shown in Table 5, under the section "Keyword Category". We observe that IMPLICA-TION-Positive, although smaller in size, leads to better performance on both downstream tasks, com-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">SST</th>
<th style="text-align: center;">MRPC</th>
<th style="text-align: center;">CoLA</th>
<th style="text-align: center;">STS</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa-Large</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">88.9</td>
</tr>
<tr>
<td style="text-align: left;">Apollo</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">$\mathbf{8 9 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on the dev set of GLUE benchmark. Following Devlin et al. (2019), we do not report performance on the WNLI dataset. Please refer to Section 4.2 for further details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model (Dataset, Loss functions)</th>
<th style="text-align: center;">ReClor</th>
<th style="text-align: center;">LogiQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa (RANDOM, MLM)</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">35.0</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (RANDOM, s-MLM)</td>
<td style="text-align: center;">$\mathbf{6 3 . 8}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (IMPLICATION, MLM)</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">36.6</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (IMPLICATION, s-MLM)</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (IMPLICATION, s-MLM + E-CLS)</td>
<td style="text-align: center;">$\mathbf{6 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Effect of IMPLICATION dataset and the loss functions on the dev set performance of ReClor and LogiQA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ReClor</th>
<th style="text-align: center;">LogiQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Keyword Category</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">IMPLICATION-Positive</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICATION-Negative</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">37.6</td>
</tr>
<tr>
<td style="text-align: left;">IMPLICATION</td>
<td style="text-align: center;">$\mathbf{6 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">POS Category</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">$\mathbf{6 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Base + Nouns</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: left;">Base + Nouns + Random</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">36.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation of design choices involved in keyword-based dataset selection and s-MLM loss function implementation. We report the performance on the dev set of each dataset. Please refer to Section 4.4 for more details.
pared to IMPLICATION-Negative. One reason for this is that the sentences with positive keywords are more likely related to reasoning than the negative counterparts because the negative keywords are used in many diverse scenarios in the English language. For example, the word "still" can be used in a non-logical manner such as "I am still waiting for the call". Overall, we observe that the combined IMPLICATION dataset leads to the best performance, demonstrating that both the positive and negative implication keywords are essential to improve logical reasoning.</p>
<p>Effect of POS tag category In this, we analyze the effect of the parts-of-speech (POS) tags we use to mask tokens in our s-MLM loss. We consider the following categories:</p>
<ul>
<li>Base: This consists of the POS tags used in Apollo, i.e., ADJ, ADV, CONJ, CCONJ, PART, SCONJ, and VERB.
<img alt="img-4.jpeg" src="img-4.jpeg" /></li>
</ul>
<p>Figure 5: Average performance on the dev set of ReClor and LogiQA with increasing number of trainable layers of Apollo. The pink dashed line shows the average performance of RoBERTa-Large when all layers are finetuned. Please refer to Section 4.4 for more details.</p>
<ul>
<li>Nouns: Here, we consider the tags referring to nouns and pronouns, i.e., NOUN, PRON, and PROPN.</li>
<li>Random: This consists of remaining categories such as ADP, INTJ, DET, PUNCT, etc. To study the effect of the POS tags, we incrementally add the "Nouns" and "Random" categories to the base case and evaluate the effect of pretraining using the s-MLM loss. The results of this ablation are shown in Table 5, under the section "POS Category". We observe that masking nouns and pronouns ("Nouns") leads to a significant performance drop. We attribute this drop to the fact that predicting a correct noun in a sentence would likely require more world knowledge than logical reasoning. Using the remaining categories for selective masking ("Random"), effectively making the loss function equivalent to random MLM, leads to some drop in performance as well, indicating that our set of POS tag categories is indeed more useful to learn logical reasoning.</li>
</ul>
<p>Effect of the number of trainable layers In order to study the effect of training different numbers of parameters of the RoBERTa model, we vary the number of trainable layers of the transformer architecture between 1 and 24 (i.e., training the complete model). The results are shown in Figure 5. The blue solid line shows the performance of</p>
<p>APOLLO and the purple dashed line denotes the average performance of RoBERTa-Large when all layers are finetuned. From the plot, we observe that with increasing the number of trainable layers, the performance improves till layer 2, and then continues to degrade until all the layers are being trained. Prior works (Tenney et al., 2019) have shown that PLMs learn syntactic-level information in the lower layers of the transformer and semanticlevel information in the upper layers. Thus, we hypothesize that the logical reasoning task initially benefits from an increasing number of trainable layers, as the semantic information needed to understand logic is being captured. But lower layers that contain the syntactic information do not benefit as much when trained using the same data as they are less related to high-level logical reasoning. The full model finetuning surprisingly performs quite well as all the model layers along with the token embeddings are being trained specifically for the logical reasoning task. But it takes significantly larger compute to finetune such a model. Overall, we find that by training the topmost two layers of the model, we are able to achieve the best performance on both datasets and hence we follow this across all variants of APOLLO.</p>
<h2>5 Related Works</h2>
<p>Logical Reasoning in LMs Reasoning in natural language has been a prevalent problem in NLP. In recent years, logical reasoning in textual data has seen an increasing focus. ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) are reading comprehension-style datasets focused on questions that require reasoning using information from a given context. Prior works have predominantly used language models (Wang et al., 2022; Jiao et al., 2022) or graph neural networks (GNNs) (Huang et al., 2021; Xu et al., 2022; Li et al., 2022; Ouyang et al., 2022) to perform logical reasoning over text. Wang et al. (2022) proposed LRReasoner, which parses symbolic logical structures from the training data of ReClor for data augmentation using logical context extensions to train a PLM. Jiao et al. (2022) proposed MERIt, that used Wikipedia to generate sentence pairs for contrastive learning that are logically related, and trained the PLM using contrastive loss. DAGN (Huang et al., 2021) uses the discourse structure of the texts to perform logical reasoning using GNNs. Focal ReAsoner (Ouyang et al., 2022) constructs logical graphs using the chain of
facts present in a task instance and uses GNNs to reason on the graph. GNN-based methods are not directly in scope since our main objective is to improve the logical reasoning skills of language models. Following (Jiao et al., 2022), we compare our method with two GNN-based representative methods DAGN and Focal ReASONER.</p>
<p>Both LRReasoner and Focal ReASONER use data augmentation that is specific to the task being solved, making the pretraining process specific to the downstream dataset, and thus not generalizable across tasks. While MERIt addresses this issue by using Wikipedia to generate logical graphs, their contrastive loss formulation requires counterfactual data augmentation, which potentially distorts the factual knowledge present in the pretrained model. Additionally, their approach is restricted to using Wikipedia as the data source since they heavily rely on forming entity graphs from Wikipedia texts. In contrast, we propose a simple continued pretraining strategy by modifying the masked language modeling loss (Devlin et al., 2019) and sentence classification loss to improve the logical reasoning ability of language models. Our approach is simple to integrate during pretraining, is not dependent on any data processing, and generalizes well across different datasets.</p>
<p>Along a related line, Clark et al. (2020) used synthetically generated data to teach PLMs to perform logical deductions over a given rule base to predict the entailment of a hypothesis. This led to some recent developments in trying to build systems that can generate step-by-step reasoning chains that demonstrate the model's reasoning process (Saha et al., 2020; Tafjord et al., 2021; Sanyal et al., 2022b). While this progress is encouraging, the use of synthetic data for training the models limits the generality of the logical reasoning skills learned by these models. Recent works have questioned if these models are indeed learning to perform logical reasoning in a robust manner or just learning some shortcuts from training data (Zhang et al., 2022; Sanyal et al., 2022a). In contrast, our method uses real-world sentences which alleviates the issue of using synthetic datasets for reasoning.</p>
<p>Selective masking A key step in the processing of masked language modeling loss (Devlin et al., 2019) is to determine which tokens to mask. Originally, Devlin et al. (2019) randomly mask 15\% of tokens. Prior works have tried different techniques to select which tokens to mask. For exam-</p>
<p>ple, ERNIE (Zhang et al., 2019) and EntityBERT (Lin et al., 2021) mask named entities to perform better knowledge-driven tasks. Other prior works (Gu et al., 2020; Lad et al., 2022) calculate the importance of words for a specific task and selectively mask the most important words. In this work, we explore the use of selective masking in the context of logical reasoning, using a novel heuristic of selecting specific POS-tagged words.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we proposed APOLLO, an adaptive pre-trained language model with logical reasoning abilities. We use a subset of Wikipedia sentences for continued pretraining of the model using two self-supervised loss functions. The choice of the training dataset and loss functions are guided by the goal to include more reasoning-related sentences and training signals, respectively. Through experiments on two logical reasoning datasets and ablation studies, we demonstrate the effectiveness of our proposed approach. Overall, we show that APOLLO is a generalized solution to improving logical reasoning in language models.</p>
<p>A key advantage of APOLLO is that the pretraining steps are independent of the dataset used to train the model and the downstream task format. This opens the scope to use a larger text corpus for training such as C4 (Raffel et al., 2020). Additionally, expanding on the keywords beyond positive and negative implications (for example, conditionals such as "if-then", "either-or", etc.) can also benefit the training pipeline.</p>
<h2>7 Limitation</h2>
<p>A limitation of this approach is the trade-off between completeness and noise in the training data. While our method using keywords to extract text from Wikipedia is effective, IMPLICATION likely contains redundant sentences that cannot improve the model's logical reasoning capability. A better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs. Additionally, using POS tagging limits the application of this approach to languages with well-defined POS taggers. Switching to a more universal semantic tagging system (Abzianidze and Bos, 2017) can potentially alleviate this.</p>
<h2>Acknowledgements</h2>
<p>This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity(IARPA), via Contract No. 2019-19051600007, the DARPA MCS program under Contract No.N660011924033, the Defense Advanced Research Projects Agency with award W911NF-1920271, NSF IIS 2048211, NSF SMA 1829268, and gift awards from Google, Amazon, JP Morgan, and Sony. We would like to thank all the collaborators in the USC INK research lab for their constructive feedback on the work.</p>
<h2>References</h2>
<p>Lasha Abzianidze and Johan Bos. 2017. Towards universal semantic tagging. In IWCS 2017 - 12th International Conference on Computational Semantics Short papers.</p>
<p>Razieh Baradaran, Razieh Ghiasi, and Hossein Amirkhani. 2022. A survey on machine reading comprehension systems. Natural Language Engineering, 28(6):683-732.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3882-3890. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun. 2020. Train no evil: Selective masking for task-guided pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6966-6974, Online. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Ana Marasovi, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving debera using electra-style pretraining with gradient-disentangled embedding sharing.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention.</p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear.</p>
<p>Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. 2021. DAGN: Discourse-aware graph network for logical reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5848-5855, Online. Association for Computational Linguistics.</p>
<p>Fangkai Jiao, Yangyang Guo, Xuemeng Song, and Liqiang Nie. 2022. MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3496-3509, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.</p>
<p>Tanish Lad, Himanshu Maheshwari, Shreyas Kottukkal, and Radhika Mamidi. 2022. Using selective masking as a bridge between pre-training and fine-tuning. arXiv preprint arXiv:2211.13815.</p>
<p>Xiao Li, Gong Cheng, Ziheng Chen, Yawei Sun, and Yuzhong Qu. 2022. AdaLoGN: Adaptive logic graph network for reasoning-based machine reading comprehension. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7147-7161, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Chen Lin, Timothy Miller, Dmitriy Dligach, Steven Bethard, and Guergana Savova. 2021. EntityBERT: Entity-centric masking strategy for model pretraining for the clinical domain. In Proceedings of the 20th Workshop on Biomedical Language Processing, pages 191-201, Online. Association for Computational Linguistics.</p>
<p>Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI'20.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, and Jingfei Du an. 2019a. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. RoBERTa: a robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Siru Ouyang, Zhuosheng Zhang, and hai zhao. 2022. Fact-driven logical reasoning.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof generation for interpretable reasoning over rules. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 122-136, Online. Association for Computational Linguistics.</p>
<p>Soumya Sanyal, Zeyi Liao, and Xiang Ren. 2022a. Robustlr: Evaluating robustness to logical perturbation in deductive reasoning.</p>
<p>Soumya Sanyal, Harman Singh, and Xiang Ren. 2022b. FaiRR: Faithful and robust deductive reasoning over natural language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1075-1093, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: A continual pre-training framework for language understanding. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8968-8975.</p>
<p>Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3319-3328. PMLR.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In the Proceedings of ICLR.</p>
<p>Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022. Logic-driven context extension and data augmentation for logical reasoning of text. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1619-1629, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, Franois Lagunas, Lysandre Debut, Morgan Funtowicz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Mutar, Jeff Boudier, and Anna Tordjmann. 2020. Datasets. GitHub. Note: https://github.com/huggingface/datasets.</p>
<p>Fangzhi Xu, Jun Liu, Qika Lin, Yudai Pan, and Lingling Zhang. 2022. Logiformer: A two-branch graph transformer network for interpretable logical reasoning. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22, page 1055-1065, New York, NY, USA. Association for Computing Machinery.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations (ICLR).</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, KaiWei Chang, and Guy Van den Broeck. 2022. On the paradox of learning to reason from data.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1441-1451, Florence, Italy. Association for Computational Linguistics.</p>
<p>Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering.</p>
<h1>A List of Keywords</h1>
<p>In this section, we list the set of keywords that we use to filter the entire WikiPedia data. Any sentence that contains one of the keywords is considered as part of our filtered dataset IMPLICATION. The keywords are divided into two types as described below:</p>
<ul>
<li>Positive implication (Entailment): These keywords are present in sentences where the reason generally entails the inference. Examples of such keywords would be "therefore", "accordingly", etc. We consider the following keywords in this category: "therefore", "accordingly", "so", "thus", "consequently", "hence", "thence", "and so", "for this reason", "in consequence", "on account of", "on the "grounds", "since", "therefrom", "thereupon", "to that end", "whence", and "wherefore".</li>
<li>Negative implication (Contradiction): The keywords in this category are usually present in sentences where the reason contradicts the inference. For example, keywords such as "but", "although", etc., come under this category. Here, we consider the following keywords: "but", "although", "however", "nevertheless", "on the other hand", "still", "though", and "yet".</li>
</ul>
<h2>B Hyperparameter Details</h2>
<p>In continual pretraining, we select the learning rate from the set ${7 e-6,1 e-5,7 e-5}$, batch size 4 , gradient accumulation step size from the set ${64,128}$, warmup ratio 0.1 , and train the model on a cluster of 8 A100 GPUs. To fine-tune a continually pretrained checkpoint, we use the training data of each dataset separately. We select learning rate from the set ${8 e-6,1 e-5,5 e-5}$, batch size of 4 , and gradient accumulation step size 1 . To train the models we use a cluster of 8 A100 GPUs, which typically takes around 20 hours for the largest model.</p>
<h1>A A1. Did you describe the limitations of your work? 7</h1>
<p>A2. Did you discuss any potential risks of your work?
7
$\checkmark$ A3. Do the abstract and introduction summarize the paper's main claims?
1
$\square$ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B $\square$ Did you use or create scientific artifacts?</h2>
<p>Not applicable. 1
$\square$ B1. Did you cite the creators of artifacts you used?
Not applicable. 1
$\square$ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
$\square$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Not applicable. Left blank.
$\square$ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
We use Wikipedia as the data source which is a standard practice in language model pretraining
$\square$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
$\checkmark$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
3.1</p>
<p>C Did you run computational experiments?
4
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<h2>Appendix B</h2>
<p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Appendix B
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
4.1</p>
<p>C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
2</p>
<h1>D Did you use human annotators (e.g., crowdworkers) or research with human participants?</h1>
<p>Left blank.
D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://huggingface.co/datasets/ wikipedia&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
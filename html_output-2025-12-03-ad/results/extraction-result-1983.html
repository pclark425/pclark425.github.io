<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-44.html">extraction-schema-44</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of bloat control methods, diversity maintenance techniques, and their effects on program size, population diversity, and solution quality in genetic programming or evolutionary algorithms.</div>
                <p><strong>Paper ID:</strong> paper-278959835</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.21991v1.pdf" target="_blank">Bridging Fitness With Search Spaces By Fitness Supremums: A Theoretical Study on LGP</a></p>
                <p><strong>Paper Abstract:</strong> Genetic programming has undergone rapid development in recent years. However, theoretical studies of genetic programming are far behind. One of the major obstacles to theoretical studies is the challenge of developing a model to describe the relationship between fitness values and program genotypes. In this paper, we take linear genetic programming (LGP) as an example to study the fitness-to-genotype relationship. We find that the fitness expectation increases with fitness supremum over instruction editing distance, considering 1) the fitness supremum linearly increases with the instruction editing distance in LGP, 2) the fitness infimum is fixed, and 3) the fitness probabilities over different instruction editing distances are similar. We then extend these findings to explain the bloat effect and the minimum hitting time of LGP based on instruction editing distance. The bloat effect happens because it is more likely to produce better offspring by adding instructions than by removing them, given an instruction editing distance from the optimal program. The analysis of the minimum hitting time suggests that for a basic LGP genetic operator (i.e., freemut), maintaining a necessarily small program size and mutating multiple instructions each time can improve LGP performance. The reported empirical results verify our hypothesis.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1983.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1983.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of bloat control methods, diversity maintenance techniques, and their effects on program size, population diversity, and solution quality in genetic programming or evolutionary algorithms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>freemut</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>freemut (add/remove random instruction mutation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple LGP genetic operator that produces offspring by either adding or removing random instructions; used in this paper as the primary variation operator for theoretical analysis and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>linear genome (sequence of register-based instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_crossover</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_mutation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_metric</strong></td>
                            <td>number of instructions</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_results</strong></td>
                            <td>Empirical observation: population average program size consistently grows during evolution despite a 50% add / 50% remove operator balance (Fig. 5). No absolute numeric sizes are reported in the text, but the trend is a monotonic increase in mean program length across generations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative Square Error (RSE) on symbolic regression benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mutating multiple instructions per application (freemut with step size u) reduces expected minimum hitting time and accelerates convergence. Tested u values: 1,3,5,7,9,12,15. Results: small u (u=1) had the worst convergence; u in the small-to-moderate range (example u=9) converged fastest and to best fitness in 3 of 4 example problems; u>10 showed diminishing/negative returns (performance slightly worse when truncation effects occur). Exact numeric RSE values are in paper tables/figures but not given verbatim in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_diversity_relationship</strong></td>
                            <td>Not explicitly quantified. Paper explains that freemut dynamics produce bloat because, given an editing distance to an optimal program, adding instructions is more likely to reduce or maintain that editing distance than removing instructions (Corollary 4.1), thus the operator dynamics push average size upward even when add/remove rates are balanced.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_performance_relationship</strong></td>
                            <td>Complex: Randomly sampled programs of larger size have worse expected fitness (Theorem 4 and Fig. 4), yet freemut's local dynamics (additions more often constructive) cause bloat during directed search because additions can more frequently reduce editing distance to optima. Also, mutating multiple instructions (larger u) improves search performance (reduced hitting time) up to a truncation point.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_performance_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>building_block_preservation</strong></td>
                            <td>Discussed qualitatively: LGP reuses intermediate results via registers (exons) and introns can protect useful code; the linear representation facilitates reuse of building blocks (intermediate register values) and structural introns are detectable, increasing potential to recover concise exons.</td>
                        </tr>
                        <tr>
                            <td><strong>neutral_variation_discussion</strong></td>
                            <td>Yes — introns are explicitly treated as neutral material. The paper distinguishes introns (neutral code) from exons, defines neutral bloating factor (Ω) for intron-driven growth, and shows neutral moves (adding introns) increase the number of programs with the same fitness and contribute to bloat.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>symbolic regression (benchmark problems: Nguyen4, Nguyen5, Nguyen7, Keijzer11, R1, Airfoil, BHouse, Tower, CCN, Redwine)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Default instruction set I (baseline) and variants of instruction sets (fx1.1, fx2, fx4, exp, add+100, add+1000); different freemut step sizes u compared to each other.</td>
                        </tr>
                        <tr>
                            <td><strong>timing_of_bloat_control</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_objective_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_bloat_diversity_executability</strong></td>
                            <td>Freemut with single-instruction mutation (u=1) is slower and finds worse solutions compared to moderate multi-instruction mutations (e.g., u=9) which reduce hitting time and often reach better fitness; nevertheless, population programs still bloat (grow) during evolution because additions more frequently reduce editing distance, even when add/remove operator rates are balanced. Randomly sampled larger programs have worse mean fitness (Fig.4), indicating a tension between local operator-driven bloat and global expected fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>contradicts_theory</strong></td>
                            <td>The paper challenges prior theoretical explanations that relied on strong assumptions (e.g., infinite populations, two-valued fitness) by providing an alternative mechanism for bloat emergence under realistic freemut dynamics; it does not produce a simple contradiction to the general idea that bloat harms parsimony, but it nuances why bloat occurs in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>Supports the idea that neutral/structural code (introns) and operator dynamics contribute to bloat and that larger genomes change the distribution of reachable fitness values; supports the notion that mutation step size affects search efficiency (larger step sizes up to a point improve hitting time).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1983.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1983.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of bloat control methods, diversity maintenance techniques, and their effects on program size, population diversity, and solution quality in genetic programming or evolutionary algorithms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>fitness-supremum model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fitness supremum based on instruction editing distance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical model that upper-bounds fitness differences by a linear function of instruction editing distance to an optimal program, connecting genotype editing distance δ to a fitness supremum and thus to expected fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>linear genome (instruction sequences); analysis depends on counting differing instructions</td>
                        </tr>
                        <tr>
                            <td><strong>uses_crossover</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_mutation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_metric</strong></td>
                            <td>number of instructions (used to define layers in the 'exploding lasagna model')</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_results</strong></td>
                            <td>Theorem 4: the expectation of editing distance to an optimal program E[δ*(ρ)] increases with program size m. Empirical verification: mean fitness (RSE) of randomly sampled programs increases (worsens) with program size m across example symbolic regression problems (Fig. 4). No absolute numeric averages printed in the body.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Expectation of fitness over program subspaces (RSE in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Model predicts and experiments confirm that search spaces with larger program size have larger (worse) expected fitness; modifying instruction sets to reduce instruction sensitivity (smaller ∆(I*,Ψ) and ∆(I^2,Ψ)) leads to better test performance (smaller RSE) across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_diversity_relationship</strong></td>
                            <td>By linking editing distance to fitness expectation, the model explains why populations move toward larger programs (bloat) under standard operators: although larger-size layers have worse global expected fitness, local supremum dynamics and similar fitness probability distributions across distances make constructive additions more probable.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_performance_relationship</strong></td>
                            <td>Smaller program size layers have better expected fitness, so maintaining small program size is beneficial; yet local operator behavior can still cause bloat because additions have a higher chance to decrease editing distance (thus locally improving fitness), producing an apparent paradox resolved by the supremum model.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_performance_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>building_block_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>neutral_variation_discussion</strong></td>
                            <td>The model incorporates neutral variation implicitly via similar fitness probability assumptions across editing distances and explicitly uses neutral bloating factors in subsequent analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>linear genetic programming (theoretical; validated on symbolic regression benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Instruction set variants (fx1.1, fx2, fx4, exp, add+100, add+1000) used to vary ∆(I*,Ψ) and ∆(I^2,Ψ); default instruction set I as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>timing_of_bloat_control</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_objective_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_bloat_diversity_executability</strong></td>
                            <td>Key insight: fitness expectation increases with instruction editing distance supremum; therefore program-size layers (m) with larger expected δ* have worse expected fitness; this theoretically backs recommendations to initialize with small programs and to encourage growth from small to large since local search dynamics more likely reduce δ* when starting small.</td>
                        </tr>
                        <tr>
                            <td><strong>contradicts_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>Supports a mechanistic explanation of how genotype neighborhood structure (editing distance) governs expected fitness, giving theoretical grounding to empirical observations about bloat and program-size effects.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1983.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1983.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of bloat control methods, diversity maintenance techniques, and their effects on program size, population diversity, and solution quality in genetic programming or evolutionary algorithms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>neutral/non-neutral bloating factors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neutral bloating factor (Ω) and non-neutral bloating factor (Λ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analytic quantities defined to quantify how the number of programs with the same fitness (neutral bloat) or changed fitness (non-neutral) increases when program size grows by adding instructions (introns/exons), with provable upper and lower bounds depending on instruction set size and registers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>linear genome (instruction sequences with registers; intron positions considered)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_crossover</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_mutation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_metric</strong></td>
                            <td>number of instructions (m1 -> m2 growth used to define Ω(m1,m2) and Λ(m1,m2))</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_results</strong></td>
                            <td>Theoretical bounds: both lower and upper bounds on Ω and Λ scale with n^(m2-m1) (n = number of possible instructions) and other combinatorial factors; neutral bloating (Ω) therefore can grow exponentially with the number of added instructions, implying rapid growth in the count of programs that are neutral variants of a given exon set as size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bloat_diversity_relationship</strong></td>
                            <td>Neutral bloating increases the count of distinct genotypes that map to the same fitness (increasing genotypic redundancy) and thus enlarges neutral networks; this increases neutral drift and contributes to bloat, as neutral additions proliferate without immediate fitness penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_performance_relationship</strong></td>
                            <td>By exponentially increasing the number of neutral variants (Ω), larger program sizes increase the chance of producing neutral offspring; while neutral offspring do not change fitness immediately, they change the search topology and can both mask conciseness and enable later constructive edits—contributes to long-term bloat dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_performance_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>building_block_preservation</strong></td>
                            <td>Implicit: neutral bloating protects existing exon code by surrounding it with many neutral variants; structural introns make concise exons easier to identify and potentially preserve.</td>
                        </tr>
                        <tr>
                            <td><strong>neutral_variation_discussion</strong></td>
                            <td>Extensively discussed: introns and neutral additions are central; paper provides Lemma 3 and Lemma 4 giving bounds on neutral (Ω) and non-neutral (Λ) bloating factors and explains how neutral variation contributes to bloat.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>linear genetic programming (analytical), validated on symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>timing_of_bloat_control</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_objective_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_bloat_diversity_executability</strong></td>
                            <td>Neutral bloat (Ω) grows combinatorially/exponentially with the number of added introns; this creates large neutral neighborhoods that increase chance of bloat even when fitness is unchanged, altering the effective search landscape and affecting how diversity and evolvability interact with program size.</td>
                        </tr>
                        <tr>
                            <td><strong>contradicts_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>Provides quantitative theoretical support to previously qualitative/empirical claims that neutral variation and introns drive bloat in LGP.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1983.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1983.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of bloat control methods, diversity maintenance techniques, and their effects on program size, population diversity, and solution quality in genetic programming or evolutionary algorithms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>small-init + multi-instruction mutation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initialization with small programs combined with larger freemut variation step size (u)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recommended procedure from the paper: initialize population with small program sizes and use multi-instruction mutation (freemut with u>1) to improve LGP performance and reduce expected hitting time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_method</strong></td>
                            <td>initialization and operator configuration (small initial program size; larger mutation step size)</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_control_mechanism</strong></td>
                            <td>Keeping initial and, where possible, maintained program sizes small reduces the expected editing distance E[δ*] and therefore reduces the fitness supremum; using larger mutation step size increases the probability of making constructive reductions in editing distance per generation, reducing minimum hitting time.</td>
                        </tr>
                        <tr>
                            <td><strong>is_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>linear genome</td>
                        </tr>
                        <tr>
                            <td><strong>uses_crossover</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_mutation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_metric</strong></td>
                            <td>number of instructions</td>
                        </tr>
                        <tr>
                            <td><strong>program_size_results</strong></td>
                            <td>Recommendation to initialize programs small (e.g., 5–20 instructions used in experiments for initializations but the paper emphasizes starting small conceptually). Empirically, population size still grows (bloat) during evolution, but small initialization yields better early-stage exploration and higher probability to reduce δ* when expanding.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative Square Error (RSE) and minimum hitting time (generations to reach optimum)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Empirical: larger variation step sizes u in freemut (tested u in {1,3,5,7,9,12,15}) increase upper bounds on constructive moving rate and decrease minimum hitting time for u in a moderate range (observed u≈9 frequently best); u too large (beyond ~10 in their tests) exhibits truncation/plateau effects and can slightly worsen performance.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_diversity_relationship</strong></td>
                            <td>Not presented as an explicit diversity-maintenance measure; rather, the recommendation aims to control the negative effects of bloat on expected fitness while accepting that bloat may still emerge due to operator dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>bloat_performance_relationship</strong></td>
                            <td>Keeping initial program size small reduces expected δ* and thus expected fitness supremum, improving starting search conditions. Increasing mutation step size accelerates reduction of δ* (improves search performance) but does not by itself prevent long-term bloat.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_performance_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>building_block_preservation</strong></td>
                            <td>Implicit: starting small favors discovering concise exon structures (building blocks) earlier; structural introns are easier to identify and so concise solutions are more accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>neutral_variation_discussion</strong></td>
                            <td>Paper notes that neutral introns remain a significant factor and that starting small helps because concisely structured exons are easier to find than redundant exon arrangements with many introns.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>linear genetic programming; symbolic regression benchmarks in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared different freemut step sizes; baseline small-step u=1 vs moderate u (3–9) vs large u (12–15). Also compared initialization ranges: 5 to 20 instructions used as initializations in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>timing_of_bloat_control</strong></td>
                            <td>Initialization (start-of-run) and operator configuration applied continuously during evolution (i.e., fixed operator step-size throughout runs).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_objective_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_bloat_diversity_executability</strong></td>
                            <td>Practical guideline: initialize populations with small program sizes to keep E[δ*] small and use moderate multi-instruction mutation (freemut with u in a moderate range, e.g., u=3..9) to improve constructive move probability and reduce hitting time; however, this does not inherently prevent long-term bloat produced by neutral additions.</td>
                        </tr>
                        <tr>
                            <td><strong>contradicts_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>Supports theoretical predictions (Theorem 5 and related numerical results) that larger mutation step sizes (within a range) increase constructive moving rate and reduce expected hitting time; supports initializing small to improve search effectiveness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neutral Variations Cause Bloat in Linear GP <em>(Rating: 2)</em></li>
                <li>Studying bloat control and maintenance of effective code in linear genetic programming for symbolic regression <em>(Rating: 2)</em></li>
                <li>Bounding bloat in genetic programming <em>(Rating: 2)</em></li>
                <li>A Schema Theory Analysis of the Evolution of Size in Genetic Programming with Linear Representations <em>(Rating: 1)</em></li>
                <li>How the Combinatorics of Neutral Spaces Leads Genetic Programming to Discover Simple Solutions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1983",
    "paper_id": "paper-278959835",
    "extraction_schema_id": "extraction-schema-44",
    "extracted_data": [
        {
            "name_short": "freemut",
            "name_full": "freemut (add/remove random instruction mutation)",
            "brief_description": "A simple LGP genetic operator that produces offspring by either adding or removing random instructions; used in this paper as the primary variation operator for theoretical analysis and experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "bloat_control_method": null,
            "bloat_control_mechanism": null,
            "is_adaptive": false,
            "adaptation_mechanism": null,
            "representation_type": "linear genome (sequence of register-based instructions)",
            "uses_crossover": false,
            "uses_mutation": true,
            "program_size_metric": "number of instructions",
            "program_size_results": "Empirical observation: population average program size consistently grows during evolution despite a 50% add / 50% remove operator balance (Fig. 5). No absolute numeric sizes are reported in the text, but the trend is a monotonic increase in mean program length across generations.",
            "diversity_metric": null,
            "diversity_results": null,
            "performance_metric": "Relative Square Error (RSE) on symbolic regression benchmarks",
            "performance_results": "Mutating multiple instructions per application (freemut with step size u) reduces expected minimum hitting time and accelerates convergence. Tested u values: 1,3,5,7,9,12,15. Results: small u (u=1) had the worst convergence; u in the small-to-moderate range (example u=9) converged fastest and to best fitness in 3 of 4 example problems; u&gt;10 showed diminishing/negative returns (performance slightly worse when truncation effects occur). Exact numeric RSE values are in paper tables/figures but not given verbatim in the text.",
            "bloat_diversity_relationship": "Not explicitly quantified. Paper explains that freemut dynamics produce bloat because, given an editing distance to an optimal program, adding instructions is more likely to reduce or maintain that editing distance than removing instructions (Corollary 4.1), thus the operator dynamics push average size upward even when add/remove rates are balanced.",
            "bloat_performance_relationship": "Complex: Randomly sampled programs of larger size have worse expected fitness (Theorem 4 and Fig. 4), yet freemut's local dynamics (additions more often constructive) cause bloat during directed search because additions can more frequently reduce editing distance to optima. Also, mutating multiple instructions (larger u) improves search performance (reduced hitting time) up to a truncation point.",
            "diversity_performance_relationship": null,
            "building_block_preservation": "Discussed qualitatively: LGP reuses intermediate results via registers (exons) and introns can protect useful code; the linear representation facilitates reuse of building blocks (intermediate register values) and structural introns are detectable, increasing potential to recover concise exons.",
            "neutral_variation_discussion": "Yes — introns are explicitly treated as neutral material. The paper distinguishes introns (neutral code) from exons, defines neutral bloating factor (Ω) for intron-driven growth, and shows neutral moves (adding introns) increase the number of programs with the same fitness and contribute to bloat.",
            "problem_characteristics": null,
            "problem_domain": "symbolic regression (benchmark problems: Nguyen4, Nguyen5, Nguyen7, Keijzer11, R1, Airfoil, BHouse, Tower, CCN, Redwine)",
            "comparison_baseline": "Default instruction set I (baseline) and variants of instruction sets (fx1.1, fx2, fx4, exp, add+100, add+1000); different freemut step sizes u compared to each other.",
            "timing_of_bloat_control": null,
            "multi_objective_approach": false,
            "key_findings_bloat_diversity_executability": "Freemut with single-instruction mutation (u=1) is slower and finds worse solutions compared to moderate multi-instruction mutations (e.g., u=9) which reduce hitting time and often reach better fitness; nevertheless, population programs still bloat (grow) during evolution because additions more frequently reduce editing distance, even when add/remove operator rates are balanced. Randomly sampled larger programs have worse mean fitness (Fig.4), indicating a tension between local operator-driven bloat and global expected fitness.",
            "contradicts_theory": "The paper challenges prior theoretical explanations that relied on strong assumptions (e.g., infinite populations, two-valued fitness) by providing an alternative mechanism for bloat emergence under realistic freemut dynamics; it does not produce a simple contradiction to the general idea that bloat harms parsimony, but it nuances why bloat occurs in practice.",
            "supports_theory": "Supports the idea that neutral/structural code (introns) and operator dynamics contribute to bloat and that larger genomes change the distribution of reachable fitness values; supports the notion that mutation step size affects search efficiency (larger step sizes up to a point improve hitting time).",
            "uuid": "e1983.0"
        },
        {
            "name_short": "fitness-supremum model",
            "name_full": "Fitness supremum based on instruction editing distance",
            "brief_description": "A theoretical model that upper-bounds fitness differences by a linear function of instruction editing distance to an optimal program, connecting genotype editing distance δ to a fitness supremum and thus to expected fitness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "bloat_control_method": null,
            "bloat_control_mechanism": null,
            "is_adaptive": null,
            "adaptation_mechanism": null,
            "representation_type": "linear genome (instruction sequences); analysis depends on counting differing instructions",
            "uses_crossover": null,
            "uses_mutation": true,
            "program_size_metric": "number of instructions (used to define layers in the 'exploding lasagna model')",
            "program_size_results": "Theorem 4: the expectation of editing distance to an optimal program E[δ*(ρ)] increases with program size m. Empirical verification: mean fitness (RSE) of randomly sampled programs increases (worsens) with program size m across example symbolic regression problems (Fig. 4). No absolute numeric averages printed in the body.",
            "diversity_metric": null,
            "diversity_results": null,
            "performance_metric": "Expectation of fitness over program subspaces (RSE in experiments)",
            "performance_results": "Model predicts and experiments confirm that search spaces with larger program size have larger (worse) expected fitness; modifying instruction sets to reduce instruction sensitivity (smaller ∆(I*,Ψ) and ∆(I^2,Ψ)) leads to better test performance (smaller RSE) across benchmarks.",
            "bloat_diversity_relationship": "By linking editing distance to fitness expectation, the model explains why populations move toward larger programs (bloat) under standard operators: although larger-size layers have worse global expected fitness, local supremum dynamics and similar fitness probability distributions across distances make constructive additions more probable.",
            "bloat_performance_relationship": "Smaller program size layers have better expected fitness, so maintaining small program size is beneficial; yet local operator behavior can still cause bloat because additions have a higher chance to decrease editing distance (thus locally improving fitness), producing an apparent paradox resolved by the supremum model.",
            "diversity_performance_relationship": null,
            "building_block_preservation": null,
            "neutral_variation_discussion": "The model incorporates neutral variation implicitly via similar fitness probability assumptions across editing distances and explicitly uses neutral bloating factors in subsequent analysis.",
            "problem_characteristics": null,
            "problem_domain": "linear genetic programming (theoretical; validated on symbolic regression benchmarks)",
            "comparison_baseline": "Instruction set variants (fx1.1, fx2, fx4, exp, add+100, add+1000) used to vary ∆(I*,Ψ) and ∆(I^2,Ψ); default instruction set I as baseline.",
            "timing_of_bloat_control": null,
            "multi_objective_approach": false,
            "key_findings_bloat_diversity_executability": "Key insight: fitness expectation increases with instruction editing distance supremum; therefore program-size layers (m) with larger expected δ* have worse expected fitness; this theoretically backs recommendations to initialize with small programs and to encourage growth from small to large since local search dynamics more likely reduce δ* when starting small.",
            "contradicts_theory": null,
            "supports_theory": "Supports a mechanistic explanation of how genotype neighborhood structure (editing distance) governs expected fitness, giving theoretical grounding to empirical observations about bloat and program-size effects.",
            "uuid": "e1983.1"
        },
        {
            "name_short": "neutral/non-neutral bloating factors",
            "name_full": "Neutral bloating factor (Ω) and non-neutral bloating factor (Λ)",
            "brief_description": "Analytic quantities defined to quantify how the number of programs with the same fitness (neutral bloat) or changed fitness (non-neutral) increases when program size grows by adding instructions (introns/exons), with provable upper and lower bounds depending on instruction set size and registers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "bloat_control_method": null,
            "bloat_control_mechanism": null,
            "is_adaptive": null,
            "adaptation_mechanism": null,
            "representation_type": "linear genome (instruction sequences with registers; intron positions considered)",
            "uses_crossover": null,
            "uses_mutation": true,
            "program_size_metric": "number of instructions (m1 -&gt; m2 growth used to define Ω(m1,m2) and Λ(m1,m2))",
            "program_size_results": "Theoretical bounds: both lower and upper bounds on Ω and Λ scale with n^(m2-m1) (n = number of possible instructions) and other combinatorial factors; neutral bloating (Ω) therefore can grow exponentially with the number of added instructions, implying rapid growth in the count of programs that are neutral variants of a given exon set as size increases.",
            "diversity_metric": null,
            "diversity_results": null,
            "performance_metric": null,
            "performance_results": null,
            "bloat_diversity_relationship": "Neutral bloating increases the count of distinct genotypes that map to the same fitness (increasing genotypic redundancy) and thus enlarges neutral networks; this increases neutral drift and contributes to bloat, as neutral additions proliferate without immediate fitness penalty.",
            "bloat_performance_relationship": "By exponentially increasing the number of neutral variants (Ω), larger program sizes increase the chance of producing neutral offspring; while neutral offspring do not change fitness immediately, they change the search topology and can both mask conciseness and enable later constructive edits—contributes to long-term bloat dynamics.",
            "diversity_performance_relationship": null,
            "building_block_preservation": "Implicit: neutral bloating protects existing exon code by surrounding it with many neutral variants; structural introns make concise exons easier to identify and potentially preserve.",
            "neutral_variation_discussion": "Extensively discussed: introns and neutral additions are central; paper provides Lemma 3 and Lemma 4 giving bounds on neutral (Ω) and non-neutral (Λ) bloating factors and explains how neutral variation contributes to bloat.",
            "problem_characteristics": null,
            "problem_domain": "linear genetic programming (analytical), validated on symbolic regression",
            "comparison_baseline": null,
            "timing_of_bloat_control": null,
            "multi_objective_approach": false,
            "key_findings_bloat_diversity_executability": "Neutral bloat (Ω) grows combinatorially/exponentially with the number of added introns; this creates large neutral neighborhoods that increase chance of bloat even when fitness is unchanged, altering the effective search landscape and affecting how diversity and evolvability interact with program size.",
            "contradicts_theory": null,
            "supports_theory": "Provides quantitative theoretical support to previously qualitative/empirical claims that neutral variation and introns drive bloat in LGP.",
            "uuid": "e1983.2"
        },
        {
            "name_short": "small-init + multi-instruction mutation",
            "name_full": "Initialization with small programs combined with larger freemut variation step size (u)",
            "brief_description": "A recommended procedure from the paper: initialize population with small program sizes and use multi-instruction mutation (freemut with u&gt;1) to improve LGP performance and reduce expected hitting time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "bloat_control_method": "initialization and operator configuration (small initial program size; larger mutation step size)",
            "bloat_control_mechanism": "Keeping initial and, where possible, maintained program sizes small reduces the expected editing distance E[δ*] and therefore reduces the fitness supremum; using larger mutation step size increases the probability of making constructive reductions in editing distance per generation, reducing minimum hitting time.",
            "is_adaptive": false,
            "adaptation_mechanism": null,
            "representation_type": "linear genome",
            "uses_crossover": false,
            "uses_mutation": true,
            "program_size_metric": "number of instructions",
            "program_size_results": "Recommendation to initialize programs small (e.g., 5–20 instructions used in experiments for initializations but the paper emphasizes starting small conceptually). Empirically, population size still grows (bloat) during evolution, but small initialization yields better early-stage exploration and higher probability to reduce δ* when expanding.",
            "diversity_metric": null,
            "diversity_results": null,
            "performance_metric": "Relative Square Error (RSE) and minimum hitting time (generations to reach optimum)",
            "performance_results": "Empirical: larger variation step sizes u in freemut (tested u in {1,3,5,7,9,12,15}) increase upper bounds on constructive moving rate and decrease minimum hitting time for u in a moderate range (observed u≈9 frequently best); u too large (beyond ~10 in their tests) exhibits truncation/plateau effects and can slightly worsen performance.",
            "bloat_diversity_relationship": "Not presented as an explicit diversity-maintenance measure; rather, the recommendation aims to control the negative effects of bloat on expected fitness while accepting that bloat may still emerge due to operator dynamics.",
            "bloat_performance_relationship": "Keeping initial program size small reduces expected δ* and thus expected fitness supremum, improving starting search conditions. Increasing mutation step size accelerates reduction of δ* (improves search performance) but does not by itself prevent long-term bloat.",
            "diversity_performance_relationship": null,
            "building_block_preservation": "Implicit: starting small favors discovering concise exon structures (building blocks) earlier; structural introns are easier to identify and so concise solutions are more accessible.",
            "neutral_variation_discussion": "Paper notes that neutral introns remain a significant factor and that starting small helps because concisely structured exons are easier to find than redundant exon arrangements with many introns.",
            "problem_characteristics": null,
            "problem_domain": "linear genetic programming; symbolic regression benchmarks in experiments",
            "comparison_baseline": "Compared different freemut step sizes; baseline small-step u=1 vs moderate u (3–9) vs large u (12–15). Also compared initialization ranges: 5 to 20 instructions used as initializations in experiments.",
            "timing_of_bloat_control": "Initialization (start-of-run) and operator configuration applied continuously during evolution (i.e., fixed operator step-size throughout runs).",
            "multi_objective_approach": false,
            "key_findings_bloat_diversity_executability": "Practical guideline: initialize populations with small program sizes to keep E[δ*] small and use moderate multi-instruction mutation (freemut with u in a moderate range, e.g., u=3..9) to improve constructive move probability and reduce hitting time; however, this does not inherently prevent long-term bloat produced by neutral additions.",
            "contradicts_theory": null,
            "supports_theory": "Supports theoretical predictions (Theorem 5 and related numerical results) that larger mutation step sizes (within a range) increase constructive moving rate and reduce expected hitting time; supports initializing small to improve search effectiveness.",
            "uuid": "e1983.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neutral Variations Cause Bloat in Linear GP",
            "rating": 2
        },
        {
            "paper_title": "Studying bloat control and maintenance of effective code in linear genetic programming for symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "Bounding bloat in genetic programming",
            "rating": 2
        },
        {
            "paper_title": "A Schema Theory Analysis of the Evolution of Size in Genetic Programming with Linear Representations",
            "rating": 1
        },
        {
            "paper_title": "How the Combinatorics of Neutral Spaces Leads Genetic Programming to Discover Simple Solutions",
            "rating": 2
        }
    ],
    "cost": 0.018126249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging Fitness With Search Spaces By Fitness Supremums: A Theoretical Study on LGP
28 May 2025</p>
<p>Zhixing Huang zhixing.huang@ecs.vuw.ac.nz 0000-0001-9560-3020
for Science</p>
<p>Yi Mei yi.mei@ecs.vuw.ac.nz 0000-0003-0682-1363
for Science</p>
<p>Fangfang Zhan fangfang.zhang@ecs.vuw.ac.nz 0000-0003-4463-9538
for Science</p>
<p>Mengjie Zhang mengjie.zhang@ecs.vuw.ac.nz 0000-0003-4463-9538
for Science</p>
<p>Wolfgang Banzhaf banzhafw@msu.edu 0000-0002-6382-3245
for Science</p>
<p>Bridging Fitness With Search Spaces By Fitness Supremums: A Theoretical Study on LGP
28 May 20258CADB14244E4D55FE498CEA65A57A33EarXiv:2505.21991v1[cs.NE]Genetic ProgrammingFitness SupremumInstruction Editing DistanceBloat EffectMinimum Hitting Time
Genetic programming has undergone rapid development in recent years.However, theoretical studies of genetic programming are far behind.One of the major obstacles to theoretical studies is the challenge of developing a model to describe the relationship between fitness values and program genotypes.In this paper, we take linear genetic programming (LGP) as an example to study the fitness-to-genotype relationship.We find that the fitness expectation increases with fitness supremum over instruction editing distance, considering 1) the fitness supremum linearly increases with the instruction editing distance in LGP, 2) the fitness infimum is fixed, and 3) the fitness probabilities over different instruction editing distances are similar.We then extend these findings to explain the bloat effect and the minimum hitting time of LGP based on instruction editing distance.The bloat effect happens because it is more likely to produce better offspring by adding instructions than by removing them given an instruction editing distance from the optimal program.The analysis of the minimum hitting time suggests that for a basic LGP genetic operator (i.e., freemut), maintaining a necessarily small program size and mutating multiple instructions each time can improve LGP performance.The reported empirical results verify our hypothesis.</p>
<p>Introduction</p>
<p>Genetic programming (GP) is a representative of stochastic symbolic search methods.GP applies genetic operators to manipulate symbolic solutions based on given primitives and predefined program structures (e.g., tree structures).GP has been widely applied to knowledge discovery Huang et al. (2022b); Qu et al. (2023); Al-Helali et al. (2024), image classification Fan et al. (2024); Sun and Zhang (2024); Haut et al. (2024), and combinatorial optimization problems Shi et al. (2022); Huang et al. (2024); Zeiträg et al. (2022).Despite its success, most understanding of GP behavior is derived empirically and from experiments.For example, we usually initialize GP programs with small program sizes and encourage GP to increase program size during the search.However, there is no theoretical explanation for these recommended designs.We are also not sure why and when these recommended designs are effective.It would be better to understand GP behaviors from a theoretical perspective first, before moving on to large-scale experiments.One of the obstacles in theoretical studies of GP is the weak causal link between fitness values and search space configurations.A small move in the search space might lead to a huge difference in fitness values.This weak causal link renders modeling the relationship between fitness values and search spaces very hard.</p>
<p>In this paper, we analyze GP behaviors by fitness supremums (i.e., the possible worst case of fitness values).Specifically, the expectation of GP fitness increases with the fitness supremum when:</p>
<ol>
<li>the fitness infimum is fixed (e.g., 0 in most cases of this paper as we focus on minimization problems), 2. the probability of sampling programs with the same fitness value is similar when the fitness supremum increases.</li>
</ol>
<p>In this case, fitness expectation increases with fitness supremum as there is likely more larger fitness values.With this in mind, we prove that 1) the fitness supremum in GP has a simple relationship with genotype editing distance, and 2) the probability of sampling programs with the same fitness value over different genotype editing distances are similar.In other words, the increment of fitness supremum defined by genotype editing distance implies the increment of fitness expectation.This provides a perspective for analyzing fitness values via genotype editing distance and allows to connect fitness values with search spaces.The empirical results show that the proposed model based on fitness supremums consistently explains the bloat effect and the choice of variation step size in GP.</p>
<p>Specifically, we take linear genetic programming (LGP) as an example to perform theoretical analyses.LGP is one of the popular variants of GP methods Nordin (1994); Brameier and Banzhaf (2007).The most prominent features of LGP are its linear representation of register-based instructions and the sequential execution of these instructions.The linear representation of LGP allows us to model LGP individuals as vectors of instructions and to count the variation of LGP programs (e.g., the number of different instructions) straightforwardly.There have been a number of studies taking LGP as an example to analyze GP evolution.For example, Hu et al.Hu et al. (2011Hu et al. ( , 2012Hu et al. ( , 2013)); Hu and Banzhaf (2018) analyzed the robustness and evolvability in LGP.</p>
<p>Here, we focus on understanding LGP behavior based on the fitness supremum and instruction editing distance.There are three main contributions.First, we develop a linear model between the fitness supremum and the instruction editing distance in Section 3, showing that a small instruction editing distance implies a small fitness supremum.Second, we analyze the distribution of fitness supremums over the entire search space based on the instruction editing distance in Section 4. The distribution of the instruction editing distance theoretically backs up the strategy of searching from small to large programs and explains the bloat effect in LGP.Third, we analyze the expected minimum hitting time of LGP search by the reduction of instruction editing distance based on a basic genetic operator in Section 5. Our model suggests that Program(x 0 , x 1 ) =</p>
<p>(1/x 1 +1)x 0 2 Ins0:</p>
<p>Ins1:</p>
<p>Ins2:</p>
<p>Ins3:</p>
<p>Figure 1: An LGP example composed of four instructions (from Ins0 to Ins3).Ins2 highlighted by "//" is an intron that does not affect the program output.</p>
<p>reasonably large variation step sizes are beneficial for LGP with the basic genetic operator, further verified by empirical results.In the rest of this paper, Section 2 introduces the basic concepts of LGP, and Section 6 summarizes the conclusions.The proofs of lemmas are given in Appendix 7.</p>
<p>Preliminary</p>
<p>2.1 Individual Representation</p>
<p>Linear Representation</p>
<p>The representation of LGP individuals is a sequence of register-based instructions.Fig. 1 is an example of an LGP individual.Using the first instruction in Fig. 1 as an example, each instruction σ consists of three parts: destination register (σ des = R3), function (σ f un = /), and source registers (σ src = {R0, x 1 }).The source registers specify the input of the function.The function accepts the values in source registers, executes, and stores the results in the destination register.For basic LGP using binary functions, each instruction has one destination register, one function, and two source registers.These registers and functions are given beforehand, and all the possible instructions form a combinatorial set of instructions I.</p>
<p>An LGP program executes these instructions sequentially to form a computer program.Before execution of the instruction sequence, LGP performs pre-execution operations such as register initialization.The source register initialization assigns registers input features or constants.In Fig. 1, LGP initializes registers by input features x 0 and x 1 alternatively, which is a common initialization strategy in LGP Brameier and Banzhaf (2007); Huang et al. (2022a).Then, LGP executes the instructions one by one.After that, LGP performs post-execution operations based on the designated output registers.Here, LGP outputs the value in R0 as the final output.Note that LGP can naturally have more than one output if we designate multiple output registers.</p>
<p>The linear representation of LGP has a number of advantages over the tree structures of basic GP.For example, the linear representation facilitates the reusing of intermediate results (i.e., building blocks) in LGP.Intermediate results are stored in registers.LGP easily reuses these results by taking them as source registers.In addition, the linear representation is very similar to many computer languages (e.g., C++ and Python), which is an efficient coding style.</p>
<p>Introns and Exons</p>
<p>LGP individuals output the final results by output registers.However, not all the instructions in an LGP individual contribute to the final output.An instruction that does not contribute to the final output is called "intron".In contrast, an instruction that contributes to the final output is called "exon".In Fig. 1, the third instruction commented with "//" is an intron, all other instructions are exons.</p>
<p>The possible number of introns increases with instruction positions Brameier and Banzhaf (2007).Suppose an LGP program manipulates a register set of size γ, among which there are γ out output registers.Then the possible number of introns grows from the first to the last instruction, from 0, n γ , 2n γ , ..., to (γ−γout)n γ , where n is the total number of instructions.Particularly, n γ is the number of possible introns when there are γ − 1 effective destination registers, and (γ−γout)n γ is the number of possible introns when there are γ out effective destination registers.Effective destination registers are those registers contributing to the final output.Note that we do not know exactly which instruction is an intron unless a specific program is given, but mainly models the growth of the number of introns here.In Fig. 1, the LGP program has four registers R0 to R3 in the primitive set and has one output register (γ = 4, γ out = 1).Therefore, the number of possible introns at the position of the fourth instruction is (γ−γout)n γ = 3n 4 .At the positions of the second and third instruction, the number of possible introns is n 2 since there are two effective destination registers R0 and R1 given by the source registers of the final instruction.</p>
<p>Four Levels of Search Information</p>
<p>LGP has four levels of search information, i.e., genotype, phenotype, semantics, and fitness values.The genotype is the raw program representation that directly defines the search space of LGP search (i.e., instruction sequences).The phenotype is the effective part of the genotype (i.e., {genotype} \ {introns}) or the observable representation of the genotype (e.g., a directed acyclic graph).The semantics is the behavior of an LGP program.The semantics of LGP programs is usually the output of an LGP program given a certain input.The fitness value is a judging metric over LGP program behavior.The fitness value defines the optimization objective in a straightforward manner.</p>
<p>The semantics of LGP programs is defined as a vector.Given A different inputs, the semantics of an LGP program is defined as a A × (γ + B) dimensional vector, where γ is the total number of registers and B is the total number of input features.The input features are kept unchanged (e.g., x 0 and x 1 in Fig. 1), while the register values change with program execution.Fig. 2 is an example of semantics over executing the LGP program of Fig. 1.After initializing registers based on the given input, we have a 6-dimensional input semantic vector, where the first four elements are register values, and the last two elements are input features.The first instruction accepts the input semantics and outputs its results to the fourth register R3 (i.e., 2/3 = 0.67).After all instructions are executed, the LGP program outputs the final semantics vector.There is a target semantics for approximation by the LGP system, although it may be unknown in some cases (e.g., unsupervised problems).Fig. 2 right shows a schematic diagram of the semantic movement, where the black dot is the given input semantics, the white dots are the output semantics of each instruction, and the black star is the target semantics.</p>
<p>Fitness Supremums on LGP 3, 2, 3, 2, 3]   [2, 3, 2, 0.67, 2, 3]   [2, 2 .67, 2, 0.67, 2, 3] [2, 2.67, 1.33, 0.67, 2, 3] [5.34, 2.67, 1.33, 0.67, 2, 3]
R0 = R0 × R1 //R2 = R0 − R3 R1 = R3 + x 0 R3 = R0 / x 1 Ins0: Ins1: Ins2: Ins3: [2,</p>
<p>Evolutionary Framework</p>
<p>LGP searches programs based on an evolutionary framework.First, LGP initializes a population of individuals, each individual representing a program.LGP evaluates these individuals by problem-specific fitness evaluation functions.As long as the stopping criteria are not satisfied, LGP iteratively selects parents, breeds new programs by applying genetic operators to the parents, and evaluates the fitness of these new programs.Finally, LGP outputs the best individual.In this paper, LGP optimizes minimization problems, that is, smaller fitness indicates better performance, and the minimum fitness is 0.</p>
<p>The genetic operators in LGP essentially define the neighborhood of an LGP individual.</p>
<p>LGP individuals move within their neighborhood to explore the search space.There are three types of moves for LGP individuals, constructive moves, neutral moves, and destructive moves.A constructive move (con) means that an LGP individual moves closer to the optimal individual.A neutral move (neu) means that an individual remains at the same distance to the optimal individual after movement.A destructive move (des) means that an individual moves further away from the optimal one.Many genetic operators for LGP have been explored.For the sake of simplicity, this paper takes adding and removing a random instruction as the only genetic operator of LGP (known as "freemut" in Brameier and Banzhaf (2007)).Freemut is one of the simplest operators for evolving LGP.It is commonly used in investigating LGP evolution Hu and Banzhaf (2018).Freemut has some advantages for investigating LGP evolution.For example, the number of changing instructions by freemut equals the instruction editing distance, simplifying analysis.</p>
<p>Related Work</p>
<p>Existing theoretical studies of GP mainly focus on runtime analysis and the explanation of bloat effects.However, because of the weak causality between fitness values and search spaces, they have many over-strong assumptions and are far from practice.For example, in runtime analysis, existing studies focus on two well-designed problems, ORDER and MAJORITY Durrett et al. (2011);Neumann (2012); Moraglio and Mambrini (2013); Mambrini and Oliveto (2016).ORDER and MAJORITY are two spe-cial problems that evaluate GP programs without explicit program execution, just using program inspection.Specifically, the correct programs in ORDER must apply a positive terminal x i before its complement −x i .Correct programs in MAJORITY must apply all the positive terminals, and the number of positive terminals must be larger than the number of corresponding complements.By directly defining fitness based on program genotypes, ORDER and MAJORITY create strong causality between fitness values and search spaces, which facilitates theoretical analysis.In addition, Durrett et al. (2011);Neumann (2012); Moraglio and Mambrini (2013); Mambrini and Oliveto (2016) apply problem-specific genetic operators to model the variation on GP programs, which are uneasy to be extended to other domains.</p>
<p>Although some existing studies have explained the bloat effect of GP in a qualitative way or by empirical analysis Soule and Heckendorn (2002); Brameier and Banzhaf (2003); Vanneschi et al. (2010); Sotto and Melo (2016), only a few studies model the bloat effect quantitatively.Freitag and Poli Freitag McPhee and Poli (2001) explained the cause of bloat effect by comparing the expectation of program size over generations.However, the quantitative explanation of bloat effects in Freitag McPhee and Poli (2001) is based on three assumptions.First, the GP population is infinitely large to cover the whole search space and only applies crossover to breed offspring.Second, the fitness function is a two-valued function, returning either 1 or 1 + f .Third, the expectation of program size is obtainable.Given that these assumptions hardly hold in practice, the theoretical explanation of Freitag McPhee and Poli (2001) is restrictive.Doerr et al. Doerr et al. (2017) also analyzed the running time of GP based on ORDER and MAJORITY, evolved by problem-specific genetic operators.</p>
<p>To the best of our knowledge, theoretical studies of GP are not kept up with the application of GP methods.They are restricted by using strong assumptions and tricky problem-specific designs.We believe that the weak causality between fitness values and search spaces is the key reason for these limitations.Here, we relax the weak causality between fitness values and search spaces by fitness supremums.</p>
<p>Basic Definitions</p>
<p>Definition 1.Given an input semantics s 0 and a set of programs P, the semantic space Ψ(s 0 |P) is the set of output semantics of all the programs ρ in P taking s 0 as inputs.
Ψ(s 0 |P) = {ρ(s 0 )|ρ ∈ P}.
Remark.We simply assume the optimal program ρ * ∈ P, and the target output semantics s * ∈ Ψ(s 0 |P).</p>
<p>Definition 2. Given a set of optimal programs P * , I * is the instruction set of all the possible instructions in ρ * .
I * = {σ|σ ∈ ρ * , ρ * ∈ P * }.
Definition 3. Given a set of optimal programs P * , Ψ * is the semantic set that includes s 0 and all the intermediate and output semantics after sequentially executing each instruction in ρ * ∈ P * given s 0 .</p>
<p>Definition 4. Given a semantic space Ψ and an optimal semantic space Ψ * ⊆ Ψ, ∆ Ψ is the minimal upper bound on the distance between any semantics in Ψ and any semantics in Ψ * .
∆ Ψ = sup s1∈Ψ,s2∈Ψ * ||s 1 − s 2 ||.
Remark.If P contains the identity program ρ I (s) = s, then ∆ Ψ ≥ ||s * − s 0 || Definition 5. Given a semantic space Ψ and a fitness function f : Ψ → R, ∆ f (Ψ) is the minimal upper bound on the fitness difference between any one semantics in Ψ and the target semantics s * , normalized by their semantic difference.
∆ f (Ψ) = sup s∈Ψ ||f (s) − f (s * )|| ||s − s * || .
Definition 6.Given a semantic space Ψ and the instruction set of optimal programs I * , ∆ (I * ,Ψ) is the minimal upper bound on the change in the difference between any two semantics in Ψ by applying any single instruction in I * to them.
∆ (I * ,Ψ) = sup s1,s2∈Ψ, σ∈I * (||σ(s 1 ) − σ(s 2 )|| − ||s 1 − s 2 ||).
Definition 7. Given a semantic space Ψ, a set of instructions I so that σ(s) ∈ Ψ (∀s ∈ Ψ, σ ∈ I), and a set of instructions of optimal programs I * ⊆ I, ∆ (I 2 ,Ψ) is the minimal upper bound on the difference between the two differences: the difference between any two semantics in Ψ and the difference after applying any instruction in I and any instruction in I * to them, respectively.
∆ (I 2 ,Ψ) = sup s1,s2∈Ψ, σ1∈I,σ2∈I * (||σ 1 (s 1 ) − σ 2 (s 2 )|| − ||s 1 − s 2 ||).
Lemma 1.Given a semantic space Ψ and a set of instructions I, we have 0 ≤ ∆ (I * ,Ψ) ≤ ∆ (I 2 ,Ψ) .(For proof refer to Appendix 7.1)</p>
<p>The notations used in this paper are summarized in Table 1.We will introduce their meanings when they first occur as well.</p>
<p>Smaller Instruction Editing Distance Implies Smaller Fitness Supremum</p>
<p>This section first develops a linear relationship between instruction editing distance δ and fitness supremums.Second, we prove that the probability of fitness values over different instruction editing distance δ ρ1,ρ2 is similar when I and f (Ψ) are well designed.The empirical results verify the relationship.</p>
<p>Fitness Supremums and Fitness Distributions</p>
<p>Theorem 1 (Supremums on Fitness Gaps).Given an LGP search space
P I,L := {σ L • σ L−1 • • • • • σ 1 (•) | σ 1 , . . . , σ L ∈ I},
and its corresponding semantic space Ψ, where I is the set of instructions and L is the maximal program length, a set of optimal LGP programs P * I * ,L and its corresponding semantic space Ψ * where I * is the set of all the possible instructions in P * , an input semantics s 0 , and a fitness function f (•), then the fitness gap between any LGP program ρ ∈ P I,L and any program ρ * ∈ P * I * ,L has the following supremums.
|f (ρ) − f (ρ * )| ≤ ∆ f (Ψ) (∆ (I 2 ,Ψ) δ ρ,ρ * + ∆ (I * ,Ψ) (L − δ ρ,ρ * )),(1)|f (ρ) − f (ρ * )| ≤ ∆ f (Ψ) ∆ Ψ ,(2)
where δ ρ,ρ * stands for the number of different instructions between ρ and ρ * , i.e., their editing distance.The target semantics.s 0</p>
<p>The input semantics.σ An instruction.σ(s) is the output semantics by instruction σ given s.</p>
<p>I</p>
<p>The set of all possible instructions.n</p>
<p>The size of I, |I| = n.</p>
<p>Ψ</p>
<p>The semantic space given an instruction set I and problem inputs.
ρ A program, represented as a sequence of instructions, i.e., ρ = σ m • σ m−1 • • • • • σ 1 (•),
where m is the number of instructions (program size).P An LGP search space, which is essentially a set of programs.ρ *</p>
<p>The optimal program.|ρ|</p>
<p>The number of instructions in program ρ. ρ(s)</p>
<p>The output semantics by program ρ, given input semantics s. δ ρ1,ρ2</p>
<p>The instruction editing distance between two programs ρ 1 and ρ 2 .The instruction editing distance to an optimal program is δ
* (ρ) = δ ρ * ,ρ . f (x)
The fitness value of x. x can be a semantics s or a program (genotype) ρ. m</p>
<p>The program size.m * is the smallest program size that represents the optimal solution ρ * .For a specific problem, there is a predefined maximum program size L. γ</p>
<p>The number of registers.γ out denotes the number of output registers.ϕ</p>
<p>The program size of accessible optimal programs.Ω(m 1 , m 2 ) The neutral bloating factor of an LGP program bloating from program size m 1 to m 2 .Λ(m 1 , m 2 ) The non-neutral bloating factor of an LGP program bloating from program size m 1 to m 2 .η</p>
<p>The normalization factor of duplicated programs after variation.</p>
<p>Proof.Denote ρ as ρ 1 and ρ * and ρ 2 , let
ρ 1 = σ 1,L •• • ••σ 1,1 (•), and ρ 2 = σ 2,L •• • ••σ 2,1 (•).
Given the input semantics s 0 , let s ij be the output semantics after the jth (j = 1, . . ., L) instruction of ρ i (i = 1, 2).</p>
<p>For the jth instruction, there are two possible situations:
(a) If σ 1,j = σ 2,j , then ||s 1j − s 2j || ≤ ||s 1(j−1) − s 2(j−1) || + ∆ (I * ,Ψ) (from Definition 6). (b) If σ 1,j ̸ = σ 2,j , then ||s 1j − s 2j || ≤ ||s 1(j−1) − s 2(j−1) || + ∆ (I 2 ,Ψ) (from Definition 7).
ρ 1 and ρ 2 have δ ρ,ρ * different instructions and L − δ ρ,ρ * same instructions.Therefore, after enumerating the total L instructions, the above situation (a) occurs L − δ ρ,ρ * times, and the above situation (b) occurs δ ρ,ρ * times.Therefore,
||ρ(s 0 ) − ρ * (s 0 )|| ≤ ∆ (I 2 ,Ψ) δ ρ,ρ * + ∆ (I * ,Ψ) (L − δ ρ,ρ * ).
Then, from Definition 5, we have (by substituting the supremums on ||ρ(s
0 ) − ρ * (s 0 )|| as ||s − s * ||) |f (ρ) − f (ρ * )| ≤ ∆ f (Ψ) (∆ (I 2 ,Ψ) δ ρ,ρ * + ∆ (I * ,Ψ) (L − δ ρ,ρ * )),(3)
Eq. ( 1) is proven.</p>
<p>On the other hand, from Definition 5 we have
|f (ρ) − f (ρ * )| ≤ ∆ f (Ψ) ||ρ(s 0 ) − ρ * (s 0 )||.
Then, from Definition 4 we have 2) is proven.
0 ≤ ||ρ(s 0 ) − ρ * (s 0 )|| ≤ ∆ Ψ . Therefore, |f (ρ) − f (ρ * )| ≤ ∆ f (Ψ) ∆ Ψ , Eq. (
Remark.Without loss of generality, we simply assume inf(f (ρ)) = f (ρ * ) = 0 (e.g., the distance to the target semantics), then the fitness supremum of ρ is re-written as:
sup f (ρ) = ∆ f (Ψ) min{(∆ (I 2 ,Ψ) − ∆ (I * ,Ψ) )δ * (ρ) + ∆ (I * ,Ψ) L, ∆ Ψ }.(4)
From Lemma 1, we have ∆ (I 2 ,Ψ) − ∆ (I * ,Ψ) ≥ 0. Therefore, Theorem 1 (in particular Eq. ( 1)) shows that within a predefined LGP search space and its corresponding semantic space, a smaller (larger) editing distance from the optimal program ρ * implies a smaller (larger) fitness supremum.</p>
<p>Theorem 2 (Similar Fitness Probability).Assuming an LGP search space P containing the target program (ρ * ∈ P) is given, for any editing distance d, let the program subspace P d = {ρ ∈ P|δ ρ,ρ * ≤ d}, and for any fitness v ∈ f
(P d ) = {f (ρ)|ρ ∈ P d }, let the program subspace P (d,v) = {ρ ∈ P d |f (ρ) = v}, then for any distances d ≥ d 0 &gt; 0 and v ∈ f (P d0 ), if ||P (d+1,v) \ P (d,v) || ||P d+1 \ P d || ≤ ||P (d,v) || ||P d || + ||P (d,v) || ||P d+1 \ P d || ,(5)
then we have
||P (d+1,v) || ||P d+1 || − ||P (d,v) || ||P d || &lt; ||P (d,v) || ||P d || ,(6)
where ||P|| stands for the number of programs in the program space P.</p>
<p>Proof.First, we have
||P (d+1,v) || ||P d+1 || − ||P (d,v) || ||P d || = ||P (d+1,v) || • ||P d || − ||P (d,v) || • ||P d+1 || ||P d+1 || • ||P d || = (||P (d+1,v) \ P (d,v) || + ||P (d,v) ||) • ||P d || − ||P (d,v) || • ||P d+1 || ||P d+1 || • ||P d || = ||P (d+1,v) \ P (d,v) || • ||P d || − ||P (d,v) || • ||P d+1 \ P d || ||P d+1 || • ||P d || .
In addition, since
||P (d+1,v) \ P (d,v) || ||P d+1 \ P d || ≤ ||P (d,v) || ||P d || + ||P (d,v) || ||P d+1 \ P d || ,
we have
||P (d+1,v) \ P (d,v) || • ||P d || − ||P (d,v) || • ||P d+1 \ P d || ≤ ||P (d,v) || • ||P d ||.
Therefore,
||P (d+1,v) || ||P d+1 || − ||P (d,v) || ||P d || ≤ ||P (d,v) || ||P d+1 || &lt; ||P (d,v) || ||P d || . Second, if ||P (d+1,v) || ||P d+1 || − ||P (d,v) || ||P d || &lt; 0, Evolutionary Computation Volume x, Number x Then ||P (d,v) || ||P d || − ||P (d+1,v) || ||P d+1 || = ||P (d,v) || • ||P d+1 \ P d || − ||P (d+1,v) \ P (d,v) || • ||P d || ||P d+1 || • ||P d || ≤ ||P (d,v) || • ||P d+1 \ P d || ||P d || • ||P d+1 || &lt; ||P (d,v) || ||P d || .
Overall, we have
||P (d+1,v) || ||P d+1 || − ||P (d,v) || ||P d || &lt; ||P (d,v) || ||P d || . Note that ||P d+1 || &gt; ||P d || &gt; 0||P (d,v) || ||P d || =: Pr(f (ρ) = v|δ ρ,ρ * ≤ d),
and
||P (d+1,v) \P (d,v) || ||P d+1 \P d ||
stands for the conditional probability
||P (d+1,v) \ P (d,v) || ||P d+1 \ P d || =: Pr(f (ρ) = v|δ ρ,ρ * = d + 1),
where "=:" indicates denoting the left side by the right notation.Therefore, Theorem 2 implies that when the LGP program space expands from distance d to distance d + 1 (by including more distant programs from the target program), if the probability for each fitness value does not increase by more than ||P (d,v) || ||P d+1 \P d || (Eq.( 5)), then the fitness probability over the search spaces P d+1 and P d are also similar to each other (at most by Pr(f (ρ) = v|δ ρ,ρ * ≤ d) for the probability of each fitness value).Pr(f (ρ) = v|δ ρ,ρ * ≤ d) is small enough when the fitness function has multiple possible values across the search space.For example, the R-square fitness function R 2 (ρ) ∈ [0, 1] in regression results in small enough Pr(f (ρ) = v|δ ρ,ρ * ≤ d), while an indicator function of finding optimal programs or not f (ρ) ∈ {0, 1} does not.</p>
<p>Theorem 1 shows that reducing δ ρ,ρ * is equivalent to reducing the supremum on f (ρ) based on a fixed infimum f (ρ * ) = 0. Theorem 2 shows that the fitness probability below the supremum is similar between the search spaces P d and P d+1 when the fitness function satisfies Eq. ( 5).Therefore, the increase (reduction) of δ ρ,ρ * implies the increase (reduction) of the expectation of fitness over δ ρ,ρ * , which provides a connection between fitness and search space.This enables us to analyze the LGP evolution behaviors from the genotype perspective since LGP prefers moving to search spaces with better fitness, i.e., smaller δ ρ,ρ * .</p>
<p>Verification of Fitness Supremum and Expectation</p>
<p>To verify the relationship between fitness supremum and its expectaction, we investigate the test performance of LGP with various fitness supremums, caused by different ∆ (I * ,Ψ) and ∆ (I 2 ,Ψ) , and L. We select symbolic regression problems, a typical application of LGP, as our test problems.We evolve LGP based on the recommended parameter settings in existing LGP studies Huang et al. (2022a).Specifically,
I ∪ {σ ′ |σ ′ (x) = 1.1σ(x), ∀σ(x) ∈ I} fx2 I ∪ {σ ′ |σ ′ (x) = 2σ(x), ∀σ(x) ∈ I} fx4 I ∪ {σ ′ |σ ′ (x) = 4σ(x), ∀σ(x) ∈ I} exp I ∪ {σ ′ |σ ′ (x) = exp(x)} add+100 I ∪ {σ ′ |σ ′ (x) = σ(x) + 100, ∀σ(x) ∈ I ∧ σ f un = "+ ′′ } add+1000 I ∪ {σ ′ |σ ′ (x) = σ(x) + 1000, ∀σ(x) ∈ I ∧ σ f un = "+ ′′ }
LGP initializes programs with 5 to 20 random instructions and ∆ (I 2 ,Ψ) would be large.With this in mind, we design six instruction sets, as shown in Table 2.We ensure that the search spaces based on these instruction sets must include ρ * .</p>
<p>From "fx1.1" to "fx4", we include additional functions into the default primitive set by multiplying coefficients 1.1, 2, and 4, respectively.By scaling the output of each instruction, we scale ∆ (I * ,Ψ) and ∆ (I 2 ,Ψ) by a factor of 4 in "fx4", 2 in "fx2", and 1.1 in "fx1.1".In "exp", "add+100", and "add+1000", we include one more function in the instruction set.Specifically, we include exp(x) in "exp", the instruction that adds 100 to the output for the instructions with addition in "add+100", and the instruction adding 1000 to the output for the instructions with addition in "add+1000".Given that the default instruction set I has been well designed to necessarily include ρ * by existing studies, it is hard to further reduce ∆ (I * ,Ψ) and ∆ (I 2 ,Ψ) from the default instruction set.</p>
<p>To verify GP performance, we test GP on ten benchmark problems, including Nguyen4, Nguyen5, Nguyen7, Keijzer11, R1, Airfoil, BHouse, Tower, CCN, and Redwine.</p>
<p>LGP minimizes the relative square error (RSE, i.e., fitness) of programs on these problems.</p>
<p>Table 3 shows the test RSE of LGP with different instruction sets.The best mean performance is highlighted in bold.We perform the Friedman test and Wilcoxon ranksum test with a significance level of 0.05 to analyze the results.Although "fx1.1", "fx2", and "fx4" all double the instruction set, indicating a much larger search space, "fx1.1" has statistically similar performance with I, while the other two are significantly worse than I in many cases.In addition, methods with instructions whose outputs vary greatly with different inputs (e.g., "fx4" and "exp") are worse than those with instructions whose outputs are less sensitive, in terms of the mean rank.For example, "fx4" (mean rank is 6.3) is worse than "fx2" (mean rank is 5.5), and "exp" (mean rank is 3.45) is worse than "add+100" (mean rank is 3).The results show that smaller ∆ (I * ,Ψ) and ∆ (I 2 ,Ψ) (i.e., a smaller upper bound on ∆f * ) likely imply better test performance of LGP in practice.</p>
<p>Estimation on Expectation of δ over Search Space</p>
<p>This section estimates the expectation of δ over the search space by relaxing the expectation to its upper bounds.The expectation of δ allows us to estimate the expectation of fitness values over the search space.To facilitate the understanding of our modeling, we visualize the search space of LGP and name it the "exploding lasagna model".</p>
<p>Exploding Lasagna Model</p>
<p>We divide the entire search space based on program size m.Fig. 3 shows an exploding lasagna model.The term "lasagna" indicates that there are multiple layers in the models.Each layer in the model is composed of all the possible LGP solutions with the same program size m (the same number of instructions).The term "exploding" indicates the exponentially increasing solution number in each layer.Given the total number of n different instructions, there are n m possible programs in the m-th layer.Let ρ * (m) be an optimal/target program with m instructions, and m * the size of the shortest possible optimal program, then from an optimal program ρ * (m) (m ≥ m * ), we can obtain the optimal programs in its adjacent layers (ρ * (m±1) ) by adding or removing an intron instruction, as shown in the red regions in Fig. 3. Specifically, the internal exploding lasagna model increases its program size with a bloating factor Ω(m * , m)(m ≥ m * ) (see Lemma 3).An LGP solution jumps from a layer of m to m + 1 by adding an instruction and jumps to m − 1 by removing an instruction.An</p>
<p>LGP solution is at most m * + m away from ρ * (see Lemma 2).</p>
<p>Estimation on δ Expectation over Search Space</p>
<p>As δ * (ρ) defines the fitness supremum and implies the increment of fitness expectation, estimating δ * (ρ) over the search space helps analyze the change of fitness expectation over the search space.We estimate the expectation of δ by counting the number of solutions that access ρ * with at least δ steps.Here, we focus on a case in which m * is included in the search space.</p>
<p>Let |ρ| be the number of instructions in program ρ, δ * (ρ) be the editing distance from a program ρ to its closest optimal program, P m be the set of programs with m instructions (i.distance from a program with size m to its closest target program is:
E[δ * (ρ) | ρ ∈ P m ] = ρ∈Pm δ * ρ n m = sup δ * (m) d=inf δ * (m) d n m • ||P δ * =d m ||.(7)
Lemma 2 (Infimum and Supremum on δ * ).Suppose the smallest program size that represents ρ * is m * , and LGP only uses adding or removing one random instruction as genetic operators to produce offspring.Then an LGP solution with size m has infimum and supremum on δ * (m) to access ρ * .Specifically,
inf δ * (m) = max{0, m * − m} ≤ δ * (m) ≤ m * + m = sup δ * (m) .
(For proof refer to Appendix 7.2)</p>
<p>Lemma 3 (Lower and Upper Bounds on Neutral Bloating Factor Ω(m 1 , m 2 )).The term "neutral bloat" indicates that the number of solutions with the same fitness increases when the program size increases by adding introns.Suppose that an LGP program has γ out output registers (γ out ≤ γ).When the LGP program increases its size from m 1 to m 2 , the increasing factor of the number of LGP programs by adding introns into this program is Ω(m 1 , m 2 ).Then the lower and upper bounds on the neutral bloating factor are:
γ−γout i=ω in γ m2−m1 &lt; Ω(m 1 , m 2 ) &lt; m 2 (γ − γ out + ω)(γ − γ out − ω + 1)n 2γ m2−m1 ,(8)
where ω = max{0, γ − γ out − m 1 }.(For proof refer to Appendix 7.4)</p>
<p>Remark.Lemma 3 shows that both the lower and upper bounds on Ω(m 1 , m 2 ) are a function of n m2−m1 .We can imagine that the number of optimal solutions that are generated from P 0 n −m1 where m 1 ≥ m * .This implies that finding the programs with the most concise exons (the rest of the program filled by introns, m 1 = m * ) has a much higher probability than with redundant exons (m 1 &gt; m * ) in LGP.For example, to search a target program ρ * (x) = x + 1, a concise program ρ 1 = {R0 = x + 1} is much eaiser to be found than ρ 2 = {R0 = 0 + 1; R0 = R0 + x}.This conclusion backs up the empirical observations in Banzhaf et al. (2024).</p>
<p>Furthermore, because the introns in LGP, especially the structural introns, are easily detected by a backward visiting method Brameier and Banzhaf (2007), LGP has a great potential to find concise solutions, which is essential for improving interpretability.</p>
<p>Lemma 4 (Lower and Upper Bounds on Non-neutral Bloating Factor Λ(m 1 , m 2 )).</p>
<p>The lower and upper bounds on the Λ(m 1 , m 2 ) are:
λ i=γout in γ m2−m1 &lt; Λ(m 1 , m 2 ) &lt; m2(γout+λ)(λ−γout+1)n 2γ m2−m1
where λ = min{γ, γ out + m 1 }.(For proof refer to Appendix 7.5)
Theorem 3 (Similar δ * Probability). The probability of δ * = d given a program size m (||P δ * =d m ||/n m ) is similar over different m, i.e., | ||P δ * =d m || n m − ||P δ * =d m+1 || n m+1 | ≤ ϵ,
where ϵ is a small enough positive number.Therefore,
| ||P δ * =d m || n m − ||P δ * =d m+1 || n m+1 | = 1 n m |||P δ * =d m ||− ||P δ * =d m ||Ω(m, m + 1) + ||P δ * =d±1 m ||Λ(m, m + 1) n |.
Based on Lemma 3 and Lemma 4, both Ω(m, m + 1) and Λ(m, m + 1) are linearly correlated to n as both of their upper and lower bounds are linearly correlated to n.Therefore, we have
| ||P δ * =d m || n m − ||P δ * =d m+1 || n m+1 | = 1 n m |α 1 ||P δ * =d m || + α 2 ||P δ * =d±1 m |||,
where α 1 and α 2 are coefficients of Ω(m, m + 1) and Λ(m, m + 1) by reducing n.Because n is a combinatorial number of α 1 and α 2 , and P
δ * ={d,d±1} m is a subset of P m , |α 1 ||P δ * =d m || + α 2 ||P δ * =d±1 m
||| is much smaller than n m especially when d is relatively small.Thus,
| ||P δ * =d m || n m − ||P δ * =d m+1 || n m+1 | ≤ ϵ.
Theorem 4. The expectation of δ * (ρ) increases with program size m, i.e., are similar enough and there are more possible
E[δ * (ρ) | ρ ∈ P m+1 ] − E[δ * (ρ) | ρ ∈ P m ] &gt; 0 Proof. E[δ * (ρ) | ρ ∈ P m+1 ]−E[δ * (ρ) | ρ ∈ P m ] = sup δ * (m+1) d=inf δ * (m+1) d n m+1 •||P δ * =d m+1 ||− sup δ * (m) d=inf δ * (m) d n m •||P δ * =d m ||(d in E[δ * (ρ) | ρ ∈ P m+1 ], E[δ * (ρ) | ρ ∈ P m+1 ] − E[δ * (ρ) | ρ ∈ P m ] &gt; 0.
Based on Theorem 1 to 4, we analyze the change of fitness expectation over the search space
E[f (ρ(m)]. Let E[δ * (ρ (m) )] be the abbreviation of E[δ * (ρ) | ρ ∈ P m ], we have the following chain. E[δ * (ρ (m) )] → E[sup f (ρ (m) )] → E[f (ρ (m) )]
Specifically, the expectation of δ * (ρ) increases with program size m, δ * (ρ) defines the fitness supremum on ρ with size m (sup f (ρ (m) )) based on Theorem 1.The increase of the expectation of fitness supremum implies the increase of the fitness expectation E[f (ρ (m) )] as the fitness infimum is 0 and f (ρ (m) ) distribution is similar over δ * (Theorem 2).Therefore, search space with larger program size has larger (worse) fitness expectation.</p>
<p>Empirical Verification</p>
<p>Theorem 4 gives us a way to estimate the expectation of δ * over the search space with a program size of m (i.e., E[δ * (ρ) | ρ ∈ P m ]).To verify E[δ * (ρ) | ρ ∈ P m ], we investigate the mean fitness of randomly sampled programs with different program sizes over 50 independent runs, as shown in Fig. 4. We can see that the mean fitness (RSE) of randomly sampled programs increases with the program size m in the four example symbolic regression problems (i.e., Nguyen4, Nguyen5, Keijzer11, and R1), verifying the increase of
E[δ * (ρ) | ρ ∈ P m ] and E[f (ρ (m) )].</p>
<p>Bloat Effect in LGP</p>
<p>The bloat effect is the phenomenon that GP program size increases significantly while fitness changes slightly or not at all.This section models the LGP bloat effect by Definition 8.
E[d m − δ * (ρ) | ρ ∈ P dm m+1 ] − E[d m − δ * (ρ) | ρ ∈ P dm m ] ≥ 0. Proof. Based on Lemma 2, we have inf δ * (m+1) ≤ inf δ * (m) and min{sup δ * (m+1) , d m } ≥ min{sup δ * (m) , d m }.
In other words, there are more adding items in
E[d m − δ * (ρ) | ρ ∈ P dm m+1 ] than in E[d m − δ * (ρ) | ρ ∈ P dm m ].
Then based on Theorem 3, we have
E[d m − δ * (ρ) | ρ ∈ P dm m+1 ] − E[d m − δ * (ρ) | ρ ∈ P dm m ] ≥ 0.
Remark.Corollary 4.1 implies Adding instructions is more likely to reduce and maintain δ * (ρ) than removing instructions.
E[d m − δ * (ρ) | ρ ∈ P dm m+1 ] − E[d m − δ * (ρ) | ρ ∈ P dm m−1 ] ≥ 0.
Given that the reduction of δ * (ρ) implies the reduction of fitness expectation, LGP prefers larger programs than smaller programs, that is, bloat effect.</p>
<p>Fig. 5 verifies the bloat effect.Fig. 5 shows that the program size of the LGP population consistently grows during evolution, although there is no bias in genetic operators (50% for adding instructions and 50% for removing).</p>
<p>Theorem 4 and the explanation of bloat effect back up the existing recommended initialization strategy and operator settings in LGP.First, we initialize the LGP population to a small program size so that we can have a small E[δ * (ρ) | ρ ∈ P m ].Second, we encourage LGP to search from small to large program size so that we can reduce δ * (ρ) with a higher probability.This explanation of the bloat effect also implies that it happens when we have a given editing distance d m .However, a randomly sampled LGP population (e.g., initial population) might include a wide range of editing distances.Therefore, the average program size over an LGP population might decrease in the very early stage of evolution.This phenomenon is commonly observed in existing LGP studies Huang et al. (2024).In this beginning stage of evolution, LGP is disseminating relatively good individuals across the population so that most of the selected parents in breeding have a similar editing distance.Later, bloat happens.</p>
<p>Expected Minimum Hitting Time of Optimal LGP Individuals</p>
<p>The hitting time of optimal LGP individuals indicates the running time with which LGP finds optimal individuals.This section performs a case study on the expected minimum hitting time of LGP individuals based on a basic genetic operator, mutating random instructions without further constraints (also known as freemut in Brameier and Banzhaf (2007)).Specifically, we model the LGP evolution as reducing fitness supre-
Target program ρ * : R0 = R0 × R1 //R2 = R0 − R3 R0 = R0 + 1 R0 = x 0 + 1 Ins0: Ins1: Ins2: Ins3: Program ρ: R0 = x 0 + 1 Ins0: R0 = R0 + 1 R0 = x 0 + 1 Ins0: Ins1: R0 = R0×R0 R0 = x 0 + 1 Ins0: Ins1: R1 = R0 + 1 R0 = x 0 + 1 Ins0:
Ins1:</p>
<p>Adding necessary instruction</p>
<p>Adding unnecessary instruction</p>
<p>Adding intron mums.</p>
<p>LGP hits the optimal solutions within an expected minimum hitting time if</p>
<p>LGP always moves with the upper bound on the constructive moving rate.A constructive move indicates a variation on LGP programs with a better instruction editing distance.where P (ρ,o) is the set of offspring programs that are generated by applying operator o to ρ, and
P ∆δ * =∆d (ρ,o)
is a subset of P (ρ,o) in which ∆δ * (ρ,o) = ∆d.We define the move from ρ to ρ ′ (o) to be constructive if ∆δ * (ρ,o) &lt; 0. Then the expectation of constructive moves from ρ to ρ ′ is (Definition 9):
E[∆δ * (ρ) | ∆δ * (ρ) &lt; 0] = o∈O Pr(o)   ∆d∈{all possible ∆δ * (ρ,o) &lt;0} ∆d • ||P ∆δ * =∆d (ρ,o) || ||P (ρ,o) ||   .(11)
We focus on a basic LGP genetic operator, freemut, that produces offspring by adding or removing instructions.Freemut might add (remove) necessary and unnecessary instructions and introns into (from) program ρ.Both necessary and unnecessary instructions contribut to the program output, but necessary instructions constitute the target program while unnecessary instructions do not.Fig. 6 shows an example of necessary and unnecessary instructions and introns for a specific program ρ = {R0 = x+1} when ρ * = {R0 = x + 1; R0 = R0 + 1}.</p>
<p>Lemma 5. Given an LGP individual ρ whose δ * (ρ) = a + r (i.e., accessing ρ * by adding a necessary instructions and removing r unnecessary instructions), we assume that an operator o +u that adds u instructions into ρ to produce offspring will add any number of unnecessary instructions (j) with the same probability.Then, the upper bound on the number of offspring that reduces δ * (ρ) by ∆δ * (ρ,o+u) = i is
||P ∆δ * =i (ρ,o+u) || = 1 min{⌊(u − i)/2⌋, δ * (ρ) − i} + 1 min{⌊(u−i)/2⌋,δ * (ρ)−i} j=0 δ * (ρ) i + j Λ(m + i + j, m + i + 2j)Ω(m + i + 2j, m + u),
where Λ(•) and Ω(•) are the upper bounds on Λ and Ω, respectively.(For proof refer to Appendix 7.8) Lemma 6.Given an LGP individual ρ whose δ * (ρ) = a + r (i.e., accessing ρ * by adding a necessary instructions and removing r unnecessary instructions), we assume that an operator o −u that removes u instructions from ρ to produce offspring will remove any number of unnecessary instructions (j) with the same probability.Then, the upper bound on the number of offspring that reduces δ * (ρ) by ∆δ * (ρ,o−u) = i is
||P ∆δ * =i (ρ,o−u) || = |ρ| u .
(For proof refer to Appendix 7.9) Theorem 5. When LGP only has two operators, adding and removing u random instructions, each with 50% operator rate, then the upper bound on constructive moving rate
|E[∆δ * (ρ) | ∆δ * (ρ) &lt; 0]| is: |E[∆δ * (ρ) | ∆δ * (ρ) &lt; 0]| = 1 2 1 |ρ|+u u n u I1 i1=1 i 1 ||P ∆δ * =i1 (ρ,o+u) || + I 2 (1 + I 2 ) 2 ,
where
I 1 = min{δ * (ρ), u}, I 2 = min{δ * (ρ), |ρ|, δ * (ρ)+|ρ|−1 2 , u}.
Proof.Assume that δ * (ρ) = a + r where a is the number of necessary to-add instructions and r is the number of necessary to-remove instructions.</p>
<p>1.For adding u instructions, the number of offspring
||P (ρ,o+u) || is |ρ|+u u n u .
2. For removing u instructions, the number of offspring
||P (ρ,o−u) || is |ρ| u .
Thus, substituting Lemmas 5 and 6 into Eq.( 11), i.e., replacing ||P ∆δ * =∆d (ρ,o±u) || by their upper bounds, we have the upper bound on constructive moving rate:
|E[∆δ * (ρ) | ∆δ * (ρ) &lt; 0]| = 1 2 1 |ρ|+u u n u I1 i1=1 i 1 ||P ∆δ * =i1 (ρ,o+u) || + 1 |ρ| u I2 i2=1 i 2 ||P ∆δ * =i2 (ρ,o−u) || = 1 2 1 |ρ|+u u n u I1 i1=1 i 1 ||P ∆δ * =i1 (ρ,o+u) || + I2 i2=1 i 2 ,(12)
where I 1 = min{δ * (ρ), u}, and
I 2 = min{δ * (ρ), |ρ|, δ * (ρ)+|ρ|−1 2
, u}.The proofs of I 1 and I 2 refer to Appendices 7.8 and 7.9.and set ϵ = 1 × 10 −4 .We make the following three observations:</p>
<p>1.The moving rate upper bounds and minimum hitting time is correlated to editing distance to the optimal solution (d).A large d implies a larger moving rate and a smaller hitting time.3. When the variation step size u increases from 1 to 10, we can see an increase in moving rate upper bounds and a reduction of minimum hitting time.However, when u is larger than 10, most of the probability in |E[∆δ * (ρ) | ∆δ * (ρ) &lt; 0]| are truncated to 1.We cannot see an increase or reduction.This suggests that when we consider the variation step size of freemut in a range of small values (e.g., u ∈ [1 .. 10]), a large step size u improves LGP effectiveness.</p>
<p>Empirical Verification of Theorem 5</p>
<p>We verify Theorem 5 by comparing the convergence with the expected moving rates and minimum generations.Fig. 9 shows the convergence curves of LGP evolved by freemut with u = 1, 3, 5, 7, 9, 12, 15 instructions.We can see that in the four example problems, LGP with large variation step sizes (e.g., u = 9, 12, 15) converges to better fitness within fewer generations.Particularly, the blue curve (u = 9) has the best convergence performance in three of the four problems.On the other hand, LGP with small variation step sizes (e.g., u = 1) has the worst convergence speed among the compared settings.Although in Keijzer11, LGP with u = 9 ends up with higher (worse) fitness values than u = 3, 5, the gap is marginal.</p>
<p>For u = 12, 15 where the constructive moving rate upper bounds are truncated, the convergence performance is slightly worse than u = 9.This implies that when the truncation in Theorem 5 happens, the performance of LGP might decrease with the increase of u.The empirical results confirm that, given a limited range of variation step sizes for freemut, mutating multiple instructions in freemut is better than mutating only one instruction, which is implied by Theorem 5 and its numerical results (Fig. 7  and 8).</p>
<p>Conclusions</p>
<p>In this paper, we have modeled LGP behavior theoretically by fitness supremums which are linearly correlated to instruction editing distance.This finding bridges the gap between fitness values and LGP search spaces and dispenses the problem-specific designs in existing GP theoretical analysis (e.g., focusing on unrealistic problems OR-DER and MAJORITY).We show that relaxing fitness to its supremum based on instruction editing distance is a feasible way to understand LGP behavior.</p>
<p>There are four key insights:</p>
<ol>
<li>Theorem 1 implies that a) reducing the semantic gap between input and target output semantics and b) reducing the semantic gap between different instructions accepting different input semantics lead to better fitness expectation.</li>
</ol>
<p>Fitness Supremums on LGP Proof. 1.For the infimum, ∵ when m &lt; m * , the ideal case for ρ (m) to reach ρ * is to add m * − m instructions.In this case, the instructions in ρ (m) are exactly the ones in ρ * (m * ) and are arranged in the correct order.When m ≥ m * , ρ (m) already includes ρ * (m) (i.e., δ * = 0).∴ max{0, m * − m} ≤ δ * (m) .</p>
<ol>
<li>For the supremum, ∵ the worst case for ρ (m) is that all the instructions in ρ (m) are excluded from ρ * (m) .∴ ρ (m) has to first remove all its instructions (i.e., m moves) and then add the right instructions in the right order (i.e., m * moves).∴ the minimum upper bound δ * (m) is m * + m.</li>
</ol>
<p>Infimum and Supremum on
= m + d − 2 min{ d−m * +m 2 , d, m} ≤ ϕ (m,d) ≤ m + d − 2 max{0, d − m * } = sup ϕ (m,d) .
Proof.Given an LGP program with size m, ρ (m) , denote r as the number of removing instructions, and a is the number of adding instructions, so that δ * (ρ) = d = r + a. Then we have the following three relationships:</p>
<ol>
<li>
<p>the accessible program size of ρ * is equivalent to ϕ (m,d) = m − r + a, and ϕ (m,d) must be greater than or equal to m * (m * &gt; 0).</p>
</li>
<li>
<p>the number of removing instructions r must be less than or equal to the program size m.</p>
</li>
<li>
<p>the number of adding instruction a must be less than or equal to the smallest program size of ρ * , i.e., m * .</p>
</li>
</ol>
<p>The three relationships are formulated as follows.
ϕ (m,d) = m − r + a ≥ m * &gt; 0, m ≥ r ≥ 0, m * ≥ a ≥ 0,
∴ After substituting a = d − r and r = d − a, respectively, we have:
max{0, d − m * } ≤ r ≤ min{ d − m * + m 2 , d, m}, max{0, d − m, m * + d − m 2 } ≤ a ≤ min{d, m * }.</p>
<p>Lemma 3</p>
<p>The lower and upper bounds of the neutral bloating factor is:
γ−γ out i=ω in γ m 2 −m 1 &lt; Ω(m1, m2) &lt; m 2 (γ−γ out +ω)(γ−γ out −ω+1)n 2γ m 2 −m 1 .
Proof.When an LGP program increases its size from m1 to m2 by neutral moves, the LGP program has to add m2 −m1 introns.These m2 −m1 introns allocate at different instruction positions over the program.Denote αi(i = ω, ω + 1, ..., γ − γout) as the number of instruction positions that are added intron instructions, we have
αω + αω+1 + ... + αγ−γ out = m2 − m1.
where ω denotes the minimum number of ineffective registers ω = max{1, γ − γout − m1}.</p>
<p>Different instruction positions imply different numbers of possible introns.For example, given that the LGP program has γout output registers, there are (γ−γ out )n γ instructions being introns following the last exon.These (γ−γ out )n γ instructions are composed of the instructions whose destination registers are excluded from the output register set.For the basic LGP in which each instruction has up to two source registers, there are (γ−γ out −1)n γ introns following the second last exon but preceding to the last exon.∴ For a certain number of possible introns in γ , it repeats ai times in the combination, and we have in γ α i</p>
<p>.</p>
<p>Suppose there are up to Ai instruction positions for a certain number of introns in γ to repeat αi times, ∴ there are A i α i combinations for each certain number of introns, and the total number of the maximal instruction positions for adding introns is Aω + Aω+1 + ... + Aγ−γ out = m2.</p>
<p>Based on this rule, the possible numbers of introns at each instruction position are ωn γ , (ω+1)n γ , ..., (γ−γ out )n γ where ω = max{0, γ − γout − m1}.∴ The total number of combinations of introns is defined as Ω(m1, m2).
Ω(m1, m2) = αω + αω+1 + ... + αγ−γ out = m2 − m1, Aω + Aω+1 + ... + Aγ−γ out = m2 Aω αω ωn γ αω × Aω+1 αω+1 (ω + 1)n γ α ω+1 × ... × Aγ−γ out αγ−γ out (γ − γout)n γ α γ−γ out .
1.For the lower bound, we just simply sum up the items in which only one of the αi(i = ω, ω + 1, ..., γ − γout) is non-zero, and have
Ω(m1, m2) &gt; γ−γ out i=ω A i α i in γ m 2 −m 1 . Other A j 0 jn γ α j = 1(j ̸ = i, αj = 0, A j 0 = 1). ∵ 1 ≤ A i α i , ∴ γ−γ out i=ω in γ m 2 −m 1 &lt; Ω(m1, m2).∵ A i ! (A i −α i )! = (Ai − αi + 1)(Ai − αi + 2)...Ai (i = ω, ω + 1, ..., γ − γout), ∴ A i ! (A i −α i )!
has αi multiplying items, and each item must be less than or equal to m2 since Ai ≤ m2.∴ A B has αω + αω+1 + ... + αγ−γ out = m2 − m1 multiplying items which are all ≤ m2.
∴ C 1 C 2 &lt; m m 2 −m 1 2 (m2 − m1)! where (m2 − m1)! has m2 − m1 items that are all larger than 1. ∴ C1 C2C3 ωn γ αω (ω + 1)n γ α ω+1 ... (γ − γout)n γ α γ−γ out &lt; m m 2 −m 1 2 (m2 − m1)! C ωn γ αω ... (γ − γout)n γ α γ−γ out = m m 2 −m 1 2 m2 − m1 αω, αω+1, ..., αγ−γ out ωn γ αω ... (γ − γout)n γ α γ−γ out . ∴ (Multinomial Theorem) Ω(m1, m2) &lt; m m 2 −m 1 2 αω +α ω+1 +...+α γ−γ out =m 2 −m 1 m2 − m1 αω, αω+1, ..., αγ−γ out ωn γ αω (ω + 1)n γ α ω+1 ... (γ − γout)n γ α γ−γ out = m m 2 −m 1 2 ωn γ + (ω + 1)n γ + ... + (γ − γout)n γ m 2 −m 1 = m2(γ − γout + ω)(γ − γout − ω + 1)n 2γ m 2 −m 1 .</p>
<p>Lemma 4</p>
<p>The lower and upper bounds of the non-neutral bloating factor is:
λ i=γ out in γ m 2 −m 1 &lt; Λ(m1, m2) &lt; m 2 (γ out +λ)(λ−γ out +1)n 2γ m 2 −m 1
where λ = min{γ, γout + m1}.</p>
<p>Proof.The proof is similar to Lemma 7.4, where ω and γ − γout are replaced by γout and λ, respectively.</p>
<p>Number of Unique Solutions After Removing r Instructions</p>
<p>Given ||P 0 m || unique programs, the number of unique programs after removing r instructions from each of the ρ ∈ P 0 m programs is:
||P 0 m || m r ηr ,
where ηr is the normalization factor of duplicated programs after removing.ηr has its own lower and upper bounds.</p>
<p>Number of Unique Solutions After Adding a Instructions</p>
<p>Given N unique programs with size of m, the number of unique programs after adding a instructions into each of the N program is:
N m+a a n a ηa ,
where ηa is the normalization factor of duplicated programs after adding.</p>
<p>1 &lt; ηa &lt; N m + a a .</p>
<p>Proof. 1) For the lower bound of ηa:</p>
<p>The minimum number of duplicated programs after adding is 1, which indicates no duplication.∴ 1 &lt; ηa.</p>
<p>2) For the upper bound of ηa: ∵ a unique program at most generates m+a a duplicated offspring when the newly added instructions are the same as the instructions in the program, and all the instructions in the program are the same.∵ a generated program at most has N − 1 duplicated ones that are generated from other N − 1 unique programs.∴ ηa &lt; N m+a a .</p>
<p>Lemma 5</p>
<p>Given an LGP individual ρ whose δ * (ρ) = a + r (i.e., accessing ρ * by adding a necessary instructions and removing r unnecessary instructions), we assume that an operator o+u that adds u instructions into ρ to produce offspring will add any number of unnecessary instructions (j) with the same probability.Then, the upper bound of the number of offspring that reduces δ * (ρ) by ∆δ * (ρ,o +u ) = i is ∵ the constructive moves by the operator o+u consist of three types of moves: 1) adding instructions necessary instructions, 2) adding unnecessary exons, and 3) adding introns.</p>
<p>∴ suppose that o+u constructively reduces δ * (ρ) by i, then LGP adds i + j necessary instructions, j unnecessary exons, and u − i − 2j introns, then the number of neighbors of an individual move is a i+j Λ(m + i + j, m + i + 2j)Ω(m + i + 2j, m + u) ηa ,</p>
<p>where ηa is the number of the duplicated new programs after adding u instructions.∵ i + j ≤ a and i ≥ 1,j, u − i − 2j ≥ 0 (Appendix 7.3).∴ 1 ≤ i ≤ min{a, u} and 0 ≤ j ≤ min{⌊(u − i)/2⌋, a − i}.</p>
<p>∴ we enumerate j to estimate the expected total number of neighbors that reduce δ * (ρ) by i is (based on Lemmas 3 and 4 and Appendix 7.</p>
<p>Lemma 6</p>
<p>Given an LGP individual ρ whose δ * (ρ) = a + r (i.e., accessing ρ * by adding a necessary instructions and removing r unnecessary instructions), we assume that an operator o−u that removes u instructions from ρ to produce offspring will remove any number of unnecessary instructions (j) with the same probability.Then, the upper bound of the number of offspring that reduces δ * (ρ) by ∆δ * (ρ,o −u ) = i is
||P ∆δ * =i (ρ,o −u ) || = |ρ| u .
Proof.Based on Appendix 7.3, we have 0 ≤ r ≤ min{δ * (ρ), |ρ|, δ * (ρ)+|ρ|−1 2 } by reducing m * as 1 and relaxing the lower bound.</p>
<p>∵ the constructive moves by removing u instructions consist of three types of moves: 1) removing unnecessary exons, 2) removing necessary instructions, and 3) removing introns.</p>
<p>∴ suppose that o−u constructively reduces δ * (ρ) by i, LGP removes i + j unnecessary exons, j necessary instructions, and u − i − 2j introns, where i + j ≤ r, i ≥ 1, j, u − i − 2j ≥ 0.</p>
<p>∴ 1 ≤ i ≤ min{r, u}, 0 ≤ j ≤ min{⌊(u − i)/2⌋, r − i}.</p>
<p>∴ Suppose there are Nnec necessary instructions and Nintron introns in the program.Given i and j, the number of neighbors of an LGP individual is</p>
<p>Fitness</p>
<p>Figure 2 :
2
Figure 2: The semantics of executing the LGP program in Fig. 1 given an input of [x 0 , x 1 ] = [2, 3].The manipulated registers are highlighted in blue font.The right figure is a schematic diagram of semantic movements by the example program in the 6-dimensional space.</p>
<p>Evolutionary Computation Volume x, Number xTable 3: Test relative square error (standard deviation) of LGP with different instruction</p>
<p>FitnessFigure 3 :
3
Figure 3: The exploding lasagna model for an LGP search space.</p>
<p>m * Evolutionary Computation Volume x, Number x by adding introns is exponentially increasing with program size.Specifically, the ratio of P 0 m over P m (i.e., ||P 0 m ||/n m ) for a given m is at least a ratio of</p>
<p>Proof.</p>
<p>The program set P δ * =d m+1 can be constructed from P δ * =d m , P δ * =d−1 m , and P δ * =d+1 m by adding one intron, or one wrong instruction, or one correct instruction, respectively.Then we have ||P δ * =d m+1 || = ||P δ * =d m ||Ω(m, m + 1) + ||P δ * =d±1 m ||Λ(m, m + 1).</p>
<p>9) Based on Lemma 2, inf δ * (m+1) ≤ inf δ * (m) , and sup δ * (m+1) &gt; sup δ * (</p>
<p>Figure 4 :
4
Figure 4: The mean fitness (RSE) over LGP initial populations with program size m.The shadow indicates the standard deviation of the fitness over 50 independent runs for a given program size.</p>
<p>Figure 5 :
5
Figure 5: The average program size (the number of instructions) ±std. of the population over 50 independent runs for the example problems.</p>
<p>Figure 6 :
6
Figure 6: An example of necessary and unnecessary instructions and introns.</p>
<p>Definition 9 .
9
Assume that an LGP individual ρ with size m has an initial δ * (ρ) = d.Let ρ ′ (o) be the offspring after one of the genetic operators o ∈ O is applied to ρ for once, where O is the set of genetic operators.Let ∆δ * (ρ,o) = δ * (ρ ′ (o) ) − δ * (ρ) be the difference of δ * between ρ ′ (o) and ρ.Then, the expectation of ∆δ * given a program ρ is defined as:</p>
<p>Figure 7 :
7
Figure 7: Each dot is a value of the upper bounds on the constructive moving rate of δ * given variation step size u, editing distance d, and program size m in Nguyen4.The two sub-figures show the values in two perspectives.The dark color indicates a large constructive moving rate upper bounds.</p>
<p>Figure 8 :
8
Figure 8: Each dot is a value of the minimum hitting time (number of generations) variation step size u, editing distance d, and program size m in Nguyen4.The two subfigures show the values in two perspectives.The dark color indicates a small minimum hitting time.</p>
<p>Figure 9 :
9
Figure 9: The convergence curves of LGP with adding and removing u instructions</p>
<p>∴</p>
<p>For a given m and δ * (ρ) = d, 1. ϕ (m,d) has its infimum when ρ removes the most r = min{ d−m * +m 2 , d, m} instructions.We have inf ϕ (m,d) = m + d − 2 min{ d − m * + m 2 , d, m}. 2. ϕ (m,d) has its supremum when ρ removes the least r = max{0, d − m * } instructions.We have sup ϕ (m,d) = m + d − 2 max{0, d − m * }.</p>
<p>out αγ−γ out =: C1 C2 × C3 , where C1 = Aω!Aω+1!...Aγ−γ out !, C2 = (Aω − αω)!(Aω+1 − αω+1)!...(Aγ−γ out − αγ−γ out )!, C3 = αω!αω+1!...αγ−γ out !.</p>
<p>1</p>
<p>&lt; ηr &lt; m r (γ − γout)n γ r .Evolutionary Computation Volume x, Number x Proof. 1) For the lower bound of ηr: The minimum number of duplicated programs after removing is 1, which indicates no duplication.∴ 1 &lt; ηr. 2) For the upper bound of ηr: ∵ the ||P 0 m || unique programs have the same m * instructions and m−m * different intron instructions.∴ the generated programs might be duplicated if we remove the intron instructions.∴ we have the largest number of duplicated programs when all the r removing instructions are intron instructions.∵ different positions have different numbers of possible intron instructions.∴ we have the largest number of duplicated programs when all the r instruction positions have the largest number of possible intron instructions.∴ the largest number of duplicated programs is m−m * r (γ−γ out )n γ r .∵ m − m * &lt; m, ∴ m−m *</p>
<p>||P</p>
<p>∆δ * =i (ρ,o +u ) || = 1 min{⌊(u − i)/2⌋, δ * (ρ) − i} + 1 min{⌊(u−i)/2⌋,δ * (ρ)−i} j=0 δ * (ρ) i + j Λ(m + i + j, m + i + 2j)Ω(m + i + 2j, m + u),where Λ(•) and Ω(•) are the upper bounds of Λ and Ω, respectively.Proof.Based on Appendix 7.3, we have max{0, δ * (ρ) − m} ≤ a ≤ δ * (ρ) by reducing m * and relaxing the lower and upper bounds of a.</p>
<p>7):||P ∆δ * =i (ρ,o +u ) || = min{⌊(u−i)/2⌋,a−i} j=0 Pr(j) a i + j Λ(m + i + j, m + i + 2j)Ω(m + i + 2j, m + u) ≤ min{⌊(u−i)/2⌋,δ * (ρ)−i} j=0 Pr(j) δ * (ρ) i + j Λ(m + i + j, m + i + 2j)Ω(m + i + 2j, m + u),where 1 ≤ i ≤ min{δ * (ρ), u}.∵ we assume that different values of j share the same probability, ∴ Pr(j) = 1 min{⌊(u−i)/2⌋,d−i}+1 .</p>
<p>−2j /ηr, where ηr is the number of duplicated new programs by removing instructions.∵ r + Nnec + Nintron = |ρ|.∴ we enumerate j to estimate the expected total number of neighbors that reduce δ * (ρ) by i Evolutionary Computation Volume x, Number x</p>
<p>Table 1 :
1
Notations and their meanings
xThe upper bound on x.xThe lower bound on x.sAn LGP program semantics, represented as a vector of register values.s  *</p>
<p>as they can be easily constructed by adding instructions into ρ</p>
<ul>
<li>.Remark.In Theorem 2, ||P (d,v) || ||P d || stands for the conditional probability</li>
</ul>
<p>Table 2 :
2
Manipulated Instruction Set.
NameInstruction SetdefaultIfx1.1</p>
<p>Denote the program size of ρ * that are accessible by ρ (m) within d steps as ϕ (m,d) , then ϕ (m,d) has its infimum and supremum of inf ϕ (m,d)</p>
<p>Accessible ρ * Program Sizes ϕ</p>
<p>Evolutionary Computation Volume x, Number x
. Theorem 4 implies that the fitness expectation of LGP individuals increases (getting worse) with program size. This theoretically backs up the recommended initialization strategy of small GP programs
.3.
Corollary 4.1 explains that bloat happens because it is more likely to reduce and maintain fitness by adding instructions than by removing them, given a program size and a certain editing distance to optimal solutions.
4. Theorem 5 suggests that LGP with the basic genetic operator (i.e., freemut) should use a large variation step size and keep program size small to achieve better performance.
We believe that these results strengthen the theoretical foundation of GP and provide a possible perspective (i.e., fitness supremums based on instruction editing distance) for the promotion of further theoretical studies in genetic programming.Proof of Lemmas and CorollariesLemma 1Given a semantic space Ψ and a set of instructions I, we have 0 ≤ ∆ (I * ,Ψ) ≤ ∆ (I 2 ,Ψ) .Second, by Definition 6 and Definition 7, we haveLemma 2Suppose the smallest program size that represents ρ * is m * , and LGP only uses adding or removing one random instruction as genetic operators to produce offspring.Then an LGP solution with size m, ρ (m) , has lower and upper bounds of δ * to access ρ * .Specifically,is (based on Appendix 7.6):where, u}. ∵ We assume that different values of j share the same probability, ∴ Pr(j) =
Genetic Programming for Feature Selection Based on Feature Removal Impact in High-Dimensional Symbolic Regression. B Al-Helali, Q Chen, B Xue, M Zhang, IEEE Transactions on Emerging Topics in Computational Intelligence. 2024</p>
<p>How the Combinatorics of Neutral Spaces Leads Genetic Programming to Discover Simple Solutions. W Banzhaf, T Hu, G Ochoa, Genetic Programming Theory and Practice XX. Singapore; SingaporeSpringer Nature2024</p>
<p>Neutral Variations Cause Bloat in Linear GP. M Brameier, W Banzhaf, Proceedings of European Conference on Genetic Programming. European Conference on Genetic Programming2003</p>
<p>Linear Genetic Programming. M Brameier, W Banzhaf, 2007Springer US</p>
<p>Bounding bloat in genetic programming. B Doerr, T Ötzing, J A G Lagodzinski, J Lengler, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceBerlin GermanyACM2017</p>
<p>Computational complexity analysis of simple genetic programming on two problems modeling isolated program semantics. G Durrett, F Neumann, O' Reilly, U.-M , Proceedings of the 11th workshop proceedings on Foundations of genetic algorithms. the 11th workshop proceedings on Foundations of genetic algorithmsACM2011</p>
<p>A genetic programming-based method for image classification with small training data. Knowledge-Based Systems. Q Fan, Y Bi, B Xue, M Zhang, 2024283111188</p>
<p>A Schema Theory Analysis of the Evolution of Size in Genetic Programming with Linear Representations. N Freitag Mcphee, R Poli, Proceedings of European Conference on Genetic Programming. European Conference on Genetic ProgrammingBerlin HeidelbergSpringer20012038</p>
<p>Accelerating Image Analysis Research with Active Learning Techniques in Genetic Programming. N Haut, W Banzhaf, B Punch, D Colbry, Genetic Programming Theory and Practice XX. Springer Nature2024</p>
<p>Neutrality, Robustness, and Evolvability in Genetic Programming. T Hu, W Banzhaf, Genetic Programming Theory and Practice XIV. 2018</p>
<p>Robustness and evolvability of recombination in linear genetic programming. T Hu, W Banzhaf, J H Moore, European Conference on Genetic Programming. 20137831</p>
<p>Robustness, evolvability, and accessibility in linear genetic programming. T Hu, J L Payne, W Banzhaf, J H Moore, European Conference on Genetic Programming. 2011</p>
<p>Evolutionary dynamics on multiple scales: A quantitative analysis of the interplay between genotype, phenotype, and fitness in linear genetic programming. T Hu, J L Payne, W Banzhaf, J H Moore, Genetic Programming and Evolvable Machines. 1332012</p>
<p>A Further Investigation to Improve Linear Genetic Programming in Dynamic Job Shop Scheduling. Z Huang, Y Mei, F Zhang, M Zhang, Proceedings of IEEE Symposium Series on Computational Intelligence. IEEE Symposium Series on Computational Intelligence2022a</p>
<p>Toward Evolving Dispatching Rules With Flow Control Operations By Grammar-Guided Linear Genetic Programming. Z Huang, Y Mei, F Zhang, M Zhang, IEEE Transactions on Evolutionary Computation. 2024</p>
<p>Semantic Linear Genetic Programming for Symbolic Regression. Z Huang, Y Mei, J Zhong, IEEE Transactions on Cybernetics. 5422022b</p>
<p>On the Analysis of Simple Genetic Programming for Evolving Boolean Functions. A Mambrini, P S Oliveto, Proceedings of European Conference on Genetic Programming. European Conference on Genetic ProgrammingSpringer International Publishing20169594</p>
<p>. H Zhixing, M Yi, Z Fangfang, Z Mengjie, B Wolfgang, Evolutionary Computation. </p>
<p>Runtime analysis of mutation-based geometric semantic genetic programming for basis functions regression. A Moraglio, A Mambrini, Proceedings of Genetic and evolutionary computation conference. Genetic and evolutionary computation conferenceACM2013</p>
<p>Computational complexity analysis of multi-objective genetic programming. F Neumann, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceACM2012</p>
<p>A compiling genetic programming system that directly manipulates the machine code. P Nordin, Advances in Genetic Programming. 11994</p>
<p>Automated Discovery of Efficient Behavior Strategies for Distributed Shape Formation of Swarm Robots by Genetic Programming. Y Qu, B Xin, Q Wang, M Wang, M Guo, IEEE Transactions on Automation Science and Engineering. 2023</p>
<p>A novel tree-based representation for evolving analog circuits and its application to memristor-based pulse generation circuit. X Shi, L L Minku, X Yao, Genetic Programming and Evolvable Machines. 2342022</p>
<p>Studying bloat control and maintenance of effective code in linear genetic programming for symbolic regression. L F D P Sotto, V V Melo, Neurocomputing. 1802016</p>
<p>An Analysis of the Causes of Code Growth in Genetic Programming. T Soule, R B Heckendorn, Genetic Programming and Evolvable Machines. 32002</p>
<p>Automatic Feature Construction-Based Genetic Programming for Degraded Image Classification. Y Sun, Z Zhang, Applied Sciences. 14416132024</p>
<p>Measuring bloat, overfitting and functional complexity in genetic programming. L Vanneschi, M Castelli, S Silva, Proceedings of the Genetic and evolutionary computation Conference. the Genetic and evolutionary computation Conference2010</p>
<p>Surrogate-assisted automatic evolving of dispatching rules for multi-objective dynamic job shop scheduling using genetic programming. Y Zeiträg, J R Figueira, N Horta, R Neves, Expert Systems with Applications. 2091181942022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc" target="_blank">Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter is explored, and a novel and effective prompting method, explicit \uline{c}ode-based \ULine{s}elf-\uline {v}erification~(CSV), is proposed to further boost the mathematical reasoning potential of GPN.</p>
                <p><strong>Paper Abstract:</strong> Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \textbf{(53.9\% $\to$ 84.3\%)}.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6659.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6659.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4-Code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 variant with an integrated code-generation-and-execution environment (Code Interpreter). The model generates stepwise natural language and Python code, can execute code, observe outputs, and iteratively modify its reasoning (self-debugging).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-Code</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (paper treats GPT4-Code as a closed-source OpenAI model).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH; GSM8K; MMLU-Math; MMLU-STEM</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems (arithmetic, algebra, number theory, counting & probability, precalculus, geometry), numeric computation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems (sometimes converted into symbolic/Python code by the model)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied; dataset-defined difficulty (MATH levels; includes hard university-style contest problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and controlled prompts for code usage: (Prompt 1) no code, (Prompt 2) single code block (PAL-style), (Basic) unrestricted code use, and CSV (explicit code-based self-verification); also combined with sampling (self-consistency) and verification-guided weighted majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact answer match on dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On MATH: GPT4-Code baseline 69.69% (zero-shot); with explicit CSV 73.54%; with CSV + verification-guided weighted majority voting (k=16) 84.32%. On GSM8K: GPT4-Code 92.9%; GPT4-Code + CSV + Voting (k=5) 97.0%. On MMLU-Math: GPT4-Code 87.5%; GPT4-Code + CSV + Voting 89.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper-level interpretability: code usage frequency strongly correlates with higher accuracy; GPT4-Code generates short, incremental code fragments interleaved with natural-language reasoning, executes them, inspects outputs, and performs 'self-debugging' (fixing code and reasoning after execution errors). The model frequently uses symbolic/math libraries (sympy is the most-used package). The authors computed a confusion matrix for verification states and report high verification precision (average precision ≈ 95.88%), indicating verified-True solutions are usually correct.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Failures include: code-execution errors (runtime or logic) producing implausible outputs, natural-language-only verification performs worse or can degrade performance, geometry tasks limited by multimodality needs; verification can be 'Uncertain' or False negatives/positives (verification not perfect); examples show arithmetic mistakes or incorrect code leading to wrong intermediate values before self-debugging corrects some but not all errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not a model-size scaling study; empirical trend observed: increasing 'Code Usage Frequency' (more frequent, incremental code generation + execution) improves accuracy, especially on harder problems. Verification-guided voting further amplifies gains; no analysis across model parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6659.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CSV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit Code-based Self-Verification (CSV)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot prompting technique that instructs GPT4-Code to generate code that explicitly verifies its final answer, classify the verification as True/False/Uncertain, and, if False, re-run a corrected solution loop until verified True or return Uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-Code (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (CSV is a prompting method introduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH; GSM8K; MMLU-Math; MMLU-STEM</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems and numeric verification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language problems; model produces Python code for verification</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Dataset-defined; shows larger relative gains on higher-difficulty problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot explicit instruction: 'To solve the problem using code interpreter step by step, and please verify your answer using code interpreter.' The method adds a verification stage V that outputs True/False/Uncertain and triggers iterative correction on False.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (dataset exact-match)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On MATH: CSV alone raises GPT4-Code from 69.69% → 73.54% (absolute +3.85%). With CSV + verification-guided voting: 84.32% overall. On GSM8K and MMLU-Math similarly improves results (see GPT4-Code entry).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>CSV introduces an explicit verification stage V applied via generated code, producing verification labels (True/False/Uncertain). CSV increases Code Usage Frequency and leverages code-based checks (e.g., recomputing numeric expressions, using sympy) to catch and correct errors. Verified-True answers are highly reliable per the confusion-matrix analysis. The paper shows CSV leads to more solutions reaching a verified True state, which improves weighted voting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Verification can be 'Uncertain' when the model cannot find a code-based verification; verification is not perfect (false positives and false negatives occur), and some problems (e.g., geometry requiring multimodal reasoning) gain little from CSV. If verification fails to be executed or mis-classifies correctness, it can mislead weighted voting unless weights are tuned properly.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>CSV increases code usage and accuracy; larger empirical gains on harder items. No evidence provided about dependence on model parameter scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6659.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VW-voting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verification-Guided Weighted Majority Voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of self-consistency voting that weights candidate answers by their CSV verification states (weights w_T > w_U > w_F) when aggregating multiple sampled solution paths to produce a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-Code + CSV</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (voting/aggregation technique introduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH; GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language with multiple sampled solution chains (k samples); each sample includes a verification state</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied; especially effective on harder problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Sampling multiple reasoning/code paths (self-consistency style) and aggregating answers using Score(a)=sum_v w_v * count(answer a with verification state v), with w_T>w_U>w_F. Authors experimented with k up to 16 (MATH) and k=5 (GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (dataset exact-match); also precision/recall for verification classifier</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On MATH: CSV + VW-voting (k=16) → 84.32% (from 69.69% baseline). On GSM8K: GPT4-Code + CSV + Voting (k=5) → 97.0%. Verification precision improved average precision by +22.3% and recall by +5.6% in their analysis; reported average verification precision ≈ 95.88%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors computed confusion matrices for verification results (TP/TN/FP/FN), derived precision/recall and showed high precision for True-labelled solutions. Weighting by verification state amplifies correct answers approved by code-based verification. They ablated weights w_v and found the scheme robust when w_T > w_U >= w_F; poorly chosen weights can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If verification labels are unreliable (false positives where verification says True but answer wrong) aggregation can be misled; poorly chosen weighting (e.g., w_T < w_F) can degrade performance; limited if few solutions reach True state.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance improves with number of sampled reasoning paths up to tested ranges (e.g., greater gains moving from naive majority to verification-guided voting); gains particularly strong when more solutions reach verified True.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6659.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code Usage Frequency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code Usage Frequency (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-level measure introduced in the paper counting how often the model executes code while solving a problem; used to analyze correlation between frequency of code execution and final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-Code (analysis applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A (conceptual/analytical variable introduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (main), additional analysis on other datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math reasoning and arithmetic computation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language with optional code-generation/execution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Dataset levels; correlation strongest on higher-difficulty items</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Analyzed across four prompt types: Prompt 1 (no code), Prompt 2 (single code block), Basic (unrestricted), CSV (explicit verification) to vary code usage frequency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correlation between Code Usage Frequency and accuracy; absolute accuracy gains reported for prompt comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prompt 2 (single code) gave +6.9% over Prompt 1 (no code). Basic/unrestricted and CSV with higher code usage produced substantially higher accuracy; CSV increased code usage and accuracy beyond Basic prompt (see GPT4-Code numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Higher code usage correlates positively with accuracy across subjects and difficulty levels. Mechanisms: short incremental code fragments allow precise computations and let the model 'self-debug' based on execution outputs. Harder problems required more code invocations. The model uses code both to compute and to check logical steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>More frequent code generation increases opportunities for runtime/logic errors; without self-debugging or verification, single-code strategies (PAL-like) can fail when the one code block contains an error. Also, geometry tasks benefit less from code frequency due to multimodality needs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Accuracy increases with code usage frequency, with larger benefits at higher difficulty levels; no evidence provided about model-parameter scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6659.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language model (PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously-proposed method where language models emit a single program (e.g., Python) that is executed by an external interpreter to improve numerical computation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program-aided language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PAL (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Method (program synthesis + external execution)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (method described in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used in prior work on GSM8K and similar arithmetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Numeric computation via program synthesis (single-code-block per problem)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language problem → single Python program output → external execution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied; improves computation but lacks multi-step self-debugging</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Prompt to emit one Python program that computes the answer (single-code invocation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy improvements over non-code baselines in prior work (cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper contrasts PAL's single-code-block approach with GPT4-Code's frequent, incremental code generation and self-debugging; PAL lacks internal iterative code-execution-feedback loops unless supplemented by verification.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Single-code approach can fail if the generated code has errors and no mechanism to iteratively fix it; lacks built-in verification unless combined with an external verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper; mentioned as prior art that benefits from external code execution but is less dynamic than GPT4-Code's multi-step code use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6659.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sympy (package)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>sympy (Python symbolic mathematics library)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python library for symbolic mathematics frequently used by GPT4-Code in generated verification and computation code; reported as the most-used package in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4-Code (generates code that imports/uses sympy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>N/A (Python library)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (package-usage statistics reported on MATH dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Symbolic computation / exact arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Model-generated Python code that imports and calls sympy routines</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Used across difficulties; common in problems requiring symbolic manipulation or exact arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Not a prompting method; appears in model-generated code under CSV and unconstrained prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Usage frequency fraction and co-occurrence with correct/wrong solutions reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table reports sympy used in ~41.68% of all generated code invocations on MATH; in correct-code cases sympy usage fraction ≈ 39.07%; in wrong-code cases ≈ 47.24%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>High sympy usage indicates GPT4-Code leverages symbolic routines for verification and exact computation. The fact sympy appears more often in wrong-code cases than correct-code cases may indicate that sympy is used in harder subproblems or that incorrect usage of sympy contributes to some failures; authors highlight sympy as central to many solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Incorrect use of libraries (wrong API usage or wrong logic) can lead to execution errors or wrong outputs; reliance on sympy does not guarantee correctness and can surface bugs that require self-debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed; statistics reported only as usage fractions on MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6659.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large language model (used as a baseline in the paper). Reported baseline (non-Code-Interpreter) performance on MATH and other tasks for comparison to GPT4-Code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (reported baseline), other general benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions with chain-of-thought (CoT) or other prompts</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied (MATH dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Standard zero-shot and Chain-of-Thought baselines referenced (GPT-4 CoT, GPT-4 PHP etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported GPT-4 overall accuracy on MATH: 42.20% (paper's baseline reference).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper uses GPT-4 as a contrast to GPT4-Code to show the benefit of integrated code execution; no internal mechanism analysis for GPT-4 provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower computation accuracy on arithmetic-heavy tasks relative to GPT4-Code due to lack of integrated code execution/self-debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6659.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6659.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that encourages LLMs to produce intermediate reasoning steps in natural language to improve multi-step reasoning; used as related work and baseline (Zero-shot-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT (prompting technique)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Prompting strategy for transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used in related work and baseline comparisons (GSM8K, MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning and arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language with chain-of-thought responses</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot-CoT (e.g., 'Let's think step by step'), few-shot CoT variants and self-consistency sampling discussed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy improvements relative to direct prompting reported in prior work; used as baseline in paper comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper contrasts CoT (NL chains) with code-based chains (SL chains) showing that Python-symbolic chains improve computation when combined with execution/verification.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Natural-language-only chains sometimes produce incorrect arithmetic computations; less reliable than code execution for exact numeric computation according to authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not directly studied in this paper beyond being part of baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program-aided language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6659",
    "paper_id": "paper-1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT4-Code",
            "name_full": "GPT-4 Code Interpreter (GPT4-Code)",
            "brief_description": "OpenAI's GPT-4 variant with an integrated code-generation-and-execution environment (Code Interpreter). The model generates stepwise natural language and Python code, can execute code, observe outputs, and iteratively modify its reasoning (self-debugging).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4-Code",
            "model_family": "Transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in this paper (paper treats GPT4-Code as a closed-source OpenAI model).",
            "benchmark_name": "MATH; GSM8K; MMLU-Math; MMLU-STEM",
            "task_type": "Multi-step math word problems (arithmetic, algebra, number theory, counting & probability, precalculus, geometry), numeric computation and verification",
            "problem_format": "Natural-language math word problems (sometimes converted into symbolic/Python code by the model)",
            "difficulty_level": "Varied; dataset-defined difficulty (MATH levels; includes hard university-style contest problems)",
            "prompting_method": "Zero-shot and controlled prompts for code usage: (Prompt 1) no code, (Prompt 2) single code block (PAL-style), (Basic) unrestricted code use, and CSV (explicit code-based self-verification); also combined with sampling (self-consistency) and verification-guided weighted majority voting.",
            "performance_metric": "Accuracy (exact answer match on dataset)",
            "performance_value": "On MATH: GPT4-Code baseline 69.69% (zero-shot); with explicit CSV 73.54%; with CSV + verification-guided weighted majority voting (k=16) 84.32%. On GSM8K: GPT4-Code 92.9%; GPT4-Code + CSV + Voting (k=5) 97.0%. On MMLU-Math: GPT4-Code 87.5%; GPT4-Code + CSV + Voting 89.2%.",
            "internal_analysis": "Paper-level interpretability: code usage frequency strongly correlates with higher accuracy; GPT4-Code generates short, incremental code fragments interleaved with natural-language reasoning, executes them, inspects outputs, and performs 'self-debugging' (fixing code and reasoning after execution errors). The model frequently uses symbolic/math libraries (sympy is the most-used package). The authors computed a confusion matrix for verification states and report high verification precision (average precision ≈ 95.88%), indicating verified-True solutions are usually correct.",
            "failure_modes": "Failures include: code-execution errors (runtime or logic) producing implausible outputs, natural-language-only verification performs worse or can degrade performance, geometry tasks limited by multimodality needs; verification can be 'Uncertain' or False negatives/positives (verification not perfect); examples show arithmetic mistakes or incorrect code leading to wrong intermediate values before self-debugging corrects some but not all errors.",
            "scaling_trend": "Not a model-size scaling study; empirical trend observed: increasing 'Code Usage Frequency' (more frequent, incremental code generation + execution) improves accuracy, especially on harder problems. Verification-guided voting further amplifies gains; no analysis across model parameter counts.",
            "uuid": "e6659.0",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "CSV",
            "name_full": "Explicit Code-based Self-Verification (CSV)",
            "brief_description": "A zero-shot prompting technique that instructs GPT4-Code to generate code that explicitly verifies its final answer, classify the verification as True/False/Uncertain, and, if False, re-run a corrected solution loop until verified True or return Uncertain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT4-Code (as used in experiments)",
            "model_family": "Transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not applicable (CSV is a prompting method introduced in this paper).",
            "benchmark_name": "MATH; GSM8K; MMLU-Math; MMLU-STEM",
            "task_type": "Multi-step math word problems and numeric verification",
            "problem_format": "Natural-language problems; model produces Python code for verification",
            "difficulty_level": "Dataset-defined; shows larger relative gains on higher-difficulty problems",
            "prompting_method": "Zero-shot explicit instruction: 'To solve the problem using code interpreter step by step, and please verify your answer using code interpreter.' The method adds a verification stage V that outputs True/False/Uncertain and triggers iterative correction on False.",
            "performance_metric": "Accuracy (dataset exact-match)",
            "performance_value": "On MATH: CSV alone raises GPT4-Code from 69.69% → 73.54% (absolute +3.85%). With CSV + verification-guided voting: 84.32% overall. On GSM8K and MMLU-Math similarly improves results (see GPT4-Code entry).",
            "internal_analysis": "CSV introduces an explicit verification stage V applied via generated code, producing verification labels (True/False/Uncertain). CSV increases Code Usage Frequency and leverages code-based checks (e.g., recomputing numeric expressions, using sympy) to catch and correct errors. Verified-True answers are highly reliable per the confusion-matrix analysis. The paper shows CSV leads to more solutions reaching a verified True state, which improves weighted voting.",
            "failure_modes": "Verification can be 'Uncertain' when the model cannot find a code-based verification; verification is not perfect (false positives and false negatives occur), and some problems (e.g., geometry requiring multimodal reasoning) gain little from CSV. If verification fails to be executed or mis-classifies correctness, it can mislead weighted voting unless weights are tuned properly.",
            "scaling_trend": "CSV increases code usage and accuracy; larger empirical gains on harder items. No evidence provided about dependence on model parameter scale.",
            "uuid": "e6659.1",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "VW-voting",
            "name_full": "Verification-Guided Weighted Majority Voting",
            "brief_description": "A variant of self-consistency voting that weights candidate answers by their CSV verification states (weights w_T &gt; w_U &gt; w_F) when aggregating multiple sampled solution paths to produce a final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT4-Code + CSV",
            "model_family": "Transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not applicable (voting/aggregation technique introduced here).",
            "benchmark_name": "MATH; GSM8K",
            "task_type": "Multi-step math word problems",
            "problem_format": "Natural-language with multiple sampled solution chains (k samples); each sample includes a verification state",
            "difficulty_level": "Varied; especially effective on harder problems",
            "prompting_method": "Sampling multiple reasoning/code paths (self-consistency style) and aggregating answers using Score(a)=sum_v w_v * count(answer a with verification state v), with w_T&gt;w_U&gt;w_F. Authors experimented with k up to 16 (MATH) and k=5 (GSM8K).",
            "performance_metric": "Accuracy (dataset exact-match); also precision/recall for verification classifier",
            "performance_value": "On MATH: CSV + VW-voting (k=16) → 84.32% (from 69.69% baseline). On GSM8K: GPT4-Code + CSV + Voting (k=5) → 97.0%. Verification precision improved average precision by +22.3% and recall by +5.6% in their analysis; reported average verification precision ≈ 95.88%.",
            "internal_analysis": "Authors computed confusion matrices for verification results (TP/TN/FP/FN), derived precision/recall and showed high precision for True-labelled solutions. Weighting by verification state amplifies correct answers approved by code-based verification. They ablated weights w_v and found the scheme robust when w_T &gt; w_U &gt;= w_F; poorly chosen weights can hurt performance.",
            "failure_modes": "If verification labels are unreliable (false positives where verification says True but answer wrong) aggregation can be misled; poorly chosen weighting (e.g., w_T &lt; w_F) can degrade performance; limited if few solutions reach True state.",
            "scaling_trend": "Performance improves with number of sampled reasoning paths up to tested ranges (e.g., greater gains moving from naive majority to verification-guided voting); gains particularly strong when more solutions reach verified True.",
            "uuid": "e6659.2",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Code Usage Frequency",
            "name_full": "Code Usage Frequency (concept)",
            "brief_description": "A task-level measure introduced in the paper counting how often the model executes code while solving a problem; used to analyze correlation between frequency of code execution and final accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT4-Code (analysis applied)",
            "model_family": "Transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "N/A (conceptual/analytical variable introduced in this paper).",
            "benchmark_name": "MATH (main), additional analysis on other datasets",
            "task_type": "Multi-step math reasoning and arithmetic computation",
            "problem_format": "Natural-language with optional code-generation/execution",
            "difficulty_level": "Dataset levels; correlation strongest on higher-difficulty items",
            "prompting_method": "Analyzed across four prompt types: Prompt 1 (no code), Prompt 2 (single code block), Basic (unrestricted), CSV (explicit verification) to vary code usage frequency",
            "performance_metric": "Correlation between Code Usage Frequency and accuracy; absolute accuracy gains reported for prompt comparisons",
            "performance_value": "Prompt 2 (single code) gave +6.9% over Prompt 1 (no code). Basic/unrestricted and CSV with higher code usage produced substantially higher accuracy; CSV increased code usage and accuracy beyond Basic prompt (see GPT4-Code numbers).",
            "internal_analysis": "Higher code usage correlates positively with accuracy across subjects and difficulty levels. Mechanisms: short incremental code fragments allow precise computations and let the model 'self-debug' based on execution outputs. Harder problems required more code invocations. The model uses code both to compute and to check logical steps.",
            "failure_modes": "More frequent code generation increases opportunities for runtime/logic errors; without self-debugging or verification, single-code strategies (PAL-like) can fail when the one code block contains an error. Also, geometry tasks benefit less from code frequency due to multimodality needs.",
            "scaling_trend": "Accuracy increases with code usage frequency, with larger benefits at higher difficulty levels; no evidence provided about model-parameter scaling.",
            "uuid": "e6659.3",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "PAL",
            "name_full": "Program-Aided Language model (PAL)",
            "brief_description": "A previously-proposed method where language models emit a single program (e.g., Python) that is executed by an external interpreter to improve numerical computation accuracy.",
            "citation_title": "Program-aided language models",
            "mention_or_use": "mention",
            "model_name": "PAL (method)",
            "model_family": "Method (program synthesis + external execution)",
            "model_size": null,
            "training_data_description": "Not applicable (method described in related work).",
            "benchmark_name": "Used in prior work on GSM8K and similar arithmetic datasets",
            "task_type": "Numeric computation via program synthesis (single-code-block per problem)",
            "problem_format": "Natural-language problem → single Python program output → external execution",
            "difficulty_level": "Varied; improves computation but lacks multi-step self-debugging",
            "prompting_method": "Prompt to emit one Python program that computes the answer (single-code invocation)",
            "performance_metric": "Accuracy improvements over non-code baselines in prior work (cited in paper)",
            "performance_value": null,
            "internal_analysis": "Paper contrasts PAL's single-code-block approach with GPT4-Code's frequent, incremental code generation and self-debugging; PAL lacks internal iterative code-execution-feedback loops unless supplemented by verification.",
            "failure_modes": "Single-code approach can fail if the generated code has errors and no mechanism to iteratively fix it; lacks built-in verification unless combined with an external verifier.",
            "scaling_trend": "Not analyzed in this paper; mentioned as prior art that benefits from external code execution but is less dynamic than GPT4-Code's multi-step code use.",
            "uuid": "e6659.4",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "sympy (package)",
            "name_full": "sympy (Python symbolic mathematics library)",
            "brief_description": "A Python library for symbolic mathematics frequently used by GPT4-Code in generated verification and computation code; reported as the most-used package in the experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4-Code (generates code that imports/uses sympy)",
            "model_family": "N/A (Python library)",
            "model_size": null,
            "training_data_description": "N/A",
            "benchmark_name": "MATH (package-usage statistics reported on MATH dataset)",
            "task_type": "Symbolic computation / exact arithmetic",
            "problem_format": "Model-generated Python code that imports and calls sympy routines",
            "difficulty_level": "Used across difficulties; common in problems requiring symbolic manipulation or exact arithmetic",
            "prompting_method": "Not a prompting method; appears in model-generated code under CSV and unconstrained prompts",
            "performance_metric": "Usage frequency fraction and co-occurrence with correct/wrong solutions reported",
            "performance_value": "Table reports sympy used in ~41.68% of all generated code invocations on MATH; in correct-code cases sympy usage fraction ≈ 39.07%; in wrong-code cases ≈ 47.24%.",
            "internal_analysis": "High sympy usage indicates GPT4-Code leverages symbolic routines for verification and exact computation. The fact sympy appears more often in wrong-code cases than correct-code cases may indicate that sympy is used in harder subproblems or that incorrect usage of sympy contributes to some failures; authors highlight sympy as central to many solutions.",
            "failure_modes": "Incorrect use of libraries (wrong API usage or wrong logic) can lead to execution errors or wrong outputs; reliance on sympy does not guarantee correctness and can surface bugs that require self-debugging.",
            "scaling_trend": "Not analyzed; statistics reported only as usage fractions on MATH.",
            "uuid": "e6659.5",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4 (baseline)",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's large language model (used as a baseline in the paper). Reported baseline (non-Code-Interpreter) performance on MATH and other tasks for comparison to GPT4-Code.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_family": "Transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MATH (reported baseline), other general benchmarks",
            "task_type": "Multi-step math word problems",
            "problem_format": "Natural-language questions with chain-of-thought (CoT) or other prompts",
            "difficulty_level": "Varied (MATH dataset)",
            "prompting_method": "Standard zero-shot and Chain-of-Thought baselines referenced (GPT-4 CoT, GPT-4 PHP etc.)",
            "performance_metric": "Accuracy",
            "performance_value": "Reported GPT-4 overall accuracy on MATH: 42.20% (paper's baseline reference).",
            "internal_analysis": "Paper uses GPT-4 as a contrast to GPT4-Code to show the benefit of integrated code execution; no internal mechanism analysis for GPT-4 provided here.",
            "failure_modes": "Lower computation accuracy on arithmetic-heavy tasks relative to GPT4-Code due to lack of integrated code execution/self-debugging.",
            "scaling_trend": "Not analyzed in this paper.",
            "uuid": "e6659.6",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that encourages LLMs to produce intermediate reasoning steps in natural language to improve multi-step reasoning; used as related work and baseline (Zero-shot-CoT).",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "CoT (prompting technique)",
            "model_family": "Prompting strategy for transformer LLMs",
            "model_size": null,
            "training_data_description": "N/A",
            "benchmark_name": "Used in related work and baseline comparisons (GSM8K, MATH)",
            "task_type": "Multi-step reasoning and arithmetic",
            "problem_format": "Natural-language with chain-of-thought responses",
            "difficulty_level": "Varied",
            "prompting_method": "Zero-shot-CoT (e.g., 'Let's think step by step'), few-shot CoT variants and self-consistency sampling discussed",
            "performance_metric": "Accuracy improvements relative to direct prompting reported in prior work; used as baseline in paper comparisons",
            "performance_value": null,
            "internal_analysis": "Paper contrasts CoT (NL chains) with code-based chains (SL chains) showing that Python-symbolic chains improve computation when combined with execution/verification.",
            "failure_modes": "Natural-language-only chains sometimes produce incorrect arithmetic computations; less reliable than code execution for exact numeric computation according to authors' experiments.",
            "scaling_trend": "Not directly studied in this paper beyond being part of baseline comparisons.",
            "uuid": "e6659.7",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.016773,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Solving Challenging Math Word Problems Using GPT-4 CODe Interpreter With Code-Based Self-Verification</h1>
<p>Aojun Zhou ${ }^{1^{<em>}}$ Ke Wang ${ }^{2^{</em>}}$ Zimu Lu ${ }^{3^{<em>}}$ Weikang Shi ${ }^{4^{</em>}}$ Sichun Luo ${ }^{5^{<em>}}$ Zipeng Qin ${ }^{1}$<br>Shaoqing Lu ${ }^{6}$ Anya Jia ${ }^{7}$ Linqi Song ${ }^{5}$ Mingjie Zhan ${ }^{1^{</em>}}$ Hongsheng Li ${ }^{1^{3}}$<br>${ }^{1}$ Multimedia Laboratory (MMLab), The Chinese University of Hong Kong<br>${ }^{2}$ Nanjing University ${ }^{3}$ University of Science and Technology of China<br>${ }^{4}$ Tsinghua University ${ }^{5}$ City University of Hong Kong<br>${ }^{6}$ Changsha University of Science and Technology ${ }^{7}$ Tufts University<br>{aojunzhou, wangk.gm, sichunluo2, zmjdll}@gmail.com<br>luzimu@mail.ustc.edu.cn shiwk20@mails.tsinghua.edu.cn<br>linqi.song@cityu.edu.hk hsli@ee.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit code-based self-verification (CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as "False", the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset ( $\mathbf{5 3 . 9 \%} \rightarrow \mathbf{8 4 . 3 \%}$ ).</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023) have shown impressive success in various tasks, such as common sense understanding and code generation. However, they still fall short in mathematical reasoning, often producing nonsensical or inaccurate content and struggling with complex calculations. Previous attempts to tackle these challenges include the Chain-of-Thought (CoT) (Wei et al., 2022) framework, which enhances LLMs' logical reasoning abilities by generating intermediate steps in their reasoning process. Additionally, PAL (Gao et al., 2023) introduces a novel approach by using the Python programming interpreter to improve computational accuracy.
In recent advancements, OpenAI has unveiled an improved version of GPT-4, namely the GPT-4 Code Interpreter ${ }^{12}$ or GPT4-Code, which is proficient at providing logical natural language reason-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ing, alongside step-by-step Python code. Notably, it can generate and execute code incrementally, and subsequently present the executed code's output back to the LLM. The addition of code generation and execution to natural language outputs has shown promising results in solving mathematical reasoning problems. Our initial experiments show that GPT4-Code achieved an impressive zero-shot accuracy of $69.7 \%$ on the challenging MATH dataset (Hendrycks et al., 2021), marking a significant improvement of $27.5 \%$ over GPT-4's performance ( $42.2 \%$ ).</p>
<p>While GPT4-Code has demonstrated proficiency in solving math problems, there has been a notable absence of systematic analysis focusing on understanding and further enhancing its mathematical problem-solving abilities. A key distinction between GPT4-Code and its predecessor, GPT-4, lies in GPT4-Code's ability to automatically generate and execute code. Therefore, this paper presents pilot experiments that investigate GPT4-Code's code generation and execution mechanism using specific code-constrained prompts. The analysis reveals that GPT4-Code's strong performance is not solely due to its code generation and execution abilities, but also its capacity to adjust its problemsolving strategies based on feedback from code execution-a process we term self-debugging (illustrated in Tab. 7 and Tab. 8). Due to the fact that code generation evolves its reasoning step-by-step and performs self-debugging after code execution errors, there is an increased frequency of code usage. Hence, we introduce the concept of Code Usage Frequency to differentiate these unique prompting strategies to quantitatively analyze the impact of code-constrained prompts on GPT4Code for mathematical problem-solving.</p>
<p>The step-by-step code generation and self-debugging mechanisms highlight the critical role of code in mathematical problem-solving. Nevertheless, the self-debugging mechanism only verifies each step of the generated code while lacks the verification of the reasoning steps and the final answer, which has been demonstrated to be of vital importance to the math problem-solving abilities of LLMs (Cobbe et al., 2021; Lightman et al., 2023; Weng et al., 2023).</p>
<p>We therefore ask the question: can we fully exploit the code generation and self-debugging mechanisms in GPT4-code, so that it can automatically verify and correct its solutions, without extra assistance from other models or users?</p>
<p>To answer this question, we propose a simple yet effective prompting technique termed the explicit code-based self-verification (CSV), which guides GPT4-Code to generate additional code that verifies the answer and adjusts the reasoning steps if there's a flaw in reasoning. Unlike previous methods that rely on external language models for verification (Lightman et al., 2023; Cobbe et al., 2021), our approach leverages GPT4-Code's inherent strengths. This approach offers two key benefits: (1) When the verification indicates an answer is False, GPT4-Code can rectify its prior solution and provide an improved alternative. (2) Solutions verified as True tend to be more reliable, akin to human problem-solving. However, even if a solution is self-verified as False, we do not directly abandon it. Instead, we propose a weighted majority voting strategy that incorporates the code-based solution verification results, as opposed to relying exclusively on the frequency of answers. We assign different weights to the solutions according to their verification states, reflecting the solutions' varying levels of reliability. In alignment with the Code Usage Frequency analysis from our pilot experiments, our explicit code-based self-verification prompt boosts GPT4-Code's accuracy in mathematical problem-solving with increased code usage.</p>
<p>Empirical study demonstrates the effectiveness of our proposed framework on the MATH, GSM8K, and MMLU-Math datasets using GPT4-Code. Our approach achieves an impressive accuracy of $\mathbf{8 4 . 3 2 \%}$ on the MATH dataset, greatly outperforming the base GPT4-Code and previous state-of-the-art methods. Additionally, we are making our experimental data on the MMLU-Math and MATH datasets publicly available, enabling result replication and facilitating fine-tuning of open-source LLM models (e.g., LLaMA 2 (Touvron et al., 2023)) to further enhance mathematical problemsolving capabilities with the assistance of code generation.</p>
<p>This paper's main contributions can be summarized in three key aspects:</p>
<ul>
<li>This study provides the first systematic analysis of code generation, execution, and selfdebugging's role in mathematical problem-solving. Our findings reveal that GPT4-Code's impressive mathematical problem-solving proficiency is primarily attributed to its step-bystep code generation and dynamic solution refinement based on code execution outcomes.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example from the 277th number theory problem in the MATH dataset: (a) Prompt 1 generates intermediate text-only reasoning steps. (b) Prompt 2 generates intermediate text reasoning steps and post-hoc Python code, akin to PAL (Gao et al., 2023). (c) Basic prompt that set no constrain on code usage. Details of the three prompts are presented in Sec. 3.1. (For more examples, see Tab. 9 and Tab. 10.)</p>
<ul>
<li>We introduce the innovative explicit code-based self-verification (CSV) prompt, which leverages GPT4-Code's advanced code generation mechanism. This prompt guides the model to verify the answer and then reevaluate its solution with code. CSV not only extends the verification to the logic behind problem-solving but also improves the efficacy of the majority voting method by integrating the verification states.</li>
<li>Additionally, we have contributed to the LLM community by creating two new instructionfollowing datasets: MATH-code and MMLU-Math-code. These datasets are designed to enhance the mathematical reasoning capabilities of open-source models.</li>
</ul>
<h1>2 RELATED WORK</h1>
<p>Chain-of-Thought Reasoning. The Chain-of-Thought (CoT) prompting approach proposed by Wei et al. (2022) is a notable contribution that showcases the multi-step reasoning capabilities of LLMs. By simply adding "Let's think step by step" before questions, Kojima et al. (2022) implements Zero-shot-CoT, which can serve as a strong zero-shot baseline. Further research extends the reasoning</p>
<p>capabilities of CoT by applying majority voting to improve self-consistency (Wang et al., 2023), choosing few-shot examples and output chains with more complex reasoning steps (Fu et al., 2022), breaking down the problem to simpler sub-problems (Zhou et al., 2023), or even expanding Chain-of-thought to Tree-of-Thoughts (Yao et al., 2023). Similar to Zero-shot-CoT, our method apply "step by step"-like prompts to regularize GPT4-Code's use of code without the careful design of step-bystep few-shot examples. Additionally, We enhance majority voting to verification-guided weighted majority voting, leveraging the results of CSV as voting weights.
Solving Math Problems with Code. Large language models have been found to be less accurate in performing arithmetic calculations, such as addition, subtraction, multiplication, etc (Cobbe et al., 2021; Lewkowycz et al., 2022; Gao et al., 2023; Lu et al., 2022). Consequently, previous works have attempted to solve math problems with the assistance of code. The GSM8K dataset (Cobbe et al., 2021) uses calculation annotations to extract all arithmetic calculations solved by an external calculator: the Python eval function. To further leverage the role of code in LLMs, Program-Aided Language model (PAL) (Gao et al., 2023) as well as Program of Thoughts (PoT) (Chen et al., 2022) interpret the math problems as Python codes and execute the codes with an external Python interpreter to obtain the answer. Although they can get more accurate answers than some non-code methods, many generated codes have execution errors or get wrong answers due to the lack of verification mechanism. Our approach not only utilizes the ability of GPT4-Code to generate multi-step codes and refine codes that fail to run, but also uses CSV to enhance the reliability and accuracy of the answers.</p>
<p>Self-Verification. Human problem solving is not always a one-time success, but rather requires iterative thinking, verification, and refinement. Unlike previous studies that train an additional verifier to verify the correctness of final answers (Cobbe et al., 2021) or intermediate steps (Lightman et al., 2023; Li et al., 2023), Weng et al. (2023) showed the self-verification abilities of LLMs by generating multiple answers and ranking them by self-verification scores. Furthermore, SELF-REFINE proposed by Madaan et al. (2023) iteratively refines its output through self-generated feedback. Unlike these self-verification methods that require LLMs to give verification feedback in natural language, our method applies generated codes to verify the answers and votes on different answers based on the verification results, thus improving the accuracy of the verification and making full use of the information in the verification process.</p>
<h1>3 Method</h1>
<p>We first conduct a pilot experiment with GPT4-Code on the challenging MATH dataset (Hendrycks et al., 2021). Remarkably, it achieves an accuracy of $69.7 \%$, significantly surpassing the previous state-of-the-art performance of $53.9 \%$ (Zheng et al., 2023). Encouraged by the compelling performance of GPT4-Code, we strive to systematically explore and analyze its underlying code mechanisms. In Sec. 3.1, we illustrate, via our code-constrained prompts design, that GPT4-Code's robust performance in solving math problems derives not only from its ability to generate accurate step-by-step code, but also from its self-debugging mechanism. In Sec. 3.2, we aim to leverage GPT4-Code's self-debugging strengths to further improve its mathematical problem-solving ability.</p>
<h3>3.1 Pilot Experiments on analyzing Code Usage of GPT4-Code</h3>
<p>To explore the impact of code usage on GPT4-Code's math problem-solving capabilities, we adopt a straightforward approach by constraining GPT4-Code's interaction with code through thoughtfully constructed prompts. Specifically, we introduce two code-constrained prompts and the basic prompt for comparison:</p>
<ul>
<li>Prompt 1: No code usage is allowed: In response to this prompt, GPT4-Code is prohibited from incorporating code into its solutions. This prompts GPT4-Code to rely solely on Natural Language (NL) reasoning chain, resembling solutions in the CoT framework (Wei et al., 2022). The resulting sequence of reasoning steps is depicted as $\mathbf{C}_{\mathrm{NL}}$, with an example given in Fig. 1 (a).</li>
<li>Prompt 2: Code can be used only once: In this prompt setting, GPT4-Code is permitted to employ code exclusively within a single code block to generate the solution, mirroring</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance on MATH dataset of different levels by applying different prompts to adjust the frequency of code usage. (a) Comparison of overall accuracy between the 4 prompts. (b) Code usage frequency is in proportion to accuracy in all five levels and this phenomenon is especially apparent when the problems are relatively complicated (i.e. with higher level).
the PAL approach introduced by Gao et al. (2023). We denote this sequence as $\mathbf{C}_{\mathrm{SL}}$, representing a series of Symbolic Language (SL), such as Python. An example is shown in Fig. 1 (b).</p>
<ul>
<li>Basic Prompt: GPT4-Code is prompted to tackle the problem without any restrictions on code usage. This prompt leads to GPT4-Code's typical functioning pattern, which can be denoted as $\mathbf{C}=\left(\left(\mathbf{c 1}<em _mathrm_SL="\mathrm{SL">{\mathrm{NL}}, \mathbf{c 1}</em>}}\right),\left(\mathbf{c 2<em _mathrm_SL="\mathrm{SL">{\mathrm{NL}}, \mathbf{c 2}</em>\right), \ldots\right)$, representing a sequential list of reasoning steps, each consisted of both natural language and Python code, with an example shown in Fig. 1 (c).}</li>
</ul>
<p>Apart from the specific example in Fig. 1, we introduce Code Usage Frequency to record the number of the code execution for different prompts. The results of the experiments using these prompts are shown in Fig. 2 (b). This figure illustrates a positive correlation between the better performance of GPT4-Code and the higher Code Usage Frequency. More specifically,</p>
<p>Prompt 1 v.s. Prompt 2: Prompt 1 results in almost negligible code usage, while Prompt 2 results in approximately 1 time's code usage. Prompt 2 yields an accuracy gain of 6.9 percent over Prompt 1. This suggests that the Python code chains $\mathbf{C}<em L="L" _mathbf_N="\mathbf{N">{\mathbf{S L}}$, can improve computational capability more than the natural language chains $\mathbf{C}</em>$. This observation is consistent with the findings in previous Python-based prompting methods (Gao et al., 2023; Chen et al., 2022). However, employing code only once comes with an inherent drawback - the model lacks the ability to self-debugging when the code output triggers an error or produces an implausible outcome.}</p>
<p>Prompt 2 v.s. Basic Prompt: The Basic Prompt consistently produces solutions that entail multiple instances of code usage, resulting in a large Code Usage Frequency. Additionally, the Basic Prompt exhibits notably enhanced accuracy. These improvements in Code Usage Frequency and accuracy might be attributable to two unique advantages: (1) Generating code in brief and frequent segments, divided among natural language reasoning steps, tends to result in higher accuracy. (2) The model possesses the capability to evaluate the results of code execution and make corrections to solution steps if the outcomes contain bugs or are deemed illogical, as illustrated in Tab. 7 and Tab. 8.</p>
<p>From these observations, it is plausible to enhance and build upon the favorable attributes of GPT4Code, to further improve its precision in tackling math problems.</p>
<h1>3.2 Explicit Code-based Self-Verification Prompting</h1>
<p>Inspired by the observations on Code Usage Frequency analysis, we seek to harness the capabilities of GPT4-Code. These capabilities include the model's aptitude for generating accurate code, evaluating the outcomes of code execution, and automatically adjusting reasoning steps of solutions when</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Question from the 712th intermediate algebra problem in the MATH dataset. (a) Without selfverification, the model generates a wrong answer. (b) With self-verification, the model corrects the error and generates the correct answer. The CSV prompt: To solve the problem using code interpreter step by step, and please verify your answer using code interpreter.
needed. However, despite these advantages, GPT4-Code currently falls short in assuring solution correctness. Consequently, our objective is to utilize these strengths to augment solution verification.</p>
<p>To achieve this objective, we propose the technique termed as explicit code-based self-verification (CSV). This method prompts GPT4-Code to explicitly validate its answer through code generation. By implementing this prompt, we introduce an extra verification stage to the solution $\mathbf{C}$, referred to as $\mathbf{V}$. The verification outcome $\mathbf{V}$ can be classified as True, False, or Uncertain. An Uncertain classification indicates that GPT4-Code encountered difficulties in identifying an effective method for answer verification, thereby abstaining from delivering a definitive verification result. Leveraging GPT4-Code's inherent autonomous capabilities, we can formulate the proposed prompting as follows:</p>
<p>$$
\mathbf{C} \rightarrow \mathbf{V}=\left{\begin{array}{ll}
\text { True } &amp; \rightarrow \text { final answer } \
\text { False } &amp; \rightarrow \mathbf{C}_{\text {new }} \rightarrow \mathbf{V} \rightarrow \cdots \rightarrow \text { True } \rightarrow \text { final answer } \
\text { Uncertain } &amp; \rightarrow \text { final answer }
\end{array}\right.
$$</p>
<p>An example is presented in Fig. 3 (b). Incorporated with CSV, the model becomes capable of using code to verify answers, then reviewing and adjusting how it arrived at the solution if the verification result is False, aiming at obtaining the correct answer. Upon refining and correcting the initial solution, we anticipate a notable increase in accuracy. It is worth noting that both the verification and rectification stages are code-based. This inevitably results in increased Code Usage Frequency, akin to the aforementioned analysis, which will be further demonstrated in subsequent experiments.</p>
<p>We perform experiments with CSV, and these results can be found in Fig. 2. The experiment here is conducted with GPT4-Code on MATH (Hendrycks et al., 2021). In Fig. 2 (b), the accuracy achieved with our proposed CSV prompt consistently surpasses that of the Basic Prompt across all designated difficulty levels ${ }^{3}$. Meanwhile, the Code Usage Frequency receives a clear increase.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Before the advent of GPT4-Code, prior frameworks (Lightman et al., 2023; Cobbe et al., 2021) depended on an external LLM to use natural language for verification and well-designed few-shot example prompts. In contrast, our approach simplifies the process by relying solely on a straightforward prompt for GPT4-Code, all in a zero-shot manner. This enables GPT4-Code to autonomously verify and independently rectify its solutions using the advanced code execution mechanism, thereby eliminating the need for customized few-shot examples.</p>
<p>Given that CSV can effectively verify problem-solving answers, we can naturally integrate the verification states into majority voting, akin to the methodology embraced in self-consistency CoT (Wang et al., 2023). Answers deemed True through verification are generally more trustworthy, reflecting the problem-solving approach seen in human cognition (Newell \&amp; Simon, 1972; Wang \&amp; Chiew, 2010). This improved reliability can be leveraged in the widely-used majority voting process. To exploit this insight, we introduce verification-guided weighted majority voting, which assigns different weights to the states of the verification process.</p>
<p>In practice, it sometimes occurs that once an answer is confirmed as False, no additional verification is conducted, yielding a False verification state. We allocate corresponding weights these states of True, Uncertain, False: $w_{\mathrm{T}}, w_{\mathrm{U}}$, and $w_{\mathrm{F}}$, respectively.</p>
<p>Similar to the Self-consistency with CoT (CoT-SC) (Wang et al., 2023) in Fig. 4 (a)(ii), our framework can sample $k$ paths. For simplicity, we extract pairs of final answers and their corresponding verification results from $k$ solutions, represented as $\left(v^{i}, a^{i}\right), i=1,2, \ldots, k$, where $v^{i}$ and $a^{i}$ denote the $i$-th final answer and final verification result, respectively.</p>
<p>So the voting score for each candidate answer $a$ can be expressed as:</p>
<p>$$
\operatorname{Score}(a)=\sum_{\left{v^{i}\right}} w_{v}\left(#\left{i \mid a^{i}=a \text { and } v^{i}=v\right}\right), \quad v \in{\text { True, Uncertain, False }}
$$</p>
<p>Here, $a$ represents a candidate answer, $v$ denotes the state of verification, and $w_{v}$ is an element from the set $\left{w_{\mathrm{T}}, w_{\mathrm{U}}, w_{\mathrm{F}}\right}$. Each $w_{v}$ signifies the degree of confidence associated with its corresponding verification state.</p>
<p>Finally, we select the answer with the highest score from all candidate answers,</p>
<p>$$
\text { Output }=\underset{a}{\arg \max } \operatorname{Score}(a)
$$</p>
<p>where $\operatorname{Score}(a)$ refers to the score of answer $a$ according to Eq. 1.
It should be noted that when $w_{v}=1$ for all $w_{v} \in\left{w_{\mathrm{T}}, w_{\mathrm{U}}, w_{\mathrm{F}}\right}$, Eq. 1 becomes equivalent to the naive majority voting employed in Self-Consistency with CoT (CoT-SC) (Wang et al., 2023). Typically, we set $w_{\mathrm{T}}&gt;w_{\mathrm{U}}&gt;w_{\mathrm{F}}$, which means that an answer verified true has a greater confidence than the one with an uncertain state of verification, while an answer verified false has the lowest degree of confidence. An example of the calculation process within verification-guided weighted majority voting is illustrated in Fig. 4.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 PERFORMANCE ON MATH</h3>
<p>The MATH dataset (Hendrycks et al., 2021) is recognized as the most challenging math word problem dataset, as also highlighted by Chen et al. (Chen et al., 2023). Most of our experiments and the corresponding analyses are performed on the MATH benchmark. Tab. 1 compares the performance of the GPT4-Code against other models. GPT4-Code reaches 69.69\% on MATH (Hendrycks et al., 2020), largely surpassing the previous state of art result ( $53.90 \%$ ), which shows that GPT4-Code exhibits strong abilities in solving math problems and is used as our baseline for ablation study. On top of GPT4-Code, our method further improves its accuracy, raising the result to $73.54 \%$ after adding explicit code-based self-verification, and $84.32 \%$ after adding both explicit code-based selfverification and verification-guided weighted majority voting (the number of sampled paths is 16).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Illustration of the Naive majority voting (Wang et al., 2023) and our Verification-guided weighted majority voting. The full pipeline of the proposed Verification-guided Weighted Majority Voting framework. We use the model to generate several different solutions. Then we detect the self-verification state of each solution, and classify them into three states: True, Uncertain, and False. According to the state of the verification, we assign each solution a different weight, and use the classified result to vote the score of each possible answer.</p>
<p>Table 1: Accuracy (\%) on MATH dataset. VW-voting is the abbreviation for the verification-guided weighted majority voting. (Overall: The results across various MATH subtopics (Hendrycks et al., 2021))</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Code-based <br> Verification</th>
<th style="text-align: center;">VW- <br> Voting</th>
<th style="text-align: center;">Intermediate <br> Algebra</th>
<th style="text-align: center;">Precalculus</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Number <br> Theory</th>
<th style="text-align: center;">Counting \&amp; <br> Probability</th>
<th style="text-align: center;">PreAlgebra</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.20</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">34.12</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (CoT)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">50.36</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (PHP)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">53.90</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">69.69</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">73.54</td>
</tr>
<tr>
<td style="text-align: left;">Improvement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+6.5$</td>
<td style="text-align: center;">$+2.4$</td>
<td style="text-align: center;">$+8.6$</td>
<td style="text-align: center;">$+7.6$</td>
<td style="text-align: center;">$+6.7$</td>
<td style="text-align: center;">$+8.2$</td>
<td style="text-align: center;">$+3.3$</td>
<td style="text-align: center;">$+3.85$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">84.32</td>
</tr>
<tr>
<td style="text-align: left;">Improvement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+24.3$</td>
<td style="text-align: center;">$+16.3$</td>
<td style="text-align: center;">$+11.5$</td>
<td style="text-align: center;">$+16.9$</td>
<td style="text-align: center;">$+18.4$</td>
<td style="text-align: center;">$+5.3$</td>
<td style="text-align: center;">$+12.0$</td>
<td style="text-align: center;">$+14.63$</td>
</tr>
</tbody>
</table>
<p>Note that this astonishingly high result is based on the strong abilities of the base model GPT4-Code, and our method amplifies its good qualities of GPT4-Code, with the ability to verify solutions.</p>
<p>Note that although adding Code-based Self-verification can improve the performance of every individual subject, the extent of improvement varies from subject to subject, from $7.6 \%$ to only $0.6 \%$. In particular, the Geometry problem only has an increased accuracy of $0.6 \%$, even though the original GPT4-Code accuracy is only $54.0 \%$, which is low among the subjects. This discrepancy may be attributed to the fact that solving geometry problems often requires multi-modality (Chen et al., 2023), a concept beyond the scope of this paper.</p>
<h1>4.2 PERFORMANCE ON OTHER DATASETS</h1>
<p>In addition to the challenging MATH dataset, we have also performed our method on other reasoning datasets such as GSM8K (Cobbe et al., 2021), MMLU-Math, and MMLU-STEM (Hendrycks et al., 2020). The corresponding results can be viewed in Tab. 2 and Tab. 3. When integrated on top of GPT-</p>
<p>Table 2: Performance on GSM8K dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Sampled paths</th>
<th style="text-align: center;">Accuracy( $\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5 (5-shot)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (5-shot CoT)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (PHP)</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">96.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Model selection)</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">96.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;">$\mathbf{5}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performances on MMLU dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Accuracy( $\%$</th>
<th style="text-align: center;">Few-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chinchilla (Hoffmann et al., 2022)</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">Galactica (Taylor et al., 2022)</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">$\mathbf{8 9 . 2}$</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">OpenLLM</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">$\mathbf{8 7 . 0}$</td>
<td style="text-align: center;">zero-shot</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The four points on each curve correspond to results using Prompt 1, Prompt 2, Basic Prompt and Code-based Self-verification Prompt, respectively. (a) The accuracy of different levels at various code usage frequencies. (b) The accuracy of different subjects at various code usage frequencies.</p>
<p>4-code, our method outperforms other methods in the competition, achieving state-of-the-art results across all datasets. Other subjects in MMLU benchmarks are provided in Fig. 8. A comparative analysis of our results with those of previous state-of-the-art techniques and open-source models is also provided.</p>
<p>Tab. 2 illustrates that verification-guided majority voting is an effective framework to reduce the number of sampled paths, compared to GPT-4 with model selection (Zhao et al., 2023) and PHP (Zheng et al., 2023).</p>
<p>Tab. 3 presents a comparison of our model's performance with existing models (Hoffmann et al., 2022; Taylor et al., 2022) on the MMLU-Math dataset and with state-of-the-art open-sourced models ${ }^{4}$ on MMLU-STEM. The open-source models remain significantly outpaced by their closedsource counterparts. To address this gap, we have made the dataset and will make it publicly available in the near future. Our intention is to facilitate the fine-tuning of open-source LLMs. For example, the open-source model LLaMA 2 (Touvron et al., 2023) can potentially utilize this data to further bolster its math reasoning capabilities.</p>
<h1>4.3 CODE USAGE FREQUENCY OF PROPOSED PROMPTS</h1>
<p>Analogous to the approach taken in Sec. 3.1, we gather data to elucidate the correlation between accuracy and Code Usage Frequency across various dimensions - prompts (proposed CSV prompt as well as prompts used in pilot experiments), subjects, and difficulty levels. As shown in Fig. 5, the model's behavior is in accordance with our expectations when adding the code-based prompts. Each line in Fig. 5 has an obvious trend of going upwards, proving that the increase of Code Usage Frequency induces a general improvement in accuracy. The performance gain when using more code is more obvious in the higher difficulty levels, while in lower levels, the performance gain is not very prominent, as shown in Fig. 5 (a). Also, the Code Usage Frequency increases steadily with</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Comparison Self-verification with/without explicit code-based prompt (Overall:The results across various MATH subtopics (Hendrycks et al., 2021))</p>
<table>
<thead>
<tr>
<th>GPT4-Code</th>
<th>Verification Method</th>
<th>Intermediate Algebra</th>
<th>Precalculus</th>
<th>Geometry</th>
<th>Number Theory</th>
<th>Counting &amp; Probability</th>
<th>PreAlgebra</th>
<th>Algebra</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>without Verification</td>
<td>50.1</td>
<td>51.5</td>
<td>53.4</td>
<td>77.2</td>
<td>70.6</td>
<td>86.3</td>
<td>83.6</td>
<td>69.69</td>
</tr>
<tr>
<td></td>
<td>Nature Language</td>
<td>52.6</td>
<td>48.7</td>
<td>50.8</td>
<td>79.9</td>
<td>72.5</td>
<td>83.1</td>
<td>82.6</td>
<td>69.29</td>
</tr>
<tr>
<td></td>
<td>Code-based</td>
<td>+2.5</td>
<td>-2.8</td>
<td>-2.6</td>
<td>+2.7</td>
<td>+1.9</td>
<td>-3.2</td>
<td>-1.0</td>
<td>-0.40</td>
</tr>
<tr>
<td></td>
<td></td>
<td>56.6</td>
<td>53.9</td>
<td>54.0</td>
<td>85.6</td>
<td>77.3</td>
<td>86.5</td>
<td>86.9</td>
<td>73.54</td>
</tr>
<tr>
<td></td>
<td></td>
<td>+6.5</td>
<td>+2.4</td>
<td>+0.6</td>
<td>+8.4</td>
<td>+6.7</td>
<td>+0.2</td>
<td>+3.3</td>
<td>+3.85</td>
</tr>
</tbody>
</table>
<p>the increase of difficulty levels. This shows that the harder math problems require more frequent code usage, which implies that invoking code multiple times might be an important reason why GPT4-Code have such an advantage in solving difficult math problems. There is a similar trend in Fig. 5 (b).</p>
<h1>4.4 Ablation Study and Discussion</h1>
<p>Comparisons between Natural Language and Code-based Self-Verification: to underscore the significance of code in the self-verification stage, we employed a distinct natural language selfverification prompt. In this approach, GPT4-Code is directed to verify the solution through natural language instead of relying on code-based verification, as presented in Tab. 4. The accuracy achieved with this method was slightly lower than that of the Basic Prompt. Moreover, we observed a decline in accuracy for 4 of the 7 subtopics, indicating that relying solely on natural language verification can not only compromise accuracy but also negatively impact performance. In contrast, code-based verification enhances accuracy across all 7 subtopics when compared to the Basic Prompt.</p>
<p>Analysis of Verification-guided Weighted Majority Voting: we initially compiled the confusion matrix (TP/TN/FP/FN), capturing solutions with self-verification that matches the True and False states mentioned in Eq. 1 from five distinct sampled paths. The details of the confusion matrix are presented in Appendix A.1.1. From this data, we computed Precision, Recall, and Accuracy. (Solutions in the True state are seen as positive.) The results are presented in Fig. 6. In comparison to Accuracy, we observed numerical enhancements of $22.3 \%$ and $5.6 \%$ in the average Precision and Recall, respectively. In particular, the average Precision registered at $95.88 \%$. This implies that the Accuracy has the potential to become much higher, if more solutions reach the verified True state before giving the final answer.</p>
<p>Hyper-parameters ablation in Verification-guided Weighted Majority Voting: we also performed ablation studies on the hyper-parameter $w_{v} \in\left{w_{\mathrm{T}}, w_{\mathrm{U}}, w_{\mathrm{F}}\right}$ in Eq. 1. When the hyperparameter setting satisfied $w_{\mathrm{T}}&gt;w_{\mathrm{U}} \geq w_{\mathrm{F}}$, the performance of the verification-guided weighted majority voting consistently surpassed that of the naive majority voting methods across all sampled paths. In contrast, when we set the hyper-parameter $\left(w_{\mathrm{T}}=0.5, w_{\mathrm{U}}=0.5, w_{\mathrm{F}}=1\right)$, the performance under this configuration was worse than the naive majority voting. Therefore, our proposed method, verification-guided weighted majority voting, is easy to tune and robust.</p>
<h2>5 CONCLUSION AND LIMITATION</h2>
<p>In this paper, we begin with pilot experiments on GPT4-Code to explore how its use of code impacts its performance in mathematical reasoning. By analyzing Code Usage Frequency and accuracy, we determine that GPT4-Code's skill in solving math problems can be largely attributed to its ability to generate and execute code, as well as its effectiveness in adjusting and rectifying solutions when confronted with implausible execution outputs. Expanding on this understanding, we introduce the ideas of explicit code-based self-verification and verification-guided weighted majority voting, with the goal of enhancing GPT4-Code's mathematical capabilities.
However, there are limitations in our work that we plan to explore further in the future. Firstly, our analysis and improvements are currently focused on GPT4-Code, which is somewhat restrictive. We aim to apply the methods to other LLMs. Secondly, our explicit code-based self-verification and verification-guided weighted majority voting technique could potentially create more accurate datasets. These datasets would include detailed step-by-step code-based solution generation and</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (a). shows the precision, recall, and accuracy on different reasoning paths. (b). shows the accuracy in response to the number of sampled reasoning paths when the weight is set to different values.
code-based validation, which could help improve open-source LLMs like LLaMA 2 (Touvron et al., 2023) and enhance their mathematical abilities. Although we haven't yet investigated this approach, we leave it for future work.</p>
<h1>REFERENCES</h1>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022.</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset, 2023.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/CorpusID: 221516475 .</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213, 2022.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, 2023.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
A. Newell and H.A. Simon. Human Problem Solving. ACS symposium series. Prentice-Hall, 1972. ISBN 9780134454030. URL https://books.google.com.hk/books?id= h03uAAAAMAAJ.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw.</p>
<p>Yingxu Wang and Vincent Chiew. On the cognitive process of human problem solving. Cognitive Systems Research, 11(1):81-92, 2010. ISSN 1389-0417.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=_VjQ1MeSB_J.</p>
<p>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification, 2023.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023.</p>
<p>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023.</p>
<h1>APPENDIX</h1>
<p>This Appendix contains two sections. The first section provides the experiment details, including the detailed experiment results on MATH and MMLU datasets. The second section presents some examples of GPT4-Code.</p>
<h2>A EXPERIMENT DETAILS</h2>
<h2>A. 1 DETAILED EXPERIMENT RESULT ON MATH DATASET</h2>
<h2>A.1.1 Confusion Matrix</h2>
<p>A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. It's particularly useful for classification problems, and we utilize it to analyze the performance of our verification process.</p>
<p>The matrix itself is a two-dimensional grid, $2 \times 2$, for the binary classification of verification results. Each row of the matrix represents the instances in a predicted class, which is determined by the verification results given by the language model, while each column represents the instances in an actual class, which is determined by the actual correctness of the answer given by the model. Tab. 5 shows how the matrix looks for our verification process:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 5: Confusion Matrix of Verification</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Answer Correct</td>
<td style="text-align: center;">Answer Wrong</td>
</tr>
<tr>
<td style="text-align: left;">Verification True</td>
<td style="text-align: center;">TP</td>
<td style="text-align: center;">FP</td>
</tr>
<tr>
<td style="text-align: left;">Verification False</td>
<td style="text-align: center;">FN</td>
<td style="text-align: center;">TN</td>
</tr>
</tbody>
</table>
<p>Here's what the four terms mean:</p>
<ul>
<li>True Positive (TP): The cases in which the model's verification result is 'True', and the answer is actually correct.</li>
<li>True Negative (TN): The cases in which the model's verification result is 'False', and the answer is actually wrong.</li>
<li>False Positive (FP): The cases in which the model's verification result is 'True', but the answer is actually wrong.</li>
<li>False Negative (FN): The cases in which the model's verification result is 'False', but the answer is actually correct.</li>
</ul>
<p>This matrix is very helpful for measuring more than just straightforward accuracy, based on which Precision and Recall are two important metrics. They are defined in Eq. 3 and their meanings are as follows:</p>
<ul>
<li>Precision is the fraction of relevant instances among the retrieved instances. It is a measure of the accuracy of the classifier when it predicts the positive class.</li>
<li>Recall is the fraction of the total amount of relevant instances that were actually retrieved. It is a measure of the ability of a classifier to find all the positive instances.</li>
</ul>
<p>$$
\text { Precision }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}, \text { Recall }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}
$$</p>
<p>In other words, precision answers the question "What proportion of verified TRUE answers was actually correct?", while recall answers "What proportion of actual correct answers was verified TRUE?" Given its meaning, verification-guided voting is bounded to be effective when the precision of verification is high.</p>
<h1>A.1.2 Python Package Usage analysis</h1>
<p>Tab. 6 outlines the usage of various Python packages in our experiments. Among them, we found that the sympy package is utilized most frequently, highlighting its central role in the computational tasks performed.</p>
<p>Table 6: Python package usage frequency on MATH dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Package</th>
<th style="text-align: center;">sympy</th>
<th style="text-align: center;">numpy</th>
<th style="text-align: center;">math</th>
<th style="text-align: center;">fractions</th>
<th style="text-align: center;">itertools</th>
<th style="text-align: center;">cmath</th>
<th style="text-align: center;">scipy</th>
<th style="text-align: center;">matplotlib</th>
<th style="text-align: center;">functools</th>
<th style="text-align: center;">collections</th>
<th style="text-align: center;">statistics</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.4168</td>
<td style="text-align: center;">0.0284</td>
<td style="text-align: center;">0.1590</td>
<td style="text-align: center;">0.0094</td>
<td style="text-align: center;">0.0034</td>
<td style="text-align: center;">0.0034</td>
<td style="text-align: center;">0.0016</td>
<td style="text-align: center;">0.0010</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">0.0002</td>
</tr>
<tr>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">0.3907</td>
<td style="text-align: center;">0.0241</td>
<td style="text-align: center;">0.1638</td>
<td style="text-align: center;">0.0110</td>
<td style="text-align: center;">0.0029</td>
<td style="text-align: center;">0.0026</td>
<td style="text-align: center;">0.0009</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0006</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">correct per code</td>
<td style="text-align: center;">0.3323</td>
<td style="text-align: center;">0.0205</td>
<td style="text-align: center;">0.1393</td>
<td style="text-align: center;">0.0094</td>
<td style="text-align: center;">0.0025</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0005</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">0.4724</td>
<td style="text-align: center;">0.0383</td>
<td style="text-align: center;">0.1493</td>
<td style="text-align: center;">0.0058</td>
<td style="text-align: center;">0.0045</td>
<td style="text-align: center;">0.0052</td>
<td style="text-align: center;">0.0032</td>
<td style="text-align: center;">0.0026</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">wrong per code</td>
<td style="text-align: center;">0.3194</td>
<td style="text-align: center;">0.0259</td>
<td style="text-align: center;">0.1009</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.0031</td>
<td style="text-align: center;">0.0035</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0018</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">c/w per code</td>
<td style="text-align: center;">$104 \%$</td>
<td style="text-align: center;">$79 \%$</td>
<td style="text-align: center;">$138 \%$</td>
<td style="text-align: center;">$238 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$34 \%$</td>
<td style="text-align: center;">$14 \%$</td>
<td style="text-align: center;">$57 \%$</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
</tr>
</tbody>
</table>
<h2>A. 2 Detailed EXPERIMENT Result on MMLU DATASET</h2>
<p>Fig. 8 illustrates that GPT4-Code performs relatively poorly in certain domains, such as engineering and the humanities, with a particularly marked deficiency in virology, where it achieves a score of less than $60 \%$. These observations delineate specific areas that call for further investigation and refinement, thus outlining the direction for future improvements in the model.</p>
<h2>B EXAMPLES</h2>
<p>In this section, we provide more examples.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: GPT4-Code performance on MMLU dataset on: (a) subsubcategory, (b) category, (c) subcategory</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Model performance on MMLU dataset on: (a) subsubcategory, (b) category, (c) subcategory</p>
<p>Math Problem: $0 . \overline{54}$ as a fraction in lowest terms.
Sure, we can solve this problem by setting up an equation, solving it, and simplifying the fraction.
Let's call $0 . \overline{54}$ as $x$.
The decimal $0 . \overline{54}$ means that the digits " 54 " repeat indefinitely. So, we can multiply $x$ by 100 (since " 54 " is a two-digit number) to get another equation.
This gives us $100 x=54 . \overline{54}$.
By subtracting the original equation from this new one, we eliminate the repeating decimal: $100 x-x=54 . \overline{54}-0 . \overline{54}$.
Let's perform these steps in the next Python cell.</p>
<div class="codehilite"><pre><span></span><code># define the symbol
x = symbols(&#39;x&#39;)
# define the equations
eq1 = Eq(x, 0.54545454545454545454545454545454) # approximating the
        repeating decimal
</code></pre></div>

<p>. . . . .</p>
<h1>RESULT</h1>
<p>TypeError Traceback (most recent call last)
・.....
Apologies for the mistake. We can't directly subtract two equations in Python. Instead, we should subtract the right-hand sides and left-hand sides of the equations separately to get a new equation. Let's correct that.</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nx">subtract</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">right</span><span class="w"> </span><span class="nx">hand</span><span class="w"> </span><span class="nx">sides</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">left</span><span class="w"> </span><span class="nx">hand</span><span class="w"> </span><span class="nx">sides</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">equations</span>
<span class="w">    </span><span class="nx">separately</span>
<span class="nx">rhs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">simplify</span><span class="p">(</span><span class="nx">eq2</span><span class="p">.</span><span class="nx">rhs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">eq1</span><span class="p">.</span><span class="nx">rhs</span><span class="p">)</span>
<span class="nx">lhs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">simplify</span><span class="p">(</span><span class="nx">eq2</span><span class="p">.</span><span class="nx">lhs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">eq1</span><span class="p">.</span><span class="nx">lhs</span><span class="p">)</span>
</code></pre></div>

<p>. . . . .</p>
<h2>RESULT</h2>
<p>Eq(99*x, 54.0)
After subtraction, we get the equation $99 x=54$.
Next, we can solve for $x$ to express $0 . \overline{54}$ as a fraction.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> solve for x
solution = solve(new_eq, x)
</code></pre></div>

<p>solution[0]</p>
<h2>RESULT</h2>
<h2>0.545454545454545</h2>
<p>The solution for $x$ is approximately 0.545454545454545 , but we need to express it as a fraction. Let's convert this decimal into a fraction.</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nx">convert</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">decimal</span><span class="w"> </span><span class="nx">solution</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">fraction</span>
<span class="nx">fraction</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">nsimplify</span><span class="p">(</span><span class="nx">solution</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<p>fraction</p>
<h2>RESULT</h2>
<p>6/11
The decimal $0 . \overline{54}$ can be expressed as the fraction $\frac{6}{11}$.</p>
<h2>Answer: $\frac{6}{11}$</h2>
<p>Table 7: An example in which GPT4-Code self-debugs when the code execution emits an error.</p>
<p>Math Problem: A bus comes by Jerry's bus stop every 20 minutes starting at exactly 5:13 a.m. If Jerry shows up at exactly 8:35 a.m., how many minutes will he have to wait for the next bus?
Step 1: Calculate the total minutes from 5:13 a.m. to 8:35 a.m.
Total minutes from 5:13 a.m. to 8:35 a.m. is $180+22$ minutes.
Step 2: Calculate how many buses have come by during this time.
Step 3: Calculate the time when the last bus before 8:35 a.m. came by.</p>
<ul>
<li>The time when the last bus came by is $5: 13$ a.m. $+20 \times\lfloor n\rfloor$ minutes.</li>
</ul>
<p>Step 4: Calculate the waiting time.
Now, let's calculate each step.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Step 1: Calculate the total minutes from 5:13 a.m. to 8:35 a.m.
total_minutes = 180 + 22
</code></pre></div>

<p>・.. .</p>
<h1>RESULT</h1>
<h2>-311</h2>
<p>It seems that there's a mistake in the calculations. The waiting time cannot be negative. This error might have occurred in Step 3, where I calculated the time of the last bus in minutes past midnight, but then in Step 4, I subtracted this from the total time from 5:13 a.m. to 8:35 a.m. I should correct Step 4 to calculate the time until the next bus, which is either 0 if a bus has just arrived at 8:35 a.m., or 20 minutes minus the time since the last bus if a bus arrived before 8:35 a.m.</p>
<p>Let's correct this.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Corrected Step 4: Calculate the time until the next bus
time_since_last_bus = total_minutes - last_bus_time
</code></pre></div>

<p>・.. .</p>
<h2>RESULT</h2>
<p>331
There still seems to be a mistake in the calculations. The waiting time calculated is significantly more than 20 minutes, which is not possible given that the bus comes every 20 minutes.
Looking back at the calculations, it seems that the error is in Step 3 when calculating the time the last bus came. The last bus time should be calculated from 5:13 a.m., not from midnight. Let's correct this.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Corrected Step 3: Calculate the time when the last bus before 8:35
    a.m. came by.
last_bus_time_corrected = 5 <span class="gs">* 60 + 13 + num_buses *</span> 20
</code></pre></div>

<p>・.. .</p>
<h2>RESULT</h2>
<p>331
It seems there is still an error. Let's try a different approach.</p>
<ol>
<li>Calculate the total minutes that have passed since the first bus at 5:13 a.m. until 8:35 a.m.</li>
<li>Since the buses come every 20 minutes, calculate how many complete cycles of 20 minutes have passed in the calculated total minutes.</li>
<li>The time Jerry will have to wait is the remainder of the total minutes divided by 20 (which is the time since the last bus) subtracted from 20 (which is the bus cycle).
This approach should give us the correct waiting time. Let's calculate it.</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Calculate total minutes from 5:13 a.m. to 8:35 a.m.
total_minutes = 3 * 60 + 22 # 3 hours and 22 minutes
</code></pre></div>

<p>・.. .</p>
<h2>RESULT</h2>
<p>18
So, Jerry will have to wait for 18 minutes for the next bus.</p>
<h2>Answer: 18</h2>
<p>Table 8: An example in which GPT4-Code adjusts its solution based on the result of code execution.</p>
<p>Math Problem: Solve for $x: 3^{2 x}+19=10^{x}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">code 0</th>
<th style="text-align: center;">code 1</th>
<th style="text-align: center;">code more</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">This is a difficult equation to solve and there's no simple analytical solution using elementary functions. In real world scenarios, such equations are typically solved numerically. For example, using a graphing calculator, a computer, or a mathematical software. <br> However, if you want an approximation of the solution, it is advisable to use numerical methods such as the bisection method, Newton's method, or others. <br> If you would like me to provide an algorithm to find the solution numerically, let me know.</td>
<td style="text-align: center;">Answer: <br> The equation you're asking to solve, $3^{2 x}+19=10^{x}$, cannot be solved algebraically due to the nature of the functions and operations involved. Therefore, we'll need to use numerical methods such as Newton's method or Bisection method for root finding. <br> from scipy.optimize import fsolve import numpy as np <br> # Defining the function <br> def func (x) : <br> return $3 * *(2 * x)+$ $19-10 * * x$ <br> # Initial guess for $x$, it can be anything, 1.0 is chosen arbitrarily here $\mathrm{x} 0=1.0$ <br> # Using fsolve to find the root <br> root $=$ fsolve(func, x0 ) <br> $\mathrm{x}=$ root [0] <br> x <br> RESULT <br> 1.001831315399663 <br> So the answer is 1.00 .</td>
<td style="text-align: center;">Rewrite the equation as: $3^{2 x}-10+19=0$ <br> Let's denote $3^{x}=a$, and $10^{x}=b$ <br> so $a^{2}-b+19=0$ <br> now we have two equations: <br> 1. $a=3^{x}$ <br> 2. $b=10^{x}$ Let's solve this with Python. <br> from scipy.optimize import root <br> import numpy as np <br> # Define the system of equations <br> def equations (vars) : $\mathrm{x}, \mathrm{a}, \mathrm{b}=$ vars <br> eq1 $=\mathrm{a}-3 * * \mathrm{x}$ <br> eq2 $=\mathrm{b}-10 * * \mathrm{x}$ <br> eq3 $=\mathrm{a} * * 2-\mathrm{b}+$ <br> 19 <br> return [eq1, eq2, <br> eq3] <br> $\qquad$ <br> RESULT: <br> 0.0 <br> $3^{2.0}+19=10^{0}$ <br> The left-hand side becomes $1+19=20$ and the right-hand side becomes 1. Thus, the original equation does not hold true for $x=0$. <br> Let's try a different initial guess for the solution to see if that helps. <br> # Adjust initial guess for $x, a, b$ <br> $x 0=[1,3,10]$ <br> # Solve system of equations <br> solution $=$ root ( equations, x0) <br> # Extract solution <br> $\mathrm{x}=$ solution.x[0] <br> x <br> RESULT: <br> 2.0000000000004463 <br> We know that $3^{2.2}=3^{4}=81$ and $10^{2}=100$. The left-hand side becomes 100 and the righthand side is also 100 . Thus, the original equation holds true for $x=2$.</td>
</tr>
</tbody>
</table>
<h1>Answer: 2</h1>
<p>Table 9: An example in which no restriction on code use succeeds while not using code and using code only once fails.</p>
<p>Math Problem: What is the sum of the digits in the terminating decimal representation of the fraction $\frac{4321}{5^{7} \cdot 2^{8}}$ ?</p>
<h2>code 0</h2>
<p>The given fraction is</p>
<p>$$
\frac{4321}{5^{7} \cdot 2^{8}}
$$</p>
<p>To simplify this fraction, we can notice that $5^{7} \cdot 2^{8}=10^{7}$, which would mean the fraction is equivalent to $4321 \times 10^{-7}$, i.e., the number 4321 with the decimal point moved 7 places to the left. This would result in the number 0.0004321 .</p>
<p>To get the sum of the digits in the terminating decimal representation of this number, we just need to add up the digits in 4321, since the zeros do not contribute to the sum.
So, the sum is $4+3+2+1=10$. Therefore, the answer is 10</p>
<h2>code 1</h2>
<p>Answer:
First, we need to simplify the fraction.
Given that $4321=2^{5} \cdot 3^{3} \cdot$ $5^{0} \cdot 7^{0} \cdot 11^{0} \cdot 13^{1}$, we can simplify the fraction $\frac{4321}{5^{7} \cdot 2^{8}}$ to $\frac{2^{5} \cdot 3^{3} \cdot 5^{0} \cdot 7^{0} \cdot 11^{0} \cdot 13^{1}}{2^{3} \cdot 2^{8}}=3^{3} \cdot 13^{1} \cdot$ $2^{-3} \cdot 5^{-7}$.
Next, we need to compute the decimal representation and sum the digits.
# Compute the decimal representation of the fraction
value $=(3 * * 3 * 13 * * 1)$
/ (2<strong>3 * 5</strong>7)
# Convert to string and replace the decimal point
str_value = str(value) .replace('.','')
# Compute the sum of the digits
digit_sum = sum(int( digit) for digit in str_value)
digit_sum
RESULT:
18</p>
<h2>code more</h2>
<p>Answer:
The denominator can be rewritten as $10^{\min (7,8)} \times$ $5^{\max (0,7-8)} \times 2^{\max (0,8-7)}$, where $\min (a, b)$ is the minimum of $a$ and $b$, and $\max (a, b)$ is the maximum of $a$ and $b$. Now let's calculate the value of $\min (7,8), \max (0,7-8)$, and $\max (0,8-7)$.
min_ab = min(a, b)
max_0_a_minus_b = max $(0, a-b)$
max_0_b_minus_a = max $(0, b-a)$
min_ab,
max_0_a_minus_b,
max_0_b_minus_a
RESULT:
$(7,0,1)$
Now, let's simplify the fraction $\frac{4321}{10^{7} \times 5^{7} \times 2^{7}}$.
# define the numerator and denominator
numerator $=4321$
denominator $=10 * * 7 *$ $5 * * 0 * 2 * * 1$
. . . . . .</p>
<h2>RESULT:</h2>
<p>0.00021605</p>
<p>We exclude the zeros before the first non-zero digit and the decimal point as they do not contribute to the sum.
Thus, the sum of the digits is the sum of $2,1,6,0$, and 5 . Let's calculate it.
# digits in the fraction
digits $=[2,1,6,0$, 5]
# calculate the sum of the digits
sum_of_digits = sum( digits)
sum_of_digits</p>
<h2>RESULT:</h2>
<p>14
So, the final answer is: 14</p>
<p>Answer: 14
Table 10: An example in which no restriction on code use succeeds while not using code and using code only once fails.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9031 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9031</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9031</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-264426589</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.14389v1.pdf" target="_blank">Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models -- excelling at a wide range of NLP tasks -- to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9031.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9031.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned autoregressive large language model from OpenAI (GPT-3.5-Turbo) evaluated zero-shot on a cognitive-emotion appraisal benchmark to produce Likert ratings and natural-language rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned autoregressive transformer (OpenAI GPT-3.5 family) used via instruction prompts; generated both numeric Likert ratings and free-text rationales in a single prompt for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A dataset/benchmark of 241 COVID-related Reddit posts annotated on 24 cognitive-emotion appraisal dimensions (21 applicable in this domain). Task: zero-shot prediction of 1–9 Likert ratings per dimension (plus NA) and generation of natural-language rationales; dimensions capture appraisal constructs from psychology (e.g., responsibility, controllability, expectedness, pleasantness).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Likert-rating: Spearman's ρ = 0.388 (significant), MAE = 1.694; NA detection F1 ≈ 0.918. Rationale automatic metrics: BLEU-4 0.044, ROUGE-L 0.224, BERTScore 0.347; rationale length ≈ 58.1 tokens, % novel bigrams 81.8%. Human evaluation (rationales): factuality 0.84, relevance 0.88, justification 0.93, usefulness 0.85 (normalized 0–1 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotators (inter-annotator): average Spearman's ρ = 0.497, Krippendorff's alpha = 0.647, mean absolute difference = 1.734. Human-written rationale human-eval: factuality 0.73, relevance 0.88, justification 0.95, usefulness 0.72 (normalized 0–1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ChatGPT is the best-performing model among those evaluated: for Likert ratings it attains the highest Spearman correlation among LLMs but remains below average inter-annotator Spearman; for rationales ChatGPT's outputs score on par with or slightly above human annotators in human evaluations (higher factuality/usefulness), although automatic metrics remain modest.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot, instruction-tuned setting. ChatGPT was prompted with a 1-step prompt that elicits both a 1–9 rating and a natural-language rationale; responses parsed by extracting the first numeric 1–9. Temperature = 0.1. Evaluation averaged across 5 sampling seeds. NA label treated as binary classification for F1. Models were run on 4x Nvidia A40 GPUs via HuggingFace/OpenAI APIs as applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Dataset small (241 posts) and domain-specific (COVID-19 Reddit); gold annotations come from 2 trained annotators (double-annotated subset of 40). Appraisal prediction is inherently subjective (inter-annotator variability). Automatic metrics poorly capture rationale correctness; human evaluation required. ChatGPT's higher human-eval scores partly attributable to verbosity/extractiveness, which can inflate perceived usefulness/factuality. Only zero-shot evaluation was performed (no fine-tuning).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9031.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5-XXL (instruction-tuned T5, 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-fine-tuned encoder-decoder transformer (FLAN-T5-XXL) evaluated zero-shot with a two-step pipeline to predict appraisal Likert ratings and to produce rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction fine-tuned variant of T5 (encoder-decoder) with ~11B parameters, optimized for following natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same as above: 21 applicable appraisal dimensions from 241 Reddit posts; tasks: Likert rating prediction and rationale generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Likert-rating: second-best among evaluated LLMs (specific Spearman/MAE values not reported in text excerpt). NA detection and ratings performance lower than ChatGPT. Rationale automatic metrics: BLEU-4 0.008, ROUGE-L 0.066, BERTScore 0.053; rationale length ≈ 45.3 tokens, % novel bigrams 16.0% (more extractive). Human evaluation (rationales): factuality 0.40, relevance 0.29, justification 0.24, usefulness 0.13 (normalized 0–1).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator metrics as in paper: avg Spearman 0.497, Krippendorff's alpha 0.647; human rationale-eval: factuality 0.73, relevance 0.88, justification 0.95, usefulness 0.72.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>FLAN-T5-XXL performed behind ChatGPT: it was the second-best for Likert ratings but produced poorer-quality rationales (both by automatic and human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot two-step prompting pipeline: first elicit numeric rating, then conditionally elicit rationale (used because 1-step prompting produced ill-formed outputs for most non-ChatGPT models). Temperature = 0.1. Outputs parsed and cleaned via regex to extract first numeric 1–9.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>FLAN-T5's rationales were notably more extractive and shorter; it sometimes returned the same text for rating and rationale, reducing relevance/usefulness. Like others, evaluated only zero-shot; performance may differ after fine-tuning/in-context learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9031.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned LLaMA-based model (Alpaca) evaluated zero-shot in a two-step pipeline for appraisal ratings and rationale generation; showed good NA detection but poor rating correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-following model derived from LLaMA and fine-tuned on an instruction dataset (Stanford Alpaca); smaller open-source model commonly used as a baseline for instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same appraisal benchmark; tasks: Likert ratings (1–9 with NA) and rationale generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>NA detection F1 ≈ 0.918 (tied with ChatGPT). Likert-rating predictions: poor — reported negative correlations with gold labels in overall results (specific Spearman not provided in excerpt). Rationale automatic metrics: BLEU-4 0.040, ROUGE-L 0.230, BERTScore 0.297; rationale length ≈ 48.7 tokens, % novel bigrams 71.9%. Human evaluation (rationales): factuality 0.55, relevance 0.82, justification 0.82, usefulness 0.51 (normalized 0–1).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator and rationale-eval baselines as reported in paper (avg Spearman 0.497; rationale human-eval factuality 0.73 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Alpaca-7B could reliably detect whether an appraisal dimension was 'not mentioned' but performed poorly on producing correlated numeric ratings (negative correlations reported); its rationales scored middling in human evaluation (better relevance/justification than FLAN-T5, worse factuality/usefulness compared to ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot, two-step prompting (rating then rationale). Models sampled 5 times and averaged. Temperature = 0.1. Observed that increasing Alpaca model size to Alpaca-13B did not improve and sometimes degraded performance on rating task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Alpaca tended to give constant ratings for some dimensions (lack of variance), producing zero correlation in those dimensions. Open-source instruction-tuned models lagged ChatGPT on the rating task; rationale quality was variable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9031.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger variant of Alpaca evaluated similarly; authors observed degraded or unstable performance compared to the 7B Alpaca on this appraisal task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-following LLaMA-derived model at ~13B parameters (Alpaca family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same dataset and tasks: 1–9 Likert rating prediction and rationale generation per appraisal dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to perform worse than Alpaca-7B on the Likert-rating task (instances of degraded performance; some rationale outputs degenerated to placeholder text in Alpaca-13B). Automatic/human-rationale metrics for Alpaca-13B were reported in appendices (overall poor relative to Alpaca-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator statistics as in the paper (avg Spearman 0.497; Krippendorff's alpha 0.647).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Alpaca-13B underperformed Alpaca-7B on this task (counterintuitive scaling behavior noted by authors); both underperform ChatGPT on rating correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot two-step prompting. Observed failure modes (e.g., returning 'Tell us why.' as rationale many times for Alpaca-13B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution against assuming larger open-source instruction-tuned models will necessarily perform better on subjective appraisal tasks; stability of outputs and degenerate generations observed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9031.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dolly-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dolly-V2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned model (Dolly v2) evaluated in the appraisal benchmark that generally underperformed on both rating correlation and rationale quality relative to stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dolly-V2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-following language model (Dolly family), evaluated zero-shot in a two-step prompting pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same appraisal dimensions and tasks: numeric rating prediction and rationale generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported poor performance on Likert-rating correlations; rationale metrics (automatic/human) lower than ChatGPT and Alpaca-7B. Specific numeric values for Dolly-7B are provided in appendices (overall low).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator baselines as reported in paper (e.g., human rationale-eval factuality 0.73 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Dolly-7B underperformed ChatGPT and Alpaca-7B on both rating and rationale tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot two-step prompting; temperature 0.1; outputs cleaned and parsed the same way as other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Open-source Dolly models showed weaker performance and in some cases lack of variance or nonsensical outputs for rationales.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9031.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dolly-12B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dolly-V2 (12B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Dolly variant evaluated; similarly underperformed on appraisal rating correlation and rationale quality relative to ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dolly-V2 (12B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLM (Dolly family) at ~12B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Likert rating prediction (1–9) and rationale generation across appraisal dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported poor/negative correlations on Likert-rating task for some models including Dolly-12B; rationale automatic and human-eval metrics are low compared to ChatGPT. Exact numbers are in appendices; aggregate description in main text indicates poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator baselines as reported (avg Spearman 0.497; human rationale-eval factuality 0.73 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Dolly-12B performed worse than ChatGPT and in many respects worse than some smaller instruction-tuned models on rating correlation and rationale quality.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot two-step prompting, temperature 0.1, outputs aggregated over 5 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>As with other open-source models, Dolly family models displayed variable quality and in many cases lacked the nuance needed to match human appraisal labels.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9031.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (davinci, non-instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vanilla GPT-3 davinci (non-instruction-tuned) was tried but produced largely nonsensical or ill-formed outputs for the appraisal tasks and thus its results were excluded from the main analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 davinci</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base GPT-3 model (not instruction-tuned); the authors attempted to prompt it but found outputs often failed to follow the task format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same appraisal benchmark but non-instruction-tuned models failed to produce sensible rating+rationale pairs in this zero-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Not reported quantitatively — authors state non-instruction-tuned models (including GPT-3 davinci) largely failed to generate sensible outputs and were therefore not included in paper's main quantitative tables.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator baselines as reported in the paper (avg Spearman 0.497 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 davinci was not competitive in this zero-shot appraisal evaluation and produced ill-formed responses; excluded from main results.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tried with the same prompting templates but produced nonsensical responses; therefore authors omitted numerical evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Non-instruction-tuned base models may require in-context examples or fine-tuning to perform format-sensitive tasks like joint numeric rating + rationale generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9031.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9031.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B / 13B) models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Base LLaMA models were experimented with (7B and 13B) but produced poor or repetitive outputs on the appraisal tasks; authors show examples illustrating failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B / 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLaMA family models (non-instruction-tuned) tested via the two-step prompting templates; found to produce nonsensical/repetitive outputs for rating and rationale generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>COVIDET-APPRAISALS (cognitive appraisal dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same appraisal benchmark; non-instruction-tuned LLaMA variants were unable to reliably follow the task format in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Not included in main quantitative results — authors report LLaMA outputs were nonsensical and therefore not evaluated numerically in the main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human annotator baselines per paper (avg Spearman 0.497 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA (non-instruction-tuned) failed to produce usable outputs for this task in the zero-shot prompting setup used by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tested with the paper's 2-step prompting templates; example outputs in appendices show highly repetitive or off-topic generations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Again, non-instruction-tuned models often require either instruction fine-tuning or in-context examples to perform structured labelling + rationale tasks; poor zero-shot behavior does not imply inability after finetuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human emotion knowledge representation emerges in large language models and supports discrete emotion inference <em>(Rating: 2)</em></li>
                <li>Appraisal theories for emotion classification in text <em>(Rating: 1)</em></li>
                <li>ChatGPT: Jack of all trades, master of none <em>(Rating: 1)</em></li>
                <li>The false promise of imitating proprietary llms <em>(Rating: 1)</em></li>
                <li>Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9031",
    "paper_id": "paper-264426589",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "GPT-3.5-Turbo (ChatGPT)",
            "brief_description": "An instruction-tuned autoregressive large language model from OpenAI (GPT-3.5-Turbo) evaluated zero-shot on a cognitive-emotion appraisal benchmark to produce Likert ratings and natural-language rationales.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (ChatGPT)",
            "model_description": "Instruction-tuned autoregressive transformer (OpenAI GPT-3.5 family) used via instruction prompts; generated both numeric Likert ratings and free-text rationales in a single prompt for this task.",
            "model_size": null,
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "A dataset/benchmark of 241 COVID-related Reddit posts annotated on 24 cognitive-emotion appraisal dimensions (21 applicable in this domain). Task: zero-shot prediction of 1–9 Likert ratings per dimension (plus NA) and generation of natural-language rationales; dimensions capture appraisal constructs from psychology (e.g., responsibility, controllability, expectedness, pleasantness).",
            "llm_performance": "Likert-rating: Spearman's ρ = 0.388 (significant), MAE = 1.694; NA detection F1 ≈ 0.918. Rationale automatic metrics: BLEU-4 0.044, ROUGE-L 0.224, BERTScore 0.347; rationale length ≈ 58.1 tokens, % novel bigrams 81.8%. Human evaluation (rationales): factuality 0.84, relevance 0.88, justification 0.93, usefulness 0.85 (normalized 0–1 scale).",
            "human_baseline_performance": "Human annotators (inter-annotator): average Spearman's ρ = 0.497, Krippendorff's alpha = 0.647, mean absolute difference = 1.734. Human-written rationale human-eval: factuality 0.73, relevance 0.88, justification 0.95, usefulness 0.72 (normalized 0–1).",
            "performance_comparison": "ChatGPT is the best-performing model among those evaluated: for Likert ratings it attains the highest Spearman correlation among LLMs but remains below average inter-annotator Spearman; for rationales ChatGPT's outputs score on par with or slightly above human annotators in human evaluations (higher factuality/usefulness), although automatic metrics remain modest.",
            "experimental_details": "Zero-shot, instruction-tuned setting. ChatGPT was prompted with a 1-step prompt that elicits both a 1–9 rating and a natural-language rationale; responses parsed by extracting the first numeric 1–9. Temperature = 0.1. Evaluation averaged across 5 sampling seeds. NA label treated as binary classification for F1. Models were run on 4x Nvidia A40 GPUs via HuggingFace/OpenAI APIs as applicable.",
            "limitations_or_caveats": "Dataset small (241 posts) and domain-specific (COVID-19 Reddit); gold annotations come from 2 trained annotators (double-annotated subset of 40). Appraisal prediction is inherently subjective (inter-annotator variability). Automatic metrics poorly capture rationale correctness; human evaluation required. ChatGPT's higher human-eval scores partly attributable to verbosity/extractiveness, which can inflate perceived usefulness/factuality. Only zero-shot evaluation was performed (no fine-tuning).",
            "uuid": "e9031.0"
        },
        {
            "name_short": "FLAN-T5-XXL",
            "name_full": "FLAN-T5-XXL (instruction-tuned T5, 11B)",
            "brief_description": "An instruction-fine-tuned encoder-decoder transformer (FLAN-T5-XXL) evaluated zero-shot with a two-step pipeline to predict appraisal Likert ratings and to produce rationales.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-XXL",
            "model_description": "Instruction fine-tuned variant of T5 (encoder-decoder) with ~11B parameters, optimized for following natural-language instructions.",
            "model_size": "11B",
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Same as above: 21 applicable appraisal dimensions from 241 Reddit posts; tasks: Likert rating prediction and rationale generation.",
            "llm_performance": "Likert-rating: second-best among evaluated LLMs (specific Spearman/MAE values not reported in text excerpt). NA detection and ratings performance lower than ChatGPT. Rationale automatic metrics: BLEU-4 0.008, ROUGE-L 0.066, BERTScore 0.053; rationale length ≈ 45.3 tokens, % novel bigrams 16.0% (more extractive). Human evaluation (rationales): factuality 0.40, relevance 0.29, justification 0.24, usefulness 0.13 (normalized 0–1).",
            "human_baseline_performance": "Human annotator metrics as in paper: avg Spearman 0.497, Krippendorff's alpha 0.647; human rationale-eval: factuality 0.73, relevance 0.88, justification 0.95, usefulness 0.72.",
            "performance_comparison": "FLAN-T5-XXL performed behind ChatGPT: it was the second-best for Likert ratings but produced poorer-quality rationales (both by automatic and human evaluation).",
            "experimental_details": "Zero-shot two-step prompting pipeline: first elicit numeric rating, then conditionally elicit rationale (used because 1-step prompting produced ill-formed outputs for most non-ChatGPT models). Temperature = 0.1. Outputs parsed and cleaned via regex to extract first numeric 1–9.",
            "limitations_or_caveats": "FLAN-T5's rationales were notably more extractive and shorter; it sometimes returned the same text for rating and rationale, reducing relevance/usefulness. Like others, evaluated only zero-shot; performance may differ after fine-tuning/in-context learning.",
            "uuid": "e9031.1"
        },
        {
            "name_short": "Alpaca-7B",
            "name_full": "Alpaca (7B)",
            "brief_description": "An open-source instruction-tuned LLaMA-based model (Alpaca) evaluated zero-shot in a two-step pipeline for appraisal ratings and rationale generation; showed good NA detection but poor rating correlation.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "An instruction-following model derived from LLaMA and fine-tuned on an instruction dataset (Stanford Alpaca); smaller open-source model commonly used as a baseline for instruction-following.",
            "model_size": "7B",
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Same appraisal benchmark; tasks: Likert ratings (1–9 with NA) and rationale generation.",
            "llm_performance": "NA detection F1 ≈ 0.918 (tied with ChatGPT). Likert-rating predictions: poor — reported negative correlations with gold labels in overall results (specific Spearman not provided in excerpt). Rationale automatic metrics: BLEU-4 0.040, ROUGE-L 0.230, BERTScore 0.297; rationale length ≈ 48.7 tokens, % novel bigrams 71.9%. Human evaluation (rationales): factuality 0.55, relevance 0.82, justification 0.82, usefulness 0.51 (normalized 0–1).",
            "human_baseline_performance": "Human annotator and rationale-eval baselines as reported in paper (avg Spearman 0.497; rationale human-eval factuality 0.73 etc.).",
            "performance_comparison": "Alpaca-7B could reliably detect whether an appraisal dimension was 'not mentioned' but performed poorly on producing correlated numeric ratings (negative correlations reported); its rationales scored middling in human evaluation (better relevance/justification than FLAN-T5, worse factuality/usefulness compared to ChatGPT).",
            "experimental_details": "Zero-shot, two-step prompting (rating then rationale). Models sampled 5 times and averaged. Temperature = 0.1. Observed that increasing Alpaca model size to Alpaca-13B did not improve and sometimes degraded performance on rating task.",
            "limitations_or_caveats": "Alpaca tended to give constant ratings for some dimensions (lack of variance), producing zero correlation in those dimensions. Open-source instruction-tuned models lagged ChatGPT on the rating task; rationale quality was variable.",
            "uuid": "e9031.2"
        },
        {
            "name_short": "Alpaca-13B",
            "name_full": "Alpaca (13B)",
            "brief_description": "A larger variant of Alpaca evaluated similarly; authors observed degraded or unstable performance compared to the 7B Alpaca on this appraisal task.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "Alpaca-13B",
            "model_description": "An instruction-following LLaMA-derived model at ~13B parameters (Alpaca family).",
            "model_size": "13B",
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Same dataset and tasks: 1–9 Likert rating prediction and rationale generation per appraisal dimension.",
            "llm_performance": "Reported to perform worse than Alpaca-7B on the Likert-rating task (instances of degraded performance; some rationale outputs degenerated to placeholder text in Alpaca-13B). Automatic/human-rationale metrics for Alpaca-13B were reported in appendices (overall poor relative to Alpaca-7B).",
            "human_baseline_performance": "Human annotator statistics as in the paper (avg Spearman 0.497; Krippendorff's alpha 0.647).",
            "performance_comparison": "Alpaca-13B underperformed Alpaca-7B on this task (counterintuitive scaling behavior noted by authors); both underperform ChatGPT on rating correlation.",
            "experimental_details": "Zero-shot two-step prompting. Observed failure modes (e.g., returning 'Tell us why.' as rationale many times for Alpaca-13B).",
            "limitations_or_caveats": "Authors caution against assuming larger open-source instruction-tuned models will necessarily perform better on subjective appraisal tasks; stability of outputs and degenerate generations observed.",
            "uuid": "e9031.3"
        },
        {
            "name_short": "Dolly-7B",
            "name_full": "Dolly-V2 (7B)",
            "brief_description": "An open-source instruction-tuned model (Dolly v2) evaluated in the appraisal benchmark that generally underperformed on both rating correlation and rationale quality relative to stronger models.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "Dolly-V2 (7B)",
            "model_description": "Open-source instruction-following language model (Dolly family), evaluated zero-shot in a two-step prompting pipeline.",
            "model_size": "7B",
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Same appraisal dimensions and tasks: numeric rating prediction and rationale generation.",
            "llm_performance": "Reported poor performance on Likert-rating correlations; rationale metrics (automatic/human) lower than ChatGPT and Alpaca-7B. Specific numeric values for Dolly-7B are provided in appendices (overall low).",
            "human_baseline_performance": "Human annotator baselines as reported in paper (e.g., human rationale-eval factuality 0.73 etc.).",
            "performance_comparison": "Dolly-7B underperformed ChatGPT and Alpaca-7B on both rating and rationale tasks in this study.",
            "experimental_details": "Zero-shot two-step prompting; temperature 0.1; outputs cleaned and parsed the same way as other open-source models.",
            "limitations_or_caveats": "Open-source Dolly models showed weaker performance and in some cases lack of variance or nonsensical outputs for rationales.",
            "uuid": "e9031.4"
        },
        {
            "name_short": "Dolly-12B",
            "name_full": "Dolly-V2 (12B)",
            "brief_description": "A larger Dolly variant evaluated; similarly underperformed on appraisal rating correlation and rationale quality relative to ChatGPT.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "Dolly-V2 (12B)",
            "model_description": "Open-source instruction-tuned LLM (Dolly family) at ~12B parameters.",
            "model_size": "12B",
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Likert rating prediction (1–9) and rationale generation across appraisal dimensions.",
            "llm_performance": "Reported poor/negative correlations on Likert-rating task for some models including Dolly-12B; rationale automatic and human-eval metrics are low compared to ChatGPT. Exact numbers are in appendices; aggregate description in main text indicates poor performance.",
            "human_baseline_performance": "Human annotator baselines as reported (avg Spearman 0.497; human rationale-eval factuality 0.73 etc.).",
            "performance_comparison": "Dolly-12B performed worse than ChatGPT and in many respects worse than some smaller instruction-tuned models on rating correlation and rationale quality.",
            "experimental_details": "Zero-shot two-step prompting, temperature 0.1, outputs aggregated over 5 runs.",
            "limitations_or_caveats": "As with other open-source models, Dolly family models displayed variable quality and in many cases lacked the nuance needed to match human appraisal labels.",
            "uuid": "e9031.5"
        },
        {
            "name_short": "GPT-3 davinci",
            "name_full": "GPT-3 (davinci, non-instruction-tuned)",
            "brief_description": "Vanilla GPT-3 davinci (non-instruction-tuned) was tried but produced largely nonsensical or ill-formed outputs for the appraisal tasks and thus its results were excluded from the main analyses.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3 davinci",
            "model_description": "Base GPT-3 model (not instruction-tuned); the authors attempted to prompt it but found outputs often failed to follow the task format.",
            "model_size": null,
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Same appraisal benchmark but non-instruction-tuned models failed to produce sensible rating+rationale pairs in this zero-shot setup.",
            "llm_performance": "Not reported quantitatively — authors state non-instruction-tuned models (including GPT-3 davinci) largely failed to generate sensible outputs and were therefore not included in paper's main quantitative tables.",
            "human_baseline_performance": "Human annotator baselines as reported in the paper (avg Spearman 0.497 etc.).",
            "performance_comparison": "GPT-3 davinci was not competitive in this zero-shot appraisal evaluation and produced ill-formed responses; excluded from main results.",
            "experimental_details": "Tried with the same prompting templates but produced nonsensical responses; therefore authors omitted numerical evaluation.",
            "limitations_or_caveats": "Non-instruction-tuned base models may require in-context examples or fine-tuning to perform format-sensitive tasks like joint numeric rating + rationale generation.",
            "uuid": "e9031.6"
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (7B / 13B) models",
            "brief_description": "Base LLaMA models were experimented with (7B and 13B) but produced poor or repetitive outputs on the appraisal tasks; authors show examples illustrating failure modes.",
            "citation_title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B / 13B)",
            "model_description": "Base LLaMA family models (non-instruction-tuned) tested via the two-step prompting templates; found to produce nonsensical/repetitive outputs for rating and rationale generation.",
            "model_size": "7B / 13B",
            "test_battery_name": "COVIDET-APPRAISALS (cognitive appraisal dimensions)",
            "test_description": "Same appraisal benchmark; non-instruction-tuned LLaMA variants were unable to reliably follow the task format in zero-shot.",
            "llm_performance": "Not included in main quantitative results — authors report LLaMA outputs were nonsensical and therefore not evaluated numerically in the main tables.",
            "human_baseline_performance": "Human annotator baselines per paper (avg Spearman 0.497 etc.).",
            "performance_comparison": "LLaMA (non-instruction-tuned) failed to produce usable outputs for this task in the zero-shot prompting setup used by the authors.",
            "experimental_details": "Tested with the paper's 2-step prompting templates; example outputs in appendices show highly repetitive or off-topic generations.",
            "limitations_or_caveats": "Again, non-instruction-tuned models often require either instruction fine-tuning or in-context examples to perform structured labelling + rationale tasks; poor zero-shot behavior does not imply inability after finetuning.",
            "uuid": "e9031.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human emotion knowledge representation emerges in large language models and supports discrete emotion inference",
            "rating": 2,
            "sanitized_title": "human_emotion_knowledge_representation_emerges_in_large_language_models_and_supports_discrete_emotion_inference"
        },
        {
            "paper_title": "Appraisal theories for emotion classification in text",
            "rating": 1,
            "sanitized_title": "appraisal_theories_for_emotion_classification_in_text"
        },
        {
            "paper_title": "ChatGPT: Jack of all trades, master of none",
            "rating": 1,
            "sanitized_title": "chatgpt_jack_of_all_trades_master_of_none"
        },
        {
            "paper_title": "The false promise of imitating proprietary llms",
            "rating": 1,
            "sanitized_title": "the_false_promise_of_imitating_proprietary_llms"
        },
        {
            "paper_title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
            "rating": 1,
            "sanitized_title": "can_chatgpt_understand_too_a_comparative_study_on_chatgpt_and_finetuned_bert"
        }
    ],
    "cost": 0.0197795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</p>
<p>Hongli Zhan honglizhan@utexas.edu 
Department of Linguistics
The University of Texas at Austin</p>
<p>Desmond C Ong desmond.ong@utexas.edu 
Department of Psychology
The University of Texas at Austin</p>
<p>Junyi Jessy Li 
Rohan Taori 
Department of Linguistics
The University of Texas at Austin</p>
<p>Ishaan Gulrajani 
Tianyi Zhang 
Yann Dubois 
Xuechen Li 
Carlos Guestrin 
Percy Liang 
Tatsunori B Hashimoto 
Stanford 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Baptiste Rozière 
Naman Goyal 
Eric Hambro 
Thomas Wolf 
Lysandre Debut 
Victor Sanh 
Julien Chaumond 
Clement Delangue 
Anthony Moi 
Pier- Ric Cistac 
Tim Rault 
Remi Louf 
Morgan Funtow- Icz 
Joe Davison 
Sam Shleifer 
Patrick Von Platen 
Clara Ma 
Yacine Jernite 
Julien Plu 
Canwen Xu 
Teven Le Scao 
Sylvain Gugger 
Mariama Drame 
Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models
05B03258E923CD81B878CF518AD18CBAarXivpreprintarXiv:2304.03347.
The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values(Scherer, 2005).Thus, the same situation can often result in different emotional experiences.While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals.This work fills the gap by presenting COVIDET-APPRAISALS, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts.COVIDET-APPRAISALS presents an ideal testbed to evaluate the ability of large language models -excelling at a wide range of NLP tasks -to automatically assess and explain cognitive appraisals.We found that while the best models are performant, opensourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models.We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.</p>
<p>Introduction</p>
<p>Emotions constitute a crucial aspect of people's lives, and understanding them has a profound impact on improving public mental health problems as well as policy-making (Choudhury and De, 2014;Gjurković and Šnajder, 2018;Arora et al., 2021;Uban et al., 2021).The emotions we experience involve complex processes: the same situation can often result in different emotional experiences, based on an individual's subjective evaluations.These are called cognitive appraisals, and have been extensively studied in psychology through theoretical, behavioral, and hand-coded studies (Arnold, 1960;Lazarus, 1966;Lazarus et al., 1980;Roseman, 1984;Scherer et al., 1984; Smith and I've recently heard of stories that people who are getting mildly sick a er being vaccinated are still coming down with long covid.People on the covid long hauler subreddit are saying that we're going to have a huge problem on our hands because no vaccine protects against long covid… This isn't going to end, isn't it?Right as I think I've tasted freedom, I nd out information that makes me want to stay inside forever like a hermit...</p>
<p>[Scale]: 1 → [Rationale]: The narrator does not talk about themselves in relation to the COVID situation in a way that makes it seem like they believe they are responsible.</p>
<p>1.</p>
<p>To what extent did the narrator think that THEY were responsible for causing the situation?</p>
<p>[Scale]: 2 → [Rationale]: The narrator mentions how many people who are vaccinated and get mildly sick still contract long COVID, but they do not talk about them in a way that seems like they are blaming them for the situation.</p>
<p>2.</p>
<p>To what extent did the narrator think that OTHER PEOPLE were responsible for causing the situation?…</p>
<p>[Scale]: 3 → [Rationale]: The narrator really didn't expect this situation since they mention being able to taste freedom, believing the pandemic is ending, when suddenly they heard news that vaccinated people are still getting long covid and now they think the pandemic will never end.</p>
<p>To what extent did the narrator EXPECT the situation to occur?</p>
<p>[Scale]: 9 → [Rationale]: The narrator mentions long COVID and the inability to currently protect against long COVID.This is a factor that is still outside of anyone's control, including doctors and nurses, and the situation continues to evolve in uncontrollable ways.</p>
<p>3.</p>
<p>To what extent did the narrator think that CIRCUMSTANCES BEYOND ANYONE'S CONTROL were responsible for causing the situation?Figure 1: An example from COVIDET-APPRAISALS.The fact that the narrator is blaming nobody but circumstances beyond anyone's control for causing long-COVID contributes to their feeling of sadness.We showcase an annotation together with LLMs' responses in Appendix §A.Ellsworth, 1985;Weiner, 1985;Clore and Ortony, 2000;Roseman and Smith, 2001;Scherer et al., 2001;Ellsworth and Scherer, 2003;Sander et al., 2005;Ong et al., 2015Ong et al., , 2019;;Ortony et al., 2022;Yeo and Ong, 2023).For instance, being fired from a job, if judged to be due to one's own controllable mistakes, could result in regret; if evaluated to be unfair and due to someone else's intentional actions, would make one feel angry; and if appraised to be leaving a toxic work environment, could instead result in relief and even happiness.The different dimensions along which people subjectively inter-arXiv:2310.14389v1[cs.CL] 22 Oct 2023 pret or appraise the situation characterizes the specific emotions they feel (Moors et al., 2013).</p>
<p>Although emotion detection is a well-established NLP task (Strapparava and Mihalcea, 2007;Mihalcea and Strapparava, 2012;Wang et al., 2012;Lei et al., 2014;Abdul-Mageed and Ungar, 2017;Khanpour and Caragea, 2018;Liu et al., 2019;Sosea and Caragea, 2020;Demszky et al., 2020;Desai et al., 2020;Sosea et al., 2022), it mostly involves classification from text to emotion labels directly, skipping the appraisal step that is necessary to interpret why the emotion is experienced by an individual in a particular event.Hence, we do not yet have a data-driven understanding of these cognitive appraisals in textual data.Yet recent work has started to show its necessity : Hofmann et al. (2020) showed that appraisals are informative for an emotion detection model; Zhan et al. (2022) further recognized appraisals to be an integral part of emotion triggers, though appraisals were not explicit in their work.</p>
<p>This work aims at construing an empirical, explicit understanding of perceived cognitive appraisals in human readers and large language models (LLMs) alike, via a comprehensive 24 dimensions, along with their corresponding natural language rationales.A language model's capability of assessing cognitive appraisals reflects a more nuanced understanding of emotions, where it could contextualize individual subjectivity in responses to the same situation, while offering explanations ("they are feeling [emotion] because of [appraisal]").This could be groundwork for emotional support agents, e.g., one capable of positive reframing (Ziems et al., 2022) or producing empathetic responses.</p>
<p>We first introduce COVIDET-APPRAISALS, a dataset of 24 appraisal dimensions annotated across 241 Reddit posts sourced from Zhan et al. (2022) about COVID-19.Each post was manually annotated with 24 appraisal dimensions from a recent meta-analysis covering all appraisal dimensions proposed and studied in the literature (Yeo and Ong, 2023).For each appraisal dimension, annotators not only rated the extent to which they perceived the narrator is experiencing the said dimension, but also provided a rationale in their own language to justify their rating selection.An example from COVIDET-APPRAISALS is shown in Figure 1.</p>
<p>COVIDET-APPRAISALS serves as an ideal testbed to evaluate the capability of a model to un-cover implicit information for emotion understanding.Benchmarking on COVIDET-APPRAISALS, we evaluate the performance of LLMs to (1) provide Likert-scale ratings for the appraisal dimensions; and (2) generate natural language rationales for their ratings.The elicitation of the rationales can be seen as a way of probing (Le Scao and Rush, 2021;Gu et al., 2022), where we prefix a question with an elaborated situation.We evaluate a range of LLMs, including ChatGPT, Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), Dolly (Conover et al., 2023).With an extensive human evaluation of the natural language rationales from LLMs as well as our annotators, we find that ChatGPT performs on par with (and in some cases better than) human-annotated data; this opens a new avenue of investigation to improve its performance on emotion-related tasks (Kocoń et al., 2023).In comparison, other open-sourced LLMs fall short on this task, presenting a new challenge in the future development of emotionally intelligent open models.</p>
<p>We publicly release our annotated dataset COVIDET-APPRAISALS, model outputs, and our human evaluation data at https://github.com/honglizhan/CovidET-Appraisals-Public.</p>
<p>Background and Related Work</p>
<p>Cognitive Appraisal Theories.The cognitive appraisal theories of emotion state that emotions arise from an individual's subjective understanding and interpretation of situations that hold personal importance for their overall well-being (Arnold, 1960;Lazarus, 1966;Lazarus et al., 1980;Roseman, 1984;Scherer et al., 1984;Smith and Ellsworth, 1985;Weiner, 1985;Clore and Ortony, 2000;Roseman and Smith, 2001;Scherer et al., 2001;Sander et al., 2005;Ortony et al., 2022).In practical terms, people interpret and appraise situations along a range of different dimensions, and it is the specific manner in which they appraise their situations that give rise to the distinct emotions they experience.The primary focus of cognitive appraisal theories of emotions revolves around the identification of these appraisal dimensions that are associated with specific emotional experiences and how these dimensions contribute to distinguishing between different emotional states (Lazarus, 1993;Roseman, 1996;Scherer et al., 2001;Moors, 2010;Scherer and Moors, 2019).</p>
<p>While appraisal theorists agree on the impor-tance of motivationally-relevant appraisals in triggering emotions, they have not reached a consensus on the specific appraisal dimensions that play a significant role in this process (Yeo and Ong, 2023).</p>
<p>Various theories have put forth distinct sets of appraisal dimensions that are considered crucial in triggering and distinguishing emotions.From prior literature, Yeo and Ong (2023) identified and assembled a taxonomy of all appraisal dimensions that have been studied, and produced a condensed list of 24 cognitive appraisal dimensions which we focus on in this paper.</p>
<p>Cognitive Appraisals in NLP.Appraisals provide the necessary computational structure allowing for the distillation of real-life situations that depend on a multitude of factors into a (large but) finite set of appraisal dimensions (Ong et al., 2015).Despite its importance, however, few works have explored the implications of cognitive appraisals on emotions in NLP.Hofmann et al. (2020) experimented with a small set of cognitive appraisal dimensions (including attention, certainty, effort, pleasantness, responsibility, control, and circumstance) to assist the automatic detection of emotions in text, and found that accurate predictions of appraisal dimensions boost emotion classification performance.They introduced a dataset of 1, 001 sentences following the template "I feel [emotion], when ..." (average sentence length: 27 tokens).In comparison, our work covers a much wider range of 24 appraisal dimensions found in prior literature, over lengthy (176 tokens on average) Reddit posts that were natural and emotionally charged.We also collect natural language rationales as a key contribution to reveal human's in-depth understanding of such cognitive appraisals in context.</p>
<p>Recent studies (Zhan et al., 2022;Sosea et al., 2023) acknowledged both what happened and how one appraised the situation as inherent components of emotion triggers, although the appraisal of events was not explicit in their work.Instead we provide datasets and perform evaluation on appraisals explicitly, such that language models can build on this work to achieve a comprehensive and explicit understanding of cognitive appraisals from written text.</p>
<p>LLMs on Emotion-Related Tasks.Autoregressive LLMs have been explored extensively in emotion-related tasks such as sentiment analysis (Zhong et al., 2023;Qin et al., 2023;Susnjak, 2023), emotion recognition (Kocoń et al., 2023), disclosing the representation of human emotions encapsulated in LLMs (Li et al., 2023), and interpreting mental health analysis (Yang et al., 2023).However, few have tapped into the understanding of cognitive appraisals of emotions innate in LLMs.In this work, we dive into the extent to which LLMs comprehend the profound cognitive appraisals underlying emotions in situations, and further elicit natural language rationales from the language models to disclose the reason behind such predictions from the otherwise baffling black-box LLMs (Gilpin et al., 2018).Aligning with Marasović et al. (2020) who performed human evaluation on rationales generated by GPT, we additionally perform an in-depth human evaluation of the rationales from human annotators and LLMs alike on the novel task of providing natural language explanations for cognitive appraisals of situations that underlie narrators' emotional experiences.</p>
<p>3 The COVIDET-APPRAISALS Dataset COVIDET-APPRAISALS contains 241 Reddit posts sampled from the COVIDET dataset (Zhan et al., 2022), where the Reddit posts are sourced from r/COVID19_support.Each post is manually annotated with one or more of the 7 emotions: anger, anticipation, joy, trust, fear, sadness, and disgust.The 241 posts in COVIDET-APPRAISALS have an average of 175.82 tokens and 2.67 emotions per post.From Yeo and Ong (2023)'s work, we identify 24 cognitive emotion appraisal dimensions (Table 1).We provide the instructions given to the annotators (including the full questions for each of these 24 dimensions) in Appendix §B.</p>
<p>Annotators.We recruited 2 linguistics students at a university to work on our annotation task; both of them are native speakers of English.Both annotators underwent training using a set of posts already annotated by our group.Throughout the annotation, we monitored the inter-annotator agreement and provided feedback on their work.</p>
<p>Instructions.Given a Reddit post from COVIDET, annotators are asked to judge 24 emotion appraisal dimensions pertaining to how the narrator feels about and views the situation that they are going through (e.g., whether the narrator feels the situation they are in is something they could control).For each appraisal dimension, annotators need to select a Likert rating on the   scales of 1 to 9. A "not mentioned" (NA) option is provided in case the dimension being asked is absent in the given post.In addition, we also ask the annotators to provide rationales for their ratings in the form of natural language explanations.On average, our trained annotators spent around 30 minutes to complete the annotation of one post.Owing to the immense effort involved, we doubly annotate 40 posts to measure inter-annotator agreement while leaving the rest annotated by one annotator.</p>
<p>Post-Processing and Aggregation.Given a fixed topic (COVID-19 in our case), it is highly likely that certain dimensions frequently don't ap-ply (Yeo and Ong, 2023).This can be seen in Figure 2 which plots the percentage of NA labels: dimensions such as civ (consistency with internal values), fair (fairness), and csn (consistency with social norms) contain mostly NA labels (around 80%).Therefore, we remove these dimensions from subsequent analyses and evaluations of the dataset.This results in a total of 21 applicable appraisal dimensions in COVIDET-APPRAISALS.</p>
<p>We collected 241 posts in total.For the subset of 40 posts that are doubly annotated, we aggregate the Likert-scale ratings by taking the mean of each post's ratings for each appraisal dimension (if an annotator labels a dimension as NA, we then exclude the particular dimension of that post that they annotate).In terms of the rationales, we consider both rationales as ground truth references and use multi-reference metrics in our experiments.</p>
<p>Inter-Annotator Agreement.We report interannotator agreement on the Likert-scale ratings.Since there is no reliable, automatic way to evaluate natural language rationales (as discussed in §4), we evaluate them with human validation in §7.2.</p>
<p>To measure the agreement for selecting the NA label, we average the Fleiss' Kappa values (Fleiss, 1971;Randolph, 2005) across all 24 appraisal dimensions, yielding a value of 0.769 indicating substantial agreement (Artstein and Poesio, 2008).</p>
<p>For the 1-9 Likert-scale ratings, we report on the 21 applicable dimensions: (1) Spearman's ρ between our two annotators, calculated per dimension then averaged across all dimensions; (2) Krippendorff's alpha (using interval distance) (Krippendorff, 1980); and (3) mean absolute difference (abs.delta).Here the agreement is calculated if neither annotator gave a NA judgment.Krippendorff's alpha yields a value of 0.647 indicating substantial agreement (Artstein and Poesio, 2008).The average Spearman's correlation is 0.497 with significance, and the absolute delta values also have a small mean of 1.734.These measures indicate that while the task is subjective, annotators do align with each other with only a small difference compared to the scale of ratings (1-9).Agreement values differ by dimension, which we showcase in Appendix C.</p>
<p>Dataset Analysis</p>
<p>How do the scales distribute across dimensions and emotions?The distribution of the Likertscale ratings is shown in Figure 3.The rat-  ings for some dimensions are consistent (e.g., dimensions crsp (circumstances-responsibility), ccrl (circumstances-controllable), and chl (challenge)), whereas for some other dimensions, the ratings have higher variance (e.g., dimensions ocrl (othercontrollable) and loss).</p>
<p>We analyze the connections between our Likertscale annotations and COVIDET's emotion annotations.Figure 4 shows the mean Likert-scale rating for each dimension within each post with respect to the perceived emotion.While it is evident that most dimensions show consistency (the posts are all related to COVID-19), some emotions stand out distinctly in particular dimensions.For example, trust and joy have higher Likert-scale ratings on dimensions pfc (problem-focused coping) and gcnd (goal conduciveness) compared to other emotions, suggesting the inter-correlation between these appraisal dimensions with positive emotions.We further explore whether appraisal dimensions alone are indicative of perceived emotions already annotated in COVIDET in Appendix §D.1.</p>
<p>What are the characteristics of the natural language rationales?On average, each rationale is 1.2 sentences (std.dev= 0.4) and 28.9 tokens (std.dev= 10.0)long.Following Marfurt and Henderson (2021), we also measure the abstractiveness of the rationales from our human annotators by calculating the percentage of novel bigrams in the rationales with respect to the Reddit posts and in- Table 2: Automatic measures of similarity on the natural language rationales of COVIDET-APPRAISALS.BASELINE-P denotes "baseline (same dimension, different posts)", and BASELINE-D denotes "baseline (same post, different dimensions)".</p>
<p>structions (i.e., evaluating a specific appraisal dimension) that the annotators were given.As shown in Table 4, our human annotators attain a % of novel bigrams of 86.7%, indicating a high abstractiveness.We showcase the most prominent topics extracted from the annotated rationales using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) in Appendix §D.2.</p>
<p>Are rationales repetitive?We also look into automatic measures of similarity to assess how much rationales from different annotators, or from different dimensions/posts, differ from one another.Specifically, we calculate BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and re-scaled BERTScore (Zhang et al., 2019) between our two annotators' rationales.We establish 2 random baselines for comparison: (1) rationales of the same dimension from different posts;</p>
<p>(2) rationales from different dimensions within the same post.In each case we report similarity between 3 randomly sampled rationales and the annotated ones.</p>
<p>Table 2 shows that the textual similarity in all conditions are somewhat low; the BLEU and ROUGE scores show that there is very little lexical overlap, although BERTScore shows higher semantic similarity between two annotators for the same dimension within the same post.Upon closer inspection, we observe that these commonly used automatic measures do not adequately capture semantic similarity in our dataset (see Appendix §D.3 for an example).This adds to the challenge of evaluating rationales; as a result, we resort to the human evaluation in §7.2.</p>
<p>Can LLMs understand emotional appraisals?</p>
<p>COVIDET-APPRAISALS provides an ideal testbed that evaluates models' performance on predicting both the Likert ratings, as well as their nat-ural language explanations.Using COVIDET-APPRAISALS, we evaluate the zero-shot performance of LLMs in an attempt to evaluate their innate ability to comprehend emotional appraisals from social media text without in-context learning.</p>
<p>Models.We evaluate the following instructiontuned LLMs 1 : 1) ChatGPT, i.e., GPT-3.5-Turbo;</p>
<p>2) FLAN-T5-XXL (11B) (Chung et al., 2022), which is the instruction fine-tuned version of T5 (Raffel et al., 2020); 3) Alpaca (7B, 13B Prompts and Setup.The templates for prompting the LLMs are shown in Appendix Figure 17.</p>
<p>After extensive experimentation, we found that only ChatGPT is able to generate both a rating and a rationale with a single prompt; this type of "1-step" prompting leads to ill-formed responses for other models.Thus, for models other than Chat-GPT, we instead use a pipeline or "2-step" prompting similar to the strategy used in Press et al. (2022): we first elicit the rating for the appraisal dimension, then conditioned on the response for the rating we further elicit the rationale for the selection.</p>
<p>We carry out all our experiments on 4 Nvidia A40 GPUs.We use the HuggingFace Transformers (Wolf et al., 2020) library for model inference.We set the temperature value of all models to 0.1. 2To enable a fair comparison of models, we sample from the LLMs five times with different model initializations and report average values for both scales and rationales.</p>
<p>Evaluation: Likert-Scale Ratings</p>
<p>We report model performance for Likert-scale ratings on the 21 applicable dimensions using two 1 While we have also experimented with non-instructiontuned LLMs (including GPT-3 davinci and LLaMA (7B and 13B), they largely fail to generate sensible outputs for this task.</p>
<p>We showcase examples of responses from non-instructiontuned models in Appendix §A.For these reasons, we do not include their results in this paper. 2We experimented with higher temperatures on a validation set consisting of 10 Reddit posts annotated by our group which are not included in COVIDET-APPRAISALS, and the models yielded worse and more unstable performance.Table 3: Experiment results from LLMs.† indicates p &lt; 0.1 for Spearman correlation, and † † indicates p &lt; 0.05.In addition, we also provide the results of the F1 score on measuring the agreement between the models' ratings and the gold ratings for selecting the "not mentioned" label across all 24 dimensions.</p>
<p>standard regression metrics: Mean Absolute Error (MAE) and Spearman's correlation.We treat the selection of the NA labels as a binary classification task and report F1 measures across all 24 dimensions.For the 40 gold examples that were doubly annotated by human annotators, we consider a dimension as NA when both annotators select the label.</p>
<p>Results.To evaluate the performance, we clean the responses elicited from the LLMs.Specifically, we use regular expressions to extract the first numeric value ranging from 1-9 from the scale responses3 .The results of the models' performance are shown in Table 3.We showcase examples of the models' responses in Appendix §A.Additional analyses of the LLMs' responses are shown in Appendix §G.</p>
<p>For the NA labels (Table 3, right), ChatGPT and Alpaca-7B score the highest with an F1 of 0.918.In general, the average performance across the language models we evaluate is 0.774 for F1, indicating these models are performant at predicting whether a dimension applies.</p>
<p>For the Likert-rating predictions, results show that ChatGPT-3.5 consistently yields the highest performance compared to the other language models, with a significant Spearman's correlation of 0.388 and an MAE of 1.694.We note that FLAN-T5-XXL is the second best-performing model.Alpaca and Dolly perform poorly on our task, with negative correlations with the gold labels4 .Inter- estingly, we notice a drop in performance when the size of the model parameters increases for Alpaca.</p>
<p>The results highlight the challenging nature of our task, and the gap between open-sourced LLMs vs.</p>
<p>ChatGPT (Gudibande et al., 2023).Additionally, we also measure the systems' performance on all 24 appraisal dimensions, including the 3 appraisal dimensions where the NA rates are around 80%. Results revealed marginal change in performance across all LLMs.For most LLMs the performance dropped as expected: measured with Spearman's ρ, ChatGPT-3.5 (↓ 0.018), Alpaca-7B (↓ 0.008), and Dolly-12B (↓ 0.007).On the other hand, the performance of FLAN-T5 (↑ 0.005), Alpaca-13B (↑ 0.027), and Dolly-7B (↑ 0.020) increased.</p>
<p>Evaluation: Rationales</p>
<p>As rationalizing emotional appraisals with natural language is a novel task, we perform both automatic ( §7.1) and human evaluation ( §7.2).</p>
<p>Automatic Evaluation</p>
<p>We use commonly used automatic reference-based metrics including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019), comparing generated rationales vs. annotated ones (in a multi-reference fashion).</p>
<p>Results.Similar to the performance in selecting Likert-scale ratings, ChatGPT remains the bestperforming language model in providing natural language rationales (Table 4).The values Chat-GPT achieves are lower than, though comparable to, those between different rationales from our two annotators.Alpaca-7B also achieves comparable performance in these automatic measures, the language models (specifically, Alpaca-7B and Dolly-12B) for some of the dimensions lack variance (i.e., they gave a constant rating for certain appraisal dimensions).Therefore, the Spearman correlation is set to zero in these dimensions, indicating no correlation.despite its relatively poor capability in terms of selecting Likert-scale ratings.We note that FLAN-T5 lags behind considerably compared to Chat-GPT and Alpaca-7B.We provide the additional auto-evaluation statistics for other LLMs including Dolly-7B, Dolly-12B, and Alpaca-13B in Appendix Table 11.</p>
<p>How long and how abstractive are the rationales generated by LLMs?In addition, we also measure the length and abstractiveness of the rationales generated by LLMs.Following the setup in §4, we evaluate abstractiveness using % of novel bigrams, comparing LLMs' generated rationales against the Reddit posts as well as the prompts (i.e., evaluating a specific appraisal dimension) they were given.As shown in Table 4, rationales generated by LLMs are at least 1.5x longer than those provided by our annotators, with ChatGPT being the most verbose.The LLMs also provide rationales that are more extractive compared to our annotators, with FLAN-T5 being the most extractive.</p>
<p>Human Evaluation</p>
<p>Data.Because the natural language rationales are explanations for a particular rating, we only evaluate and analyze LLM-generated rationales when the model made a near-correct prediction of the Likert-scale rating for that particular dimension compared against the gold human ratings.Specifically, we sample the intersection of (post, dimension) tuples where the 3 best-performing LLMs' (i.e., ChatGPT, FLAN-T5, and Alpaca-7B) ratings fall in the range of an absolute difference of 1 to one of the annotated scale-ratings.In cases where there are 2 gold annotations for a particular dimension, both are evaluated.In Appendix §F we also show the human evaluation of rationales for such intersection of all LLMs.We additionally evaluate human-written rationales as well, and we mix those (in random order) with LLMs' responses.</p>
<p>The above desiderata results in an evaluation of 108 rationales annotated by human annotators and 65 natural language rationales from each LLM.The evaluation covers 19 out of the 21 applicable dimensions (no such overlap is found for dimensions crsp (circumstances-responsibility) and pls (pleasantness)).Moreover, we make sure that there are no ground truth labels annotated by the human annotators in which the rating is NA.</p>
<p>Instructions.Given a Reddit post and the scale provided by the human annotators or the LLM (blinded to the annotators), annotators are asked to judge the rationales pertaining to the emotion appraisal dimension regarding the post as well as the stated scale.The rationales are distributed to annotators at random.We evaluate the natural language rationales based on the following criteria.In Appendix §H, We provide the detailed instructions and examples given to the annotators, together with the layout of the human evaluation task. 1) Factuality: For the rationale, the model may not generate something that is factual: sometimes it generates rationales for the sole purpose of justifying its answer (Ye and Durrett, 2022).Therefore, we include the aspect of hallucination and factuality as one of our evaluation criteria, and ask evaluators whether the rationale faithfully reflects what's stated in the post.Options of "Yes", "Minor Error", and "No" are provided.</p>
<p>2) Relevance: We evaluate whether the rationale directly addresses the specific appraisal dimension question that is being asked about the post.We ask evaluators on a Likert-scale of 1 to 5, with 1 being "least relevant" and 5 being "most relevant", whether the rationale focuses on the specific aspect of the post that is being appraised, and whether it strays off-topic or provides irrelevant information.</p>
<p>3) Justification: We ask human evaluators whether the rationale justifies the selected scale by adequately explaining why the selected rating scale is the most appropriate or relevant one to use for the aspect being evaluated.Annotators need to select either "Yes" or "No".</p>
<p>4) Usefulness: Finally, we evaluate whether the rationale provides useful or informative insights or explanations of useful information pertaining to the appraisal dimension being judged.Options of "Yes", "Maybe", and "No" can be selected.</p>
<p>Annotators.We recruit annotators from the Amazon Mechanical Turk (MTurk) to work on our human evaluation task.The crowd workers were involved in a pre-annotation qualification as well as training process before commencing the evaluation of the natural language rationales.We assign 2 crowd workers per natural language rationale evaluation.We ensure that the crowd workers earn a minimum salary of $10 per hour.</p>
<p>We report the inter-evaluator agreement using Krippendorff's Alpha with interval distance in Table 5, showing substantial agreement (Artstein and Poesio, 2008) across all criteria.</p>
<p>Label Transformation.For the convenience of measuring inter-annotator agreement as well as interpreting the results, we convert the labels of each criterion to numeric values within the range of 0 to 1. Specifically, for criteria Factuality, Justification, and Usefulness, "Yes" is converted to 1, "Minor Error/Maybe" to 0.5, and "No" to 0. As for the criterion Relevance which is judged on a 5-scale Likert rating, we map the Likert scale of 1 into 0, 2 into 0.25, 3 into 0.5, 4 into 0.75, and 5 into 1.</p>
<p>Results.The result of the mean ratings for each criterion from the human evaluation task is provided in Table 4.We provide box plots of the ratings as well as the human evaluation results for the rationales from all 6 LLMs in Appendix §F.</p>
<p>From Table 4 we observe that our human annotators and ChatGPT provide natural language rationales of the highest quality among all models, according to human evaluators.Surprisingly, we find ChatGPT performs on par with our human annotators, with (slightly) better performance in terms of factuality and usefulness.This can be attributed to the verbosity and extractiveness of ChatGPT (as shown in Table 4), especially in dimensions where the scale rating is low.We showcase an example in Appendix §I.</p>
<p>Alpaca-7B attains lower results compared to the other LLMs, especially in terms of the criteria factuality and usefulness.FLAN-T5, on the other hand, ranks the worst on all criteria among the LLMs.Further analysis reveals that FLAN-T5 occasionally generates responses for natural language rationales that are the same as its scale answers, resulting in irrelevant and useless rationales.</p>
<p>Conclusion</p>
<p>To achieve a more accurate and holistic understanding of emotions from written text, NLP models need to work towards understanding the subjective cognitive appraisals of emotions underlying  situations.In this work, we construe an empirical and explicit understanding of perceived cognitive appraisals in human readers and LLMs alike.We present COVIDET-APPRAISALS, a dataset of 241 Reddit posts annotated with a comprehensive range of 24 subjective cognitive appraisals that follow a situation, along with their corresponding natural language rationales.Experiments reveal that COVIDET-APPRAISALS is a vital resource to evaluate the capability of a language model to uncover implicit information for emotional understanding.</p>
<p>Our thorough evaluation of LLMs' performance on assessing emotion appraisal dimensions emphasizes that COVIDET-APPRAISALS is a challenging benchmark, and our in-depth human evaluation of the natural language rationales indicates potential areas of improvement (e.g., improving the factuality and usefulness of the rationales) for open-source LLMs.</p>
<p>Limitations</p>
<p>This work presents a new dataset entitled COVIDET-APPRAISALS to evaluate LLMs' capability in cognitive emotion appraisals.Due to the highly demanding nature of our task (e.g., the same situation can result in different subjective evaluations), COVIDET-APPRAISALS is annotated by 2 annotators.Future work can explore a larger pool of annotators.Furthermore, it should be acknowledged that COVIDET-APPRAISALS is restricted to social media posts during the COVID-19 pandemic, and they are written in English solely.This makes it challenging to evaluate LLMs' ability in other domains as well as languages.Also, we note the appraisals we collect are from the perceived end, which are not subjective appraisals from the narrators and authors themselves.We note that the size of COVIDET-APPRAISALS is relatively small.We have not intended this dataset to be one for supervised model training but rather a very high-quality dataset for evaluation (since this is the first dataset of its kind).A key reason is that the collection of appraisal annotations is both challenging and time-consuming: we have 24 dimensions to analyze per post, and the annotation for one post for one trained annotator takes half an hour.Future work may establish the validity of training data obtained from LLMs, and explore approaches such as distillation.</p>
<p>In addition, we experiment with LLMs under a zero-shot setup only, while we highlight that this is the first work towards the assessment of cognitive appraisals of emotions in language models, and it lays the foundation for future research on deciphering the intrinsic emotional dynamics that remain unexplored in current state-of-the-art models.We believe that this warrants a careful construction of the dataset with thorough analysis; and we leave these interesting engineering questions to future work.</p>
<p>B Dataset Annotation Framework</p>
<p>We provide the instructions given to the annotators in Figure 13.In addition, we also provide the layout for the annotation task (which includes the full questions for each of the 24 cognitive emotion appraisal dimensions abbreviated in</p>
<p>C Inter-Annotator Agreement by Dimension in COVIDET-APPRAISALS</p>
<p>To better understand the inter-annotator agreement pertaining to each emotion appraisal dimension in COVIDET-APPRAISALS, we measure Spearman's ρ and Krippendorff's alpha on each of the 21 applicable dimensions.We provide the inter-annotator agreement statistics per dimension in Figure 5.As the plot shows, the human annotators have strong agreement on dimensions such as efc (emotionfocused coping) and pfc (problem-focused coping), whilst disagreeing with each other most often on dimensions grlv (goal relevance), exp (expectedness), and loss.This can be attributed to the nature of our domain: in these Reddit posts, the narrator is mainly sharing their experiences in life around COVID-19, while preserving doubts about the future.</p>
<p>D Additional Dataset Analyses</p>
<p>D.1 Are the Dimensions Informative for Emotions?</p>
<p>The cognitive appraisal theories provide insights into the nature of the appraisal dimensions in distinguishing various emotions (Hofmann et al., 2020;Yeo and Ong, 2023): while different individuals may appraise the same situation distinctively, they are more likely to experience the same emotion when a consistent appraisal pattern emerges.For example, the cognitive dimension pls (pleasantness) is often linked to joy, but unlikely to be associated with disgust (Smith and Ellsworth, 1985).Therefore, specific emotions are hypothesized to stem from corresponding appraisal patterns (Yeo and Ong, 2023).By understanding how individuals appraise the situations they experience, we can subsequently make predictions regarding their emotional state.As a result, appraisal dimensions are valuable in differentiating emotional states, especially in cases where the emotions are highly interchangeable (e.g., disgust and anger).</p>
<p>Here, using the cognitive appraisal dimensions annotated in COVIDET-APPRAISALS, we further explore and validate whether these appraisal dimensions alone are indicative of perceived emotions already annotated in COVIDET.While in the ideal scenario, both the appraisal and the objective event need to be present for emotion prediction, this small experiment will allow us to gauge which dimensions are more likely discriminative for a particular emotion.For each of the 7 emo- tion classes labeled in COVIDET, we train a logistic regression model using the scales of the annotated 21 applicable appraisal dimensions as features.We split COVIDET-APPRAISALS using a random 80:20 train-test partitioning, and aggregate the Likert-scale ratings for the 40 posts that are doubly annotated by our human annotators following the aggregation setup discussed in §3.We downsample the training data for each logistic regression model to handle class imbalance issues.In addition, we encode the "not mentioned" (NA) labels as an independent real-valued feature, and substitute their values with 0. To prevent features of different scales or magnitudes from having a disproportionate influence on the models, we Z-normalize the scale ratings within each dimension for each annotator.</p>
<p>The F1 scores for each emotion using the trained logistic regression models on the test set are reported in Table 6.We observe that the models are most capable at predicting emotions such as fear and sadness, whilst performing poorly on emotions disgust and trust.This is possibly due to the domain of our dataset: in COVIDET, fear and sadness are the most commonly found emotions whereas disgust and trust are scarcely present.On average, the classifiers achieve an average F1 of 0.22 on the test set across all emotions.</p>
<p>To reveal the appraisal dimensions that are indicative of each emotion, we examine the weights from the trained logistic regression models.Specifically, we aim to validate the emotion appraisal dimensions that Yeo and Ong (2023) identified to be predictive of emotions (including anger, fear, joy, sadness, and disgust) from prior studies in psychology.In Table 7, we show the appraisal dimensions found to be either positively predictive (+) or negatively predictive (−) of emotions.Please note that these indications are extracted from a recent meta-analysis from Yeo and Ong (2023) with significance (p &lt; 0.05).In Table 7, we highlight the indicative appraisal dimensions captured by our logistic regression models that are in line with Yeo and Ong (2023)'s findings.We observe a certain degree of overlap between Yeo and Ong (2023)'s identified emotion appraisal dimensions that are predictive of emotions and those captured by our logistic regression models.It should be noted that some appraisal dimensions may not be useful for all emotions included in Table 7, since in COVIDET there are no Reddit posts annotated with neutral emotions: for example, as shown in for all other emotions.However, when compared to neutral emotions (i.e., in texts where no emotions are present), crsp (circumstances-responsibility) may be a negative indicator for disgust.Therefore, experimenting with COVIDET-APPRAISALS may not reveal the extensive range of appraisal dimensions indicative of each emotion.Further investigations are needed to explore the predictability of these appraisal dimensions for emotions compared against neutral emotions.</p>
<p>D.2 Topic Variations in Rationales</p>
<p>We use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to extract topics from the natural language rationales annotated in COVIDET-APPRAISALS.Stop-words such as common English function words and words that occur frequently in our instructions (e.g., narrator, situation) are removed prior to the topic modeling.</p>
<p>The most prominent topic extracted by the LDA model for each dimension is shown in Table 8.We notice clear patterns of topics related to the ap-praisal dimension being assessed.For example, in dimension crsp (circumstances-responsibility) we observe narrators of Reddit posts worrying about and blaming Delta, a COVID-19 variant, for causing the status quo, whereas in dimension fml (familiarity) we note people are generally unfamiliar with the situation, as they are prone to seek advice and probe for information on the forum.</p>
<p>D.3 An Example of Semantic Similarity</p>
<p>As discussed in §4, commonly used automatic measures such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andBERTScore (Zhang et al., 2019) do not adequately capture semantic similarity in COVIDET-APPRAISALS.Taking the post in Figure 1    this situation since they mention being able to taste freedom, believing the pandemic is ending, when suddenly they heard news that vaccinated people are still getting long covid and now they think the pandemic will never end."convey the reasons for why the narrator fails to expect the situation to occur.However, the automatic metrics reveal low agreement between these two rationales, with a BLEU-4 score of 0.018, ROUGE-L of 0.231, and a re-scaled BERTSCORE of 0.237.This finding is in line with work showing the challenges of evaluating generation (Gehrmann et al., 2021;Celikyilmaz et al., 2020); we similarly conclude that automatic evaluation metrics may poorly reflect the correctness of a rationale for a subjective emotion appraisal dimension.</p>
<p>E Prompt Templates</p>
<p>The templates for prompting the LLMs are shown in Figure 17.We use "1-step" prompting to elicit both a rating and a rationale with a single prompt from ChatGPT.For all other language models, we apply "2-step" prompting, which first elicits the rating for the appraisal dimension, then conditioned on the response for the rating we further elicit the rationale for the selection.</p>
<p>F Full LLM Rationale Measures</p>
<p>Rationale Automatic Evaluation.We provide the full statistics of the automatic rationale agreement measured using BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andBERTScore (Zhang et al., 2019) for the all 6 LLMs' responses against the gold annotations in Table 9.</p>
<p>As discussed in §7.1, ChatGPT is the most performant language model in providing natural language rationales, with values from these metrics comparable to those between different rationales from our two annotators.Alpaca-7B also achieves comparable performance in these automatic measures, despite its relatively poor capability in terms of selecting Likert-scale ratings.</p>
<p>In addition, we observe that other language models such as FLAN-T5 and Dolly lag behind considerably compared to ChatGPT and Alpaca-7B.Enchantingly, the automatic metrics suggest that Alpaca-13B is the worst language model among our LLMs under assessment, with a markable degradation from Alpaca-7B.Further investigation reveals that Alpaca-13B tends to respond with "Tell us why.</s>" when prompted to generate the natural language rationale for the Likert-scale rating it selects, which takes up more than 84% of its rationale responses.The debasement of the Alpaca model in spite of the increase in the model's scale raises questions regarding the scaling law in our current task of appraising cognitive emotion dimensions in context.</p>
<p>Rationale Human Evaluation.We provide the box plots of the results from the human evaluation for the most-performant 3 language models (i.e., ChatGPT, Alpaca-7B, and FLAN-T5) in Figure 6.</p>
<p>Furthermore, we also provide the results for the human evaluation regarding all 6 LLMs assessed in this paper.Following the setup in §7.2, we evaluate and analyze LLM-generated rationales when the model made a near-correct prediction of the Likertscale rating for that particular dimension compared against the gold human ratings.Specifically, we sample the intersection of dimensions (post, dimension) tuples where all 6 LLMs' (i.e., ChatGPT, FLAN-T5, Alpaca-7B, Alpaca-13B, Dolly-7B, and Dolly-12B) ratings fall in the range of an abso- Table 11: Experiment results from LLMs.We report the average performance across five independent runs.A more comprehensive report of the automatic metrics BLEU-4, ROUGE-L, and BERTSCORE is provided in Table 9, Appendix §F.lute difference of 1 to one of the annotated scaleratings.This results in 30 rationales annotated by human annotators and 26 natural language rationales from each LLM.We report the inter-evaluator agreement using Krippendorff's Alpha with interval distance in Table 10, which shows substantial agreement (Artstein and Poesio, 2008) across all criteria.
IDFWXDOLW\ MXVWLILHV J R OG F K D W J S W I OD Q W [ [ O D OS D F D % UHOHYDQFH J R OG F K D W J S W I OD Q W [ [ O D OS D F D % XVHIXOQHVV
Results from the human evaluation for all 6 LLMs are reported in Table 11.We observe that apart from ChatGPT and Alpaca-7B, all other LLMs including FLAN-T5, Alpaca-13B, Dolly-7B, and Dolly-12B achieve similarly low performance on providing natural language rationales for cognitive emotion appraisals.We provide the box plots of the results from the human evaluation for all 6 language models in Figure 7.</p>
<p>G Model Responses Analyses</p>
<p>The LLMs' performance in terms of Likert-scale rating selections measured using Spearman correlation and Krippendorff's alpha against the gold annotations are shown in Figure 8.Additionally, the box plots for each LLM's Likert-scale ratings are shown in Figure 9.
IDFWXDOLW\ MXVWLILHV J R OG F K D W J S W I OD Q W [ [ O D OS D F D % D OS D F D % G R OO \ Y % G R OO \ Y % UHOHYDQFH J R OG F K D W J S W I OD Q W [ [ O D OS D F D % D OS D F D % G R OO \ Y % G R OO \ Y % XVHIXOQHVV</p>
<p>H Human Evaluation Framework</p>
<p>We provide the instructions given to the human evaluators of the rationales (described in §7.2) in Figure 18 and Figure 19.Additionally, we showcase the human evaluation task layout in Figure 20.</p>
<p>I Why Does ChatGPT Perform (Slightly)</p>
<p>Better Than Human Annotators in Providing Rationales?</p>
<p>As discussed in §7.2, ChatGPT was scored slightly higher in terms of factuality and usefulness on providing natural language rationales than our human annotators, according to human evaluators.This can be attributed to ChatGPT's wordiness and extractiveness (as shown in Table 4), especially in cognitive emotion appraisal dimensions where the scale rating is low.As an example, we showcase in Table 12 where both ChatGPT and our human annotator give the same rating for a dimension, but ChatGPT scores higher than our human experts on  metrics factuality and usefulness.As shown in the example, given the same Reddit post as well as the instruction to evaluate the cognitive emotion appraisal dimension orcl (othercontrollable), both our human annotator and Chat-GPT give a Likert rating of 1 indicating a small extent to which the narrator believes other people were controlling what was happening in the situation.Similarly, in their natural language rationales, both our annotator and ChatGPT express that there is no mention of other people controlling the situation in the Reddit post.However, in the post, although to a small degree, the narrator is hinting that other people are in control of the situation since their school is deciding on the mask policy for the school year.This lowers the human annotation to a mean factuality rating of 0.25 and a mean usefulness rating of 0.50.On the other hand, ChatGPT iterates and summarizes the information provided in the post, mentioning that the narrator is "seeking advice on where to find masks that are more suitable for their needs".The provision of such information thus leads to a higher score than our annotator, resulting in a mean rating of 0.75 for both factuality and usefulness.</p>
<p>REDDIT POST I want to clarify that I am in no way shape an anti masker, I believe we do need them during the pandemic and any future outbreaks.However, the masks are a nightmare for me personally.They trigger off my migraines, I could hardly concentrate at school because of it.I also have a sensory disability, which makes wearing masks 10 times harder.Everything is brighter, louder, and everything feels more crowded when I wear them.Its horrible.And it looks like we have to wear them for the next school year, and its my final year.I cant risk it.So I want to know if any you know where I can get masks that are migraine/sensory friendly?While still doing the job a normal medical mask does.Thanks.</p>
<p>APPRAISAL DIMENSION ocrl</p>
<p>To what extent did the narrator think that OTHER PEOPLE were controlling what was happening in the situation?
HUMAN ANNOTATIONS → SCALE: 1 → RATIONALE:</p>
<p>Reddit Post</p>
<p>I've recently heard of stories that people who are getting mildly sick after being vaccinated are still coming down with long covid.People on the covid long hauler subreddit are saying that we're going to have a huge problem on our hands because no vaccine protects against long covid....This isn't going to end, isn't it?Right as I think I've tasted freedom, I find out information that makes me want to stay inside forever like a hermit… The narrator mentions long COVID and the inability to currently protect against long COVID.This is a factor that is still outside of anyone's control, including doctors and nurses, and the situation continues to evolve in uncontrollable ways.</p>
<p>Cognitive Emotion Appraisal Dimension</p>
<p>[Responses from LLMs evaluated in the paper]</p>
<p>ChatGPT
➔ [Scale]: 7 ➔ [Rationale]:
The narrator seems to be acknowledging that the situation is beyond anyone's control, as they mention "stories" and information that they have heard.They also express a sense of helplessness and resignation, stating that "This isn't going to end, isn't it?"and feeling like they want to "stay inside forever like a hermit."However, they do not explicitly state that circumstances are completely responsible, as they may still believe that there are some actions that can be taken to mitigate the situation.</p>
<p>FLAN-T5-XXL(11B) Dolly-V2(12B)</p>
<p>➔ [Scale]: 7 ➔ [Rationale]: I think circumstances were completely responsible for causing the situation.The narrator believes that getting vaccinated gives you the same immunity to the long covid as a natural infection would.However, there is research to suggest that this is not the case.The narrator is also assuming that everyone who has gotten vaccinated has gotten the same ineffective vaccination as him, which is not the case.Additionally, the text suggests that the long covid is not going to end, which is also not the case.The long covid is a pandemic that is going to last for many years, not just a few months.The pandemic is going to cause a huge problem on the US population, because there is no vaccine that protects against long covid, and the long covid can cause serious health issues.</p>
<p>[Responses from other LLMs] LLaMA(7B)</p>
<p>🤖LLaMA(13B)</p>
<p>➔ [Scale]: I think that the narrator thinks that the circumstances were not at all responsible for causing the situation.The narrator says that people are getting mildly sick after being vaccinated, and that people are still getting long covid.The narrator also says that people are saying that we are going to have a huge problem on our hands because no vaccine protects against long covid.The narrator also says that this isn't going to end, and that people are saying that we are going to have a huge problem on our hands because no vaccine protects against long covid.The narrator also says that this isn't going to end, isn't it?The narrator also says that right as they think they have tasted freedom, they find out information that makes them want to stay inside forever like a hermit.The narrator also says that this isn't going to end, isn't it?The narrator also says that right as they think they have tasted freedom, they find out information that makes them want to stay inside forever like a hermit.The narrator also says that this isn't going to end, isn't it?The narrator also says that right ➔ [Rationale]: I think that the narrator thinks that the circumstances were not at all responsible for causing the situation.</p>
<p>The narrator says that people are getting mildly sick after being vaccinated, and that people are still getting long covid.</p>
<p>The narrator also says that people are saying that we are going to have a huge problem on our hands because no vaccine protects against long covid.The narrator also says that this isn't going to end, and that people are saying that we are going to have a huge problem on our hands because no vaccine protects against long covid.The narrator also says that this isn't going to end, isn't it?The narrator also says that right as they think they have tasted freedom, they find out information that makes them want to stay inside forever like a hermit.The narrator also says that this isn't going to end, isn't it?The narrator also says that right as they think they have tasted freedom, they find out information that makes them want to stay inside forever like a hermit.The narrator also says that this isn't going to end, isn't it?</p>
<p>The narrator also says that right GPT-3-davinci(vanillamodel)  We ask you to judge 24 aspects about how they feel about and view the situation.(For example, whether they felt the situation they were in was something they could control.)</p>
<p>For each of these aspects, please select a rating using the scales we provide (from 1 to 9).You can also select the [NOT MENTIONED] option in case the aspect is not mentioned in the given post.</p>
<p>In</p>
<p>Question:</p>
<p>To what extent did the narrator think that THEY were responsible for causing the situation?--&gt; Rating: 6 (out of 9) --&gt; Reason : The narrator expresses concern about potentially spreading COVID-19 to their parents, even though they have been vaccinated and their parents have also been vaccinated.They seem to recognize that there is a potential for the virus to mutate and evade immunity, but also seem to feel some level of personal responsibility for this outcome.The text suggests that the narrator feels some level of guilt or responsibility for causing the situation.Given the above text, to what extent did the narrator think that THEY were responsible for causing the situation?Rate on a scale from 1 to 9, with 1 being "Narrator thought they were: Not at all responsible" and 9 being "Narrator thought they were: Completely responsible".If the text doesn't address this question, please mark it as "NA".Additionally, tell us why.The format of the answer should be as follows:</p>
<p><likert>[]</likert><rationale>[]</rationale> 1st-Step: Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.</p>
<h3 Post="Post" Reddit="Reddit">input:</h3>
<h3>Instruction:</h3>
<p>To what extent did the narrator think that THEY were responsible for causing the situation?Rate on a scale from 1 to 9, with 1 being "Narrator thought they were: Not at all responsible" and 9 being "Narrator thought they were: Completely responsible".</p>
<p>If the text doesn't address this question, please mark it as "NA".</p>
<h3>Response: 2nd-Step: Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.</h3>
<h3 Post="Post" Reddit="Reddit">input:</h3>
<h3>Instruction:</h3>
<p>To what extent did the narrator think that THEY were responsible for causing the situation?Rate on a scale from 1 to 9, with 1 being "Narrator thought they were: Not at all responsible" and 9 being "Narrator thought they were: Completely responsible".</p>
<p>If the text doesn't address this question, please mark it as "NA".The selected scale is {scale answer from the 1st-step}.Tell us why.</p>
<h3>Response:</h3>
<p>Instructions and Examples for Evaluating Rationales for Emotion Dimensions [Instructions]</p>
<p>This is an annotation task for evaluating the rationales for selected ratings.During the evaluation, you will first read a Reddit post.Then, you will see a pair of (question, answer) relating to the emotional state of the author of the post.The answer will first give a rating (that is verified to be correct)on a scale of 1 to 9 (including a "Not Mentioned" label is provided in case the aspect is not mentioned in the post), followed by a rationale explaining why the rating is selected.The rationale is the portion we ask you to evaluate.</p>
<p>The evaluation will be conducted based on 4 criteria, namely "factual consistency", "relevance", "justifiability", and "usefulness".The detailed instructions for each question are shown below.</p>
<p>1) Is the rationale factually consistent with the post?</p>
<p>★ Whether the rationale faithfully reflects whatʼs stated in the post.In other words, does the rationale accurately describe what the post is saying, or does it misrepresent or hallucinate the content in some way?○ "Yes": if the rationale is accurate with no errors ○ "Minor Error": if the rationale contains some minor errors or omissions ○ "No": if the rationale contains significant errors, misrepresentations, or significant hallucinations to the question 2) Is the rationale relevant to the aspect question being asked?★ Whether the rationale directly addresses the specific question that is being asked about the post.This means that the rationale should be focused on the specific aspect of the post that is being evaluated, and should not stray off-topic or provide irrelevant information.(Most Relevant) (Least Relevant) 5 4 3 2 1 ○ -○ -○ -○ -○ 3) Does the rationale justify the selected scale?</p>
<p>★ Does the rationale adequately explain why the selected rating scale is the most appropriate or relevant one to use for the aspect being evaluated?Yes No ○ --○ 4) Is the rationale useful (informative)?</p>
<p>★ Whether the rationale provides useful or informative insights or explanations related to the aspect being evaluated.</p>
<p>In other words, does the rationale provide helpful information or insights that can aid in evaluating the aspect question being asked?○ "Yes": if the rationale provides clear and informative explanations or insights for the reasons why the Likert-scale rating is selected ○ "Maybe": if the rationale provides some information that may be useful, but is not entirely clear or may not fully address the reasons why the Likert-scale rating is selected ○ "No": if the rationale does not provide any useful or informative insights or explanations for the reasons why the Likert-scale rating is selected [Examples] Reddit Post:</p>
<p>I'm being vague as to not give away my employer but I'm frustrated and wondering how you're coping with being one of the only fields in CA that will be stuck in the past with no end date in sight.I have a lot of various traumas from this, as I'm sure many do, and being le out as the rest of my (all vaccinated, myself included) family gets to finally unmask at work is definitely not helping my mental state.</p>
<p>Example Rationales to Evaluate:</p>
<p>Dimension 20: To what extent did the narrator think that the situation was FAMILIAR?(For example, if the narrator thought that they had experienced this situation before in the past.)</p>
<p>➔ [Scale]: Situation was (Not at all familiar) (Completely familiar) 1 2 3 4 5 6 7 8 9 Not mentioned ○ -○ -○ -○ -○ -• -○ -○ -○ -○ ➔ [Rationale]: The narrator seems to be familiar with the situation of being frustrated and traumatized due to their personal health conditions, as indicated by a rating of 6.This indicates that the narrator believes that the situation cannot be reversed and that something of value has been permanently lost.</p>
<p>Human Evaluation</p>
<p>Figure 2 :
2
Figure 2: Percentage of "not mentioned" labels in each dimension in COVIDET-APPRAISALS.</p>
<p>Figure 3 :
3
Figure 3: Distribution of the ratings for each dimension.</p>
<p>Figure 4 :
4
Figure 4: Mean Likert-scale ratings for each dimension in each emotion.</p>
<p>A</p>
<p>Dataset Example and LLM ResponsesIn Figure10, Figure11, and Figure12, we showcase an annotation from COVIDET-APPRAISALS together with LLMs' responses regarding dimension 3 crsp (circumstances-responsibility).In addition to LLMs evaluated in this paper (including ChatGPT, FLAN-T5-XXL, Alpaca (7B, 13B), and Dolly-V2 (7B, 12B)), we also present responses elicited from other non-instruction-tuned models such as GPT-3-davinci (a vanilla base model of GPT-3) and LLaMA (7B, 13B) (Touvron et al., 2023) using the "2-step" prompting template given in Figure17.As the example shows, these noninstruction-tuned LLMs perform poorly on our task of cognitive emotion appraisal, generating nonsensical responses for both selecting Likert-scale ratings as well as providing natural language rationales.</p>
<p>Figure 5 :F1
5
Figure 5: Inter-annotator agreement of the Likert-scale ratings within each dimension.The dimensions are ranked by the order of Spearman's ρ, and the colors indicate the inter-annotator agreement measured by Krippendorff's alpha using interval distance.</p>
<p>Figure 6 :
6
Figure 6: Box plots of the results from the human evaluation task for the most-performant 3 LLMs (i.e., Chat-GPT, Alpaca-7B, and FLAN-T5).</p>
<p>Figure 7 :
7
Figure 7: Box plots of the results from the human evaluation task for all 6 LLMs.</p>
<p>Figure 8 :
8
Figure8: LLMs' performance in terms of Spearman correlation and Krippendorff's alpha (using interval distance) against the gold annotations within each group of dimensions (averaged performance across 5 independent runs).</p>
<p>Figure 9 :
9
Figure 9: Box-plots for the LLMs' Likert-scale rating responses, measured across 5 independent runs.</p>
<p>Dimension 3 :
3
To what extent did the narrator think that CIRCUMSTANCES BEYOND ANYONE'S CONTROL were responsible for causing the situation?(For example, if the narrator thought that the situation was caused by luck, or by weather, or by some other things outside anyone's control.)</p>
<p>1</p>
<p>Figure 10: A comprehensive example from COVIDET-APPRAISALS for Dimension 3 (circumstances-responsibility) (Page 1/3).</p>
<p>Figure 12 :
12
Figure 12: A comprehensive example from COVIDET-APPRAISALS for Dimension 3 (circumstances-responsibility) (Page 3/3).</p>
<p>Figure 13 :
13
Figure 13: Instructions to annotators for COVIDET-APPRAISALS.</p>
<p>Figure 14 :
14
Figure 14: Annotation task layout for COVIDET-APPRAISALS (Page 1/3).</p>
<p>Figure 15 :
15
Figure 15: Annotation task layout for COVIDET-APPRAISALS (Page 2/3).</p>
<p>Figure 16 :
16
Figure 16: Annotation task layout for COVIDET-APPRAISALS (Page 3/3).</p>
<p>Figure 17 :
17
Figure 17: Prompt templates (taking dimension 1 as an example).</p>
<p>Figure 18 :
18
Figure 18: Instructions for the human evaluation described in §7.2 (Page 1/2).</p>
<p>Figure 19 :
19
Figure 19: Instructions for the human evaluation described in §7.2 (Page 2/2).</p>
<p>Figure 20 :
20
Figure 20: Task layout for the human evaluation.</p>
<p>Table 1 :
1Percentage of NA values0 20 40 60 80civfaircsnprd NA Values Percentage in Each Dimension fex eff exp fml grlv ocrl efc orsp gcnd chl loss attn thr scrl crsp pls ccrlcrtsrsppfcDimensions
The 24 appraisal dimensions and their abbreviations we used throughout this paper.See Appendix §B for full questions for each dimension, and Figure1for an example of how the items for 1: self-responsibility, 2: other-responsibility, 3: circumstances-responsibility, and 24: expectedness were framed.</p>
<p>Table 4 :
4
Experiment results from LLMs.Additional evaluations of all language models (including Alpaca-13B, Dolly-7B, and Dolly-12B) are provided in Table11.A more comprehensive report of the automatic metrics BLEU-4, ROUGE-L, and BERTSCORE is provided in Table9, Appendix §F.
LENGTHABSTRACTIVENESSAUTO EVALHUMAN EVAL# TOKENS%NOVEL BIGRAMSBLEU-4 ROUGE-L BERTSCFAC REL JUS USEANNOTATORS28.986.7%--0.730.880.950.72CHATGPT58.081.8%0.0440.2240.3470.840.880.930.85FLAN-T545.316.0%0.0080.0660.0530.400.29 0.24 0.13ALPACA-7B48.671.9%0.0400.2300.2970.550.82 0.82 0.51</p>
<p>Table 5 :
5
Inter-annotator agreement statistics for the human evaluation task, measured using Krippendorff's Alpha with interval distance.</p>
<p>Xi Ye and Greg Durrett.2022.The unreliability of explanations in few-shot prompting for textual reasoning.In Advances in Neural Information Processing Systems.
Gerard Yeo and Desmond C. Ong. 2023. A meta-analytic review of the associations between cognitiveappraisals and emotions in cognitive appraisal theory.PsyArXiv.Hongli Zhan, Tiberiu Sosea, Cornelia Caragea, andJunyi Jessy Li. 2022. Why do you feel this way?summarizing triggers of emotions in social mediaposts. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing,pages 9436-9453, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-berger, and Yoav Artzi. 2019. Bertscore: Evaluatingtext generation with BERT. In International Confer-ence on Learning Representations.Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, andDacheng Tao. 2023. Can chatgpt understand too?a comparative study on chatgpt and fine-tuned bert.arXiv preprint arXiv:2302.10198.Caleb Ziems, Minzhi Li, Anthony Zhang, and DiyiYang. 2022. Inducing positive perspectives with textreframing. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3682-3700, Dublin, Ireland.Association for Computational Linguistics.</p>
<p>Table 1
1) in</p>
<p>Table 6 :
6
F1 scores of each emotion using the trained logistic regression model on the test set.</p>
<p>Table 7 :
71srspSelf-responsibility+++2orspOther-responsibility+++3crspCircumstances-responsibility++4pfcProblem-focused coping--+  † †5grlvGoal Relevance+  †+++6attnAttentional activity++++7efcEmotion-focused coping-+-8scrlSelf-Controllable-+-9ocrlOther-Controllable+10ccrlCircumstances-Controllable++11prdPredictability---12thrThreat+  †+-++13plsPleasantness--+--14crtCertainty-+-15gcndGoal Conduciveness-+-+17fexFuture expectancy+19lossLoss++-+20fmlFamiliarity--21effEffort+-+22chlChallenge24expExpectedness+
Cognitive emotion appraisal dimensions that are predictive of emotions (including anger, fear, joy, sadness, and disgust), identified by a recent meta-analysis conducted byYeo and Ong (2023).+ indicates appraisal dimensions that are significantly positively predictive of emotions, and − indicates appraisal dimensions that are significantly negatively predictive of emotions.We highlight in red the indicative appraisal dimensions captured by our logistic regression models that are in line with Yeo and Ong (2023)'s findings.† signifies weights in our logistic regression models with p &lt; 0.1, and † † signifies significant weights with p &lt; 0.05.</p>
<p>Table 7 ,
7
crsp (circumstances-responsibility) is found to be positively indicative for fear and sadness, while neutral
srsporspcrsppfcgrlvattnefcbelieveresponsiblecontrolcopefindsattendcoperesponsiblepeoplebelievesbelieveconcernsbelievesemotionallydoesbelievescircumstancesdoesnhighlyneedsomewhatdoesndoescovidcopingrelevantwantfeelingcausingcovidresponsiblehavingcovidbelievestrugglingfocusedvaccinatedblamevaccineinfectedcovidbelievereactionbelievedeltabelievesstuckadvicecovidbelievessomewhatoutsidecoviddoseaskingbelievessomewhatblamepandemicdifficultendingpandemicdoesnvaccinatedcausingworriedtimepandemictryingcopingscrlocrlccrlprdthrplscrtcontrolpeoplecontrolhappenthreatenedfindsuncertainbelievecontrolcovidbelievecovidunpleasantunsuredoesbelievesbelievespredictfeelsfeelingcertainbelieveswaitcircumstancesdoesndoescovidconsequencesdoesnvaccineoutsidecovidexpresspandemicvaccinecovidcoviddeltadonfeelingworriedcovidfeelsomewhatunderstandsunablehealthpleasantunderstandvaccinedoesunderstandpredictionthreatconfusedsomewhatvaccinatedbelievebelievemakessomewhatfeeldeltapandemicseptemberpandemicinformationsensevaccinefullygcndfexlossfmleffchlexpwantworsesensesubjecteffortfindsoccurfindsbetterdoesinformationdealchallengingdidinconsistentbelieveexpressmeaningmentalcovidexpectcoviddoeslossadvicebelievesvaccinatedmentionshighlybelieveslostaskinglothighlysomewhatwantsgettingbelievesmentionsexertpandemicexpectedvaccinatedcovidcovidunfamiliartryvaccinecoviddondeltapandemicfamiliarbelieveworriedexpectingfeelworriedvaccinatedcovidcoviddeltamentiontryingvariantopportunitysomewhatneedvariantvaccinated</p>
<p>Table 8 :
8
LDA results on the annotated rationales for each appraisal dimension.</p>
<p>Table 9 :
9
The full rationale statistics measured for LLMs' responses against the gold annotations, measured across 5 independent runs.
FACRELJUSUSEEVALUATORS0.721 0.711 0.632 0.672</p>
<p>Table 10 :
10
Inter-annotator agreement statistics for the human evaluation task, measured using Krippendorff's Alpha with interval distance.</p>
<p>The narrator does not mention anyone else as controlling what's happening in the situation.
FACTUALITY: 0.25 (Minor Error + No)USEFULNESS: 0.50 (Maybe + Maybe)CHATGPT'S RESPONSES→ SCALE: 1→ RATIONALE: The text does not suggest that other peopleare controlling the situation. The narrator is simply seekingadvice on where to find masks that are more suitable for theirneeds.FACTUALITY: 0.75 (Yes + Minor Error)USEFULNESS: 0.75 (Yes + Maybe)</p>
<p>Table 12 :
12
An example found in our human evaluation where ChatGPT scores higher than our human annotators on metrics factuality and usefulness.</p>
<p>The narrator believes that circumstances beyond anyone's control were responsible for causing the situation to a moderate extent.They believe that luck, weather, and other external factors played a role in creating the current situation, but that it was not completely due to luck or external factors.
➔ [Scale]: 1➔ [Rationale]: 1🤖Alpaca(7B) ➔ [Scale]: 5➔ [Rationale]: 🤖Alpaca(13B) ➔ [Scale]: 0➔ [Rationale]: Tell us why.🤖Dolly-V2(7B) ➔ [Scale]: 0➔ [Rationale]: I don't know what you're talking about, but I'm going to go outside and play with my new remote-controlledhelicopter.</p>
<p>Scale]: I think the narrator thinks that circumstances beyond anyone's control are responsible for causing the situation. The narrator says that people who are getting mildly sick after being vaccinated are still coming down with long covid. The narrator also says that people on the covid long hauler subreddit are saying that we're going to have a huge problem on our hands because no vaccine protects against long covid. The narrator also says that this isn't going to end, isn't it? Right as I think I've tasted freedom, I find out information that makes me want to stay inside forever like a hermit.\n\n### Instruction: \nTo what extent did the narrator think that PEOPLE'S BEHAVIOR was responsible for causing the situation? (For example, if the narrator thought that the situation was caused by people's bad decisions, or by people's lack of knowledge, or by people's lack of effort, or by people's lack of concern, or by people's lack of cooperation, or by people's lack of self-control, or by people's lack of self-dis</p>
<p>➔ [Rationale]: I think the narrator thinks that people's behavior is responsible for causing the situation.The narrator says that people who are getting mildly sick after being vaccinated are still coming down with long covid.The narrator also says that people on the covid long hauler subreddit are saying that we're going to have a huge problem on our hands because no vaccine protects against long covid.The narrator also says that this isn't going to end, isn't it?Right as I think I've tasted freedom, I find out information that makes me want to stay inside forever like a hermit.\n\n###Instruction: \nTo what extent did the narrator think that PEOPLE'S BEHAVIOR was responsible for causing the situation?(For example, if the narrator thought that the situation was caused by people's bad decisions, or by people's lack of knowledge, or by people's lack of effort, or by people's lack of concern, or by people's lack of cooperation, or by people's lack of self-control, or by people's lack of self-dis.Tell</p>
<p>➔ [</p>
<p>\nThe narrator thinks that CIRCUMSTANCES BEYOND ANYONE'S CONTROL are responsible for causing the situation. The narrator thinks that the situation is caused by luck. The narrator thinks that the situation is caused by weather. The narrator thinks that the situation is caused by some other things outside anyone's control. The narrator doesn't think that the situation is caused by CIRCUMSTANCES BEYOND ANYONE'S CONTROL.\n\nThe narrator thinks that CIRCUMSTANCES BEYOND ANYONE'S CONTROL are responsible for causing the situation. The narrator thinks that the situation is caused by luck. The narrator thinks that the situation is caused by weather. The narrator thinks that the situation is caused by some other things outside anyone's control. The narrator doesn't think that the situation is caused by CIRCUMSTANCES BEYOND ANYONE'S CONTROL.\n\nThe narrator thinks that CIRCUMSTANCES BEYOND ANYONE'S CONTROL are responsible for causing the situation. The narrator thinks that 2</p>
<p>➔ [Scale]: Figure 11: A comprehensive example from COVIDET-APPRAISALS for Dimension 3 (circumstances-responsibility) (Page 2/3).thesituation</p>
<p>is caused by luck. The narrator thinks that the situation is caused by weather. The narrator thinks that the situation is caused by some other things outside anyone's control. The narrator ➔ [Rationale]: \nThe</p>
<p>narrator thinks that CIRCUMSTANCES BEYOND ANYONE'S CONTROL are responsible for causing the situation.The narrator thinks that the situation is caused by luck.The narrator thinks that the situation is caused by weather.The narrator thinks that the situation is caused by some other things outside anyone's control.The narrator doesn't think that the situation is caused by CIRCUMSTANCES BEYOND ANYONE'S CONTROL.\n\nThenarrator thinks that CIRCUMSTANCES BEYOND ANYONE'S CONTROL are responsible for causing the situation.The narrator thinks that the situation is caused by luck.The narrator thinks that the situation is caused by weather.The narrator thinks that the situation is caused by some other things outside anyone's control.The narrator doesn't think that the situation is caused by CIRCUMSTANCES BEYOND ANYONE'S CONTROL.\n\nThenarrator thinks that CIRCUMSTANCES BEYOND ANYONE'S CONTROL are responsible for causing the situation.The narrator thinks that the situation is caused by luck.The narrator thinks that the situation is caused by weather.The narrator thinks that the situation is caused by some other things outside anyone's control.The narrator 3</p>
<p>addition, please tell us why you picked that rating!
Examples are provided below.Please read the following examples:Reddit Post: "Just recently went grocery shopping. I alsoforgot to bring my receipt to the tailor, so I had to go backhome after grocery shopping just to pick it up and deliver itto her to pick my trimmed down pants. And now I feel likeI'm spreading COVID-19 to my parents, despite myselfgetting vaccinated against it. Like, I know the best I'mgoing to experience is mild symptoms anytime I <em>do</em> getit, plus my parents are vaccinated as well, so either they'llexperience mild symptoms or no symptoms at all any timeI spread it to them. But COVID-19 has the potential tomutate and thus evade our immune systems much moreeasily. And with that many unvaccinated people spreadingit among each other for the sake of their otherwisenonexistent "freedoms", it's going to mutate and infect allof us vaccinated people and kill us all. So is there anyonewho will help me with this? Thanks!"
For example, one of Alpaca-7B's scale responses is "The narrator thought that Circumstances Beyond Anyone's Control were responsible for causing the situation to a moderate extent (4 on a scale of 1-9).</s>". After cleaning, the response is formatted to "4".
As shown in Appendix Figure9, the ratings generated by
AcknowledgementsThis research was partially supported by National Science Foundation (NSF) grant IIS-2107524.We thank Kathryn Kazanas and Keziah Kaylyn Reina for their dedication and hard work on the annotation of COVIDET-APPRAISALS.We also thank our reviewers for their insightful feedback and comments.:An instruction-following llama model.https:// github.com/tatsu-lab/stanford_alpaca.
EmoNet: Fine-grained emotion detection with gated recurrent neural networks. Muhammad Abdul, -Mageed , Lyle Ungar, 10.18653/v1/P17-1067Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Emotion and personality. Arnold Magda, 1960Columbia University Press</p>
<p>Role of Emotion in Excessive Use of Twitter During COVID-19 Imposed Lockdown in India. Anshika Arora, Pinaki Chakraborty, M P S Bhatia, Prabhat Mittal, 10.1007/s41347-020-00174-3Journal of Technology in Behavioral Science. 622021</p>
<p>Survey article: Inter-coder agreement for computational linguistics. Ron Artstein, Massimo Poesio, 10.1162/coli.07-034-R2Computational Linguistics. 3442008</p>
<p>Latent dirichlet allocation. Andrew Y David M Blei, Michael I Ng, Jordan, Journal of Machine Learning Research. 32003. Jan</p>
<p>Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, arXiv:2006.14799Evaluation of text generation: A survey. 2020arXiv preprint</p>
<p>Mental health discourse on reddit: Self-disclosure, social support, and anonymity. Munmun De, Choudhury , Sushovan De, ICWSM. 2014</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Cognition in emotion: Always, sometimes, or never. Cognitive neuroscience of emotion. L Gerald, Andrew Clore, Ortony, 2000</p>
<p>Free dolly: Introducing the world's first truly open instructiontuned llm. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin, 2023</p>
<p>GoEmotions: A dataset of fine-grained emotions. Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, Sujith Ravi, 10.18653/v1/2020.acl-main.372Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Detecting perceived emotions in hurricane disasters. Shrey Desai, Cornelia Caragea, Junyi Jessy Li, 10.18653/v1/2020.acl-main.471Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Appraisal processes in emotion. C Phoebe, Klaus R Ellsworth, Scherer, 2003Oxford University Press</p>
<p>Measuring nominal scale agreement among many raters. Jl Fleiss, 10.1037/h0031619Psychological bulletin. 7651971</p>
<p>The gem benchmark: Natural language generation, its evaluation and metrics. Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Raghavi Khyathi, Miruna Chandu, Dipanjan Clinciu, Kaustubh Das, Dhole, Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021). the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)2021</p>
<p>Explaining explanations: An overview of interpretability of machine learning. Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael A Specter, Lalana Kagal, IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA). 2018. 2018</p>
<p>Reddit: A gold mine for personality prediction. Matej Gjurković, Jan Šnajder, 10.18653/v1/W18-1112Proceedings of the Second Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media. the Second Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social MediaNew Orleans, Louisiana, USAAssociation for Computational Linguistics2018</p>
<p>DREAM: Improving situational QA by first elaborating the situation. Yuling Gu, Bhavana Dalvi, Peter Clark, 10.18653/v1/2022.naacl-main.82Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, arXiv:2305.15717The false promise of imitating proprietary llms. 2023arXiv preprint</p>
<p>Appraisal theories for emotion classification in text. Jan Hofmann, Enrica Troiano, Kai Sassenberg, Roman Klinger, 10.18653/v1/2020.coling-main.11Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Finegrained emotion detection in health-related online posts. Hamed Khanpour, Cornelia Caragea, 10.18653/v1/D18-1147Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>ChatGPT: Jack of all trades, master of none. Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Kocoń, Bartłomiej Koptyra, Wiktoria Mieleszczenko-Kowszewicz, Piotr Miłkowski, Marcin Oleksy, Maciej Piasecki, Łukasz Radliński, 10.1016/j.inffus.2023.101861Information Fusion. Konrad Wojtasik, Stanisław Woźniak, and Przemysław Kazienko991018612023</p>
<p>Chatgpt: Jack of all trades, master of none. Information Fusion. Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, 2023101861</p>
<p>Content analysis : an introduction to its methodology. Sage commtext series. Klaus Krippendorff, 1980Sage PublicationsBeverly Hills</p>
<p>Psychological stress and the coping process. Lazarus Richard, 1966McGraw-Hill</p>
<p>From psychological stress to the emotions: A history of changing outlooks. Lazarus Richard, Annual review of psychology. 4411993</p>
<p>Emotions: A cognitive-phenomenological analysis. Allen D Richard S Lazarus, Susan Kanner, Folkman, Theories of emotion. Elsevier1980</p>
<p>How many data points is a prompt worth?. Le Teven, Alexander Scao, Rush, 10.18653/v1/2021.naacl-main.208Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Towards building a social emotion detection system for online news. Jingsheng Lei, Yanghui Rao, Qing Li, Xiaojun Quan, Liu Wenyin, 10.1016/j.future.2013.09.024Future Generation Computer Systems. 372014</p>
<p>Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Zhiyuan Liu, Dan Zhang, arXiv:2302.09582Human emotion knowledge representation emerges in large language models and supports discrete emotion inference. 2023arXiv preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>DENS: A dataset for multi-class emotion analysis. Chen Liu, Muhammad Osama, Anderson , 10.18653/v1/D19-1656Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsDe Andrade. 2019</p>
<p>Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs. Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Le Ronan, Noah A Bras, Yejin Smith, Choi, 10.18653/v1/2020.findings-emnlp.253Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Sentencelevel planning for especially abstractive summarization. Andreas Marfurt, James Henderson, 10.18653/v1/2021.newsum-1.1Proceedings of the Third Workshop on New Frontiers in Summarization. the Third Workshop on New Frontiers in SummarizationDominican RepublicAssociation for Computational Linguistics2021</p>
<p>Lyrics, music, and emotions. Rada Mihalcea, Carlo Strapparava, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningJeju IslandKorea. Association for Computational Linguistics2012</p>
<p>Theories of emotion causation: A review. Agnes Moors, 2010Psychology Press</p>
<p>Appraisal theories of emotion: State of the art and future development. Agnes Moors, Phoebe C Ellsworth, Klaus R Scherer, Nico H Frijda, 10.1177/1754073912468165Emotion Review. 522013</p>
<p>Modeling emotion in complex stories: The stanford emotional narratives dataset. C Desmond, Zhengxuan Ong, Zhi-Xuan Wu, Marianne C Tan, Isabella Reddan, Alison Kahhalé, Jamil Mattek, Zaki, IEEE Transactions on Affective Computing. 122019</p>
<p>Affective cognition: Exploring lay theories of emotion. C Desmond, Jamil Ong, Noah D Zaki, Goodman, 10.1016/j.cognition.2015.06.010Cognition. 1432015</p>
<p>The cognitive structure of emotions. Andrew Ortony, Gerald L Clore, Allan Collins, 2022Cambridge university press</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.03350Measuring and narrowing the compositionality gap in language models. 2022arXiv preprint</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, arXiv:2302.06476Is chatgpt a general-purpose natural language processing task solver? arXiv preprint. 2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa. Randolph Justus, 2005Online submission</p>
<p>Cognitive determinants of emotion: A structural theory. Ira J Roseman, Review of personality &amp; social psychology. 1984</p>
<p>Appraisal determinants of emotions: Constructing a more accurate and comprehensive theory. Ira J Roseman, Cognition &amp; Emotion. 1031996</p>
<p>Appraisal theory. Appraisal processes in emotion: Theory, methods, research. J Ira, Craig A Roseman, Smith, 2001</p>
<p>A systems approach to appraisal mechanisms in emotion. David Sander, Didier Grandjean, Klaus R Scherer, Neural networks. 1842005</p>
<p>What are emotions? and how can they be measured?. Klaus R Scherer, Social Science Information. 442005</p>
<p>The emotion process: Event appraisal and component differentiation. R Klaus, Agnes Scherer, Moors, Annual review of psychology. 702019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5824 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5824</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5824</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-9d81bc8bebf1beb936427c224afb219b54a64f1e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9d81bc8bebf1beb936427c224afb219b54a64f1e" target="_blank">Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Domain Conditional Pointwise Mutual Information is introduced, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task.</p>
                <p><strong>Paper Abstract:</strong> Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition—wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. “computer” and “PC.” Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5824.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5824.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMI_DC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Conditional Pointwise Mutual Information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot scoring function that divides the conditional probability of an answer given a prompt by the answer's probability within a task-specific domain premise, to compensate for surface-form competition among valid lexicalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice across many benchmarks (COPA, StoryCloze, HellaSwag, RACE, ARC, OBQA, CommonsenseQA, BoolQ, RTE, CB, SST-2/5, AG's News, TREC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot (and some few-shot) multiple-choice and classification tasks where the model scores candidate answer strings for correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot templated prompts where each candidate hypothesis y_i is scored by PMI_DC = P(y_i | x, domain) / P(y_i | domain). Domain is represented by a short domain-premise string (e.g. 'because' or '? the answer is:').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against standard LM (argmax P(y|x)), AvG (length-normalized log-probabilities), UnC (P(y|domain) alone), and Contextual Calibration (CC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (GPT-3 175B): COPA accuracy PMI_DC 89.2% (vs LM 85.2%, AvG 82.8%, UnC 56.0%); OBQA accuracy PMI_DC 58.0% (vs AvG 43.8%, LM 33.2%, UnC 10.6%); CommonsenseQA PMI_DC 66.7% (vs LM 61.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Across evaluated models and 16 dataset splits, PMI_DC achieved the best or tied best score on the largest fraction of datasets (see Table 2); it often outperformed LM and AvG across GPT-2 and GPT-3 sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Varies by task; examples include +4.0 percentage points on COPA (PMI_DC 89.2% vs LM 85.2% for GPT-3 175B) and +14.8 points on OBQA (PMI_DC 58.0% vs AvG 43.8% for GPT-3 175B).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>PMI_DC reduces 'surface form competition' by factoring out how likely an answer string is in the domain independent of the premise, so multiple lexical variants of the correct concept no longer steal probability mass; it thus measures mutual information between premise and hypothesis within the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Three datasets where PMI_DC did not consistently win: HellaSwag (AvG/length-normalization dominated), ARC Easy (LM dominated), and BoolQ (UnC dominated for some model sizes) — discussed as failure cases where dataset characteristics favor other biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5824.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Generative LM Scoring (argmax P(y|x))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline zero-shot method that selects the answer option with the highest generative probability conditioned on the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same multiple-choice datasets as above (COPA, StoryCloze, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot selection by comparing P(y_i | x) across candidate answer strings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt + candidate continuation(s); select candidate with highest unconditional or conditional probability according to the left-to-right causal LM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared directly to PMI_DC (divide by domain probability), AvG (length normalization), UnC, and CC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (GPT-3 175B): COPA LM 85.2% (vs PMI_DC 89.2%), OBQA LM 33.2% (vs PMI_DC 58.0%), HellaSwag LM 57.6% (AvG much higher).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LM underperforms when surface-form competition is severe; however, when surface-form competition is removed (see COPA Flipped) LM performs similarly to PMI_DC.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>On COPA Flipped LM matched PMI_DC (both 83.6% for GPT-3 175B) whereas on original COPA PMI_DC had a +4.0 point advantage (89.2% vs 85.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (relative to PMI_DC) in settings with many synonymous surface forms; no effect when surface-form competition is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LM directly reflects the model's preferred surface forms; if multiple lexicalizations of the correct concept exist, their probability mass is split among them, causing LM to underestimate conceptual correctness when the listed option is an infrequent surface form.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5824.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AvG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Length-normalized Log-likelihood (Average per-token log-probability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scoring method that divides the summed log-probability of an answer by its token length (per-token average) to reduce length-related bias in LM scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice datasets (notably HellaSwag, COPA, others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot selection where candidate strings are scored by average log-probability (sum log-probs / length).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same templated prompts as LM but scoring uses length-normalized average log-probability instead of raw probability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to LM, PMI_DC, UnC, and CC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (GPT-3 175B): HellaSwag AvG 77.2% vs LM 57.6% and PMI_DC 53.5% (AvG substantially better on HellaSwag); COPA AvG 82.8% vs PMI_DC 89.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>AvG dominates on HellaSwag but is outperformed by PMI_DC on many other tasks; AvG sometimes approximates correcting for unconditional likelihood via token-length normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large on HellaSwag (≈+19.6 percentage points over LM for GPT-3 175B: 77.2% vs 57.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (for some datasets, e.g., HellaSwag); reduced or mixed on others.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Length normalization may partly compensate for unconditional probability (rare surface forms tend to be longer or less frequent at subword level); BPE token-length can approximate unigram probability so AvG can resemble a crude unconditional correction.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>AvG is less effective than PMI_DC on datasets where surface-form competition driven by domain frequency (not merely length) is the main issue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5824.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unconditional (In-domain) Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that scores each candidate by its probability following a short domain-premise only (P(y | domain)), ignoring the specific question/prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice datasets including BoolQ, others</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot scoring that tests how much dataset label bias or domain priors alone predict answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Domain-only prompt (e.g., using only 'because' or '? the answer is:') and ranking candidates by P(y | domain).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to LM, AvG, PMI_DC, and CC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (GPT-2 125M on BoolQ): UnC accuracy 62.2% vs LM 58.8% and PMI_DC 51.1%, showing UnC best for that model/dataset combination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>UnC can dominate on datasets with strong prior/label biases (e.g., yes/no distributions) or when questions require reasoning beyond the LM's capability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>On GPT-2 125M BoolQ, UnC +3.4 points over LM (62.2% vs 58.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (for datasets with strong domain priors or where questions are too hard for direct conditional modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>UnC succeeds when the domain prior (e.g., how often 'yes' vs 'no' follows a domain prompt) contains more predictive signal than the LM's conditional reasoning for that model size; indicates dataset label bias or insufficient model reasoning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>On many other tasks UnC is far weaker than PMI_DC and LM (e.g., OBQA and CommonsenseQA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5824.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contextual Calibration (CC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual Calibration (learned affine transform from content-free inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A calibration method that learns an affine transformation (w, b) using content-free inputs so that in the absence of evidence the LM yields uniform scores over the label set; applied to improve few-shot/zero-shot predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrate before use: Improving few-shot performance of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (evaluated among other sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Several multiple-choice datasets (reported on a subset in the paper, e.g., RTE, SST-2, CB)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot/few-shot calibration to reduce biases like label frequency and common-token biases via affine correction computed from content-free prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Apply affine transform to LM scores where transform parameters are estimated by feeding content-free or neutral inputs and averaging across them.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared directly to raw LM, AvG, PMI_DC, and UnC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (GPT-3 175B on RTE): CC accuracy 57.8% vs PMI_DC 64.3% (PMI_DC higher). On some tasks CC helps over raw LM but PMI_DC often outperformed CC across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PMI_DC generally outperforms CC in the majority of datasets tested; CC performs well on some tasks where simple calibration corrects dominant biases.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Varies by dataset; e.g., RTE (175B) PMI_DC +6.5 points over CC (64.3% vs 57.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (CC helps over raw LM in some cases, but PMI_DC often improves further).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CC corrects certain global biases (majority label, recency, common-token biases) but does not directly remove surface-form competition tied to domain-conditioned lexical frequencies; PMI_DC directly compensates for common-token/domain frequency bias.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5824.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPA_Flipped</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COPA Flipped (premise/hypothesis inversion for scoring-by-premise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset transformation that reverses COPA's cause/effect pairs so that model scoring evaluates P(premise | hypothesis) (scoring-by-premise), removing surface-form competition between different hypothesis strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COPA (Choice of Plausible Alternatives)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cause/effect multiple-choice: original COPA asks to choose the second clause (cause/effect) given the first; COPA Flipped swaps them so the continuation is fixed and different premises are scored.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Manual transformation flipping premise and hypothesis and substituting inverse relation tokens (e.g., 'because' ↔ 'so') so the LM can score how likely the (now shared) continuation is given each candidate premise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared COPA original (scoring continuations) vs COPA Flipped (scoring premises / scoring-by-premise) and against PMI_DC, LM, AvG.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (GPT-3 175B): On original COPA PMI_DC 89.2% vs LM 85.2%; on COPA Flipped LM/AvG/PMI_DC all produce the same result ~83.6% (LM matches PMI_DC when surface-form competition is removed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Flipping removes the advantage of PMI_DC: methods that previously underperformed (LM, AvG) achieve parity with PMI_DC when surface-form competition is eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>On COPA (GPT-3 175B) PMI_DC had +4.0 points over LM; on COPA Flipped the difference disappears (both ≈83.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>neutralized the PMI_DC advantage; LM improved to parity (improved relative to original COPA).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Scoring the same surface form across options (scoring-by-premise) removes surface-form competition, demonstrating that PMI_DC's gains are primarily due to correcting that competition rather than better semantic understanding per se.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5824.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Variability / Templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt templates and prompt-robustness evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation across multiple human-written prompt templates to assess sensitivity of scoring methods (especially PMI_DC) to prompt wording and structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (sentiment classification) robustness study; broader use of templating across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot classification with 15 different prompts/templates (from Zhao et al., 2021) to measure mean and variance of accuracies across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple short prompt templates for the same task (e.g., different phrasings around sentiment question). For PMI_DC, domain premise is adapted per template; for LM/AvG the same template variants are used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compare mean and std dev across 15 prompts for UnC, LM, PMI_DC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (SST-2, GPT-3 175B across 15 prompts): mean accuracies UnC 49.9%, LM 72.5% (std 15.7), PMI_DC 74.8% (std 14.0) — PMI_DC had the highest mean and similar robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PMI_DC maintained the highest mean accuracy across prompt variants and comparable or slightly lower variance than LM in this test.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>PMI_DC mean > LM mean by ~2.3 points on SST-2 (74.8% vs 72.5%) in the multi-prompt evaluation for GPT-3 175B; prompt choice produces large std deviations (≈14–16 points).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved robustness (PMI_DC had higher mean across prompt variants).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>While prompt wording strongly affects raw LM scores, PMI_DC's domain-conditioned normalization reduces sensitivity to prompt-specific surface-form biases by factoring out domain-level lexical priors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5824.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot Prompting (4-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot priming (in-context learning with example exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluations using a small number (4) of in-context examples to prime the LM (few-shot) and measure whether PMI_DC advantages extend beyond zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–1.6B (reported table sample includes 125M–1.6B rows; GPT-3 results also discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (closed-set) and CommonsenseQA (open-set / CQA) few-shot evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>4-shot experiments using 5 random samples of 4 examples to prime the model; measure mean and standard deviation of accuracy across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot templates where 4 labeled examples are prepended before the test prompt; scoring methods (LM/AvG/PMI_DC) applied to primed context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared few-shot PMI_DC vs few-shot LM/AvG and vs zero-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (SST-2, GPT models summarized in Table 4): For a 1.6B model SST-2 4-shot mean accuracies: Unc 49.9%, LM 85.4%, PMI_DC 89.4% (PMI_DC higher); for CQA (1.6B) Unc 16.0%, LM 46.2%, PMI_DC 47.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PMI_DC advantages generally persist in few-shot setups, often improving over LM; however LM can outperform PMI_DC in some model/dataset pairs (not universally).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example SST-2 1.6B 4-shot: PMI_DC +4.0 points over LM (89.4% vs 85.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (PMI_DC generally improves few-shot performance but results can be mixed for some sizes/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Domain-conditioned normalization remains beneficial when the model is primed with examples because surface-form competition still affects conditional generation; few-shot context does not fully resolve surface-form competition.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>LM was superior for a couple of models on SST-2 in the few-shot trials (noted by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5824.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5824.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Premise / Template Design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of domain-premise strings and templating (e.g., 'because', '? the answer is:')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Design choice to estimate in-domain unconditional probabilities by using short domain-relevant strings as a proxy for P(y | domain) — an essential component of PMI_DC and templated zero-shot scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice tasks (CommonsenseQA, ARC, OBQA, COPA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Templates consist of a premise prompt X, a domain-premise string X_domain (usually the tail keyword of a relation, e.g., 'because' for causal relations), and candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Scoring uses P(y | x) divided by P(y | X_domain); domain-premise selection is manual and task-specific (full template examples are provided in Appendix/Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Contrasted implicitly with using a generic unconditional prompt (like 'A:') as done in prior work; compared with using no domain-premise (LM) and with content-free inputs for CC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using domain-premise enabled PMI_DC improvements across many datasets; e.g., CommonsenseQA domain prompt '? the answer is:' with PMI_DC improved scoring over LM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors note prior work (Brown et al., 2020) used a generic 'A:' unconditional reference; domain-specific premises produced more consistent and stronger corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not reported as single scalar; contributes to the PMI_DC gains cited above (examples in COPA/OBQA/CQA rows).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (domain-premise yields better unconditional estimates than generic content-free prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>GPT models are not trained to give reliable unconditional probabilities for arbitrary strings; domain-premise strings constrain the unconditional distribution to the task's domain and produce a better estimate for the denominator in PMI_DC, stabilizing the correction.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 1)</em></li>
                <li>Choice of plausible alternatives: An evaluation of commonsense causal reasoning <em>(Rating: 1)</em></li>
                <li>HellaSwag: Can a machine really finish your sentence? <em>(Rating: 2)</em></li>
                <li>How can we know what language models know? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5824",
    "paper_id": "paper-9d81bc8bebf1beb936427c224afb219b54a64f1e",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "PMI_DC",
            "name_full": "Domain Conditional Pointwise Mutual Information",
            "brief_description": "A zero-shot scoring function that divides the conditional probability of an answer given a prompt by the answer's probability within a task-specific domain premise, to compensate for surface-form competition among valid lexicalizations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–175B",
            "task_name": "Multiple-choice across many benchmarks (COPA, StoryCloze, HellaSwag, RACE, ARC, OBQA, CommonsenseQA, BoolQ, RTE, CB, SST-2/5, AG's News, TREC)",
            "task_description": "Zero-shot (and some few-shot) multiple-choice and classification tasks where the model scores candidate answer strings for correctness.",
            "problem_format": "Zero-shot templated prompts where each candidate hypothesis y_i is scored by PMI_DC = P(y_i | x, domain) / P(y_i | domain). Domain is represented by a short domain-premise string (e.g. 'because' or '? the answer is:').",
            "comparison_format": "Compared against standard LM (argmax P(y|x)), AvG (length-normalized log-probabilities), UnC (P(y|domain) alone), and Contextual Calibration (CC).",
            "performance": "Example (GPT-3 175B): COPA accuracy PMI_DC 89.2% (vs LM 85.2%, AvG 82.8%, UnC 56.0%); OBQA accuracy PMI_DC 58.0% (vs AvG 43.8%, LM 33.2%, UnC 10.6%); CommonsenseQA PMI_DC 66.7% (vs LM 61.0%).",
            "performance_comparison": "Across evaluated models and 16 dataset splits, PMI_DC achieved the best or tied best score on the largest fraction of datasets (see Table 2); it often outperformed LM and AvG across GPT-2 and GPT-3 sizes.",
            "format_effect_size": "Varies by task; examples include +4.0 percentage points on COPA (PMI_DC 89.2% vs LM 85.2% for GPT-3 175B) and +14.8 points on OBQA (PMI_DC 58.0% vs AvG 43.8% for GPT-3 175B).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "PMI_DC reduces 'surface form competition' by factoring out how likely an answer string is in the domain independent of the premise, so multiple lexical variants of the correct concept no longer steal probability mass; it thus measures mutual information between premise and hypothesis within the domain.",
            "counterexample_or_null_result": "Three datasets where PMI_DC did not consistently win: HellaSwag (AvG/length-normalization dominated), ARC Easy (LM dominated), and BoolQ (UnC dominated for some model sizes) — discussed as failure cases where dataset characteristics favor other biases.",
            "uuid": "e5824.0",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "LM",
            "name_full": "Standard Generative LM Scoring (argmax P(y|x))",
            "brief_description": "Baseline zero-shot method that selects the answer option with the highest generative probability conditioned on the prompt.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–175B",
            "task_name": "Same multiple-choice datasets as above (COPA, StoryCloze, etc.)",
            "task_description": "Zero-shot selection by comparing P(y_i | x) across candidate answer strings.",
            "problem_format": "Prompt + candidate continuation(s); select candidate with highest unconditional or conditional probability according to the left-to-right causal LM.",
            "comparison_format": "Compared directly to PMI_DC (divide by domain probability), AvG (length normalization), UnC, and CC.",
            "performance": "Example (GPT-3 175B): COPA LM 85.2% (vs PMI_DC 89.2%), OBQA LM 33.2% (vs PMI_DC 58.0%), HellaSwag LM 57.6% (AvG much higher).",
            "performance_comparison": "LM underperforms when surface-form competition is severe; however, when surface-form competition is removed (see COPA Flipped) LM performs similarly to PMI_DC.",
            "format_effect_size": "On COPA Flipped LM matched PMI_DC (both 83.6% for GPT-3 175B) whereas on original COPA PMI_DC had a +4.0 point advantage (89.2% vs 85.2%).",
            "format_effect_direction": "reduced (relative to PMI_DC) in settings with many synonymous surface forms; no effect when surface-form competition is removed.",
            "explanation_or_hypothesis": "LM directly reflects the model's preferred surface forms; if multiple lexicalizations of the correct concept exist, their probability mass is split among them, causing LM to underestimate conceptual correctness when the listed option is an infrequent surface form.",
            "counterexample_or_null_result": null,
            "uuid": "e5824.1",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "AvG",
            "name_full": "Length-normalized Log-likelihood (Average per-token log-probability)",
            "brief_description": "A scoring method that divides the summed log-probability of an answer by its token length (per-token average) to reduce length-related bias in LM scoring.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–175B",
            "task_name": "Multiple-choice datasets (notably HellaSwag, COPA, others)",
            "task_description": "Zero-shot selection where candidate strings are scored by average log-probability (sum log-probs / length).",
            "problem_format": "Same templated prompts as LM but scoring uses length-normalized average log-probability instead of raw probability.",
            "comparison_format": "Compared to LM, PMI_DC, UnC, and CC.",
            "performance": "Example (GPT-3 175B): HellaSwag AvG 77.2% vs LM 57.6% and PMI_DC 53.5% (AvG substantially better on HellaSwag); COPA AvG 82.8% vs PMI_DC 89.2%.",
            "performance_comparison": "AvG dominates on HellaSwag but is outperformed by PMI_DC on many other tasks; AvG sometimes approximates correcting for unconditional likelihood via token-length normalization.",
            "format_effect_size": "Large on HellaSwag (≈+19.6 percentage points over LM for GPT-3 175B: 77.2% vs 57.6%).",
            "format_effect_direction": "improved (for some datasets, e.g., HellaSwag); reduced or mixed on others.",
            "explanation_or_hypothesis": "Length normalization may partly compensate for unconditional probability (rare surface forms tend to be longer or less frequent at subword level); BPE token-length can approximate unigram probability so AvG can resemble a crude unconditional correction.",
            "counterexample_or_null_result": "AvG is less effective than PMI_DC on datasets where surface-form competition driven by domain frequency (not merely length) is the main issue.",
            "uuid": "e5824.2",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "UnC",
            "name_full": "Unconditional (In-domain) Scoring",
            "brief_description": "A baseline that scores each candidate by its probability following a short domain-premise only (P(y | domain)), ignoring the specific question/prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–175B",
            "task_name": "Multiple-choice datasets including BoolQ, others",
            "task_description": "Zero-shot scoring that tests how much dataset label bias or domain priors alone predict answers.",
            "problem_format": "Domain-only prompt (e.g., using only 'because' or '? the answer is:') and ranking candidates by P(y | domain).",
            "comparison_format": "Compared to LM, AvG, PMI_DC, and CC.",
            "performance": "Example (GPT-2 125M on BoolQ): UnC accuracy 62.2% vs LM 58.8% and PMI_DC 51.1%, showing UnC best for that model/dataset combination.",
            "performance_comparison": "UnC can dominate on datasets with strong prior/label biases (e.g., yes/no distributions) or when questions require reasoning beyond the LM's capability.",
            "format_effect_size": "On GPT-2 125M BoolQ, UnC +3.4 points over LM (62.2% vs 58.8%).",
            "format_effect_direction": "improved (for datasets with strong domain priors or where questions are too hard for direct conditional modeling).",
            "explanation_or_hypothesis": "UnC succeeds when the domain prior (e.g., how often 'yes' vs 'no' follows a domain prompt) contains more predictive signal than the LM's conditional reasoning for that model size; indicates dataset label bias or insufficient model reasoning capacity.",
            "counterexample_or_null_result": "On many other tasks UnC is far weaker than PMI_DC and LM (e.g., OBQA and CommonsenseQA).",
            "uuid": "e5824.3",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Contextual Calibration (CC)",
            "name_full": "Contextual Calibration (learned affine transform from content-free inputs)",
            "brief_description": "A calibration method that learns an affine transformation (w, b) using content-free inputs so that in the absence of evidence the LM yields uniform scores over the label set; applied to improve few-shot/zero-shot predictions.",
            "citation_title": "Calibrate before use: Improving few-shot performance of language models",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B (evaluated among other sizes)",
            "task_name": "Several multiple-choice datasets (reported on a subset in the paper, e.g., RTE, SST-2, CB)",
            "task_description": "Zero-shot/few-shot calibration to reduce biases like label frequency and common-token biases via affine correction computed from content-free prompts.",
            "problem_format": "Apply affine transform to LM scores where transform parameters are estimated by feeding content-free or neutral inputs and averaging across them.",
            "comparison_format": "Compared directly to raw LM, AvG, PMI_DC, and UnC.",
            "performance": "Example (GPT-3 175B on RTE): CC accuracy 57.8% vs PMI_DC 64.3% (PMI_DC higher). On some tasks CC helps over raw LM but PMI_DC often outperformed CC across datasets.",
            "performance_comparison": "PMI_DC generally outperforms CC in the majority of datasets tested; CC performs well on some tasks where simple calibration corrects dominant biases.",
            "format_effect_size": "Varies by dataset; e.g., RTE (175B) PMI_DC +6.5 points over CC (64.3% vs 57.8%).",
            "format_effect_direction": "mixed (CC helps over raw LM in some cases, but PMI_DC often improves further).",
            "explanation_or_hypothesis": "CC corrects certain global biases (majority label, recency, common-token biases) but does not directly remove surface-form competition tied to domain-conditioned lexical frequencies; PMI_DC directly compensates for common-token/domain frequency bias.",
            "counterexample_or_null_result": null,
            "uuid": "e5824.4",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "COPA_Flipped",
            "name_full": "COPA Flipped (premise/hypothesis inversion for scoring-by-premise)",
            "brief_description": "A dataset transformation that reverses COPA's cause/effect pairs so that model scoring evaluates P(premise | hypothesis) (scoring-by-premise), removing surface-form competition between different hypothesis strings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–175B",
            "task_name": "COPA (Choice of Plausible Alternatives)",
            "task_description": "Cause/effect multiple-choice: original COPA asks to choose the second clause (cause/effect) given the first; COPA Flipped swaps them so the continuation is fixed and different premises are scored.",
            "problem_format": "Manual transformation flipping premise and hypothesis and substituting inverse relation tokens (e.g., 'because' ↔ 'so') so the LM can score how likely the (now shared) continuation is given each candidate premise.",
            "comparison_format": "Compared COPA original (scoring continuations) vs COPA Flipped (scoring premises / scoring-by-premise) and against PMI_DC, LM, AvG.",
            "performance": "Example (GPT-3 175B): On original COPA PMI_DC 89.2% vs LM 85.2%; on COPA Flipped LM/AvG/PMI_DC all produce the same result ~83.6% (LM matches PMI_DC when surface-form competition is removed).",
            "performance_comparison": "Flipping removes the advantage of PMI_DC: methods that previously underperformed (LM, AvG) achieve parity with PMI_DC when surface-form competition is eliminated.",
            "format_effect_size": "On COPA (GPT-3 175B) PMI_DC had +4.0 points over LM; on COPA Flipped the difference disappears (both ≈83.6%).",
            "format_effect_direction": "neutralized the PMI_DC advantage; LM improved to parity (improved relative to original COPA).",
            "explanation_or_hypothesis": "Scoring the same surface form across options (scoring-by-premise) removes surface-form competition, demonstrating that PMI_DC's gains are primarily due to correcting that competition rather than better semantic understanding per se.",
            "counterexample_or_null_result": null,
            "uuid": "e5824.5",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Prompt Variability / Templates",
            "name_full": "Prompt templates and prompt-robustness evaluation",
            "brief_description": "Evaluation across multiple human-written prompt templates to assess sensitivity of scoring methods (especially PMI_DC) to prompt wording and structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3, GPT-2",
            "model_size": "125M–175B",
            "task_name": "SST-2 (sentiment classification) robustness study; broader use of templating across datasets",
            "task_description": "Zero-shot classification with 15 different prompts/templates (from Zhao et al., 2021) to measure mean and variance of accuracies across prompt variants.",
            "problem_format": "Multiple short prompt templates for the same task (e.g., different phrasings around sentiment question). For PMI_DC, domain premise is adapted per template; for LM/AvG the same template variants are used.",
            "comparison_format": "Compare mean and std dev across 15 prompts for UnC, LM, PMI_DC.",
            "performance": "Example (SST-2, GPT-3 175B across 15 prompts): mean accuracies UnC 49.9%, LM 72.5% (std 15.7), PMI_DC 74.8% (std 14.0) — PMI_DC had the highest mean and similar robustness.",
            "performance_comparison": "PMI_DC maintained the highest mean accuracy across prompt variants and comparable or slightly lower variance than LM in this test.",
            "format_effect_size": "PMI_DC mean &gt; LM mean by ~2.3 points on SST-2 (74.8% vs 72.5%) in the multi-prompt evaluation for GPT-3 175B; prompt choice produces large std deviations (≈14–16 points).",
            "format_effect_direction": "improved robustness (PMI_DC had higher mean across prompt variants).",
            "explanation_or_hypothesis": "While prompt wording strongly affects raw LM scores, PMI_DC's domain-conditioned normalization reduces sensitivity to prompt-specific surface-form biases by factoring out domain-level lexical priors.",
            "counterexample_or_null_result": null,
            "uuid": "e5824.6",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Few-shot Prompting (4-shot)",
            "name_full": "Few-shot priming (in-context learning with example exemplars)",
            "brief_description": "Evaluations using a small number (4) of in-context examples to prime the LM (few-shot) and measure whether PMI_DC advantages extend beyond zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–1.6B (reported table sample includes 125M–1.6B rows; GPT-3 results also discussed)",
            "task_name": "SST-2 (closed-set) and CommonsenseQA (open-set / CQA) few-shot evaluations",
            "task_description": "4-shot experiments using 5 random samples of 4 examples to prime the model; measure mean and standard deviation of accuracy across samples.",
            "problem_format": "Few-shot templates where 4 labeled examples are prepended before the test prompt; scoring methods (LM/AvG/PMI_DC) applied to primed context.",
            "comparison_format": "Compared few-shot PMI_DC vs few-shot LM/AvG and vs zero-shot baselines.",
            "performance": "Example (SST-2, GPT models summarized in Table 4): For a 1.6B model SST-2 4-shot mean accuracies: Unc 49.9%, LM 85.4%, PMI_DC 89.4% (PMI_DC higher); for CQA (1.6B) Unc 16.0%, LM 46.2%, PMI_DC 47.7%.",
            "performance_comparison": "PMI_DC advantages generally persist in few-shot setups, often improving over LM; however LM can outperform PMI_DC in some model/dataset pairs (not universally).",
            "format_effect_size": "Example SST-2 1.6B 4-shot: PMI_DC +4.0 points over LM (89.4% vs 85.4%).",
            "format_effect_direction": "improved (PMI_DC generally improves few-shot performance but results can be mixed for some sizes/datasets).",
            "explanation_or_hypothesis": "Domain-conditioned normalization remains beneficial when the model is primed with examples because surface-form competition still affects conditional generation; few-shot context does not fully resolve surface-form competition.",
            "counterexample_or_null_result": "LM was superior for a couple of models on SST-2 in the few-shot trials (noted by authors).",
            "uuid": "e5824.7",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Domain Premise / Template Design",
            "name_full": "Use of domain-premise strings and templating (e.g., 'because', '? the answer is:')",
            "brief_description": "Design choice to estimate in-domain unconditional probabilities by using short domain-relevant strings as a proxy for P(y | domain) — an essential component of PMI_DC and templated zero-shot scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3",
            "model_size": "125M–175B",
            "task_name": "Multiple-choice tasks (CommonsenseQA, ARC, OBQA, COPA, etc.)",
            "task_description": "Templates consist of a premise prompt X, a domain-premise string X_domain (usually the tail keyword of a relation, e.g., 'because' for causal relations), and candidate hypotheses.",
            "problem_format": "Scoring uses P(y | x) divided by P(y | X_domain); domain-premise selection is manual and task-specific (full template examples are provided in Appendix/Table 7).",
            "comparison_format": "Contrasted implicitly with using a generic unconditional prompt (like 'A:') as done in prior work; compared with using no domain-premise (LM) and with content-free inputs for CC.",
            "performance": "Using domain-premise enabled PMI_DC improvements across many datasets; e.g., CommonsenseQA domain prompt '? the answer is:' with PMI_DC improved scoring over LM.",
            "performance_comparison": "Authors note prior work (Brown et al., 2020) used a generic 'A:' unconditional reference; domain-specific premises produced more consistent and stronger corrections.",
            "format_effect_size": "Not reported as single scalar; contributes to the PMI_DC gains cited above (examples in COPA/OBQA/CQA rows).",
            "format_effect_direction": "improved (domain-premise yields better unconditional estimates than generic content-free prompts).",
            "explanation_or_hypothesis": "GPT models are not trained to give reliable unconditional probabilities for arbitrary strings; domain-premise strings constrain the unconditional distribution to the task's domain and produce a better estimate for the denominator in PMI_DC, stabilizing the correction.",
            "counterexample_or_null_result": null,
            "uuid": "e5824.8",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 1
        },
        {
            "paper_title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "rating": 1
        },
        {
            "paper_title": "HellaSwag: Can a machine really finish your sentence?",
            "rating": 2
        },
        {
            "paper_title": "How can we know what language models know?",
            "rating": 1
        }
    ],
    "cost": 0.020421,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Surface Form Competition: Why the Highest Probability Answer Isn't Always Right</h1>
<p>Ari Holtzman ${ }^{1 <em>}$ Peter West ${ }^{1,2 </em>}$ Vered Shwartz ${ }^{1,2}$ Yejin Choi ${ }^{1,2}$ Luke Zettlemoyer ${ }^{1}$<br>${ }^{1}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{2}$ Allen Institute for Artificial Intelligence<br>{ahai, pawest}@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Large language models have shown promising results in zero-shot settings (Brown et al., 2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form compe-tition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. "computer" and "PC." Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT2 and GPT-3 models on a variety of multiple choice datasets. *</p>
<h2>1 Introduction</h2>
<p>Despite the impressive results large pretrained language models have achieved in zero-shot settings (Brown et al., 2020; Radford et al., 2019), we argue that current work underestimates the zeroshot capabilities of these models on classification tasks. This is in large part due to surface form competition-a property of generative models that causes probability to be rationed between different valid strings, even ones that differ trivially, e.g., by capitalization alone. Such competition can be largely removed by scoring choices according to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>A human wants to submerge himself in water, what should he use?</p>
<p>Humans select options
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Language Models assign probability to every possible string
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: While humans select from given options, language models implicitly assign probability to every possible string. This creates surface form competition between different strings that represent the same concept. Example from CommonsenseQA (Talmor et al., 2019).</p>
<p>Domain Conditional Pointwise Mutual Information $\left(\mathrm{PMI}_{\mathrm{DC}}\right)$, which reweighs scores by how much more likely a hypothesis (answer) becomes given a premise (question) within the specific task domain.</p>
<p>Specifically, consider the example question (shown in Figure 1): "A human wants to submerge himself in water, what should he use?" with multiple choice options "Coffee cup", "Whirlpool bath", "Cup", and "Puddle." From the given options, "Whirlpool bath" is the only one that makes sense. Yet, other answers are valid and easier for a language model to generate, e.g., "Bathtub" and "A bathtub." Since all surface forms compete for finite 7038</p>
<p>probability mass, allocating significant probability mass to "Bathtub" decreases the amount of probability mass assigned to "Whirlpool bath." While the total probability of generating some correct answer may be high (i.e., across all valid surface forms), only one of these is a listed option. This is particularly problematic here, because "Whirlpool bath" will be much lower probability than "Bathtub," due to its rarity. More generally, methods that do not account for surface form competition will favor answers with fewer lexical paraphrases.
$\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ factors out the probability of a specific surface form, by instead computing how much more probable a hypothesis is when conditioned on a premise. We use a domain premise string to estimate the unconditional probability of a hypothesis in a given domain. On CommonsenseQA, for example, we compute the probability of each answer option immediately following the string "? the answer is:", and then divide the conditional probability by this estimate to calculate $\mathrm{PMI}</em>$. This scaling factor reweighs answer scores according to the surface form competition that is inherent to the domain or task, e.g. completions of the domain premise that are just inherently unlikely will be upweighted more. This allows us to directly measure how much an answer tells us about the question and vice versa (mutual information is symmetric, see §3). Valid hypotheses no longer need to compete with each other: both "Whirlpool bath" and "Bathtub" will be considered reasonable answers to the question, and so both will attain a high score.}</p>
<p>Extensive experiments show that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ consistently outperforms raw, normalized, and calibrated probability scoring methods on zero-shot multiple choice for more than a dozen datasets and it does so for every model in the GPT-2 and GPT-3 families (§4); this holds true across different possible prompts and in preliminary few-shot experiments as well. To better explain these gains, we use the distinct structure of the COPA dataset (Roemmele et al., 2011) to remove surface form competition entirely, showing that all methods perform well in this idealized setting (§5). Additionally, we analyze the only three datasets where $\mathrm{PMI}</em>$ does worse than other methods and put forward a hypothesis for why normalizing log probabilities works better than raw probabilities (§6). We conclude with a discussion of how generative models should be used for selection tasks (§7).}</p>
<h2>2 Background and Related Work</h2>
<p>Zero-shot vs. Few-Shot Zero-shot inference has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing "TL;DR" (internet slang for Too Long; Didn't Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b).</p>
<p>Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021).</p>
<p>Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer becomes problematic due to different valid surface forms competing for probability. Indeed, recent work in question answering has demonstrated the importance of considering all multiple choice options together (Khashabi et al., 2020), rather than independently assigning each answer a score and simply choosing the highest. This is a difficult strategy to adapt to left-to-right generative language models, which implicitly choose between all possible strings. Using unsupervised language models pretrained on relatively expansive corpora exacerbates surface form competition because such language models generate a much wider distribution than a given question answering dataset contains.
"What is the most populous nation in North America?" Posed with this question, a language model such as GPT-3 can generate a correct response such as "USA", "United States", or "United</p>
<p>States of America" with high probability. While correct strings like this all contribute to the probability of a correct generation, they may have vastly different probabilities: a common string "United States" will be much more likely than rarer forms like "U.S. of A.". In generative scenarios, as long as most of the probability mass goes to valid strings the generation is likely to be valid. This is not the case for multiple choice problems. Given two options, e.g., "USA" and "Canada", GPT-3 will choose the correct answer by probability. However, if we substitute out "USA" for "U.S. of A.", GPT-3 will assign higher probability to "Canada", a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as "I don't know" are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020).</p>
<p>PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling $\mathrm{PMI}_{\mathrm{DC}}$ for zero-shot question answering, though they only use the string "A:" as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see $\S 3$ for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AvG in this work). In contrast, we report a comprehensive comparison on GPT-3 and GPT-2, as well as shedding light on the underlying issue of surface form competition in $\S 5$.</p>
<p>Contextual Calibration Recently, Zhao et al. (2021) describe a new method for calibrating the probabilities of an LM using a learned affine transformation. Though geared towards few-shot learning, the authors devise a clever means of using "content free inputs" for zero-shot learning. Zhao et al. (2021) calibrate for three forms of bias: (1) majority label bias, (2) recency bias, and (3) common token bias. $\mathrm{PMI}_{\mathrm{DC}}$ directly compensates for common token bias by dividing by the domain conditional probability of each answer, and performs superior to contextual calibration (CC) in the majority of cases.</p>
<p>Prompt Sensitivity Recent work highlights LM sensitivity to inputs, and proposes to consider paraphrases of the prompt to overcome this (Davison et al., 2019; Jiang et al., 2020b), as well as noting that certain trigger tokens (Shin et al., 2020) can strongly effect the output of such models. In this work, we focus on the surface form of possible outputs, but do also analyze robustness to different prompts in $\S 4.4$.</p>
<p>Interpreting Language Models Language models tend to model selectional preferences and thematic fit (Pantel et al., 2007; Erk et al., 2010) rather than semantic plausibility (Wang et al., 2018). Probability, possibility and plausibility are distinct (Van der Helm, 2006), but reporting bias (Gordon and Van Durme, 2013) means that language models only model what people are likely to write (on websites that are easily crawled). $\mathrm{PMI}_{\mathrm{DC}}$ aims to adjust for these challenges to better measure the underlying agreement between language models and human judgements, but of course is still subject to the limits and biases of the language model used.</p>
<h2>3 Zero-shot Scoring Strategies</h2>
<p>This paper does not define any new modeling or finetuning methods. Rather, we propose the broad use of $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ scoring for any given model and prompt. $\mathrm{PMI}</em>$ compensates for the fact that different correct answers compete for probability, even though only one will be listed as the correct multiple choice option.}</p>
<p>We begin by describing the two most common methods currently in use.</p>
<h3>3.1 Standard Methods</h3>
<p>Our first baseline is simply selecting the highestprobability option, e.g., baselines in Zhao et al. (2021) and Jiang et al. (2020b), which we refer to as LM. Given a prompt $\mathbf{x}$ (e.g. "The bar closed") and a set of possible answers $\mathbf{y}<em n="n">{1}, \cdots, \mathbf{y}</em>$ (e.g. "it was crowded.", "it was 3 AM."), LM is defined:</p>
<p>$$
\underset{i}{\arg \max } P\left(\mathbf{y}_{i} \mid \mathbf{x}\right)
$$</p>
<p>However, using length normalized log-likelihoods (Brown et al., 2020) has become standard due to its superior performance, and is also commonly used in generation (Mao et al., 2019; Oluwatobi and Mueller, 2020). For causal language models, e.g.,</p>
<p>Template</p>
<p>Premise ( $\mathbf{X}$ ):
The bar closed because
Domain Premise ( $\mathbf{X}<em 1="1">{\text {domain }}$ ):
because
Hypothesis $1\left(\mathbf{y}</em>\right)$ :
it was crowded.
Hypothesis $2\left(\mathbf{y}_{2}\right)$ :
it was 3am.</p>
<h2>Scoring Functions</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: An example from COPA (Roemmele et al., 2011) with the template we use as well as the scoring functions we test. LM returns the highest probability option, while AvG length-normalizes log-likelihoods and chooses the highest option. $\mathrm{PMI}<em i="i">{\mathrm{DC}}$ is a measurement of the mutual information between hypothesis and premise, intuitively how much $\mathbf{x}$ explains $\mathbf{y}</em>$ are averaged over solutions that cause "content free inputs" to yield uniform scores over a given label set, see Zhao et al. (2021).}$ and vice versa. CC is an affine transform of LM, where $\mathbf{w}$ and $\mathbf{b</p>
<p>GPT-2 and GPT-3, Equation 1 can be decomposed:</p>
<p>$$
P\left(\mathbf{y}<em j="1">{i} \mid \mathbf{x}\right)=\prod</em>\right)
$$}^{\ell_{i}} P\left(y_{i}^{j} \mid \mathbf{x}, y_{i}^{1}, \cdots, y_{i}^{j-1</p>
<p>where $y_{i}^{j}$ is the $j$ th token of $\mathbf{y}<em i="i">{i}$ and $\ell</em>$. The AvG strategy can thus be defined as:}$ is the number of tokens in $\mathbf{y}_{i</p>
<p>$$
\arg \max <em j="1">{i} \frac{\sum</em>
$$}^{\ell_{i}} \log P\left(y_{i}^{j} \mid \mathbf{x}, \mathbf{y}^{1 \cdots j-1}\right)}{\ell_{i}</p>
<h3>3.2 Domain Conditional PMI</h3>
<p>Our core claim is that direct probability is not an adequate zero-shot scoring function due to surface form competition. A natural solution is to factor out the probability of specific surface forms, which is what Pointwise Mutual Information (PMI) does:</p>
<p>$$
\operatorname{PMI}(\mathbf{x}, \mathbf{y})=\log \frac{P(\mathbf{y} \mid \mathbf{x})}{P(\mathbf{y})}=\log \frac{P(\mathbf{x} \mid \mathbf{y})}{P(\mathbf{x})}
$$</p>
<p>In effect, this is how much more likely the hypothesis ("it was 3 AM.") becomes given the premise ("The bar closed because"), see Figure 2 for the full example. In a multiple-choice setting-where the premise $\mathbf{x}$ does not change across hypotheses-this is proportional to $P(\mathbf{x} \mid \mathbf{y})$, i.e. the probability of the premise given the hypothesis. We call this scoring-by-premise and it is the reverse of $\mathrm{LM}, P(\mathbf{y} \mid \mathbf{x})$. We use scoring-by-premise to show the presence of surface form competition in $\S 5$.</p>
<p>While Equation 2 estimates how related premise $\mathbf{x}$ is to hypothesis $\mathbf{y}$ in general, we found that estimates of $P(\mathbf{y})$ vary wildly. GPT-2 and GPT-3 are not trained to produce unconditional estimates of document excerpts, an issue which is exacerbated by the fact that many possible answers are extremely rare in a large scrape of public web pages. This causes the unconditional probability of such answers to be poorly calibrated for the purposes of a given task.</p>
<p>We are specifically trying to measure $P(\mathbf{y})$ in a given domain, e.g., for the "because" relation in our running example, shown in Figures $2 \&amp; 3$. To quantify this, we propose Domain Conditional PMI:</p>
<p>$$
\begin{aligned}
\mathrm{PMI}<em _domain="{domain" _text="\text">{\mathrm{DC}}(\mathbf{x}, \mathbf{y}, \text { domain }) &amp; =\frac{P(\mathbf{y} \mid \mathbf{x}, \text { domain })}{P(\mathbf{y} \mid \text { domain })} \
&amp; =\frac{P(\mathbf{y} \mid \mathbf{x}, \text { domain })}{P\left(\mathbf{y} \mid \mathbf{x}</em>
\end{aligned}
$$}}\right)</p>
<p>or how much $\mathbf{x}$ tells us about $\mathbf{y}$ within a domain.
Typically, $P(\mathbf{y} \mid \mathbf{x}$, domain $)=P(\mathbf{y} \mid \mathbf{x})$ because the premise $\mathbf{x}$ typically implies the domain, e.g., "The bar closed because" sets the model up to predict an independent clause that is the cause of some event, without further representation of the domain. In order to estimate $P(\mathbf{y} \mid$ domain $)$-the probability of seeing hypothesis $\mathbf{y}$ in a given domain-we use a short domain-relevant string $\mathbf{x}<em _domain="{domain" _text="\text">{\text {domain }}$, which we call a "domain premise", usually just the ending of the conditional premise $\mathbf{x}$. For example, to predict a causal relation like in Figure 2 we use $\mathbf{x}</em> \mid$ because $)$-how}}=$ "because" and thus divide by $P(\mathbf{y</p>
<p>likely $y$ is to be a "cause" . For examples of each template see Appendix B.</p>
<h3>3.3 Non-standard Baselines</h3>
<p>Unconditional We also compare to the unconditional (in-domain) estimate as a scoring function:</p>
<p>$$
\underset{i}{\arg \max } P\left(\mathbf{y}<em _domain="{domain" _text="\text">{i} \mid \mathbf{x}</em>\right)
$$}</p>
<p>We refer to this as UnC. It ignores the premise completely, only using a domain premise $\mathbf{x}_{\text {domain }}$ (e.g., using $P(\mathbf{y} \mid$ because $)$ as the score). Yet, it is sometimes competitive, for instance on BoolQ (Clark et al., 2019). UnC is a sanity check on whether zero-shot inference is actually using the information in the question to good effect.</p>
<p>Contextual Calibration Finally, we compare to the reported zero-shot numbers of Zhao et al. (2021). Contextual Calibration adjusts LM with an affine transform to make a closed set of answers equally likely in the absence of evidence. Contextual Calibration thus requires computing matrices $\mathbf{w}$ and $\mathbf{b}$ for a number of "content free inputs" and then averaging these weights, see Zhao et al. (2021) for details. In contrast, $\mathrm{PMI}_{\mathrm{DC}}$ requires nothing but a human-written template (as all zero-shot methods do, including Contextual Calibration), can be computed as the difference of two log probabilities, and is naturally applicable to datasets where the set of valid answers varies between questions.</p>
<h2>4 Multiple Choice Experiments</h2>
<h3>4.1 Setup</h3>
<p>We use GPT-2 via the HuggingFace Transformers library (Wolf et al., 2020) and GPT-3 via OpenAI's beta API. ${ }^{\dagger}$ We do not finetune any models, nor do we alter their output. See Appendix B for examples from each dataset in our templated format.</p>
<h3>4.2 Datasets</h3>
<p>We report results on 16 splits of 13 datasets, and briefly describe each dataset here.</p>
<p>Continuation These datasets require the model to select a continuation to previous text, making them a natural way to test language models. Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) asks for cause and effect relationships, as shown in Figure 2. StoryCloze (SC) (Mostafazadeh et al., 2017) gives the model a choice between two</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>alternative endings to 5 sentence stories. Finally, HellaSwag (HS) (Zellers et al., 2019) uses GPT-2 to generate, BERT to filter, and crowd workers to verify possible continuations to a passage. Following previous work (Brown et al., 2020) we report development set numbers for COPA and HS.</p>
<p>Question Answering RACE-M \&amp; -H (R-M \&amp; R-H) (Lai et al., 2017) are both drawn from English exams given in China, the former being given to Middle Schoolers and the latter to High Schoolers. Similarly, ARC Easy \&amp; Challenge (ARC-E \&amp; ARC-C) (Clark et al., 2018) are standardized tests described as "natural, grade-school science questions," with the "Easy" split found to be solvable by either a retrieval or word co-occurrence system, and the rest of the questions put in the "Challenge" split. Open Book Question Answering (OBQA) (Mihaylov et al., 2018) is similar to both of these, but was derived using (and intended to be tested with) a knowledge source (or "book") available; we do not make use of the given knowledge source, following Brown et al. (2020). Finally, CommonsenseQA (CQA) (Talmor et al., 2019) leverages CONCEPTNET (Speer et al., 2017) to encourage crowd workers to write questions with challenging distractors. We report development set numbers on CQA because their test set is not public.</p>
<p>Open Set vs. Closed Set Datasets The above datasets are all "open set" in that multiple choice answers may be any string. Below we describe "closed set" datasets with a fixed set of answers.</p>
<p>Boolean Question Answering BoolQ (BQ) (Clark et al., 2019) poses yes/no (i.e. Boolean) questions based on a multi-sentence passage.</p>
<p>Entailment Entailment datasets focus on the question of whether a hypothesis sentence B is entailed by a premise sentence A. Recognizing Textual Entailment (RTE) (Dagan et al., 2005) requires predicting an "entailment" or "contradiction" label while Commitment Bank (CB) (De Marneffe et al., 2019) adds a "neutral" label. Following previous work (Brown et al., 2020) we report development set numbers for both RTE and CB.</p>
<p>Text Classification We consider three more complex classification datasets: SST-2 \&amp; -5 (Socher et al., 2013) for various granularities of sentiment classification, AG's News (Zhang et al., 2015) (AGN) for topic classification, and TREC (Li and Roth, 2002) for question classification.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2.7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">6.7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">13B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">175B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HS</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R-M</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R-H</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ARC-E</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BQ</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">53.76</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGN</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of scoring algorithms when using GPT-3 for zero-shot inference on multiple choice questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Percent of Ties or Wins by Method</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: right;">Unc</td>
<td style="text-align: right;">LM</td>
<td style="text-align: right;">Avg</td>
<td style="text-align: right;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: right;">CC</td>
</tr>
<tr>
<td style="text-align: left;">125 M</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{6 8 . 7 5}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">350 M</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{6 8 . 7 5}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">760 M</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{7 5 . 0 0}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">1.6 B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{8 0 . 0 0}$</td>
<td style="text-align: right;">20.00</td>
</tr>
<tr>
<td style="text-align: left;">2.7B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">$\mathbf{8 6 . 6 6}$</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">6.7B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">25.00</td>
<td style="text-align: right;">25.00</td>
<td style="text-align: right;">$\mathbf{7 5 . 0 0}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">$\mathbf{6 8 . 7 5}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">175B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">$\mathbf{6 2 . 5 0}$</td>
<td style="text-align: right;">6.25</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentage of datasets that a given method produced the best score or was tied with other methods, aggregated over each model size. The first four rows use GPT-2 (full data available in the Appendix), while the final four rows use GPT-3 and summarize data from Table 1. Since ties are included, rows sometimes sum to more than 100. CC is only measured on the 5 datasets we use where Zhao et al. (2021) also report accuracies.</p>
<h3>4.3 Results</h3>
<p>We report zero-shot results for GPT-3 in Table 1, with GPT-2 results available in Appendix A. A summarized view is shown in Table 2, which aggregates the percentage of splits where a given method achieves the best score or ties for first-place. In this summarized view it is clear that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ consistently outperforms other scoring methods when assessed over a variety of datasets. The smallest margin (in number of datasets won or tied) between $\mathrm{PMI}</em>$ and the best competing method is on GPT-3}</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Prompt Robustness on SST-2</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: right;">Unc</td>
<td style="text-align: right;">LM</td>
<td style="text-align: right;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">125 M</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$56.8_{7.3}$</td>
<td style="text-align: right;">$58.8_{7.6}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">350 M</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$58.0_{11.3}$</td>
<td style="text-align: right;">$60.3_{11.4}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">760 M</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$57.0_{9.2}$</td>
<td style="text-align: right;">$67.7_{13.4}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">1.6 B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$57.3_{8.2}$</td>
<td style="text-align: right;">$69.8_{13.3}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">2.7B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$56.1_{9.0}$</td>
<td style="text-align: right;">$66.2_{15.7}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">6.7B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$59.5_{10.7}$</td>
<td style="text-align: right;">$67.9_{13.6}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$63.0_{14.9}$</td>
<td style="text-align: right;">$71.7_{16.1}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">175B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$72.5_{15.7}$</td>
<td style="text-align: right;">$74.8_{14.0}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 3: The mean and standard deviations over the 15 templates considered for SST-2 in (Zhao et al., 2021). AvG is excluded, as it is equivalent to LM since all the given templates use single-token answers.</p>
<p>175B with AvG, but that margin is over 40 percentage points. This does not imply that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ is always better or that it will be better by a large margin, though it often is. It does suggest that $\mathrm{PMI}</em>$ is a significantly better bet on a new dataset.}</p>
<h3>4.4 Robustness</h3>
<p>To verify that these trends hold across different prompts, we report the mean and standard deviation over the fifteen different prompts considered in (Zhao et al., 2021) for SST-2. Table 3 shows, $\mathrm{PMI}_{\mathrm{DC}}$ always maintains the highest mean, often by a hefty margin. Scores are lower than in Table 1 because many of the prompts used are optimized for few-shot rather than zero-shot scoring.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg $\quad$ PMI $_{\text {DC }}$</td>
</tr>
<tr>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$63.6_{7.4}$</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">$15.5_{0}$</td>
<td style="text-align: center;">$29.9_{1.6}$</td>
<td style="text-align: center;">$32.7_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">350 M</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$76.3_{13.8}$</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">$16.5_{0}$</td>
<td style="text-align: center;">$37.6_{2.1}$</td>
<td style="text-align: center;">$40.4_{2.3}$</td>
</tr>
<tr>
<td style="text-align: center;">760 M</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$85.9_{7.2}$</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">$16.1_{0}$</td>
<td style="text-align: center;">$41.5_{2.6}$</td>
<td style="text-align: center;">$42.4_{2.5}$</td>
</tr>
<tr>
<td style="text-align: center;">1.6B</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$85.4_{1.7}$</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">$16.0_{0}$</td>
<td style="text-align: center;">$46.2_{1.5}$</td>
<td style="text-align: center;">$47.7_{1.9}$</td>
</tr>
</tbody>
</table>
<p>Table 4: The mean and standard deviation for 5 randomly sampled sets of 4 examples used for few-shot inference. We include a closed answer dataset (SST-2) and an open answer dataset (CQA). For SST-2 AVG is equivalent to LM due to using single-token answers.</p>
<h3>4.5 Few-shot</h3>
<p>While our focus in this paper is on zero-shot scoring, $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ is just as applicable to few-shot scenarios. In Table 4 we report 4 -shot results on one closed set dataset (SST-2) and one open set dataset (CQA). We show the mean of 5 randomly sampled sets of 4 examples that are used to prime the model for the task, along with standard deviations. The overall trend on both datasets clearly favors $\mathrm{PMI}</em>$, though LM is superior for two models on SST-2.}</p>
<h2>5 Removing Surface Form Competition</h2>
<p>What if we used the probability of the premise given the hypothesis, $P\left(\mathbf{x} \mid \mathbf{y}<em i="i">{i}\right)$, instead? While we are still measuring the probability of a surface form (e.g. "the bar closed."), it is the same surface form across different options ("It was crowded so", "It was 3 AM so"), eliminating the surface form competition. $\mathbf{y}</em>$ to be likely. We call this scoring-by-premise.}$ and $\mathbf{y}_{i}^{\prime}$ can now both attain high scores if they are both correct answers, by causing $\mathbf{x</p>
<p>Causal language models like GPT-3 cannot measure this directly, because they are only capable of conditioning on past tokens to predict future tokens. We exploit the structure of the COPA dataset to create "COPA Flipped" via a simple transformation, shown in Figure 3. COPA consists of cause and effect pairs (CAUSE so EFFECT, and EFFECT because CAUSE). In the original dataset, whatever comes second (either CAUSE or EFFECT) has two options that a model must choose between. These can be reversed by switching CAUSE and EFFECT, then substituting the natural inverse relation ("because" $\rightarrow$ "so" and "so" $\rightarrow$ "because" ).</p>
<p>Removing Surface Form Competition</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COPA Flipped</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
</tr>
<tr>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.2</td>
</tr>
<tr>
<td style="text-align: center;">350 M</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;">760 M</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;">1.6B</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">76.8</td>
</tr>
<tr>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">83.6</td>
</tr>
</tbody>
</table>
<p>Table 5: LM does better on COPA Flipped than COPA because surface form competition is removed when scoring-by-premise, see $\S 5$. Methods that don't directly adjust for competing surface forms (LM and AVG) have the same score as $\mathrm{PMI}_{\mathrm{DC}}$ on COPA Flipped.</p>
<h3>5.1 Results</h3>
<p>Table 5 shows scores on COPA and COPA Flipped side-by-side. On COPA Flipped everything except UnC produces the exact same result. This is because flipping the hypothesis and premise means that it's the context that changes and not the continuation. LM, AVG, and $\mathrm{PMI}_{\mathrm{DC}}$ only differ from each other over different continuations, not over different contexts for the same continuation.</p>
<p>On COPA Flipped all methods generally perform similarly to $\mathrm{PMI}_{\mathrm{DC}}$ on the unflipped version. This is because surface form competition has been eradicated: we are measuring how well different prefixes condition a model to predict a fixed continuation rather than which continuation is highest probability. Unlike LM, where different answers compete for probability, in COPA Flipped it only matters how likely each answer can make the question. This is not subject to surface form competition because there is only one string being so scored, so it is not competing with any other strings for probability mass.</p>
<p>Not all datasets are so easily flippable, so manually flipping individual questions to remove surface form competition is not a generally applicable strategy. Luckily, $\mathrm{PMI}_{\mathrm{DC}}$ is symmetric:</p>
<p>$$
\begin{aligned}
&amp; \arg \max <em i="i">{i} \frac{P\left(\mathbf{y}</em>} \mid \mathbf{x}, \text { domain }\right)}{P\left(\mathbf{y<em i="i">{i} \mid \text { domain }\right)} \
= &amp; \arg \max </em>} \frac{P\left(\mathbf{x} \mid \mathbf{y<em i="i">{i}, \text { domain }\right)}{P(\mathbf{x} \mid \text { domain })} \
= &amp; \arg \max </em>\right)
\end{aligned}
$$} P\left(\mathbf{x} \mid \mathbf{y}_{i}, \text { domain </p>
<p>In theory, the answer selected by $\mathrm{PMI}_{\mathrm{DC}}$ should be the same between COPA and COPA Flipped</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: In $\S 5$ we experiment with with flipping the premise and hypothesis so that the highest probability premise is chosen as the answer, i.e. scoring-by-premise. The transformation above the dashed line shows the experimental setup used in $\S 5.1$, while the extra distractor below the dashed line is used for illustrative purposes in $\S 5.2$.
as PMI is symmetric, though we expect some differences due to "so" and "because" not being perfect inverses and shuffled references. Thus, $\mathrm{PMI}_{\mathrm{DC}}$ does better on COPA than COPA Flipped, likely due to more natural phrasing in the original dataset.</p>
<p>These results suggest that surface form competition is the primary cause of the depressed performance of LM and AVG in comparison to $\mathrm{PMI}_{\mathrm{DC}}$.</p>
<h3>5.2 In-depth Example</h3>
<p>Scoring-by-Premise Improves LM Figure 3 shows an example of transforming one question from COPA to COPA Flipped. In the example depicted, when we use GPT-3 to calculate $P$, we get:</p>
<p>$$
P\left(\mathbf{y}<em 2="2">{1} \mid \mathbf{x}\right)&gt;P\left(\mathbf{y}</em>\right)
$$} \mid \mathbf{x</p>
<p>which is wrong, since bars usually close at fixed, late-night closing times, rather than because of being overcrowded. However we also find that</p>
<p>$$
\begin{aligned}
P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}<em 1="1">{2}\right) &amp; &gt;P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}</em>\right) \
\frac{P\left(\mathbf{y}<em 2="2">{2} \mid \mathbf{x}\right)}{P\left(\mathbf{y}</em>} \mid \mathbf{x<em 1="1">{\text {domain }}\right)} &amp; &gt;\frac{P\left(\mathbf{y}</em>} \mid \mathbf{x}\right)}{P\left(\mathbf{y<em _domain="{domain" _text="\text">{1} \mid \mathbf{x}</em>
\end{aligned}
$$}}\right)</p>
<p>indicating that scoring-by-premise causes the right answer to be selected and that $\mathrm{PMI}_{\mathrm{DC}}$ successfully simulates scoring by premise in this example.</p>
<p>Stability Over Valid Answers To see how scoring-by-premise allows multiple correct options to achieve high scores, consider the slightly perturbed $\mathbf{y}<em 2="2">{2}^{\prime}$ and $\hat{\mathbf{x}}</em>$ in Figure 3. The inequalities shown above still hold when substituting
$\mathbf{y}}^{\prime<em 2="2">{2} \rightarrow \mathbf{y}</em>}^{\prime}$ and $\hat{\mathbf{x}<em 2="2">{2} \rightarrow \hat{\mathbf{x}}</em>$ :}^{\prime</p>
<p>$$
\begin{aligned}
P\left(\mathbf{y}<em 2="2">{1} \mid \mathbf{x}\right) &amp; &gt;P\left(\mathbf{y}</em>\right) \
P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}}^{\prime} \mid \mathbf{x<em 1="1">{2}^{\prime}\right) &amp; &gt;P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}</em>\right) \
\frac{P\left(\mathbf{y}<em 2="2">{2}^{\prime} \mid \mathbf{x}\right)}{P\left(\mathbf{y}</em>}^{\prime} \mid \mathbf{x<em 1="1">{\text {domain }}\right)} &amp; &gt;\frac{P\left(\mathbf{y}</em>} \mid \mathbf{x}\right)}{P\left(\mathbf{y<em _domain="{domain" _text="\text">{1} \mid \mathbf{x}</em>
\end{aligned}
$$}}\right)</p>
<p>with the key difference that the conditional probability of $\mathbf{y}_{2}^{\prime}$ is much lower:</p>
<p>$$
\begin{aligned}
&amp; \log P\left(\mathbf{y}<em 2="2">{2} \mid \mathbf{x}\right) \approx-16 \
&amp; \log P\left(\mathbf{y}</em>\right) \approx-20
\end{aligned}
$$}^{\prime} \mid \mathbf{x</p>
<p>This is undesirable, as both $\mathbf{y}<em 2="2">{2}$ and $\mathbf{y}</em>}^{\prime}$ are correct answers with similar meanings. Yet, when scoring-by-premise the conditional probability of $\hat{\mathbf{y}}$ is stable when substituting $\hat{\mathbf{x}<em 2="2">{2} \rightarrow \hat{\mathbf{x}}</em>$ :}^{\prime</p>
<p>$$
\begin{aligned}
&amp; \log P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}<em 2="2">{2}\right) \approx-12 \
&amp; \log P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}</em>\right) \approx-12
\end{aligned}
$$}^{\prime</p>
<p>This suggests that eliminating surface form competition allows different correct answers to score well, as they are no longer competing for probability mass. Specifically, "it was 3 AM" and "it was 3:30AM" score wildly differently in COPA but nearly identically in COPA Flipped.</p>
<h2>6 Analysis</h2>
<p>Failure Cases There are three datasets where $\mathrm{PMI}_{\mathrm{DC}}$ does not consistently outperform other methods: HellaSwag, ARC Easy, and BoolQ. Surprisingly, each is dominated by a different method.</p>
<p>HellaSwag is most amenable to Avg. On examination we find that HellaSwag is more focused on the internal coherence of the hypotheses, rather than external coherence, i.e. how much a premise and hypothesis match. This is likely due to HellaSwag being generated by GPT-2 (Radford et al., 2019) and filtered with BERT, as it contains relatively on-topic but intrinsically strange hypotheses that humans can distinguish from natural data.</p>
<p>ARC Easy yields the highest scores to LM, i.e., selecting the highest probability option. Clark et al. (2018) note that ARC Easy questions can be solved by a retrieval or word co-occurrence baseline, while examples that were answered incorrectly by both were put into the Challenge split. This suggests a bias towards a priori likely phrases. Manual inspection reveals many stock answers, e.g., "[clouds are generated when] ocean water evaporates and then condenses in the air," supporting our hypothesis.</p>
<p>Finally, BoolQ, a reading comprehension dataset in which all answers are either "yes" or "no", is best solved by an unconditional baseline. This is because the dataset presents truly complex questions that require more reasoning than GPT-2 or 3 are capable of out of the box. Indeed, none of the methods reported do better than the majority baseline, except $\mathrm{PMI}_{\mathrm{DC}}$ with the largest GPT-3 model.</p>
<p>Why does length normalization work? Past work offers little explanation for why AvG should be a successful strategy, other than the intuition that estimates are strongly length biased and require compensation. Length bias may be caused by the final softmax layer of current language models assigning too much probability mass to irrelevant options at each time-step, as noted in open-ended generation, character-level language modeling, and machine translation (Holtzman et al., 2020; AlRfou et al., 2019; Peters et al., 2019).</p>
<p>Another (not mutually exclusive) argument is that length normalization may account for unconditional probability in a similar way to $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$. Length normalization is often measured over Byte Pair Encoding (BPE) tokens (Sennrich et al., 2016) and BPE tends to produce vocabularies where most tokens are equally frequent (Wang et al., 2020). Recent evidence suggests that language is approximately uniformly information dense (Levy, 2018; Levy and Jaeger, 2007; Jaeger, 2006). As such, length in BPE tokens may correspond roughly to a unigram estimate of log-probability, supposing that BPE tokens have approximately uniform uni-
gram frequency. The adjustment made by AvG is still somewhat different than $\mathrm{PMI}</em>$, (division of log terms rather than subtraction) but could have a similar effect, if length and probability correlate.}</p>
<h2>7 Discussion</h2>
<p>Language Models are density estimation functions that assign probability to every possible string, but there are often many strings that could represent a given idea equally well. Our key observation is that a generative model assigning probability to a string that represents a certain option isn't equivalent to selecting the concept an option corresponds to. We expect surface form competition anywhere that generative models are used where more than one string could represent the same concept.
$\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ aligns the predictions being made by the model more closely with the actual task posed by multiple choice questions: "choose the hypothesis that explains the premise" rather than "generate the exact surface form of the hypothesis". From this perspective, $\mathrm{PMI}</em>$ does not go far enough, because the model still cannot consider the given set of options altogether when selecting its choice. This matters when answers interact with each other, e.g., "all of the above".}</p>
<h2>8 Conclusion</h2>
<p>We conduct a large-scale comparison of standard and recent scoring functions for zero-shot inference across all GPT-2 and GPT-3 models. We show that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ consistently outperforms previous scoring functions on a wide variety of multiple choice datasets. We also argue that compensating for surface form competition is the cause of this boost, by demonstrating that other methods work just as well as $\mathrm{PMI}</em>$ when surface form competition is eliminated. In future work we would like to explore how surface form competition affects generation, as we hypothesize that it may be the cause of overly generic outputs under high model uncertainty.}</p>
<h2>Acknowledgments</h2>
<p>This work was supported in part by the ARO (AROW911NF-16-1-0121), the NSF (IIS1562364), DARPA under the MCS program through NIWC Pacific (N66001-19-2-4031) and the Allen Institute for AI (AI2). We thank Mitchell Wortsman, Gabriel Ilharco, Tim Dettmers, and Rik Koncel-Kedziorski for thorough and insightful feedback on preliminary drafts.</p>
<h2>References</h2>
<p>Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3159-3166.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPICNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107-124.</p>
<p>Katrin Erk, Sebastian Padó, and Ulrike Padó. 2010. A flexible, corpus-driven model of regular and inverse selectional preferences. Computational Linguistics, 36(4):723-763.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Jonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25-30.</p>
<p>Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2013.</p>
<p>Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In Proceedings of the IEEE international conference on computer vision, pages 27122719.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. Warp: Word-level adversarial reprogramming. arXiv preprint arXiv:2101.00121.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. International Conference on Learning Representations.</p>
<p>Tim Florian Jaeger. 2006. Redundancy and syntactic reduction in spontaneous speech. Ph.D. thesis, Stanford University Stanford, CA.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2020a. How can we know when language models know? arXiv preprint arXiv:2012.00955.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020b. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785794.</p>
<p>Roger Levy. 2018. Communicative efficiency, uniform information density, and the rational speech act theory. In $\operatorname{CogSci}$.</p>
<p>Roger Levy and T Florian Jaeger. 2007. Speakers optimize information density through syntactic reduction. Advances in neural information processing systems, 19:849.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110-119.</p>
<p>Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</p>
<p>Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian McAuley, and Garrison Cottrell. 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5990-5995.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391.</p>
<p>Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46-51.</p>
<p>Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. 2016. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3349-3358.</p>
<p>Olabiyi Oluwatobi and Erik Mueller. 2020. DLGNet: A transformer-based model for dialogue response generation. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 54-62, Online. Association for Computational Linguistics.</p>
<p>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Hu man Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 564-571, Rochester, New York. Association for Computational Linguistics.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.</p>
<p>Ben Peters, Vlad Niculae, and André FT Martins. 2019. Sparse sequence-to-sequence models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504-1519.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. arXiv preprint arXiv:2102.07350.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, pages 90-95.</p>
<p>Bernardino Romera-Paredes and Philip Torr. 2015. An embarrassingly simple approach to zero-shot learning. In International conference on machine learning, pages 2152-2161. PMLR.</p>
<p>Timo Schick and Hinrich Schütze. 2020a. Exploiting cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676.</p>
<p>Timo Schick and Hinrich Schütze. 2020b. Fewshot text generation with pattern-exploiting training. arXiv preprint arXiv:2012.11926.</p>
<p>Timo Schick and Hinrich Schütze. 2020c. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725 .</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158.</p>
<p>Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, and Zhiting Hu. 2019. Targetguided open-domain conversation. In Proceedings</p>
<p>of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5624-5634.</p>
<p>Ruud Van der Helm. 2006. Towards a clarification of probability, possibility and plausibility: how semantics could help futures practice to improve. Foresight.</p>
<p>Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020. Neural machine translation with byte-level subwords. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 91549160 .</p>
<p>Su Wang, Greg Durrett, and Katrin Erk. 2018. Modeling semantic plausibility by injecting world knowledge. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 303-308, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan Zhao, and Rui Yan. 2017. Towards implicit contentintroducing for generative short-text conversation systems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 2190-2199.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in Neural Information Processing Systems, 28:649-657.</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.</p>
<p>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2109.01652.</p>
<p>Kun Zhou, Kai Zhang, Yu Wu, Shujie Liu, and Jingsong Yu. 2019. Unsupervised context rewriting for open domain conversation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1834-1844.</p>
<h1>A GPT-2 Results</h1>
<p>Table 6 shows the results for zero-shot multiple choice using GPT-2.</p>
<h2>B Templates</h2>
<p>Table 7 shows an example of each template used for each dataset.</p>
<p>Multiple Choice Accuracy on GPT-2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Params.</th>
<th style="text-align: center;">125M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">350M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">760M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">1.6B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.676</td>
</tr>
<tr>
<td style="text-align: center;">HS</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.384</td>
</tr>
<tr>
<td style="text-align: center;">R-M</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.415</td>
</tr>
<tr>
<td style="text-align: center;">R-H</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.330</td>
</tr>
<tr>
<td style="text-align: center;">ARC-E</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.562</td>
</tr>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.252</td>
</tr>
<tr>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.224</td>
</tr>
<tr>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.386</td>
</tr>
<tr>
<td style="text-align: center;">BQ</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.563</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.477</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.840</td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.304</td>
</tr>
<tr>
<td style="text-align: center;">AGN</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.648</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.228</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of scoring algorithms when using GPT-2 for zero-shot inference on multiple choice questions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Continuation</td>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">The man broke his tree [p [because]] [he got a hole in his neck. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">StoryClose</td>
<td style="text-align: center;">[I tipped the bottle] [p [m]] [the liquid in the bottle from. $]. \mathrm{UH}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jennifer has a big exam tomorrow. She got so stressed, she pulled an all-nighter. She went into class the next day, weary as can be. Her teacher stated that the test is postponed for next week.]p [The story continues:] [p [Jennifer felt bittersweet about it. $]. \mathrm{UH}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HellaSweg</td>
<td style="text-align: center;">[A female chef in white uniform shows a stack of baking pans in a large kitchen presenting them. the pans] [ [contain egg yolks and baking soda. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">RACE</td>
<td style="text-align: center;">There is not enough oil in the world now. As time goes by, it becomes less and less, so what are we going to do when it runs out $[\ldots]]$. question: [According to the passage, which of the following statements is true] $[\mathrm{P}]$ ? $]$ ] answer: [There is more petroleum than we can use now. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">What carries oxygen throughout the body? [p [the answer is:] [p [red blood cells. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">Which of these would let the most heat travel through? [p [the answer is:] [p [a steel spoon in a cafeteria. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">Where can I stand on a river to see water falling without getting wet? [p [the answer is:] [p [bridge. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">Boolean QA</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">title: [The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016 [...] [p question: [Have the San Jose Sharks won a Stanley Cup? [p [answer:] [p [No. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">Time Warner is the world's largest media and Internet company. [p question: [Time Warner is the world's largest company.] $]$ ] true or false? answer: $]_{\text {DP }}$ [true. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">question: Given that [What fun to hear Artemis laugh. She's such a serious child.] Is [I didn't know she had a sense of humor. ] $]$ true, false, or neither? [the answer is:] [p [true. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">Text <br> Classification</td>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">"[Illuminating if overly salky documentary] $]$ " [The quote] has a tone that is] [p [positive. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">"[Illuminating if overly salky documentary] $]$ " [The quote] has a tone that is] [p [neutral. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AG's News</td>
<td style="text-align: center;">title: [Economic growth in Japan slows down as the country experiences a drop in domestic and corporate [...] [p summary: Expansion slows in Japan] [ [topic:] [p [Sports. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">Who developed the vaccination against polio? [p [The answer to this question will be] [p [a person. $]. \mathrm{UH}$</td>
</tr>
</tbody>
</table>
<p>Table 7: The templates used for each task, along with an example instance (with a single random candidate answer). Original questions (premises) are colored blue, and original answers (hypotheses) are colored red. Long premises are abbreviated with "[...]". The full premises, conditional hypotheses and domain premises are marked in $[\cdot]<em _mathrm_UH="\mathrm{UH">{\mathrm{P}}$, $\left[{ }^{\mathrm{P}}\right]</em>$ respectively. For a complete description of our templating methodology, please see our code at https://github.com/peterwestuw/surface-form-competition}}$, and $\left[{ }^{\mathrm{R}}\right]_{\mathrm{DP}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ https://beta.openai.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
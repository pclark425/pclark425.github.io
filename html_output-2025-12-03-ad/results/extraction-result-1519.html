<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1519 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1519</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1519</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-d3b41091add237d06297154f89e9e443c25ba939</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d3b41091add237d06297154f89e9e443c25ba939" target="_blank">Conglomerate Multi-fidelity Gaussian Process Modeling, with Application to Heavy-Ion Collisions</a></p>
                <p><strong>Paper Venue:</strong> SIAM/ASA J. Uncertain. Quantification</p>
                <p><strong>Paper TL;DR:</strong> A new CONglomerate multi-FIdelity Gaussian process (CONFIG) model is proposed, which embeds this conglomerate structure within a novel non-stationary covariance function and can capture prior knowledge on the numerical convergence of conglomerate simulators, which allows for cost-efficient emulation of multi-fidelity systems.</p>
                <p><strong>Paper Abstract:</strong> In an era where scientific experimentation is often costly, multi-fidelity emulation provides a powerful tool for predictive scientific computing. While there has been notable work on multi-fidelity modeling, existing models do not incorporate an important"conglomerate"property of multi-fidelity simulators, where the accuracies of different simulator components are controlled by different fidelity parameters. Such conglomerate simulators are widely encountered in complex nuclear physics and astrophysics applications. We thus propose a new CONglomerate multi-FIdelity Gaussian process (CONFIG) model, which embeds this conglomerate structure within a novel non-stationary covariance function. We show that the proposed CONFIG model can capture prior knowledge on the numerical convergence of conglomerate simulators, which allows for cost-efficient emulation of multi-fidelity systems. We demonstrate the improved predictive performance of CONFIG over state-of-the-art models in a suite of numerical experiments and two applications, the first for emulation of cantilever beam deflection and the second for emulating the evolution of the quark-gluon plasma, which was theorized to have filled the Universe shortly after the Big Bang.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1519.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1519.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FEA cantilever beam simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finite Element Analysis cantilever beam simulator (3D mesh)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-fidelity finite-element simulator for 3D cantilever beam deflection where three mesh-fidelity parameters (one per spatial dimension) control simulation accuracy; used in the paper as an application to evaluate multi-fidelity emulation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>FEA cantilever beam simulator</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Finite element analysis simulator that computes deflection of a cantilever beam under load; fidelity is controlled via mesh element sizes (three fidelity parameters corresponding to each spatial dimension in the 3D FEA).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / structural finite element analysis</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Multi-fidelity via mesh discretization: high-fidelity corresponds to very fine mesh (limiting solution as mesh sizes -> 0), low-fidelity corresponds to coarse mesh; fidelity is continuous via mesh-size parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity parameters are mesh element sizes for each spatial dimension; smaller t values mean finer mesh and higher computational cost; fidelity affects numerical discretization error of FEA solution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CONFIG Gaussian Process emulator (and baseline multi-fidelity GP models: KOH, TWY)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gaussian process emulator that decomposes the output as phi(x) (limiting exact solution) + delta(x,t) (discrepancy), using a non-stationary covariance in fidelity parameters (Kernel 1 recommended for this scenario); compared against Kennedy–O'Hagan and Tuo–Wu–Yu style models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn an emulator that predicts the limiting highest-fidelity cantilever beam deflection (the t->0 exact solution) from multi-fidelity FEA simulation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Limiting highest-fidelity simulator (predict t -> 0 / exact FEA solution); effectively transfer from lower-fidelity runs to predictions of highest-fidelity behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>CONFIG (with Kernel 1) outperforms existing multi-fidelity approaches when multiple mesh-fidelity parameters interact (Scenario 1). Kernel 1 is recommended for this FEA setting because it models smooth interactions between multiple mesh parameters; naively aggregating multiple fidelities into a single artificial ranking or ignoring conglomerate structure yields poorer predictive results.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper emphasizes that only when all fidelity parameters approach their highest setting (t_r -> 0 for all r) should the discrepancy vanish; it does not specify a numeric minimal fidelity threshold, but argues that neglecting stage-wise/multi-parameter fidelity leads to poor emulation. No explicit minimal fidelity (e.g., a required mesh size) is given.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported conceptual failure modes include: (1) assigning an arbitrary single fidelity ranking (aggregating multiple mesh parameters) or (2) ignoring the conglomerate multi-fidelity structure — both lead to significantly poorer predictive performance from emulators. No specific numeric failure thresholds are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conglomerate Multi-fidelity Gaussian Process Modeling, with Application to Heavy-Ion Collisions', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1519.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1519.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Three-stage heavy-ion collision simulator (QGP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consolidated multi-stage heavy-ion collision simulator for Quark-Gluon Plasma evolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conglomerate, multi-stage simulator modeling (1) initial energy deposition, (2) hydrodynamic evolution of the quark-gluon plasma, and (3) particlization; each stage has its own fidelity parameters (e.g., mesh sizes, time-step scales), and the simulator is used as the motivating application to evaluate CONFIG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Three-stage heavy-ion collision simulator (QGP pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Sequential multi-stage simulator for heavy-ion collisions: (i) models initial energy deposition, (ii) simulates hydrodynamic evolution of the quark-gluon plasma, and (iii) converts fluid to particles (particlization). Each stage can have distinct fidelity controls such as spatial mesh resolution and temporal discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>high-energy nuclear physics / quark-gluon plasma hydrodynamics (fluid dynamics in nuclear physics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Stage-wise multi-fidelity: each stage has its own fidelity parameters; high-fidelity is the limiting case when all stage fidelity parameters -> 0 (fine meshes / small time-steps); lower fidelity arises from coarser meshes / larger time-steps at one or more stages.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity parameters include stage-specific mesh sizes and time discretizations; numerical error per stage can be bounded by polynomial decay C * t^xi, and overall error can be bounded by a sum over stage-wise errors. Kernel 2 (Brownian-like, non-stationary) is recommended to encode these convergence-rate characteristics (with hyperparameters l_r set to stage error exponents xi_r if known).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CONFIG Gaussian Process emulator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gaussian process emulator with decomposition eta(x,t)=phi(x)+delta(x,t), using a product kernel K_x^phi + K_x^delta * K_t, where K_t (Kernel 2) is a non-stationary kernel designed to reflect polynomial numerical convergence across sequential stages; hyperparameters can encode per-stage convergence rates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate/predict the limiting highest-fidelity QGP simulation outputs (physical observables from heavy-ion collisions) by learning from multi-stage, multi-fidelity simulation data and predicting the t->0 limit.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Prediction of the highest-fidelity simulation limit (t -> 0) / emulate exact/highest-fidelity outputs; effectively transfer from lower- and medium-fidelity multi-stage runs to the limiting high-fidelity behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>CONFIG with Kernel 2 captures prior information about stage-wise numerical convergence (error decay rates) and yields improved predictive performance over baseline multi-fidelity methods; Kernel 2 is especially appropriate for sequential-stage simulators (Scenario 2). The paper reports overall improved emulation accuracy on the QGP application versus state-of-the-art alternatives, though no numeric performance metrics are given in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The authors state that only when all stage fidelity parameters go to their highest settings (all t_r -> 0) should the discrepancy converge to zero; they recommend setting Kernel 2 hyperparameters l_r to known per-stage convergence exponents xi_r when available, but do not give a numeric minimal-fidelity cutoff. They caution that if some stages remain at positive fidelity, non-negligible discrepancy remains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Two failure-related points are discussed: (1) Ignoring the conglomerate multi-stage fidelity structure (e.g., aggregating fidelities or imposing an artificial ranking) can significantly worsen emulator predictions. (2) Kernel 2 yields highly non-smooth sample paths (Brownian-like) which can be counterintuitive for discretization properties; this is a potential modeling concern rather than an empirical transfer failure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conglomerate Multi-fidelity Gaussian Process Modeling, with Application to Heavy-Ion Collisions', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Predicting the output from a complex computer code when fast approximations are available <em>(Rating: 2)</em></li>
                <li>The Mathematical Theory of Finite Element Methods <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1519",
    "paper_id": "paper-d3b41091add237d06297154f89e9e443c25ba939",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "FEA cantilever beam simulator",
            "name_full": "Finite Element Analysis cantilever beam simulator (3D mesh)",
            "brief_description": "A multi-fidelity finite-element simulator for 3D cantilever beam deflection where three mesh-fidelity parameters (one per spatial dimension) control simulation accuracy; used in the paper as an application to evaluate multi-fidelity emulation methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "FEA cantilever beam simulator",
            "simulator_description": "Finite element analysis simulator that computes deflection of a cantilever beam under load; fidelity is controlled via mesh element sizes (three fidelity parameters corresponding to each spatial dimension in the 3D FEA).",
            "scientific_domain": "mechanics / structural finite element analysis",
            "fidelity_level": "Multi-fidelity via mesh discretization: high-fidelity corresponds to very fine mesh (limiting solution as mesh sizes -&gt; 0), low-fidelity corresponds to coarse mesh; fidelity is continuous via mesh-size parameters.",
            "fidelity_characteristics": "Fidelity parameters are mesh element sizes for each spatial dimension; smaller t values mean finer mesh and higher computational cost; fidelity affects numerical discretization error of FEA solution.",
            "model_or_agent_name": "CONFIG Gaussian Process emulator (and baseline multi-fidelity GP models: KOH, TWY)",
            "model_description": "Gaussian process emulator that decomposes the output as phi(x) (limiting exact solution) + delta(x,t) (discrepancy), using a non-stationary covariance in fidelity parameters (Kernel 1 recommended for this scenario); compared against Kennedy–O'Hagan and Tuo–Wu–Yu style models.",
            "reasoning_task": "Learn an emulator that predicts the limiting highest-fidelity cantilever beam deflection (the t-&gt;0 exact solution) from multi-fidelity FEA simulation outputs.",
            "training_performance": null,
            "transfer_target": "Limiting highest-fidelity simulator (predict t -&gt; 0 / exact FEA solution); effectively transfer from lower-fidelity runs to predictions of highest-fidelity behaviour.",
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "CONFIG (with Kernel 1) outperforms existing multi-fidelity approaches when multiple mesh-fidelity parameters interact (Scenario 1). Kernel 1 is recommended for this FEA setting because it models smooth interactions between multiple mesh parameters; naively aggregating multiple fidelities into a single artificial ranking or ignoring conglomerate structure yields poorer predictive results.",
            "minimal_fidelity_discussion": "The paper emphasizes that only when all fidelity parameters approach their highest setting (t_r -&gt; 0 for all r) should the discrepancy vanish; it does not specify a numeric minimal fidelity threshold, but argues that neglecting stage-wise/multi-parameter fidelity leads to poor emulation. No explicit minimal fidelity (e.g., a required mesh size) is given.",
            "failure_cases": "Reported conceptual failure modes include: (1) assigning an arbitrary single fidelity ranking (aggregating multiple mesh parameters) or (2) ignoring the conglomerate multi-fidelity structure — both lead to significantly poorer predictive performance from emulators. No specific numeric failure thresholds are provided.",
            "uuid": "e1519.0",
            "source_info": {
                "paper_title": "Conglomerate Multi-fidelity Gaussian Process Modeling, with Application to Heavy-Ion Collisions",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Three-stage heavy-ion collision simulator (QGP)",
            "name_full": "Consolidated multi-stage heavy-ion collision simulator for Quark-Gluon Plasma evolution",
            "brief_description": "A conglomerate, multi-stage simulator modeling (1) initial energy deposition, (2) hydrodynamic evolution of the quark-gluon plasma, and (3) particlization; each stage has its own fidelity parameters (e.g., mesh sizes, time-step scales), and the simulator is used as the motivating application to evaluate CONFIG.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Three-stage heavy-ion collision simulator (QGP pipeline)",
            "simulator_description": "Sequential multi-stage simulator for heavy-ion collisions: (i) models initial energy deposition, (ii) simulates hydrodynamic evolution of the quark-gluon plasma, and (iii) converts fluid to particles (particlization). Each stage can have distinct fidelity controls such as spatial mesh resolution and temporal discretization.",
            "scientific_domain": "high-energy nuclear physics / quark-gluon plasma hydrodynamics (fluid dynamics in nuclear physics)",
            "fidelity_level": "Stage-wise multi-fidelity: each stage has its own fidelity parameters; high-fidelity is the limiting case when all stage fidelity parameters -&gt; 0 (fine meshes / small time-steps); lower fidelity arises from coarser meshes / larger time-steps at one or more stages.",
            "fidelity_characteristics": "Fidelity parameters include stage-specific mesh sizes and time discretizations; numerical error per stage can be bounded by polynomial decay C * t^xi, and overall error can be bounded by a sum over stage-wise errors. Kernel 2 (Brownian-like, non-stationary) is recommended to encode these convergence-rate characteristics (with hyperparameters l_r set to stage error exponents xi_r if known).",
            "model_or_agent_name": "CONFIG Gaussian Process emulator",
            "model_description": "Gaussian process emulator with decomposition eta(x,t)=phi(x)+delta(x,t), using a product kernel K_x^phi + K_x^delta * K_t, where K_t (Kernel 2) is a non-stationary kernel designed to reflect polynomial numerical convergence across sequential stages; hyperparameters can encode per-stage convergence rates.",
            "reasoning_task": "Emulate/predict the limiting highest-fidelity QGP simulation outputs (physical observables from heavy-ion collisions) by learning from multi-stage, multi-fidelity simulation data and predicting the t-&gt;0 limit.",
            "training_performance": null,
            "transfer_target": "Prediction of the highest-fidelity simulation limit (t -&gt; 0) / emulate exact/highest-fidelity outputs; effectively transfer from lower- and medium-fidelity multi-stage runs to the limiting high-fidelity behavior.",
            "transfer_performance": null,
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "CONFIG with Kernel 2 captures prior information about stage-wise numerical convergence (error decay rates) and yields improved predictive performance over baseline multi-fidelity methods; Kernel 2 is especially appropriate for sequential-stage simulators (Scenario 2). The paper reports overall improved emulation accuracy on the QGP application versus state-of-the-art alternatives, though no numeric performance metrics are given in the provided text.",
            "minimal_fidelity_discussion": "The authors state that only when all stage fidelity parameters go to their highest settings (all t_r -&gt; 0) should the discrepancy converge to zero; they recommend setting Kernel 2 hyperparameters l_r to known per-stage convergence exponents xi_r when available, but do not give a numeric minimal-fidelity cutoff. They caution that if some stages remain at positive fidelity, non-negligible discrepancy remains.",
            "failure_cases": "Two failure-related points are discussed: (1) Ignoring the conglomerate multi-stage fidelity structure (e.g., aggregating fidelities or imposing an artificial ranking) can significantly worsen emulator predictions. (2) Kernel 2 yields highly non-smooth sample paths (Brownian-like) which can be counterintuitive for discretization properties; this is a potential modeling concern rather than an empirical transfer failure.",
            "uuid": "e1519.1",
            "source_info": {
                "paper_title": "Conglomerate Multi-fidelity Gaussian Process Modeling, with Application to Heavy-Ion Collisions",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Predicting the output from a complex computer code when fast approximations are available",
            "rating": 2
        },
        {
            "paper_title": "The Mathematical Theory of Finite Element Methods",
            "rating": 2
        }
    ],
    "cost": 0.01460575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Conglomerate Multi-Fidelity Gaussian Process Modeling, with Application to Heavy-Ion Collisions</h1>
<p>Yi Ji ${ }^{1,5}$, Henry Shaowu Yuchi ${ }^{2,5}$<br>Derek Soeder, J.-F. Paquet ${ }^{3,4}$<br>Steffen A. Bass, V. Roshan Joseph, C. F. Jeff Wu, Simon Mak ${ }^{1, *}$<br>October 2, 2023</p>
<h4>Abstract</h4>
<p>In an era where scientific experimentation is often costly, multi-fidelity emulation provides a powerful tool for predictive scientific computing. While there has been notable work on multi-fidelity modeling, existing models do not incorporate an important "conglomerate" property of multi-fidelity simulators, where the accuracies of different simulator components are controlled by different fidelity parameters. Such conglomerate simulators are widely encountered in complex nuclear physics and astrophysics applications. We thus propose a new CONglomerate multi-FIdelity Gaussian process (CONFIG) model, which embeds this conglomerate structure within a novel non-stationary covariance function. We show that the proposed CONFIG model can capture prior knowledge on the numerical convergence of conglomerate simulators, which allows for cost-efficient emulation of multi-fidelity systems. We demonstrate the improved predictive performance of CONFIG over state-of-the-art models in a suite of numerical experiments and two applications, the first for emulation of cantilever beam deflection and the second for emulating the evolution of the quark-gluon plasma, which was theorized to have filled the Universe shortly after the Big Bang.</p>
<p>Keywords: Bayesian Nonparametrics, Computer Experiments, Multi-Fidelity Modeling, Surrogate Modeling, Quark-Gluon Plasma.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Computer experimentation is widely used for modeling complex scientific and engineering systems, particularly when physical experiments are costly, unethical, or impossible to perform. This shift from physical to computer experimentation has found success in a wide range of physical science applications, including rocket design (Mak et al. 2018), solar irradiance modeling (Sun et al. 2019) and 3D printing (Chen et al. 2021). However, as systems become more complex and realistic, such computer experiments also become more expensive, thus placing a heavy computational burden on design exploration and optimization. Statistical emulators (Santner et al. 2003) have shown great promise in tackling this limitation. The idea is simple but effective: computer experiments are first performed at carefully chosen design points, then used as training data to fit an emulator model to efficiently predict and quantify uncertainty on the expensive virtual experiment.</p>
<p>In recent years, however, with the increasing sophistication of modern scientific problems, an emerging challenge for emulators is the simulation of high-fidelity training data, which can be prohibitively expensive. One way to address this is via multi-fidelity emulation, which makes use of training simulation data of multiple fidelities (or accuracy) for model fitting. Such multi-fidelity data can often be generated by varying different fidelity parameters, which control the precision of the numerical experiment. There are a wide variety of fidelity parameters, ranging from mesh sizes for finite element analysis (Park et al. 1997, More \&amp; Bindu 2015) to time-steps for dynamical system simulation (Vanden-Eijnden 2003). The goal is to leverage information from lower-fidelity (but cheaper) simulations to enhance predictions for the high-fidelity (but expensive) model, thus allowing for improved emulation and uncertainty quantification (for the highest-fidelity code) at lower computational costs.</p>
<p>There has been much recent work on multi-fidelity emulation, particularly for Gaussian process (GP) modeling. This includes the seminal work of Kennedy \&amp; O'Hagan (2000), which presented a first-order autoregressive model for integrating information over a hierarchy of simulation models, from lowest to highest fidelity. This Kennedy-O'Hagan model</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Three-stage simulation of the quark-gluon plasma.</p>
<p>has then been extended in various works, including a Bayesian hierarchical implementation in Qian et al. (2006), the multi-fidelity optimization in Forrester et al. (2007), and the nonlinear fusion model in Perdikaris et al. (2017). Tuo et al. (2014) proposed a multi-fidelity emulator for finite element analysis (FEA), which utilizes the discretization mesh size as the single fidelity parameter. This emulator models the bias induced by discretization mesh elements via a GP, and is related to the state-of-the-art grid convergence index approach typically employed in FEA ROACHE (1994), Bect et al. (2021). Such multi-fidelity models have been widely applied in engineering design and scientific computing; see, e.g., Kou &amp; Zhang (2019), Shi et al. (2020), Jin et al. (2021), Liyanage et al. (2022a). Experimental design for such emulators have been explored (Xiong et al. 2013), including a sequential design strategy in He et al. (2017). Similar ideas have also been applied for broader applications in data fusion (Ghoreishi &amp; Allaire 2019), Bayesian optimization (Poloczek et al. 2017, Moss et al. 2021), and transfer learning (Tighineanu et al. 2022).</p>
<p>The above methods, however, have limitations when applied to our motivating nuclear physics application. Here, we are studying the Quark-Gluon Plasma (QGP), a deconfined phase of nuclear matter consisting of elementary quarks and gluons. The QGP was theorized to have filled the Universe shortly after the Big Bang, and the study of this plasma sheds light on the properties of this unique phase of matter. This plasma can be simulated at a small scale by virtually colliding heavy ions together at near-light speeds in particle</p>
<p>colliders. Simulating such collisions requires a "conglomerate" system of complex dynamical models to faithfully capture the detailed evolution of the plasma. Consider in particular the three-stage simulation framework in Everett et al. (2021) (see also (Gale et al. 2013, Heinz \&amp; Snellings 2013, De Souza et al. 2016)), which models the initial energy disposition of the heavy ions, the hydrodynamic evolution of the plasma after the collision, and the subsequent conversion of nuclear fluid into particles. Figure 1 visualizes this conglomerate (specifically, multi-stage) procedure. At each stage, the simulation of the component physics can involve multiple and different fidelity parameters, controlling, e.g., the size of the hydrodynamics spatial mesh, or the time-scale for dynamic evolution.</p>
<p>This conglomerate multi-fidelity framework, where the simulator comprises multiple submodels for simulating different physics of a complex phenomenon, poses several challenges for existing multi-fidelity emulators. First, since there are multiple fidelity parameters to set for each simulation stage, the resulting simulation runs typically cannot be ranked from lowest to highest fidelity, which is required for a direct application of Kennedy-O'Hagantype models. For example, to gauge the effects of three fidelity parameters, the physicist may choose to run the simulator in three different ways, each with higher fidelity at one stage and lower fidelity at the remaining stages. A priori, it is unclear if these three simulation approaches can be ranked from lowest to highest fidelity. Second, unlike the multi-fidelity emulator in Tuo et al. (2014) (which allows only one fidelity parameter), there are multiple fidelity parameters that should be accounted for when training emulators with conglomerate simulations. Neglecting this conglomerate structure for emulation can result in significantly poorer predictive performance, as we show later. A broader emulation model is thus needed to tackle the challenges presented by conglomerate multi-fidelity simulators, which are widely encountered in nuclear physics (Ji et al. 2021) and astrophysics (Ho et al. 2022).</p>
<p>We propose in this work a new GP emulator that addresses these challenges. The proposed CONglomerate multi-FIdelity Gaussian process (CONFIG) model makes use of a novel non-stationary covariance function, which captures prior information on the nu-</p>
<p>merical convergence of conglomerate simulators. Our emulator is applicable for a variety of complex multi-physics simulators, where each physics (with its corresponding fidelity parameters) is jointly simulated via a conglomerate framework. By embedding this underlying conglomerate structure within its kernel specification, the CONFIG model can yield improved emulation performance and uncertainty quantification over existing methods for predicting the limiting highest-fidelity simulator. This is demonstrated in a suite of numerical experiments, a beam deflection problem in finite element analysis, and an application to the motivating heavy-ion collision problem. Section 2 reviews several existing multi-fidelity emulators and outlines the motivating QGP problem. Section 3 presents the model specification for the proposed CONFIG emulator. Section 4 discusses implementation details for CONFIG, including parameter estimation and experimental design. Section 5 compares the proposed model with existing methods on a suite of numerical experiments. Finally, Section 6 demonstrates the effectiveness of CONFIG for the motivating QGP application as well as a cantilever beam deflection problem. Section 7 concludes the paper.</p>
<h1>2 Preliminaries \&amp; Motivation</h1>
<p>In this section, we first provide an overview of conglomerate multi-fidelity simulators and their use for complex multi-physics applications. We then briefly introduce the Gaussian process model and review the Kennedy-O'Hagan model in (Kennedy \&amp; O'Hagan 2000) and the multi-fidelity model in Tuo et al. (2014). Finally, we discuss the limitations of such models for our QGP application, thus motivating the proposed CONFIG model.</p>
<h3>2.1 Conglomerate Multi-fidelity Simulation</h3>
<p>With an urgent need for reliable simulation of complex phenomena involving multiple physical mechanisms and/or components, conglomerate multi-fidelity simulations are now increasingly used in modern scientific and engineering applications, such as structural studies Sun et al. (2010), DiazDelaO \&amp; Adhikari (2012), engine combustion (Narayanan et al.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualizing examples of Scenarios 1 and 2 for conglomerate multi-fidelity simulations. The left plot shows an example of Scenario 1 for cantilever beam deflection, where the three fidelity parameters specify the size of the finite elements for the beam. The right plot shows an example of Scenario 2 for heavy-ion collisions, where different fidelity parameters control simulation precision at different stages of the collision system.</p>
<p>2023) and high-energy physics Kumar et al. (2023). Such simulators model the complex phenomenon via either multiple submodels that account for different physics (e.g., hydrodynamic evolution, nuclear particlization), or multiple components (e.g., spatial mesh, time discretization) that facilitate the simulation procedure. Consequently, the simulation of the <em>overall</em> phenomenon typically involves <em>multiple</em> fidelity parameters, each controlling the simulation accuracy of individual submodels or components. This poses a key challenge for existing emulator models.</p>
<p>To tackle this, it is useful to first understand different types of conglomerate simulators encountered in applications. In our experience, this falls roughly into two scenarios (see Figure 2):</p>
<ul>
<li><strong>Scenario 1</strong>: The simulator consists of multiple fidelity parameters for simulating a <em>single</em> mechanism or phenomenon. Such parameters control different means for varying simulation precision, e.g., via spatial meshing or temporal discretization. One example is the FEA of a cantilever beam deflection under stress, where three mesh fidelity parameters can be used used for each dimension of the three-dimensional</li>
</ul>
<p>finite element analysis. We will investigate this application further in Section 6.1.</p>
<ul>
<li>Scenario 2: The simulator comprises multiple stages that are performed sequentially over time, where a separate phenomenon is simulated at each stage, with associated fidelity parameters. Multiple mechanisms are thus involved in simulating the desired phenomenon. This is the case for our motivating nuclear physics problem (Figure 1), where multiple mesh size parameters control simulation precision in each of the three consecutive stages for heavy-ion collisions. We will investigate this application further in Section 6.2.</li>
</ul>
<p>Motivated by these two scenarios, we will present later two variations of the CONFIG model that tackle each of these scenarios; more on this in Section 3.</p>
<h1>2.2 Gaussian Process Modeling</h1>
<p>Gaussian process (GP) modeling is a popular Bayesian nonparametric approach for supervised learning (Williams \&amp; Rasmussen 2006), with broad applications for computer experiments (Santner et al. 2003). The specification of a GP model involves two key ingredients: the mean function and the covariance function. Let $\mathbf{x} \in[0,1]^{p}$ be the input parameters (sufficiently scaled) for the simulator, and let $\eta(\mathbf{x})$ be the corresponding output of the simulator. A GP model places the following prior on the unknown response surface $\eta(\cdot):$</p>
<p>$$
\eta(\cdot) \sim \mathcal{G} \mathcal{P}(\mu(\cdot), k(\cdot, \cdot))
$$</p>
<p>Here, $\mu(\cdot)$ is the mean function controlling the centrality of the stochastic process. If appropriate basis functions $\boldsymbol{f}(\boldsymbol{x})$ are known, one can model the mean function as $\mu(\boldsymbol{x})=\boldsymbol{f}(\boldsymbol{x})^{T} \boldsymbol{\beta}$, where $\boldsymbol{\beta}$ are the corresponding coefficients on $\boldsymbol{f}(\boldsymbol{x})$. In the absence of such information, $\mu(\cdot)$ is typically set to be a constant. The function $k(\cdot, \cdot)$ is the covariance function that controls the smoothness of its sample paths. Common choices of $k(\cdot, \cdot)$ include the squaredexponential and Matérn kernels (Santner et al. 2003).</p>
<p>Let $\mathcal{D}=\left{\mathbf{x}<em n="n">{1}, \cdots, \mathbf{x}</em>}\right}$ denote the simulated input points, and $\mathbf{y}=\left[\eta\left(\mathbf{x<em n="n">{1}\right), \cdots, \eta\left(\mathbf{x}</em>^{}\right)\right]$ be the simulated outputs. Assuming that the kernel hyperparameters are fixed and known (we will discuss the estimation of such parameters later in Section 4.1), the predictive distribution $\eta\left(\mathbf{x<em>}\right)$ at the new input $\mathbf{x}^{</em>}$ conditional on data ${\mathcal{D}, \mathbf{y}}$ is given by:</p>
<p>$$
\eta\left(\mathbf{x}^{<em>}\right) \mid \mathcal{D}, \mathbf{y} \sim \mathcal{G} \mathcal{P}\left(\hat{\mu}\left(\mathbf{x}^{</em>}\right), s^{2}\left(\mathbf{x}^{*}\right)\right)
$$</p>
<p>Here, the posterior mean and variance are given by:</p>
<p>$$
\begin{aligned}
\hat{\mu}\left(\mathbf{x}^{<em>}\right) &amp; =\mu\left(\mathbf{x}^{</em>}\right)+\mathbf{k}\left(\mathbf{x}^{<em>}, \mathcal{D}\right)^{T} \mathbf{K}(\mathcal{D})^{-1}(\mathbf{y}-\boldsymbol{\mu}(\mathcal{D})) \
s^{2}\left(\mathbf{x}^{</em>}\right) &amp; =k\left(\mathbf{x}^{<em>}, \mathbf{x}^{</em>}\right)-\mathbf{k}\left(\mathbf{x}^{<em>}, \mathcal{D}\right)^{T} \mathbf{K}(\mathcal{D})^{-1} \mathbf{k}\left(\mathbf{x}^{</em>}, \mathcal{D}\right)
\end{aligned}
$$</p>
<p>where $\mathbf{k}\left(\mathbf{x}^{<em>}, \mathcal{D}\right)=\left[k\left(\mathbf{x}^{</em>}, \mathbf{x}<em n="n">{1}\right), \cdots, k\left(\mathbf{x}^{*}, \mathbf{x}</em>}\right)\right]$ is the vector of covariances, $\boldsymbol{\mu}(\mathcal{D})=\left[\mu\left(\mathbf{x<em n="n">{1}\right), \cdots\right.$, $\left.\mu\left(\mathbf{x}</em>)$ is the covariance matrix for the training data. The models introduced later in this paper will make use of these closed-form predictive equations with different choices of covariance functions.}\right)\right]$ is the vector of means, and $\mathbf{K}(\mathcal{D</p>
<h1>2.3 The Kennedy-O'Hagan model</h1>
<p>In the seminal work of Kennedy \&amp; O'Hagan (2000), the authors proposed a first-order autoregressive model for linking outputs from a hierarchy of $H$ simulators, from lowest fidelity (simulator 1) to the highest fidelity (simulator $H$ ). Let $\eta_{h}(\mathbf{x})$ denote the output from simulator $h$ at standardized input parameters $\mathbf{x} \in[0,1]^{p}$. The Kennedy-O'Hagan $(\mathrm{KOH})$ model is specified as:</p>
<p>$$
\eta_{h}(\mathbf{x})=\rho_{h-1} \eta_{h-1}(\mathbf{x})+\delta_{h}(\mathbf{x}), \quad h=2, \cdots, H
$$</p>
<p>Here, $\rho_{h-1}$ is a regression scale factor, and $\delta_{h}(\mathbf{x})$ is a bias term that models the discrepancy between simulator $h-1$ and $h$. The bias term $\delta_{h}(\mathbf{x})$ may be modeled by a stationary GP with a squared-exponential covariance function (Santner et al. 2003):</p>
<p>$$
\operatorname{Cov}\left[\delta_{h}(\mathbf{x}), \delta_{h}\left(\mathbf{x}^{\prime}\right)\right]=\sigma_{h}^{2} \exp \left{-\sum_{i=1}^{p} \phi_{h, i}\left(\mathbf{x}<em i="i">{i}-\mathbf{x}</em>\right}
$$}^{\prime}\right)^{2</p>
<p>where $\phi_{h, i}$ is the weight parameter for the $i^{\text {th }}$ input parameter at the $h^{\text {th }}$ fidelity level. Such a model allows one to integrate information from a sequence of simulator models with varying fidelity levels, to efficiently emulate the highest-fidelity simulator model.</p>
<p>The KOH multi-fidelity model has subsequently been extended in a variety of ways, including a Bayesian implementation in Qian \&amp; Wu (2008) and a nonlinear extension in Perdikaris et al. (2015); see also Reese et al. (2004), DiazDelaO \&amp; Adhikari (2012), Fricker et al. (2013). This modeling framework is also closely related to the idea of co-kriging (Stein \&amp; Corsten 1991) in spatial statistics, and was employed for sequential co-kriging design (Le Gratiet \&amp; Cannamela 2015). However, the aforementioned methods assume that the multi-fidelity training data can be ranked from lowest to highest fidelity. As such, this body of literature does not directly apply to the motivating problem of conglomerate multifidelity emulation, where simulation accuracy is controlled by multiple fidelity parameters, and thus there is no clear ranking of training data from lowest to highest fidelity. There are several ways to force existing models on this problem, but each has its shortcomings. One could design the data such that the training simulations are ranked (e.g., increasing all fidelity parameters simultaneously), but this would result in highly inefficient designs which fail to sufficiently explore the space of fidelity parameters. One could also arbitrarily assign a single "artificial" fidelity level for each simulation, which imposes a ranking on the training runs. This, however, neglects the rich conglomerate multi-fidelity framework (i.e., the "science") for the simulator, which can lead to significantly poorer predictive performance from the emulator, as we show later.</p>
<h1>2.4 The Tuo-Wu-Yu model</h1>
<p>For problems where the fidelity level is controlled by a single continuous fidelity parameter $t$ (e.g., mesh size), Tuo et al. (2014) proposed an alternate model (we call this the TWY model) which can make use of such information. Let $\eta(\mathbf{x}, t)$ denote the deterministic code output at standardized inputs $\mathbf{x} \in[0,1]^{p}$ and at fidelity parameter $t$. Here, $t$ is typically assumed to be between 0 and 1 , with a smaller $t$ indicating a finer mesh size or, equivalently,</p>
<p>higher mesh density. The TWY model adopts the following model for $\eta(\mathbf{x}, t)$ :</p>
<p>$$
\eta(\mathbf{x}, t)=\eta(\mathbf{x}, 0)+\delta(\mathbf{x}, t)=: \phi(\mathbf{x})+\delta(\mathbf{x}, t)
$$</p>
<p>Here, $\phi(\mathbf{x}):=\eta(\mathbf{x}, 0)$ denotes the "exact" simulation output at input $\mathbf{x}$ at the highest (limiting) fidelity $t=0$, and $\delta(\mathbf{x}, t)$ denotes the discrepancy (or bias) between this exact solution and realized simulation output with mesh size $t$. In practical problems, the exact solution $\eta(\mathbf{x}, 0)$ is typically not obtainable numerically since some level of approximation (e.g., mesh or time discretization) is needed for simulating the system. The goal is to leverage simulation training data of the form $\left{\eta\left(\mathbf{x}<em i="i">{i}, t</em>, 0)$.}\right)\right}_{i=1}^{n}$ along with an appropriate model on (6) to predict the exact solution $\eta(\mathbf{x</p>
<p>Since $\phi(\mathbf{x})$ and $\delta(\mathbf{x}, t)$ are unknown a priori, these terms are modeled in Tuo et al. (2014) by two independent Gaussian processes. For $\phi(\mathbf{x})$, a standard GP prior is assigned with constant mean and a stationary correlation (e.g., squared-exponential) function. For the bias term $\delta(\mathbf{x}, t)$, a non-stationary zero-mean GP prior is assigned, with covariance function:</p>
<p>$$
\operatorname{Cov}\left[\delta\left(\mathbf{x}<em 1="1">{\mathbf{1}}, t</em>}\right), \delta\left(\mathbf{x<em 2="2">{\mathbf{2}}, t</em>}\right)\right]=\sigma_{2}^{2} K_{\mathbf{x}}^{\delta}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>
$$}\right) \min \left(t_{1}, t_{2}\right)^{l</p>
<p>where $K_{\mathbf{x}}^{\delta}(\cdot, \cdot)$ is a stationary correlation function on input parameters $\mathbf{x}$. One can view this as a product of two kernels, where the kernel on the fidelity parameter $t$ is non-stationary and closely resembles that of a Brownian motion. This separable kernel structure has been utilized in Picheny \&amp; Ginsbourger (2013) as well.</p>
<p>This choice of non-stationary kernel over the single fidelity parameter $t$ can be reasoned from a Bayesian modeling perspective. Consider the GP model with covariance function (7) as a prior model on discrepancy $\delta(\mathbf{x}, t)$. Before observing the data, one can show that:</p>
<p>$$
\lim _{t \rightarrow 0} \delta(\mathbf{x}, t)=0, \quad \text { for all } \mathbf{x} \in[0,1]^{p} \text { almost surely. }
$$</p>
<p>The TWY model thus assumes a priori that the discrepancy term should converge to 0 as fidelity parameter $t$ goes to 0 , or equivalently, the simulation output $\eta(\mathbf{x}, t)$ converges to the exact solution $\phi(\mathbf{x})$, as we increase the fidelity of the simulator. This can be seen as a</p>
<p>way of integrating prior information on the numerical convergence of the simulator within the prior specification of the emulator model. One can further set the kernel parameter $l$ to capture additional information on known numerical convergence rates of the simulator; see Tuo et al. (2014) for details.</p>
<p>For the target conglomerate setting where multiple fidelity parameters are present, the TWY model needs to be further extended. A simple modification might be to first assign for each simulation run an "artificial" fidelity, e.g., the average of the multiple fidelity parameters, then use this single aggregate fidelity level with the TWY model for multifidelity emulation. However, such an approach ignores the rich conglomerate structure of the simulation framework, which can lead to poor predictive performance. We show later that, by integrating directly the conglomerate multi-fidelity nature of the simulation framework (i.e., the "science") within the CONFIG model, we can achieve significantly improved predictive performance in numerical experiments and for the motivating nuclear physics application.</p>
<h1>3 The CONFIG Model</h1>
<p>Given these limitations, we now present the proposed CONFIG model for the efficient emulation of conglomerate multi-fidelity simulations. Our model adopts a novel non-stationary Gaussian process model which captures prior information on the numerical convergence behavior of conglomerate simulators. Below, we outline the general CONFIG model specification, then present two choices of non-stationary covariance functions which capture this desired prior information.</p>
<p>Let $\mathbf{x} \in[0,1]^{p}$ be the vector of $p$ standardized simulation inputs for the computer code (again assumed to be deterministic), and suppose there are $q$ fidelity parameters (denoted by $\mathbf{t} \in[0,1]^{q}$ ) which control simulation accuracy in the code. These may, e.g., consist of different mesh sizes for domain discretization and time steps at different simulation stages. As before, a smaller fidelity parameter $t_{r}$ (with other fidelity parameters held constant) yields more accurate simulations at higher computational costs, with $t_{r}=0$ denoting the</p>
<p>highest (limiting) fidelity level. Let $\eta(\mathbf{x}, \mathbf{t})$ denote the deterministic code output at inputs $\mathbf{x}$ and fidelity parameters $\mathbf{t}$. The CONFIG model assumes the following decomposition of $\eta(\mathbf{x}, \mathbf{t}):$</p>
<p>$$
\eta(\mathbf{x}, \mathbf{t})=\eta(\mathbf{x}, \mathbf{0})+\delta(\mathbf{x}, \mathbf{t}):=\phi(\mathbf{x})+\delta(\mathbf{x}, \mathbf{t})
$$</p>
<p>Similar to before, $\phi(\mathbf{x}):=\eta(\mathbf{x}, \mathbf{0})$ models the "exact" simulation solution at the highest (limiting) fidelity setting of $\mathbf{t} \rightarrow \mathbf{0}$, and $\delta(\mathbf{x}, \mathbf{t})$ models the numerical discrepancy (or error) between the exact solution $\phi(\mathbf{x})$ and the simulated output $\eta(\mathbf{x}, \mathbf{t})$. Since both $\phi(\mathbf{x})$ and $\delta(\mathbf{x}, \mathbf{t})$ are unknown, we again place independent Gaussian process priors on both terms. For $\phi(\mathbf{x})$, a standard GP is assigned with user-defined basis functions for the mean and a stationary correlation function. In a later implementation, we will make use of linear basis functions along with the popular squared-exponential correlation function:</p>
<p>$$
\operatorname{Cov}\left[\phi\left(\mathbf{x}<em 2="2">{1}\right), \phi\left(\mathbf{x}</em>}\right)\right]=\sigma_{1}^{2} K_{\mathbf{x}}^{\phi}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>\right}
$$}\right)=\sigma_{1}^{2} \exp \left{-\sum_{s=1}^{p} \gamma_{s}\left(x_{1, s}-x_{2, s}\right)^{2</p>
<p>where $\gamma_{s}$ is the weight parameter for the $s^{\text {th }}$ input dimension.
For the bias term $\delta(\mathbf{x}, \mathbf{t})$, we will carefully specify a new non-stationary covariance function that captures one's prior knowledge on the numerical convergence behavior. One desirable property of $\delta(\mathbf{x}, \mathbf{t})$ is the limiting constraint:</p>
<p>$$
\lim _{\mathbf{t} \rightarrow \mathbf{0}} \delta(\mathbf{x}, \mathbf{t})=0, \quad \text { for all } \mathbf{x} \in[0,1]^{p} \text { almost surely. }
$$</p>
<p>In words, for any inputs $\mathbf{x}$, the simulation output $\eta(\mathbf{x}, \mathbf{t})$ should converge to the underlying exact solution $\phi(\mathbf{x})$ when all fidelity parameters converge to zero, i.e., all fidelity levels are set to their highest (limiting) setting. Property (11) should thus be satisfied almost surely if the simulator enjoys theoretical convergence guarantees (e.g., weak convergence of PDE solutions) or is trusted to converge empirically. Another desirable property is that, for a fidelity parameter $t_{r}$ and fixed levels of the remaining fidelity parameters $\mathbf{t}_{-r} \neq \mathbf{0}$, we have:</p>
<p>$$
\lim <em r="r">{t</em>
$$} \rightarrow 0} \delta(\mathbf{x}, \mathbf{t}) \neq 0, \quad \text { for all } \mathbf{x} \in[0,1]^{p} \text { almost surely. </p>
<p>In words, for any inputs $\mathbf{x}$ and any positive fidelity parameters $\mathbf{t}_{-r}$, there should be a non-negligible discrepancy between the simulation output $\eta(\mathbf{x}, \mathbf{t})$ and the underlying true</p>
<p>solution $\phi(\mathbf{x})$. This is again intuitive when the variables in $\mathbf{t}<em -r="-r">{-r}$ are fidelity parameters since the simulator should not be expected to reach the true solution when some of these parameters are not at their highest fidelities, i.e., $\mathbf{t}</em>$. The two limiting constraints thus describe how fidelity parameters determine the discrepancy behavior of the simulator: only when all fidelity parameters approach zero should the simulator converge to the true solution.} \neq \mathbf{0</p>
<p>To satisfy these two properties, we place a Gaussian process prior on $\delta(\mathbf{x}, \mathbf{t})$ with product covariance form:</p>
<p>$$
\operatorname{Cov}\left[\delta\left(\mathbf{x}<em 1="1">{1}, \mathbf{t}</em>}\right), \delta\left(\mathbf{x<em 2="2">{2}, \mathbf{t}</em>}\right)\right]=\sigma_{2}^{2} K_{\mathbf{x}}^{\delta}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}\right) K_{\mathbf{t}}\left(\mathbf{t<em 2="2">{1}, \mathbf{t}</em>\right)
$$</p>
<p>i.e., the effect of input variables and fidelity parameters are assumed to be separable for $\delta$. For the first kernel $K_{\mathbf{x}}^{\delta}(\cdot, \cdot)$, one can employ a standard stationary kernel; we make use of the squared-exponential kernel:</p>
<p>$$
K_{\mathbf{x}}^{\delta}\left(\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>\right}
$$}\right)=\exp \left{-\sum_{s=1}^{p} \alpha_{s}\left(x_{1, s}-x_{2, s}\right)^{2</p>
<p>in our later implementation. For the second kernel $K_{\mathbf{t}}\left(\mathbf{t}<em 2="2">{1}, \mathbf{t}</em>\right)$, a careful non-stationary specification is needed to satisfy the aforementioned two properties; one can show that this non-stationarity is necessary but not sufficient for satisfying these properties, see Tuo et al. (2014) and later discussion. We will present the next two choices for this kernel, which cater to the two common scenarios for conglomerate multi-fidelity simulators from Section 2.1.</p>
<p>We note that these kernel choices are but recommendations - the modeler should carefully consider prior domain knowledge to carefully select a kernel that captures such knowledge. With the kernel $K_{\mathbf{t}}$ specified (along with kernels $K_{\mathbf{x}}^{\delta}$ and $K_{\mathbf{x}}^{\phi}$ ), one can show that the response surface $\eta(\mathbf{x}, \mathbf{t})$ follows a Gaussian process model, with covariance function:</p>
<p>$$
K_{\eta}\left{\left(\mathbf{x}<em 1="1">{1}, \mathbf{t}</em>}\right),\left(\mathbf{x<em 2="2">{2}, \mathbf{t}</em>}\right)\right}:=\operatorname{Cov}\left[\eta\left(\mathbf{x<em 1="1">{1}, \mathbf{t}</em>}\right), \eta\left(\mathbf{x<em 2="2">{2}, \mathbf{t}</em>}\right)\right]=\sigma_{1}^{2} K_{\mathbf{x}}^{\phi}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}\right)+\sigma_{2}^{2} K_{\mathbf{x}}^{\delta}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}\right) K_{\mathbf{t}}\left(\mathbf{t<em 2="2">{1}, \mathbf{t}</em>\right)
$$</p>
<p>The predictive equations for the CONFIG model then follow immediately from the standard GP equations (2) and (3) with kernel $K_{\eta}$ given above, with the desired prediction point</p>
<p>$\left(\mathbf{x}^{*}, \mathbf{0}\right)$ as the goal is to predict the (limiting) highest-fidelity setting. We provide further details on these predictive equations in Section 4.1.1.</p>
<h1>3.1 Kernel Option 1</h1>
<p>Consider the first kernel choice for $K_{\mathbf{t}}$ (Kernel 1), which we recommend for Scenario 1 above. This takes the non-stationary form:</p>
<p>$$
K_{\mathbf{t}}\left(\mathbf{t}<em 2="2">{1}, \mathbf{t}</em>\right}+1
$$}\right)=\exp \left{-\sum_{r=1}^{q} \theta_{r}\left(t_{1, r}-t_{2, r}\right)^{2}\right}-\exp \left{-\sum_{r=1}^{q} \theta_{r} t_{1, r}^{2}\right}-\exp \left{-\sum_{r=1}^{q} \theta_{r} t_{2, r}^{2</p>
<p>Here, $\theta_{r}$ denotes the weight parameter for the $r^{\text {th }}$ fidelity parameter. A larger $\theta_{r}$ indicates greater sensitivity of discrepancy $\delta$ to the $r^{\text {th }}$ fidelity parameter, and vice versa. One can check that, with this kernel (34), the two desired properties (11) and (12) for $\delta$ are satisfied, meaning such a kernel indeed captures the aforementioned prior information on numerical convergence behavior.</p>
<p>Kernel 1 is inspired by the non-stationary covariance function in Gul et al. (2018), which was proposed for a different task of uncertainty propagation for system outputs. The rationale for this kernel here is as follows. For simplicity, let $\delta(\mathbf{t})$ denote the bias term at some fixed input $\mathbf{x}$. One way to ensure $\delta(\mathbf{t})$ satisfies the limiting condition (11), i.e., $\lim _{\mathbf{t} \rightarrow \mathbf{0}} \delta(\mathbf{t})=0$, is to represent it as a difference of two terms:</p>
<p>$$
\delta(\mathbf{t})=\kappa(\mathbf{t})-\kappa(\mathbf{0})
$$</p>
<p>where $\kappa(\cdot)$ can be modeled as a GP. In words, the limiting condition on $\delta(\cdot)$ is enforced by centering $\kappa$ by its response at the limiting fidelity $\mathbf{0}$. The covariance function for $\delta$ can then be written as:</p>
<p>$$
\begin{aligned}
\operatorname{Cov}\left[\delta\left(\mathbf{t}<em 2="2">{1}\right), \delta\left(\mathbf{t}</em>}\right)\right] &amp; =\operatorname{Cov}\left[\kappa\left(\mathbf{t<em 2="2">{1}\right)-\kappa(\mathbf{0}), \kappa\left(\mathbf{t}</em>)\right] \
&amp; =\operatorname{Cov}\left[\kappa\left(\mathbf{t}}\right)-\kappa(\mathbf{0<em 2="2">{1}\right), \kappa\left(\mathbf{t}</em>}\right)\right]-\operatorname{Cov}\left[\kappa\left(\mathbf{t<em 2="2">{1}\right), \kappa(\mathbf{0})\right]-\operatorname{Cov}\left[\kappa\left(\mathbf{t}</em>)]
\end{aligned}
$$}\right), \kappa(\mathbf{0})\right]+\operatorname{Cov}[\kappa(\mathbf{0}), \kappa(\mathbf{0</p>
<p>Kernel 1 in (34) can be recovered from (36) with a squared-exponential correlation function on $\kappa$, and satisfies the desired limiting condition (11) by construction.</p>
<p>Kernel 1 has several appealing features for conglomerate multi-fidelity emulation. First, in many applications, one may have prior knowledge of the continuity of the underlying numerical solutions (e.g., from FEA theory). With Kernel 1, the corresponding prior process on discrepancy $\delta$ can be shown to yield continuous sample paths, thus capturing such prior knowledge from a Bayesian perspective. Second, the form of this kernel provides a flexible framework for modeling interactions between fidelity parameters across different stages. Compared to the additive structure in Kernel 2 introduced later, the latent GP model on $\kappa(\cdot)$ (with the squared-exponential kernel) provides a flexible framework for learning interactions between different fidelity parameters. Because of this, Kernel 1 appears to work best in Scenario 1 for emulating a single mechanism with multiple fidelity parameters, e.g., the FEA for beam deflection with different fidelities for each dimension, as such systems often have significant interaction effects between fidelity parameters, e.g., between mesh sizes of each dimension.</p>
<h1>3.2 Kernel Option 2</h1>
<p>Consider next the second choice for $K_{\mathbf{t}}$ (Kernel 2), which we recommend for the multi-stage sequential simulations in Scenario 2. This kernel takes the non-stationary form:</p>
<p>$$
K_{\mathbf{t}}\left(\mathbf{t}<em 2="2">{1}, \mathbf{t}</em>
$$}\right)=\left[\sum_{r=1}^{q} \theta_{r} \min \left(t_{1, r}, t_{2, r}\right)^{l_{r}}\right]^{l</p>
<p>Here, $\theta_{r}$ is a weight parameter for the $r^{\text {th }}$ fidelity parameter, and $l_{r}$ and $l$ are kernel hyperparameters which we discuss later. Similar to Kernel 1, a greater $\theta_{r}$ allows for greater sensitivity of the discrepancy $\delta$ to the $r^{\text {th }}$ fidelity parameter. We can again show that with this kernel (38), the two properties (11) and (12) for bias $\delta$ are satisfied. This follows from the observations that $K_{\mathbf{t}}\left(\mathbf{t}^{\prime}, \mathbf{t}^{\prime}\right)$ approaches 0 as $\mathbf{t}^{\prime} \rightarrow \mathbf{0}$, and that given non-zero entries in $\mathbf{t}^{\prime}, K_{\mathbf{t}}\left(\mathbf{t}^{\prime}, \mathbf{t}^{\prime}\right) \neq 0$ (see Appendix for further discussion). Such a kernel choice thus captures the desired prior information on numerical convergence. With Kernel 2, the resulting prior</p>
<p>process on discrepancy $\delta$ can be viewed as a multivariate extension of a standard Brownian motion model (Durrett 2019), and extends the non-stationary model (7) in Tuo et al. (2014), which tackled only the case of one fidelity parameter.</p>
<p>Kernel 2 has several appealing features for conglomerate multi-fidelity emulation, particularly when the multiple stages are performed sequentially over time (see Scenario 2 at the start of the section). One can show that the parametrization of this kernel is directly inspired by (and thus can capture prior information on) standard numerical convergence results for multi-stage simulators. To see why, consider first the simple setting of a single fidelity parameter $t$, and let $v_{0}$ and $v_{t}$ be the exact and simulated solutions at fidelity $t$ respectively. In the case of finite element analysis (where $t$ is the mesh grid size), it is well-known (Brenner \&amp; Scott 2008) that the numerical error of the simulator can be upper bounded as:</p>
<p>$$
\left|\nu_{0}-\nu_{t}\right| \leq C t^{\xi}
$$</p>
<p>where $|\cdot|$ is an appropriate norm on the solution space, $\xi$ is a rate parameter, and $C$ is a constant. In words, the numerical error resulting from mesh discretization decays polynomially as mesh size $t$ decreases. Similar polynomial decay rates have also been shown for a broad range of fidelity parameters in numerical solvers, e.g., for elliptical PDEs (Hundsdorfer et al. 2003) and large-eddy simulations in fluid mechanics (Templeton et al. 2015).</p>
<p>Consider now the multi-stage simulators from Scenario 2, where a separate phenomenon is simulated sequentially at each stage. Suppose, at stage $r$, its precision is controlled by a fidelity parameter $t_{r}$. For this parameter $t_{r}$, further suppose the simulation error at this stage can be bounded by (20) with rate parameter $\xi_{r}$. One example of this is multistage finite element simulators when each stage involves a distinct finite element model (FEM) whose precision depends on a mesh size parameter $t_{r}$. Similar to before, let $\nu_{0}$ and $\nu_{t_{1}, \cdots, t_{q}}$ denote the exact solution and the simulated solution at fidelity parameters $t_{1}, \cdots, t_{q}$. Applying the triangle inequality iteratively, the error between $\nu_{t_{1}, \cdots, t_{q}}$ and $\nu_{0}$ can then be bounded as:</p>
<p>$$
\begin{aligned}
\left|v_{\mathbf{0}}-v_{t_{1}, \cdots, t_{q}}\right| &amp; \leq\left|v_{\mathbf{0}}-v_{t_{1}, 0, \cdots, 0}\right|+\left|v_{t_{1}, 0, \cdots, 0}-v_{t_{1}, t_{2}, 0, \cdots, 0}\right|+\cdots \
&amp; +\left|v_{t_{1}, \cdots, t_{q-1}, 0}-v_{t_{1}, \cdots, t_{q-1}, t_{q}}\right| \
&amp; \leq \sum_{r=1}^{q} C_{r} t_{r}^{\xi_{r}}
\end{aligned}
$$</p>
<p>where $C_{1}, \cdots, C_{q}$ are again constants. We now show that Kernel 2 indeed captures the error bound (21) as prior information within its kernel specification. To see why, consider the prior standard deviation of the discrepancy term $\delta(\mathbf{x}, \mathbf{t})$. From a Bayesian modeling perspective, this should capture the modeler's prior belief on the expected numerical error of the simulator. With $K_{\mathbf{t}}$ set as Kernel 2, one can show that this prior standard deviation takes the form:</p>
<p>$$
\sqrt{\operatorname{Var}{\delta(\mathbf{x}, \mathbf{t})}}=\sigma_{2}\left[\sum_{r=1}^{q} \theta_{r} t_{r}^{l_{r}}\right]^{1 / 2}
$$</p>
<p>Comparing (22) with (21), we see that they are precisely the same with the kernel hyperparameters set as $l=2$ and $l_{r}=\xi_{r}$ for $r=1, \cdots, q$. This suggests that with $K_{\mathbf{t}}$ chosen as Kernel 2, the resulting prior model on discrepancy $\delta(\mathbf{x}, \mathbf{t})$ indeed captures (on expectation) the numerical error convergence of the multi-stage simulator.</p>
<p>The above connection also helps guide how the specification of hyperparameters for Kernel 2. If the rate parameters $\xi_{1}, \cdots, \xi_{q}$ can be identified via a careful analysis of the error bound (20) at each stage, one can use simply set the hyperparameters as $l_{r}=\xi_{r}$ for $r=1, \cdots, q$. However, for more complex multi-stage simulators, one may not be able to identify the precise error convergence rates at each stage. In such cases, the kernel hyperparameters can be estimated via maximum likelihood or a fully Bayesian approach (see Section 4.1) or set at a fixed value (e.g., $l_{r}=l=2$ ). Whether such hyperparameters are set a priori or inferred from data, the infusion of such prior information can yield noticeably improved predictive performance for multi-fidelity emulation, as we show later in Section 6.2.</p>
<p>It is worth noting that, with the Brownian-like Kernel 2, sample paths from the discrepancy process $\delta(\mathbf{x}, \mathbf{t})$ will be highly non-smooth. In particular, within any neighborhood around $\mathbf{t}=\mathbf{0}$, the discrepancy $\delta(\mathbf{x}, \mathbf{t})$ will equal 0 an infinite number of times. This may be unintuitive for other properties of discretization error (see, e.g., Equation (1) of Bect et al. (2021)), which require that $\delta(\mathbf{x}, \mathbf{t})=0$ only when $\mathbf{t}=0$. Our justification for Kernel 2 is not from such properties, but rather from its ability to embed prior information on expected numerical convergence via its non-stationary specification. In applications where trajectory smoothness is a concern, Kernel 1 may be a better kernel choice; more on this below.</p>
<p>Figure 3 visualizes the two proposed nonstationary kernels in the simple setting with a single fidelity parameter $t$. For Kernel 1, we set $\theta_{q}=1$ and $q=1$, and for Kernel 2, we set $l=l_{q}=2$ and $q=1$. We see these two kernels have noticeably different shapes: Kernel 1 shows a smooth and gradual increase as either $t_{1}$ or $t_{2}$ increases, whereas Kernel 2 exhibits a sharper increase and has a cusp along the line $t_{1}=t_{2}$. This cusp causes the highly non-smooth sample paths from Kernel 2, whereas the smoother Kernel 1 induces smoother sample paths, as can be seen from the corresponding sample paths in Figure 3.</p>
<h1>3.3 Kernel Recommendation</h1>
<p>We provide next a concise summary of kernel recommendation for the CONFIG model. In applications where the conglomerate simulator uses multiple fidelity parameters (e.g., spatial mesh size or temporal discretization) for simulating a single mechanism (Scenario 1), we recommend the use of Kernel 1 (34), which can better account for stronger interactions between different fidelity parameters. We will encounter such an application in Section 6.1. On the other hand, in applications where the conglomerate simulator comprises of multiple sequential stages that model for separate mechanisms, we recommend the use of Kernel 2 (38), which can be justified via numerical error analysis for these simulators. Our motivating QGP application falls within this setting, which we will investigate further in Section 6.2.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Visualizing Kernel 1 (34) with corresponding sample paths using $q=1$ and $\theta=1$.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Visualizing Kernel 2 (38) with corresponding sample paths</p>
<p>$$
\text { using } q=1, \theta=1, l=l_{1}=2 \text {. }
$$</p>
<p>Figure 3: Visualization of both kernel options (with corresponding sample paths) for CONFIG with a single fidelity parameter $t$.</p>
<p>There are, of course, applications that may not cleanly fall within the two presented scenarios; in such cases, careful consideration is needed for an informed kernel specification. In our later numerical experiments, we have found that when there is little prior knowledge on the degree of interaction between fidelity parameters, Kernel 2 seems to be a much more robust choice for predictive modeling; we would thus recommend Kernel 2 for such problems.</p>
<h1>4 Implementation</h1>
<p>We now discuss important implementation details for the CONFIG model. We present two parameter inference approaches, the first via maximum likelihood and the second via a fully Bayesian formulation for incorporating external knowledge and richer uncertainty quantification. We then outline plausible experimental design strategies.</p>
<h1>4.1 Parameter Inference</h1>
<h3>4.1.1 Maximum Likelihood</h3>
<p>We first present a maximum likelihood approach for estimating the CONFIG model parameters. Let $\boldsymbol{\Theta}<em 1="1">{\text {MLE }}=\left(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\alpha}, \boldsymbol{\theta}, \sigma</em>}^{2}, \sigma_{2}^{2}\right)$ be the set of parameters to infer, where $\gamma$ is the vector of weight parameters for $K_{\mathbf{x}}^{\phi}, \boldsymbol{\alpha}$ is the vector of weight parameters for $K_{\mathbf{x}}^{\delta}$, and $\boldsymbol{\theta}$ is the vector of weight parameters for the CONFIG kernel $K_{\mathbf{t}}$ (either Kernel 1 or Kernel 2). Here, we presume the hyperparameters $l_{r}$ in Kernel 2 (38) are pre-specified (similar to Tuo et al. (2014)) and thus not included in the parameter set $\boldsymbol{\Theta<em r="r">{\text {MLE }}$; in the setting where $l</em>}$ needs to be estimated, we can simply include them in $\boldsymbol{\Theta<em i="i">{\text {MLE }}$. Since $\eta(\mathbf{x}, \mathbf{t})$ can be expressed as a GP with kernel given in (15), one can easily obtain an analytic expression for the likelihood function to optimize. More specifically, let the simulated multi-fidelity training data be $\mathbf{y}=\left(\eta\left(\mathbf{x}</em>}, \mathbf{t<em i="1">{i}\right)\right)</em>}^{n}$, and let the matrix of basis functions for the GP mean be $\mathbf{F}=\left(\mathbf{f}\left(\mathbf{x<em 1="1">{1}, \mathbf{t}</em>}\right)^{T} ; \mathbf{f}\left(\mathbf{x<em 2="2">{2}, \mathbf{t}</em>}\right)^{T} ; \cdots ; \mathbf{f}\left(\mathbf{x<em n="n">{n}, \mathbf{t}</em>$. We thus aim to maximize the log-likelihood of the CONFIG model, given by:}\right)^{T}\right)$, with corresponding coefficients $\boldsymbol{\beta</p>
<p>$$
\max <em _mathrm_MLE="\mathrm{MLE">{\Theta</em>)\right}
$$}}}\left{-\frac{1}{2} \log \operatorname{det} \boldsymbol{\Sigma}-\frac{1}{2}(\mathbf{y}-\mathbf{F} \boldsymbol{\beta})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{y}-\mathbf{F} \boldsymbol{\beta</p>
<p>where $\operatorname{det} \boldsymbol{\Sigma}$ is the determinant of the covariance matrix $\boldsymbol{\Sigma}$.
While the optimization problem (23) is quite high-dimensional, standard non-linear optimization algorithms, such as the L-BFGS-B method (Nocedal \&amp; Wright 1999) appears to work well. One can further speed up this optimization procedure via an informed initialization of the parameters $\boldsymbol{\Theta}<em _mathbf_x="\mathbf{x">{\text {MLE }}$. In particular, we have found that the correlation parameters $\boldsymbol{\alpha}$ can be well-initialized by first fitting a standard Gaussian process model with kernel $K</em>$ over the full training data (ignoring fidelity parameters). With these initial estimates, we then perform the L-BFGS-B non-linear optimization, as implemented in the R package stats (Byrd et al. 1995).}}^{\delta</p>
<p>After maximum likelihood estimation, we would ideally like to integrate such estimates along with their uncertainties within the GP predictive equations (2), to predict the limiting highest-fidelity surface $\eta\left(\mathbf{x}^{<em>}, \mathbf{0}\right)$ at a new input $\mathbf{x}^{</em>}$. However, this integration of uncertainty</p>
<p>is difficult to do in closed form for all parameters (see (Santner et al. 2003)). We can, however, integrate estimation uncertainty on the mean coefficients $\boldsymbol{\beta}$ in an efficient manner. Following (Kennedy \&amp; O'Hagan 2000, Bect et al. 2021), the CONFIG predictive mean and variance of $\eta\left(\mathbf{x}^{<em>}, \mathbf{0}\right)$ with such uncertainty integrated (denoted as $\hat{\mu}_{\mathbf{0}}\left(\mathbf{x}^{</em>}\right)$ and $s_{\mathbf{0}}^{2}\left(\mathbf{x}^{*}\right)$, respectively) becomes:</p>
<p>$$
\begin{aligned}
\hat{\mu}_{\mathbf{0}}\left(\mathbf{x}^{<em>}\right)= &amp; \mu\left(\mathbf{x}^{</em>}, \mathbf{0}\right)+\mathbf{k}\left(\mathbf{x}^{<em>}, \mathcal{D}\right)^{T} \mathbf{K}(\mathcal{D})^{-1}\left(\mathbf{y}<em _mathbf_0="\mathbf{0">{\mathcal{D}}-\boldsymbol{\mu}(\mathcal{D})\right) \
s</em>^{}}^{2}\left(\mathbf{x</em>}\right)= &amp; k\left(\mathbf{x}^{<em>}, \mathbf{x}^{</em>}\right)-\mathbf{k}\left(\mathbf{x}^{<em>}, \mathcal{D}\right)^{T} \mathbf{K}(\mathcal{D})^{-1} \mathbf{k}\left(\mathbf{x}^{</em>}, \mathcal{D}\right)+\left(\mathbf{f}\left(\mathbf{x}^{<em>}, \mathbf{0}\right)-\mathbf{k}\left(\mathbf{x}^{</em>}, \mathcal{D}\right)^{T} \mathbf{K}(\mathcal{D})^{-1} \mathbf{F}\right)^{T} \
&amp; \left(\mathbf{F}^{T} \mathbf{K}(\mathcal{D})^{-1} \mathbf{F}\right)^{-1}\left(\mathbf{f}\left(\mathbf{x}^{<em>}, \mathbf{0}\right)-\mathbf{k}\left(\mathbf{x}^{</em>}, \mathcal{D}\right)^{T} \mathbf{K}(\mathcal{D})^{-1} \mathbf{F}\right)
\end{aligned}
$$</p>
<p>Unknown model parameters in (24) can then be plugged in via the maximum likelihood estimates (23). With this, we can then construct the $95 \%$ predictive interval on the limiting highest-fidelity output $\eta\left(\mathbf{x}^{*}, \mathbf{0}\right)$ as:</p>
<p>$$
\left(\hat{\mu}<em _mathbf_0="\mathbf{0">{\mathbf{0}}\left(\mathbf{x}^{<em>}\right)-1.96 \sqrt{s_{\mathbf{0}}^{2}\left(\mathbf{x}^{</em>}\right)}, \hat{\mu}</em>^{}}\left(\mathbf{x<em>}\right)+1.96 \sqrt{s_{\mathbf{0}}^{2}\left(\mathbf{x}^{</em>}\right)}\right)
$$</p>
<h1>4.1.2 Fully Bayesian Inference</h1>
<p>In situations where a richer quantification of uncertainty is desired, a fully Bayesian approach to parameter inference may be appropriate. Below, we present one such approach for the CONFIG model which leverages a Metropolis-within-Gibbs algorithm (Gelman et al. 1995) for posterior sampling. For an easier derivation of the full conditional distributions, we consider a reparametrization of the covariance kernel (15) for $\eta(\mathbf{x}, \mathbf{t})$ as:</p>
<p>$$
K_{\eta}\left{\left(\mathbf{x}<em 1="1">{1}, \mathbf{t}</em>}\right),\left(\mathbf{x<em 2="2">{2}, \mathbf{t}</em>}\right)\right}=\sigma^{2}\left{K_{\mathbf{x}}^{\phi}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}\right)+\lambda K_{\mathbf{x}}^{\delta}\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}\right) K_{\mathbf{t}}\left(\mathbf{t<em 2="2">{1}, \mathbf{t}</em>\right)\right}
$$</p>
<p>where $\sigma^{2}:=\sigma_{1}^{2}$ and $\lambda:=\sigma_{2}^{2} / \sigma_{1}^{2}$. Here, the new parameter $\lambda$ captures the degree of nonstationarity in the kernel from the influence of the fidelity parameters $\mathbf{t}$. When $\lambda=0$, the covariance kernel becomes a stationary kernel that depends on only input parameters $\mathbf{x}$.</p>
<p>With this reparametrization, the parameter set to infer is given by $\boldsymbol{\Theta}_{\mathrm{B}}=(\boldsymbol{\beta}, \boldsymbol{\gamma}, \boldsymbol{\alpha}, \boldsymbol{\theta}, \sigma^{2}, \lambda)$. It is straightforward to show that:</p>
<p>$$
\mathbf{y} \mid \boldsymbol{\Theta}_{\mathrm{B}} \sim \mathcal{N}(\mathbf{F} \boldsymbol{\beta}, \boldsymbol{\Sigma})
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Department of Statistical Science, Duke University
${ }^{2}$ H. Milton Stewart School of Industrial \&amp; Systems Engineering, Georgia Institute of Technology
${ }^{3}$ Department of Physics, Duke University
${ }^{4}$ Department of Physics and Astronomy \&amp; Department of Mathematics, Vanderbilt University
${ }^{5}$ Joint first authors
${ }^{*}$ Corresponding author&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>